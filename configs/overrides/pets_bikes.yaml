# @package _group_
env: bikes
env_config:
  num_trucks: 10
  action_per_day: 2
  next_day_method: random #sequential
  initial_distribution: zeros #zeros
  bikes_per_truck: 8
  fix_bikes_per_truck: true
  start_walk_dist_max: 0.2
  end_walk_dist_max: 1000.
  trip_duration: 0.5
  past_trip_data: null #src/env/bikes_data/all_trips_LouVelo_merged.csv
  weather_data: null #src/env/bikes_data/weather_data.csv
  centroids_coord: src/env/bikes_data/LouVelo_centroids/LouVelo_centroids_coords.npy #src/env/bikes_data/acbo_centroids/acbo_centroids_coords.npy #src/env/bikes_data/LouVelo_centroids/LouVelo_centroids_coords.npy
  #centroids_idx: [93,83,80,98,6,84,87,99,56,97] #Number of centroids or list of indices
  station_dependencies: src/env/bikes_data/LouVelo_centroids/factors_radius_1-2.npy #src/env/bikes_data/acbo_centroids/factors_radius_3.npy #src/env/bikes_data/LouVelo_centroids/factors_radius_1-2.npy
model_wrapper:
  _target_: src.model.dict_model_wrapper.OneDTransitionRewardModelDictSpace
  model_input_obs_key: ["bikes_distr", "time_counter"] #["bikes_distr","day_of_week", "demands", "month", "time_counter"]
  model_input_act_key: []
  model_output_key: ["bikes_distr"]
learned_rewards: true
trial_length: ${overrides.env_config.action_per_day} #Max length episode
initial_exploration_steps: 100
num_steps: 100000000 #Number of env steps before end of training
num_episodes: 10000 #Number of env episodes before end of training
render_mode: human #null|rgb_array|human
model_path: null #/mnt/c/Users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/exp/pets_adapted/wandb/hypergrid/2023.10.06/154944

#num_elites: 5
model_batch_size: 256
validation_ratio: 0
freq_train_model: 20
patience: ${overrides.num_epochs_train_model}
num_epochs_train_model: 25
dataset_size: 100000

planning_horizon: ${overrides.env_config.action_per_day}
cem_num_iters: 5
cem_elite_ratio: 0.1
cem_population_size: 350
cem_alpha: 0.1 #0.1 ??
cem_clipped_normal: false