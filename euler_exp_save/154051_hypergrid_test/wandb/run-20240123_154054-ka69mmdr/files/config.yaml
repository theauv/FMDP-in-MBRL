wandb_version: 1

seed:
  desc: null
  value: 0
device:
  desc: null
  value: cpu
run_name:
  desc: null
  value: hypergrid_test
debug_mode:
  desc: null
  value: false
silent:
  desc: null
  value: false
dataset_folder_name:
  desc: null
  value: datasets
dataset_size:
  desc: null
  value: 100000
model_batch_size:
  desc: null
  value: 10
validation_ratio:
  desc: null
  value: 0.2
num_epochs_train_model:
  desc: null
  value: 100000
patience:
  desc: null
  value: 100000
learned_rewards:
  desc: null
  value: false
experiment_dir:
  desc: null
  value: wandb
root_dir:
  desc: null
  value: ./exp
action_optimizer:
  desc: null
  value:
    _target_: mbrl.planning.CEMOptimizer
    num_iterations: 5
    elite_ratio: 0.1
    population_size: 350
    alpha: 0.1
    lower_bound: ???
    upper_bound: ???
    return_mean_elites: true
    device: cpu
    clipped_normal: false
algorithm:
  desc: null
  value:
    name: pets_adapted
    agent:
      _target_: mbrl.planning.TrajectoryOptimizerAgent
      action_lb: ???
      action_ub: ???
      planning_horizon: 15
      optimizer_cfg:
        _target_: mbrl.planning.CEMOptimizer
        num_iterations: 5
        elite_ratio: 0.1
        population_size: 350
        alpha: 0.1
        lower_bound: ???
        upper_bound: ???
        return_mean_elites: true
        device: cpu
        clipped_normal: false
      replan_freq: 1
      verbose: false
    normalize: true
    normalize_double_precision: true
    target_is_delta: true
    initial_exploration_steps: 400
    freq_train_model: 50
    learned_rewards: false
    dataset_size: 500000
    num_particles: 20
dynamics_model:
  desc: null
  value:
    model_trainer:
      _target_: src.util.model_trainer.ModelTrainerOverriden
    model:
      _target_: src.model.simple.Simple
      device: cpu
      num_layers: 6
      in_size: ???
      out_size: ???
      hid_size: 400
      activation_fn_cfg:
        _target_: torch.nn.SiLU
overrides:
  desc: null
  value:
    env: hypergrid
    env_config:
      step_penalty: -1
      grid_dim: 2
      grid_size: 5.0
      size_end_box: 1.0
      step_size: 1.0
      n_obstacles: null
      size_obstacles: 1
    model_wrapper:
      _target_: mbrl.models.OneDTransitionRewardModel
    learned_rewards: false
    trial_length: 400
    num_steps: 500000
    num_episodes: 100
    render_mode: null
    model_path: null
    num_elites: 5
    model_lr: 0.00075
    model_wd: 3.0e-05
    model_batch_size: 256
    validation_ratio: 0
    freq_train_model: 50
    patience: 25
    num_epochs_train_model: 25
    dataset_size: 500000
    planning_horizon: 15
    cem_num_iters: 5
    cem_elite_ratio: 0.1
    cem_population_size: 350
    cem_alpha: 0.1
    cem_clipped_normal: false
experiment:
  desc: null
  value:
    with_tracking: true
    plot_local: false
    api_name: wandb
    run_configs:
      project: hucrl_fmdp
      group: null
      name: test_model_benchmark
      settings: null
      tags: null
_wandb:
  desc: null
  value:
    python_version: 3.9.9
    cli_version: 0.15.11
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1706020854.222895
    t:
      1:
      - 1
      - 5
      - 50
      - 53
      - 55
      2:
      - 1
      - 5
      - 50
      - 53
      - 55
      3:
      - 7
      - 13
      - 16
      - 23
      4: 3.9.9
      5: 0.15.11
      8:
      - 5
      13: linux-x86_64
    m:
    - 1: env_episode
      6:
      - 2
      - 3
    - 1: episode_steps
      5: 1
      6:
      - 1
      - 3
    - 1: episode_reward
      5: 1
      6:
      - 1
      - 3
    - 1: env_step
      6:
      - 2
      - 3
    - 1: step_reward
      5: 4
      6:
      - 1
      - 3
    - 1: step_time_0_6
      6:
      - 2
      - 3
    - 1: step_reward_0_6
      5: 6
      6:
      - 1
      - 3
    - 1: step_time_6_12
      6:
      - 2
      - 3
    - 1: step_reward_6_12
      5: 8
      6:
      - 1
      - 3
    - 1: step_time_12_18
      6:
      - 2
      - 3
    - 1: step_reward_12_18
      5: 10
      6:
      - 1
      - 3
    - 1: step_time_18_24
      6:
      - 2
      - 3
    - 1: step_reward_18_24
      5: 12
      6:
      - 1
      - 3
    - 1: train_iteration
      6:
      - 2
      - 3
    - 1: total_avg_loss
      5: 20
      6:
      - 1
      - 3
    - 1: eval_score
      5: 20
      6:
      - 1
      - 3
    - 1: best_eval_score
      5: 20
      6:
      - 1
      - 3
    - 1: trajectory_optimizer_iteration
      6:
      - 2
      - 3
    - 1: trajectory_optimizer_eval
      5: 18
      6:
      - 1
      - 3
    - 1: train_epoch
      6:
      - 2
      - 3
