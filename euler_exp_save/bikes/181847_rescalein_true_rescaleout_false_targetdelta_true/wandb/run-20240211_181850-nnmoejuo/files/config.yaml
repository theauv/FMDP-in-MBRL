wandb_version: 1

seed:
  desc: null
  value: 0
device:
  desc: null
  value: cpu
log_frequency_agent:
  desc: null
  value: 1000
save_video:
  desc: null
  value: false
debug_mode:
  desc: null
  value: false
experiment_dir:
  desc: null
  value: wandb
root_dir:
  desc: null
  value: ./exp
algorithm:
  desc: null
  value:
    name: pets_adapted
    agent:
      _target_: mbrl.planning.TrajectoryOptimizerAgent
      action_lb: ???
      action_ub: ???
      planning_horizon: 4
      optimizer_cfg:
        _target_: mbrl.planning.CEMOptimizer
        num_iterations: 20
        elite_ratio: 0.1
        population_size: 350
        alpha: 0.1
        lower_bound: ???
        upper_bound: ???
        return_mean_elites: true
        device: cpu
        clipped_normal: false
      replan_freq: 1
      verbose: false
    normalize: false
    rescale_input: true
    rescale_output: false
    normalize_double_precision: true
    target_is_delta: true
    initial_exploration_steps: 4
    freq_train_model: 20
    learned_rewards: true
    dataset_size: 100000
    num_particles: 20
dynamics_model:
  desc: null
  value:
    model_trainer:
      _target_: src.util.model_trainer.ModelTrainerOverriden
    model:
      _target_: src.model.gaussian_process.MultiOutputGP
      device: cpu
      in_size: ???
      out_size: ???
      mean: Linear
      kernel: Matern
      scale_kernel: true
      reward_factors: null
    batch_size: 100000
    model_lr: 0.05
    model_wd: 0.0
overrides:
  desc: null
  value:
    env: bikes
    env_config:
      num_trucks: 1
      action_per_day: 4
      next_day_method: random
      initial_distribution: zeros
      bikes_per_truck: 5
      fix_bikes_per_truck: true
      start_walk_dist_max: 0.2
      end_walk_dist_max: 1000.0
      trip_duration: 0.5
      past_trip_data: null
      weather_data: null
      centroids_coord: src/env/bikes_data/5_centroids/5_centroids.npy
      station_dependencies: src/env/bikes_data/5_centroids/factors_radius_4.npy
    model_wrapper:
      _target_: src.model.dict_model_wrapper.OneDTransitionRewardModelDictSpace
      model_input_obs_key:
      - bikes_distr
      - time_counter
      model_input_act_key: []
      model_output_key:
      - bikes_distr
    learned_rewards: true
    trial_length: 4
    initial_exploration_steps: 50
    num_steps: 100000000
    num_episodes: 10000
    render_mode: null
    model_path: null
    model_batch_size: 256
    validation_ratio: 0
    freq_train_model: 20
    patience: 10
    num_epochs_train_model: 10
    dataset_size: 100000
    planning_horizon: 4
    cem_num_iters: 20
    cem_elite_ratio: 0.1
    cem_population_size: 350
    cem_alpha: 0.1
    cem_clipped_normal: false
action_optimizer:
  desc: null
  value:
    _target_: mbrl.planning.CEMOptimizer
    num_iterations: 20
    elite_ratio: 0.1
    population_size: 350
    alpha: 0.1
    lower_bound: ???
    upper_bound: ???
    return_mean_elites: true
    device: cpu
    clipped_normal: false
experiment:
  desc: null
  value:
    with_tracking: true
    plot_local: false
    api_name: wandb
    run_configs:
      project: hucrl_fmdp
      group: test_targetisdelta_art_4step_5centroid_14429fa0e0cb397e6f42a58ad0c9f1502f2f180d
      name: rescalein_true_rescaleout_false_targetdelta_true
      settings: null
      tags: null
_wandb:
  desc: null
  value:
    python_version: 3.9.9
    cli_version: 0.15.11
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1707671930.586161
    t:
      1:
      - 1
      - 5
      - 41
      - 50
      - 53
      - 55
      2:
      - 1
      - 5
      - 41
      - 50
      - 53
      - 55
      3:
      - 7
      - 13
      - 16
      - 23
      4: 3.9.9
      5: 0.15.11
      8:
      - 5
      13: linux-x86_64
    m:
    - 1: env_episode
      6:
      - 2
      - 3
    - 1: episode_steps
      5: 1
      6:
      - 1
      - 3
    - 1: episode_reward
      5: 1
      6:
      - 1
      - 3
    - 1: env_step
      6:
      - 2
      - 3
    - 1: step_reward
      5: 4
      6:
      - 1
      - 3
    - 1: step_time_0_6
      6:
      - 2
      - 3
    - 1: step_reward_0_6
      5: 6
      6:
      - 1
      - 3
    - 1: step_time_6_12
      6:
      - 2
      - 3
    - 1: step_reward_6_12
      5: 8
      6:
      - 1
      - 3
    - 1: step_time_12_18
      6:
      - 2
      - 3
    - 1: step_reward_12_18
      5: 10
      6:
      - 1
      - 3
    - 1: step_time_18_24
      6:
      - 2
      - 3
    - 1: step_reward_18_24
      5: 12
      6:
      - 1
      - 3
    - 1: train_iteration
      6:
      - 2
      - 3
    - 1: total_avg_loss
      5: 22
      6:
      - 1
      - 3
    - 1: eval_score
      5: 22
      6:
      - 1
      - 3
    - 1: best_eval_score
      5: 22
      6:
      - 1
      - 3
    - 1: train_r2_score
      5: 22
      6:
      - 1
      - 3
    - 1: eval_r2_score
      5: 22
      6:
      - 1
      - 3
    - 1: trajectory_optimizer_iteration
      6:
      - 2
      - 3
    - 1: trajectory_optimizer_eval
      5: 20
      6:
      - 1
      - 3
    - 1: train_epoch
      6:
      - 2
      - 3
