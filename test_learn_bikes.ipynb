{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "from mbrl.models import Model\n",
    "from src.env.bikes import Bikes\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load env dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 46]) torch.Size([10000, 10])\n",
      "{'bikes_distr': slice(0, 43, None), 'day': slice(43, 44, None), 'month': slice(44, 45, None), 'time_counter': slice(45, 46, None)} {'truck_centroid': slice(0, 5, None), 'truck_num_bikes': slice(5, 10, None)}\n"
     ]
    }
   ],
   "source": [
    "load_dir = \"datasets/Bikes/None\"\n",
    "path = pathlib.Path(load_dir) / \"replay_buffer.npz\"\n",
    "buffer=np.load(path)\n",
    "next_obs = torch.tensor(buffer[\"next_obs\"], dtype=torch.float32)\n",
    "obs = torch.tensor(buffer[\"obs\"], dtype=torch.float32)\n",
    "act = torch.round(torch.tensor(buffer[\"action\"], dtype=torch.float32))\n",
    "reward = torch.tensor(buffer[\"reward\"], dtype=torch.float32)\n",
    "print(obs.shape, act.shape)\n",
    "\n",
    "num_centroids = 43\n",
    "map_obs = {\n",
    "    \"bikes_distr\": slice(0, num_centroids),\n",
    "    \"day\": slice(num_centroids, 44),\n",
    "    \"month\": slice(44, 45),\n",
    "    \"time_counter\": slice(45, 46),\n",
    "}\n",
    "num_trucks = act.shape[-1]//2\n",
    "map_act={\n",
    "    \"truck_centroid\": slice(0, num_trucks),\n",
    "    \"truck_num_bikes\": slice(num_trucks, 2*num_trucks),\n",
    "}\n",
    "\n",
    "print(map_obs, map_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute delta bikes and obs += delta_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n",
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n"
     ]
    }
   ],
   "source": [
    "obs_before_action = obs.clone()\n",
    "\n",
    "resize = False\n",
    "while obs.ndim < 3:\n",
    "    assert act.ndim == obs.ndim\n",
    "    obs = obs[None, ...]\n",
    "    act = act[None, ...]\n",
    "    resize = True\n",
    "\n",
    "ensemble_size = obs.shape[0]\n",
    "batch_size = obs.shape[1]\n",
    "distr_size = len(obs[0, 0, map_obs[\"bikes_distr\"]]) #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "# Compute delta_bikes in a parallel way\n",
    "delta_bikes = np.zeros((ensemble_size, batch_size, distr_size), dtype=int)\n",
    "truck_centroids = act[..., map_act[\"truck_centroid\"]] #self.map_act[\"truck_centroid\"]\n",
    "truck_bikes = act[..., map_act[\"truck_num_bikes\"]] #self.map_act[\"truck_num_bikes\"]\n",
    "n = distr_size\n",
    "truck_centroids = np.reshape(\n",
    "    truck_centroids, (truck_centroids.shape[0] * truck_centroids.shape[1], -1)\n",
    ")\n",
    "offset = np.arange(truck_centroids.shape[0])[..., None]\n",
    "truck_centroids_offset = truck_centroids + offset * n\n",
    "unq, inv = np.unique(truck_centroids_offset.ravel(), return_inverse=True)\n",
    "unq = unq.astype(int)\n",
    "sol = np.bincount(inv, truck_bikes.ravel())\n",
    "delta_bikes[\n",
    "    unq // (batch_size * n),\n",
    "    (unq % (batch_size * n)) // n,\n",
    "    (unq % (batch_size * n)) % n,\n",
    "] = sol\n",
    "\n",
    "if resize:\n",
    "    delta_bikes = delta_bikes.reshape((batch_size, -1))\n",
    "    act = act.reshape((batch_size, -1))\n",
    "    obs = obs.reshape((batch_size, -1))\n",
    "\n",
    "# Update obs\n",
    "obs[..., map_obs[\"bikes_distr\"]] += delta_bikes #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "print(torch.sum(obs-obs_before_action, axis=-1))\n",
    "print(torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "assert torch.all(torch.sum(obs-obs_before_action, axis=-1) == torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "# Super long check to see if preprocess is good, and it is so far\n",
    "for obs_, previous_obs_, truck_centroids, truck_num_bikes in zip(obs, obs_before_action, act[...,map_act[\"truck_centroid\"]], act[...,map_act[\"truck_num_bikes\"]]):\n",
    "    bikes_idx = torch.nonzero(truck_num_bikes, as_tuple=True)[0]\n",
    "    truck_centroids = truck_centroids[bikes_idx]\n",
    "    centroids_new_bikes = torch.nonzero(obs_-previous_obs_, as_tuple=True)[0]\n",
    "    # print(torch.sort(centroids_new_bikes).values)\n",
    "    # print(torch.unique(truck_centroids))\n",
    "    # print(torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids))\n",
    "    assert torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y from obs and next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 44]) torch.Size([10000, 43])\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 10000\n",
    "input_obs_keys = [\"bikes_distr\", \"time_counter\"]\n",
    "input_act_keys = [] #not implemented\n",
    "output_keys = [\"bikes_distr\"]\n",
    "\n",
    "input_mask = np.zeros(obs.shape[-1])\n",
    "for key in input_obs_keys:\n",
    "    input_mask[map_obs[key]] = 1\n",
    "input_mask = np.ma.make_mask(input_mask)\n",
    "\n",
    "output_mask = np.zeros(obs.shape[-1])\n",
    "for key in output_keys:\n",
    "    output_mask[map_obs[key]] = 1\n",
    "output_mask = np.ma.make_mask(output_mask)\n",
    "\n",
    "assert obs.ndim == 2\n",
    "assert next_obs.ndim == 2\n",
    "x = obs[:dataset_size, input_mask]\n",
    "y = next_obs[:dataset_size, output_mask]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe add a date proxy, weather ? holiday, week-end ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional output preds (e.g. reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 43]) torch.Size([10000, 1])\n",
      "torch.Size([10000, 44])\n"
     ]
    }
   ],
   "source": [
    "learned_rewards = True\n",
    "if learned_rewards:\n",
    "    reward_ = reward[:dataset_size, ...].unsqueeze(-1)\n",
    "    print(y.shape, reward_.shape)\n",
    "    y = torch.cat([y, reward_], dim=-1)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_x, test_x and train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n"
     ]
    }
   ],
   "source": [
    "test_split_ratio = 0.2\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# idx = torch.randperm(x.shape[0])\n",
    "# x = x[idx, :]\n",
    "# y = y[idx, :]\n",
    "\n",
    "train_x = x[int(test_split_ratio*dataset_size):, ...]\n",
    "train_y = y[int(test_split_ratio*dataset_size):, ...]\n",
    "test_x = x[:int(test_split_ratio*dataset_size), ...]\n",
    "test_y = y[:int(test_split_ratio*dataset_size), ...]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.linear_regression import LinearRegression\n",
    "\n",
    "learningRate = 0.01 \n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 6.957015514373779, R2 -2.453197717666626\n",
      "Eval loss 6.7792158126831055, R2 -2.1895952224731445\n",
      "epoch 1, loss 6.699021339416504, R2 -2.350365161895752\n",
      "Eval loss 6.543105602264404, R2 -2.10159969329834\n",
      "epoch 2, loss 6.4637556076049805, R2 -2.2563884258270264\n",
      "Eval loss 6.327634334564209, R2 -2.0212156772613525\n",
      "epoch 3, loss 6.2491135597229, R2 -2.170450210571289\n",
      "Eval loss 6.130894660949707, R2 -1.9477280378341675\n",
      "epoch 4, loss 6.053185939788818, R2 -2.0918033123016357\n",
      "Eval loss 5.951156139373779, R2 -1.8805022239685059\n",
      "epoch 5, loss 5.874239921569824, R2 -2.019775152206421\n",
      "Eval loss 5.786849498748779, R2 -1.8189501762390137\n",
      "epoch 6, loss 5.710702419281006, R2 -1.9537533521652222\n",
      "Eval loss 5.6365509033203125, R2 -1.7625449895858765\n",
      "epoch 7, loss 5.561150074005127, R2 -1.893183946609497\n",
      "Eval loss 5.498967170715332, R2 -1.7108089923858643\n",
      "epoch 8, loss 5.424286365509033, R2 -1.8375617265701294\n",
      "Eval loss 5.372925281524658, R2 -1.6633076667785645\n",
      "epoch 9, loss 5.298941135406494, R2 -1.7864333391189575\n",
      "Eval loss 5.2573628425598145, R2 -1.6196480989456177\n",
      "epoch 10, loss 5.184048175811768, R2 -1.7393814325332642\n",
      "Eval loss 5.151313781738281, R2 -1.5794700384140015\n",
      "epoch 11, loss 5.078641891479492, R2 -1.6960303783416748\n",
      "Eval loss 5.053902626037598, R2 -1.542452335357666\n",
      "epoch 12, loss 4.981847763061523, R2 -1.6560401916503906\n",
      "Eval loss 4.9643354415893555, R2 -1.5083022117614746\n",
      "epoch 13, loss 4.892869472503662, R2 -1.6191012859344482\n",
      "Eval loss 4.881890296936035, R2 -1.4767524003982544\n",
      "epoch 14, loss 4.8109869956970215, R2 -1.5849324464797974\n",
      "Eval loss 4.805913925170898, R2 -1.4475606679916382\n",
      "epoch 15, loss 4.735546112060547, R2 -1.5532805919647217\n",
      "Eval loss 4.735811710357666, R2 -1.4205079078674316\n",
      "epoch 16, loss 4.665953636169434, R2 -1.5239120721817017\n",
      "Eval loss 4.671046257019043, R2 -1.395397424697876\n",
      "epoch 17, loss 4.601672649383545, R2 -1.4966212511062622\n",
      "Eval loss 4.611128330230713, R2 -1.3720489740371704\n",
      "epoch 18, loss 4.54221248626709, R2 -1.4712135791778564\n",
      "Eval loss 4.555613994598389, R2 -1.3502988815307617\n",
      "epoch 19, loss 4.4871320724487305, R2 -1.4475220441818237\n",
      "Eval loss 4.504101753234863, R2 -1.3300001621246338\n",
      "epoch 20, loss 4.436028957366943, R2 -1.425385594367981\n",
      "Eval loss 4.456226348876953, R2 -1.3110171556472778\n",
      "epoch 21, loss 4.388538360595703, R2 -1.4046640396118164\n",
      "Eval loss 4.411656379699707, R2 -1.293230414390564\n",
      "epoch 22, loss 4.344330787658691, R2 -1.3852287530899048\n",
      "Eval loss 4.370090961456299, R2 -1.2765278816223145\n",
      "epoch 23, loss 4.303105354309082, R2 -1.3669625520706177\n",
      "Eval loss 4.331256866455078, R2 -1.2608107328414917\n",
      "epoch 24, loss 4.264589309692383, R2 -1.349759817123413\n",
      "Eval loss 4.294907569885254, R2 -1.2459880113601685\n",
      "epoch 25, loss 4.2285380363464355, R2 -1.3335241079330444\n",
      "Eval loss 4.260817527770996, R2 -1.231978178024292\n",
      "epoch 26, loss 4.194725036621094, R2 -1.3181692361831665\n",
      "Eval loss 4.228782653808594, R2 -1.218705654144287\n",
      "epoch 27, loss 4.162948131561279, R2 -1.3036137819290161\n",
      "Eval loss 4.1986188888549805, R2 -1.2061047554016113\n",
      "epoch 28, loss 4.133022785186768, R2 -1.2897878885269165\n",
      "Eval loss 4.170157432556152, R2 -1.1941132545471191\n",
      "epoch 29, loss 4.1047821044921875, R2 -1.2766258716583252\n",
      "Eval loss 4.143245697021484, R2 -1.1826770305633545\n",
      "epoch 30, loss 4.078073978424072, R2 -1.264068365097046\n",
      "Eval loss 4.117745876312256, R2 -1.1717448234558105\n",
      "epoch 31, loss 4.052760601043701, R2 -1.2520626783370972\n",
      "Eval loss 4.093532085418701, R2 -1.1612706184387207\n",
      "epoch 32, loss 4.028717041015625, R2 -1.2405591011047363\n",
      "Eval loss 4.070489883422852, R2 -1.1512131690979004\n",
      "epoch 33, loss 4.005830764770508, R2 -1.229514241218567\n",
      "Eval loss 4.048516750335693, R2 -1.1415364742279053\n",
      "epoch 34, loss 3.9839978218078613, R2 -1.2188878059387207\n",
      "Eval loss 4.027518272399902, R2 -1.1322073936462402\n",
      "epoch 35, loss 3.963125705718994, R2 -1.2086427211761475\n",
      "Eval loss 4.00740909576416, R2 -1.1231920719146729\n",
      "epoch 36, loss 3.943129301071167, R2 -1.1987468004226685\n",
      "Eval loss 3.9881114959716797, R2 -1.1144652366638184\n",
      "epoch 37, loss 3.9239320755004883, R2 -1.1891685724258423\n",
      "Eval loss 3.969555377960205, R2 -1.1060009002685547\n",
      "epoch 38, loss 3.9054641723632812, R2 -1.1798852682113647\n",
      "Eval loss 3.951677083969116, R2 -1.0977760553359985\n",
      "epoch 39, loss 3.887662649154663, R2 -1.17086660861969\n",
      "Eval loss 3.9344189167022705, R2 -1.0897698402404785\n",
      "epoch 40, loss 3.870469570159912, R2 -1.1620945930480957\n",
      "Eval loss 3.91772723197937, R2 -1.0819649696350098\n",
      "epoch 41, loss 3.8538331985473633, R2 -1.15354585647583\n",
      "Eval loss 3.9015557765960693, R2 -1.0743422508239746\n",
      "epoch 42, loss 3.8377058506011963, R2 -1.1452027559280396\n",
      "Eval loss 3.885859489440918, R2 -1.0668883323669434\n",
      "epoch 43, loss 3.822044849395752, R2 -1.1370511054992676\n",
      "Eval loss 3.8706002235412598, R2 -1.0595885515213013\n",
      "epoch 44, loss 3.806811809539795, R2 -1.1290721893310547\n",
      "Eval loss 3.855741500854492, R2 -1.0524301528930664\n",
      "epoch 45, loss 3.7919702529907227, R2 -1.1212540864944458\n",
      "Eval loss 3.8412511348724365, R2 -1.0454013347625732\n",
      "epoch 46, loss 3.777489423751831, R2 -1.1135830879211426\n",
      "Eval loss 3.827099323272705, R2 -1.0384925603866577\n",
      "epoch 47, loss 3.763338804244995, R2 -1.1060495376586914\n",
      "Eval loss 3.8132591247558594, R2 -1.031693696975708\n",
      "epoch 48, loss 3.749493360519409, R2 -1.0986409187316895\n",
      "Eval loss 3.7997066974639893, R2 -1.0249967575073242\n",
      "epoch 49, loss 3.7359282970428467, R2 -1.0913506746292114\n",
      "Eval loss 3.786419153213501, R2 -1.018392562866211\n",
      "epoch 50, loss 3.722621440887451, R2 -1.0841681957244873\n",
      "Eval loss 3.773376703262329, R2 -1.0118775367736816\n",
      "epoch 51, loss 3.7095537185668945, R2 -1.077086091041565\n",
      "Eval loss 3.760560989379883, R2 -1.005442500114441\n",
      "epoch 52, loss 3.696707010269165, R2 -1.0700969696044922\n",
      "Eval loss 3.7479560375213623, R2 -0.9990827441215515\n",
      "epoch 53, loss 3.6840646266937256, R2 -1.0631948709487915\n",
      "Eval loss 3.7355453968048096, R2 -0.9927929043769836\n",
      "epoch 54, loss 3.671612501144409, R2 -1.0563757419586182\n",
      "Eval loss 3.723315954208374, R2 -0.9865682721138\n",
      "epoch 55, loss 3.659336566925049, R2 -1.0496317148208618\n",
      "Eval loss 3.7112553119659424, R2 -0.9804039001464844\n",
      "epoch 56, loss 3.6472249031066895, R2 -1.0429587364196777\n",
      "Eval loss 3.6993520259857178, R2 -0.9742974638938904\n",
      "epoch 57, loss 3.635266065597534, R2 -1.036352276802063\n",
      "Eval loss 3.6875956058502197, R2 -0.9682446122169495\n",
      "epoch 58, loss 3.6234495639801025, R2 -1.0298100709915161\n",
      "Eval loss 3.6759769916534424, R2 -0.9622417092323303\n",
      "epoch 59, loss 3.611767292022705, R2 -1.0233266353607178\n",
      "Eval loss 3.6644864082336426, R2 -0.9562857151031494\n",
      "epoch 60, loss 3.600210189819336, R2 -1.0168986320495605\n",
      "Eval loss 3.6531171798706055, R2 -0.9503766894340515\n",
      "epoch 61, loss 3.588771343231201, R2 -1.0105235576629639\n",
      "Eval loss 3.641862154006958, R2 -0.9445092678070068\n",
      "epoch 62, loss 3.5774424076080322, R2 -1.0042001008987427\n",
      "Eval loss 3.6307144165039062, R2 -0.938682496547699\n",
      "epoch 63, loss 3.566218852996826, R2 -0.9979240298271179\n",
      "Eval loss 3.6196682453155518, R2 -0.9328933358192444\n",
      "epoch 64, loss 3.555093765258789, R2 -0.9916934370994568\n",
      "Eval loss 3.6087183952331543, R2 -0.9271419644355774\n",
      "epoch 65, loss 3.544062614440918, R2 -0.9855060577392578\n",
      "Eval loss 3.5978596210479736, R2 -0.9214257597923279\n",
      "epoch 66, loss 3.5331199169158936, R2 -0.9793609380722046\n",
      "Eval loss 3.587087392807007, R2 -0.9157434701919556\n",
      "epoch 67, loss 3.522261619567871, R2 -0.9732555150985718\n",
      "Eval loss 3.576397657394409, R2 -0.9100925922393799\n",
      "epoch 68, loss 3.511484384536743, R2 -0.9671885967254639\n",
      "Eval loss 3.5657873153686523, R2 -0.9044750332832336\n",
      "epoch 69, loss 3.500784158706665, R2 -0.9611591696739197\n",
      "Eval loss 3.5552520751953125, R2 -0.8988866209983826\n",
      "epoch 70, loss 3.4901576042175293, R2 -0.95516437292099\n",
      "Eval loss 3.5447890758514404, R2 -0.8933271169662476\n",
      "epoch 71, loss 3.479602336883545, R2 -0.9492051601409912\n",
      "Eval loss 3.534395694732666, R2 -0.8877967596054077\n",
      "epoch 72, loss 3.469114303588867, R2 -0.9432786703109741\n",
      "Eval loss 3.524068832397461, R2 -0.8822934627532959\n",
      "epoch 73, loss 3.458692789077759, R2 -0.9373850226402283\n",
      "Eval loss 3.5138068199157715, R2 -0.8768169283866882\n",
      "epoch 74, loss 3.448333740234375, R2 -0.9315223693847656\n",
      "Eval loss 3.5036072731018066, R2 -0.8713657855987549\n",
      "epoch 75, loss 3.4380362033843994, R2 -0.9256904125213623\n",
      "Eval loss 3.493467330932617, R2 -0.8659411072731018\n",
      "epoch 76, loss 3.427798271179199, R2 -0.9198890924453735\n",
      "Eval loss 3.4833860397338867, R2 -0.8605422377586365\n",
      "epoch 77, loss 3.417617082595825, R2 -0.9141164422035217\n",
      "Eval loss 3.4733614921569824, R2 -0.8551663756370544\n",
      "epoch 78, loss 3.4074933528900146, R2 -0.9083724617958069\n",
      "Eval loss 3.4633920192718506, R2 -0.8498155474662781\n",
      "epoch 79, loss 3.397423028945923, R2 -0.9026559591293335\n",
      "Eval loss 3.4534761905670166, R2 -0.844487190246582\n",
      "epoch 80, loss 3.3874058723449707, R2 -0.8969671726226807\n",
      "Eval loss 3.443612813949585, R2 -0.8391829133033752\n",
      "epoch 81, loss 3.377441167831421, R2 -0.8913053870201111\n",
      "Eval loss 3.433800458908081, R2 -0.8339012265205383\n",
      "epoch 82, loss 3.3675270080566406, R2 -0.8856700658798218\n",
      "Eval loss 3.4240384101867676, R2 -0.8286418914794922\n",
      "epoch 83, loss 3.3576619625091553, R2 -0.8800604343414307\n",
      "Eval loss 3.4143245220184326, R2 -0.8234056234359741\n",
      "epoch 84, loss 3.347846508026123, R2 -0.8744763731956482\n",
      "Eval loss 3.4046597480773926, R2 -0.8181899785995483\n",
      "epoch 85, loss 3.338078498840332, R2 -0.8689174652099609\n",
      "Eval loss 3.3950414657592773, R2 -0.8129969239234924\n",
      "epoch 86, loss 3.328357696533203, R2 -0.8633833527565002\n",
      "Eval loss 3.385469436645508, R2 -0.8078241348266602\n",
      "epoch 87, loss 3.318682909011841, R2 -0.8578742742538452\n",
      "Eval loss 3.375943183898926, R2 -0.8026737570762634\n",
      "epoch 88, loss 3.309053897857666, R2 -0.8523887395858765\n",
      "Eval loss 3.366461753845215, R2 -0.7975444793701172\n",
      "epoch 89, loss 3.2994697093963623, R2 -0.8469271063804626\n",
      "Eval loss 3.3570241928100586, R2 -0.7924339771270752\n",
      "epoch 90, loss 3.2899296283721924, R2 -0.8414897322654724\n",
      "Eval loss 3.347630023956299, R2 -0.7873462438583374\n",
      "epoch 91, loss 3.280433177947998, R2 -0.8360752463340759\n",
      "Eval loss 3.3382790088653564, R2 -0.782278835773468\n",
      "epoch 92, loss 3.2709801197052, R2 -0.8306840658187866\n",
      "Eval loss 3.328970432281494, R2 -0.7772310376167297\n",
      "epoch 93, loss 3.2615694999694824, R2 -0.8253156542778015\n",
      "Eval loss 3.3197035789489746, R2 -0.7722030282020569\n",
      "epoch 94, loss 3.2522013187408447, R2 -0.8199703097343445\n",
      "Eval loss 3.310478448867798, R2 -0.7671962976455688\n",
      "epoch 95, loss 3.242875099182129, R2 -0.8146461248397827\n",
      "Eval loss 3.3012936115264893, R2 -0.7622082233428955\n",
      "epoch 96, loss 3.2335894107818604, R2 -0.8093461394309998\n",
      "Eval loss 3.292149066925049, R2 -0.7572406530380249\n",
      "epoch 97, loss 3.2243452072143555, R2 -0.8040676712989807\n",
      "Eval loss 3.283045530319214, R2 -0.7522918581962585\n",
      "epoch 98, loss 3.2151412963867188, R2 -0.7988111972808838\n",
      "Eval loss 3.2739815711975098, R2 -0.7473629117012024\n",
      "epoch 99, loss 3.20597767829895, R2 -0.7935764789581299\n",
      "Eval loss 3.26495623588562, R2 -0.7424537539482117\n",
      "epoch 100, loss 3.1968533992767334, R2 -0.7883630394935608\n",
      "Eval loss 3.2559702396392822, R2 -0.7375633716583252\n",
      "epoch 101, loss 3.1877691745758057, R2 -0.7831715941429138\n",
      "Eval loss 3.2470226287841797, R2 -0.7326926589012146\n",
      "epoch 102, loss 3.1787238121032715, R2 -0.7780008912086487\n",
      "Eval loss 3.2381136417388916, R2 -0.7278404235839844\n",
      "epoch 103, loss 3.169717311859131, R2 -0.7728521823883057\n",
      "Eval loss 3.2292425632476807, R2 -0.7230077981948853\n",
      "epoch 104, loss 3.160749912261963, R2 -0.7677234411239624\n",
      "Eval loss 3.2204091548919678, R2 -0.7181940674781799\n",
      "epoch 105, loss 3.151820421218872, R2 -0.7626171112060547\n",
      "Eval loss 3.211613655090332, R2 -0.713399350643158\n",
      "epoch 106, loss 3.1429293155670166, R2 -0.7575309872627258\n",
      "Eval loss 3.202854871749878, R2 -0.7086217999458313\n",
      "epoch 107, loss 3.134075880050659, R2 -0.7524652481079102\n",
      "Eval loss 3.1941328048706055, R2 -0.7038647532463074\n",
      "epoch 108, loss 3.1252598762512207, R2 -0.7474203109741211\n",
      "Eval loss 3.185448169708252, R2 -0.6991247534751892\n",
      "epoch 109, loss 3.1164817810058594, R2 -0.7423961162567139\n",
      "Eval loss 3.176799774169922, R2 -0.6944034695625305\n",
      "epoch 110, loss 3.1077404022216797, R2 -0.7373918890953064\n",
      "Eval loss 3.168187141418457, R2 -0.6897020936012268\n",
      "epoch 111, loss 3.09903621673584, R2 -0.732407808303833\n",
      "Eval loss 3.1596107482910156, R2 -0.6850171685218811\n",
      "epoch 112, loss 3.0903682708740234, R2 -0.7274439334869385\n",
      "Eval loss 3.1510703563690186, R2 -0.6803517937660217\n",
      "epoch 113, loss 3.081737518310547, R2 -0.7225000858306885\n",
      "Eval loss 3.1425650119781494, R2 -0.6757035851478577\n",
      "epoch 114, loss 3.0731427669525146, R2 -0.717576801776886\n",
      "Eval loss 3.1340951919555664, R2 -0.6710742712020874\n",
      "epoch 115, loss 3.0645837783813477, R2 -0.7126729488372803\n",
      "Eval loss 3.1256604194641113, R2 -0.6664624810218811\n",
      "epoch 116, loss 3.056061029434204, R2 -0.707788348197937\n",
      "Eval loss 3.1172609329223633, R2 -0.6618680953979492\n",
      "epoch 117, loss 3.047574281692505, R2 -0.7029240131378174\n",
      "Eval loss 3.108896017074585, R2 -0.6572921276092529\n",
      "epoch 118, loss 3.0391225814819336, R2 -0.6980789303779602\n",
      "Eval loss 3.1005654335021973, R2 -0.6527342200279236\n",
      "epoch 119, loss 3.0307059288024902, R2 -0.693252682685852\n",
      "Eval loss 3.0922698974609375, R2 -0.6481932997703552\n",
      "epoch 120, loss 3.022325277328491, R2 -0.688447117805481\n",
      "Eval loss 3.084007740020752, R2 -0.6436703205108643\n",
      "epoch 121, loss 3.013979434967041, R2 -0.6836590766906738\n",
      "Eval loss 3.075779914855957, R2 -0.6391650438308716\n",
      "epoch 122, loss 3.0056676864624023, R2 -0.6788920164108276\n",
      "Eval loss 3.0675859451293945, R2 -0.6346772313117981\n",
      "epoch 123, loss 2.9973909854888916, R2 -0.674143373966217\n",
      "Eval loss 3.059425115585327, R2 -0.6302063465118408\n",
      "epoch 124, loss 2.9891488552093506, R2 -0.6694135665893555\n",
      "Eval loss 3.0512983798980713, R2 -0.6257531642913818\n",
      "epoch 125, loss 2.9809410572052, R2 -0.6647024750709534\n",
      "Eval loss 3.0432045459747314, R2 -0.6213171482086182\n",
      "epoch 126, loss 2.9727675914764404, R2 -0.6600099802017212\n",
      "Eval loss 3.035144090652466, R2 -0.6168985962867737\n",
      "epoch 127, loss 2.964627504348755, R2 -0.6553372740745544\n",
      "Eval loss 3.027116537094116, R2 -0.6124966144561768\n",
      "epoch 128, loss 2.95652174949646, R2 -0.6506815552711487\n",
      "Eval loss 3.0191221237182617, R2 -0.6081117987632751\n",
      "epoch 129, loss 2.94844913482666, R2 -0.6460459232330322\n",
      "Eval loss 3.011159896850586, R2 -0.6037442684173584\n",
      "epoch 130, loss 2.9404103755950928, R2 -0.6414275765419006\n",
      "Eval loss 3.003230571746826, R2 -0.599393904209137\n",
      "epoch 131, loss 2.9324047565460205, R2 -0.6368287205696106\n",
      "Eval loss 2.995333194732666, R2 -0.5950596928596497\n",
      "epoch 132, loss 2.9244322776794434, R2 -0.632247805595398\n",
      "Eval loss 2.9874680042266846, R2 -0.5907423496246338\n",
      "epoch 133, loss 2.9164931774139404, R2 -0.6276847124099731\n",
      "Eval loss 2.979635238647461, R2 -0.5864419341087341\n",
      "epoch 134, loss 2.9085867404937744, R2 -0.6231400370597839\n",
      "Eval loss 2.971834182739258, R2 -0.5821581482887268\n",
      "epoch 135, loss 2.9007129669189453, R2 -0.6186133027076721\n",
      "Eval loss 2.964064598083496, R2 -0.5778905153274536\n",
      "epoch 136, loss 2.892871618270874, R2 -0.6141045689582825\n",
      "Eval loss 2.956326961517334, R2 -0.5736399292945862\n",
      "epoch 137, loss 2.8850631713867188, R2 -0.6096136569976807\n",
      "Eval loss 2.9486207962036133, R2 -0.5694054961204529\n",
      "epoch 138, loss 2.877286672592163, R2 -0.6051405668258667\n",
      "Eval loss 2.940945625305176, R2 -0.5651879906654358\n",
      "epoch 139, loss 2.869542360305786, R2 -0.6006853580474854\n",
      "Eval loss 2.9333019256591797, R2 -0.5609864592552185\n",
      "epoch 140, loss 2.8618297576904297, R2 -0.5962474346160889\n",
      "Eval loss 2.9256889820098877, R2 -0.5568004846572876\n",
      "epoch 141, loss 2.854149103164673, R2 -0.5918272733688354\n",
      "Eval loss 2.9181067943573, R2 -0.552631139755249\n",
      "epoch 142, loss 2.8465003967285156, R2 -0.5874249935150146\n",
      "Eval loss 2.910555362701416, R2 -0.5484783053398132\n",
      "epoch 143, loss 2.8388829231262207, R2 -0.5830398201942444\n",
      "Eval loss 2.9030346870422363, R2 -0.544340968132019\n",
      "epoch 144, loss 2.8312971591949463, R2 -0.5786718130111694\n",
      "Eval loss 2.8955447673797607, R2 -0.5402195453643799\n",
      "epoch 145, loss 2.823742389678955, R2 -0.5743213295936584\n",
      "Eval loss 2.8880844116210938, R2 -0.5361143350601196\n",
      "epoch 146, loss 2.816218614578247, R2 -0.5699880123138428\n",
      "Eval loss 2.8806545734405518, R2 -0.5320250391960144\n",
      "epoch 147, loss 2.8087260723114014, R2 -0.5656715035438538\n",
      "Eval loss 2.8732550144195557, R2 -0.5279510617256165\n",
      "epoch 148, loss 2.8012642860412598, R2 -0.5613717436790466\n",
      "Eval loss 2.865885019302368, R2 -0.5238933563232422\n",
      "epoch 149, loss 2.7938334941864014, R2 -0.5570898652076721\n",
      "Eval loss 2.8585450649261475, R2 -0.5198509097099304\n",
      "epoch 150, loss 2.7864327430725098, R2 -0.5528238415718079\n",
      "Eval loss 2.8512344360351562, R2 -0.5158240795135498\n",
      "epoch 151, loss 2.7790627479553223, R2 -0.5485759377479553\n",
      "Eval loss 2.8439533710479736, R2 -0.511812150478363\n",
      "epoch 152, loss 2.7717230319976807, R2 -0.5443440675735474\n",
      "Eval loss 2.8367016315460205, R2 -0.5078163743019104\n",
      "epoch 153, loss 2.764413356781006, R2 -0.5401290655136108\n",
      "Eval loss 2.8294789791107178, R2 -0.5038355588912964\n",
      "epoch 154, loss 2.757133722305298, R2 -0.5359307527542114\n",
      "Eval loss 2.8222858905792236, R2 -0.4998704791069031\n",
      "epoch 155, loss 2.7498838901519775, R2 -0.5317487120628357\n",
      "Eval loss 2.8151214122772217, R2 -0.4959207773208618\n",
      "epoch 156, loss 2.742663621902466, R2 -0.5275831818580627\n",
      "Eval loss 2.807986259460449, R2 -0.49198588728904724\n",
      "epoch 157, loss 2.7354736328125, R2 -0.523434042930603\n",
      "Eval loss 2.8008790016174316, R2 -0.4880659282207489\n",
      "epoch 158, loss 2.7283129692077637, R2 -0.5193008780479431\n",
      "Eval loss 2.7938010692596436, R2 -0.4841611087322235\n",
      "epoch 159, loss 2.7211813926696777, R2 -0.5151845216751099\n",
      "Eval loss 2.7867510318756104, R2 -0.4802720248699188\n",
      "epoch 160, loss 2.714078903198242, R2 -0.5110838413238525\n",
      "Eval loss 2.7797298431396484, R2 -0.47639724612236023\n",
      "epoch 161, loss 2.707005500793457, R2 -0.5069995522499084\n",
      "Eval loss 2.7727365493774414, R2 -0.47253742814064026\n",
      "epoch 162, loss 2.6999616622924805, R2 -0.5029305815696716\n",
      "Eval loss 2.7657713890075684, R2 -0.46869200468063354\n",
      "epoch 163, loss 2.692945957183838, R2 -0.4988790452480316\n",
      "Eval loss 2.75883412361145, R2 -0.46486127376556396\n",
      "epoch 164, loss 2.6859593391418457, R2 -0.4948423504829407\n",
      "Eval loss 2.751924753189087, R2 -0.46104612946510315\n",
      "epoch 165, loss 2.6790013313293457, R2 -0.4908217489719391\n",
      "Eval loss 2.7450430393218994, R2 -0.45724478363990784\n",
      "epoch 166, loss 2.672071695327759, R2 -0.48681697249412537\n",
      "Eval loss 2.7381889820098877, R2 -0.45345842838287354\n",
      "epoch 167, loss 2.665170192718506, R2 -0.4828278720378876\n",
      "Eval loss 2.7313625812530518, R2 -0.44968655705451965\n",
      "epoch 168, loss 2.658297300338745, R2 -0.47885358333587646\n",
      "Eval loss 2.7245631217956543, R2 -0.44592878222465515\n",
      "epoch 169, loss 2.6514523029327393, R2 -0.47489631175994873\n",
      "Eval loss 2.7177910804748535, R2 -0.44218558073043823\n",
      "epoch 170, loss 2.6446352005004883, R2 -0.4709530174732208\n",
      "Eval loss 2.7110462188720703, R2 -0.4384567439556122\n",
      "epoch 171, loss 2.637845993041992, R2 -0.4670264422893524\n",
      "Eval loss 2.7043285369873047, R2 -0.4347417652606964\n",
      "epoch 172, loss 2.631084442138672, R2 -0.4631147086620331\n",
      "Eval loss 2.6976373195648193, R2 -0.4310416281223297\n",
      "epoch 173, loss 2.6243503093719482, R2 -0.45921820402145386\n",
      "Eval loss 2.6909730434417725, R2 -0.42735564708709717\n",
      "epoch 174, loss 2.6176438331604004, R2 -0.4553368389606476\n",
      "Eval loss 2.684335231781006, R2 -0.4236826002597809\n",
      "epoch 175, loss 2.610964775085449, R2 -0.4514707326889038\n",
      "Eval loss 2.6777243614196777, R2 -0.4200247526168823\n",
      "epoch 176, loss 2.6043128967285156, R2 -0.44761961698532104\n",
      "Eval loss 2.6711392402648926, R2 -0.41638076305389404\n",
      "epoch 177, loss 2.5976881980895996, R2 -0.4437834620475769\n",
      "Eval loss 2.664581060409546, R2 -0.41275036334991455\n",
      "epoch 178, loss 2.591090440750122, R2 -0.43996259570121765\n",
      "Eval loss 2.658048629760742, R2 -0.4091334342956543\n",
      "epoch 179, loss 2.584519624710083, R2 -0.436156302690506\n",
      "Eval loss 2.6515424251556396, R2 -0.40553054213523865\n",
      "epoch 180, loss 2.577975034713745, R2 -0.4323645830154419\n",
      "Eval loss 2.645062208175659, R2 -0.40194207429885864\n",
      "epoch 181, loss 2.571457624435425, R2 -0.42858850955963135\n",
      "Eval loss 2.6386077404022217, R2 -0.39836663007736206\n",
      "epoch 182, loss 2.5649662017822266, R2 -0.42482659220695496\n",
      "Eval loss 2.632179021835327, R2 -0.39480456709861755\n",
      "epoch 183, loss 2.558501720428467, R2 -0.42107951641082764\n",
      "Eval loss 2.6257758140563965, R2 -0.39125677943229675\n",
      "epoch 184, loss 2.552063226699829, R2 -0.4173468351364136\n",
      "Eval loss 2.6193981170654297, R2 -0.387721449136734\n",
      "epoch 185, loss 2.5456511974334717, R2 -0.4136286675930023\n",
      "Eval loss 2.613046169281006, R2 -0.38420000672340393\n",
      "epoch 186, loss 2.5392649173736572, R2 -0.4099251627922058\n",
      "Eval loss 2.6067190170288086, R2 -0.38069239258766174\n",
      "epoch 187, loss 2.5329043865203857, R2 -0.40623557567596436\n",
      "Eval loss 2.600417375564575, R2 -0.37719783186912537\n",
      "epoch 188, loss 2.5265703201293945, R2 -0.40256062150001526\n",
      "Eval loss 2.5941405296325684, R2 -0.37371644377708435\n",
      "epoch 189, loss 2.520261287689209, R2 -0.39889973402023315\n",
      "Eval loss 2.5878889560699463, R2 -0.3702477514743805\n",
      "epoch 190, loss 2.5139782428741455, R2 -0.395253449678421\n",
      "Eval loss 2.581662178039551, R2 -0.36679354310035706\n",
      "epoch 191, loss 2.507720708847046, R2 -0.39162060618400574\n",
      "Eval loss 2.5754597187042236, R2 -0.36335185170173645\n",
      "epoch 192, loss 2.501488447189331, R2 -0.3880023956298828\n",
      "Eval loss 2.569282054901123, R2 -0.3599228858947754\n",
      "epoch 193, loss 2.495281219482422, R2 -0.384398490190506\n",
      "Eval loss 2.56312894821167, R2 -0.35650667548179626\n",
      "epoch 194, loss 2.4890995025634766, R2 -0.3808076083660126\n",
      "Eval loss 2.5570003986358643, R2 -0.3531041145324707\n",
      "epoch 195, loss 2.482942581176758, R2 -0.37723129987716675\n",
      "Eval loss 2.550895929336548, R2 -0.34971383213996887\n",
      "epoch 196, loss 2.4768106937408447, R2 -0.37366876006126404\n",
      "Eval loss 2.5448157787323, R2 -0.34633657336235046\n",
      "epoch 197, loss 2.470703363418579, R2 -0.37011992931365967\n",
      "Eval loss 2.538759708404541, R2 -0.3429725170135498\n",
      "epoch 198, loss 2.464621067047119, R2 -0.36658555269241333\n",
      "Eval loss 2.5327277183532715, R2 -0.33962053060531616\n",
      "epoch 199, loss 2.4585635662078857, R2 -0.3630640208721161\n",
      "Eval loss 2.526719570159912, R2 -0.33628153800964355\n",
      "epoch 200, loss 2.4525301456451416, R2 -0.3595561683177948\n",
      "Eval loss 2.520735025405884, R2 -0.3329548239707947\n",
      "epoch 201, loss 2.446521282196045, R2 -0.3560614585876465\n",
      "Eval loss 2.5147745609283447, R2 -0.3296409845352173\n",
      "epoch 202, loss 2.4405364990234375, R2 -0.35258129239082336\n",
      "Eval loss 2.5088374614715576, R2 -0.3263399302959442\n",
      "epoch 203, loss 2.4345762729644775, R2 -0.34911346435546875\n",
      "Eval loss 2.5029237270355225, R2 -0.32305091619491577\n",
      "epoch 204, loss 2.4286396503448486, R2 -0.3456600308418274\n",
      "Eval loss 2.4970333576202393, R2 -0.3197740316390991\n",
      "epoch 205, loss 2.422727346420288, R2 -0.3422192931175232\n",
      "Eval loss 2.491166591644287, R2 -0.3165096044540405\n",
      "epoch 206, loss 2.4168386459350586, R2 -0.33879148960113525\n",
      "Eval loss 2.4853227138519287, R2 -0.31325846910476685\n",
      "epoch 207, loss 2.4109737873077393, R2 -0.33537790179252625\n",
      "Eval loss 2.479501962661743, R2 -0.3100183308124542\n",
      "epoch 208, loss 2.405132532119751, R2 -0.33197706937789917\n",
      "Eval loss 2.4737040996551514, R2 -0.30679118633270264\n",
      "epoch 209, loss 2.3993148803710938, R2 -0.32858896255493164\n",
      "Eval loss 2.4679293632507324, R2 -0.3035760223865509\n",
      "epoch 210, loss 2.3935205936431885, R2 -0.3252142667770386\n",
      "Eval loss 2.462177276611328, R2 -0.3003724217414856\n",
      "epoch 211, loss 2.3877499103546143, R2 -0.321852445602417\n",
      "Eval loss 2.4564478397369385, R2 -0.2971815764904022\n",
      "epoch 212, loss 2.382002353668213, R2 -0.31850290298461914\n",
      "Eval loss 2.4507410526275635, R2 -0.2940017879009247\n",
      "epoch 213, loss 2.3762779235839844, R2 -0.3151673376560211\n",
      "Eval loss 2.445056438446045, R2 -0.29083511233329773\n",
      "epoch 214, loss 2.3705763816833496, R2 -0.311844140291214\n",
      "Eval loss 2.439394474029541, R2 -0.2876792550086975\n",
      "epoch 215, loss 2.3648977279663086, R2 -0.3085336983203888\n",
      "Eval loss 2.4337549209594727, R2 -0.284536212682724\n",
      "epoch 216, loss 2.3592419624328613, R2 -0.3052355945110321\n",
      "Eval loss 2.4281375408172607, R2 -0.2814045250415802\n",
      "epoch 217, loss 2.353609085083008, R2 -0.30195072293281555\n",
      "Eval loss 2.4225423336029053, R2 -0.27828434109687805\n",
      "epoch 218, loss 2.347998857498169, R2 -0.2986781895160675\n",
      "Eval loss 2.416968584060669, R2 -0.27517613768577576\n",
      "epoch 219, loss 2.3424110412597656, R2 -0.2954182028770447\n",
      "Eval loss 2.411417245864868, R2 -0.27207934856414795\n",
      "epoch 220, loss 2.336845636367798, R2 -0.2921707034111023\n",
      "Eval loss 2.4058873653411865, R2 -0.26899453997612\n",
      "epoch 221, loss 2.3313026428222656, R2 -0.2889356315135956\n",
      "Eval loss 2.4003798961639404, R2 -0.26592084765434265\n",
      "epoch 222, loss 2.325782060623169, R2 -0.285712331533432\n",
      "Eval loss 2.394893169403076, R2 -0.2628585696220398\n",
      "epoch 223, loss 2.3202834129333496, R2 -0.282502144575119\n",
      "Eval loss 2.3894286155700684, R2 -0.2598083019256592\n",
      "epoch 224, loss 2.3148066997528076, R2 -0.27930453419685364\n",
      "Eval loss 2.3839850425720215, R2 -0.2567688524723053\n",
      "epoch 225, loss 2.309351921081543, R2 -0.27611860632896423\n",
      "Eval loss 2.378563642501831, R2 -0.2537408173084259\n",
      "epoch 226, loss 2.3039193153381348, R2 -0.2729451358318329\n",
      "Eval loss 2.3731627464294434, R2 -0.25072482228279114\n",
      "epoch 227, loss 2.298508405685425, R2 -0.2697835862636566\n",
      "Eval loss 2.367783308029175, R2 -0.2477196604013443\n",
      "epoch 228, loss 2.293118953704834, R2 -0.2666341960430145\n",
      "Eval loss 2.362424850463867, R2 -0.2447255253791809\n",
      "epoch 229, loss 2.2877514362335205, R2 -0.2634966969490051\n",
      "Eval loss 2.3570873737335205, R2 -0.24174267053604126\n",
      "epoch 230, loss 2.282404899597168, R2 -0.2603714168071747\n",
      "Eval loss 2.3517706394195557, R2 -0.23877061903476715\n",
      "epoch 231, loss 2.2770802974700928, R2 -0.2572578191757202\n",
      "Eval loss 2.346475124359131, R2 -0.2358100414276123\n",
      "epoch 232, loss 2.2717766761779785, R2 -0.25415611267089844\n",
      "Eval loss 2.3411998748779297, R2 -0.23286060988903046\n",
      "epoch 233, loss 2.2664942741394043, R2 -0.25106632709503174\n",
      "Eval loss 2.3359456062316895, R2 -0.22992153465747833\n",
      "epoch 234, loss 2.261232852935791, R2 -0.24798811972141266\n",
      "Eval loss 2.330712080001831, R2 -0.226994127035141\n",
      "epoch 235, loss 2.2559926509857178, R2 -0.2449217289686203\n",
      "Eval loss 2.3254988193511963, R2 -0.22407713532447815\n",
      "epoch 236, loss 2.2507734298706055, R2 -0.2418668121099472\n",
      "Eval loss 2.320305824279785, R2 -0.22117109596729279\n",
      "epoch 237, loss 2.245574951171875, R2 -0.23882412910461426\n",
      "Eval loss 2.3151328563690186, R2 -0.21827572584152222\n",
      "epoch 238, loss 2.2403974533081055, R2 -0.2357928305864334\n",
      "Eval loss 2.3099803924560547, R2 -0.2153911590576172\n",
      "epoch 239, loss 2.2352399826049805, R2 -0.23277269303798676\n",
      "Eval loss 2.3048479557037354, R2 -0.2125178426504135\n",
      "epoch 240, loss 2.2301037311553955, R2 -0.2297644168138504\n",
      "Eval loss 2.2997355461120605, R2 -0.20965415239334106\n",
      "epoch 241, loss 2.224987506866455, R2 -0.22676724195480347\n",
      "Eval loss 2.2946431636810303, R2 -0.20680230855941772\n",
      "epoch 242, loss 2.2198920249938965, R2 -0.22378180921077728\n",
      "Eval loss 2.2895705699920654, R2 -0.20395983755588531\n",
      "epoch 243, loss 2.2148168087005615, R2 -0.2208075076341629\n",
      "Eval loss 2.284518003463745, R2 -0.20112867653369904\n",
      "epoch 244, loss 2.209761619567871, R2 -0.2178443968296051\n",
      "Eval loss 2.279484987258911, R2 -0.1983075588941574\n",
      "epoch 245, loss 2.204726457595825, R2 -0.21489262580871582\n",
      "Eval loss 2.2744712829589844, R2 -0.19549749791622162\n",
      "epoch 246, loss 2.199711561203003, R2 -0.21195203065872192\n",
      "Eval loss 2.269477128982544, R2 -0.1926974654197693\n",
      "epoch 247, loss 2.194716691970825, R2 -0.2090221345424652\n",
      "Eval loss 2.264502763748169, R2 -0.18990756571292877\n",
      "epoch 248, loss 2.189741373062134, R2 -0.20610418915748596\n",
      "Eval loss 2.259547472000122, R2 -0.1871282458305359\n",
      "epoch 249, loss 2.184786319732666, R2 -0.20319639146327972\n",
      "Eval loss 2.2546114921569824, R2 -0.18435931205749512\n",
      "epoch 250, loss 2.1798505783081055, R2 -0.20030060410499573\n",
      "Eval loss 2.24969482421875, R2 -0.18160028755664825\n",
      "epoch 251, loss 2.1749343872070312, R2 -0.197415292263031\n",
      "Eval loss 2.2447972297668457, R2 -0.17885206639766693\n",
      "epoch 252, loss 2.1700379848480225, R2 -0.19454075396060944\n",
      "Eval loss 2.2399187088012695, R2 -0.17611341178417206\n",
      "epoch 253, loss 2.165160894393921, R2 -0.19167722761631012\n",
      "Eval loss 2.2350590229034424, R2 -0.17338478565216064\n",
      "epoch 254, loss 2.1603031158447266, R2 -0.18882440030574799\n",
      "Eval loss 2.230217933654785, R2 -0.17066675424575806\n",
      "epoch 255, loss 2.1554646492004395, R2 -0.18598251044750214\n",
      "Eval loss 2.225396156311035, R2 -0.16795848309993744\n",
      "epoch 256, loss 2.1506452560424805, R2 -0.1831512451171875\n",
      "Eval loss 2.220592975616455, R2 -0.16525989770889282\n",
      "epoch 257, loss 2.145845413208008, R2 -0.18033017218112946\n",
      "Eval loss 2.215808153152466, R2 -0.1625710129737854\n",
      "epoch 258, loss 2.141064167022705, R2 -0.17752057313919067\n",
      "Eval loss 2.2110421657562256, R2 -0.1598929464817047\n",
      "epoch 259, loss 2.1363022327423096, R2 -0.17472118139266968\n",
      "Eval loss 2.206294298171997, R2 -0.1572244018316269\n",
      "epoch 260, loss 2.131558656692505, R2 -0.17193256318569183\n",
      "Eval loss 2.2015652656555176, R2 -0.15456534922122955\n",
      "epoch 261, loss 2.1268341541290283, R2 -0.16915422677993774\n",
      "Eval loss 2.19685435295105, R2 -0.1519162803888321\n",
      "epoch 262, loss 2.1221282482147217, R2 -0.16638639569282532\n",
      "Eval loss 2.1921615600585938, R2 -0.14927710592746735\n",
      "epoch 263, loss 2.117440938949585, R2 -0.16362901031970978\n",
      "Eval loss 2.1874868869781494, R2 -0.14664727449417114\n",
      "epoch 264, loss 2.1127724647521973, R2 -0.1608818918466568\n",
      "Eval loss 2.182830333709717, R2 -0.14402756094932556\n",
      "epoch 265, loss 2.1081221103668213, R2 -0.15814481675624847\n",
      "Eval loss 2.178192138671875, R2 -0.1414164900779724\n",
      "epoch 266, loss 2.1034905910491943, R2 -0.15541881322860718\n",
      "Eval loss 2.1735713481903076, R2 -0.13881617784500122\n",
      "epoch 267, loss 2.098877191543579, R2 -0.15270255506038666\n",
      "Eval loss 2.168968677520752, R2 -0.1362248659133911\n",
      "epoch 268, loss 2.0942816734313965, R2 -0.14999622106552124\n",
      "Eval loss 2.164383888244629, R2 -0.1336430162191391\n",
      "epoch 269, loss 2.0897045135498047, R2 -0.14730070531368256\n",
      "Eval loss 2.1598165035247803, R2 -0.13107100129127502\n",
      "epoch 270, loss 2.0851454734802246, R2 -0.14461492002010345\n",
      "Eval loss 2.1552670001983643, R2 -0.1285077929496765\n",
      "epoch 271, loss 2.080604314804077, R2 -0.14193905889987946\n",
      "Eval loss 2.1507351398468018, R2 -0.12595435976982117\n",
      "epoch 272, loss 2.0760812759399414, R2 -0.13927388191223145\n",
      "Eval loss 2.1462204456329346, R2 -0.12341012805700302\n",
      "epoch 273, loss 2.071575880050659, R2 -0.1366182267665863\n",
      "Eval loss 2.141723394393921, R2 -0.12087514996528625\n",
      "epoch 274, loss 2.0670881271362305, R2 -0.13397254049777985\n",
      "Eval loss 2.1372435092926025, R2 -0.11834981292486191\n",
      "epoch 275, loss 2.0626182556152344, R2 -0.13133645057678223\n",
      "Eval loss 2.1327807903289795, R2 -0.11583306640386581\n",
      "epoch 276, loss 2.058166027069092, R2 -0.12871089577674866\n",
      "Eval loss 2.128335475921631, R2 -0.11332594603300095\n",
      "epoch 277, loss 2.0537312030792236, R2 -0.12609438598155975\n",
      "Eval loss 2.1239073276519775, R2 -0.1108279675245285\n",
      "epoch 278, loss 2.049314022064209, R2 -0.12348835915327072\n",
      "Eval loss 2.1194961071014404, R2 -0.10833887755870819\n",
      "epoch 279, loss 2.0449140071868896, R2 -0.12089160084724426\n",
      "Eval loss 2.1151018142700195, R2 -0.10585934668779373\n",
      "epoch 280, loss 2.0405311584472656, R2 -0.11830518394708633\n",
      "Eval loss 2.110724449157715, R2 -0.10338842123746872\n",
      "epoch 281, loss 2.036165952682495, R2 -0.11572795361280441\n",
      "Eval loss 2.1063640117645264, R2 -0.10092657059431076\n",
      "epoch 282, loss 2.031817674636841, R2 -0.11316023021936417\n",
      "Eval loss 2.102020263671875, R2 -0.09847354888916016\n",
      "epoch 283, loss 2.027486562728882, R2 -0.11060206592082977\n",
      "Eval loss 2.0976932048797607, R2 -0.09602963924407959\n",
      "epoch 284, loss 2.023172378540039, R2 -0.10805385559797287\n",
      "Eval loss 2.0933823585510254, R2 -0.09359442442655563\n",
      "epoch 285, loss 2.0188751220703125, R2 -0.10551505535840988\n",
      "Eval loss 2.0890884399414062, R2 -0.09116819500923157\n",
      "epoch 286, loss 2.0145950317382812, R2 -0.10298555344343185\n",
      "Eval loss 2.084810972213745, R2 -0.08875080943107605\n",
      "epoch 287, loss 2.010331392288208, R2 -0.10046546906232834\n",
      "Eval loss 2.080549955368042, R2 -0.08634260296821594\n",
      "epoch 288, loss 2.006084442138672, R2 -0.09795475006103516\n",
      "Eval loss 2.076305389404297, R2 -0.08394220471382141\n",
      "epoch 289, loss 2.001854419708252, R2 -0.0954534113407135\n",
      "Eval loss 2.0720767974853516, R2 -0.08155150711536407\n",
      "epoch 290, loss 1.99764084815979, R2 -0.09296146035194397\n",
      "Eval loss 2.0678646564483643, R2 -0.07916903495788574\n",
      "epoch 291, loss 1.9934438467025757, R2 -0.09047859162092209\n",
      "Eval loss 2.063668727874756, R2 -0.07679495215415955\n",
      "epoch 292, loss 1.9892631769180298, R2 -0.08800475299358368\n",
      "Eval loss 2.059488534927368, R2 -0.07443004101514816\n",
      "epoch 293, loss 1.985098958015442, R2 -0.08554084599018097\n",
      "Eval loss 2.0553245544433594, R2 -0.07207304239273071\n",
      "epoch 294, loss 1.980950951576233, R2 -0.08308577537536621\n",
      "Eval loss 2.0511765480041504, R2 -0.06972524523735046\n",
      "epoch 295, loss 1.9768192768096924, R2 -0.08063951134681702\n",
      "Eval loss 2.047044277191162, R2 -0.06738527119159698\n",
      "epoch 296, loss 1.9727038145065308, R2 -0.07820257544517517\n",
      "Eval loss 2.0429275035858154, R2 -0.06505442410707474\n",
      "epoch 297, loss 1.9686044454574585, R2 -0.07577461749315262\n",
      "Eval loss 2.0388269424438477, R2 -0.06273173540830612\n",
      "epoch 298, loss 1.9645209312438965, R2 -0.0733555480837822\n",
      "Eval loss 2.0347418785095215, R2 -0.06041736528277397\n",
      "epoch 299, loss 1.9604532718658447, R2 -0.07094565033912659\n",
      "Eval loss 2.030672311782837, R2 -0.05811114236712456\n",
      "epoch 300, loss 1.9564015865325928, R2 -0.06854448467493057\n",
      "Eval loss 2.026618719100952, R2 -0.05581362545490265\n",
      "epoch 301, loss 1.9523659944534302, R2 -0.06615247577428818\n",
      "Eval loss 2.0225799083709717, R2 -0.05352437123656273\n",
      "epoch 302, loss 1.9483458995819092, R2 -0.06376872956752777\n",
      "Eval loss 2.018556833267212, R2 -0.051242902874946594\n",
      "epoch 303, loss 1.9443416595458984, R2 -0.0613945871591568\n",
      "Eval loss 2.0145490169525146, R2 -0.048970405012369156\n",
      "epoch 304, loss 1.9403530359268188, R2 -0.0590287446975708\n",
      "Eval loss 2.010556936264038, R2 -0.04670548439025879\n",
      "epoch 305, loss 1.9363796710968018, R2 -0.05667147785425186\n",
      "Eval loss 2.006579637527466, R2 -0.04444916546344757\n",
      "epoch 306, loss 1.9324220418930054, R2 -0.054323576390743256\n",
      "Eval loss 2.002617597579956, R2 -0.042200252413749695\n",
      "epoch 307, loss 1.928479790687561, R2 -0.0519840270280838\n",
      "Eval loss 1.9986704587936401, R2 -0.03996044769883156\n",
      "epoch 308, loss 1.9245529174804688, R2 -0.049653053283691406\n",
      "Eval loss 1.9947385787963867, R2 -0.03772798925638199\n",
      "epoch 309, loss 1.920641303062439, R2 -0.04733055830001831\n",
      "Eval loss 1.9908212423324585, R2 -0.03550402820110321\n",
      "epoch 310, loss 1.9167447090148926, R2 -0.045016493648290634\n",
      "Eval loss 1.9869191646575928, R2 -0.03328744322061539\n",
      "epoch 311, loss 1.9128636121749878, R2 -0.04271119832992554\n",
      "Eval loss 1.9830317497253418, R2 -0.031079303473234177\n",
      "epoch 312, loss 1.9089975357055664, R2 -0.040414825081825256\n",
      "Eval loss 1.9791594743728638, R2 -0.028878964483737946\n",
      "epoch 313, loss 1.9051463603973389, R2 -0.038126397877931595\n",
      "Eval loss 1.9753015041351318, R2 -0.02668650634586811\n",
      "epoch 314, loss 1.9013103246688843, R2 -0.03584650531411171\n",
      "Eval loss 1.9714584350585938, R2 -0.024501854553818703\n",
      "epoch 315, loss 1.8974889516830444, R2 -0.03357508406043053\n",
      "Eval loss 1.9676296710968018, R2 -0.022324848920106888\n",
      "epoch 316, loss 1.8936827182769775, R2 -0.03131196275353432\n",
      "Eval loss 1.963815689086914, R2 -0.020156513899564743\n",
      "epoch 317, loss 1.8898911476135254, R2 -0.029057107865810394\n",
      "Eval loss 1.960016131401062, R2 -0.017995232716202736\n",
      "epoch 318, loss 1.8861140012741089, R2 -0.026810623705387115\n",
      "Eval loss 1.956230878829956, R2 -0.015841728076338768\n",
      "epoch 319, loss 1.8823518753051758, R2 -0.024572253227233887\n",
      "Eval loss 1.9524600505828857, R2 -0.013695911504328251\n",
      "epoch 320, loss 1.8786041736602783, R2 -0.022342270240187645\n",
      "Eval loss 1.948703646659851, R2 -0.011557752266526222\n",
      "epoch 321, loss 1.8748712539672852, R2 -0.02012002468109131\n",
      "Eval loss 1.9449610710144043, R2 -0.009427503682672977\n",
      "epoch 322, loss 1.871152639389038, R2 -0.01790660060942173\n",
      "Eval loss 1.9412329196929932, R2 -0.007304733619093895\n",
      "epoch 323, loss 1.8674482107162476, R2 -0.015700941905379295\n",
      "Eval loss 1.937518835067749, R2 -0.005189467687159777\n",
      "epoch 324, loss 1.8637583255767822, R2 -0.013503432273864746\n",
      "Eval loss 1.933819055557251, R2 -0.0030818255618214607\n",
      "epoch 325, loss 1.860082745552063, R2 -0.011313969269394875\n",
      "Eval loss 1.9301329851150513, R2 -0.0009817426325753331\n",
      "epoch 326, loss 1.8564213514328003, R2 -0.0091325044631958\n",
      "Eval loss 1.9264609813690186, R2 0.0011108680628240108\n",
      "epoch 327, loss 1.8527741432189941, R2 -0.0069587393663823605\n",
      "Eval loss 1.9228026866912842, R2 0.0031962720677256584\n",
      "epoch 328, loss 1.849141001701355, R2 -0.0047935303300619125\n",
      "Eval loss 1.9191583395004272, R2 0.005274030379951\n",
      "epoch 329, loss 1.8455220460891724, R2 -0.002635771641507745\n",
      "Eval loss 1.9155277013778687, R2 0.007344175595790148\n",
      "epoch 330, loss 1.8419170379638672, R2 -0.00048583204625174403\n",
      "Eval loss 1.911910891532898, R2 0.0094069242477417\n",
      "epoch 331, loss 1.8383259773254395, R2 0.0016556436894461513\n",
      "Eval loss 1.9083075523376465, R2 0.011462829075753689\n",
      "epoch 332, loss 1.83474862575531, R2 0.003790064249187708\n",
      "Eval loss 1.9047178030014038, R2 0.013511013239622116\n",
      "epoch 333, loss 1.831185221672058, R2 0.005915929097682238\n",
      "Eval loss 1.90114164352417, R2 0.015552038326859474\n",
      "epoch 334, loss 1.8276355266571045, R2 0.00803439226001501\n",
      "Eval loss 1.8975791931152344, R2 0.017585640773177147\n",
      "epoch 335, loss 1.8240994215011597, R2 0.010145225562155247\n",
      "Eval loss 1.8940298557281494, R2 0.019612176343798637\n",
      "epoch 336, loss 1.8205769062042236, R2 0.012248293496668339\n",
      "Eval loss 1.8904939889907837, R2 0.021631700918078423\n",
      "epoch 337, loss 1.8170682191848755, R2 0.014343603514134884\n",
      "Eval loss 1.8869712352752686, R2 0.023643720895051956\n",
      "epoch 338, loss 1.8135727643966675, R2 0.01643139123916626\n",
      "Eval loss 1.8834619522094727, R2 0.025649048388004303\n",
      "epoch 339, loss 1.810091257095337, R2 0.018511570990085602\n",
      "Eval loss 1.879965901374817, R2 0.02764720842242241\n",
      "epoch 340, loss 1.8066227436065674, R2 0.020584069192409515\n",
      "Eval loss 1.8764829635620117, R2 0.02963748760521412\n",
      "epoch 341, loss 1.8031675815582275, R2 0.022649168968200684\n",
      "Eval loss 1.8730131387710571, R2 0.03162161260843277\n",
      "epoch 342, loss 1.799725890159607, R2 0.024706710129976273\n",
      "Eval loss 1.8695563077926636, R2 0.03359835222363472\n",
      "epoch 343, loss 1.7962971925735474, R2 0.026757197454571724\n",
      "Eval loss 1.8661125898361206, R2 0.035567864775657654\n",
      "epoch 344, loss 1.7928820848464966, R2 0.028799474239349365\n",
      "Eval loss 1.86268150806427, R2 0.03753087297081947\n",
      "epoch 345, loss 1.7894797325134277, R2 0.03083452396094799\n",
      "Eval loss 1.85926353931427, R2 0.03948676958680153\n",
      "epoch 346, loss 1.7860907316207886, R2 0.03286238759756088\n",
      "Eval loss 1.855858325958252, R2 0.04143548384308815\n",
      "epoch 347, loss 1.7827147245407104, R2 0.034882839769124985\n",
      "Eval loss 1.8524658679962158, R2 0.04337778314948082\n",
      "epoch 348, loss 1.7793515920639038, R2 0.03689625486731529\n",
      "Eval loss 1.8490861654281616, R2 0.04531320556998253\n",
      "epoch 349, loss 1.7760014533996582, R2 0.03890189900994301\n",
      "Eval loss 1.8457190990447998, R2 0.047241196036338806\n",
      "epoch 350, loss 1.7726640701293945, R2 0.040900323539972305\n",
      "Eval loss 1.8423646688461304, R2 0.04916287958621979\n",
      "epoch 351, loss 1.7693394422531128, R2 0.042891718447208405\n",
      "Eval loss 1.8390227556228638, R2 0.05107757821679115\n",
      "epoch 352, loss 1.7660273313522339, R2 0.044875726103782654\n",
      "Eval loss 1.8356932401657104, R2 0.05298576503992081\n",
      "epoch 353, loss 1.7627283334732056, R2 0.046852778643369675\n",
      "Eval loss 1.8323763608932495, R2 0.05488714948296547\n",
      "epoch 354, loss 1.7594419717788696, R2 0.04882245883345604\n",
      "Eval loss 1.8290717601776123, R2 0.05678162723779678\n",
      "epoch 355, loss 1.756168007850647, R2 0.05078518018126488\n",
      "Eval loss 1.825779676437378, R2 0.058669984340667725\n",
      "epoch 356, loss 1.7529065608978271, R2 0.05274062976241112\n",
      "Eval loss 1.8224998712539673, R2 0.0605509877204895\n",
      "epoch 357, loss 1.7496578693389893, R2 0.05468924343585968\n",
      "Eval loss 1.8192322254180908, R2 0.062425803393125534\n",
      "epoch 358, loss 1.7464215755462646, R2 0.05663057789206505\n",
      "Eval loss 1.815976858139038, R2 0.06429417431354523\n",
      "epoch 359, loss 1.7431973218917847, R2 0.0585651621222496\n",
      "Eval loss 1.8127336502075195, R2 0.06615561991930008\n",
      "epoch 360, loss 1.7399855852127075, R2 0.06049254164099693\n",
      "Eval loss 1.8095024824142456, R2 0.06801068782806396\n",
      "epoch 361, loss 1.7367862462997437, R2 0.06241312995553017\n",
      "Eval loss 1.8062835931777954, R2 0.0698590949177742\n",
      "epoch 362, loss 1.7335989475250244, R2 0.06432659178972244\n",
      "Eval loss 1.8030763864517212, R2 0.07170123606920242\n",
      "epoch 363, loss 1.7304238080978394, R2 0.0662333220243454\n",
      "Eval loss 1.7998813390731812, R2 0.07353667914867401\n",
      "epoch 364, loss 1.7272610664367676, R2 0.06813319027423859\n",
      "Eval loss 1.7966982126235962, R2 0.07536597549915314\n",
      "epoch 365, loss 1.7241101264953613, R2 0.07002619653940201\n",
      "Eval loss 1.7935266494750977, R2 0.07718877494335175\n",
      "epoch 366, loss 1.7209712266921997, R2 0.07191240042448044\n",
      "Eval loss 1.7903672456741333, R2 0.07900473475456238\n",
      "epoch 367, loss 1.7178442478179932, R2 0.07379184663295746\n",
      "Eval loss 1.7872194051742554, R2 0.08081495761871338\n",
      "epoch 368, loss 1.7147294282913208, R2 0.07566453516483307\n",
      "Eval loss 1.7840831279754639, R2 0.08261878043413162\n",
      "epoch 369, loss 1.711626410484314, R2 0.07753035426139832\n",
      "Eval loss 1.7809587717056274, R2 0.08441600203514099\n",
      "epoch 370, loss 1.7085351943969727, R2 0.07938964664936066\n",
      "Eval loss 1.777846097946167, R2 0.08620703220367432\n",
      "epoch 371, loss 1.7054556608200073, R2 0.08124211430549622\n",
      "Eval loss 1.7747446298599243, R2 0.08799193799495697\n",
      "epoch 372, loss 1.702387809753418, R2 0.08308832347393036\n",
      "Eval loss 1.7716552019119263, R2 0.08977054804563522\n",
      "epoch 373, loss 1.6993318796157837, R2 0.0849272683262825\n",
      "Eval loss 1.7685768604278564, R2 0.09154302626848221\n",
      "epoch 374, loss 1.6962872743606567, R2 0.08675996214151382\n",
      "Eval loss 1.765510082244873, R2 0.0933089479804039\n",
      "epoch 375, loss 1.6932542324066162, R2 0.08858606964349747\n",
      "Eval loss 1.7624545097351074, R2 0.09506905823945999\n",
      "epoch 376, loss 1.6902333498001099, R2 0.09040576964616776\n",
      "Eval loss 1.7594105005264282, R2 0.09682311862707138\n",
      "epoch 377, loss 1.6872233152389526, R2 0.09221890568733215\n",
      "Eval loss 1.7563776969909668, R2 0.09857071191072464\n",
      "epoch 378, loss 1.6842248439788818, R2 0.09402567893266678\n",
      "Eval loss 1.7533562183380127, R2 0.10031277686357498\n",
      "epoch 379, loss 1.6812375783920288, R2 0.09582545608282089\n",
      "Eval loss 1.7503458261489868, R2 0.10204851627349854\n",
      "epoch 380, loss 1.6782621145248413, R2 0.09761916846036911\n",
      "Eval loss 1.7473466396331787, R2 0.10377775877714157\n",
      "epoch 381, loss 1.6752976179122925, R2 0.09940630942583084\n",
      "Eval loss 1.7443586587905884, R2 0.10550152510404587\n",
      "epoch 382, loss 1.67234468460083, R2 0.10118718445301056\n",
      "Eval loss 1.7413815259933472, R2 0.10721918940544128\n",
      "epoch 383, loss 1.6694027185440063, R2 0.10296199470758438\n",
      "Eval loss 1.7384154796600342, R2 0.10893101990222931\n",
      "epoch 384, loss 1.6664719581604004, R2 0.10473000258207321\n",
      "Eval loss 1.7354602813720703, R2 0.11063659936189651\n",
      "epoch 385, loss 1.6635520458221436, R2 0.10649197548627853\n",
      "Eval loss 1.7325161695480347, R2 0.11233627051115036\n",
      "epoch 386, loss 1.660643458366394, R2 0.10824736952781677\n",
      "Eval loss 1.7295829057693481, R2 0.11403036117553711\n",
      "epoch 387, loss 1.6577459573745728, R2 0.10999668389558792\n",
      "Eval loss 1.7266604900360107, R2 0.11571815609931946\n",
      "epoch 388, loss 1.654859185218811, R2 0.11173991858959198\n",
      "Eval loss 1.7237489223480225, R2 0.11740051954984665\n",
      "epoch 389, loss 1.6519832611083984, R2 0.11347677558660507\n",
      "Eval loss 1.7208479642868042, R2 0.1190766841173172\n",
      "epoch 390, loss 1.6491186618804932, R2 0.11520752310752869\n",
      "Eval loss 1.7179577350616455, R2 0.12074754387140274\n",
      "epoch 391, loss 1.6462645530700684, R2 0.1169319599866867\n",
      "Eval loss 1.715078353881836, R2 0.12241211533546448\n",
      "epoch 392, loss 1.6434211730957031, R2 0.11865034699440002\n",
      "Eval loss 1.7122093439102173, R2 0.12407118827104568\n",
      "epoch 393, loss 1.6405887603759766, R2 0.12036273628473282\n",
      "Eval loss 1.709350824356079, R2 0.1257247030735016\n",
      "epoch 394, loss 1.6377670764923096, R2 0.12206894904375076\n",
      "Eval loss 1.70650315284729, R2 0.12737222015857697\n",
      "epoch 395, loss 1.6349560022354126, R2 0.1237691342830658\n",
      "Eval loss 1.703665852546692, R2 0.1290140300989151\n",
      "epoch 396, loss 1.632155179977417, R2 0.12546324729919434\n",
      "Eval loss 1.7008388042449951, R2 0.13065002858638763\n",
      "epoch 397, loss 1.6293654441833496, R2 0.12715135514736176\n",
      "Eval loss 1.6980226039886475, R2 0.13228058815002441\n",
      "epoch 398, loss 1.6265859603881836, R2 0.1288335919380188\n",
      "Eval loss 1.6952162981033325, R2 0.13390570878982544\n",
      "epoch 399, loss 1.6238170862197876, R2 0.13050967454910278\n",
      "Eval loss 1.692420482635498, R2 0.13552510738372803\n",
      "epoch 400, loss 1.621058464050293, R2 0.13217996060848236\n",
      "Eval loss 1.689634919166565, R2 0.13713869452476501\n",
      "epoch 401, loss 1.6183103322982788, R2 0.13384421169757843\n",
      "Eval loss 1.6868597269058228, R2 0.1387471705675125\n",
      "epoch 402, loss 1.615572452545166, R2 0.1355026811361313\n",
      "Eval loss 1.684094786643982, R2 0.14034982025623322\n",
      "epoch 403, loss 1.6128451824188232, R2 0.13715527951717377\n",
      "Eval loss 1.6813398599624634, R2 0.1419467031955719\n",
      "epoch 404, loss 1.6101278066635132, R2 0.13880200684070587\n",
      "Eval loss 1.6785948276519775, R2 0.14353875815868378\n",
      "epoch 405, loss 1.607420802116394, R2 0.1404430866241455\n",
      "Eval loss 1.675860047340393, R2 0.1451249122619629\n",
      "epoch 406, loss 1.6047240495681763, R2 0.14207793772220612\n",
      "Eval loss 1.6731352806091309, R2 0.14670538902282715\n",
      "epoch 407, loss 1.6020376682281494, R2 0.14370717108249664\n",
      "Eval loss 1.67042076587677, R2 0.14828093349933624\n",
      "epoch 408, loss 1.5993608236312866, R2 0.14533080160617828\n",
      "Eval loss 1.6677159070968628, R2 0.14985094964504242\n",
      "epoch 409, loss 1.5966944694519043, R2 0.1469487100839615\n",
      "Eval loss 1.6650209426879883, R2 0.15141522884368896\n",
      "epoch 410, loss 1.5940380096435547, R2 0.1485610455274582\n",
      "Eval loss 1.662335991859436, R2 0.15297451615333557\n",
      "epoch 411, loss 1.5913913249969482, R2 0.1501672863960266\n",
      "Eval loss 1.6596604585647583, R2 0.15452829003334045\n",
      "epoch 412, loss 1.5887547731399536, R2 0.15176784992218018\n",
      "Eval loss 1.656995415687561, R2 0.15607692301273346\n",
      "epoch 413, loss 1.5861279964447021, R2 0.15336310863494873\n",
      "Eval loss 1.6543394327163696, R2 0.15761993825435638\n",
      "epoch 414, loss 1.5835109949111938, R2 0.15495263040065765\n",
      "Eval loss 1.6516937017440796, R2 0.159157857298851\n",
      "epoch 415, loss 1.5809041261672974, R2 0.15653659403324127\n",
      "Eval loss 1.649057388305664, R2 0.16069069504737854\n",
      "epoch 416, loss 1.578307032585144, R2 0.15811492502689362\n",
      "Eval loss 1.6464307308197021, R2 0.16221831738948822\n",
      "epoch 417, loss 1.5757192373275757, R2 0.1596878468990326\n",
      "Eval loss 1.6438136100769043, R2 0.1637403815984726\n",
      "epoch 418, loss 1.5731414556503296, R2 0.16125503182411194\n",
      "Eval loss 1.641205906867981, R2 0.16525721549987793\n",
      "epoch 419, loss 1.5705732107162476, R2 0.16281677782535553\n",
      "Eval loss 1.6386079788208008, R2 0.16676917672157288\n",
      "epoch 420, loss 1.5680145025253296, R2 0.16437311470508575\n",
      "Eval loss 1.6360195875167847, R2 0.16827574372291565\n",
      "epoch 421, loss 1.5654655694961548, R2 0.16592392325401306\n",
      "Eval loss 1.6334404945373535, R2 0.1697772890329361\n",
      "epoch 422, loss 1.562926173210144, R2 0.167469322681427\n",
      "Eval loss 1.6308706998825073, R2 0.17127369344234467\n",
      "epoch 423, loss 1.5603959560394287, R2 0.1690092533826828\n",
      "Eval loss 1.6283103227615356, R2 0.17276503145694733\n",
      "epoch 424, loss 1.5578755140304565, R2 0.1705438494682312\n",
      "Eval loss 1.6257593631744385, R2 0.1742515116930008\n",
      "epoch 425, loss 1.5553643703460693, R2 0.17207306623458862\n",
      "Eval loss 1.6232177019119263, R2 0.17573247849941254\n",
      "epoch 426, loss 1.552862524986267, R2 0.17359687387943268\n",
      "Eval loss 1.620685338973999, R2 0.17720860242843628\n",
      "epoch 427, loss 1.5503703355789185, R2 0.17511537671089172\n",
      "Eval loss 1.618161916732788, R2 0.1786801517009735\n",
      "epoch 428, loss 1.5478870868682861, R2 0.17662858963012695\n",
      "Eval loss 1.6156479120254517, R2 0.1801462024450302\n",
      "epoch 429, loss 1.5454133749008179, R2 0.17813649773597717\n",
      "Eval loss 1.613142728805542, R2 0.18160732090473175\n",
      "epoch 430, loss 1.5429484844207764, R2 0.17963910102844238\n",
      "Eval loss 1.6106469631195068, R2 0.18306311964988708\n",
      "epoch 431, loss 1.5404932498931885, R2 0.18113650381565094\n",
      "Eval loss 1.608160376548767, R2 0.18451452255249023\n",
      "epoch 432, loss 1.5380467176437378, R2 0.18262872099876404\n",
      "Eval loss 1.6056824922561646, R2 0.18596087396144867\n",
      "epoch 433, loss 1.5356096029281616, R2 0.18411558866500854\n",
      "Eval loss 1.6032137870788574, R2 0.18740209937095642\n",
      "epoch 434, loss 1.5331814289093018, R2 0.18559733033180237\n",
      "Eval loss 1.6007541418075562, R2 0.18883877992630005\n",
      "epoch 435, loss 1.5307624340057373, R2 0.1870739609003067\n",
      "Eval loss 1.598302960395813, R2 0.19027051329612732\n",
      "epoch 436, loss 1.5283522605895996, R2 0.1885453760623932\n",
      "Eval loss 1.5958611965179443, R2 0.1916971653699875\n",
      "epoch 437, loss 1.5259511470794678, R2 0.1900116503238678\n",
      "Eval loss 1.5934278964996338, R2 0.1931193470954895\n",
      "epoch 438, loss 1.5235589742660522, R2 0.19147324562072754\n",
      "Eval loss 1.5910035371780396, R2 0.19453640282154083\n",
      "epoch 439, loss 1.5211753845214844, R2 0.19292908906936646\n",
      "Eval loss 1.5885881185531616, R2 0.195948988199234\n",
      "epoch 440, loss 1.5188010931015015, R2 0.1943800449371338\n",
      "Eval loss 1.5861811637878418, R2 0.19735656678676605\n",
      "epoch 441, loss 1.5164353847503662, R2 0.19582639634609222\n",
      "Eval loss 1.5837831497192383, R2 0.19875933229923248\n",
      "epoch 442, loss 1.5140784978866577, R2 0.1972670555114746\n",
      "Eval loss 1.581393837928772, R2 0.2001575529575348\n",
      "epoch 443, loss 1.511730432510376, R2 0.19870303571224213\n",
      "Eval loss 1.5790129899978638, R2 0.20155078172683716\n",
      "epoch 444, loss 1.509390950202942, R2 0.20013436675071716\n",
      "Eval loss 1.5766406059265137, R2 0.20293964445590973\n",
      "epoch 445, loss 1.5070604085922241, R2 0.20156002044677734\n",
      "Eval loss 1.5742770433425903, R2 0.20432348549365997\n",
      "epoch 446, loss 1.5047380924224854, R2 0.20298117399215698\n",
      "Eval loss 1.5719218254089355, R2 0.20570287108421326\n",
      "epoch 447, loss 1.5024245977401733, R2 0.20439733564853668\n",
      "Eval loss 1.569575309753418, R2 0.20707769691944122\n",
      "epoch 448, loss 1.500119686126709, R2 0.20580846071243286\n",
      "Eval loss 1.5672372579574585, R2 0.20844773948192596\n",
      "epoch 449, loss 1.4978233575820923, R2 0.20721477270126343\n",
      "Eval loss 1.5649075508117676, R2 0.20981331169605255\n",
      "epoch 450, loss 1.4955353736877441, R2 0.20861627161502838\n",
      "Eval loss 1.5625863075256348, R2 0.2111741602420807\n",
      "epoch 451, loss 1.493255853652954, R2 0.21001295745372772\n",
      "Eval loss 1.5602730512619019, R2 0.21253056824207306\n",
      "epoch 452, loss 1.4909850358963013, R2 0.21140466630458832\n",
      "Eval loss 1.5579687356948853, R2 0.21388214826583862\n",
      "epoch 453, loss 1.488722324371338, R2 0.21279166638851166\n",
      "Eval loss 1.555672287940979, R2 0.21522969007492065\n",
      "epoch 454, loss 1.486467957496643, R2 0.21417388319969177\n",
      "Eval loss 1.5533841848373413, R2 0.21657219529151917\n",
      "epoch 455, loss 1.484222173690796, R2 0.2155512273311615\n",
      "Eval loss 1.5511044263839722, R2 0.21791042387485504\n",
      "epoch 456, loss 1.4819844961166382, R2 0.21692386269569397\n",
      "Eval loss 1.5488327741622925, R2 0.21924398839473724\n",
      "epoch 457, loss 1.479755163192749, R2 0.21829190850257874\n",
      "Eval loss 1.5465692281723022, R2 0.2205733209848404\n",
      "epoch 458, loss 1.4775340557098389, R2 0.21965497732162476\n",
      "Eval loss 1.544313907623291, R2 0.2218981385231018\n",
      "epoch 459, loss 1.4753209352493286, R2 0.22101348638534546\n",
      "Eval loss 1.5420668125152588, R2 0.22321835160255432\n",
      "epoch 460, loss 1.4731162786483765, R2 0.22236722707748413\n",
      "Eval loss 1.539827823638916, R2 0.22453442215919495\n",
      "epoch 461, loss 1.4709196090698242, R2 0.2237163931131363\n",
      "Eval loss 1.537596583366394, R2 0.22584576904773712\n",
      "epoch 462, loss 1.4687310457229614, R2 0.22506083548069\n",
      "Eval loss 1.5353734493255615, R2 0.22715316712856293\n",
      "epoch 463, loss 1.4665504693984985, R2 0.22640103101730347\n",
      "Eval loss 1.5331584215164185, R2 0.22845600545406342\n",
      "epoch 464, loss 1.464377999305725, R2 0.22773593664169312\n",
      "Eval loss 1.5309510231018066, R2 0.22975410521030426\n",
      "epoch 465, loss 1.462213397026062, R2 0.22906659543514252\n",
      "Eval loss 1.5287517309188843, R2 0.2310483604669571\n",
      "epoch 466, loss 1.460057020187378, R2 0.23039278388023376\n",
      "Eval loss 1.5265604257583618, R2 0.23233799636363983\n",
      "epoch 467, loss 1.4579083919525146, R2 0.23171447217464447\n",
      "Eval loss 1.5243767499923706, R2 0.23362380266189575\n",
      "epoch 468, loss 1.4557676315307617, R2 0.23303116858005524\n",
      "Eval loss 1.5222011804580688, R2 0.2349047213792801\n",
      "epoch 469, loss 1.4536347389221191, R2 0.23434361815452576\n",
      "Eval loss 1.5200332403182983, R2 0.2361816018819809\n",
      "epoch 470, loss 1.451509714126587, R2 0.23565158247947693\n",
      "Eval loss 1.5178728103637695, R2 0.2374543994665146\n",
      "epoch 471, loss 1.449392557144165, R2 0.23695503175258636\n",
      "Eval loss 1.5157204866409302, R2 0.23872289061546326\n",
      "epoch 472, loss 1.447283148765564, R2 0.23825402557849884\n",
      "Eval loss 1.5135756731033325, R2 0.23998688161373138\n",
      "epoch 473, loss 1.4451816082000732, R2 0.23954854905605316\n",
      "Eval loss 1.5114386081695557, R2 0.2412470281124115\n",
      "epoch 474, loss 1.4430875778198242, R2 0.2408386617898941\n",
      "Eval loss 1.5093092918395996, R2 0.24250292778015137\n",
      "epoch 475, loss 1.441001296043396, R2 0.24212436378002167\n",
      "Eval loss 1.5071873664855957, R2 0.24375440180301666\n",
      "epoch 476, loss 1.438922643661499, R2 0.24340590834617615\n",
      "Eval loss 1.5050731897354126, R2 0.24500204622745514\n",
      "epoch 477, loss 1.4368516206741333, R2 0.2446829229593277\n",
      "Eval loss 1.502966284751892, R2 0.24624523520469666\n",
      "epoch 478, loss 1.4347881078720093, R2 0.24595509469509125\n",
      "Eval loss 1.5008668899536133, R2 0.24748443067073822\n",
      "epoch 479, loss 1.4327322244644165, R2 0.24722331762313843\n",
      "Eval loss 1.4987752437591553, R2 0.24871955811977386\n",
      "epoch 480, loss 1.430683970451355, R2 0.24848714470863342\n",
      "Eval loss 1.4966908693313599, R2 0.24995075166225433\n",
      "epoch 481, loss 1.4286431074142456, R2 0.24974671006202698\n",
      "Eval loss 1.4946140050888062, R2 0.25117766857147217\n",
      "epoch 482, loss 1.4266095161437988, R2 0.2510019838809967\n",
      "Eval loss 1.4925445318222046, R2 0.2524004280567169\n",
      "epoch 483, loss 1.4245836734771729, R2 0.2522531747817993\n",
      "Eval loss 1.490482211112976, R2 0.2536192238330841\n",
      "epoch 484, loss 1.4225651025772095, R2 0.2534996271133423\n",
      "Eval loss 1.4884275197982788, R2 0.25483420491218567\n",
      "epoch 485, loss 1.4205539226531982, R2 0.2547421157360077\n",
      "Eval loss 1.4863801002502441, R2 0.25604480504989624\n",
      "epoch 486, loss 1.4185500144958496, R2 0.2559802830219269\n",
      "Eval loss 1.4843398332595825, R2 0.25725170969963074\n",
      "epoch 487, loss 1.4165534973144531, R2 0.25721433758735657\n",
      "Eval loss 1.482306957244873, R2 0.2584543526172638\n",
      "epoch 488, loss 1.4145642518997192, R2 0.25844427943229675\n",
      "Eval loss 1.480281114578247, R2 0.25965335965156555\n",
      "epoch 489, loss 1.4125821590423584, R2 0.2596697211265564\n",
      "Eval loss 1.4782625436782837, R2 0.26084813475608826\n",
      "epoch 490, loss 1.4106074571609497, R2 0.26089122891426086\n",
      "Eval loss 1.4762510061264038, R2 0.2620391547679901\n",
      "epoch 491, loss 1.408639907836914, R2 0.2621084451675415\n",
      "Eval loss 1.474246859550476, R2 0.26322612166404724\n",
      "epoch 492, loss 1.406679630279541, R2 0.2633216083049774\n",
      "Eval loss 1.4722497463226318, R2 0.26440930366516113\n",
      "epoch 493, loss 1.404726266860962, R2 0.26453065872192383\n",
      "Eval loss 1.4702595472335815, R2 0.2655884921550751\n",
      "epoch 494, loss 1.402780294418335, R2 0.2657358944416046\n",
      "Eval loss 1.4682766199111938, R2 0.2667637765407562\n",
      "epoch 495, loss 1.4008411169052124, R2 0.26693645119667053\n",
      "Eval loss 1.4663006067276, R2 0.2679350972175598\n",
      "epoch 496, loss 1.398909091949463, R2 0.2681334614753723\n",
      "Eval loss 1.4643315076828003, R2 0.26910287141799927\n",
      "epoch 497, loss 1.3969841003417969, R2 0.26932600140571594\n",
      "Eval loss 1.4623695611953735, R2 0.2702665328979492\n",
      "epoch 498, loss 1.3950660228729248, R2 0.27051466703414917\n",
      "Eval loss 1.4604144096374512, R2 0.27142637968063354\n",
      "epoch 499, loss 1.3931552171707153, R2 0.27169930934906006\n",
      "Eval loss 1.4584664106369019, R2 0.27258267998695374\n",
      "epoch 500, loss 1.3912510871887207, R2 0.2728798985481262\n",
      "Eval loss 1.4565249681472778, R2 0.2737348675727844\n",
      "epoch 501, loss 1.38935387134552, R2 0.274056613445282\n",
      "Eval loss 1.4545905590057373, R2 0.2748834788799286\n",
      "epoch 502, loss 1.3874638080596924, R2 0.275229275226593\n",
      "Eval loss 1.4526629447937012, R2 0.2760283350944519\n",
      "epoch 503, loss 1.38558030128479, R2 0.27639806270599365\n",
      "Eval loss 1.4507421255111694, R2 0.277169406414032\n",
      "epoch 504, loss 1.3837037086486816, R2 0.2775627374649048\n",
      "Eval loss 1.448828101158142, R2 0.27830663323402405\n",
      "epoch 505, loss 1.3818340301513672, R2 0.2787235677242279\n",
      "Eval loss 1.4469207525253296, R2 0.2794402837753296\n",
      "epoch 506, loss 1.379970908164978, R2 0.279880553483963\n",
      "Eval loss 1.4450204372406006, R2 0.2805699408054352\n",
      "epoch 507, loss 1.3781147003173828, R2 0.2810334861278534\n",
      "Eval loss 1.4431265592575073, R2 0.28169623017311096\n",
      "epoch 508, loss 1.376265287399292, R2 0.2821827828884125\n",
      "Eval loss 1.441239356994629, R2 0.28281885385513306\n",
      "epoch 509, loss 1.3744224309921265, R2 0.2833278477191925\n",
      "Eval loss 1.4393588304519653, R2 0.2839374542236328\n",
      "epoch 510, loss 1.3725862503051758, R2 0.28446924686431885\n",
      "Eval loss 1.437484860420227, R2 0.28505271673202515\n",
      "epoch 511, loss 1.37075674533844, R2 0.2856067717075348\n",
      "Eval loss 1.4356175661087036, R2 0.28616446256637573\n",
      "epoch 512, loss 1.368933916091919, R2 0.2867405116558075\n",
      "Eval loss 1.433756947517395, R2 0.28727206587791443\n",
      "epoch 513, loss 1.3671176433563232, R2 0.2878704369068146\n",
      "Eval loss 1.4319026470184326, R2 0.28837642073631287\n",
      "epoch 514, loss 1.3653079271316528, R2 0.2889965772628784\n",
      "Eval loss 1.4300549030303955, R2 0.28947713971138\n",
      "epoch 515, loss 1.3635046482086182, R2 0.2901190519332886\n",
      "Eval loss 1.4282138347625732, R2 0.2905741035938263\n",
      "epoch 516, loss 1.3617080450057983, R2 0.29123762249946594\n",
      "Eval loss 1.4263790845870972, R2 0.2916678488254547\n",
      "epoch 517, loss 1.3599178791046143, R2 0.29235219955444336\n",
      "Eval loss 1.4245507717132568, R2 0.2927577793598175\n",
      "epoch 518, loss 1.3581340312957764, R2 0.2934632897377014\n",
      "Eval loss 1.4227290153503418, R2 0.29384419322013855\n",
      "epoch 519, loss 1.3563568592071533, R2 0.29457056522369385\n",
      "Eval loss 1.420913577079773, R2 0.2949272096157074\n",
      "epoch 520, loss 1.354585886001587, R2 0.295674204826355\n",
      "Eval loss 1.4191044569015503, R2 0.29600638151168823\n",
      "epoch 521, loss 1.3528214693069458, R2 0.2967740595340729\n",
      "Eval loss 1.4173016548156738, R2 0.29708224534988403\n",
      "epoch 522, loss 1.3510632514953613, R2 0.29787030816078186\n",
      "Eval loss 1.4155051708221436, R2 0.2981545627117157\n",
      "epoch 523, loss 1.3493114709854126, R2 0.29896289110183716\n",
      "Eval loss 1.4137150049209595, R2 0.2992236316204071\n",
      "epoch 524, loss 1.3475658893585205, R2 0.30005186796188354\n",
      "Eval loss 1.4119311571121216, R2 0.30028900504112244\n",
      "epoch 525, loss 1.3458267450332642, R2 0.3011370301246643\n",
      "Eval loss 1.4101533889770508, R2 0.3013509511947632\n",
      "epoch 526, loss 1.3440937995910645, R2 0.3022187054157257\n",
      "Eval loss 1.4083820581436157, R2 0.3024093806743622\n",
      "epoch 527, loss 1.3423670530319214, R2 0.30329668521881104\n",
      "Eval loss 1.4066168069839478, R2 0.30346450209617615\n",
      "epoch 528, loss 1.340646505355835, R2 0.3043712377548218\n",
      "Eval loss 1.4048577547073364, R2 0.3045162856578827\n",
      "epoch 529, loss 1.3389321565628052, R2 0.30544185638427734\n",
      "Eval loss 1.4031049013137817, R2 0.30556461215019226\n",
      "epoch 530, loss 1.3372238874435425, R2 0.30650919675827026\n",
      "Eval loss 1.4013581275939941, R2 0.30660954117774963\n",
      "epoch 531, loss 1.3355220556259155, R2 0.3075728714466095\n",
      "Eval loss 1.399617314338684, R2 0.307651162147522\n",
      "epoch 532, loss 1.333825945854187, R2 0.3086329996585846\n",
      "Eval loss 1.3978829383850098, R2 0.30868905782699585\n",
      "epoch 533, loss 1.3321361541748047, R2 0.3096899390220642\n",
      "Eval loss 1.3961541652679443, R2 0.30972397327423096\n",
      "epoch 534, loss 1.3304522037506104, R2 0.31074288487434387\n",
      "Eval loss 1.3944315910339355, R2 0.31075528264045715\n",
      "epoch 535, loss 1.3287744522094727, R2 0.3117922842502594\n",
      "Eval loss 1.3927152156829834, R2 0.31178340315818787\n",
      "epoch 536, loss 1.327102780342102, R2 0.3128383457660675\n",
      "Eval loss 1.3910045623779297, R2 0.31280824542045593\n",
      "epoch 537, loss 1.3254371881484985, R2 0.3138812184333801\n",
      "Eval loss 1.3893001079559326, R2 0.31382977962493896\n",
      "epoch 538, loss 1.3237773180007935, R2 0.3149203062057495\n",
      "Eval loss 1.387601375579834, R2 0.3148481845855713\n",
      "epoch 539, loss 1.322123408317566, R2 0.31595578789711\n",
      "Eval loss 1.3859087228775024, R2 0.3158629238605499\n",
      "epoch 540, loss 1.320475459098816, R2 0.31698811054229736\n",
      "Eval loss 1.3842217922210693, R2 0.316874623298645\n",
      "epoch 541, loss 1.318833589553833, R2 0.3180168867111206\n",
      "Eval loss 1.3825408220291138, R2 0.31788313388824463\n",
      "epoch 542, loss 1.3171974420547485, R2 0.3190423250198364\n",
      "Eval loss 1.3808656930923462, R2 0.31888827681541443\n",
      "epoch 543, loss 1.315567135810852, R2 0.32006433606147766\n",
      "Eval loss 1.3791964054107666, R2 0.3198902904987335\n",
      "epoch 544, loss 1.313942551612854, R2 0.32108309864997864\n",
      "Eval loss 1.3775328397750854, R2 0.3208891451358795\n",
      "epoch 545, loss 1.3123239278793335, R2 0.32209813594818115\n",
      "Eval loss 1.3758751153945923, R2 0.3218844532966614\n",
      "epoch 546, loss 1.310711145401001, R2 0.3231099843978882\n",
      "Eval loss 1.374222993850708, R2 0.32287687063217163\n",
      "epoch 547, loss 1.309104084968567, R2 0.3241184949874878\n",
      "Eval loss 1.3725768327713013, R2 0.3238661289215088\n",
      "epoch 548, loss 1.3075025081634521, R2 0.32512366771698\n",
      "Eval loss 1.3709362745285034, R2 0.32485195994377136\n",
      "epoch 549, loss 1.3059067726135254, R2 0.3261255919933319\n",
      "Eval loss 1.3693013191223145, R2 0.32583484053611755\n",
      "epoch 550, loss 1.3043169975280762, R2 0.32712411880493164\n",
      "Eval loss 1.367672085762024, R2 0.32681453227996826\n",
      "epoch 551, loss 1.3027324676513672, R2 0.32811954617500305\n",
      "Eval loss 1.3660485744476318, R2 0.32779109477996826\n",
      "epoch 552, loss 1.3011538982391357, R2 0.32911133766174316\n",
      "Eval loss 1.3644307851791382, R2 0.3287646174430847\n",
      "epoch 553, loss 1.2995808124542236, R2 0.33010008931159973\n",
      "Eval loss 1.3628182411193848, R2 0.3297349214553833\n",
      "epoch 554, loss 1.2980133295059204, R2 0.3310854136943817\n",
      "Eval loss 1.3612116575241089, R2 0.33070212602615356\n",
      "epoch 555, loss 1.2964513301849365, R2 0.3320677578449249\n",
      "Eval loss 1.3596103191375732, R2 0.33166608214378357\n",
      "epoch 556, loss 1.294895052909851, R2 0.3330465257167816\n",
      "Eval loss 1.358014702796936, R2 0.33262717723846436\n",
      "epoch 557, loss 1.293344259262085, R2 0.33402219414711\n",
      "Eval loss 1.3564244508743286, R2 0.3335852324962616\n",
      "epoch 558, loss 1.2917991876602173, R2 0.3349948227405548\n",
      "Eval loss 1.35483980178833, R2 0.33454006910324097\n",
      "epoch 559, loss 1.2902593612670898, R2 0.33596399426460266\n",
      "Eval loss 1.3532606363296509, R2 0.3354920446872711\n",
      "epoch 560, loss 1.2887250185012817, R2 0.33693018555641174\n",
      "Eval loss 1.3516868352890015, R2 0.33644071221351624\n",
      "epoch 561, loss 1.287196159362793, R2 0.3378930389881134\n",
      "Eval loss 1.350118637084961, R2 0.33738645911216736\n",
      "epoch 562, loss 1.285672664642334, R2 0.33885276317596436\n",
      "Eval loss 1.3485558032989502, R2 0.3383292257785797\n",
      "epoch 563, loss 1.2841546535491943, R2 0.339809387922287\n",
      "Eval loss 1.3469982147216797, R2 0.3392691910266876\n",
      "epoch 564, loss 1.2826420068740845, R2 0.3407628834247589\n",
      "Eval loss 1.345445990562439, R2 0.34020596742630005\n",
      "epoch 565, loss 1.2811347246170044, R2 0.34171319007873535\n",
      "Eval loss 1.3438993692398071, R2 0.3411398231983185\n",
      "epoch 566, loss 1.279632806777954, R2 0.3426603674888611\n",
      "Eval loss 1.3423577547073364, R2 0.34207066893577576\n",
      "epoch 567, loss 1.278136134147644, R2 0.34360453486442566\n",
      "Eval loss 1.3408215045928955, R2 0.3429986834526062\n",
      "epoch 568, loss 1.2766449451446533, R2 0.34454548358917236\n",
      "Eval loss 1.3392906188964844, R2 0.34392350912094116\n",
      "epoch 569, loss 1.2751588821411133, R2 0.3454834222793579\n",
      "Eval loss 1.337765097618103, R2 0.34484556317329407\n",
      "epoch 570, loss 1.2736780643463135, R2 0.3464182913303375\n",
      "Eval loss 1.3362445831298828, R2 0.34576454758644104\n",
      "epoch 571, loss 1.272202491760254, R2 0.3473500609397888\n",
      "Eval loss 1.3347294330596924, R2 0.3466809093952179\n",
      "epoch 572, loss 1.2707322835922241, R2 0.34827879071235657\n",
      "Eval loss 1.3332194089889526, R2 0.34759417176246643\n",
      "epoch 573, loss 1.2692670822143555, R2 0.34920448064804077\n",
      "Eval loss 1.3317148685455322, R2 0.34850457310676575\n",
      "epoch 574, loss 1.267807126045227, R2 0.3501271903514862\n",
      "Eval loss 1.3302150964736938, R2 0.34941205382347107\n",
      "epoch 575, loss 1.2663524150848389, R2 0.3510468006134033\n",
      "Eval loss 1.3287205696105957, R2 0.3503166735172272\n",
      "epoch 576, loss 1.2649027109146118, R2 0.3519635498523712\n",
      "Eval loss 1.3272311687469482, R2 0.35121849179267883\n",
      "epoch 577, loss 1.2634581327438354, R2 0.352877140045166\n",
      "Eval loss 1.325747013092041, R2 0.3521173596382141\n",
      "epoch 578, loss 1.2620186805725098, R2 0.3537878692150116\n",
      "Eval loss 1.3242679834365845, R2 0.35301342606544495\n",
      "epoch 579, loss 1.2605843544006348, R2 0.354695588350296\n",
      "Eval loss 1.3227938413619995, R2 0.3539067506790161\n",
      "epoch 580, loss 1.259155035018921, R2 0.3556002974510193\n",
      "Eval loss 1.3213248252868652, R2 0.35479724407196045\n",
      "epoch 581, loss 1.2577307224273682, R2 0.3565021753311157\n",
      "Eval loss 1.3198609352111816, R2 0.3556850254535675\n",
      "epoch 582, loss 1.2563116550445557, R2 0.3574009835720062\n",
      "Eval loss 1.31840181350708, R2 0.356569766998291\n",
      "epoch 583, loss 1.2548973560333252, R2 0.3582969009876251\n",
      "Eval loss 1.3169478178024292, R2 0.3574517071247101\n",
      "epoch 584, loss 1.2534880638122559, R2 0.35918989777565\n",
      "Eval loss 1.31549870967865, R2 0.3583310842514038\n",
      "epoch 585, loss 1.252083659172058, R2 0.3600800335407257\n",
      "Eval loss 1.3140544891357422, R2 0.3592076003551483\n",
      "epoch 586, loss 1.2506842613220215, R2 0.3609674572944641\n",
      "Eval loss 1.3126153945922852, R2 0.3600812256336212\n",
      "epoch 587, loss 1.2492899894714355, R2 0.3618518114089966\n",
      "Eval loss 1.3111810684204102, R2 0.3609522581100464\n",
      "epoch 588, loss 1.247900366783142, R2 0.36273327469825745\n",
      "Eval loss 1.3097517490386963, R2 0.3618205785751343\n",
      "epoch 589, loss 1.2465157508850098, R2 0.3636116683483124\n",
      "Eval loss 1.308327317237854, R2 0.36268606781959534\n",
      "epoch 590, loss 1.2451359033584595, R2 0.3644874393939972\n",
      "Eval loss 1.3069076538085938, R2 0.3635488748550415\n",
      "epoch 591, loss 1.2437609434127808, R2 0.3653603494167328\n",
      "Eval loss 1.305492877960205, R2 0.36440905928611755\n",
      "epoch 592, loss 1.242390751838684, R2 0.36623045802116394\n",
      "Eval loss 1.304082989692688, R2 0.3652666211128235\n",
      "epoch 593, loss 1.241025447845459, R2 0.36709776520729065\n",
      "Eval loss 1.302677869796753, R2 0.3661212921142578\n",
      "epoch 594, loss 1.239664912223816, R2 0.36796215176582336\n",
      "Eval loss 1.3012773990631104, R2 0.3669733703136444\n",
      "epoch 595, loss 1.2383091449737549, R2 0.36882397532463074\n",
      "Eval loss 1.2998816967010498, R2 0.36782264709472656\n",
      "epoch 596, loss 1.2369581460952759, R2 0.3696828782558441\n",
      "Eval loss 1.2984910011291504, R2 0.368669331073761\n",
      "epoch 597, loss 1.235611915588379, R2 0.37053874135017395\n",
      "Eval loss 1.297104835510254, R2 0.36951348185539246\n",
      "epoch 598, loss 1.234270453453064, R2 0.3713921308517456\n",
      "Eval loss 1.29572331905365, R2 0.3703550100326538\n",
      "epoch 599, loss 1.2329334020614624, R2 0.37224283814430237\n",
      "Eval loss 1.294346809387207, R2 0.3711937665939331\n",
      "epoch 600, loss 1.2316012382507324, R2 0.3730907142162323\n",
      "Eval loss 1.292974591255188, R2 0.37202998995780945\n",
      "epoch 601, loss 1.2302738428115845, R2 0.37393561005592346\n",
      "Eval loss 1.2916072607040405, R2 0.3728634715080261\n",
      "epoch 602, loss 1.2289509773254395, R2 0.37477797269821167\n",
      "Eval loss 1.290244460105896, R2 0.3736943304538727\n",
      "epoch 603, loss 1.2276326417922974, R2 0.37561771273612976\n",
      "Eval loss 1.288886308670044, R2 0.37452271580696106\n",
      "epoch 604, loss 1.2263190746307373, R2 0.37645453214645386\n",
      "Eval loss 1.2875328063964844, R2 0.37534868717193604\n",
      "epoch 605, loss 1.225009799003601, R2 0.3772888481616974\n",
      "Eval loss 1.2861837148666382, R2 0.37617194652557373\n",
      "epoch 606, loss 1.2237054109573364, R2 0.3781203031539917\n",
      "Eval loss 1.2848395109176636, R2 0.37699252367019653\n",
      "epoch 607, loss 1.2224054336547852, R2 0.3789491653442383\n",
      "Eval loss 1.2834995985031128, R2 0.3778105080127716\n",
      "epoch 608, loss 1.2211101055145264, R2 0.3797755241394043\n",
      "Eval loss 1.282164216041565, R2 0.37862613797187805\n",
      "epoch 609, loss 1.2198190689086914, R2 0.38059893250465393\n",
      "Eval loss 1.2808336019515991, R2 0.37943917512893677\n",
      "epoch 610, loss 1.2185328006744385, R2 0.3814197778701782\n",
      "Eval loss 1.2795072793960571, R2 0.38024961948394775\n",
      "epoch 611, loss 1.2172508239746094, R2 0.38223809003829956\n",
      "Eval loss 1.278185486793518, R2 0.38105764985084534\n",
      "epoch 612, loss 1.2159734964370728, R2 0.38305360078811646\n",
      "Eval loss 1.2768681049346924, R2 0.3818630278110504\n",
      "epoch 613, loss 1.21470046043396, R2 0.38386669754981995\n",
      "Eval loss 1.2755552530288696, R2 0.3826659917831421\n",
      "epoch 614, loss 1.2134318351745605, R2 0.38467729091644287\n",
      "Eval loss 1.2742468118667603, R2 0.383466511964798\n",
      "epoch 615, loss 1.212167739868164, R2 0.3854849636554718\n",
      "Eval loss 1.2729427814483643, R2 0.38426458835601807\n",
      "epoch 616, loss 1.2109079360961914, R2 0.38629016280174255\n",
      "Eval loss 1.2716432809829712, R2 0.38506004214286804\n",
      "epoch 617, loss 1.2096526622772217, R2 0.3870929479598999\n",
      "Eval loss 1.2703479528427124, R2 0.38585299253463745\n",
      "epoch 618, loss 1.2084016799926758, R2 0.38789302110671997\n",
      "Eval loss 1.269057035446167, R2 0.38664358854293823\n",
      "epoch 619, loss 1.2071548700332642, R2 0.38869044184684753\n",
      "Eval loss 1.2677706480026245, R2 0.387431800365448\n",
      "epoch 620, loss 1.2059125900268555, R2 0.38948532938957214\n",
      "Eval loss 1.2664884328842163, R2 0.3882174491882324\n",
      "epoch 621, loss 1.204674482345581, R2 0.3902777433395386\n",
      "Eval loss 1.2652103900909424, R2 0.389000803232193\n",
      "epoch 622, loss 1.2034409046173096, R2 0.3910678029060364\n",
      "Eval loss 1.2639367580413818, R2 0.3897815942764282\n",
      "epoch 623, loss 1.2022113800048828, R2 0.3918549716472626\n",
      "Eval loss 1.2626676559448242, R2 0.3905601501464844\n",
      "epoch 624, loss 1.2009861469268799, R2 0.39263975620269775\n",
      "Eval loss 1.2614026069641113, R2 0.3913361430168152\n",
      "epoch 625, loss 1.1997653245925903, R2 0.39342212677001953\n",
      "Eval loss 1.2601418495178223, R2 0.39210978150367737\n",
      "epoch 626, loss 1.198548436164856, R2 0.39420196413993835\n",
      "Eval loss 1.2588852643966675, R2 0.39288103580474854\n",
      "epoch 627, loss 1.1973360776901245, R2 0.3949793577194214\n",
      "Eval loss 1.2576329708099365, R2 0.39364996552467346\n",
      "epoch 628, loss 1.1961278915405273, R2 0.39575421810150146\n",
      "Eval loss 1.2563849687576294, R2 0.39441654086112976\n",
      "epoch 629, loss 1.1949236392974854, R2 0.39652660489082336\n",
      "Eval loss 1.255141019821167, R2 0.39518052339553833\n",
      "epoch 630, loss 1.1937237977981567, R2 0.3972965478897095\n",
      "Eval loss 1.2539012432098389, R2 0.3959423899650574\n",
      "epoch 631, loss 1.1925278902053833, R2 0.39806410670280457\n",
      "Eval loss 1.252665638923645, R2 0.39670178294181824\n",
      "epoch 632, loss 1.1913362741470337, R2 0.3988291025161743\n",
      "Eval loss 1.251434087753296, R2 0.3974590003490448\n",
      "epoch 633, loss 1.1901488304138184, R2 0.39959192276000977\n",
      "Eval loss 1.2502068281173706, R2 0.39821383357048035\n",
      "epoch 634, loss 1.1889653205871582, R2 0.40035197138786316\n",
      "Eval loss 1.24898362159729, R2 0.3989661931991577\n",
      "epoch 635, loss 1.1877858638763428, R2 0.4011096954345703\n",
      "Eval loss 1.2477645874023438, R2 0.39971646666526794\n",
      "epoch 636, loss 1.1866105794906616, R2 0.4018650949001312\n",
      "Eval loss 1.2465494871139526, R2 0.4004642367362976\n",
      "epoch 637, loss 1.1854395866394043, R2 0.4026182293891907\n",
      "Eval loss 1.2453385591506958, R2 0.40120986104011536\n",
      "epoch 638, loss 1.1842721700668335, R2 0.40336868166923523\n",
      "Eval loss 1.2441315650939941, R2 0.40195319056510925\n",
      "epoch 639, loss 1.183109164237976, R2 0.4041168987751007\n",
      "Eval loss 1.2429287433624268, R2 0.4026940166950226\n",
      "epoch 640, loss 1.1819499731063843, R2 0.4048627018928528\n",
      "Eval loss 1.2417298555374146, R2 0.4034327566623688\n",
      "epoch 641, loss 1.1807947158813477, R2 0.40560615062713623\n",
      "Eval loss 1.240535020828247, R2 0.4041692912578583\n",
      "epoch 642, loss 1.1796436309814453, R2 0.4063475430011749\n",
      "Eval loss 1.2393441200256348, R2 0.40490350127220154\n",
      "epoch 643, loss 1.1784964799880981, R2 0.40708616375923157\n",
      "Eval loss 1.2381571531295776, R2 0.4056354761123657\n",
      "epoch 644, loss 1.1773531436920166, R2 0.40782251954078674\n",
      "Eval loss 1.2369742393493652, R2 0.40636518597602844\n",
      "epoch 645, loss 1.1762137413024902, R2 0.40855672955513\n",
      "Eval loss 1.235795259475708, R2 0.4070928692817688\n",
      "epoch 646, loss 1.1750783920288086, R2 0.4092884063720703\n",
      "Eval loss 1.234620213508606, R2 0.4078180491924286\n",
      "epoch 647, loss 1.1739468574523926, R2 0.41001787781715393\n",
      "Eval loss 1.2334489822387695, R2 0.4085410237312317\n",
      "epoch 648, loss 1.1728192567825317, R2 0.41074511408805847\n",
      "Eval loss 1.2322818040847778, R2 0.40926188230514526\n",
      "epoch 649, loss 1.1716954708099365, R2 0.4114699363708496\n",
      "Eval loss 1.2311183214187622, R2 0.4099806845188141\n",
      "epoch 650, loss 1.1705756187438965, R2 0.41219252347946167\n",
      "Eval loss 1.2299588918685913, R2 0.4106971025466919\n",
      "epoch 651, loss 1.1694594621658325, R2 0.41291287541389465\n",
      "Eval loss 1.228803277015686, R2 0.411411315202713\n",
      "epoch 652, loss 1.1683473587036133, R2 0.4136309325695038\n",
      "Eval loss 1.2276513576507568, R2 0.41212359070777893\n",
      "epoch 653, loss 1.1672389507293701, R2 0.4143466055393219\n",
      "Eval loss 1.2265034914016724, R2 0.41283339262008667\n",
      "epoch 654, loss 1.1661343574523926, R2 0.4150601625442505\n",
      "Eval loss 1.225359320640564, R2 0.413541316986084\n",
      "epoch 655, loss 1.1650334596633911, R2 0.415771484375\n",
      "Eval loss 1.2242189645767212, R2 0.4142468273639679\n",
      "epoch 656, loss 1.1639363765716553, R2 0.4164807200431824\n",
      "Eval loss 1.223082423210144, R2 0.4149504601955414\n",
      "epoch 657, loss 1.1628432273864746, R2 0.41718733310699463\n",
      "Eval loss 1.2219496965408325, R2 0.41565170884132385\n",
      "epoch 658, loss 1.1617536544799805, R2 0.4178919494152069\n",
      "Eval loss 1.2208205461502075, R2 0.41635096073150635\n",
      "epoch 659, loss 1.1606677770614624, R2 0.41859427094459534\n",
      "Eval loss 1.2196952104568481, R2 0.4170480966567993\n",
      "epoch 660, loss 1.15958571434021, R2 0.41929444670677185\n",
      "Eval loss 1.2185735702514648, R2 0.41774311661720276\n",
      "epoch 661, loss 1.158507227897644, R2 0.41999244689941406\n",
      "Eval loss 1.2174557447433472, R2 0.4184357821941376\n",
      "epoch 662, loss 1.1574326753616333, R2 0.4206882417201996\n",
      "Eval loss 1.2163416147232056, R2 0.4191267192363739\n",
      "epoch 663, loss 1.15636146068573, R2 0.4213818609714508\n",
      "Eval loss 1.21523118019104, R2 0.41981542110443115\n",
      "epoch 664, loss 1.1552941799163818, R2 0.4220733344554901\n",
      "Eval loss 1.214124321937561, R2 0.4205020070075989\n",
      "epoch 665, loss 1.1542304754257202, R2 0.42276257276535034\n",
      "Eval loss 1.213021159172058, R2 0.4211864769458771\n",
      "epoch 666, loss 1.1531702280044556, R2 0.42344969511032104\n",
      "Eval loss 1.2119216918945312, R2 0.4218690097332001\n",
      "epoch 667, loss 1.152113676071167, R2 0.42413464188575745\n",
      "Eval loss 1.210825800895691, R2 0.4225492775440216\n",
      "epoch 668, loss 1.151060938835144, R2 0.42481744289398193\n",
      "Eval loss 1.2097336053848267, R2 0.42322760820388794\n",
      "epoch 669, loss 1.150011658668518, R2 0.4254980981349945\n",
      "Eval loss 1.2086448669433594, R2 0.42390382289886475\n",
      "epoch 670, loss 1.1489659547805786, R2 0.42617666721343994\n",
      "Eval loss 1.2075600624084473, R2 0.42457816004753113\n",
      "epoch 671, loss 1.1479237079620361, R2 0.42685309052467346\n",
      "Eval loss 1.206478476524353, R2 0.42525041103363037\n",
      "epoch 672, loss 1.1468851566314697, R2 0.42752739787101746\n",
      "Eval loss 1.2054005861282349, R2 0.4259204864501953\n",
      "epoch 673, loss 1.1458499431610107, R2 0.4281996786594391\n",
      "Eval loss 1.2043261528015137, R2 0.4265887141227722\n",
      "epoch 674, loss 1.1448183059692383, R2 0.42886972427368164\n",
      "Eval loss 1.203255534172058, R2 0.427254855632782\n",
      "epoch 675, loss 1.143790364265442, R2 0.4295377731323242\n",
      "Eval loss 1.20218825340271, R2 0.42791908979415894\n",
      "epoch 676, loss 1.142765998840332, R2 0.43020373582839966\n",
      "Eval loss 1.2011244297027588, R2 0.4285811483860016\n",
      "epoch 677, loss 1.1417447328567505, R2 0.43086758255958557\n",
      "Eval loss 1.2000640630722046, R2 0.4292413592338562\n",
      "epoch 678, loss 1.1407270431518555, R2 0.4315294027328491\n",
      "Eval loss 1.199007511138916, R2 0.4298994243144989\n",
      "epoch 679, loss 1.139712929725647, R2 0.4321891963481903\n",
      "Eval loss 1.1979540586471558, R2 0.43055570125579834\n",
      "epoch 680, loss 1.1387022733688354, R2 0.4328468441963196\n",
      "Eval loss 1.1969043016433716, R2 0.43120983242988586\n",
      "epoch 681, loss 1.1376949548721313, R2 0.4335027039051056\n",
      "Eval loss 1.1958578824996948, R2 0.43186214566230774\n",
      "epoch 682, loss 1.1366912126541138, R2 0.4341561496257782\n",
      "Eval loss 1.194814920425415, R2 0.43251246213912964\n",
      "epoch 683, loss 1.135690689086914, R2 0.43480774760246277\n",
      "Eval loss 1.1937754154205322, R2 0.43316081166267395\n",
      "epoch 684, loss 1.1346936225891113, R2 0.435457319021225\n",
      "Eval loss 1.1927391290664673, R2 0.4338071942329407\n",
      "epoch 685, loss 1.133699893951416, R2 0.4361048638820648\n",
      "Eval loss 1.1917065382003784, R2 0.4344516694545746\n",
      "epoch 686, loss 1.1327097415924072, R2 0.4367503225803375\n",
      "Eval loss 1.1906771659851074, R2 0.4350942373275757\n",
      "epoch 687, loss 1.1317226886749268, R2 0.4373938739299774\n",
      "Eval loss 1.1896512508392334, R2 0.4357347786426544\n",
      "epoch 688, loss 1.1307389736175537, R2 0.4380354583263397\n",
      "Eval loss 1.1886286735534668, R2 0.4363735318183899\n",
      "epoch 689, loss 1.1297587156295776, R2 0.43867504596710205\n",
      "Eval loss 1.1876094341278076, R2 0.4370101988315582\n",
      "epoch 690, loss 1.1287819147109985, R2 0.4393126368522644\n",
      "Eval loss 1.1865935325622559, R2 0.4376452565193176\n",
      "epoch 691, loss 1.1278082132339478, R2 0.439948171377182\n",
      "Eval loss 1.1855809688568115, R2 0.43827810883522034\n",
      "epoch 692, loss 1.1268376111984253, R2 0.4405817985534668\n",
      "Eval loss 1.1845715045928955, R2 0.43890926241874695\n",
      "epoch 693, loss 1.125870704650879, R2 0.4412135183811188\n",
      "Eval loss 1.1835657358169556, R2 0.4395383596420288\n",
      "epoch 694, loss 1.1249067783355713, R2 0.4418431222438812\n",
      "Eval loss 1.1825629472732544, R2 0.4401657283306122\n",
      "epoch 695, loss 1.1239460706710815, R2 0.4424710273742676\n",
      "Eval loss 1.181563377380371, R2 0.4407912492752075\n",
      "epoch 696, loss 1.1229888200759888, R2 0.4430968463420868\n",
      "Eval loss 1.1805672645568848, R2 0.441415011882782\n",
      "epoch 697, loss 1.1220346689224243, R2 0.4437207281589508\n",
      "Eval loss 1.1795743703842163, R2 0.44203662872314453\n",
      "epoch 698, loss 1.1210837364196777, R2 0.444342702627182\n",
      "Eval loss 1.1785848140716553, R2 0.442656546831131\n",
      "epoch 699, loss 1.120136022567749, R2 0.4449627697467804\n",
      "Eval loss 1.1775983572006226, R2 0.443274587392807\n",
      "epoch 700, loss 1.1191915273666382, R2 0.44558092951774597\n",
      "Eval loss 1.1766149997711182, R2 0.44389083981513977\n",
      "epoch 701, loss 1.1182503700256348, R2 0.44619715213775635\n",
      "Eval loss 1.1756350994110107, R2 0.44450509548187256\n",
      "epoch 702, loss 1.1173120737075806, R2 0.44681164622306824\n",
      "Eval loss 1.1746582984924316, R2 0.44511768221855164\n",
      "epoch 703, loss 1.1163771152496338, R2 0.4474240243434906\n",
      "Eval loss 1.1736847162246704, R2 0.44572848081588745\n",
      "epoch 704, loss 1.1154451370239258, R2 0.4480346739292145\n",
      "Eval loss 1.172714114189148, R2 0.4463373124599457\n",
      "epoch 705, loss 1.1145163774490356, R2 0.4486433267593384\n",
      "Eval loss 1.171746850013733, R2 0.44694459438323975\n",
      "epoch 706, loss 1.113590955734253, R2 0.449250191450119\n",
      "Eval loss 1.1707826852798462, R2 0.44754981994628906\n",
      "epoch 707, loss 1.1126682758331299, R2 0.4498552978038788\n",
      "Eval loss 1.1698215007781982, R2 0.44815337657928467\n",
      "epoch 708, loss 1.1117489337921143, R2 0.4504584074020386\n",
      "Eval loss 1.1688636541366577, R2 0.44875508546829224\n",
      "epoch 709, loss 1.110832691192627, R2 0.4510595202445984\n",
      "Eval loss 1.1679089069366455, R2 0.44935494661331177\n",
      "epoch 710, loss 1.1099194288253784, R2 0.45165884494781494\n",
      "Eval loss 1.1669572591781616, R2 0.4499532878398895\n",
      "epoch 711, loss 1.1090092658996582, R2 0.45225656032562256\n",
      "Eval loss 1.166008472442627, R2 0.4505496025085449\n",
      "epoch 712, loss 1.1081022024154663, R2 0.4528522491455078\n",
      "Eval loss 1.1650630235671997, R2 0.4511442184448242\n",
      "epoch 713, loss 1.1071981191635132, R2 0.45344626903533936\n",
      "Eval loss 1.1641205549240112, R2 0.4517371356487274\n",
      "epoch 714, loss 1.1062970161437988, R2 0.4540383219718933\n",
      "Eval loss 1.1631810665130615, R2 0.4523281753063202\n",
      "epoch 715, loss 1.1053991317749023, R2 0.4546285569667816\n",
      "Eval loss 1.1622446775436401, R2 0.45291760563850403\n",
      "epoch 716, loss 1.1045039892196655, R2 0.4552170932292938\n",
      "Eval loss 1.161311388015747, R2 0.4535053074359894\n",
      "epoch 717, loss 1.103611946105957, R2 0.455803781747818\n",
      "Eval loss 1.1603810787200928, R2 0.45409107208251953\n",
      "epoch 718, loss 1.1027228832244873, R2 0.4563887417316437\n",
      "Eval loss 1.1594538688659668, R2 0.4546753168106079\n",
      "epoch 719, loss 1.1018368005752563, R2 0.4569718539714813\n",
      "Eval loss 1.15852952003479, R2 0.455257773399353\n",
      "epoch 720, loss 1.1009538173675537, R2 0.45755326747894287\n",
      "Eval loss 1.157608151435852, R2 0.4558386504650116\n",
      "epoch 721, loss 1.1000736951828003, R2 0.45813265442848206\n",
      "Eval loss 1.1566897630691528, R2 0.4564177691936493\n",
      "epoch 722, loss 1.099196434020996, R2 0.45871058106422424\n",
      "Eval loss 1.1557743549346924, R2 0.45699501037597656\n",
      "epoch 723, loss 1.0983220338821411, R2 0.4592866003513336\n",
      "Eval loss 1.1548619270324707, R2 0.45757052302360535\n",
      "epoch 724, loss 1.0974507331848145, R2 0.45986106991767883\n",
      "Eval loss 1.1539523601531982, R2 0.45814448595046997\n",
      "epoch 725, loss 1.0965824127197266, R2 0.4604334533214569\n",
      "Eval loss 1.153045892715454, R2 0.45871686935424805\n",
      "epoch 726, loss 1.0957168340682983, R2 0.4610041677951813\n",
      "Eval loss 1.1521421670913696, R2 0.45928728580474854\n",
      "epoch 727, loss 1.0948539972305298, R2 0.4615732431411743\n",
      "Eval loss 1.151241660118103, R2 0.4598563015460968\n",
      "epoch 728, loss 1.0939942598342896, R2 0.46214058995246887\n",
      "Eval loss 1.1503437757492065, R2 0.46042346954345703\n",
      "epoch 729, loss 1.0931373834609985, R2 0.46270620822906494\n",
      "Eval loss 1.1494488716125488, R2 0.4609890878200531\n",
      "epoch 730, loss 1.0922832489013672, R2 0.463269978761673\n",
      "Eval loss 1.1485567092895508, R2 0.46155300736427307\n",
      "epoch 731, loss 1.0914320945739746, R2 0.46383219957351685\n",
      "Eval loss 1.147667646408081, R2 0.4621152877807617\n",
      "epoch 732, loss 1.0905836820602417, R2 0.46439263224601746\n",
      "Eval loss 1.1467812061309814, R2 0.46267586946487427\n",
      "epoch 733, loss 1.089738130569458, R2 0.4649513363838196\n",
      "Eval loss 1.1458978652954102, R2 0.46323487162590027\n",
      "epoch 734, loss 1.0888952016830444, R2 0.46550846099853516\n",
      "Eval loss 1.1450172662734985, R2 0.4637921452522278\n",
      "epoch 735, loss 1.0880552530288696, R2 0.46606385707855225\n",
      "Eval loss 1.1441394090652466, R2 0.4643479287624359\n",
      "epoch 736, loss 1.087218165397644, R2 0.46661749482154846\n",
      "Eval loss 1.1432644128799438, R2 0.46490204334259033\n",
      "epoch 737, loss 1.0863838195800781, R2 0.46716955304145813\n",
      "Eval loss 1.1423922777175903, R2 0.46545445919036865\n",
      "epoch 738, loss 1.0855520963668823, R2 0.4677198529243469\n",
      "Eval loss 1.141522765159607, R2 0.4660053551197052\n",
      "epoch 739, loss 1.0847232341766357, R2 0.46826857328414917\n",
      "Eval loss 1.1406563520431519, R2 0.4665546417236328\n",
      "epoch 740, loss 1.0838972330093384, R2 0.46881571412086487\n",
      "Eval loss 1.1397924423217773, R2 0.46710216999053955\n",
      "epoch 741, loss 1.0830737352371216, R2 0.46936094760894775\n",
      "Eval loss 1.1389315128326416, R2 0.4676482379436493\n",
      "epoch 742, loss 1.0822532176971436, R2 0.4699047803878784\n",
      "Eval loss 1.138073205947876, R2 0.4681926965713501\n",
      "epoch 743, loss 1.0814353227615356, R2 0.4704468548297882\n",
      "Eval loss 1.1372177600860596, R2 0.4687355160713196\n",
      "epoch 744, loss 1.0806200504302979, R2 0.4709872305393219\n",
      "Eval loss 1.1363649368286133, R2 0.46927690505981445\n",
      "epoch 745, loss 1.0798075199127197, R2 0.47152596712112427\n",
      "Eval loss 1.1355148553848267, R2 0.469816654920578\n",
      "epoch 746, loss 1.0789977312088013, R2 0.47206324338912964\n",
      "Eval loss 1.1346676349639893, R2 0.47035473585128784\n",
      "epoch 747, loss 1.078190565109253, R2 0.4725988209247589\n",
      "Eval loss 1.1338229179382324, R2 0.47089147567749023\n",
      "epoch 748, loss 1.0773860216140747, R2 0.4731327295303345\n",
      "Eval loss 1.1329809427261353, R2 0.47142645716667175\n",
      "epoch 749, loss 1.0765842199325562, R2 0.4736650586128235\n",
      "Eval loss 1.1321418285369873, R2 0.4719597101211548\n",
      "epoch 750, loss 1.0757849216461182, R2 0.4741959273815155\n",
      "Eval loss 1.13130521774292, R2 0.4724917709827423\n",
      "epoch 751, loss 1.074988603591919, R2 0.47472503781318665\n",
      "Eval loss 1.1304712295532227, R2 0.47302210330963135\n",
      "epoch 752, loss 1.0741946697235107, R2 0.47525253891944885\n",
      "Eval loss 1.129639983177185, R2 0.4735510051250458\n",
      "epoch 753, loss 1.0734033584594727, R2 0.4757785499095917\n",
      "Eval loss 1.1288114786148071, R2 0.47407835721969604\n",
      "epoch 754, loss 1.0726147890090942, R2 0.4763028919696808\n",
      "Eval loss 1.1279854774475098, R2 0.4746040999889374\n",
      "epoch 755, loss 1.0718286037445068, R2 0.4768257141113281\n",
      "Eval loss 1.127162218093872, R2 0.4751284420490265\n",
      "epoch 756, loss 1.071045160293579, R2 0.4773469865322113\n",
      "Eval loss 1.1263415813446045, R2 0.47565117478370667\n",
      "epoch 757, loss 1.0702643394470215, R2 0.4778668284416199\n",
      "Eval loss 1.1255234479904175, R2 0.47617244720458984\n",
      "epoch 758, loss 1.0694860219955444, R2 0.47838476300239563\n",
      "Eval loss 1.1247080564498901, R2 0.4766920804977417\n",
      "epoch 759, loss 1.068710446357727, R2 0.4789014160633087\n",
      "Eval loss 1.1238950490951538, R2 0.4772103428840637\n",
      "epoch 760, loss 1.0679371356964111, R2 0.4794164001941681\n",
      "Eval loss 1.1230849027633667, R2 0.4777270555496216\n",
      "epoch 761, loss 1.0671665668487549, R2 0.4799298942089081\n",
      "Eval loss 1.122277021408081, R2 0.4782422184944153\n",
      "epoch 762, loss 1.0663983821868896, R2 0.4804418385028839\n",
      "Eval loss 1.1214720010757446, R2 0.47875598073005676\n",
      "epoch 763, loss 1.0656328201293945, R2 0.4809522330760956\n",
      "Eval loss 1.1206693649291992, R2 0.479268342256546\n",
      "epoch 764, loss 1.0648696422576904, R2 0.48146113753318787\n",
      "Eval loss 1.119869351387024, R2 0.47977903485298157\n",
      "epoch 765, loss 1.064109206199646, R2 0.481968492269516\n",
      "Eval loss 1.1190717220306396, R2 0.48028838634490967\n",
      "epoch 766, loss 1.0633511543273926, R2 0.48247432708740234\n",
      "Eval loss 1.118276834487915, R2 0.48079612851142883\n",
      "epoch 767, loss 1.0625954866409302, R2 0.4829786419868469\n",
      "Eval loss 1.1174843311309814, R2 0.48130255937576294\n",
      "epoch 768, loss 1.0618425607681274, R2 0.48348161578178406\n",
      "Eval loss 1.1166945695877075, R2 0.48180752992630005\n",
      "epoch 769, loss 1.0610917806625366, R2 0.4839828610420227\n",
      "Eval loss 1.1159069538116455, R2 0.4823109805583954\n",
      "epoch 770, loss 1.060343623161316, R2 0.4844827353954315\n",
      "Eval loss 1.1151219606399536, R2 0.4828130900859833\n",
      "epoch 771, loss 1.0595979690551758, R2 0.48498108983039856\n",
      "Eval loss 1.1143395900726318, R2 0.483313649892807\n",
      "epoch 772, loss 1.058854579925537, R2 0.485478013753891\n",
      "Eval loss 1.1135594844818115, R2 0.4838128387928009\n",
      "epoch 773, loss 1.058113932609558, R2 0.4859733283519745\n",
      "Eval loss 1.1127820014953613, R2 0.4843106269836426\n",
      "epoch 774, loss 1.0573755502700806, R2 0.486467182636261\n",
      "Eval loss 1.1120070219039917, R2 0.48480671644210815\n",
      "epoch 775, loss 1.0566394329071045, R2 0.4869597256183624\n",
      "Eval loss 1.111234426498413, R2 0.48530179262161255\n",
      "epoch 776, loss 1.0559059381484985, R2 0.4874506890773773\n",
      "Eval loss 1.1104642152786255, R2 0.4857950508594513\n",
      "epoch 777, loss 1.055174708366394, R2 0.48794031143188477\n",
      "Eval loss 1.109696388244629, R2 0.48628711700439453\n",
      "epoch 778, loss 1.0544459819793701, R2 0.4884282946586609\n",
      "Eval loss 1.108931064605713, R2 0.48677775263786316\n",
      "epoch 779, loss 1.0537196397781372, R2 0.48891496658325195\n",
      "Eval loss 1.108168125152588, R2 0.48726704716682434\n",
      "epoch 780, loss 1.0529955625534058, R2 0.4894000291824341\n",
      "Eval loss 1.1074076890945435, R2 0.4877547323703766\n",
      "epoch 781, loss 1.0522737503051758, R2 0.4898839294910431\n",
      "Eval loss 1.10664963722229, R2 0.4882413148880005\n",
      "epoch 782, loss 1.051554560661316, R2 0.4903661012649536\n",
      "Eval loss 1.1058939695358276, R2 0.4887262284755707\n",
      "epoch 783, loss 1.0508376359939575, R2 0.4908469319343567\n",
      "Eval loss 1.1051405668258667, R2 0.4892098605632782\n",
      "epoch 784, loss 1.0501229763031006, R2 0.4913264214992523\n",
      "Eval loss 1.1043896675109863, R2 0.48969218134880066\n",
      "epoch 785, loss 1.0494107007980347, R2 0.49180442094802856\n",
      "Eval loss 1.1036410331726074, R2 0.4901730716228485\n",
      "epoch 786, loss 1.0487008094787598, R2 0.492281049489975\n",
      "Eval loss 1.10289466381073, R2 0.4906526505947113\n",
      "epoch 787, loss 1.0479931831359863, R2 0.4927562475204468\n",
      "Eval loss 1.102150797843933, R2 0.4911308288574219\n",
      "epoch 788, loss 1.0472878217697144, R2 0.4932299852371216\n",
      "Eval loss 1.1014093160629272, R2 0.4916076362133026\n",
      "epoch 789, loss 1.0465847253799438, R2 0.49370238184928894\n",
      "Eval loss 1.1006700992584229, R2 0.49208298325538635\n",
      "epoch 790, loss 1.0458838939666748, R2 0.49417343735694885\n",
      "Eval loss 1.0999330282211304, R2 0.4925571382045746\n",
      "epoch 791, loss 1.0451854467391968, R2 0.49464303255081177\n",
      "Eval loss 1.0991984605789185, R2 0.49302998185157776\n",
      "epoch 792, loss 1.0444891452789307, R2 0.4951113760471344\n",
      "Eval loss 1.0984662771224976, R2 0.49350130558013916\n",
      "epoch 793, loss 1.0437953472137451, R2 0.4955781102180481\n",
      "Eval loss 1.0977362394332886, R2 0.4939713478088379\n",
      "epoch 794, loss 1.043103575706482, R2 0.4960436522960663\n",
      "Eval loss 1.097008466720581, R2 0.4944400489330292\n",
      "epoch 795, loss 1.0424141883850098, R2 0.4965077340602875\n",
      "Eval loss 1.0962830781936646, R2 0.4949073791503906\n",
      "epoch 796, loss 1.0417269468307495, R2 0.4969704747200012\n",
      "Eval loss 1.0955599546432495, R2 0.49537351727485657\n",
      "epoch 797, loss 1.0410419702529907, R2 0.49743175506591797\n",
      "Eval loss 1.0948389768600464, R2 0.49583834409713745\n",
      "epoch 798, loss 1.0403591394424438, R2 0.4978918135166168\n",
      "Eval loss 1.0941203832626343, R2 0.4963015615940094\n",
      "epoch 799, loss 1.0396788120269775, R2 0.498350590467453\n",
      "Eval loss 1.093403935432434, R2 0.4967637360095978\n",
      "epoch 800, loss 1.039000391960144, R2 0.49880772829055786\n",
      "Eval loss 1.0926897525787354, R2 0.49722450971603394\n",
      "epoch 801, loss 1.0383243560791016, R2 0.499263733625412\n",
      "Eval loss 1.091977834701538, R2 0.4976840317249298\n",
      "epoch 802, loss 1.0376503467559814, R2 0.4997182786464691\n",
      "Eval loss 1.0912681818008423, R2 0.4981422424316406\n",
      "epoch 803, loss 1.0369784832000732, R2 0.5001716613769531\n",
      "Eval loss 1.0905606746673584, R2 0.49859902262687683\n",
      "epoch 804, loss 1.036309003829956, R2 0.5006236433982849\n",
      "Eval loss 1.089855432510376, R2 0.49905461072921753\n",
      "epoch 805, loss 1.0356415510177612, R2 0.5010742545127869\n",
      "Eval loss 1.0891523361206055, R2 0.49950897693634033\n",
      "epoch 806, loss 1.0349763631820679, R2 0.5015235543251038\n",
      "Eval loss 1.0884515047073364, R2 0.49996206164360046\n",
      "epoch 807, loss 1.0343133211135864, R2 0.5019714832305908\n",
      "Eval loss 1.0877528190612793, R2 0.5004138350486755\n",
      "epoch 808, loss 1.0336523056030273, R2 0.5024181604385376\n",
      "Eval loss 1.087056279182434, R2 0.5008642673492432\n",
      "epoch 809, loss 1.0329934358596802, R2 0.5028635263442993\n",
      "Eval loss 1.0863618850708008, R2 0.5013136267662048\n",
      "epoch 810, loss 1.0323368310928345, R2 0.503307580947876\n",
      "Eval loss 1.085669755935669, R2 0.5017614364624023\n",
      "epoch 811, loss 1.0316821336746216, R2 0.5037503838539124\n",
      "Eval loss 1.084979772567749, R2 0.5022081136703491\n",
      "epoch 812, loss 1.0310297012329102, R2 0.5041918754577637\n",
      "Eval loss 1.084291934967041, R2 0.5026535391807556\n",
      "epoch 813, loss 1.030379295349121, R2 0.5046321153640747\n",
      "Eval loss 1.0836061239242554, R2 0.5030977129936218\n",
      "epoch 814, loss 1.029731035232544, R2 0.5050710439682007\n",
      "Eval loss 1.0829224586486816, R2 0.5035406351089478\n",
      "epoch 815, loss 1.0290849208831787, R2 0.5055084824562073\n",
      "Eval loss 1.0822410583496094, R2 0.5039821863174438\n",
      "epoch 816, loss 1.0284408330917358, R2 0.5059449076652527\n",
      "Eval loss 1.0815616846084595, R2 0.5044227242469788\n",
      "epoch 817, loss 1.0277988910675049, R2 0.506380021572113\n",
      "Eval loss 1.0808844566345215, R2 0.5048618912696838\n",
      "epoch 818, loss 1.0271588563919067, R2 0.5068137049674988\n",
      "Eval loss 1.0802091360092163, R2 0.5052997469902039\n",
      "epoch 819, loss 1.02652108669281, R2 0.5072462558746338\n",
      "Eval loss 1.0795361995697021, R2 0.5057365298271179\n",
      "epoch 820, loss 1.0258852243423462, R2 0.5076776146888733\n",
      "Eval loss 1.0788652896881104, R2 0.5061720013618469\n",
      "epoch 821, loss 1.0252513885498047, R2 0.508107602596283\n",
      "Eval loss 1.078196406364441, R2 0.50660640001297\n",
      "epoch 822, loss 1.024619698524475, R2 0.5085363388061523\n",
      "Eval loss 1.0775294303894043, R2 0.5070393085479736\n",
      "epoch 823, loss 1.0239899158477783, R2 0.5089638829231262\n",
      "Eval loss 1.0768647193908691, R2 0.5074713230133057\n",
      "epoch 824, loss 1.023362159729004, R2 0.5093901753425598\n",
      "Eval loss 1.0762019157409668, R2 0.5079019069671631\n",
      "epoch 825, loss 1.0227365493774414, R2 0.5098152160644531\n",
      "Eval loss 1.075541377067566, R2 0.5083313584327698\n",
      "epoch 826, loss 1.0221129655838013, R2 0.5102391242980957\n",
      "Eval loss 1.0748828649520874, R2 0.5087594985961914\n",
      "epoch 827, loss 1.021491289138794, R2 0.5106616616249084\n",
      "Eval loss 1.0742262601852417, R2 0.5091866254806519\n",
      "epoch 828, loss 1.020871639251709, R2 0.5110828876495361\n",
      "Eval loss 1.0735716819763184, R2 0.5096124410629272\n",
      "epoch 829, loss 1.0202538967132568, R2 0.5115030407905579\n",
      "Eval loss 1.072919249534607, R2 0.5100370645523071\n",
      "epoch 830, loss 1.019638180732727, R2 0.5119219422340393\n",
      "Eval loss 1.0722686052322388, R2 0.5104605555534363\n",
      "epoch 831, loss 1.0190244913101196, R2 0.51233971118927\n",
      "Eval loss 1.071619987487793, R2 0.5108829140663147\n",
      "epoch 832, loss 1.018412709236145, R2 0.5127562880516052\n",
      "Eval loss 1.0709737539291382, R2 0.5113039612770081\n",
      "epoch 833, loss 1.0178029537200928, R2 0.5131714344024658\n",
      "Eval loss 1.070329189300537, R2 0.5117239356040955\n",
      "epoch 834, loss 1.0171951055526733, R2 0.5135855078697205\n",
      "Eval loss 1.0696866512298584, R2 0.5121427178382874\n",
      "epoch 835, loss 1.0165891647338867, R2 0.5139983892440796\n",
      "Eval loss 1.069046139717102, R2 0.5125603079795837\n",
      "epoch 836, loss 1.015985131263733, R2 0.5144099593162537\n",
      "Eval loss 1.0684075355529785, R2 0.5129766464233398\n",
      "epoch 837, loss 1.0153831243515015, R2 0.5148205161094666\n",
      "Eval loss 1.0677709579467773, R2 0.51339191198349\n",
      "epoch 838, loss 1.0147830247879028, R2 0.5152298212051392\n",
      "Eval loss 1.0671364068984985, R2 0.5138060450553894\n",
      "epoch 839, loss 1.014184832572937, R2 0.515637993812561\n",
      "Eval loss 1.066503643989563, R2 0.5142189264297485\n",
      "epoch 840, loss 1.013588547706604, R2 0.5160448551177979\n",
      "Eval loss 1.0658729076385498, R2 0.5146308541297913\n",
      "epoch 841, loss 1.0129941701889038, R2 0.5164506435394287\n",
      "Eval loss 1.0652440786361694, R2 0.5150415301322937\n",
      "epoch 842, loss 1.0124015808105469, R2 0.5168552994728088\n",
      "Eval loss 1.0646171569824219, R2 0.5154510140419006\n",
      "epoch 843, loss 1.0118111371994019, R2 0.5172587037086487\n",
      "Eval loss 1.0639921426773071, R2 0.5158594846725464\n",
      "epoch 844, loss 1.0112224817276, R2 0.5176609754562378\n",
      "Eval loss 1.0633692741394043, R2 0.5162666440010071\n",
      "epoch 845, loss 1.0106356143951416, R2 0.5180620551109314\n",
      "Eval loss 1.0627480745315552, R2 0.5166727900505066\n",
      "epoch 846, loss 1.010050654411316, R2 0.5184620022773743\n",
      "Eval loss 1.0621289014816284, R2 0.5170778632164001\n",
      "epoch 847, loss 1.0094674825668335, R2 0.5188608765602112\n",
      "Eval loss 1.0615116357803345, R2 0.5174817442893982\n",
      "epoch 848, loss 1.0088862180709839, R2 0.5192585587501526\n",
      "Eval loss 1.0608961582183838, R2 0.5178846120834351\n",
      "epoch 849, loss 1.008306860923767, R2 0.5196549892425537\n",
      "Eval loss 1.0602827072143555, R2 0.5182861685752869\n",
      "epoch 850, loss 1.007729172706604, R2 0.5200504064559937\n",
      "Eval loss 1.0596710443496704, R2 0.5186867713928223\n",
      "epoch 851, loss 1.0071536302566528, R2 0.5204446315765381\n",
      "Eval loss 1.0590612888336182, R2 0.5190861225128174\n",
      "epoch 852, loss 1.0065796375274658, R2 0.5208377838134766\n",
      "Eval loss 1.05845308303833, R2 0.5194845795631409\n",
      "epoch 853, loss 1.0060076713562012, R2 0.5212296843528748\n",
      "Eval loss 1.057847261428833, R2 0.5198817253112793\n",
      "epoch 854, loss 1.0054373741149902, R2 0.5216205716133118\n",
      "Eval loss 1.0572429895401, R2 0.5202779173851013\n",
      "epoch 855, loss 1.004868984222412, R2 0.522010326385498\n",
      "Eval loss 1.056640625, R2 0.5206729173660278\n",
      "epoch 856, loss 1.0043022632598877, R2 0.5223990082740784\n",
      "Eval loss 1.0560401678085327, R2 0.5210669040679932\n",
      "epoch 857, loss 1.003737449645996, R2 0.5227864980697632\n",
      "Eval loss 1.0554414987564087, R2 0.5214597582817078\n",
      "epoch 858, loss 1.0031744241714478, R2 0.5231727957725525\n",
      "Eval loss 1.054844617843628, R2 0.5218515396118164\n",
      "epoch 859, loss 1.0026133060455322, R2 0.5235581398010254\n",
      "Eval loss 1.0542495250701904, R2 0.5222423076629639\n",
      "epoch 860, loss 1.0020537376403809, R2 0.5239423513412476\n",
      "Eval loss 1.0536563396453857, R2 0.5226318836212158\n",
      "epoch 861, loss 1.0014959573745728, R2 0.5243254899978638\n",
      "Eval loss 1.0530649423599243, R2 0.5230205059051514\n",
      "epoch 862, loss 1.0009400844573975, R2 0.5247073769569397\n",
      "Eval loss 1.0524753332138062, R2 0.5234079360961914\n",
      "epoch 863, loss 1.0003858804702759, R2 0.525088369846344\n",
      "Eval loss 1.0518873929977417, R2 0.5237943530082703\n",
      "epoch 864, loss 0.9998334646224976, R2 0.5254682302474976\n",
      "Eval loss 1.0513014793395996, R2 0.5241796970367432\n",
      "epoch 865, loss 0.9992826581001282, R2 0.5258468985557556\n",
      "Eval loss 1.0507171154022217, R2 0.5245640873908997\n",
      "epoch 866, loss 0.9987337589263916, R2 0.5262245535850525\n",
      "Eval loss 1.0501346588134766, R2 0.5249472260475159\n",
      "epoch 867, loss 0.9981865882873535, R2 0.5266011357307434\n",
      "Eval loss 1.0495539903640747, R2 0.5253294706344604\n",
      "epoch 868, loss 0.9976411461830139, R2 0.5269766449928284\n",
      "Eval loss 1.0489751100540161, R2 0.5257106423377991\n",
      "epoch 869, loss 0.997097373008728, R2 0.5273510813713074\n",
      "Eval loss 1.0483977794647217, R2 0.5260908007621765\n",
      "epoch 870, loss 0.9965553879737854, R2 0.5277244448661804\n",
      "Eval loss 1.0478224754333496, R2 0.5264698266983032\n",
      "epoch 871, loss 0.9960150718688965, R2 0.5280967950820923\n",
      "Eval loss 1.0472487211227417, R2 0.5268479585647583\n",
      "epoch 872, loss 0.9954763650894165, R2 0.5284680724143982\n",
      "Eval loss 1.0466768741607666, R2 0.5272248983383179\n",
      "epoch 873, loss 0.9949394464492798, R2 0.5288382172584534\n",
      "Eval loss 1.0461065769195557, R2 0.5276007652282715\n",
      "epoch 874, loss 0.9944043159484863, R2 0.5292074680328369\n",
      "Eval loss 1.0455381870269775, R2 0.5279757976531982\n",
      "epoch 875, loss 0.993870735168457, R2 0.5295754671096802\n",
      "Eval loss 1.0449714660644531, R2 0.5283496379852295\n",
      "epoch 876, loss 0.9933387637138367, R2 0.5299424529075623\n",
      "Eval loss 1.0444064140319824, R2 0.5287226438522339\n",
      "epoch 877, loss 0.9928086996078491, R2 0.5303084850311279\n",
      "Eval loss 1.0438430309295654, R2 0.5290944576263428\n",
      "epoch 878, loss 0.9922801852226257, R2 0.5306734442710876\n",
      "Eval loss 1.0432814359664917, R2 0.5294652581214905\n",
      "epoch 879, loss 0.9917532205581665, R2 0.5310373902320862\n",
      "Eval loss 1.0427215099334717, R2 0.5298351049423218\n",
      "epoch 880, loss 0.9912281632423401, R2 0.5314003229141235\n",
      "Eval loss 1.0421632528305054, R2 0.5302039980888367\n",
      "epoch 881, loss 0.9907045364379883, R2 0.5317621827125549\n",
      "Eval loss 1.0416066646575928, R2 0.5305717587471008\n",
      "epoch 882, loss 0.9901826977729797, R2 0.5321230292320251\n",
      "Eval loss 1.0410518646240234, R2 0.5309385657310486\n",
      "epoch 883, loss 0.9896624684333801, R2 0.5324828624725342\n",
      "Eval loss 1.0404986143112183, R2 0.5313044786453247\n",
      "epoch 884, loss 0.9891437292098999, R2 0.5328417420387268\n",
      "Eval loss 1.0399471521377563, R2 0.5316692590713501\n",
      "epoch 885, loss 0.9886267781257629, R2 0.5331994891166687\n",
      "Eval loss 1.0393973588943481, R2 0.5320331454277039\n",
      "epoch 886, loss 0.9881113171577454, R2 0.5335562825202942\n",
      "Eval loss 1.038849115371704, R2 0.5323959589004517\n",
      "epoch 887, loss 0.987597644329071, R2 0.5339120030403137\n",
      "Eval loss 1.0383025407791138, R2 0.5327578783035278\n",
      "epoch 888, loss 0.9870855212211609, R2 0.5342668890953064\n",
      "Eval loss 1.0377577543258667, R2 0.5331187844276428\n",
      "epoch 889, loss 0.9865749478340149, R2 0.5346206426620483\n",
      "Eval loss 1.0372145175933838, R2 0.5334786176681519\n",
      "epoch 890, loss 0.9860660433769226, R2 0.5349735021591187\n",
      "Eval loss 1.0366727113723755, R2 0.5338375568389893\n",
      "epoch 891, loss 0.9855586886405945, R2 0.5353251099586487\n",
      "Eval loss 1.0361328125, R2 0.5341954827308655\n",
      "epoch 892, loss 0.9850528836250305, R2 0.5356759428977966\n",
      "Eval loss 1.0355943441390991, R2 0.5345525145530701\n",
      "epoch 893, loss 0.9845486283302307, R2 0.5360257625579834\n",
      "Eval loss 1.0350576639175415, R2 0.5349085330963135\n",
      "epoch 894, loss 0.9840461611747742, R2 0.5363746285438538\n",
      "Eval loss 1.034522533416748, R2 0.5352634787559509\n",
      "epoch 895, loss 0.9835451245307922, R2 0.5367224216461182\n",
      "Eval loss 1.0339889526367188, R2 0.5356175899505615\n",
      "epoch 896, loss 0.9830456376075745, R2 0.5370692014694214\n",
      "Eval loss 1.0334571599960327, R2 0.5359707474708557\n",
      "epoch 897, loss 0.9825477600097656, R2 0.537415087223053\n",
      "Eval loss 1.0329267978668213, R2 0.5363228917121887\n",
      "epoch 898, loss 0.9820513129234314, R2 0.5377599596977234\n",
      "Eval loss 1.0323981046676636, R2 0.5366741418838501\n",
      "epoch 899, loss 0.9815565347671509, R2 0.5381039977073669\n",
      "Eval loss 1.03187096118927, R2 0.5370244383811951\n",
      "epoch 900, loss 0.981063187122345, R2 0.538446843624115\n",
      "Eval loss 1.0313453674316406, R2 0.5373738408088684\n",
      "epoch 901, loss 0.9805715680122375, R2 0.5387888550758362\n",
      "Eval loss 1.0308213233947754, R2 0.5377222299575806\n",
      "epoch 902, loss 0.9800814390182495, R2 0.5391299724578857\n",
      "Eval loss 1.0302989482879639, R2 0.5380696058273315\n",
      "epoch 903, loss 0.9795928597450256, R2 0.5394700169563293\n",
      "Eval loss 1.029778003692627, R2 0.5384161472320557\n",
      "epoch 904, loss 0.9791056513786316, R2 0.5398092269897461\n",
      "Eval loss 1.0292588472366333, R2 0.5387617349624634\n",
      "epoch 905, loss 0.9786200523376465, R2 0.5401473641395569\n",
      "Eval loss 1.0287410020828247, R2 0.5391063094139099\n",
      "epoch 906, loss 0.9781360030174255, R2 0.540484607219696\n",
      "Eval loss 1.0282248258590698, R2 0.5394500494003296\n",
      "epoch 907, loss 0.9776532053947449, R2 0.5408208966255188\n",
      "Eval loss 1.027710199356079, R2 0.5397928357124329\n",
      "epoch 908, loss 0.9771722555160522, R2 0.5411563515663147\n",
      "Eval loss 1.0271971225738525, R2 0.5401347279548645\n",
      "epoch 909, loss 0.9766925573348999, R2 0.5414907336235046\n",
      "Eval loss 1.0266855955123901, R2 0.540475606918335\n",
      "epoch 910, loss 0.9762144684791565, R2 0.5418241620063782\n",
      "Eval loss 1.0261753797531128, R2 0.5408157110214233\n",
      "epoch 911, loss 0.9757379293441772, R2 0.5421566367149353\n",
      "Eval loss 1.0256669521331787, R2 0.5411548018455505\n",
      "epoch 912, loss 0.9752627611160278, R2 0.5424885153770447\n",
      "Eval loss 1.0251598358154297, R2 0.5414929986000061\n",
      "epoch 913, loss 0.9747891426086426, R2 0.5428190231323242\n",
      "Eval loss 1.0246543884277344, R2 0.54183030128479\n",
      "epoch 914, loss 0.9743169546127319, R2 0.5431488156318665\n",
      "Eval loss 1.0241504907608032, R2 0.5421666502952576\n",
      "epoch 915, loss 0.9738460779190063, R2 0.5434776544570923\n",
      "Eval loss 1.0236479043960571, R2 0.542502224445343\n",
      "epoch 916, loss 0.9733769297599792, R2 0.5438056588172913\n",
      "Eval loss 1.0231469869613647, R2 0.5428367257118225\n",
      "epoch 917, loss 0.972909152507782, R2 0.5441325902938843\n",
      "Eval loss 1.0226473808288574, R2 0.5431704521179199\n",
      "epoch 918, loss 0.9724427461624146, R2 0.5444587469100952\n",
      "Eval loss 1.0221493244171143, R2 0.5435032844543457\n",
      "epoch 919, loss 0.9719778299331665, R2 0.5447839498519897\n",
      "Eval loss 1.0216528177261353, R2 0.5438352227210999\n",
      "epoch 920, loss 0.9715144038200378, R2 0.5451081991195679\n",
      "Eval loss 1.0211578607559204, R2 0.5441661477088928\n",
      "epoch 921, loss 0.9710524082183838, R2 0.5454315543174744\n",
      "Eval loss 1.020664095878601, R2 0.5444962978363037\n",
      "epoch 922, loss 0.9705917239189148, R2 0.5457541346549988\n",
      "Eval loss 1.0201719999313354, R2 0.544825553894043\n",
      "epoch 923, loss 0.9701325297355652, R2 0.5460757613182068\n",
      "Eval loss 1.019681453704834, R2 0.5451539158821106\n",
      "epoch 924, loss 0.969674825668335, R2 0.5463964343070984\n",
      "Eval loss 1.0191922187805176, R2 0.5454813241958618\n",
      "epoch 925, loss 0.9692184925079346, R2 0.5467162728309631\n",
      "Eval loss 1.0187044143676758, R2 0.5458080172538757\n",
      "epoch 926, loss 0.9687634706497192, R2 0.547035276889801\n",
      "Eval loss 1.0182181596755981, R2 0.5461337566375732\n",
      "epoch 927, loss 0.9683099985122681, R2 0.547353208065033\n",
      "Eval loss 1.0177333354949951, R2 0.5464586615562439\n",
      "epoch 928, loss 0.9678577780723572, R2 0.5476703643798828\n",
      "Eval loss 1.0172498226165771, R2 0.5467827320098877\n",
      "epoch 929, loss 0.9674071669578552, R2 0.5479865670204163\n",
      "Eval loss 1.0167678594589233, R2 0.5471057295799255\n",
      "epoch 930, loss 0.9669577479362488, R2 0.5483020544052124\n",
      "Eval loss 1.0162873268127441, R2 0.5474280714988708\n",
      "epoch 931, loss 0.9665098786354065, R2 0.5486165881156921\n",
      "Eval loss 1.01580810546875, R2 0.5477495789527893\n",
      "epoch 932, loss 0.9660632014274597, R2 0.5489302277565002\n",
      "Eval loss 1.01533043384552, R2 0.5480700731277466\n",
      "epoch 933, loss 0.9656180143356323, R2 0.5492430329322815\n",
      "Eval loss 1.014854073524475, R2 0.5483898520469666\n",
      "epoch 934, loss 0.9651742577552795, R2 0.5495550036430359\n",
      "Eval loss 1.0143790245056152, R2 0.5487086772918701\n",
      "epoch 935, loss 0.9647316932678223, R2 0.5498660206794739\n",
      "Eval loss 1.0139057636260986, R2 0.5490267276763916\n",
      "epoch 936, loss 0.9642906785011292, R2 0.550176203250885\n",
      "Eval loss 1.0134334564208984, R2 0.549344003200531\n",
      "epoch 937, loss 0.9638509154319763, R2 0.5504855513572693\n",
      "Eval loss 1.012962818145752, R2 0.549660325050354\n",
      "epoch 938, loss 0.9634124636650085, R2 0.5507940649986267\n",
      "Eval loss 1.012493371963501, R2 0.5499756932258606\n",
      "epoch 939, loss 0.9629755020141602, R2 0.551101803779602\n",
      "Eval loss 1.0120255947113037, R2 0.5502904057502747\n",
      "epoch 940, loss 0.962539792060852, R2 0.5514085292816162\n",
      "Eval loss 1.0115588903427124, R2 0.5506042838096619\n",
      "epoch 941, loss 0.9621054530143738, R2 0.5517144799232483\n",
      "Eval loss 1.0110937356948853, R2 0.5509172081947327\n",
      "epoch 942, loss 0.9616724252700806, R2 0.5520195364952087\n",
      "Eval loss 1.0106298923492432, R2 0.5512294173240662\n",
      "epoch 943, loss 0.9612406492233276, R2 0.5523238182067871\n",
      "Eval loss 1.0101674795150757, R2 0.5515407919883728\n",
      "epoch 944, loss 0.9608104825019836, R2 0.5526272654533386\n",
      "Eval loss 1.0097063779830933, R2 0.551851212978363\n",
      "epoch 945, loss 0.9603812098503113, R2 0.5529298782348633\n",
      "Eval loss 1.009246587753296, R2 0.5521608591079712\n",
      "epoch 946, loss 0.9599535465240479, R2 0.5532316565513611\n",
      "Eval loss 1.0087883472442627, R2 0.552469789981842\n",
      "epoch 947, loss 0.9595271944999695, R2 0.5535325407981873\n",
      "Eval loss 1.008331298828125, R2 0.552777886390686\n",
      "epoch 948, loss 0.9591020345687866, R2 0.5538327097892761\n",
      "Eval loss 1.0078755617141724, R2 0.5530850887298584\n",
      "epoch 949, loss 0.9586782455444336, R2 0.5541319847106934\n",
      "Eval loss 1.0074211359024048, R2 0.5533915758132935\n",
      "epoch 950, loss 0.9582557082176208, R2 0.554430365562439\n",
      "Eval loss 1.0069681406021118, R2 0.5536971688270569\n",
      "epoch 951, loss 0.9578345417976379, R2 0.554728090763092\n",
      "Eval loss 1.006516456604004, R2 0.5540019273757935\n",
      "epoch 952, loss 0.9574145674705505, R2 0.5550248622894287\n",
      "Eval loss 1.006066083908081, R2 0.5543059706687927\n",
      "epoch 953, loss 0.9569959044456482, R2 0.5553208589553833\n",
      "Eval loss 1.0056170225143433, R2 0.5546092391014099\n",
      "epoch 954, loss 0.9565785527229309, R2 0.555616021156311\n",
      "Eval loss 1.0051692724227905, R2 0.5549115538597107\n",
      "epoch 955, loss 0.9561624526977539, R2 0.5559104681015015\n",
      "Eval loss 1.0047228336334229, R2 0.5552133321762085\n",
      "epoch 956, loss 0.9557474851608276, R2 0.5562039613723755\n",
      "Eval loss 1.0042777061462402, R2 0.5555140972137451\n",
      "epoch 957, loss 0.9553340077400208, R2 0.556496798992157\n",
      "Eval loss 1.0038338899612427, R2 0.5558142066001892\n",
      "epoch 958, loss 0.9549217224121094, R2 0.5567888021469116\n",
      "Eval loss 1.0033913850784302, R2 0.5561134815216064\n",
      "epoch 959, loss 0.9545107483863831, R2 0.5570800304412842\n",
      "Eval loss 1.0029500722885132, R2 0.5564118027687073\n",
      "epoch 960, loss 0.9541009664535522, R2 0.5573703646659851\n",
      "Eval loss 1.0025103092193604, R2 0.5567096471786499\n",
      "epoch 961, loss 0.9536923766136169, R2 0.557659924030304\n",
      "Eval loss 1.002071499824524, R2 0.557006299495697\n",
      "epoch 962, loss 0.9532851576805115, R2 0.5579487085342407\n",
      "Eval loss 1.0016342401504517, R2 0.5573025345802307\n",
      "epoch 963, loss 0.9528790712356567, R2 0.5582367181777954\n",
      "Eval loss 1.001198172569275, R2 0.557597815990448\n",
      "epoch 964, loss 0.9524743556976318, R2 0.5585240125656128\n",
      "Eval loss 1.0007632970809937, R2 0.557892382144928\n",
      "epoch 965, loss 0.9520706534385681, R2 0.5588104128837585\n",
      "Eval loss 1.0003297328948975, R2 0.5581861734390259\n",
      "epoch 966, loss 0.9516683220863342, R2 0.559096097946167\n",
      "Eval loss 0.9998975396156311, R2 0.5584792494773865\n",
      "epoch 967, loss 0.9512673020362854, R2 0.5593810081481934\n",
      "Eval loss 0.9994665384292603, R2 0.5587714314460754\n",
      "epoch 968, loss 0.9508673548698425, R2 0.5596650242805481\n",
      "Eval loss 0.9990367293357849, R2 0.5590630173683167\n",
      "epoch 969, loss 0.9504687786102295, R2 0.5599483847618103\n",
      "Eval loss 0.9986082911491394, R2 0.5593536496162415\n",
      "epoch 970, loss 0.950071394443512, R2 0.5602309703826904\n",
      "Eval loss 0.9981809258460999, R2 0.5596436858177185\n",
      "epoch 971, loss 0.9496749043464661, R2 0.5605127811431885\n",
      "Eval loss 0.9977549910545349, R2 0.5599327683448792\n",
      "epoch 972, loss 0.9492798447608948, R2 0.5607937574386597\n",
      "Eval loss 0.9973300695419312, R2 0.5602213144302368\n",
      "epoch 973, loss 0.9488860368728638, R2 0.5610740184783936\n",
      "Eval loss 0.996906578540802, R2 0.5605089664459229\n",
      "epoch 974, loss 0.9484933614730835, R2 0.5613535642623901\n",
      "Eval loss 0.9964842200279236, R2 0.5607959628105164\n",
      "epoch 975, loss 0.9481019377708435, R2 0.5616323351860046\n",
      "Eval loss 0.996063232421875, R2 0.561082124710083\n",
      "epoch 976, loss 0.9477115869522095, R2 0.5619102120399475\n",
      "Eval loss 0.9956433176994324, R2 0.5613676309585571\n",
      "epoch 977, loss 0.9473225474357605, R2 0.5621875524520874\n",
      "Eval loss 0.9952245950698853, R2 0.5616522431373596\n",
      "epoch 978, loss 0.9469345808029175, R2 0.5624639391899109\n",
      "Eval loss 0.9948071837425232, R2 0.5619363188743591\n",
      "epoch 979, loss 0.9465477466583252, R2 0.5627396702766418\n",
      "Eval loss 0.9943909049034119, R2 0.5622195601463318\n",
      "epoch 980, loss 0.946162223815918, R2 0.5630146861076355\n",
      "Eval loss 0.9939759373664856, R2 0.5625019669532776\n",
      "epoch 981, loss 0.9457778930664062, R2 0.5632889270782471\n",
      "Eval loss 0.9935621619224548, R2 0.5627837181091309\n",
      "epoch 982, loss 0.9453946352005005, R2 0.5635623931884766\n",
      "Eval loss 0.9931495189666748, R2 0.563064694404602\n",
      "epoch 983, loss 0.9450125098228455, R2 0.5638351440429688\n",
      "Eval loss 0.9927381277084351, R2 0.5633450150489807\n",
      "epoch 984, loss 0.9446315765380859, R2 0.5641071796417236\n",
      "Eval loss 0.992327868938446, R2 0.5636246204376221\n",
      "epoch 985, loss 0.9442517757415771, R2 0.5643784999847412\n",
      "Eval loss 0.991918683052063, R2 0.5639034509658813\n",
      "epoch 986, loss 0.9438731074333191, R2 0.5646489262580872\n",
      "Eval loss 0.991510808467865, R2 0.5641815662384033\n",
      "epoch 987, loss 0.9434956312179565, R2 0.5649188160896301\n",
      "Eval loss 0.9911041259765625, R2 0.5644590258598328\n",
      "epoch 988, loss 0.9431193470954895, R2 0.5651878714561462\n",
      "Eval loss 0.9906986951828003, R2 0.5647356510162354\n",
      "epoch 989, loss 0.9427440762519836, R2 0.5654562711715698\n",
      "Eval loss 0.9902943968772888, R2 0.5650115609169006\n",
      "epoch 990, loss 0.9423700571060181, R2 0.5657238364219666\n",
      "Eval loss 0.9898912310600281, R2 0.5652867555618286\n",
      "epoch 991, loss 0.9419969916343689, R2 0.5659907460212708\n",
      "Eval loss 0.9894891977310181, R2 0.5655613541603088\n",
      "epoch 992, loss 0.94162517786026, R2 0.5662569403648376\n",
      "Eval loss 0.9890883564949036, R2 0.565835177898407\n",
      "epoch 993, loss 0.9412544369697571, R2 0.566522479057312\n",
      "Eval loss 0.988688588142395, R2 0.566108226776123\n",
      "epoch 994, loss 0.9408848285675049, R2 0.5667871832847595\n",
      "Eval loss 0.9882901310920715, R2 0.5663806796073914\n",
      "epoch 995, loss 0.9405163526535034, R2 0.567051351070404\n",
      "Eval loss 0.987892746925354, R2 0.5666524171829224\n",
      "epoch 996, loss 0.9401490688323975, R2 0.567314624786377\n",
      "Eval loss 0.9874964356422424, R2 0.5669234395027161\n",
      "epoch 997, loss 0.9397827386856079, R2 0.5675772428512573\n",
      "Eval loss 0.9871013760566711, R2 0.5671936869621277\n",
      "epoch 998, loss 0.9394176006317139, R2 0.5678391456604004\n",
      "Eval loss 0.9867073893547058, R2 0.5674632787704468\n",
      "epoch 999, loss 0.9390534162521362, R2 0.5681004524230957\n",
      "Eval loss 0.9863144755363464, R2 0.5677320957183838\n",
      "epoch 1000, loss 0.9386905431747437, R2 0.5683609247207642\n",
      "Eval loss 0.9859227538108826, R2 0.5680003762245178\n",
      "epoch 1001, loss 0.9383285641670227, R2 0.5686207413673401\n",
      "Eval loss 0.9855321645736694, R2 0.5682678818702698\n",
      "epoch 1002, loss 0.937967836856842, R2 0.5688799023628235\n",
      "Eval loss 0.9851427674293518, R2 0.5685346722602844\n",
      "epoch 1003, loss 0.9376081228256226, R2 0.5691382884979248\n",
      "Eval loss 0.9847544431686401, R2 0.5688008666038513\n",
      "epoch 1004, loss 0.937249481678009, R2 0.5693961381912231\n",
      "Eval loss 0.9843671917915344, R2 0.5690662860870361\n",
      "epoch 1005, loss 0.9368918538093567, R2 0.5696530342102051\n",
      "Eval loss 0.9839810132980347, R2 0.5693311095237732\n",
      "epoch 1006, loss 0.9365355372428894, R2 0.5699095129966736\n",
      "Eval loss 0.9835959076881409, R2 0.569595217704773\n",
      "epoch 1007, loss 0.936180055141449, R2 0.5701650977134705\n",
      "Eval loss 0.9832121133804321, R2 0.5698585510253906\n",
      "epoch 1008, loss 0.9358256459236145, R2 0.5704200863838196\n",
      "Eval loss 0.982829213142395, R2 0.5701213479042053\n",
      "epoch 1009, loss 0.9354723691940308, R2 0.5706743597984314\n",
      "Eval loss 0.9824475049972534, R2 0.5703833699226379\n",
      "epoch 1010, loss 0.9351202249526978, R2 0.5709280371665955\n",
      "Eval loss 0.9820669293403625, R2 0.5706447958946228\n",
      "epoch 1011, loss 0.9347689151763916, R2 0.5711809396743774\n",
      "Eval loss 0.9816873073577881, R2 0.5709055066108704\n",
      "epoch 1012, loss 0.9344188570976257, R2 0.5714331865310669\n",
      "Eval loss 0.9813089370727539, R2 0.5711655020713806\n",
      "epoch 1013, loss 0.9340697526931763, R2 0.5716847777366638\n",
      "Eval loss 0.9809315800666809, R2 0.5714249610900879\n",
      "epoch 1014, loss 0.9337216019630432, R2 0.5719357132911682\n",
      "Eval loss 0.9805552959442139, R2 0.5716835856437683\n",
      "epoch 1015, loss 0.9333746433258057, R2 0.5721860527992249\n",
      "Eval loss 0.980180025100708, R2 0.5719416737556458\n",
      "epoch 1016, loss 0.9330287575721741, R2 0.5724356174468994\n",
      "Eval loss 0.9798059463500977, R2 0.5721990466117859\n",
      "epoch 1017, loss 0.9326837658882141, R2 0.5726845264434814\n",
      "Eval loss 0.9794327020645142, R2 0.5724558234214783\n",
      "epoch 1018, loss 0.9323398470878601, R2 0.572932779788971\n",
      "Eval loss 0.979060709476471, R2 0.5727118849754333\n",
      "epoch 1019, loss 0.9319970011711121, R2 0.5731803774833679\n",
      "Eval loss 0.9786899089813232, R2 0.5729672908782959\n",
      "epoch 1020, loss 0.9316551685333252, R2 0.5734272599220276\n",
      "Eval loss 0.9783199429512024, R2 0.5732219815254211\n",
      "epoch 1021, loss 0.9313142895698547, R2 0.573673665523529\n",
      "Eval loss 0.977951169013977, R2 0.5734761953353882\n",
      "epoch 1022, loss 0.930974543094635, R2 0.5739192366600037\n",
      "Eval loss 0.9775832891464233, R2 0.5737296342849731\n",
      "epoch 1023, loss 0.9306356310844421, R2 0.5741641521453857\n",
      "Eval loss 0.9772164225578308, R2 0.5739823579788208\n",
      "epoch 1024, loss 0.9302978515625, R2 0.5744085311889648\n",
      "Eval loss 0.9768508672714233, R2 0.5742345452308655\n",
      "epoch 1025, loss 0.9299610257148743, R2 0.5746521353721619\n",
      "Eval loss 0.9764861464500427, R2 0.5744861960411072\n",
      "epoch 1026, loss 0.9296252727508545, R2 0.5748953223228455\n",
      "Eval loss 0.9761224985122681, R2 0.574737012386322\n",
      "epoch 1027, loss 0.9292904734611511, R2 0.5751375555992126\n",
      "Eval loss 0.9757599234580994, R2 0.5749873518943787\n",
      "epoch 1028, loss 0.9289566874504089, R2 0.5753793120384216\n",
      "Eval loss 0.9753983616828918, R2 0.5752368569374084\n",
      "epoch 1029, loss 0.9286239147186279, R2 0.5756204128265381\n",
      "Eval loss 0.9750378131866455, R2 0.5754859447479248\n",
      "epoch 1030, loss 0.9282920956611633, R2 0.5758609175682068\n",
      "Eval loss 0.9746782779693604, R2 0.5757341980934143\n",
      "epoch 1031, loss 0.9279612898826599, R2 0.576100766658783\n",
      "Eval loss 0.9743198752403259, R2 0.5759819149971008\n",
      "epoch 1032, loss 0.9276313781738281, R2 0.576339840888977\n",
      "Eval loss 0.9739624261856079, R2 0.5762290358543396\n",
      "epoch 1033, loss 0.9273025393486023, R2 0.5765783786773682\n",
      "Eval loss 0.9736058115959167, R2 0.5764755010604858\n",
      "epoch 1034, loss 0.9269745945930481, R2 0.5768163800239563\n",
      "Eval loss 0.9732503294944763, R2 0.5767213106155396\n",
      "epoch 1035, loss 0.9266477227210999, R2 0.5770536065101624\n",
      "Eval loss 0.9728959798812866, R2 0.5769664645195007\n",
      "epoch 1036, loss 0.9263217449188232, R2 0.5772902965545654\n",
      "Eval loss 0.9725425839424133, R2 0.5772110819816589\n",
      "epoch 1037, loss 0.9259967803955078, R2 0.577526330947876\n",
      "Eval loss 0.9721900820732117, R2 0.5774551033973694\n",
      "epoch 1038, loss 0.9256727695465088, R2 0.577761709690094\n",
      "Eval loss 0.9718385934829712, R2 0.5776983499526978\n",
      "epoch 1039, loss 0.9253497123718262, R2 0.5779964923858643\n",
      "Eval loss 0.9714881777763367, R2 0.5779411196708679\n",
      "epoch 1040, loss 0.9250275492668152, R2 0.5782307386398315\n",
      "Eval loss 0.9711386561393738, R2 0.5781832337379456\n",
      "epoch 1041, loss 0.9247065186500549, R2 0.5784642696380615\n",
      "Eval loss 0.9707902073860168, R2 0.5784246921539307\n",
      "epoch 1042, loss 0.924386203289032, R2 0.578697144985199\n",
      "Eval loss 0.9704427123069763, R2 0.5786656141281128\n",
      "epoch 1043, loss 0.924066960811615, R2 0.5789294838905334\n",
      "Eval loss 0.9700961709022522, R2 0.5789058804512024\n",
      "epoch 1044, loss 0.9237485527992249, R2 0.5791612267494202\n",
      "Eval loss 0.969750702381134, R2 0.5791455507278442\n",
      "epoch 1045, loss 0.9234312772750854, R2 0.5793922543525696\n",
      "Eval loss 0.9694061875343323, R2 0.5793845653533936\n",
      "epoch 1046, loss 0.9231147766113281, R2 0.5796228051185608\n",
      "Eval loss 0.9690625071525574, R2 0.5796230435371399\n",
      "epoch 1047, loss 0.922799289226532, R2 0.5798527002334595\n",
      "Eval loss 0.968720018863678, R2 0.5798609256744385\n",
      "epoch 1048, loss 0.9224847555160522, R2 0.5800819993019104\n",
      "Eval loss 0.9683781862258911, R2 0.5800982117652893\n",
      "epoch 1049, loss 0.9221711754798889, R2 0.5803106427192688\n",
      "Eval loss 0.9680376648902893, R2 0.5803348422050476\n",
      "epoch 1050, loss 0.9218583703041077, R2 0.5805386900901794\n",
      "Eval loss 0.96769779920578, R2 0.5805708765983582\n",
      "epoch 1051, loss 0.9215466380119324, R2 0.5807662010192871\n",
      "Eval loss 0.9673590064048767, R2 0.5808063745498657\n",
      "epoch 1052, loss 0.9212357997894287, R2 0.5809930562973022\n",
      "Eval loss 0.967021107673645, R2 0.5810412764549255\n",
      "epoch 1053, loss 0.9209257960319519, R2 0.5812193751335144\n",
      "Eval loss 0.9666842818260193, R2 0.5812755227088928\n",
      "epoch 1054, loss 0.920616626739502, R2 0.581445038318634\n",
      "Eval loss 0.9663483500480652, R2 0.5815091133117676\n",
      "epoch 1055, loss 0.9203085899353027, R2 0.5816701650619507\n",
      "Eval loss 0.9660133123397827, R2 0.5817422866821289\n",
      "epoch 1056, loss 0.9200012683868408, R2 0.5818946361541748\n",
      "Eval loss 0.9656793475151062, R2 0.5819747447967529\n",
      "epoch 1057, loss 0.9196949601173401, R2 0.582118570804596\n",
      "Eval loss 0.9653462171554565, R2 0.582206666469574\n",
      "epoch 1058, loss 0.919389545917511, R2 0.5823419094085693\n",
      "Eval loss 0.9650140404701233, R2 0.5824381113052368\n",
      "epoch 1059, loss 0.9190850257873535, R2 0.5825647115707397\n",
      "Eval loss 0.9646828770637512, R2 0.5826688408851624\n",
      "epoch 1060, loss 0.9187813997268677, R2 0.5827867984771729\n",
      "Eval loss 0.9643524289131165, R2 0.5828990340232849\n",
      "epoch 1061, loss 0.9184786677360535, R2 0.5830084085464478\n",
      "Eval loss 0.9640230536460876, R2 0.5831286907196045\n",
      "epoch 1062, loss 0.9181767702102661, R2 0.5832294225692749\n",
      "Eval loss 0.9636946320533752, R2 0.5833576917648315\n",
      "epoch 1063, loss 0.9178758859634399, R2 0.5834499001502991\n",
      "Eval loss 0.9633669853210449, R2 0.5835860967636108\n",
      "epoch 1064, loss 0.9175758361816406, R2 0.5836697220802307\n",
      "Eval loss 0.9630404710769653, R2 0.5838139653205872\n",
      "epoch 1065, loss 0.9172765612602234, R2 0.5838889479637146\n",
      "Eval loss 0.962714672088623, R2 0.5840412974357605\n",
      "epoch 1066, loss 0.9169782400131226, R2 0.5841076374053955\n",
      "Eval loss 0.9623898863792419, R2 0.5842680335044861\n",
      "epoch 1067, loss 0.9166807532310486, R2 0.5843257904052734\n",
      "Eval loss 0.9620659351348877, R2 0.5844942331314087\n",
      "epoch 1068, loss 0.916384220123291, R2 0.5845434069633484\n",
      "Eval loss 0.9617429971694946, R2 0.5847198367118835\n",
      "epoch 1069, loss 0.9160884022712708, R2 0.5847603678703308\n",
      "Eval loss 0.9614208340644836, R2 0.5849449038505554\n",
      "epoch 1070, loss 0.9157935976982117, R2 0.584976851940155\n",
      "Eval loss 0.9610996246337891, R2 0.5851693153381348\n",
      "epoch 1071, loss 0.9154996275901794, R2 0.5851926207542419\n",
      "Eval loss 0.9607793092727661, R2 0.5853933691978455\n",
      "epoch 1072, loss 0.9152064919471741, R2 0.5854079723358154\n",
      "Eval loss 0.9604598879814148, R2 0.5856167078018188\n",
      "epoch 1073, loss 0.9149142503738403, R2 0.5856227278709412\n",
      "Eval loss 0.9601413607597351, R2 0.5858394503593445\n",
      "epoch 1074, loss 0.9146226644515991, R2 0.5858369469642639\n",
      "Eval loss 0.9598237872123718, R2 0.5860617160797119\n",
      "epoch 1075, loss 0.9143322110176086, R2 0.5860505700111389\n",
      "Eval loss 0.9595069289207458, R2 0.5862833857536316\n",
      "epoch 1076, loss 0.9140425324440002, R2 0.5862635970115662\n",
      "Eval loss 0.9591912031173706, R2 0.5865045189857483\n",
      "epoch 1077, loss 0.9137535691261292, R2 0.5864760875701904\n",
      "Eval loss 0.9588761329650879, R2 0.5867251753807068\n",
      "epoch 1078, loss 0.9134655594825745, R2 0.5866881012916565\n",
      "Eval loss 0.9585620760917664, R2 0.5869451761245728\n",
      "epoch 1079, loss 0.9131784439086914, R2 0.5868995189666748\n",
      "Eval loss 0.9582487344741821, R2 0.5871646404266357\n",
      "epoch 1080, loss 0.9128920435905457, R2 0.5871103405952454\n",
      "Eval loss 0.9579364061355591, R2 0.5873836278915405\n",
      "epoch 1081, loss 0.9126065969467163, R2 0.5873207449913025\n",
      "Eval loss 0.9576249122619629, R2 0.5876020193099976\n",
      "epoch 1082, loss 0.9123218059539795, R2 0.5875305533409119\n",
      "Eval loss 0.9573142528533936, R2 0.5878198146820068\n",
      "epoch 1083, loss 0.9120379686355591, R2 0.5877397060394287\n",
      "Eval loss 0.9570044279098511, R2 0.5880371928215027\n",
      "epoch 1084, loss 0.9117549657821655, R2 0.5879483819007874\n",
      "Eval loss 0.956695556640625, R2 0.588253915309906\n",
      "epoch 1085, loss 0.911472737789154, R2 0.5881566405296326\n",
      "Eval loss 0.9563874006271362, R2 0.5884702205657959\n",
      "epoch 1086, loss 0.9111914038658142, R2 0.5883641839027405\n",
      "Eval loss 0.9560802578926086, R2 0.588685929775238\n",
      "epoch 1087, loss 0.9109108448028564, R2 0.588571310043335\n",
      "Eval loss 0.9557738900184631, R2 0.588901162147522\n",
      "epoch 1088, loss 0.9106310606002808, R2 0.5887777209281921\n",
      "Eval loss 0.9554684162139893, R2 0.5891157388687134\n",
      "epoch 1089, loss 0.9103521108627319, R2 0.5889837741851807\n",
      "Eval loss 0.9551637172698975, R2 0.5893297791481018\n",
      "epoch 1090, loss 0.9100740551948547, R2 0.5891892910003662\n",
      "Eval loss 0.9548599123954773, R2 0.5895434021949768\n",
      "epoch 1091, loss 0.9097967147827148, R2 0.5893941521644592\n",
      "Eval loss 0.9545568227767944, R2 0.589756429195404\n",
      "epoch 1092, loss 0.909520149230957, R2 0.5895985960960388\n",
      "Eval loss 0.9542548060417175, R2 0.5899689197540283\n",
      "epoch 1093, loss 0.9092444777488708, R2 0.5898024439811707\n",
      "Eval loss 0.9539535045623779, R2 0.5901809334754944\n",
      "epoch 1094, loss 0.908969521522522, R2 0.5900059342384338\n",
      "Eval loss 0.9536529779434204, R2 0.5903924107551575\n",
      "epoch 1095, loss 0.9086955189704895, R2 0.5902087688446045\n",
      "Eval loss 0.9533533453941345, R2 0.5906032919883728\n",
      "epoch 1096, loss 0.90842205286026, R2 0.5904110074043274\n",
      "Eval loss 0.9530544877052307, R2 0.5908136963844299\n",
      "epoch 1097, loss 0.9081494808197021, R2 0.5906128883361816\n",
      "Eval loss 0.9527565836906433, R2 0.5910236239433289\n",
      "epoch 1098, loss 0.9078778624534607, R2 0.5908141136169434\n",
      "Eval loss 0.9524592757225037, R2 0.5912330150604248\n",
      "epoch 1099, loss 0.9076068997383118, R2 0.5910149216651917\n",
      "Eval loss 0.9521629810333252, R2 0.5914418697357178\n",
      "epoch 1100, loss 0.9073366522789001, R2 0.5912150740623474\n",
      "Eval loss 0.9518673419952393, R2 0.591650128364563\n",
      "epoch 1101, loss 0.9070672988891602, R2 0.5914148092269897\n",
      "Eval loss 0.9515727162361145, R2 0.5918580889701843\n",
      "epoch 1102, loss 0.9067986607551575, R2 0.5916140079498291\n",
      "Eval loss 0.9512787461280823, R2 0.5920653939247131\n",
      "epoch 1103, loss 0.9065309166908264, R2 0.5918126702308655\n",
      "Eval loss 0.9509856104850769, R2 0.592272162437439\n",
      "epoch 1104, loss 0.9062638282775879, R2 0.5920109152793884\n",
      "Eval loss 0.9506933689117432, R2 0.5924784541130066\n",
      "epoch 1105, loss 0.9059975147247314, R2 0.5922086238861084\n",
      "Eval loss 0.950401782989502, R2 0.592684268951416\n",
      "epoch 1106, loss 0.9057320952415466, R2 0.5924057960510254\n",
      "Eval loss 0.9501111507415771, R2 0.5928895473480225\n",
      "epoch 1107, loss 0.9054673314094543, R2 0.5926023721694946\n",
      "Eval loss 0.9498211741447449, R2 0.5930944085121155\n",
      "epoch 1108, loss 0.9052032828330994, R2 0.5927985310554504\n",
      "Eval loss 0.949532151222229, R2 0.593298614025116\n",
      "epoch 1109, loss 0.9049400687217712, R2 0.5929941534996033\n",
      "Eval loss 0.9492439031600952, R2 0.593502402305603\n",
      "epoch 1110, loss 0.9046775698661804, R2 0.5931893587112427\n",
      "Eval loss 0.948956310749054, R2 0.5937055945396423\n",
      "epoch 1111, loss 0.9044158458709717, R2 0.5933839678764343\n",
      "Eval loss 0.9486695528030396, R2 0.593908429145813\n",
      "epoch 1112, loss 0.9041550159454346, R2 0.5935781002044678\n",
      "Eval loss 0.948383629322052, R2 0.5941106677055359\n",
      "epoch 1113, loss 0.9038947224617004, R2 0.5937718749046326\n",
      "Eval loss 0.9480985403060913, R2 0.5943125486373901\n",
      "epoch 1114, loss 0.9036352038383484, R2 0.5939649939537048\n",
      "Eval loss 0.9478141069412231, R2 0.5945138335227966\n",
      "epoch 1115, loss 0.903376579284668, R2 0.5941576361656189\n",
      "Eval loss 0.9475305676460266, R2 0.5947145819664001\n",
      "epoch 1116, loss 0.9031186103820801, R2 0.5943498015403748\n",
      "Eval loss 0.9472476840019226, R2 0.5949148535728455\n",
      "epoch 1117, loss 0.9028613567352295, R2 0.5945414900779724\n",
      "Eval loss 0.9469655752182007, R2 0.5951146483421326\n",
      "epoch 1118, loss 0.9026049375534058, R2 0.5947327613830566\n",
      "Eval loss 0.9466843008995056, R2 0.5953139662742615\n",
      "epoch 1119, loss 0.9023491740226746, R2 0.5949234366416931\n",
      "Eval loss 0.9464037418365479, R2 0.595512866973877\n",
      "epoch 1120, loss 0.9020941257476807, R2 0.5951137542724609\n",
      "Eval loss 0.9461241364479065, R2 0.5957111716270447\n",
      "epoch 1121, loss 0.9018398523330688, R2 0.595303475856781\n",
      "Eval loss 0.9458451867103577, R2 0.595909059047699\n",
      "epoch 1122, loss 0.9015862941741943, R2 0.5954926013946533\n",
      "Eval loss 0.9455668330192566, R2 0.5961064100265503\n",
      "epoch 1123, loss 0.9013334512710571, R2 0.595681369304657\n",
      "Eval loss 0.9452894330024719, R2 0.5963033437728882\n",
      "epoch 1124, loss 0.9010813236236572, R2 0.5958696603775024\n",
      "Eval loss 0.9450126886367798, R2 0.5964996814727783\n",
      "epoch 1125, loss 0.900830090045929, R2 0.5960575342178345\n",
      "Eval loss 0.944736659526825, R2 0.5966956615447998\n",
      "epoch 1126, loss 0.9005793929100037, R2 0.5962448120117188\n",
      "Eval loss 0.944461464881897, R2 0.5968910455703735\n",
      "epoch 1127, loss 0.9003293514251709, R2 0.5964316725730896\n",
      "Eval loss 0.9441869854927063, R2 0.5970861315727234\n",
      "epoch 1128, loss 0.9000802636146545, R2 0.5966180562973022\n",
      "Eval loss 0.9439133405685425, R2 0.5972805619239807\n",
      "epoch 1129, loss 0.8998317718505859, R2 0.5968039631843567\n",
      "Eval loss 0.9436403512954712, R2 0.5974746346473694\n",
      "epoch 1130, loss 0.8995838165283203, R2 0.5969895124435425\n",
      "Eval loss 0.9433680772781372, R2 0.5976681113243103\n",
      "epoch 1131, loss 0.8993368148803711, R2 0.5971744060516357\n",
      "Eval loss 0.9430967569351196, R2 0.5978612303733826\n",
      "epoch 1132, loss 0.8990903496742249, R2 0.5973588824272156\n",
      "Eval loss 0.942825973033905, R2 0.5980538725852966\n",
      "epoch 1133, loss 0.898844838142395, R2 0.597542941570282\n",
      "Eval loss 0.9425559043884277, R2 0.5982460379600525\n",
      "epoch 1134, loss 0.8985998034477234, R2 0.5977264046669006\n",
      "Eval loss 0.9422867298126221, R2 0.5984377264976501\n",
      "epoch 1135, loss 0.8983555436134338, R2 0.5979095101356506\n",
      "Eval loss 0.9420182108879089, R2 0.5986289381980896\n",
      "epoch 1136, loss 0.8981119394302368, R2 0.5980921387672424\n",
      "Eval loss 0.9417502880096436, R2 0.5988196730613708\n",
      "epoch 1137, loss 0.8978690505027771, R2 0.5982743501663208\n",
      "Eval loss 0.941483199596405, R2 0.5990099310874939\n",
      "epoch 1138, loss 0.8976268768310547, R2 0.5984559655189514\n",
      "Eval loss 0.9412169456481934, R2 0.5991997718811035\n",
      "epoch 1139, loss 0.8973853588104248, R2 0.5986372232437134\n",
      "Eval loss 0.9409512877464294, R2 0.5993891358375549\n",
      "epoch 1140, loss 0.8971445560455322, R2 0.5988180637359619\n",
      "Eval loss 0.9406862854957581, R2 0.5995779633522034\n",
      "epoch 1141, loss 0.896904468536377, R2 0.5989983677864075\n",
      "Eval loss 0.9404220581054688, R2 0.5997664928436279\n",
      "epoch 1142, loss 0.8966650366783142, R2 0.5991782546043396\n",
      "Eval loss 0.9401586651802063, R2 0.5999544858932495\n",
      "epoch 1143, loss 0.8964263200759888, R2 0.5993576645851135\n",
      "Eval loss 0.939895749092102, R2 0.6001420021057129\n",
      "epoch 1144, loss 0.8961881995201111, R2 0.599536657333374\n",
      "Eval loss 0.9396339058876038, R2 0.6003291010856628\n",
      "epoch 1145, loss 0.8959507942199707, R2 0.5997151732444763\n",
      "Eval loss 0.9393723607063293, R2 0.6005156636238098\n",
      "epoch 1146, loss 0.8957140445709229, R2 0.5998931527137756\n",
      "Eval loss 0.9391117095947266, R2 0.6007018685340881\n",
      "epoch 1147, loss 0.8954780101776123, R2 0.6000707745552063\n",
      "Eval loss 0.9388518333435059, R2 0.600887656211853\n",
      "epoch 1148, loss 0.8952426314353943, R2 0.6002479791641235\n",
      "Eval loss 0.9385926723480225, R2 0.6010728478431702\n",
      "epoch 1149, loss 0.8950079083442688, R2 0.6004247069358826\n",
      "Eval loss 0.938334047794342, R2 0.6012577414512634\n",
      "epoch 1150, loss 0.8947739601135254, R2 0.6006009578704834\n",
      "Eval loss 0.9380761981010437, R2 0.6014420390129089\n",
      "epoch 1151, loss 0.8945404887199402, R2 0.600776731967926\n",
      "Eval loss 0.9378190040588379, R2 0.6016259789466858\n",
      "epoch 1152, loss 0.8943077921867371, R2 0.6009521484375\n",
      "Eval loss 0.9375625848770142, R2 0.6018095016479492\n",
      "epoch 1153, loss 0.8940758109092712, R2 0.6011272072792053\n",
      "Eval loss 0.937306821346283, R2 0.6019925475120544\n",
      "epoch 1154, loss 0.8938443064689636, R2 0.6013016104698181\n",
      "Eval loss 0.9370516538619995, R2 0.6021751761436462\n",
      "epoch 1155, loss 0.8936136364936829, R2 0.6014756560325623\n",
      "Eval loss 0.9367973208427429, R2 0.6023573279380798\n",
      "epoch 1156, loss 0.8933835029602051, R2 0.6016492247581482\n",
      "Eval loss 0.9365437030792236, R2 0.6025390028953552\n",
      "epoch 1157, loss 0.8931541442871094, R2 0.6018224954605103\n",
      "Eval loss 0.9362905621528625, R2 0.602720320224762\n",
      "epoch 1158, loss 0.8929252624511719, R2 0.6019954085350037\n",
      "Eval loss 0.9360381960868835, R2 0.6029011607170105\n",
      "epoch 1159, loss 0.8926971554756165, R2 0.602167546749115\n",
      "Eval loss 0.9357866048812866, R2 0.6030816435813904\n",
      "epoch 1160, loss 0.8924696445465088, R2 0.6023393869400024\n",
      "Eval loss 0.9355356097221375, R2 0.6032615900039673\n",
      "epoch 1161, loss 0.8922427892684937, R2 0.6025108695030212\n",
      "Eval loss 0.9352852702140808, R2 0.6034411191940308\n",
      "epoch 1162, loss 0.8920167088508606, R2 0.6026818752288818\n",
      "Eval loss 0.9350355863571167, R2 0.6036202311515808\n",
      "epoch 1163, loss 0.8917909860610962, R2 0.6028525829315186\n",
      "Eval loss 0.9347865581512451, R2 0.6037989258766174\n",
      "epoch 1164, loss 0.8915661573410034, R2 0.6030226349830627\n",
      "Eval loss 0.9345382452011108, R2 0.6039772629737854\n",
      "epoch 1165, loss 0.8913418650627136, R2 0.6031923890113831\n",
      "Eval loss 0.9342906475067139, R2 0.6041550040245056\n",
      "epoch 1166, loss 0.8911181688308716, R2 0.6033617258071899\n",
      "Eval loss 0.9340437054634094, R2 0.6043325066566467\n",
      "epoch 1167, loss 0.8908951282501221, R2 0.6035305857658386\n",
      "Eval loss 0.933797299861908, R2 0.6045094132423401\n",
      "epoch 1168, loss 0.8906726837158203, R2 0.6036991477012634\n",
      "Eval loss 0.9335516691207886, R2 0.6046860218048096\n",
      "epoch 1169, loss 0.8904510140419006, R2 0.6038671135902405\n",
      "Eval loss 0.9333066344261169, R2 0.6048621535301208\n",
      "epoch 1170, loss 0.8902297616004944, R2 0.6040347814559937\n",
      "Eval loss 0.9330623149871826, R2 0.6050377488136292\n",
      "epoch 1171, loss 0.890009343624115, R2 0.6042019724845886\n",
      "Eval loss 0.9328185319900513, R2 0.6052131056785583\n",
      "epoch 1172, loss 0.8897894024848938, R2 0.6043688058853149\n",
      "Eval loss 0.932575523853302, R2 0.6053880453109741\n",
      "epoch 1173, loss 0.8895701169967651, R2 0.6045351028442383\n",
      "Eval loss 0.9323331713676453, R2 0.6055624485015869\n",
      "epoch 1174, loss 0.8893513679504395, R2 0.6047011613845825\n",
      "Eval loss 0.9320914149284363, R2 0.605736494064331\n",
      "epoch 1175, loss 0.8891333341598511, R2 0.604866623878479\n",
      "Eval loss 0.9318503141403198, R2 0.605910062789917\n",
      "epoch 1176, loss 0.8889159560203552, R2 0.6050317287445068\n",
      "Eval loss 0.9316097497940063, R2 0.606083333492279\n",
      "epoch 1177, loss 0.8886992335319519, R2 0.6051965355873108\n",
      "Eval loss 0.931369960308075, R2 0.6062560677528381\n",
      "epoch 1178, loss 0.888482928276062, R2 0.6053609251976013\n",
      "Eval loss 0.9311308860778809, R2 0.6064285039901733\n",
      "epoch 1179, loss 0.8882673382759094, R2 0.6055247783660889\n",
      "Eval loss 0.930892288684845, R2 0.6066004633903503\n",
      "epoch 1180, loss 0.8880524039268494, R2 0.6056883335113525\n",
      "Eval loss 0.9306542873382568, R2 0.6067720055580139\n",
      "epoch 1181, loss 0.8878378868103027, R2 0.6058512926101685\n",
      "Eval loss 0.9304170608520508, R2 0.6069431304931641\n",
      "epoch 1182, loss 0.8876240849494934, R2 0.6060140132904053\n",
      "Eval loss 0.9301803708076477, R2 0.6071138978004456\n",
      "epoch 1183, loss 0.8874108791351318, R2 0.6061763167381287\n",
      "Eval loss 0.9299443960189819, R2 0.6072842478752136\n",
      "epoch 1184, loss 0.8871984481811523, R2 0.6063381433486938\n",
      "Eval loss 0.9297089576721191, R2 0.6074541807174683\n",
      "epoch 1185, loss 0.8869863152503967, R2 0.6064996123313904\n",
      "Eval loss 0.9294742345809937, R2 0.6076236367225647\n",
      "epoch 1186, loss 0.8867748379707336, R2 0.6066606640815735\n",
      "Eval loss 0.9292400479316711, R2 0.6077927947044373\n",
      "epoch 1187, loss 0.8865640759468079, R2 0.6068213582038879\n",
      "Eval loss 0.9290065765380859, R2 0.6079614758491516\n",
      "epoch 1188, loss 0.8863538503646851, R2 0.606981635093689\n",
      "Eval loss 0.9287736415863037, R2 0.6081297397613525\n",
      "epoch 1189, loss 0.8861441612243652, R2 0.6071414351463318\n",
      "Eval loss 0.928541362285614, R2 0.6082978248596191\n",
      "epoch 1190, loss 0.8859351873397827, R2 0.6073009371757507\n",
      "Eval loss 0.9283097386360168, R2 0.6084652543067932\n",
      "epoch 1191, loss 0.8857267498970032, R2 0.6074600219726562\n",
      "Eval loss 0.9280786514282227, R2 0.6086323857307434\n",
      "epoch 1192, loss 0.8855188488960266, R2 0.6076187491416931\n",
      "Eval loss 0.9278481602668762, R2 0.6087990403175354\n",
      "epoch 1193, loss 0.8853114247322083, R2 0.6077770590782166\n",
      "Eval loss 0.9276184439659119, R2 0.6089655160903931\n",
      "epoch 1194, loss 0.8851048350334167, R2 0.6079349517822266\n",
      "Eval loss 0.9273892045021057, R2 0.609131395816803\n",
      "epoch 1195, loss 0.8848986029624939, R2 0.6080924868583679\n",
      "Eval loss 0.9271605014801025, R2 0.6092968583106995\n",
      "epoch 1196, loss 0.8846930861473083, R2 0.6082496047019958\n",
      "Eval loss 0.9269325137138367, R2 0.6094620227813721\n",
      "epoch 1197, loss 0.8844881057739258, R2 0.6084063053131104\n",
      "Eval loss 0.9267050623893738, R2 0.609626829624176\n",
      "epoch 1198, loss 0.884283721446991, R2 0.6085626482963562\n",
      "Eval loss 0.9264783263206482, R2 0.6097912192344666\n",
      "epoch 1199, loss 0.8840798735618591, R2 0.6087185740470886\n",
      "Eval loss 0.9262521266937256, R2 0.6099550724029541\n",
      "epoch 1200, loss 0.883876621723175, R2 0.6088742613792419\n",
      "Eval loss 0.926026463508606, R2 0.6101187467575073\n",
      "epoch 1201, loss 0.8836738467216492, R2 0.6090293526649475\n",
      "Eval loss 0.9258015155792236, R2 0.6102818846702576\n",
      "epoch 1202, loss 0.8834717869758606, R2 0.6091842651367188\n",
      "Eval loss 0.9255770444869995, R2 0.6104447841644287\n",
      "epoch 1203, loss 0.8832700848579407, R2 0.6093385219573975\n",
      "Eval loss 0.9253532290458679, R2 0.6106071472167969\n",
      "epoch 1204, loss 0.8830690979957581, R2 0.6094927191734314\n",
      "Eval loss 0.9251299500465393, R2 0.6107692718505859\n",
      "epoch 1205, loss 0.8828685879707336, R2 0.6096463799476624\n",
      "Eval loss 0.9249073266983032, R2 0.6109309792518616\n",
      "epoch 1206, loss 0.882668673992157, R2 0.6097996234893799\n",
      "Eval loss 0.9246851801872253, R2 0.611092209815979\n",
      "epoch 1207, loss 0.8824693560600281, R2 0.6099525094032288\n",
      "Eval loss 0.9244637489318848, R2 0.6112531423568726\n",
      "epoch 1208, loss 0.8822705745697021, R2 0.6101049780845642\n",
      "Eval loss 0.9242429137229919, R2 0.6114135980606079\n",
      "epoch 1209, loss 0.8820722699165344, R2 0.6102572083473206\n",
      "Eval loss 0.9240225553512573, R2 0.6115737557411194\n",
      "epoch 1210, loss 0.8818746209144592, R2 0.6104090213775635\n",
      "Eval loss 0.9238028526306152, R2 0.6117334961891174\n",
      "epoch 1211, loss 0.8816775679588318, R2 0.6105603575706482\n",
      "Eval loss 0.9235836267471313, R2 0.6118929386138916\n",
      "epoch 1212, loss 0.8814809918403625, R2 0.6107114553451538\n",
      "Eval loss 0.9233651161193848, R2 0.6120519042015076\n",
      "epoch 1213, loss 0.881284773349762, R2 0.610862135887146\n",
      "Eval loss 0.9231470227241516, R2 0.6122105717658997\n",
      "epoch 1214, loss 0.8810893297195435, R2 0.6110123991966248\n",
      "Eval loss 0.922929584980011, R2 0.6123688220977783\n",
      "epoch 1215, loss 0.8808943629264832, R2 0.6111622452735901\n",
      "Eval loss 0.9227128028869629, R2 0.6125268340110779\n",
      "epoch 1216, loss 0.8806998133659363, R2 0.6113118529319763\n",
      "Eval loss 0.9224964380264282, R2 0.6126843094825745\n",
      "epoch 1217, loss 0.8805060386657715, R2 0.6114610433578491\n",
      "Eval loss 0.9222807884216309, R2 0.6128414869308472\n",
      "epoch 1218, loss 0.8803125619888306, R2 0.6116098761558533\n",
      "Eval loss 0.9220654964447021, R2 0.6129982471466064\n",
      "epoch 1219, loss 0.8801198601722717, R2 0.6117584109306335\n",
      "Eval loss 0.9218510389328003, R2 0.6131546497344971\n",
      "epoch 1220, loss 0.8799275755882263, R2 0.6119064688682556\n",
      "Eval loss 0.9216369986534119, R2 0.6133107542991638\n",
      "epoch 1221, loss 0.8797358870506287, R2 0.6120541095733643\n",
      "Eval loss 0.9214234948158264, R2 0.6134663820266724\n",
      "epoch 1222, loss 0.8795446753501892, R2 0.6122015714645386\n",
      "Eval loss 0.9212104678153992, R2 0.613621711730957\n",
      "epoch 1223, loss 0.8793538808822632, R2 0.6123485565185547\n",
      "Eval loss 0.9209981560707092, R2 0.6137767434120178\n",
      "epoch 1224, loss 0.8791638016700745, R2 0.6124951839447021\n",
      "Eval loss 0.9207863211631775, R2 0.6139312386512756\n",
      "epoch 1225, loss 0.8789741396903992, R2 0.6126413941383362\n",
      "Eval loss 0.9205750823020935, R2 0.6140854954719543\n",
      "epoch 1226, loss 0.8787849545478821, R2 0.6127874255180359\n",
      "Eval loss 0.9203644394874573, R2 0.6142393946647644\n",
      "epoch 1227, loss 0.8785964250564575, R2 0.6129329800605774\n",
      "Eval loss 0.9201542735099792, R2 0.614392876625061\n",
      "epoch 1228, loss 0.8784083724021912, R2 0.6130781769752502\n",
      "Eval loss 0.9199445843696594, R2 0.6145460605621338\n",
      "epoch 1229, loss 0.878220796585083, R2 0.6132230758666992\n",
      "Eval loss 0.9197354316711426, R2 0.6146988272666931\n",
      "epoch 1230, loss 0.8780337572097778, R2 0.6133676171302795\n",
      "Eval loss 0.919526994228363, R2 0.6148512959480286\n",
      "epoch 1231, loss 0.8778473138809204, R2 0.6135117411613464\n",
      "Eval loss 0.9193190932273865, R2 0.6150033473968506\n",
      "epoch 1232, loss 0.8776612281799316, R2 0.6136555075645447\n",
      "Eval loss 0.9191114902496338, R2 0.6151551008224487\n",
      "epoch 1233, loss 0.8774757385253906, R2 0.613798975944519\n",
      "Eval loss 0.918904721736908, R2 0.6153064370155334\n",
      "epoch 1234, loss 0.8772907257080078, R2 0.6139420866966248\n",
      "Eval loss 0.9186983108520508, R2 0.6154574751853943\n",
      "epoch 1235, loss 0.877106249332428, R2 0.6140848994255066\n",
      "Eval loss 0.9184925556182861, R2 0.6156081557273865\n",
      "epoch 1236, loss 0.8769223093986511, R2 0.6142272353172302\n",
      "Eval loss 0.9182872772216797, R2 0.61575847864151\n",
      "epoch 1237, loss 0.8767389059066772, R2 0.6143694519996643\n",
      "Eval loss 0.9180824160575867, R2 0.6159083843231201\n",
      "epoch 1238, loss 0.8765559196472168, R2 0.6145111322402954\n",
      "Eval loss 0.917878270149231, R2 0.6160579323768616\n",
      "epoch 1239, loss 0.8763734102249146, R2 0.6146524548530579\n",
      "Eval loss 0.9176745414733887, R2 0.6162073016166687\n",
      "epoch 1240, loss 0.8761915564537048, R2 0.6147934794425964\n",
      "Eval loss 0.9174714088439941, R2 0.6163563132286072\n",
      "epoch 1241, loss 0.8760101199150085, R2 0.6149341464042664\n",
      "Eval loss 0.9172687530517578, R2 0.6165048480033875\n",
      "epoch 1242, loss 0.8758291006088257, R2 0.6150745749473572\n",
      "Eval loss 0.9170666933059692, R2 0.6166530251502991\n",
      "epoch 1243, loss 0.8756486773490906, R2 0.6152145266532898\n",
      "Eval loss 0.9168650507926941, R2 0.616800844669342\n",
      "epoch 1244, loss 0.8754686713218689, R2 0.6153541207313538\n",
      "Eval loss 0.9166640639305115, R2 0.6169483661651611\n",
      "epoch 1245, loss 0.875289261341095, R2 0.6154934763908386\n",
      "Eval loss 0.9164634943008423, R2 0.6170955896377563\n",
      "epoch 1246, loss 0.8751102685928345, R2 0.6156325340270996\n",
      "Eval loss 0.9162635207176208, R2 0.6172425150871277\n",
      "epoch 1247, loss 0.874931812286377, R2 0.6157711744308472\n",
      "Eval loss 0.9160639047622681, R2 0.6173890829086304\n",
      "epoch 1248, loss 0.8747537136077881, R2 0.6159095168113708\n",
      "Eval loss 0.9158650636672974, R2 0.6175351738929749\n",
      "epoch 1249, loss 0.8745761513710022, R2 0.6160473823547363\n",
      "Eval loss 0.9156665205955505, R2 0.6176810264587402\n",
      "epoch 1250, loss 0.8743993043899536, R2 0.6161851286888123\n",
      "Eval loss 0.9154685735702515, R2 0.617826521396637\n",
      "epoch 1251, loss 0.8742226362228394, R2 0.6163224577903748\n",
      "Eval loss 0.9152711033821106, R2 0.617971658706665\n",
      "epoch 1252, loss 0.8740466833114624, R2 0.6164594292640686\n",
      "Eval loss 0.9150742292404175, R2 0.6181164979934692\n",
      "epoch 1253, loss 0.8738710880279541, R2 0.616595983505249\n",
      "Eval loss 0.9148777723312378, R2 0.6182609796524048\n",
      "epoch 1254, loss 0.8736960291862488, R2 0.6167323589324951\n",
      "Eval loss 0.9146817922592163, R2 0.6184051036834717\n",
      "epoch 1255, loss 0.8735213875770569, R2 0.6168683767318726\n",
      "Eval loss 0.9144864082336426, R2 0.6185489892959595\n",
      "epoch 1256, loss 0.873347282409668, R2 0.6170039772987366\n",
      "Eval loss 0.9142915606498718, R2 0.6186924576759338\n",
      "epoch 1257, loss 0.8731736540794373, R2 0.6171393394470215\n",
      "Eval loss 0.9140971302986145, R2 0.6188356876373291\n",
      "epoch 1258, loss 0.87300044298172, R2 0.6172743439674377\n",
      "Eval loss 0.9139032363891602, R2 0.6189784407615662\n",
      "epoch 1259, loss 0.8728277683258057, R2 0.6174089908599854\n",
      "Eval loss 0.9137098789215088, R2 0.6191209554672241\n",
      "epoch 1260, loss 0.87265545129776, R2 0.6175432801246643\n",
      "Eval loss 0.9135168790817261, R2 0.6192630529403687\n",
      "epoch 1261, loss 0.8724836707115173, R2 0.6176774501800537\n",
      "Eval loss 0.9133244752883911, R2 0.6194049119949341\n",
      "epoch 1262, loss 0.8723124265670776, R2 0.6178111433982849\n",
      "Eval loss 0.9131325483322144, R2 0.6195464730262756\n",
      "epoch 1263, loss 0.8721415400505066, R2 0.6179445385932922\n",
      "Eval loss 0.912941038608551, R2 0.6196876168251038\n",
      "epoch 1264, loss 0.8719712495803833, R2 0.6180775761604309\n",
      "Eval loss 0.9127501845359802, R2 0.6198285222053528\n",
      "epoch 1265, loss 0.8718013167381287, R2 0.6182103753089905\n",
      "Eval loss 0.9125597476959229, R2 0.6199690699577332\n",
      "epoch 1266, loss 0.8716317415237427, R2 0.6183427572250366\n",
      "Eval loss 0.9123698472976685, R2 0.6201092600822449\n",
      "epoch 1267, loss 0.8714628219604492, R2 0.6184747815132141\n",
      "Eval loss 0.9121804237365723, R2 0.6202491521835327\n",
      "epoch 1268, loss 0.871294379234314, R2 0.6186066269874573\n",
      "Eval loss 0.9119912981987, R2 0.6203887462615967\n",
      "epoch 1269, loss 0.8711262345314026, R2 0.6187381148338318\n",
      "Eval loss 0.9118028283119202, R2 0.6205279231071472\n",
      "epoch 1270, loss 0.8709586262702942, R2 0.6188692450523376\n",
      "Eval loss 0.9116148948669434, R2 0.6206669211387634\n",
      "epoch 1271, loss 0.8707915544509888, R2 0.6190001368522644\n",
      "Eval loss 0.91142737865448, R2 0.6208055019378662\n",
      "epoch 1272, loss 0.870624840259552, R2 0.6191306114196777\n",
      "Eval loss 0.9112402200698853, R2 0.6209437847137451\n",
      "epoch 1273, loss 0.8704584240913391, R2 0.619260847568512\n",
      "Eval loss 0.9110537171363831, R2 0.6210817694664001\n",
      "epoch 1274, loss 0.8702927827835083, R2 0.6193907856941223\n",
      "Eval loss 0.9108675718307495, R2 0.6212194561958313\n",
      "epoch 1275, loss 0.8701273798942566, R2 0.6195202469825745\n",
      "Eval loss 0.9106820225715637, R2 0.621356725692749\n",
      "epoch 1276, loss 0.8699624538421631, R2 0.6196496486663818\n",
      "Eval loss 0.9104968309402466, R2 0.6214937567710876\n",
      "epoch 1277, loss 0.8697981238365173, R2 0.619778573513031\n",
      "Eval loss 0.9103121161460876, R2 0.6216304302215576\n",
      "epoch 1278, loss 0.8696340322494507, R2 0.6199073195457458\n",
      "Eval loss 0.9101279377937317, R2 0.6217668652534485\n",
      "epoch 1279, loss 0.8694704174995422, R2 0.6200355887413025\n",
      "Eval loss 0.9099442362785339, R2 0.6219030022621155\n",
      "epoch 1280, loss 0.8693073391914368, R2 0.62016361951828\n",
      "Eval loss 0.9097608923912048, R2 0.6220387816429138\n",
      "epoch 1281, loss 0.8691447377204895, R2 0.6202913522720337\n",
      "Eval loss 0.9095781445503235, R2 0.6221742033958435\n",
      "epoch 1282, loss 0.8689824342727661, R2 0.6204187870025635\n",
      "Eval loss 0.9093957543373108, R2 0.6223093271255493\n",
      "epoch 1283, loss 0.8688206672668457, R2 0.6205459237098694\n",
      "Eval loss 0.9092139601707458, R2 0.6224441528320312\n",
      "epoch 1284, loss 0.868659257888794, R2 0.6206727623939514\n",
      "Eval loss 0.9090325832366943, R2 0.6225786805152893\n",
      "epoch 1285, loss 0.8684983849525452, R2 0.6207992434501648\n",
      "Eval loss 0.9088515639305115, R2 0.6227129101753235\n",
      "epoch 1286, loss 0.868337869644165, R2 0.6209255456924438\n",
      "Eval loss 0.9086711406707764, R2 0.6228468418121338\n",
      "epoch 1287, loss 0.8681778907775879, R2 0.6210514307022095\n",
      "Eval loss 0.9084911346435547, R2 0.6229804158210754\n",
      "epoch 1288, loss 0.8680182099342346, R2 0.6211770176887512\n",
      "Eval loss 0.9083116054534912, R2 0.6231138110160828\n",
      "epoch 1289, loss 0.8678591251373291, R2 0.6213023662567139\n",
      "Eval loss 0.9081324338912964, R2 0.6232467889785767\n",
      "epoch 1290, loss 0.8677002787590027, R2 0.6214273571968079\n",
      "Eval loss 0.9079538583755493, R2 0.6233794689178467\n",
      "epoch 1291, loss 0.8675419092178345, R2 0.621552050113678\n",
      "Eval loss 0.9077755808830261, R2 0.6235118508338928\n",
      "epoch 1292, loss 0.8673840761184692, R2 0.621676504611969\n",
      "Eval loss 0.9075978398323059, R2 0.6236439347267151\n",
      "epoch 1293, loss 0.8672265410423279, R2 0.6218005418777466\n",
      "Eval loss 0.9074206352233887, R2 0.6237757205963135\n",
      "epoch 1294, loss 0.8670695424079895, R2 0.6219243407249451\n",
      "Eval loss 0.9072436094284058, R2 0.6239072680473328\n",
      "epoch 1295, loss 0.866912841796875, R2 0.6220479607582092\n",
      "Eval loss 0.9070672988891602, R2 0.6240384578704834\n",
      "epoch 1296, loss 0.8667567372322083, R2 0.62217116355896\n",
      "Eval loss 0.9068913459777832, R2 0.6241694092750549\n",
      "epoch 1297, loss 0.8666008710861206, R2 0.6222941875457764\n",
      "Eval loss 0.9067158102989197, R2 0.6243000030517578\n",
      "epoch 1298, loss 0.8664454817771912, R2 0.6224167346954346\n",
      "Eval loss 0.9065408110618591, R2 0.624430239200592\n",
      "epoch 1299, loss 0.8662905693054199, R2 0.6225391626358032\n",
      "Eval loss 0.906366229057312, R2 0.6245602965354919\n",
      "epoch 1300, loss 0.8661360144615173, R2 0.6226611733436584\n",
      "Eval loss 0.9061920046806335, R2 0.6246899962425232\n",
      "epoch 1301, loss 0.8659818768501282, R2 0.6227830648422241\n",
      "Eval loss 0.9060183167457581, R2 0.6248193979263306\n",
      "epoch 1302, loss 0.8658280968666077, R2 0.6229045391082764\n",
      "Eval loss 0.9058449864387512, R2 0.6249485015869141\n",
      "epoch 1303, loss 0.8656749129295349, R2 0.6230257749557495\n",
      "Eval loss 0.9056722521781921, R2 0.6250773668289185\n",
      "epoch 1304, loss 0.865522027015686, R2 0.6231465935707092\n",
      "Eval loss 0.9054998159408569, R2 0.6252058744430542\n",
      "epoch 1305, loss 0.8653694987297058, R2 0.6232672333717346\n",
      "Eval loss 0.9053278565406799, R2 0.6253341436386108\n",
      "epoch 1306, loss 0.8652174472808838, R2 0.6233875751495361\n",
      "Eval loss 0.9051562547683716, R2 0.6254620552062988\n",
      "epoch 1307, loss 0.86506587266922, R2 0.6235076189041138\n",
      "Eval loss 0.9049850702285767, R2 0.6255897283554077\n",
      "epoch 1308, loss 0.86491459608078, R2 0.6236274242401123\n",
      "Eval loss 0.9048144817352295, R2 0.6257171034812927\n",
      "epoch 1309, loss 0.8647636771202087, R2 0.623746931552887\n",
      "Eval loss 0.9046441912651062, R2 0.6258442401885986\n",
      "epoch 1310, loss 0.8646132946014404, R2 0.6238661408424377\n",
      "Eval loss 0.9044742584228516, R2 0.6259709596633911\n",
      "epoch 1311, loss 0.8644632697105408, R2 0.6239850521087646\n",
      "Eval loss 0.9043049812316895, R2 0.626097559928894\n",
      "epoch 1312, loss 0.864313542842865, R2 0.6241037249565125\n",
      "Eval loss 0.9041360020637512, R2 0.6262237429618835\n",
      "epoch 1313, loss 0.864164412021637, R2 0.6242220401763916\n",
      "Eval loss 0.9039674997329712, R2 0.626349687576294\n",
      "epoch 1314, loss 0.8640156388282776, R2 0.6243402361869812\n",
      "Eval loss 0.9037993550300598, R2 0.6264753341674805\n",
      "epoch 1315, loss 0.8638671636581421, R2 0.6244578957557678\n",
      "Eval loss 0.9036317467689514, R2 0.6266006827354431\n",
      "epoch 1316, loss 0.86371910572052, R2 0.6245754361152649\n",
      "Eval loss 0.9034643173217773, R2 0.6267258524894714\n",
      "epoch 1317, loss 0.8635714054107666, R2 0.6246926784515381\n",
      "Eval loss 0.9032976031303406, R2 0.6268505454063416\n",
      "epoch 1318, loss 0.8634241819381714, R2 0.6248096823692322\n",
      "Eval loss 0.9031310081481934, R2 0.6269751787185669\n",
      "epoch 1319, loss 0.8632773160934448, R2 0.6249263882637024\n",
      "Eval loss 0.9029650092124939, R2 0.6270994544029236\n",
      "epoch 1320, loss 0.8631308674812317, R2 0.6250428557395935\n",
      "Eval loss 0.9027994275093079, R2 0.6272232532501221\n",
      "epoch 1321, loss 0.8629847168922424, R2 0.6251589059829712\n",
      "Eval loss 0.9026342034339905, R2 0.6273470520973206\n",
      "epoch 1322, loss 0.8628390431404114, R2 0.6252748370170593\n",
      "Eval loss 0.9024694561958313, R2 0.6274704337120056\n",
      "epoch 1323, loss 0.8626937866210938, R2 0.6253904700279236\n",
      "Eval loss 0.9023051261901855, R2 0.6275935769081116\n",
      "epoch 1324, loss 0.862548828125, R2 0.625505805015564\n",
      "Eval loss 0.902141273021698, R2 0.6277164816856384\n",
      "epoch 1325, loss 0.8624044060707092, R2 0.6256209015846252\n",
      "Eval loss 0.9019777178764343, R2 0.6278389692306519\n",
      "epoch 1326, loss 0.8622602224349976, R2 0.6257355809211731\n",
      "Eval loss 0.9018145203590393, R2 0.6279613375663757\n",
      "epoch 1327, loss 0.8621164560317993, R2 0.6258501410484314\n",
      "Eval loss 0.9016517996788025, R2 0.6280832886695862\n",
      "epoch 1328, loss 0.8619731068611145, R2 0.625964343547821\n",
      "Eval loss 0.9014894962310791, R2 0.6282050609588623\n",
      "epoch 1329, loss 0.8618300557136536, R2 0.6260783672332764\n",
      "Eval loss 0.9013277888298035, R2 0.6283265352249146\n",
      "epoch 1330, loss 0.8616874814033508, R2 0.626192033290863\n",
      "Eval loss 0.9011660218238831, R2 0.6284477710723877\n",
      "epoch 1331, loss 0.8615452647209167, R2 0.6263055205345154\n",
      "Eval loss 0.9010049700737, R2 0.628568708896637\n",
      "epoch 1332, loss 0.8614034056663513, R2 0.6264187097549438\n",
      "Eval loss 0.9008442759513855, R2 0.6286893486976624\n",
      "epoch 1333, loss 0.8612619042396545, R2 0.6265316009521484\n",
      "Eval loss 0.9006839394569397, R2 0.6288097500801086\n",
      "epoch 1334, loss 0.8611208200454712, R2 0.6266442537307739\n",
      "Eval loss 0.9005241394042969, R2 0.6289297938346863\n",
      "epoch 1335, loss 0.8609800934791565, R2 0.6267566084861755\n",
      "Eval loss 0.9003646969795227, R2 0.6290496587753296\n",
      "epoch 1336, loss 0.8608397841453552, R2 0.6268687844276428\n",
      "Eval loss 0.9002054929733276, R2 0.629169225692749\n",
      "epoch 1337, loss 0.8606997728347778, R2 0.6269805431365967\n",
      "Eval loss 0.9000468850135803, R2 0.6292886137962341\n",
      "epoch 1338, loss 0.8605601787567139, R2 0.627092182636261\n",
      "Eval loss 0.8998885154724121, R2 0.6294076442718506\n",
      "epoch 1339, loss 0.8604210019111633, R2 0.6272035241127014\n",
      "Eval loss 0.8997306227684021, R2 0.6295264363288879\n",
      "epoch 1340, loss 0.8602820634841919, R2 0.627314567565918\n",
      "Eval loss 0.8995731472969055, R2 0.6296449303627014\n",
      "epoch 1341, loss 0.8601434826850891, R2 0.6274254322052002\n",
      "Eval loss 0.8994160294532776, R2 0.6297631859779358\n",
      "epoch 1342, loss 0.8600053191184998, R2 0.6275359988212585\n",
      "Eval loss 0.8992592096328735, R2 0.6298811435699463\n",
      "epoch 1343, loss 0.8598676919937134, R2 0.6276462078094482\n",
      "Eval loss 0.8991028070449829, R2 0.6299988627433777\n",
      "epoch 1344, loss 0.8597303032875061, R2 0.6277562975883484\n",
      "Eval loss 0.8989468812942505, R2 0.6301162838935852\n",
      "epoch 1345, loss 0.8595932126045227, R2 0.6278660893440247\n",
      "Eval loss 0.8987913727760315, R2 0.6302335262298584\n",
      "epoch 1346, loss 0.8594565987586975, R2 0.627975583076477\n",
      "Eval loss 0.8986361622810364, R2 0.6303504705429077\n",
      "epoch 1347, loss 0.8593202233314514, R2 0.6280848383903503\n",
      "Eval loss 0.8984813690185547, R2 0.6304671764373779\n",
      "epoch 1348, loss 0.859184205532074, R2 0.6281939148902893\n",
      "Eval loss 0.8983268737792969, R2 0.6305835843086243\n",
      "epoch 1349, loss 0.8590486645698547, R2 0.6283026337623596\n",
      "Eval loss 0.898172914981842, R2 0.630699634552002\n",
      "epoch 1350, loss 0.8589134216308594, R2 0.6284111738204956\n",
      "Eval loss 0.8980191946029663, R2 0.6308155655860901\n",
      "epoch 1351, loss 0.8587785959243774, R2 0.6285194158554077\n",
      "Eval loss 0.8978659510612488, R2 0.6309311985969543\n",
      "epoch 1352, loss 0.8586440086364746, R2 0.628627359867096\n",
      "Eval loss 0.8977131843566895, R2 0.6310466527938843\n",
      "epoch 1353, loss 0.8585098385810852, R2 0.6287351250648499\n",
      "Eval loss 0.8975605368614197, R2 0.6311616897583008\n",
      "epoch 1354, loss 0.8583759665489197, R2 0.6288426518440247\n",
      "Eval loss 0.8974084854125977, R2 0.631276547908783\n",
      "epoch 1355, loss 0.8582425713539124, R2 0.6289498805999756\n",
      "Eval loss 0.8972567319869995, R2 0.6313912272453308\n",
      "epoch 1356, loss 0.8581093549728394, R2 0.6290568709373474\n",
      "Eval loss 0.8971052765846252, R2 0.6315056085586548\n",
      "epoch 1357, loss 0.8579767346382141, R2 0.6291636824607849\n",
      "Eval loss 0.896954357624054, R2 0.6316196918487549\n",
      "epoch 1358, loss 0.8578442931175232, R2 0.629270076751709\n",
      "Eval loss 0.896803617477417, R2 0.6317335367202759\n",
      "epoch 1359, loss 0.8577122092247009, R2 0.6293763518333435\n",
      "Eval loss 0.896653413772583, R2 0.6318471431732178\n",
      "epoch 1360, loss 0.8575806021690369, R2 0.6294823288917542\n",
      "Eval loss 0.8965035676956177, R2 0.6319604516029358\n",
      "epoch 1361, loss 0.8574491143226624, R2 0.6295881271362305\n",
      "Eval loss 0.8963540196418762, R2 0.6320736408233643\n",
      "epoch 1362, loss 0.8573181629180908, R2 0.6296936869621277\n",
      "Eval loss 0.8962048888206482, R2 0.6321863532066345\n",
      "epoch 1363, loss 0.8571873307228088, R2 0.629798948764801\n",
      "Eval loss 0.8960559964179993, R2 0.6322988867759705\n",
      "epoch 1364, loss 0.8570570945739746, R2 0.6299039125442505\n",
      "Eval loss 0.8959077000617981, R2 0.6324112415313721\n",
      "epoch 1365, loss 0.856927216053009, R2 0.6300086975097656\n",
      "Eval loss 0.8957595825195312, R2 0.6325234770774841\n",
      "epoch 1366, loss 0.8567973971366882, R2 0.6301132440567017\n",
      "Eval loss 0.8956118822097778, R2 0.6326351761817932\n",
      "epoch 1367, loss 0.8566681742668152, R2 0.6302175521850586\n",
      "Eval loss 0.8954644799232483, R2 0.6327468156814575\n",
      "epoch 1368, loss 0.8565391302108765, R2 0.6303215622901917\n",
      "Eval loss 0.895317554473877, R2 0.6328582167625427\n",
      "epoch 1369, loss 0.8564106225967407, R2 0.6304253935813904\n",
      "Eval loss 0.8951709866523743, R2 0.632969319820404\n",
      "epoch 1370, loss 0.856282114982605, R2 0.63052898645401\n",
      "Eval loss 0.8950246572494507, R2 0.6330801844596863\n",
      "epoch 1371, loss 0.8561543226242065, R2 0.6306322813034058\n",
      "Eval loss 0.8948789238929749, R2 0.6331906914710999\n",
      "epoch 1372, loss 0.8560267090797424, R2 0.6307353973388672\n",
      "Eval loss 0.8947333097457886, R2 0.6333011984825134\n",
      "epoch 1373, loss 0.8558993935585022, R2 0.6308382153511047\n",
      "Eval loss 0.8945881724357605, R2 0.6334112882614136\n",
      "epoch 1374, loss 0.8557723760604858, R2 0.630940854549408\n",
      "Eval loss 0.8944433331489563, R2 0.6335211396217346\n",
      "epoch 1375, loss 0.8556457757949829, R2 0.6310431957244873\n",
      "Eval loss 0.8942988514900208, R2 0.6336308121681213\n",
      "epoch 1376, loss 0.8555195331573486, R2 0.6311454176902771\n",
      "Eval loss 0.8941548466682434, R2 0.6337401866912842\n",
      "epoch 1377, loss 0.8553934693336487, R2 0.6312472224235535\n",
      "Eval loss 0.8940110206604004, R2 0.6338493824005127\n",
      "epoch 1378, loss 0.8552679419517517, R2 0.6313489675521851\n",
      "Eval loss 0.893867552280426, R2 0.6339582800865173\n",
      "epoch 1379, loss 0.8551426529884338, R2 0.6314504146575928\n",
      "Eval loss 0.8937246203422546, R2 0.6340669393539429\n",
      "epoch 1380, loss 0.8550176024436951, R2 0.6315515637397766\n",
      "Eval loss 0.8935818672180176, R2 0.6341753602027893\n",
      "epoch 1381, loss 0.854892909526825, R2 0.6316525936126709\n",
      "Eval loss 0.893439531326294, R2 0.6342834830284119\n",
      "epoch 1382, loss 0.8547686338424683, R2 0.6317533254623413\n",
      "Eval loss 0.8932974934577942, R2 0.6343915462493896\n",
      "epoch 1383, loss 0.8546445965766907, R2 0.6318538188934326\n",
      "Eval loss 0.8931558132171631, R2 0.6344991326332092\n",
      "epoch 1384, loss 0.8545209765434265, R2 0.6319541335105896\n",
      "Eval loss 0.8930145502090454, R2 0.6346067190170288\n",
      "epoch 1385, loss 0.8543976545333862, R2 0.6320540904998779\n",
      "Eval loss 0.8928735852241516, R2 0.6347139477729797\n",
      "epoch 1386, loss 0.8542745113372803, R2 0.6321539878845215\n",
      "Eval loss 0.8927329778671265, R2 0.6348209381103516\n",
      "epoch 1387, loss 0.8541517853736877, R2 0.6322535276412964\n",
      "Eval loss 0.8925926685333252, R2 0.6349276900291443\n",
      "epoch 1388, loss 0.8540294766426086, R2 0.6323530077934265\n",
      "Eval loss 0.8924526572227478, R2 0.6350342035293579\n",
      "epoch 1389, loss 0.8539072871208191, R2 0.6324520111083984\n",
      "Eval loss 0.8923130035400391, R2 0.6351404786109924\n",
      "epoch 1390, loss 0.8537855744361877, R2 0.6325508952140808\n",
      "Eval loss 0.8921739459037781, R2 0.6352466940879822\n",
      "epoch 1391, loss 0.853664219379425, R2 0.6326495409011841\n",
      "Eval loss 0.8920350074768066, R2 0.6353524923324585\n",
      "epoch 1392, loss 0.8535429835319519, R2 0.632748007774353\n",
      "Eval loss 0.8918964862823486, R2 0.6354580521583557\n",
      "epoch 1393, loss 0.853422224521637, R2 0.6328462362289429\n",
      "Eval loss 0.891758143901825, R2 0.6355634331703186\n",
      "epoch 1394, loss 0.8533016443252563, R2 0.6329441666603088\n",
      "Eval loss 0.8916202187538147, R2 0.6356685757637024\n",
      "epoch 1395, loss 0.8531814813613892, R2 0.6330419778823853\n",
      "Eval loss 0.8914827108383179, R2 0.6357735395431519\n",
      "epoch 1396, loss 0.8530616164207458, R2 0.6331394910812378\n",
      "Eval loss 0.8913455009460449, R2 0.6358782052993774\n",
      "epoch 1397, loss 0.8529421091079712, R2 0.633236825466156\n",
      "Eval loss 0.8912086486816406, R2 0.6359826922416687\n",
      "epoch 1398, loss 0.8528228998184204, R2 0.6333339214324951\n",
      "Eval loss 0.8910719156265259, R2 0.6360868811607361\n",
      "epoch 1399, loss 0.8527039289474487, R2 0.6334308385848999\n",
      "Eval loss 0.8909357190132141, R2 0.6361909508705139\n",
      "epoch 1400, loss 0.8525852560997009, R2 0.6335275173187256\n",
      "Eval loss 0.890799880027771, R2 0.6362947225570679\n",
      "epoch 1401, loss 0.8524670004844666, R2 0.6336238384246826\n",
      "Eval loss 0.8906642198562622, R2 0.6363982558250427\n",
      "epoch 1402, loss 0.8523489832878113, R2 0.6337200999259949\n",
      "Eval loss 0.8905289173126221, R2 0.6365015506744385\n",
      "epoch 1403, loss 0.8522312045097351, R2 0.633816123008728\n",
      "Eval loss 0.8903941512107849, R2 0.6366046667098999\n",
      "epoch 1404, loss 0.8521138429641724, R2 0.6339118480682373\n",
      "Eval loss 0.8902595043182373, R2 0.6367075443267822\n",
      "epoch 1405, loss 0.8519967198371887, R2 0.6340073943138123\n",
      "Eval loss 0.8901252746582031, R2 0.6368101239204407\n",
      "epoch 1406, loss 0.8518799543380737, R2 0.6341027617454529\n",
      "Eval loss 0.889991283416748, R2 0.6369125247001648\n",
      "epoch 1407, loss 0.8517634868621826, R2 0.6341978907585144\n",
      "Eval loss 0.8898577690124512, R2 0.6370147466659546\n",
      "epoch 1408, loss 0.8516471982002258, R2 0.634292721748352\n",
      "Eval loss 0.8897244334220886, R2 0.6371167302131653\n",
      "epoch 1409, loss 0.8515313267707825, R2 0.6343874335289001\n",
      "Eval loss 0.8895913362503052, R2 0.6372185349464417\n",
      "epoch 1410, loss 0.851415753364563, R2 0.6344818472862244\n",
      "Eval loss 0.8894588351249695, R2 0.6373199820518494\n",
      "epoch 1411, loss 0.8513004183769226, R2 0.6345760822296143\n",
      "Eval loss 0.8893264532089233, R2 0.6374213695526123\n",
      "epoch 1412, loss 0.8511853814125061, R2 0.6346701979637146\n",
      "Eval loss 0.8891944289207458, R2 0.6375223994255066\n",
      "epoch 1413, loss 0.8510708212852478, R2 0.6347639560699463\n",
      "Eval loss 0.8890627026557922, R2 0.6376233100891113\n",
      "epoch 1414, loss 0.8509563207626343, R2 0.6348576545715332\n",
      "Eval loss 0.888931393623352, R2 0.6377239227294922\n",
      "epoch 1415, loss 0.8508421778678894, R2 0.6349509954452515\n",
      "Eval loss 0.8888002634048462, R2 0.6378244161605835\n",
      "epoch 1416, loss 0.8507285118103027, R2 0.6350441575050354\n",
      "Eval loss 0.8886695504188538, R2 0.6379245519638062\n",
      "epoch 1417, loss 0.8506149053573608, R2 0.6351370811462402\n",
      "Eval loss 0.8885390758514404, R2 0.6380245685577393\n",
      "epoch 1418, loss 0.8505017161369324, R2 0.6352298855781555\n",
      "Eval loss 0.8884090185165405, R2 0.638124406337738\n",
      "epoch 1419, loss 0.8503886461257935, R2 0.6353223919868469\n",
      "Eval loss 0.888279139995575, R2 0.6382238864898682\n",
      "epoch 1420, loss 0.8502761125564575, R2 0.6354148387908936\n",
      "Eval loss 0.8881496787071228, R2 0.6383232474327087\n",
      "epoch 1421, loss 0.8501636981964111, R2 0.6355069279670715\n",
      "Eval loss 0.8880205750465393, R2 0.6384223699569702\n",
      "epoch 1422, loss 0.8500516414642334, R2 0.6355987787246704\n",
      "Eval loss 0.8878917098045349, R2 0.6385212540626526\n",
      "epoch 1423, loss 0.8499398231506348, R2 0.6356905102729797\n",
      "Eval loss 0.8877631425857544, R2 0.6386199593544006\n",
      "epoch 1424, loss 0.84982830286026, R2 0.6357820630073547\n",
      "Eval loss 0.8876349329948425, R2 0.6387184262275696\n",
      "epoch 1425, loss 0.8497171401977539, R2 0.6358733773231506\n",
      "Eval loss 0.887506902217865, R2 0.638816773891449\n",
      "epoch 1426, loss 0.8496061563491821, R2 0.6359644532203674\n",
      "Eval loss 0.8873793482780457, R2 0.6389148235321045\n",
      "epoch 1427, loss 0.8494955897331238, R2 0.6360552906990051\n",
      "Eval loss 0.8872519731521606, R2 0.6390126943588257\n",
      "epoch 1428, loss 0.8493853211402893, R2 0.6361459493637085\n",
      "Eval loss 0.8871250152587891, R2 0.6391102075576782\n",
      "epoch 1429, loss 0.8492752313613892, R2 0.6362363696098328\n",
      "Eval loss 0.8869982361793518, R2 0.639207661151886\n",
      "epoch 1430, loss 0.8491653203964233, R2 0.6363266706466675\n",
      "Eval loss 0.8868718147277832, R2 0.6393048763275146\n",
      "epoch 1431, loss 0.8490559458732605, R2 0.6364166736602783\n",
      "Eval loss 0.8867457509040833, R2 0.6394019722938538\n",
      "epoch 1432, loss 0.848946750164032, R2 0.6365066170692444\n",
      "Eval loss 0.8866199254989624, R2 0.6394987106323242\n",
      "epoch 1433, loss 0.8488377928733826, R2 0.6365962624549866\n",
      "Eval loss 0.886494517326355, R2 0.6395952701568604\n",
      "epoch 1434, loss 0.8487290740013123, R2 0.6366856694221497\n",
      "Eval loss 0.8863691687583923, R2 0.6396916508674622\n",
      "epoch 1435, loss 0.8486207127571106, R2 0.6367749571800232\n",
      "Eval loss 0.8862442970275879, R2 0.6397877931594849\n",
      "epoch 1436, loss 0.848512589931488, R2 0.6368639469146729\n",
      "Eval loss 0.8861196637153625, R2 0.6398837566375732\n",
      "epoch 1437, loss 0.8484048247337341, R2 0.636952817440033\n",
      "Eval loss 0.8859953880310059, R2 0.6399795413017273\n",
      "epoch 1438, loss 0.8482972383499146, R2 0.637041449546814\n",
      "Eval loss 0.8858713507652283, R2 0.6400750875473022\n",
      "epoch 1439, loss 0.8481900095939636, R2 0.6371298432350159\n",
      "Eval loss 0.8857476711273193, R2 0.6401704549789429\n",
      "epoch 1440, loss 0.8480829000473022, R2 0.6372181177139282\n",
      "Eval loss 0.8856242895126343, R2 0.6402655243873596\n",
      "epoch 1441, loss 0.8479762077331543, R2 0.6373061537742615\n",
      "Eval loss 0.8855010867118835, R2 0.640360414981842\n",
      "epoch 1442, loss 0.8478697538375854, R2 0.6373940110206604\n",
      "Eval loss 0.8853783011436462, R2 0.6404552459716797\n",
      "epoch 1443, loss 0.8477634787559509, R2 0.6374816298484802\n",
      "Eval loss 0.8852556943893433, R2 0.6405497193336487\n",
      "epoch 1444, loss 0.8476576805114746, R2 0.6375691294670105\n",
      "Eval loss 0.8851335048675537, R2 0.6406440734863281\n",
      "epoch 1445, loss 0.8475520014762878, R2 0.6376563906669617\n",
      "Eval loss 0.8850115537643433, R2 0.6407381296157837\n",
      "epoch 1446, loss 0.8474465608596802, R2 0.6377434134483337\n",
      "Eval loss 0.8848899006843567, R2 0.6408321261405945\n",
      "epoch 1447, loss 0.8473415970802307, R2 0.6378303170204163\n",
      "Eval loss 0.8847684860229492, R2 0.6409257650375366\n",
      "epoch 1448, loss 0.847236692905426, R2 0.6379169225692749\n",
      "Eval loss 0.8846473693847656, R2 0.6410192847251892\n",
      "epoch 1449, loss 0.8471320867538452, R2 0.638003408908844\n",
      "Eval loss 0.8845265507698059, R2 0.6411126255989075\n",
      "epoch 1450, loss 0.8470278978347778, R2 0.6380895972251892\n",
      "Eval loss 0.8844060897827148, R2 0.6412057280540466\n",
      "epoch 1451, loss 0.846923828125, R2 0.6381757855415344\n",
      "Eval loss 0.8842858672142029, R2 0.6412985324859619\n",
      "epoch 1452, loss 0.8468199372291565, R2 0.638261616230011\n",
      "Eval loss 0.8841659426689148, R2 0.6413913369178772\n",
      "epoch 1453, loss 0.8467165231704712, R2 0.6383472084999084\n",
      "Eval loss 0.8840461373329163, R2 0.6414837837219238\n",
      "epoch 1454, loss 0.8466132879257202, R2 0.6384328007698059\n",
      "Eval loss 0.8839268684387207, R2 0.6415762305259705\n",
      "epoch 1455, loss 0.8465102910995483, R2 0.6385180950164795\n",
      "Eval loss 0.883807897567749, R2 0.6416683197021484\n",
      "epoch 1456, loss 0.8464074730873108, R2 0.6386032104492188\n",
      "Eval loss 0.8836889266967773, R2 0.6417602300643921\n",
      "epoch 1457, loss 0.8463050723075867, R2 0.6386881470680237\n",
      "Eval loss 0.8835703730583191, R2 0.6418518424034119\n",
      "epoch 1458, loss 0.8462029695510864, R2 0.6387728452682495\n",
      "Eval loss 0.8834522366523743, R2 0.6419433951377869\n",
      "epoch 1459, loss 0.846100926399231, R2 0.6388574242591858\n",
      "Eval loss 0.8833341598510742, R2 0.6420347690582275\n",
      "epoch 1460, loss 0.8459993004798889, R2 0.638941764831543\n",
      "Eval loss 0.8832165598869324, R2 0.6421258449554443\n",
      "epoch 1461, loss 0.8458977937698364, R2 0.6390259265899658\n",
      "Eval loss 0.8830990791320801, R2 0.6422168016433716\n",
      "epoch 1462, loss 0.8457967042922974, R2 0.6391098499298096\n",
      "Eval loss 0.8829819560050964, R2 0.6423076391220093\n",
      "epoch 1463, loss 0.8456958532333374, R2 0.6391936540603638\n",
      "Eval loss 0.8828651309013367, R2 0.6423980593681335\n",
      "epoch 1464, loss 0.8455951809883118, R2 0.6392772793769836\n",
      "Eval loss 0.8827486038208008, R2 0.642488420009613\n",
      "epoch 1465, loss 0.8454946875572205, R2 0.6393606662750244\n",
      "Eval loss 0.8826322555541992, R2 0.642578661441803\n",
      "epoch 1466, loss 0.8453945517539978, R2 0.6394439339637756\n",
      "Eval loss 0.8825161457061768, R2 0.642668604850769\n",
      "epoch 1467, loss 0.8452946543693542, R2 0.639526903629303\n",
      "Eval loss 0.882400393486023, R2 0.6427583694458008\n",
      "epoch 1468, loss 0.8451951146125793, R2 0.6396098732948303\n",
      "Eval loss 0.8822849988937378, R2 0.6428479552268982\n",
      "epoch 1469, loss 0.845095694065094, R2 0.6396925449371338\n",
      "Eval loss 0.8821697235107422, R2 0.642937421798706\n",
      "epoch 1470, loss 0.844996452331543, R2 0.6397749781608582\n",
      "Eval loss 0.88205486536026, R2 0.64302659034729\n",
      "epoch 1471, loss 0.8448976278305054, R2 0.6398572325706482\n",
      "Eval loss 0.8819401860237122, R2 0.6431155800819397\n",
      "epoch 1472, loss 0.8447989821434021, R2 0.6399393081665039\n",
      "Eval loss 0.8818257451057434, R2 0.643204391002655\n",
      "epoch 1473, loss 0.8447006344795227, R2 0.6400213241577148\n",
      "Eval loss 0.8817116618156433, R2 0.6432930827140808\n",
      "epoch 1474, loss 0.8446024656295776, R2 0.6401030421257019\n",
      "Eval loss 0.8815977573394775, R2 0.6433815360069275\n",
      "epoch 1475, loss 0.8445045948028564, R2 0.6401846408843994\n",
      "Eval loss 0.8814842104911804, R2 0.6434697508811951\n",
      "epoch 1476, loss 0.8444069623947144, R2 0.6402660012245178\n",
      "Eval loss 0.8813709020614624, R2 0.6435579061508179\n",
      "epoch 1477, loss 0.8443095684051514, R2 0.6403472423553467\n",
      "Eval loss 0.8812578320503235, R2 0.643645703792572\n",
      "epoch 1478, loss 0.844212532043457, R2 0.6404281854629517\n",
      "Eval loss 0.8811450004577637, R2 0.6437333822250366\n",
      "epoch 1479, loss 0.8441156148910522, R2 0.6405090689659119\n",
      "Eval loss 0.8810324668884277, R2 0.6438210010528564\n",
      "epoch 1480, loss 0.8440189957618713, R2 0.640589714050293\n",
      "Eval loss 0.8809201717376709, R2 0.6439082622528076\n",
      "epoch 1481, loss 0.8439224362373352, R2 0.6406701803207397\n",
      "Eval loss 0.8808082342147827, R2 0.6439953446388245\n",
      "epoch 1482, loss 0.8438263535499573, R2 0.6407504677772522\n",
      "Eval loss 0.8806964755058289, R2 0.6440823078155518\n",
      "epoch 1483, loss 0.8437303900718689, R2 0.6408306360244751\n",
      "Eval loss 0.8805849552154541, R2 0.6441690921783447\n",
      "epoch 1484, loss 0.8436347842216492, R2 0.6409106850624084\n",
      "Eval loss 0.8804737329483032, R2 0.6442556977272034\n",
      "epoch 1485, loss 0.8435392379760742, R2 0.6409903168678284\n",
      "Eval loss 0.8803627490997314, R2 0.6443421244621277\n",
      "epoch 1486, loss 0.8434440493583679, R2 0.6410699486732483\n",
      "Eval loss 0.8802521228790283, R2 0.6444283127784729\n",
      "epoch 1487, loss 0.8433490991592407, R2 0.6411493420600891\n",
      "Eval loss 0.8801416158676147, R2 0.6445143818855286\n",
      "epoch 1488, loss 0.8432544469833374, R2 0.6412286162376404\n",
      "Eval loss 0.880031406879425, R2 0.6446002125740051\n",
      "epoch 1489, loss 0.8431599140167236, R2 0.6413076519966125\n",
      "Eval loss 0.8799214959144592, R2 0.6446858644485474\n",
      "epoch 1490, loss 0.843065619468689, R2 0.6413865685462952\n",
      "Eval loss 0.8798119425773621, R2 0.6447713971138\n",
      "epoch 1491, loss 0.8429715633392334, R2 0.6414652466773987\n",
      "Eval loss 0.8797025084495544, R2 0.6448566913604736\n",
      "epoch 1492, loss 0.8428778648376465, R2 0.6415438055992126\n",
      "Eval loss 0.8795933127403259, R2 0.6449418067932129\n",
      "epoch 1493, loss 0.8427842855453491, R2 0.6416221261024475\n",
      "Eval loss 0.8794843554496765, R2 0.6450268030166626\n",
      "epoch 1494, loss 0.8426909446716309, R2 0.6417003273963928\n",
      "Eval loss 0.8793758153915405, R2 0.6451115012168884\n",
      "epoch 1495, loss 0.842598021030426, R2 0.6417784094810486\n",
      "Eval loss 0.8792673945426941, R2 0.6451961994171143\n",
      "epoch 1496, loss 0.8425050377845764, R2 0.6418561935424805\n",
      "Eval loss 0.8791592717170715, R2 0.6452804803848267\n",
      "epoch 1497, loss 0.8424124717712402, R2 0.6419338583946228\n",
      "Eval loss 0.8790513873100281, R2 0.6453647017478943\n",
      "epoch 1498, loss 0.8423201441764832, R2 0.6420114040374756\n",
      "Eval loss 0.8789437413215637, R2 0.6454488635063171\n",
      "epoch 1499, loss 0.8422278165817261, R2 0.6420886516571045\n",
      "Eval loss 0.8788362741470337, R2 0.6455327272415161\n",
      "epoch 1500, loss 0.842136025428772, R2 0.6421658396720886\n",
      "Eval loss 0.8787292242050171, R2 0.6456164717674255\n",
      "epoch 1501, loss 0.8420444130897522, R2 0.6422428488731384\n",
      "Eval loss 0.8786223530769348, R2 0.6456999778747559\n",
      "epoch 1502, loss 0.8419528603553772, R2 0.6423196792602539\n",
      "Eval loss 0.8785156011581421, R2 0.6457833051681519\n",
      "epoch 1503, loss 0.8418616652488708, R2 0.6423962712287903\n",
      "Eval loss 0.8784092664718628, R2 0.6458664536476135\n",
      "epoch 1504, loss 0.841770589351654, R2 0.6424728035926819\n",
      "Eval loss 0.8783031105995178, R2 0.6459494829177856\n",
      "epoch 1505, loss 0.8416798710823059, R2 0.6425490975379944\n",
      "Eval loss 0.8781972527503967, R2 0.6460323333740234\n",
      "epoch 1506, loss 0.8415893316268921, R2 0.6426252126693726\n",
      "Eval loss 0.8780916333198547, R2 0.6461150050163269\n",
      "epoch 1507, loss 0.8414990901947021, R2 0.6427012085914612\n",
      "Eval loss 0.8779861330986023, R2 0.646197497844696\n",
      "epoch 1508, loss 0.8414090275764465, R2 0.6427770256996155\n",
      "Eval loss 0.8778810501098633, R2 0.6462797522544861\n",
      "epoch 1509, loss 0.8413190841674805, R2 0.6428526639938354\n",
      "Eval loss 0.8777760863304138, R2 0.6463618278503418\n",
      "epoch 1510, loss 0.8412293791770935, R2 0.6429281830787659\n",
      "Eval loss 0.8776714205741882, R2 0.6464439034461975\n",
      "epoch 1511, loss 0.8411400318145752, R2 0.6430034041404724\n",
      "Eval loss 0.8775670528411865, R2 0.6465256214141846\n",
      "epoch 1512, loss 0.8410508036613464, R2 0.6430785655975342\n",
      "Eval loss 0.8774628639221191, R2 0.6466072797775269\n",
      "epoch 1513, loss 0.8409618139266968, R2 0.6431535482406616\n",
      "Eval loss 0.8773588538169861, R2 0.64668869972229\n",
      "epoch 1514, loss 0.840873122215271, R2 0.6432283520698547\n",
      "Eval loss 0.8772551417350769, R2 0.6467699408531189\n",
      "epoch 1515, loss 0.8407846093177795, R2 0.6433030366897583\n",
      "Eval loss 0.877151608467102, R2 0.6468510627746582\n",
      "epoch 1516, loss 0.840696394443512, R2 0.643377423286438\n",
      "Eval loss 0.8770484924316406, R2 0.646932065486908\n",
      "epoch 1517, loss 0.8406081199645996, R2 0.6434517502784729\n",
      "Eval loss 0.8769454956054688, R2 0.6470127701759338\n",
      "epoch 1518, loss 0.8405203223228455, R2 0.6435259580612183\n",
      "Eval loss 0.8768427968025208, R2 0.6470933556556702\n",
      "epoch 1519, loss 0.8404327034950256, R2 0.6435999274253845\n",
      "Eval loss 0.8767402172088623, R2 0.6471738219261169\n",
      "epoch 1520, loss 0.8403452634811401, R2 0.6436737179756165\n",
      "Eval loss 0.8766379356384277, R2 0.6472540497779846\n",
      "epoch 1521, loss 0.8402580618858337, R2 0.6437473893165588\n",
      "Eval loss 0.876535952091217, R2 0.647334098815918\n",
      "epoch 1522, loss 0.8401709794998169, R2 0.6438208818435669\n",
      "Eval loss 0.8764341473579407, R2 0.6474140286445618\n",
      "epoch 1523, loss 0.8400842547416687, R2 0.6438942551612854\n",
      "Eval loss 0.8763325810432434, R2 0.647493839263916\n",
      "epoch 1524, loss 0.8399977087974548, R2 0.6439674496650696\n",
      "Eval loss 0.8762311935424805, R2 0.6475734114646912\n",
      "epoch 1525, loss 0.8399112820625305, R2 0.6440404057502747\n",
      "Eval loss 0.8761301636695862, R2 0.6476527452468872\n",
      "epoch 1526, loss 0.8398252725601196, R2 0.644113302230835\n",
      "Eval loss 0.8760291934013367, R2 0.6477320194244385\n",
      "epoch 1527, loss 0.8397393226623535, R2 0.6441859602928162\n",
      "Eval loss 0.8759286403656006, R2 0.6478111147880554\n",
      "epoch 1528, loss 0.8396536111831665, R2 0.6442585587501526\n",
      "Eval loss 0.8758281469345093, R2 0.6478900909423828\n",
      "epoch 1529, loss 0.8395680785179138, R2 0.6443309187889099\n",
      "Eval loss 0.8757280111312866, R2 0.6479687690734863\n",
      "epoch 1530, loss 0.8394827842712402, R2 0.6444031000137329\n",
      "Eval loss 0.8756281137466431, R2 0.6480475068092346\n",
      "epoch 1531, loss 0.8393977284431458, R2 0.6444752216339111\n",
      "Eval loss 0.8755283355712891, R2 0.6481258869171143\n",
      "epoch 1532, loss 0.8393128514289856, R2 0.6445470452308655\n",
      "Eval loss 0.8754287958145142, R2 0.6482040882110596\n",
      "epoch 1533, loss 0.8392281532287598, R2 0.6446188688278198\n",
      "Eval loss 0.8753295540809631, R2 0.6482822895050049\n",
      "epoch 1534, loss 0.8391437530517578, R2 0.6446903944015503\n",
      "Eval loss 0.8752304911613464, R2 0.6483601927757263\n",
      "epoch 1535, loss 0.8390594720840454, R2 0.644761860370636\n",
      "Eval loss 0.8751317262649536, R2 0.6484379172325134\n",
      "epoch 1536, loss 0.8389754891395569, R2 0.6448331475257874\n",
      "Eval loss 0.8750331401824951, R2 0.6485155820846558\n",
      "epoch 1537, loss 0.8388916850090027, R2 0.6449043154716492\n",
      "Eval loss 0.8749348521232605, R2 0.648593008518219\n",
      "epoch 1538, loss 0.8388080596923828, R2 0.6449752449989319\n",
      "Eval loss 0.8748366236686707, R2 0.6486703157424927\n",
      "epoch 1539, loss 0.8387246131896973, R2 0.6450461149215698\n",
      "Eval loss 0.8747388124465942, R2 0.6487473845481873\n",
      "epoch 1540, loss 0.8386414051055908, R2 0.6451167464256287\n",
      "Eval loss 0.8746411800384521, R2 0.6488244533538818\n",
      "epoch 1541, loss 0.8385584354400635, R2 0.6451871991157532\n",
      "Eval loss 0.8745436668395996, R2 0.6489012241363525\n",
      "epoch 1542, loss 0.8384757041931152, R2 0.6452575325965881\n",
      "Eval loss 0.8744463920593262, R2 0.6489778757095337\n",
      "epoch 1543, loss 0.8383929133415222, R2 0.6453277468681335\n",
      "Eval loss 0.8743494153022766, R2 0.6490543484687805\n",
      "epoch 1544, loss 0.838310718536377, R2 0.6453979015350342\n",
      "Eval loss 0.8742525577545166, R2 0.6491307616233826\n",
      "epoch 1545, loss 0.8382285237312317, R2 0.6454676985740662\n",
      "Eval loss 0.8741559982299805, R2 0.6492068767547607\n",
      "epoch 1546, loss 0.8381465077400208, R2 0.6455374956130981\n",
      "Eval loss 0.8740596771240234, R2 0.6492829322814941\n",
      "epoch 1547, loss 0.8380647301673889, R2 0.645607054233551\n",
      "Eval loss 0.8739635944366455, R2 0.6493587493896484\n",
      "epoch 1548, loss 0.8379831314086914, R2 0.6456764340400696\n",
      "Eval loss 0.8738676905632019, R2 0.649434506893158\n",
      "epoch 1549, loss 0.8379018306732178, R2 0.6457458138465881\n",
      "Eval loss 0.8737719058990479, R2 0.6495099663734436\n",
      "epoch 1550, loss 0.8378206491470337, R2 0.645814836025238\n",
      "Eval loss 0.8736764788627625, R2 0.649585485458374\n",
      "epoch 1551, loss 0.8377397060394287, R2 0.6458837985992432\n",
      "Eval loss 0.8735813498497009, R2 0.6496606469154358\n",
      "epoch 1552, loss 0.8376588225364685, R2 0.6459526419639587\n",
      "Eval loss 0.8734861612319946, R2 0.649735689163208\n",
      "epoch 1553, loss 0.837578296661377, R2 0.6460213661193848\n",
      "Eval loss 0.873391330242157, R2 0.6498106122016907\n",
      "epoch 1554, loss 0.837497889995575, R2 0.6460898518562317\n",
      "Eval loss 0.873296856880188, R2 0.6498854160308838\n",
      "epoch 1555, loss 0.8374177813529968, R2 0.6461583971977234\n",
      "Eval loss 0.8732024431228638, R2 0.6499599814414978\n",
      "epoch 1556, loss 0.8373377919197083, R2 0.6462266445159912\n",
      "Eval loss 0.8731082081794739, R2 0.650034487247467\n",
      "epoch 1557, loss 0.8372581005096436, R2 0.6462945938110352\n",
      "Eval loss 0.8730142116546631, R2 0.6501087546348572\n",
      "epoch 1558, loss 0.8371783494949341, R2 0.6463626623153687\n",
      "Eval loss 0.8729204535484314, R2 0.6501829624176025\n",
      "epoch 1559, loss 0.8370990753173828, R2 0.6464304327964783\n",
      "Eval loss 0.8728269934654236, R2 0.6502569913864136\n",
      "epoch 1560, loss 0.8370198011398315, R2 0.6464980244636536\n",
      "Eval loss 0.8727336525917053, R2 0.6503307819366455\n",
      "epoch 1561, loss 0.8369408845901489, R2 0.6465654969215393\n",
      "Eval loss 0.8726404309272766, R2 0.6504044532775879\n",
      "epoch 1562, loss 0.8368620276451111, R2 0.6466329097747803\n",
      "Eval loss 0.8725476861000061, R2 0.650477945804596\n",
      "epoch 1563, loss 0.8367834091186523, R2 0.6467001438140869\n",
      "Eval loss 0.8724548816680908, R2 0.6505513787269592\n",
      "epoch 1564, loss 0.8367049694061279, R2 0.6467670798301697\n",
      "Eval loss 0.872362494468689, R2 0.6506246328353882\n",
      "epoch 1565, loss 0.8366267681121826, R2 0.6468340158462524\n",
      "Eval loss 0.8722702264785767, R2 0.6506977677345276\n",
      "epoch 1566, loss 0.8365487456321716, R2 0.6469008922576904\n",
      "Eval loss 0.8721780776977539, R2 0.6507706642150879\n",
      "epoch 1567, loss 0.8364707231521606, R2 0.6469674706459045\n",
      "Eval loss 0.8720862865447998, R2 0.6508434414863586\n",
      "epoch 1568, loss 0.8363931775093079, R2 0.6470339298248291\n",
      "Eval loss 0.87199467420578, R2 0.6509160995483398\n",
      "epoch 1569, loss 0.8363156914710999, R2 0.6471002697944641\n",
      "Eval loss 0.8719032406806946, R2 0.6509885787963867\n",
      "epoch 1570, loss 0.836238443851471, R2 0.6471664905548096\n",
      "Eval loss 0.8718119859695435, R2 0.651060938835144\n",
      "epoch 1571, loss 0.8361613154411316, R2 0.6472325325012207\n",
      "Eval loss 0.8717210292816162, R2 0.651133120059967\n",
      "epoch 1572, loss 0.8360844254493713, R2 0.6472984552383423\n",
      "Eval loss 0.8716301321983337, R2 0.6512051224708557\n",
      "epoch 1573, loss 0.836007833480835, R2 0.6473643183708191\n",
      "Eval loss 0.8715395331382751, R2 0.6512770056724548\n",
      "epoch 1574, loss 0.8359311819076538, R2 0.647429883480072\n",
      "Eval loss 0.8714492321014404, R2 0.6513488292694092\n",
      "epoch 1575, loss 0.8358549475669861, R2 0.6474953293800354\n",
      "Eval loss 0.8713589310646057, R2 0.6514204144477844\n",
      "epoch 1576, loss 0.8357786536216736, R2 0.6475606560707092\n",
      "Eval loss 0.8712688088417053, R2 0.6514918804168701\n",
      "epoch 1577, loss 0.835702657699585, R2 0.6476259231567383\n",
      "Eval loss 0.8711791634559631, R2 0.6515631079673767\n",
      "epoch 1578, loss 0.8356269598007202, R2 0.6476909518241882\n",
      "Eval loss 0.871089518070221, R2 0.6516343355178833\n",
      "epoch 1579, loss 0.8355513215065002, R2 0.6477558612823486\n",
      "Eval loss 0.8710001707077026, R2 0.6517053246498108\n",
      "epoch 1580, loss 0.8354759216308594, R2 0.6478206515312195\n",
      "Eval loss 0.8709109425544739, R2 0.6517761945724487\n",
      "epoch 1581, loss 0.8354007601737976, R2 0.647885262966156\n",
      "Eval loss 0.8708218932151794, R2 0.6518468856811523\n",
      "epoch 1582, loss 0.8353256583213806, R2 0.6479498147964478\n",
      "Eval loss 0.8707331418991089, R2 0.6519174575805664\n",
      "epoch 1583, loss 0.835250735282898, R2 0.6480141282081604\n",
      "Eval loss 0.8706445097923279, R2 0.6519879102706909\n",
      "epoch 1584, loss 0.8351761102676392, R2 0.6480785012245178\n",
      "Eval loss 0.870556116104126, R2 0.6520581841468811\n",
      "epoch 1585, loss 0.8351015448570251, R2 0.6481425166130066\n",
      "Eval loss 0.8704679608345032, R2 0.652128279209137\n",
      "epoch 1586, loss 0.835027277469635, R2 0.6482064723968506\n",
      "Eval loss 0.8703799843788147, R2 0.652198314666748\n",
      "epoch 1587, loss 0.8349531292915344, R2 0.6482702493667603\n",
      "Eval loss 0.8702921867370605, R2 0.6522681713104248\n",
      "epoch 1588, loss 0.8348792791366577, R2 0.6483339667320251\n",
      "Eval loss 0.8702045679092407, R2 0.6523379683494568\n",
      "epoch 1589, loss 0.8348053693771362, R2 0.6483974456787109\n",
      "Eval loss 0.8701171875, R2 0.6524075269699097\n",
      "epoch 1590, loss 0.8347318172454834, R2 0.6484609246253967\n",
      "Eval loss 0.8700299859046936, R2 0.6524769067764282\n",
      "epoch 1591, loss 0.8346583843231201, R2 0.6485242247581482\n",
      "Eval loss 0.8699429035186768, R2 0.6525461673736572\n",
      "epoch 1592, loss 0.8345851302146912, R2 0.6485872864723206\n",
      "Eval loss 0.8698561787605286, R2 0.6526154279708862\n",
      "epoch 1593, loss 0.8345120549201965, R2 0.6486502289772034\n",
      "Eval loss 0.8697695136070251, R2 0.6526843309402466\n",
      "epoch 1594, loss 0.8344390988349915, R2 0.6487131714820862\n",
      "Eval loss 0.8696830868721008, R2 0.6527531743049622\n",
      "epoch 1595, loss 0.834366500377655, R2 0.6487759351730347\n",
      "Eval loss 0.8695967793464661, R2 0.652821958065033\n",
      "epoch 1596, loss 0.8342939615249634, R2 0.648838460445404\n",
      "Eval loss 0.8695107698440552, R2 0.6528904438018799\n",
      "epoch 1597, loss 0.834221601486206, R2 0.6489009261131287\n",
      "Eval loss 0.8694248795509338, R2 0.6529590487480164\n",
      "epoch 1598, loss 0.8341493010520935, R2 0.648963212966919\n",
      "Eval loss 0.8693392872810364, R2 0.6530272960662842\n",
      "epoch 1599, loss 0.8340774178504944, R2 0.6490254998207092\n",
      "Eval loss 0.869253933429718, R2 0.6530954241752625\n",
      "epoch 1600, loss 0.8340054750442505, R2 0.6490874886512756\n",
      "Eval loss 0.8691685199737549, R2 0.653163492679596\n",
      "epoch 1601, loss 0.8339337706565857, R2 0.649149477481842\n",
      "Eval loss 0.8690834641456604, R2 0.6532313227653503\n",
      "epoch 1602, loss 0.8338623642921448, R2 0.6492111682891846\n",
      "Eval loss 0.8689985871315002, R2 0.6532991528511047\n",
      "epoch 1603, loss 0.8337910175323486, R2 0.6492728590965271\n",
      "Eval loss 0.8689138889312744, R2 0.65336674451828\n",
      "epoch 1604, loss 0.8337199687957764, R2 0.6493343710899353\n",
      "Eval loss 0.8688293695449829, R2 0.6534342169761658\n",
      "epoch 1605, loss 0.8336489796638489, R2 0.6493958234786987\n",
      "Eval loss 0.8687450289726257, R2 0.653501570224762\n",
      "epoch 1606, loss 0.8335781097412109, R2 0.6494570374488831\n",
      "Eval loss 0.8686608672142029, R2 0.653568685054779\n",
      "epoch 1607, loss 0.8335074782371521, R2 0.6495182514190674\n",
      "Eval loss 0.8685768842697144, R2 0.6536357402801514\n",
      "epoch 1608, loss 0.8334369659423828, R2 0.6495792269706726\n",
      "Eval loss 0.8684930801391602, R2 0.6537026762962341\n",
      "epoch 1609, loss 0.8333667516708374, R2 0.6496400833129883\n",
      "Eval loss 0.8684096336364746, R2 0.6537694931030273\n",
      "epoch 1610, loss 0.8332965970039368, R2 0.6497008800506592\n",
      "Eval loss 0.8683261871337891, R2 0.653836190700531\n",
      "epoch 1611, loss 0.8332266807556152, R2 0.649761438369751\n",
      "Eval loss 0.8682429194450378, R2 0.6539026498794556\n",
      "epoch 1612, loss 0.8331568837165833, R2 0.6498219966888428\n",
      "Eval loss 0.8681598901748657, R2 0.6539690494537354\n",
      "epoch 1613, loss 0.8330872654914856, R2 0.6498823165893555\n",
      "Eval loss 0.8680770397186279, R2 0.6540353298187256\n",
      "epoch 1614, loss 0.8330177664756775, R2 0.6499424576759338\n",
      "Eval loss 0.8679944276809692, R2 0.6541014313697815\n",
      "epoch 1615, loss 0.8329485058784485, R2 0.650002658367157\n",
      "Eval loss 0.8679119348526001, R2 0.6541674137115479\n",
      "epoch 1616, loss 0.8328794240951538, R2 0.650062620639801\n",
      "Eval loss 0.8678296208381653, R2 0.6542332172393799\n",
      "epoch 1617, loss 0.8328104615211487, R2 0.6501224637031555\n",
      "Eval loss 0.8677474856376648, R2 0.6542989611625671\n",
      "epoch 1618, loss 0.8327417373657227, R2 0.6501821875572205\n",
      "Eval loss 0.8676655888557434, R2 0.6543645262718201\n",
      "epoch 1619, loss 0.8326731324195862, R2 0.6502417922019958\n",
      "Eval loss 0.8675838112831116, R2 0.6544300317764282\n",
      "epoch 1620, loss 0.8326045870780945, R2 0.6503012180328369\n",
      "Eval loss 0.8675023317337036, R2 0.6544952988624573\n",
      "epoch 1621, loss 0.8325363993644714, R2 0.6503605842590332\n",
      "Eval loss 0.8674207925796509, R2 0.6545605063438416\n",
      "epoch 1622, loss 0.8324682116508484, R2 0.6504198312759399\n",
      "Eval loss 0.8673396706581116, R2 0.6546255350112915\n",
      "epoch 1623, loss 0.8324002027511597, R2 0.6504788398742676\n",
      "Eval loss 0.8672587275505066, R2 0.6546905040740967\n",
      "epoch 1624, loss 0.8323323726654053, R2 0.6505377888679504\n",
      "Eval loss 0.8671777248382568, R2 0.6547552347183228\n",
      "epoch 1625, loss 0.8322647213935852, R2 0.6505966782569885\n",
      "Eval loss 0.8670971393585205, R2 0.654819905757904\n",
      "epoch 1626, loss 0.8321974277496338, R2 0.6506553888320923\n",
      "Eval loss 0.867016613483429, R2 0.6548845171928406\n",
      "epoch 1627, loss 0.832129955291748, R2 0.6507139205932617\n",
      "Eval loss 0.8669363260269165, R2 0.654948890209198\n",
      "epoch 1628, loss 0.832062840461731, R2 0.6507724523544312\n",
      "Eval loss 0.8668561577796936, R2 0.6550131440162659\n",
      "epoch 1629, loss 0.8319959044456482, R2 0.6508306860923767\n",
      "Eval loss 0.8667762875556946, R2 0.6550772786140442\n",
      "epoch 1630, loss 0.8319291472434998, R2 0.650888979434967\n",
      "Eval loss 0.8666964769363403, R2 0.655141294002533\n",
      "epoch 1631, loss 0.8318623900413513, R2 0.650947093963623\n",
      "Eval loss 0.8666168451309204, R2 0.6552051305770874\n",
      "epoch 1632, loss 0.8317958116531372, R2 0.6510050296783447\n",
      "Eval loss 0.8665374517440796, R2 0.6552689671516418\n",
      "epoch 1633, loss 0.8317295908927917, R2 0.6510629057884216\n",
      "Eval loss 0.8664581775665283, R2 0.6553325653076172\n",
      "epoch 1634, loss 0.8316633701324463, R2 0.6511206030845642\n",
      "Eval loss 0.8663790822029114, R2 0.655396044254303\n",
      "epoch 1635, loss 0.8315973281860352, R2 0.6511781811714172\n",
      "Eval loss 0.8663002252578735, R2 0.655459463596344\n",
      "epoch 1636, loss 0.8315314054489136, R2 0.6512356400489807\n",
      "Eval loss 0.8662214279174805, R2 0.6555227041244507\n",
      "epoch 1637, loss 0.8314657211303711, R2 0.6512930393218994\n",
      "Eval loss 0.8661429286003113, R2 0.6555858254432678\n",
      "epoch 1638, loss 0.8314002156257629, R2 0.6513502597808838\n",
      "Eval loss 0.8660644292831421, R2 0.6556487679481506\n",
      "epoch 1639, loss 0.8313347101211548, R2 0.6514074206352234\n",
      "Eval loss 0.8659863471984863, R2 0.6557115912437439\n",
      "epoch 1640, loss 0.8312695026397705, R2 0.6514643430709839\n",
      "Eval loss 0.8659082055091858, R2 0.6557743549346924\n",
      "epoch 1641, loss 0.8312044739723206, R2 0.6515212655067444\n",
      "Eval loss 0.8658303618431091, R2 0.6558369398117065\n",
      "epoch 1642, loss 0.8311395645141602, R2 0.6515780091285706\n",
      "Eval loss 0.865752637386322, R2 0.6558994650840759\n",
      "epoch 1643, loss 0.8310747742652893, R2 0.651634693145752\n",
      "Eval loss 0.8656752705574036, R2 0.6559617519378662\n",
      "epoch 1644, loss 0.831010103225708, R2 0.6516910791397095\n",
      "Eval loss 0.8655978441238403, R2 0.6560240387916565\n",
      "epoch 1645, loss 0.8309456706047058, R2 0.6517475843429565\n",
      "Eval loss 0.8655205965042114, R2 0.6560860872268677\n",
      "epoch 1646, loss 0.8308812975883484, R2 0.6518039107322693\n",
      "Eval loss 0.8654435276985168, R2 0.6561481952667236\n",
      "epoch 1647, loss 0.8308171033859253, R2 0.6518599987030029\n",
      "Eval loss 0.8653666377067566, R2 0.6562100052833557\n",
      "epoch 1648, loss 0.8307530879974365, R2 0.651915967464447\n",
      "Eval loss 0.865290105342865, R2 0.656271755695343\n",
      "epoch 1649, loss 0.8306892514228821, R2 0.6519719362258911\n",
      "Eval loss 0.8652135729789734, R2 0.656333327293396\n",
      "epoch 1650, loss 0.8306255340576172, R2 0.6520277261734009\n",
      "Eval loss 0.8651371598243713, R2 0.6563948392868042\n",
      "epoch 1651, loss 0.8305619955062866, R2 0.6520833969116211\n",
      "Eval loss 0.8650609850883484, R2 0.6564562320709229\n",
      "epoch 1652, loss 0.8304985761642456, R2 0.6521389484405518\n",
      "Eval loss 0.8649851083755493, R2 0.6565174460411072\n",
      "epoch 1653, loss 0.8304353952407837, R2 0.6521944403648376\n",
      "Eval loss 0.8649091124534607, R2 0.656578540802002\n",
      "epoch 1654, loss 0.830372154712677, R2 0.6522497534751892\n",
      "Eval loss 0.8648334741592407, R2 0.6566395163536072\n",
      "epoch 1655, loss 0.8303093314170837, R2 0.6523049473762512\n",
      "Eval loss 0.8647580146789551, R2 0.6567003726959229\n",
      "epoch 1656, loss 0.8302465081214905, R2 0.6523600220680237\n",
      "Eval loss 0.8646825551986694, R2 0.6567611694335938\n",
      "epoch 1657, loss 0.830183744430542, R2 0.6524150967597961\n",
      "Eval loss 0.8646073937416077, R2 0.6568217277526855\n",
      "epoch 1658, loss 0.8301212787628174, R2 0.6524699926376343\n",
      "Eval loss 0.8645322918891907, R2 0.6568822860717773\n",
      "epoch 1659, loss 0.8300589323043823, R2 0.6525246500968933\n",
      "Eval loss 0.8644574880599976, R2 0.6569427251815796\n",
      "epoch 1660, loss 0.8299968242645264, R2 0.6525793671607971\n",
      "Eval loss 0.864382803440094, R2 0.6570029854774475\n",
      "epoch 1661, loss 0.8299346566200256, R2 0.6526339054107666\n",
      "Eval loss 0.86430823802948, R2 0.6570631265640259\n",
      "epoch 1662, loss 0.8298726677894592, R2 0.6526882648468018\n",
      "Eval loss 0.8642339110374451, R2 0.6571231484413147\n",
      "epoch 1663, loss 0.8298109769821167, R2 0.6527426242828369\n",
      "Eval loss 0.8641596436500549, R2 0.657183051109314\n",
      "epoch 1664, loss 0.8297494649887085, R2 0.6527968049049377\n",
      "Eval loss 0.8640855550765991, R2 0.6572428345680237\n",
      "epoch 1665, loss 0.8296878337860107, R2 0.652850866317749\n",
      "Eval loss 0.8640117049217224, R2 0.6573024988174438\n",
      "epoch 1666, loss 0.8296266198158264, R2 0.652904748916626\n",
      "Eval loss 0.8639379143714905, R2 0.6573620438575745\n",
      "epoch 1667, loss 0.8295653462409973, R2 0.6529586315155029\n",
      "Eval loss 0.8638644218444824, R2 0.6574215292930603\n",
      "epoch 1668, loss 0.8295043706893921, R2 0.6530123353004456\n",
      "Eval loss 0.8637909889221191, R2 0.6574807167053223\n",
      "epoch 1669, loss 0.8294433355331421, R2 0.6530659794807434\n",
      "Eval loss 0.8637178540229797, R2 0.6575400233268738\n",
      "epoch 1670, loss 0.8293826580047607, R2 0.6531195044517517\n",
      "Eval loss 0.8636447191238403, R2 0.6575990915298462\n",
      "epoch 1671, loss 0.829322099685669, R2 0.6531729102134705\n",
      "Eval loss 0.8635717034339905, R2 0.6576581001281738\n",
      "epoch 1672, loss 0.8292615413665771, R2 0.6532261371612549\n",
      "Eval loss 0.8634989261627197, R2 0.6577168107032776\n",
      "epoch 1673, loss 0.829201340675354, R2 0.6532793641090393\n",
      "Eval loss 0.8634263277053833, R2 0.6577755808830261\n",
      "epoch 1674, loss 0.8291411399841309, R2 0.6533324122428894\n",
      "Eval loss 0.8633538484573364, R2 0.6578342318534851\n",
      "epoch 1675, loss 0.8290809392929077, R2 0.65338534116745\n",
      "Eval loss 0.8632816076278687, R2 0.6578927636146545\n",
      "epoch 1676, loss 0.829021155834198, R2 0.653438150882721\n",
      "Eval loss 0.8632093667984009, R2 0.6579511165618896\n",
      "epoch 1677, loss 0.8289613723754883, R2 0.6534909009933472\n",
      "Eval loss 0.863137423992157, R2 0.6580094695091248\n",
      "epoch 1678, loss 0.8289017081260681, R2 0.6535435318946838\n",
      "Eval loss 0.8630656003952026, R2 0.6580674648284912\n",
      "epoch 1679, loss 0.8288423418998718, R2 0.653596043586731\n",
      "Eval loss 0.8629939556121826, R2 0.6581254601478577\n",
      "epoch 1680, loss 0.828782856464386, R2 0.6536484360694885\n",
      "Eval loss 0.8629224300384521, R2 0.6581835150718689\n",
      "epoch 1681, loss 0.8287237286567688, R2 0.6537007689476013\n",
      "Eval loss 0.8628510236740112, R2 0.6582412719726562\n",
      "epoch 1682, loss 0.8286646604537964, R2 0.6537529230117798\n",
      "Eval loss 0.8627798557281494, R2 0.658298909664154\n",
      "epoch 1683, loss 0.8286056518554688, R2 0.6538049578666687\n",
      "Eval loss 0.8627088069915771, R2 0.6583564877510071\n",
      "epoch 1684, loss 0.828546941280365, R2 0.6538569927215576\n",
      "Eval loss 0.8626377582550049, R2 0.6584139466285706\n",
      "epoch 1685, loss 0.8284883499145508, R2 0.6539087295532227\n",
      "Eval loss 0.862567126750946, R2 0.6584712862968445\n",
      "epoch 1686, loss 0.8284298777580261, R2 0.653960645198822\n",
      "Eval loss 0.862496554851532, R2 0.6585285663604736\n",
      "epoch 1687, loss 0.8283714652061462, R2 0.654012143611908\n",
      "Eval loss 0.8624260425567627, R2 0.6585856676101685\n",
      "epoch 1688, loss 0.8283132314682007, R2 0.6540637612342834\n",
      "Eval loss 0.8623557090759277, R2 0.6586427092552185\n",
      "epoch 1689, loss 0.8282551765441895, R2 0.6541151404380798\n",
      "Eval loss 0.8622856140136719, R2 0.6586995720863342\n",
      "epoch 1690, loss 0.8281970620155334, R2 0.6541664600372314\n",
      "Eval loss 0.8622155785560608, R2 0.6587563157081604\n",
      "epoch 1691, loss 0.8281393647193909, R2 0.6542176604270935\n",
      "Eval loss 0.8621457815170288, R2 0.658812940120697\n",
      "epoch 1692, loss 0.8280816674232483, R2 0.6542688012123108\n",
      "Eval loss 0.8620759844779968, R2 0.6588695645332336\n",
      "epoch 1693, loss 0.82802414894104, R2 0.6543197631835938\n",
      "Eval loss 0.8620064854621887, R2 0.6589260697364807\n",
      "epoch 1694, loss 0.8279668092727661, R2 0.6543706655502319\n",
      "Eval loss 0.8619370460510254, R2 0.6589823365211487\n",
      "epoch 1695, loss 0.8279094696044922, R2 0.6544214487075806\n",
      "Eval loss 0.8618679046630859, R2 0.6590386033058167\n",
      "epoch 1696, loss 0.8278522491455078, R2 0.6544721722602844\n",
      "Eval loss 0.8617988228797913, R2 0.6590947508811951\n",
      "epoch 1697, loss 0.8277952671051025, R2 0.654522716999054\n",
      "Eval loss 0.8617297410964966, R2 0.6591506600379944\n",
      "epoch 1698, loss 0.8277384638786316, R2 0.6545732021331787\n",
      "Eval loss 0.8616610169410706, R2 0.6592065691947937\n",
      "epoch 1699, loss 0.8276816606521606, R2 0.6546235680580139\n",
      "Eval loss 0.8615923523902893, R2 0.6592623591423035\n",
      "epoch 1700, loss 0.8276249766349792, R2 0.6546738147735596\n",
      "Eval loss 0.8615238070487976, R2 0.6593180298805237\n",
      "epoch 1701, loss 0.827568531036377, R2 0.6547239422798157\n",
      "Eval loss 0.8614554405212402, R2 0.6593736410140991\n",
      "epoch 1702, loss 0.8275120854377747, R2 0.6547740697860718\n",
      "Eval loss 0.8613872528076172, R2 0.6594290733337402\n",
      "epoch 1703, loss 0.8274559378623962, R2 0.6548240184783936\n",
      "Eval loss 0.8613191843032837, R2 0.659484326839447\n",
      "epoch 1704, loss 0.8273998498916626, R2 0.6548738479614258\n",
      "Eval loss 0.8612512350082397, R2 0.6595396399497986\n",
      "epoch 1705, loss 0.8273439407348633, R2 0.6549234986305237\n",
      "Eval loss 0.8611835241317749, R2 0.659594714641571\n",
      "epoch 1706, loss 0.8272879719734192, R2 0.6549732685089111\n",
      "Eval loss 0.8611158728599548, R2 0.6596497893333435\n",
      "epoch 1707, loss 0.8272324204444885, R2 0.6550227403640747\n",
      "Eval loss 0.8610482811927795, R2 0.6597046852111816\n",
      "epoch 1708, loss 0.8271768689155579, R2 0.6550722122192383\n",
      "Eval loss 0.8609809875488281, R2 0.6597594618797302\n",
      "epoch 1709, loss 0.8271214365959167, R2 0.6551215648651123\n",
      "Eval loss 0.860913872718811, R2 0.6598142385482788\n",
      "epoch 1710, loss 0.8270660638809204, R2 0.6551706790924072\n",
      "Eval loss 0.860846757888794, R2 0.6598687171936035\n",
      "epoch 1711, loss 0.8270109295845032, R2 0.6552198529243469\n",
      "Eval loss 0.8607798218727112, R2 0.6599231958389282\n",
      "epoch 1712, loss 0.8269558548927307, R2 0.6552689075469971\n",
      "Eval loss 0.8607130646705627, R2 0.6599776744842529\n",
      "epoch 1713, loss 0.8269008994102478, R2 0.6553178429603577\n",
      "Eval loss 0.8606463670730591, R2 0.6600318551063538\n",
      "epoch 1714, loss 0.8268461227416992, R2 0.6553666591644287\n",
      "Eval loss 0.8605799078941345, R2 0.6600860357284546\n",
      "epoch 1715, loss 0.826791524887085, R2 0.6554153561592102\n",
      "Eval loss 0.8605135083198547, R2 0.6601400375366211\n",
      "epoch 1716, loss 0.8267370462417603, R2 0.6554639935493469\n",
      "Eval loss 0.860447347164154, R2 0.6601940393447876\n",
      "epoch 1717, loss 0.8266825079917908, R2 0.6555125713348389\n",
      "Eval loss 0.8603813648223877, R2 0.6602478623390198\n",
      "epoch 1718, loss 0.8266282081604004, R2 0.6555610299110413\n",
      "Eval loss 0.8603153228759766, R2 0.6603015661239624\n",
      "epoch 1719, loss 0.8265740275382996, R2 0.6556093096733093\n",
      "Eval loss 0.8602496385574341, R2 0.6603553295135498\n",
      "epoch 1720, loss 0.8265200853347778, R2 0.6556575298309326\n",
      "Eval loss 0.8601839542388916, R2 0.6604087352752686\n",
      "epoch 1721, loss 0.8264662027359009, R2 0.6557056307792664\n",
      "Eval loss 0.8601184487342834, R2 0.6604622006416321\n",
      "epoch 1722, loss 0.8264122605323792, R2 0.6557536721229553\n",
      "Eval loss 0.8600530624389648, R2 0.660515546798706\n",
      "epoch 1723, loss 0.8263587355613708, R2 0.6558015942573547\n",
      "Eval loss 0.8599878549575806, R2 0.6605687737464905\n",
      "epoch 1724, loss 0.8263052105903625, R2 0.6558494567871094\n",
      "Eval loss 0.8599227666854858, R2 0.6606217622756958\n",
      "epoch 1725, loss 0.8262518048286438, R2 0.6558972001075745\n",
      "Eval loss 0.8598578572273254, R2 0.6606748104095459\n",
      "epoch 1726, loss 0.8261985182762146, R2 0.6559447646141052\n",
      "Eval loss 0.859792947769165, R2 0.6607276797294617\n",
      "epoch 1727, loss 0.8261454105377197, R2 0.6559922695159912\n",
      "Eval loss 0.8597283363342285, R2 0.6607804894447327\n",
      "epoch 1728, loss 0.8260923027992249, R2 0.6560397744178772\n",
      "Eval loss 0.8596637845039368, R2 0.6608331799507141\n",
      "epoch 1729, loss 0.8260393142700195, R2 0.6560871005058289\n",
      "Eval loss 0.8595993518829346, R2 0.6608858704566956\n",
      "epoch 1730, loss 0.8259866833686829, R2 0.6561343669891357\n",
      "Eval loss 0.8595351576805115, R2 0.6609382629394531\n",
      "epoch 1731, loss 0.8259339332580566, R2 0.6561815142631531\n",
      "Eval loss 0.8594709634780884, R2 0.6609906554222107\n",
      "epoch 1732, loss 0.8258814215660095, R2 0.6562284827232361\n",
      "Eval loss 0.8594069480895996, R2 0.6610429286956787\n",
      "epoch 1733, loss 0.8258288502693176, R2 0.6562755107879639\n",
      "Eval loss 0.8593431115150452, R2 0.6610950827598572\n",
      "epoch 1734, loss 0.8257766366004944, R2 0.6563224196434021\n",
      "Eval loss 0.859279453754425, R2 0.6611471772193909\n",
      "epoch 1735, loss 0.8257244229316711, R2 0.6563690304756165\n",
      "Eval loss 0.8592157959938049, R2 0.661199152469635\n",
      "epoch 1736, loss 0.8256723880767822, R2 0.6564157605171204\n",
      "Eval loss 0.8591523170471191, R2 0.6612510085105896\n",
      "epoch 1737, loss 0.8256204128265381, R2 0.6564623117446899\n",
      "Eval loss 0.8590890169143677, R2 0.6613028049468994\n",
      "epoch 1738, loss 0.8255685567855835, R2 0.6565088033676147\n",
      "Eval loss 0.8590258359909058, R2 0.6613545417785645\n",
      "epoch 1739, loss 0.8255168795585632, R2 0.6565551161766052\n",
      "Eval loss 0.8589627146720886, R2 0.6614060997962952\n",
      "epoch 1740, loss 0.8254653811454773, R2 0.6566014885902405\n",
      "Eval loss 0.8588998317718506, R2 0.6614575982093811\n",
      "epoch 1741, loss 0.8254138827323914, R2 0.6566476225852966\n",
      "Eval loss 0.8588371872901917, R2 0.6615089774131775\n",
      "epoch 1742, loss 0.825362503528595, R2 0.656693696975708\n",
      "Eval loss 0.8587743043899536, R2 0.6615602374076843\n",
      "epoch 1743, loss 0.8253112435340881, R2 0.6567397117614746\n",
      "Eval loss 0.858711838722229, R2 0.6616113781929016\n",
      "epoch 1744, loss 0.8252601027488708, R2 0.6567856073379517\n",
      "Eval loss 0.858649492263794, R2 0.6616625189781189\n",
      "epoch 1745, loss 0.8252091407775879, R2 0.6568313837051392\n",
      "Eval loss 0.8585872650146484, R2 0.6617134809494019\n",
      "epoch 1746, loss 0.8251583576202393, R2 0.6568771600723267\n",
      "Eval loss 0.8585250973701477, R2 0.6617642641067505\n",
      "epoch 1747, loss 0.8251075148582458, R2 0.6569227576255798\n",
      "Eval loss 0.8584630489349365, R2 0.6618151068687439\n",
      "epoch 1748, loss 0.825056791305542, R2 0.6569682955741882\n",
      "Eval loss 0.8584012985229492, R2 0.661865770816803\n",
      "epoch 1749, loss 0.825006365776062, R2 0.6570137739181519\n",
      "Eval loss 0.8583394885063171, R2 0.6619163155555725\n",
      "epoch 1750, loss 0.824955940246582, R2 0.6570591330528259\n",
      "Eval loss 0.8582778573036194, R2 0.661966860294342\n",
      "epoch 1751, loss 0.8249056935310364, R2 0.6571043133735657\n",
      "Eval loss 0.8582163453102112, R2 0.6620172262191772\n",
      "epoch 1752, loss 0.8248554468154907, R2 0.6571495532989502\n",
      "Eval loss 0.8581551909446716, R2 0.6620674729347229\n",
      "epoch 1753, loss 0.8248053789138794, R2 0.6571945548057556\n",
      "Eval loss 0.8580939173698425, R2 0.6621177196502686\n",
      "epoch 1754, loss 0.8247554898262024, R2 0.6572394967079163\n",
      "Eval loss 0.858032763004303, R2 0.6621677875518799\n",
      "epoch 1755, loss 0.8247057199478149, R2 0.6572844386100769\n",
      "Eval loss 0.8579717874526978, R2 0.6622177958488464\n",
      "epoch 1756, loss 0.8246558904647827, R2 0.6573292016983032\n",
      "Eval loss 0.8579110503196716, R2 0.6622676849365234\n",
      "epoch 1757, loss 0.8246063590049744, R2 0.6573739051818848\n",
      "Eval loss 0.8578501343727112, R2 0.6623175144195557\n",
      "epoch 1758, loss 0.824556827545166, R2 0.6574184894561768\n",
      "Eval loss 0.8577895760536194, R2 0.6623672246932983\n",
      "epoch 1759, loss 0.824507474899292, R2 0.6574630737304688\n",
      "Eval loss 0.8577291369438171, R2 0.6624168753623962\n",
      "epoch 1760, loss 0.8244581818580627, R2 0.6575074791908264\n",
      "Eval loss 0.8576688766479492, R2 0.6624663472175598\n",
      "epoch 1761, loss 0.8244090676307678, R2 0.6575518250465393\n",
      "Eval loss 0.8576086759567261, R2 0.6625157594680786\n",
      "epoch 1762, loss 0.8243598937988281, R2 0.6575960516929626\n",
      "Eval loss 0.8575485348701477, R2 0.6625650525093079\n",
      "epoch 1763, loss 0.8243110775947571, R2 0.6576402187347412\n",
      "Eval loss 0.8574886322021484, R2 0.6626142859458923\n",
      "epoch 1764, loss 0.824262261390686, R2 0.6576842665672302\n",
      "Eval loss 0.857428789138794, R2 0.662663459777832\n",
      "epoch 1765, loss 0.8242136240005493, R2 0.6577282547950745\n",
      "Eval loss 0.8573691248893738, R2 0.6627125144004822\n",
      "epoch 1766, loss 0.8241649270057678, R2 0.6577721238136292\n",
      "Eval loss 0.8573094606399536, R2 0.6627614498138428\n",
      "epoch 1767, loss 0.8241164684295654, R2 0.6578159332275391\n",
      "Eval loss 0.8572500944137573, R2 0.6628103256225586\n",
      "epoch 1768, loss 0.824068009853363, R2 0.6578596234321594\n",
      "Eval loss 0.8571906685829163, R2 0.6628590226173401\n",
      "epoch 1769, loss 0.8240197896957397, R2 0.6579033136367798\n",
      "Eval loss 0.8571315407752991, R2 0.6629076600074768\n",
      "epoch 1770, loss 0.823971688747406, R2 0.6579468250274658\n",
      "Eval loss 0.8570724725723267, R2 0.6629562377929688\n",
      "epoch 1771, loss 0.823923647403717, R2 0.6579902768135071\n",
      "Eval loss 0.8570135235786438, R2 0.6630048155784607\n",
      "epoch 1772, loss 0.8238757252693176, R2 0.6580336093902588\n",
      "Eval loss 0.8569546341896057, R2 0.6630530953407288\n",
      "epoch 1773, loss 0.8238279223442078, R2 0.6580768823623657\n",
      "Eval loss 0.856895923614502, R2 0.6631014347076416\n",
      "epoch 1774, loss 0.8237802982330322, R2 0.6581199765205383\n",
      "Eval loss 0.8568373322486877, R2 0.6631496548652649\n",
      "epoch 1775, loss 0.8237326145172119, R2 0.6581631302833557\n",
      "Eval loss 0.8567789196968079, R2 0.6631977558135986\n",
      "epoch 1776, loss 0.8236851692199707, R2 0.6582061648368835\n",
      "Eval loss 0.856720507144928, R2 0.6632457375526428\n",
      "epoch 1777, loss 0.8236377835273743, R2 0.6582490801811218\n",
      "Eval loss 0.8566622734069824, R2 0.663293719291687\n",
      "epoch 1778, loss 0.8235903978347778, R2 0.6582919359207153\n",
      "Eval loss 0.8566041588783264, R2 0.6633415222167969\n",
      "epoch 1779, loss 0.8235433101654053, R2 0.6583346128463745\n",
      "Eval loss 0.85654616355896, R2 0.663389265537262\n",
      "epoch 1780, loss 0.8234962821006775, R2 0.6583773493766785\n",
      "Eval loss 0.8564882874488831, R2 0.6634368896484375\n",
      "epoch 1781, loss 0.8234491944313049, R2 0.6584198474884033\n",
      "Eval loss 0.8564305901527405, R2 0.6634843945503235\n",
      "epoch 1782, loss 0.8234023451805115, R2 0.6584624648094177\n",
      "Eval loss 0.8563728928565979, R2 0.6635318994522095\n",
      "epoch 1783, loss 0.8233557343482971, R2 0.6585047245025635\n",
      "Eval loss 0.8563154339790344, R2 0.6635792255401611\n",
      "epoch 1784, loss 0.8233091235160828, R2 0.658547043800354\n",
      "Eval loss 0.8562580943107605, R2 0.6636266112327576\n",
      "epoch 1785, loss 0.8232624530792236, R2 0.6585893630981445\n",
      "Eval loss 0.8562008142471313, R2 0.6636737585067749\n",
      "epoch 1786, loss 0.8232160806655884, R2 0.6586315631866455\n",
      "Eval loss 0.8561436533927917, R2 0.6637208461761475\n",
      "epoch 1787, loss 0.8231697678565979, R2 0.6586735248565674\n",
      "Eval loss 0.8560866713523865, R2 0.6637678146362305\n",
      "epoch 1788, loss 0.823123574256897, R2 0.6587154865264893\n",
      "Eval loss 0.8560296297073364, R2 0.6638147830963135\n",
      "epoch 1789, loss 0.8230774402618408, R2 0.6587573289871216\n",
      "Eval loss 0.8559728264808655, R2 0.6638616323471069\n",
      "epoch 1790, loss 0.8230314254760742, R2 0.6587991714477539\n",
      "Eval loss 0.8559162020683289, R2 0.6639083027839661\n",
      "epoch 1791, loss 0.8229855895042419, R2 0.6588408946990967\n",
      "Eval loss 0.8558595776557922, R2 0.6639549732208252\n",
      "epoch 1792, loss 0.8229398131370544, R2 0.6588824987411499\n",
      "Eval loss 0.8558032512664795, R2 0.6640015244483948\n",
      "epoch 1793, loss 0.8228940963745117, R2 0.6589240431785583\n",
      "Eval loss 0.8557468056678772, R2 0.6640480756759644\n",
      "epoch 1794, loss 0.8228484392166138, R2 0.658965528011322\n",
      "Eval loss 0.8556907176971436, R2 0.6640943884849548\n",
      "epoch 1795, loss 0.8228030204772949, R2 0.6590070128440857\n",
      "Eval loss 0.8556345701217651, R2 0.6641407012939453\n",
      "epoch 1796, loss 0.8227576613426208, R2 0.659048318862915\n",
      "Eval loss 0.8555785417556763, R2 0.6641868948936462\n",
      "epoch 1797, loss 0.8227123618125916, R2 0.6590894460678101\n",
      "Eval loss 0.8555227518081665, R2 0.6642330288887024\n",
      "epoch 1798, loss 0.8226671814918518, R2 0.6591306328773499\n",
      "Eval loss 0.855466902256012, R2 0.664279043674469\n",
      "epoch 1799, loss 0.8226221799850464, R2 0.6591717004776001\n",
      "Eval loss 0.8554112911224365, R2 0.6643249988555908\n",
      "epoch 1800, loss 0.8225771188735962, R2 0.6592126488685608\n",
      "Eval loss 0.8553557991981506, R2 0.6643707752227783\n",
      "epoch 1801, loss 0.8225322365760803, R2 0.6592535376548767\n",
      "Eval loss 0.85530024766922, R2 0.6644166111946106\n",
      "epoch 1802, loss 0.822487473487854, R2 0.6592943072319031\n",
      "Eval loss 0.855245053768158, R2 0.6644622683525085\n",
      "epoch 1803, loss 0.8224428296089172, R2 0.6593350768089294\n",
      "Eval loss 0.8551898002624512, R2 0.6645078063011169\n",
      "epoch 1804, loss 0.8223982453346252, R2 0.6593757271766663\n",
      "Eval loss 0.8551348447799683, R2 0.6645534038543701\n",
      "epoch 1805, loss 0.8223538994789124, R2 0.6594161987304688\n",
      "Eval loss 0.8550798892974854, R2 0.664598822593689\n",
      "epoch 1806, loss 0.8223094940185547, R2 0.659456729888916\n",
      "Eval loss 0.855025053024292, R2 0.6646440625190735\n",
      "epoch 1807, loss 0.8222652077674866, R2 0.6594972014427185\n",
      "Eval loss 0.8549703359603882, R2 0.6646893620491028\n",
      "epoch 1808, loss 0.822221040725708, R2 0.6595374941825867\n",
      "Eval loss 0.8549156785011292, R2 0.6647345423698425\n",
      "epoch 1809, loss 0.8221768736839294, R2 0.6595777273178101\n",
      "Eval loss 0.8548611402511597, R2 0.6647796034812927\n",
      "epoch 1810, loss 0.8221330046653748, R2 0.6596178412437439\n",
      "Eval loss 0.8548068404197693, R2 0.6648245453834534\n",
      "epoch 1811, loss 0.8220891356468201, R2 0.659657895565033\n",
      "Eval loss 0.8547524809837341, R2 0.664869487285614\n",
      "epoch 1812, loss 0.8220452666282654, R2 0.659697949886322\n",
      "Eval loss 0.8546983599662781, R2 0.6649143695831299\n",
      "epoch 1813, loss 0.822001576423645, R2 0.659737765789032\n",
      "Eval loss 0.8546443581581116, R2 0.6649590134620667\n",
      "epoch 1814, loss 0.8219581246376038, R2 0.6597777009010315\n",
      "Eval loss 0.8545902967453003, R2 0.6650037169456482\n",
      "epoch 1815, loss 0.8219146132469177, R2 0.6598174571990967\n",
      "Eval loss 0.8545364141464233, R2 0.6650482416152954\n",
      "epoch 1816, loss 0.821871280670166, R2 0.6598570942878723\n",
      "Eval loss 0.8544827699661255, R2 0.6650927066802979\n",
      "epoch 1817, loss 0.8218280076980591, R2 0.6598966717720032\n",
      "Eval loss 0.8544291257858276, R2 0.6651372313499451\n",
      "epoch 1818, loss 0.8217847943305969, R2 0.6599361896514893\n",
      "Eval loss 0.8543756008148193, R2 0.6651813983917236\n",
      "epoch 1819, loss 0.8217417597770691, R2 0.6599755883216858\n",
      "Eval loss 0.8543222546577454, R2 0.6652256846427917\n",
      "epoch 1820, loss 0.8216986656188965, R2 0.6600149273872375\n",
      "Eval loss 0.8542690277099609, R2 0.6652697920799255\n",
      "epoch 1821, loss 0.821655809879303, R2 0.6600542664527893\n",
      "Eval loss 0.8542157411575317, R2 0.6653138399124146\n",
      "epoch 1822, loss 0.8216130137443542, R2 0.6600935459136963\n",
      "Eval loss 0.8541626334190369, R2 0.6653578281402588\n",
      "epoch 1823, loss 0.8215703368186951, R2 0.6601325869560242\n",
      "Eval loss 0.8541097044944763, R2 0.6654017567634583\n",
      "epoch 1824, loss 0.8215277194976807, R2 0.6601716876029968\n",
      "Eval loss 0.8540568351745605, R2 0.6654455065727234\n",
      "epoch 1825, loss 0.8214852809906006, R2 0.6602106094360352\n",
      "Eval loss 0.8540041446685791, R2 0.6654892563819885\n",
      "epoch 1826, loss 0.8214428424835205, R2 0.6602494716644287\n",
      "Eval loss 0.8539514541625977, R2 0.6655328869819641\n",
      "epoch 1827, loss 0.8214005827903748, R2 0.6602882742881775\n",
      "Eval loss 0.8538989424705505, R2 0.6655765175819397\n",
      "epoch 1828, loss 0.821358323097229, R2 0.6603270173072815\n",
      "Eval loss 0.8538466095924377, R2 0.6656199097633362\n",
      "epoch 1829, loss 0.821316123008728, R2 0.660365641117096\n",
      "Eval loss 0.8537942171096802, R2 0.6656633615493774\n",
      "epoch 1830, loss 0.8212741613388062, R2 0.6604042649269104\n",
      "Eval loss 0.8537420034408569, R2 0.6657066345214844\n",
      "epoch 1831, loss 0.821232259273529, R2 0.6604427695274353\n",
      "Eval loss 0.8536897897720337, R2 0.6657499670982361\n",
      "epoch 1832, loss 0.821190357208252, R2 0.6604812145233154\n",
      "Eval loss 0.8536377549171448, R2 0.6657930016517639\n",
      "epoch 1833, loss 0.8211486339569092, R2 0.660519540309906\n",
      "Eval loss 0.853585958480835, R2 0.6658360958099365\n",
      "epoch 1834, loss 0.8211069703102112, R2 0.660557746887207\n",
      "Eval loss 0.8535341620445251, R2 0.6658791303634644\n",
      "epoch 1835, loss 0.821065366268158, R2 0.6605960130691528\n",
      "Eval loss 0.8534824848175049, R2 0.6659219861030579\n",
      "epoch 1836, loss 0.8210239410400391, R2 0.6606341004371643\n",
      "Eval loss 0.8534309267997742, R2 0.6659648418426514\n",
      "epoch 1837, loss 0.8209823966026306, R2 0.660672128200531\n",
      "Eval loss 0.8533794283866882, R2 0.6660075783729553\n",
      "epoch 1838, loss 0.8209412097930908, R2 0.6607100963592529\n",
      "Eval loss 0.8533279299736023, R2 0.6660502552986145\n",
      "epoch 1839, loss 0.820900022983551, R2 0.6607480645179749\n",
      "Eval loss 0.853276789188385, R2 0.6660928130149841\n",
      "epoch 1840, loss 0.8208590149879456, R2 0.6607858538627625\n",
      "Eval loss 0.8532256484031677, R2 0.666135311126709\n",
      "epoch 1841, loss 0.8208180069923401, R2 0.6608235836029053\n",
      "Eval loss 0.8531745672225952, R2 0.6661777496337891\n",
      "epoch 1842, loss 0.8207769989967346, R2 0.6608612537384033\n",
      "Eval loss 0.8531236052513123, R2 0.6662200689315796\n",
      "epoch 1843, loss 0.8207362294197083, R2 0.6608988046646118\n",
      "Eval loss 0.8530728220939636, R2 0.6662623882293701\n",
      "epoch 1844, loss 0.8206955194473267, R2 0.6609363555908203\n",
      "Eval loss 0.8530219197273254, R2 0.6663045287132263\n",
      "epoch 1845, loss 0.8206548094749451, R2 0.660973846912384\n",
      "Eval loss 0.8529713153839111, R2 0.6663466095924377\n",
      "epoch 1846, loss 0.8206143379211426, R2 0.661011278629303\n",
      "Eval loss 0.8529208302497864, R2 0.6663886904716492\n",
      "epoch 1847, loss 0.8205738663673401, R2 0.6610484719276428\n",
      "Eval loss 0.8528704047203064, R2 0.666430652141571\n",
      "epoch 1848, loss 0.8205334544181824, R2 0.6610857248306274\n",
      "Eval loss 0.8528199791908264, R2 0.6664724946022034\n",
      "epoch 1849, loss 0.8204931616783142, R2 0.6611228585243225\n",
      "Eval loss 0.8527697324752808, R2 0.6665143370628357\n",
      "epoch 1850, loss 0.8204529285430908, R2 0.6611599326133728\n",
      "Eval loss 0.8527196645736694, R2 0.6665559411048889\n",
      "epoch 1851, loss 0.820412814617157, R2 0.6611969470977783\n",
      "Eval loss 0.8526695370674133, R2 0.6665976047515869\n",
      "epoch 1852, loss 0.8203728795051575, R2 0.6612338423728943\n",
      "Eval loss 0.8526196479797363, R2 0.6666391491889954\n",
      "epoch 1853, loss 0.820332944393158, R2 0.6612706780433655\n",
      "Eval loss 0.8525697588920593, R2 0.6666806936264038\n",
      "epoch 1854, loss 0.8202930688858032, R2 0.6613075137138367\n",
      "Eval loss 0.8525200486183167, R2 0.6667220592498779\n",
      "epoch 1855, loss 0.8202533721923828, R2 0.6613442301750183\n",
      "Eval loss 0.8524704575538635, R2 0.6667633652687073\n",
      "epoch 1856, loss 0.8202136754989624, R2 0.6613808870315552\n",
      "Eval loss 0.8524208068847656, R2 0.6668045520782471\n",
      "epoch 1857, loss 0.8201740980148315, R2 0.6614174246788025\n",
      "Eval loss 0.8523714542388916, R2 0.6668457388877869\n",
      "epoch 1858, loss 0.8201345801353455, R2 0.6614538431167603\n",
      "Eval loss 0.8523221015930176, R2 0.6668868660926819\n",
      "epoch 1859, loss 0.8200952410697937, R2 0.6614903211593628\n",
      "Eval loss 0.8522729277610779, R2 0.6669278740882874\n",
      "epoch 1860, loss 0.8200559020042419, R2 0.661526620388031\n",
      "Eval loss 0.8522236943244934, R2 0.666968822479248\n",
      "epoch 1861, loss 0.8200166821479797, R2 0.6615629196166992\n",
      "Eval loss 0.852174699306488, R2 0.6670096516609192\n",
      "epoch 1862, loss 0.8199775218963623, R2 0.6615990996360779\n",
      "Eval loss 0.8521257042884827, R2 0.6670504212379456\n",
      "epoch 1863, loss 0.8199384808540344, R2 0.6616353392601013\n",
      "Eval loss 0.8520768880844116, R2 0.6670911312103271\n",
      "epoch 1864, loss 0.8198994994163513, R2 0.6616713404655457\n",
      "Eval loss 0.8520280718803406, R2 0.667131781578064\n",
      "epoch 1865, loss 0.8198606371879578, R2 0.66170734167099\n",
      "Eval loss 0.8519794940948486, R2 0.6671723127365112\n",
      "epoch 1866, loss 0.819821834564209, R2 0.6617432236671448\n",
      "Eval loss 0.8519309163093567, R2 0.6672128438949585\n",
      "epoch 1867, loss 0.8197832107543945, R2 0.6617791652679443\n",
      "Eval loss 0.8518824577331543, R2 0.6672531962394714\n",
      "epoch 1868, loss 0.8197444677352905, R2 0.6618148684501648\n",
      "Eval loss 0.8518341779708862, R2 0.6672935485839844\n",
      "epoch 1869, loss 0.8197060823440552, R2 0.66185063123703\n",
      "Eval loss 0.8517858386039734, R2 0.6673337817192078\n",
      "epoch 1870, loss 0.819667637348175, R2 0.6618862152099609\n",
      "Eval loss 0.8517376780509949, R2 0.6673738956451416\n",
      "epoch 1871, loss 0.8196291923522949, R2 0.6619217395782471\n",
      "Eval loss 0.8516896367073059, R2 0.667414128780365\n",
      "epoch 1872, loss 0.8195909261703491, R2 0.661957323551178\n",
      "Eval loss 0.8516415953636169, R2 0.6674540638923645\n",
      "epoch 1873, loss 0.8195527195930481, R2 0.6619927287101746\n",
      "Eval loss 0.8515938520431519, R2 0.6674940586090088\n",
      "epoch 1874, loss 0.8195145726203918, R2 0.6620280742645264\n",
      "Eval loss 0.8515459895133972, R2 0.6675339341163635\n",
      "epoch 1875, loss 0.8194765448570251, R2 0.6620633602142334\n",
      "Eval loss 0.8514983057975769, R2 0.6675736904144287\n",
      "epoch 1876, loss 0.819438636302948, R2 0.6620985865592957\n",
      "Eval loss 0.8514506220817566, R2 0.6676134467124939\n",
      "epoch 1877, loss 0.8194007277488708, R2 0.6621337532997131\n",
      "Eval loss 0.8514032363891602, R2 0.6676531434059143\n",
      "epoch 1878, loss 0.819362998008728, R2 0.6621687412261963\n",
      "Eval loss 0.8513558506965637, R2 0.6676926612854004\n",
      "epoch 1879, loss 0.8193252682685852, R2 0.6622037887573242\n",
      "Eval loss 0.8513084053993225, R2 0.6677321791648865\n",
      "epoch 1880, loss 0.8192878365516663, R2 0.6622387170791626\n",
      "Eval loss 0.8512611985206604, R2 0.667771577835083\n",
      "epoch 1881, loss 0.8192501664161682, R2 0.662273645401001\n",
      "Eval loss 0.8512141108512878, R2 0.6678109765052795\n",
      "epoch 1882, loss 0.8192127346992493, R2 0.6623084545135498\n",
      "Eval loss 0.8511670827865601, R2 0.6678503751754761\n",
      "epoch 1883, loss 0.8191754221916199, R2 0.6623430848121643\n",
      "Eval loss 0.8511202335357666, R2 0.6678894758224487\n",
      "epoch 1884, loss 0.8191381096839905, R2 0.6623778343200684\n",
      "Eval loss 0.8510733246803284, R2 0.6679286360740662\n",
      "epoch 1885, loss 0.8191008567810059, R2 0.6624124646186829\n",
      "Eval loss 0.8510266542434692, R2 0.6679677367210388\n",
      "epoch 1886, loss 0.8190638422966003, R2 0.6624469757080078\n",
      "Eval loss 0.8509799242019653, R2 0.6680066585540771\n",
      "epoch 1887, loss 0.8190268278121948, R2 0.6624813675880432\n",
      "Eval loss 0.8509334325790405, R2 0.6680456399917603\n",
      "epoch 1888, loss 0.8189898133277893, R2 0.6625158786773682\n",
      "Eval loss 0.8508870005607605, R2 0.6680845022201538\n",
      "epoch 1889, loss 0.8189529180526733, R2 0.662550151348114\n",
      "Eval loss 0.8508405685424805, R2 0.6681233048439026\n",
      "epoch 1890, loss 0.8189162015914917, R2 0.6625843644142151\n",
      "Eval loss 0.8507943153381348, R2 0.6681619882583618\n",
      "epoch 1891, loss 0.8188794255256653, R2 0.6626186370849609\n",
      "Eval loss 0.8507480621337891, R2 0.668200671672821\n",
      "epoch 1892, loss 0.818842887878418, R2 0.6626527309417725\n",
      "Eval loss 0.8507018685340881, R2 0.668239176273346\n",
      "epoch 1893, loss 0.8188062906265259, R2 0.6626867651939392\n",
      "Eval loss 0.8506559133529663, R2 0.6682776808738708\n",
      "epoch 1894, loss 0.8187698721885681, R2 0.662720799446106\n",
      "Eval loss 0.850610077381134, R2 0.668316125869751\n",
      "epoch 1895, loss 0.8187335133552551, R2 0.6627547144889832\n",
      "Eval loss 0.8505643010139465, R2 0.6683544516563416\n",
      "epoch 1896, loss 0.8186970949172974, R2 0.6627885699272156\n",
      "Eval loss 0.8505184650421143, R2 0.6683927774429321\n",
      "epoch 1897, loss 0.8186608552932739, R2 0.6628223061561584\n",
      "Eval loss 0.8504728078842163, R2 0.6684309840202332\n",
      "epoch 1898, loss 0.81862473487854, R2 0.6628560423851013\n",
      "Eval loss 0.8504272103309631, R2 0.6684690713882446\n",
      "epoch 1899, loss 0.8185886740684509, R2 0.6628897786140442\n",
      "Eval loss 0.8503817319869995, R2 0.6685071587562561\n",
      "epoch 1900, loss 0.8185527324676514, R2 0.6629233956336975\n",
      "Eval loss 0.850336492061615, R2 0.6685451865196228\n",
      "epoch 1901, loss 0.8185167908668518, R2 0.6629568934440613\n",
      "Eval loss 0.8502911925315857, R2 0.6685830950737\n",
      "epoch 1902, loss 0.8184810280799866, R2 0.6629903316497803\n",
      "Eval loss 0.8502460718154907, R2 0.6686210036277771\n",
      "epoch 1903, loss 0.8184452056884766, R2 0.6630237698554993\n",
      "Eval loss 0.8502008318901062, R2 0.6686587333679199\n",
      "epoch 1904, loss 0.8184094429016113, R2 0.6630570888519287\n",
      "Eval loss 0.8501558899879456, R2 0.668696403503418\n",
      "epoch 1905, loss 0.8183739185333252, R2 0.6630902886390686\n",
      "Eval loss 0.8501109480857849, R2 0.6687341332435608\n",
      "epoch 1906, loss 0.8183383941650391, R2 0.6631235480308533\n",
      "Eval loss 0.850066065788269, R2 0.6687716245651245\n",
      "epoch 1907, loss 0.8183029294013977, R2 0.6631566882133484\n",
      "Eval loss 0.8500213027000427, R2 0.668809175491333\n",
      "epoch 1908, loss 0.8182675838470459, R2 0.6631898283958435\n",
      "Eval loss 0.8499767184257507, R2 0.6688465476036072\n",
      "epoch 1909, loss 0.8182322382926941, R2 0.6632228493690491\n",
      "Eval loss 0.8499321937561035, R2 0.6688839793205261\n",
      "epoch 1910, loss 0.8181970715522766, R2 0.6632556915283203\n",
      "Eval loss 0.8498876094818115, R2 0.6689212918281555\n",
      "epoch 1911, loss 0.8181619048118591, R2 0.6632886528968811\n",
      "Eval loss 0.8498432040214539, R2 0.6689586043357849\n",
      "epoch 1912, loss 0.8181268572807312, R2 0.6633214354515076\n",
      "Eval loss 0.8497989177703857, R2 0.6689956784248352\n",
      "epoch 1913, loss 0.818091869354248, R2 0.663354218006134\n",
      "Eval loss 0.8497546911239624, R2 0.6690327525138855\n",
      "epoch 1914, loss 0.8180570006370544, R2 0.663386881351471\n",
      "Eval loss 0.8497105836868286, R2 0.6690698266029358\n",
      "epoch 1915, loss 0.8180221915245056, R2 0.6634194850921631\n",
      "Eval loss 0.8496665358543396, R2 0.6691067218780518\n",
      "epoch 1916, loss 0.8179873824119568, R2 0.6634520888328552\n",
      "Eval loss 0.8496224880218506, R2 0.6691436767578125\n",
      "epoch 1917, loss 0.8179526925086975, R2 0.6634846329689026\n",
      "Eval loss 0.8495786786079407, R2 0.6691804528236389\n",
      "epoch 1918, loss 0.8179181218147278, R2 0.6635170578956604\n",
      "Eval loss 0.8495348691940308, R2 0.6692172884941101\n",
      "epoch 1919, loss 0.8178836107254028, R2 0.6635494232177734\n",
      "Eval loss 0.8494911193847656, R2 0.6692538857460022\n",
      "epoch 1920, loss 0.8178490996360779, R2 0.6635816693305969\n",
      "Eval loss 0.8494476079940796, R2 0.6692905426025391\n",
      "epoch 1921, loss 0.817814826965332, R2 0.6636139750480652\n",
      "Eval loss 0.8494039177894592, R2 0.6693271398544312\n",
      "epoch 1922, loss 0.8177804350852966, R2 0.6636461615562439\n",
      "Eval loss 0.8493605256080627, R2 0.6693636178970337\n",
      "epoch 1923, loss 0.8177462816238403, R2 0.6636783480644226\n",
      "Eval loss 0.8493173122406006, R2 0.6693999767303467\n",
      "epoch 1924, loss 0.8177120089530945, R2 0.6637102961540222\n",
      "Eval loss 0.8492739796638489, R2 0.6694363951683044\n",
      "epoch 1925, loss 0.8176779747009277, R2 0.6637423634529114\n",
      "Eval loss 0.8492306470870972, R2 0.6694726347923279\n",
      "epoch 1926, loss 0.8176440000534058, R2 0.663774311542511\n",
      "Eval loss 0.8491876721382141, R2 0.6695088744163513\n",
      "epoch 1927, loss 0.8176100850105286, R2 0.6638060808181763\n",
      "Eval loss 0.8491446375846863, R2 0.6695449948310852\n",
      "epoch 1928, loss 0.8175761699676514, R2 0.6638379096984863\n",
      "Eval loss 0.8491016626358032, R2 0.6695811748504639\n",
      "epoch 1929, loss 0.8175424337387085, R2 0.6638696193695068\n",
      "Eval loss 0.8490587472915649, R2 0.6696171164512634\n",
      "epoch 1930, loss 0.8175086975097656, R2 0.6639013886451721\n",
      "Eval loss 0.8490159511566162, R2 0.6696531176567078\n",
      "epoch 1931, loss 0.8174750804901123, R2 0.6639330387115479\n",
      "Eval loss 0.8489733934402466, R2 0.6696889400482178\n",
      "epoch 1932, loss 0.8174415230751038, R2 0.6639646291732788\n",
      "Eval loss 0.8489307761192322, R2 0.6697248220443726\n",
      "epoch 1933, loss 0.81740802526474, R2 0.663996160030365\n",
      "Eval loss 0.8488881587982178, R2 0.6697605848312378\n",
      "epoch 1934, loss 0.8173746466636658, R2 0.6640275120735168\n",
      "Eval loss 0.8488457202911377, R2 0.6697962284088135\n",
      "epoch 1935, loss 0.8173412680625916, R2 0.6640589237213135\n",
      "Eval loss 0.8488034605979919, R2 0.6698318719863892\n",
      "epoch 1936, loss 0.8173079490661621, R2 0.6640902161598206\n",
      "Eval loss 0.8487612009048462, R2 0.6698675155639648\n",
      "epoch 1937, loss 0.8172748684883118, R2 0.6641215085983276\n",
      "Eval loss 0.8487189412117004, R2 0.6699029207229614\n",
      "epoch 1938, loss 0.8172416687011719, R2 0.6641527414321899\n",
      "Eval loss 0.848676860332489, R2 0.669938325881958\n",
      "epoch 1939, loss 0.8172086477279663, R2 0.6641838550567627\n",
      "Eval loss 0.8486348390579224, R2 0.6699737310409546\n",
      "epoch 1940, loss 0.8171756267547607, R2 0.6642149090766907\n",
      "Eval loss 0.8485928773880005, R2 0.6700090765953064\n",
      "epoch 1941, loss 0.8171427845954895, R2 0.6642460227012634\n",
      "Eval loss 0.8485510349273682, R2 0.6700442433357239\n",
      "epoch 1942, loss 0.8171098232269287, R2 0.6642769575119019\n",
      "Eval loss 0.8485092520713806, R2 0.6700794696807861\n",
      "epoch 1943, loss 0.8170770406723022, R2 0.6643078327178955\n",
      "Eval loss 0.8484675288200378, R2 0.6701145768165588\n",
      "epoch 1944, loss 0.8170443773269653, R2 0.6643387079238892\n",
      "Eval loss 0.8484258055686951, R2 0.670149564743042\n",
      "epoch 1945, loss 0.8170117139816284, R2 0.664369523525238\n",
      "Eval loss 0.8483843207359314, R2 0.6701845526695251\n",
      "epoch 1946, loss 0.8169792294502258, R2 0.6644002795219421\n",
      "Eval loss 0.8483428359031677, R2 0.6702195405960083\n",
      "epoch 1947, loss 0.8169465661048889, R2 0.6644308567047119\n",
      "Eval loss 0.8483014702796936, R2 0.6702543497085571\n",
      "epoch 1948, loss 0.8169142603874207, R2 0.6644615530967712\n",
      "Eval loss 0.8482601046562195, R2 0.6702890992164612\n",
      "epoch 1949, loss 0.8168818950653076, R2 0.6644920706748962\n",
      "Eval loss 0.8482189178466797, R2 0.67032390832901\n",
      "epoch 1950, loss 0.8168495893478394, R2 0.6645225882530212\n",
      "Eval loss 0.8481777310371399, R2 0.6703585982322693\n",
      "epoch 1951, loss 0.8168172836303711, R2 0.6645529270172119\n",
      "Eval loss 0.8481367230415344, R2 0.670393168926239\n",
      "epoch 1952, loss 0.8167852163314819, R2 0.6645833849906921\n",
      "Eval loss 0.8480957746505737, R2 0.670427680015564\n",
      "epoch 1953, loss 0.8167532086372375, R2 0.664613664150238\n",
      "Eval loss 0.8480548858642578, R2 0.6704621911048889\n",
      "epoch 1954, loss 0.8167210817337036, R2 0.6646439433097839\n",
      "Eval loss 0.8480140566825867, R2 0.6704965233802795\n",
      "epoch 1955, loss 0.8166892528533936, R2 0.6646740436553955\n",
      "Eval loss 0.8479733467102051, R2 0.6705309152603149\n",
      "epoch 1956, loss 0.816657304763794, R2 0.6647042632102966\n",
      "Eval loss 0.8479326367378235, R2 0.6705651879310608\n",
      "epoch 1957, loss 0.8166255354881287, R2 0.6647343635559082\n",
      "Eval loss 0.8478920459747314, R2 0.6705994009971619\n",
      "epoch 1958, loss 0.8165938258171082, R2 0.664764404296875\n",
      "Eval loss 0.847851574420929, R2 0.6706336140632629\n",
      "epoch 1959, loss 0.8165621161460876, R2 0.664794385433197\n",
      "Eval loss 0.8478111028671265, R2 0.6706677079200745\n",
      "epoch 1960, loss 0.8165305256843567, R2 0.6648242473602295\n",
      "Eval loss 0.8477707505226135, R2 0.6707017421722412\n",
      "epoch 1961, loss 0.8164989948272705, R2 0.6648539900779724\n",
      "Eval loss 0.8477304577827454, R2 0.6707356572151184\n",
      "epoch 1962, loss 0.8164675831794739, R2 0.6648838520050049\n",
      "Eval loss 0.8476903438568115, R2 0.6707695722579956\n",
      "epoch 1963, loss 0.8164360523223877, R2 0.6649135947227478\n",
      "Eval loss 0.8476502299308777, R2 0.6708034873008728\n",
      "epoch 1964, loss 0.8164048194885254, R2 0.6649433374404907\n",
      "Eval loss 0.8476100564002991, R2 0.6708372831344604\n",
      "epoch 1965, loss 0.8163735866546631, R2 0.6649728417396545\n",
      "Eval loss 0.8475702404975891, R2 0.6708710193634033\n",
      "epoch 1966, loss 0.8163424134254456, R2 0.6650024652481079\n",
      "Eval loss 0.8475302457809448, R2 0.6709046959877014\n",
      "epoch 1967, loss 0.816311240196228, R2 0.6650319695472717\n",
      "Eval loss 0.8474904298782349, R2 0.67093825340271\n",
      "epoch 1968, loss 0.8162801861763, R2 0.6650614142417908\n",
      "Eval loss 0.8474507331848145, R2 0.6709718108177185\n",
      "epoch 1969, loss 0.8162491917610168, R2 0.6650908589363098\n",
      "Eval loss 0.847411036491394, R2 0.6710053086280823\n",
      "epoch 1970, loss 0.8162181973457336, R2 0.6651201844215393\n",
      "Eval loss 0.8473714590072632, R2 0.6710386872291565\n",
      "epoch 1971, loss 0.8161875009536743, R2 0.665149450302124\n",
      "Eval loss 0.8473320603370667, R2 0.6710720658302307\n",
      "epoch 1972, loss 0.8161566257476807, R2 0.665178656578064\n",
      "Eval loss 0.8472925424575806, R2 0.6711053848266602\n",
      "epoch 1973, loss 0.8161258697509766, R2 0.6652078032493591\n",
      "Eval loss 0.8472532629966736, R2 0.6711386442184448\n",
      "epoch 1974, loss 0.8160951733589172, R2 0.6652369499206543\n",
      "Eval loss 0.847213864326477, R2 0.6711717844009399\n",
      "epoch 1975, loss 0.8160646557807922, R2 0.6652660369873047\n",
      "Eval loss 0.8471747040748596, R2 0.6712049841880798\n",
      "epoch 1976, loss 0.8160340785980225, R2 0.6652950644493103\n",
      "Eval loss 0.8471355438232422, R2 0.6712380051612854\n",
      "epoch 1977, loss 0.8160036206245422, R2 0.6653239727020264\n",
      "Eval loss 0.8470965623855591, R2 0.6712710857391357\n",
      "epoch 1978, loss 0.815973162651062, R2 0.6653528809547424\n",
      "Eval loss 0.8470575213432312, R2 0.6713040471076965\n",
      "epoch 1979, loss 0.8159428238868713, R2 0.665381669998169\n",
      "Eval loss 0.8470187187194824, R2 0.6713368892669678\n",
      "epoch 1980, loss 0.815912663936615, R2 0.6654104590415955\n",
      "Eval loss 0.8469798564910889, R2 0.671369731426239\n",
      "epoch 1981, loss 0.8158822655677795, R2 0.6654391884803772\n",
      "Eval loss 0.8469410538673401, R2 0.6714025139808655\n",
      "epoch 1982, loss 0.8158522844314575, R2 0.6654678583145142\n",
      "Eval loss 0.8469023704528809, R2 0.6714351773262024\n",
      "epoch 1983, loss 0.8158222436904907, R2 0.6654965877532959\n",
      "Eval loss 0.8468638062477112, R2 0.6714678406715393\n",
      "epoch 1984, loss 0.8157921433448792, R2 0.6655250787734985\n",
      "Eval loss 0.8468253016471863, R2 0.6715003848075867\n",
      "epoch 1985, loss 0.8157622218132019, R2 0.665553629398346\n",
      "Eval loss 0.8467869162559509, R2 0.6715329885482788\n",
      "epoch 1986, loss 0.8157322406768799, R2 0.6655820608139038\n",
      "Eval loss 0.846748411655426, R2 0.6715654134750366\n",
      "epoch 1987, loss 0.8157026171684265, R2 0.6656104922294617\n",
      "Eval loss 0.8467101454734802, R2 0.6715978980064392\n",
      "epoch 1988, loss 0.815672755241394, R2 0.6656388640403748\n",
      "Eval loss 0.8466718792915344, R2 0.6716302633285522\n",
      "epoch 1989, loss 0.8156431317329407, R2 0.6656670570373535\n",
      "Eval loss 0.8466336727142334, R2 0.6716625094413757\n",
      "epoch 1990, loss 0.8156134486198425, R2 0.6656953692436218\n",
      "Eval loss 0.8465957045555115, R2 0.6716946959495544\n",
      "epoch 1991, loss 0.8155838251113892, R2 0.665723443031311\n",
      "Eval loss 0.8465576171875, R2 0.6717269420623779\n",
      "epoch 1992, loss 0.8155543208122253, R2 0.6657516956329346\n",
      "Eval loss 0.8465196490287781, R2 0.6717590689659119\n",
      "epoch 1993, loss 0.8155248761177063, R2 0.6657797694206238\n",
      "Eval loss 0.8464818596839905, R2 0.671791136264801\n",
      "epoch 1994, loss 0.815495491027832, R2 0.6658077239990234\n",
      "Eval loss 0.8464439511299133, R2 0.6718230843544006\n",
      "epoch 1995, loss 0.8154662847518921, R2 0.6658357381820679\n",
      "Eval loss 0.8464062213897705, R2 0.6718550324440002\n",
      "epoch 1996, loss 0.8154369592666626, R2 0.6658636331558228\n",
      "Eval loss 0.8463686108589172, R2 0.6718870401382446\n",
      "epoch 1997, loss 0.8154077529907227, R2 0.6658915877342224\n",
      "Eval loss 0.8463309407234192, R2 0.6719188690185547\n",
      "epoch 1998, loss 0.8153785467147827, R2 0.665919303894043\n",
      "Eval loss 0.8462935090065002, R2 0.6719505786895752\n",
      "epoch 1999, loss 0.8153496384620667, R2 0.6659470796585083\n",
      "Eval loss 0.8462560176849365, R2 0.6719823479652405\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[idx, :]\n",
    "    train_y = train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    test_x = test_x[idx, :]\n",
    "    test_y = test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = lr_model(train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = lr_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = lr_model(train_x)\n",
    "    test_preds = lr_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch duration: 0.25s\n",
      "Epoch: 0 Train loss 7.358, Test loss 5.934 Train R2 -1.956, Test R2 -1.428 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 1 Train loss 5.963, Test loss 5.127 Train R2 -1.439, Test R2 -1.079 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 2 Train loss 5.135, Test loss 4.696 Train R2 -1.087, Test R2 -0.850 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 3 Train loss 4.687, Test loss 4.465 Train R2 -0.860, Test R2 -0.719 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 4 Train loss 4.444, Test loss 4.315 Train R2 -0.727, Test R2 -0.608 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 5 Train loss 4.285, Test loss 4.191 Train R2 -0.609, Test R2 -0.488 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 6 Train loss 4.151, Test loss 4.074 Train R2 -0.481, Test R2 -0.378 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 7 Train loss 4.029, Test loss 3.962 Train R2 -0.364, Test R2 -0.285 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 8 Train loss 3.914, Test loss 3.856 Train R2 -0.269, Test R2 -0.199 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 9 Train loss 3.808, Test loss 3.753 Train R2 -0.182, Test R2 -0.117 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 10 Train loss 3.706, Test loss 3.651 Train R2 -0.103, Test R2 -0.054 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 11 Train loss 3.607, Test loss 3.547 Train R2 -0.041, Test R2 -0.010 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 12 Train loss 3.507, Test loss 3.441 Train R2 0.003, Test R2 0.030 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 13 Train loss 3.406, Test loss 3.331 Train R2 0.042, Test R2 0.069 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 14 Train loss 3.302, Test loss 3.219 Train R2 0.080, Test R2 0.103 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 15 Train loss 3.195, Test loss 3.106 Train R2 0.112, Test R2 0.131 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 16 Train loss 3.087, Test loss 2.996 Train R2 0.138, Test R2 0.159 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 17 Train loss 2.980, Test loss 2.893 Train R2 0.164, Test R2 0.187 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 18 Train loss 2.880, Test loss 2.801 Train R2 0.192, Test R2 0.210 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 19 Train loss 2.790, Test loss 2.721 Train R2 0.215, Test R2 0.227 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 20 Train loss 2.711, Test loss 2.654 Train R2 0.232, Test R2 0.242 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 21 Train loss 2.643, Test loss 2.595 Train R2 0.247, Test R2 0.258 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 22 Train loss 2.583, Test loss 2.541 Train R2 0.263, Test R2 0.273 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 23 Train loss 2.528, Test loss 2.491 Train R2 0.278, Test R2 0.286 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 24 Train loss 2.476, Test loss 2.442 Train R2 0.291, Test R2 0.299 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 25 Train loss 2.426, Test loss 2.394 Train R2 0.305, Test R2 0.315 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 26 Train loss 2.377, Test loss 2.346 Train R2 0.320, Test R2 0.329 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 27 Train loss 2.328, Test loss 2.298 Train R2 0.334, Test R2 0.342 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 28 Train loss 2.279, Test loss 2.250 Train R2 0.347, Test R2 0.356 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 29 Train loss 2.231, Test loss 2.201 Train R2 0.361, Test R2 0.371 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 30 Train loss 2.182, Test loss 2.153 Train R2 0.376, Test R2 0.385 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 31 Train loss 2.133, Test loss 2.106 Train R2 0.389, Test R2 0.397 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 32 Train loss 2.086, Test loss 2.062 Train R2 0.402, Test R2 0.411 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 33 Train loss 2.043, Test loss 2.021 Train R2 0.415, Test R2 0.424 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 34 Train loss 2.002, Test loss 1.982 Train R2 0.428, Test R2 0.435 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 35 Train loss 1.964, Test loss 1.945 Train R2 0.438, Test R2 0.445 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 36 Train loss 1.928, Test loss 1.909 Train R2 0.448, Test R2 0.455 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 37 Train loss 1.894, Test loss 1.874 Train R2 0.458, Test R2 0.465 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 38 Train loss 1.860, Test loss 1.841 Train R2 0.467, Test R2 0.473 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 39 Train loss 1.827, Test loss 1.808 Train R2 0.475, Test R2 0.481 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 40 Train loss 1.796, Test loss 1.777 Train R2 0.483, Test R2 0.489 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 41 Train loss 1.765, Test loss 1.747 Train R2 0.492, Test R2 0.497 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 42 Train loss 1.735, Test loss 1.718 Train R2 0.499, Test R2 0.504 \n",
      "Training epoch duration: 0.29s\n",
      "Epoch: 43 Train loss 1.706, Test loss 1.690 Train R2 0.506, Test R2 0.510 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 44 Train loss 1.678, Test loss 1.663 Train R2 0.512, Test R2 0.517 \n",
      "Training epoch duration: 0.15s\n",
      "Epoch: 45 Train loss 1.651, Test loss 1.637 Train R2 0.519, Test R2 0.522 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 46 Train loss 1.625, Test loss 1.611 Train R2 0.524, Test R2 0.528 \n",
      "Training epoch duration: 0.15s\n",
      "Epoch: 47 Train loss 1.599, Test loss 1.585 Train R2 0.530, Test R2 0.534 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 48 Train loss 1.573, Test loss 1.560 Train R2 0.536, Test R2 0.540 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 49 Train loss 1.549, Test loss 1.536 Train R2 0.541, Test R2 0.546 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 50 Train loss 1.525, Test loss 1.512 Train R2 0.546, Test R2 0.551 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 51 Train loss 1.502, Test loss 1.490 Train R2 0.551, Test R2 0.556 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 52 Train loss 1.480, Test loss 1.468 Train R2 0.556, Test R2 0.561 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 53 Train loss 1.459, Test loss 1.447 Train R2 0.560, Test R2 0.566 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 54 Train loss 1.439, Test loss 1.427 Train R2 0.565, Test R2 0.570 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 55 Train loss 1.419, Test loss 1.408 Train R2 0.569, Test R2 0.574 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 56 Train loss 1.399, Test loss 1.389 Train R2 0.573, Test R2 0.579 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 57 Train loss 1.381, Test loss 1.370 Train R2 0.578, Test R2 0.583 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 58 Train loss 1.362, Test loss 1.352 Train R2 0.582, Test R2 0.587 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 59 Train loss 1.345, Test loss 1.335 Train R2 0.585, Test R2 0.590 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 60 Train loss 1.327, Test loss 1.318 Train R2 0.589, Test R2 0.594 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 61 Train loss 1.310, Test loss 1.301 Train R2 0.593, Test R2 0.597 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 62 Train loss 1.294, Test loss 1.285 Train R2 0.596, Test R2 0.601 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 63 Train loss 1.278, Test loss 1.269 Train R2 0.600, Test R2 0.604 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 64 Train loss 1.263, Test loss 1.254 Train R2 0.603, Test R2 0.607 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 65 Train loss 1.248, Test loss 1.239 Train R2 0.606, Test R2 0.610 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 66 Train loss 1.233, Test loss 1.225 Train R2 0.609, Test R2 0.613 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 67 Train loss 1.220, Test loss 1.210 Train R2 0.612, Test R2 0.616 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 68 Train loss 1.206, Test loss 1.197 Train R2 0.614, Test R2 0.619 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 69 Train loss 1.193, Test loss 1.183 Train R2 0.617, Test R2 0.622 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 70 Train loss 1.180, Test loss 1.171 Train R2 0.620, Test R2 0.624 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 71 Train loss 1.168, Test loss 1.158 Train R2 0.622, Test R2 0.626 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 72 Train loss 1.156, Test loss 1.146 Train R2 0.624, Test R2 0.629 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 73 Train loss 1.144, Test loss 1.135 Train R2 0.627, Test R2 0.631 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 74 Train loss 1.133, Test loss 1.124 Train R2 0.629, Test R2 0.633 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 75 Train loss 1.122, Test loss 1.113 Train R2 0.631, Test R2 0.635 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 76 Train loss 1.111, Test loss 1.103 Train R2 0.633, Test R2 0.637 \n",
      "Training epoch duration: 0.17s\n",
      "Epoch: 77 Train loss 1.101, Test loss 1.092 Train R2 0.635, Test R2 0.639 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 78 Train loss 1.091, Test loss 1.083 Train R2 0.637, Test R2 0.641 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 79 Train loss 1.082, Test loss 1.073 Train R2 0.639, Test R2 0.643 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 80 Train loss 1.072, Test loss 1.064 Train R2 0.641, Test R2 0.645 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 81 Train loss 1.063, Test loss 1.055 Train R2 0.642, Test R2 0.647 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 82 Train loss 1.054, Test loss 1.046 Train R2 0.644, Test R2 0.649 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 83 Train loss 1.046, Test loss 1.038 Train R2 0.646, Test R2 0.650 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 84 Train loss 1.038, Test loss 1.029 Train R2 0.647, Test R2 0.652 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 85 Train loss 1.030, Test loss 1.021 Train R2 0.649, Test R2 0.653 \n",
      "Training epoch duration: 0.17s\n",
      "Epoch: 86 Train loss 1.022, Test loss 1.014 Train R2 0.650, Test R2 0.655 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 87 Train loss 1.015, Test loss 1.006 Train R2 0.651, Test R2 0.656 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 88 Train loss 1.007, Test loss 0.999 Train R2 0.653, Test R2 0.658 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 89 Train loss 1.000, Test loss 0.992 Train R2 0.654, Test R2 0.659 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 90 Train loss 0.993, Test loss 0.985 Train R2 0.655, Test R2 0.660 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 91 Train loss 0.987, Test loss 0.978 Train R2 0.657, Test R2 0.662 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 92 Train loss 0.981, Test loss 0.972 Train R2 0.658, Test R2 0.663 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 93 Train loss 0.974, Test loss 0.966 Train R2 0.659, Test R2 0.664 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 94 Train loss 0.968, Test loss 0.960 Train R2 0.660, Test R2 0.665 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 95 Train loss 0.963, Test loss 0.954 Train R2 0.661, Test R2 0.666 \n",
      "Training epoch duration: 0.19s\n",
      "Epoch: 96 Train loss 0.957, Test loss 0.949 Train R2 0.662, Test R2 0.667 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 97 Train loss 0.952, Test loss 0.943 Train R2 0.663, Test R2 0.668 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 98 Train loss 0.946, Test loss 0.938 Train R2 0.664, Test R2 0.669 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 99 Train loss 0.941, Test loss 0.933 Train R2 0.665, Test R2 0.670 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 100 Train loss 0.937, Test loss 0.928 Train R2 0.666, Test R2 0.671 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 101 Train loss 0.932, Test loss 0.923 Train R2 0.667, Test R2 0.672 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 102 Train loss 0.927, Test loss 0.919 Train R2 0.667, Test R2 0.672 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 103 Train loss 0.923, Test loss 0.914 Train R2 0.668, Test R2 0.673 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 104 Train loss 0.919, Test loss 0.910 Train R2 0.669, Test R2 0.674 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 105 Train loss 0.914, Test loss 0.906 Train R2 0.670, Test R2 0.675 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 106 Train loss 0.910, Test loss 0.902 Train R2 0.670, Test R2 0.675 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 107 Train loss 0.907, Test loss 0.898 Train R2 0.671, Test R2 0.676 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 108 Train loss 0.903, Test loss 0.894 Train R2 0.672, Test R2 0.677 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 109 Train loss 0.899, Test loss 0.891 Train R2 0.672, Test R2 0.677 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 110 Train loss 0.896, Test loss 0.887 Train R2 0.673, Test R2 0.678 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 111 Train loss 0.892, Test loss 0.884 Train R2 0.673, Test R2 0.679 \n",
      "Training epoch duration: 0.22s\n",
      "Epoch: 112 Train loss 0.889, Test loss 0.881 Train R2 0.674, Test R2 0.679 \n",
      "Training epoch duration: 0.32s\n",
      "Epoch: 113 Train loss 0.886, Test loss 0.877 Train R2 0.675, Test R2 0.680 \n",
      "Training epoch duration: 0.2s\n",
      "Epoch: 114 Train loss 0.883, Test loss 0.874 Train R2 0.675, Test R2 0.680 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 115 Train loss 0.880, Test loss 0.871 Train R2 0.676, Test R2 0.681 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 116 Train loss 0.877, Test loss 0.869 Train R2 0.676, Test R2 0.681 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 117 Train loss 0.874, Test loss 0.866 Train R2 0.677, Test R2 0.682 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 118 Train loss 0.872, Test loss 0.863 Train R2 0.677, Test R2 0.682 \n",
      "Training epoch duration: 0.19s\n",
      "Epoch: 119 Train loss 0.869, Test loss 0.861 Train R2 0.677, Test R2 0.683 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 120 Train loss 0.867, Test loss 0.858 Train R2 0.678, Test R2 0.683 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 121 Train loss 0.864, Test loss 0.856 Train R2 0.678, Test R2 0.683 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 122 Train loss 0.862, Test loss 0.854 Train R2 0.679, Test R2 0.684 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 123 Train loss 0.860, Test loss 0.851 Train R2 0.679, Test R2 0.684 \n",
      "Training epoch duration: 0.25s\n",
      "Epoch: 124 Train loss 0.858, Test loss 0.849 Train R2 0.679, Test R2 0.685 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 125 Train loss 0.856, Test loss 0.847 Train R2 0.680, Test R2 0.685 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 126 Train loss 0.854, Test loss 0.845 Train R2 0.680, Test R2 0.685 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 127 Train loss 0.852, Test loss 0.843 Train R2 0.680, Test R2 0.686 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 128 Train loss 0.850, Test loss 0.841 Train R2 0.681, Test R2 0.686 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 129 Train loss 0.848, Test loss 0.840 Train R2 0.681, Test R2 0.686 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 130 Train loss 0.846, Test loss 0.838 Train R2 0.681, Test R2 0.687 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 131 Train loss 0.845, Test loss 0.836 Train R2 0.682, Test R2 0.687 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 132 Train loss 0.843, Test loss 0.835 Train R2 0.682, Test R2 0.687 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 133 Train loss 0.842, Test loss 0.833 Train R2 0.682, Test R2 0.687 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 134 Train loss 0.840, Test loss 0.832 Train R2 0.682, Test R2 0.688 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 135 Train loss 0.839, Test loss 0.830 Train R2 0.683, Test R2 0.688 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 136 Train loss 0.837, Test loss 0.829 Train R2 0.683, Test R2 0.688 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 137 Train loss 0.836, Test loss 0.828 Train R2 0.683, Test R2 0.688 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 138 Train loss 0.835, Test loss 0.826 Train R2 0.683, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 139 Train loss 0.833, Test loss 0.825 Train R2 0.684, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 140 Train loss 0.832, Test loss 0.824 Train R2 0.684, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 141 Train loss 0.831, Test loss 0.823 Train R2 0.684, Test R2 0.689 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 142 Train loss 0.830, Test loss 0.822 Train R2 0.684, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 143 Train loss 0.829, Test loss 0.821 Train R2 0.684, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 144 Train loss 0.828, Test loss 0.819 Train R2 0.684, Test R2 0.690 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 145 Train loss 0.827, Test loss 0.818 Train R2 0.685, Test R2 0.690 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 146 Train loss 0.826, Test loss 0.818 Train R2 0.685, Test R2 0.690 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 147 Train loss 0.825, Test loss 0.817 Train R2 0.685, Test R2 0.690 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 148 Train loss 0.824, Test loss 0.816 Train R2 0.685, Test R2 0.690 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 149 Train loss 0.823, Test loss 0.815 Train R2 0.685, Test R2 0.690 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 150 Train loss 0.823, Test loss 0.814 Train R2 0.685, Test R2 0.691 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 151 Train loss 0.822, Test loss 0.813 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 152 Train loss 0.821, Test loss 0.813 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 153 Train loss 0.820, Test loss 0.812 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 154 Train loss 0.820, Test loss 0.811 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 155 Train loss 0.819, Test loss 0.810 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 156 Train loss 0.818, Test loss 0.810 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 157 Train loss 0.818, Test loss 0.809 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 158 Train loss 0.817, Test loss 0.809 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 159 Train loss 0.817, Test loss 0.808 Train R2 0.686, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 160 Train loss 0.816, Test loss 0.807 Train R2 0.686, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 161 Train loss 0.815, Test loss 0.807 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 162 Train loss 0.815, Test loss 0.806 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 163 Train loss 0.814, Test loss 0.806 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 164 Train loss 0.814, Test loss 0.805 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 165 Train loss 0.814, Test loss 0.805 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 166 Train loss 0.813, Test loss 0.805 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 167 Train loss 0.813, Test loss 0.804 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 168 Train loss 0.812, Test loss 0.804 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 169 Train loss 0.812, Test loss 0.803 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 170 Train loss 0.812, Test loss 0.803 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 171 Train loss 0.811, Test loss 0.803 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 172 Train loss 0.811, Test loss 0.802 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 173 Train loss 0.811, Test loss 0.802 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 174 Train loss 0.810, Test loss 0.802 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 175 Train loss 0.810, Test loss 0.801 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 176 Train loss 0.810, Test loss 0.801 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 177 Train loss 0.809, Test loss 0.801 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 178 Train loss 0.809, Test loss 0.800 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 179 Train loss 0.809, Test loss 0.800 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 180 Train loss 0.809, Test loss 0.800 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 181 Train loss 0.808, Test loss 0.800 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 182 Train loss 0.808, Test loss 0.799 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 183 Train loss 0.808, Test loss 0.799 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 184 Train loss 0.808, Test loss 0.799 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 185 Train loss 0.807, Test loss 0.799 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 186 Train loss 0.807, Test loss 0.799 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 187 Train loss 0.807, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 188 Train loss 0.807, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 189 Train loss 0.807, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 190 Train loss 0.807, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 191 Train loss 0.806, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 192 Train loss 0.806, Test loss 0.798 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 193 Train loss 0.806, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 194 Train loss 0.806, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 195 Train loss 0.806, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 196 Train loss 0.806, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 197 Train loss 0.806, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 198 Train loss 0.805, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 199 Train loss 0.805, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 200 Train loss 0.805, Test loss 0.797 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 201 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 202 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 203 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 204 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 205 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 206 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 207 Train loss 0.805, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 208 Train loss 0.804, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 209 Train loss 0.804, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 210 Train loss 0.804, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 211 Train loss 0.804, Test loss 0.796 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 212 Train loss 0.804, Test loss 0.795 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 213 Train loss 0.804, Test loss 0.795 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 214 Train loss 0.804, Test loss 0.795 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 215 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 216 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 217 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 218 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 219 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 220 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 221 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 222 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 223 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 224 Train loss 0.804, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 225 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 226 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 227 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 228 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 229 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 230 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 231 Train loss 0.803, Test loss 0.795 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 232 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 233 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 234 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 235 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 236 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 237 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 238 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 239 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 240 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 241 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 242 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 243 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 244 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 245 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 246 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 247 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 248 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 249 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 250 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 251 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 252 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 253 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 254 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 255 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 256 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 257 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 258 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 259 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 260 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 261 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 262 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 263 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 264 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 265 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 266 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 267 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 268 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 269 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 270 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 271 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 272 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 273 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 274 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 275 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 276 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 277 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 278 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 279 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 280 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 281 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 282 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 283 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 284 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 285 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 286 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 287 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 288 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 289 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 290 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 291 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 292 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 293 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 294 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 295 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 296 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 297 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 298 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 299 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 300 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 301 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 302 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 303 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 304 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 305 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 306 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 307 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 308 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 309 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 310 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 311 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 312 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 313 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 314 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 315 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 316 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 317 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 318 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 319 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 320 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 321 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 322 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 323 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 324 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 325 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 326 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 327 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 328 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 329 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 330 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 331 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 332 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 333 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 334 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 335 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 336 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 337 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 338 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 339 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 340 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 341 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 342 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 343 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 344 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.21s\n",
      "Epoch: 345 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 346 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.2s\n",
      "Epoch: 347 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 348 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 349 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 350 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 351 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 352 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 353 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 354 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 355 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 356 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 357 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 358 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 359 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 360 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 361 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 362 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 363 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 364 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 365 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 366 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.15s\n",
      "Epoch: 367 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 368 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 369 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 370 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 371 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 372 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 373 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 374 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 375 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 376 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 377 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 378 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 379 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 380 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 381 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 382 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 383 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 384 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 385 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 386 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 387 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 388 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 389 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 390 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 391 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 392 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 393 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 394 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 395 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 396 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 397 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 398 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 399 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 400 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 401 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 402 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 403 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 404 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 405 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 406 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 407 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 408 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 409 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 410 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 411 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 412 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 413 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 414 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 415 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 416 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 417 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 418 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 419 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 420 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 421 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 422 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 423 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 424 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 425 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 426 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 427 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 428 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 429 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 430 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 431 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 432 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 433 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 434 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 435 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 436 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 437 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 438 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 439 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 440 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 441 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 442 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 443 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 444 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 445 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 446 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 447 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 448 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 449 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 450 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 451 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 452 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 453 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 454 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 455 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 456 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 457 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 458 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 459 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 460 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 461 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 462 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 463 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 464 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 465 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 466 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 467 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 468 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.19s\n",
      "Epoch: 469 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 470 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 471 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 472 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 473 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 474 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 475 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 476 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 477 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 478 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 479 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 480 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 481 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 482 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 483 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 484 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 485 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 486 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 487 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 488 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 489 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 490 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 491 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 492 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 493 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 494 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 495 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 496 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 497 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 498 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 499 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 500 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 501 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 502 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 503 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 504 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 505 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 506 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 507 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 508 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 509 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 510 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 511 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 512 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 513 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 514 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 515 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 516 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 517 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 518 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 519 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 520 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 521 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 522 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 523 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 524 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 525 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 526 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 527 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 528 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 529 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 530 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 531 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 532 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 533 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 534 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 535 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 536 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 537 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 538 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 539 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 540 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 541 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.17s\n",
      "Epoch: 542 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 543 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 544 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 545 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 546 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 547 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 548 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 549 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 550 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 551 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 552 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 553 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 554 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 555 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 556 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 557 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 558 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 559 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 560 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 561 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 562 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 563 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 564 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 565 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 566 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.15s\n",
      "Epoch: 567 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.13s\n",
      "Epoch: 568 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.17s\n",
      "Epoch: 569 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 570 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 571 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 572 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 573 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 574 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 575 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 576 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 577 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 578 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 579 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 580 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 581 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 582 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 583 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 584 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 585 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 586 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 587 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 588 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 589 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 590 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 591 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 592 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 593 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 594 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 595 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 596 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 597 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 598 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 599 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 600 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 601 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 602 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 603 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 604 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 605 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 606 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 607 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 608 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 609 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 610 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 611 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 612 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 613 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 614 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 615 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 616 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 617 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 618 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 619 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 620 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 621 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 622 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 623 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 624 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 625 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 626 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 627 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 628 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 629 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 630 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 631 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 632 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 633 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 634 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 635 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 636 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 637 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 638 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 639 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 640 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 641 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 642 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 643 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 644 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 645 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 646 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 647 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 648 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 649 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 650 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 651 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 652 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 653 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 654 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 655 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 656 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 657 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 658 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 659 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 660 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 661 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 662 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 663 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 664 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 665 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 666 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 667 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 668 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 669 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 670 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 671 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 672 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 673 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 674 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 675 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 676 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 677 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 678 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 679 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 680 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 681 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.16s\n",
      "Epoch: 682 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 683 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 684 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 685 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 686 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 687 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 688 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 689 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 690 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 691 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 692 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 693 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 694 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 695 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 696 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 697 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 698 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 699 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 700 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 701 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 702 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 703 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 704 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 705 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 706 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 707 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 708 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 709 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 710 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 711 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 712 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 713 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 714 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 715 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 716 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 717 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 718 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 719 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 720 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 721 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 722 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 723 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 724 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 725 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 726 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 727 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 728 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 729 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 730 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 731 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 732 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 733 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 734 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 735 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 736 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 737 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 738 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 739 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 740 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 741 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 742 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 743 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 744 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 745 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 746 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 747 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 748 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.22s\n",
      "Epoch: 749 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 750 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 751 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 752 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 753 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 754 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 755 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 756 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 757 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 758 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 759 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 760 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 761 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 762 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 763 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 764 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 765 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 766 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 767 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 768 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 769 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 770 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 771 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 772 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 773 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 774 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 775 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 776 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 777 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.14s\n",
      "Epoch: 778 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 779 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 780 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 781 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 782 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 783 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 784 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 785 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 786 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 787 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 788 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 789 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 790 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 791 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 792 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 793 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 794 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 795 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 796 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 797 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 798 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 799 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 800 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 801 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 802 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 803 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 804 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.1s\n",
      "Epoch: 805 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 806 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 807 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 808 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 809 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 810 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 811 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 812 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 813 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 814 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 815 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 816 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 817 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 818 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 819 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 820 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 821 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 822 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 823 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 824 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 825 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 826 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 827 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 828 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 829 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 830 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 831 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 832 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 833 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 834 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 835 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 836 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 837 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 838 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 839 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 840 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 841 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 842 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 843 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 844 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 845 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 846 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 847 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 848 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 849 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 850 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 851 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 852 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 853 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 854 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 855 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 856 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 857 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 858 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 859 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 860 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 861 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 862 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 863 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 864 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 865 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 866 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 867 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 868 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 869 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 870 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 871 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 872 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 873 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 874 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 875 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 876 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 877 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 878 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 879 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 880 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 881 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 882 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 883 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 884 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 885 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 886 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 887 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 888 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 889 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 890 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 891 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 892 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 893 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 894 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 895 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 896 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 897 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 898 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 899 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 900 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 901 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 902 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 903 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 904 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.691 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 905 Train loss 0.802, Test loss 0.793 Train R2 0.685, Test R2 0.686 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 906 Train loss 0.802, Test loss 0.793 Train R2 0.681, Test R2 0.681 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 907 Train loss 0.802, Test loss 0.794 Train R2 0.674, Test R2 0.674 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 908 Train loss 0.802, Test loss 0.793 Train R2 0.669, Test R2 0.682 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 909 Train loss 0.802, Test loss 0.793 Train R2 0.675, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 910 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 911 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.685 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 912 Train loss 0.802, Test loss 0.793 Train R2 0.679, Test R2 0.686 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 913 Train loss 0.802, Test loss 0.793 Train R2 0.681, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 914 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.691 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 915 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.687 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 916 Train loss 0.802, Test loss 0.793 Train R2 0.682, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 917 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 918 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.689 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 919 Train loss 0.802, Test loss 0.793 Train R2 0.684, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 920 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 921 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.691 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 922 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.692 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 923 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 924 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 925 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 926 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 927 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.693 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 928 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 929 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 930 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 931 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 932 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 933 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 934 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.693 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 935 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 936 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 937 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 938 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 939 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 940 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 941 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 942 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 943 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 944 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 945 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 946 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 947 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 948 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 949 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 950 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 951 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 952 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 953 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 954 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 955 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 956 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 957 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 958 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 959 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 960 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 961 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 962 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 963 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 964 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 965 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 966 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 967 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 968 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 969 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 970 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 971 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 972 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 973 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 974 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 975 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 976 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 977 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 978 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 979 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 980 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 981 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 982 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 983 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 984 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 985 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 986 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 987 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 988 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 989 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 990 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 991 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 992 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 993 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 994 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 995 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 996 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 997 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 998 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 999 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1000 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1001 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1002 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1003 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1004 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1005 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1006 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1007 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1008 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1009 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1010 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1011 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1012 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1013 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1014 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1015 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1016 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1017 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1018 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1019 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1020 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1021 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1022 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1023 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1024 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1025 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1026 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1027 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1028 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1029 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1030 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1031 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1032 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1033 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1034 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1035 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1036 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1037 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1038 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1039 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1040 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1041 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1042 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1043 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1044 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1045 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1046 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1047 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1048 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1049 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1050 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1051 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1052 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1053 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1054 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1055 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1056 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1057 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1058 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1059 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1060 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1061 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1062 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1063 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1064 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1065 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1066 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1067 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1068 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1069 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1070 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1071 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1072 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1073 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1074 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1075 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1076 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1077 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1078 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1079 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1080 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1081 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1082 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1083 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1084 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1085 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1086 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1087 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1088 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1089 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1090 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1091 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1092 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1093 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1094 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1095 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1096 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1097 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1098 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1099 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1100 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1101 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1102 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1103 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1104 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1105 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1106 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1107 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1108 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1109 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1110 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1111 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1112 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1113 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1114 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1115 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1116 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1117 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1118 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1119 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1120 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1121 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1122 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1123 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1124 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1125 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1126 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1127 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1128 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1129 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1130 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1131 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1132 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1133 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1134 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1135 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1136 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1137 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1138 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1139 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1140 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1141 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1142 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1143 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1144 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1145 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1146 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1147 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1148 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1149 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1150 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1151 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1152 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1153 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1154 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1155 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1156 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1157 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1158 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1159 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1160 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1161 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1162 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1163 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1164 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1165 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1166 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1167 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1168 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1169 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1170 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1171 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1172 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1173 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1174 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1175 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1176 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1177 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1178 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1179 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1180 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1181 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1182 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1183 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1184 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1185 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1186 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1187 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1188 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1189 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1190 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1191 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1192 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1193 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1194 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1195 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1196 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1197 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1198 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1199 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1200 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1201 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1202 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1203 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1204 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1205 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1206 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1207 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1208 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1209 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1210 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1211 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1212 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1213 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1214 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1215 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1216 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1217 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1218 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1219 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1220 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1221 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1222 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1223 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1224 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1225 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1226 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1227 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1228 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1229 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1230 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1231 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1232 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1233 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1234 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1235 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1236 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1237 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1238 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1239 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1240 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1241 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1242 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1243 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1244 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1245 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1246 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1247 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1248 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1249 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1250 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1251 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1252 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1253 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1254 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1255 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1256 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1257 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1258 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1259 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1260 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1261 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1262 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1263 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1264 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1265 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1266 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1267 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1268 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1269 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1270 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1271 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1272 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1273 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1274 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1275 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1276 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1277 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1278 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1279 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1280 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1281 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1282 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1283 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1284 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1285 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1286 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1287 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1288 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1289 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1290 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1291 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1292 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1293 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1294 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1295 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1296 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1297 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1298 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1299 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1300 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1301 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1302 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1303 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1304 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1305 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1306 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1307 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1308 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1309 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1310 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1311 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1312 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1313 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1314 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1315 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1316 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1317 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1318 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1319 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1320 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1321 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1322 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1323 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1324 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1325 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1326 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1327 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1328 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1329 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1330 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1331 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1332 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1333 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1334 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1335 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1336 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1337 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1338 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1339 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1340 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1341 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1342 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1343 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1344 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1345 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1346 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1347 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1348 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1349 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1350 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1351 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1352 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1353 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1354 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1355 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1356 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1357 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1358 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1359 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1360 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1361 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1362 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1363 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1364 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1365 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1366 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1367 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1368 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1369 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1370 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1371 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1372 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1373 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1374 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1375 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1376 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1377 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1378 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1379 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1380 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1381 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1382 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1383 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1384 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1385 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1386 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1387 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1388 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1389 Train loss 0.803, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1390 Train loss 0.803, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1391 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1392 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1393 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1394 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1395 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1396 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1397 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1398 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1399 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1400 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1401 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1402 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1403 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1404 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1405 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1406 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1407 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1408 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1409 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1410 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1411 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1412 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1413 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1414 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1415 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1416 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1417 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1418 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1419 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1420 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1421 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.693 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1422 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.691 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1423 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.688 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1424 Train loss 0.802, Test loss 0.794 Train R2 0.682, Test R2 0.677 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1425 Train loss 0.803, Test loss 0.794 Train R2 0.672, Test R2 0.665 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1426 Train loss 0.803, Test loss 0.794 Train R2 0.658, Test R2 0.660 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1427 Train loss 0.803, Test loss 0.793 Train R2 0.655, Test R2 0.683 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1428 Train loss 0.802, Test loss 0.793 Train R2 0.677, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1429 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.678 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1430 Train loss 0.802, Test loss 0.793 Train R2 0.674, Test R2 0.678 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1431 Train loss 0.802, Test loss 0.793 Train R2 0.671, Test R2 0.692 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1432 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.688 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1433 Train loss 0.802, Test loss 0.793 Train R2 0.683, Test R2 0.682 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1434 Train loss 0.802, Test loss 0.793 Train R2 0.675, Test R2 0.691 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1435 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.691 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1436 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.685 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1437 Train loss 0.802, Test loss 0.793 Train R2 0.679, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1438 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.692 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1439 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.688 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1440 Train loss 0.802, Test loss 0.793 Train R2 0.682, Test R2 0.693 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1441 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1442 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.690 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1443 Train loss 0.802, Test loss 0.793 Train R2 0.684, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1444 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.692 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1445 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.691 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1446 Train loss 0.802, Test loss 0.793 Train R2 0.686, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1447 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1448 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1449 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1450 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.692 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1451 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.693 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1452 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1453 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.692 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1454 Train loss 0.802, Test loss 0.793 Train R2 0.687, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1455 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1456 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.693 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1457 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1458 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1459 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.693 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1460 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1461 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1462 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1463 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1464 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1465 Train loss 0.802, Test loss 0.793 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1466 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1467 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1468 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1469 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1470 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1471 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1472 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1473 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1474 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1475 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1476 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1477 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1478 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1479 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1480 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1481 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1482 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1483 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1484 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1485 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1486 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1487 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1488 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1489 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1490 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1491 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1492 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1493 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1494 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1495 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1496 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1497 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1498 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1499 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1500 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1501 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.11s\n",
      "Epoch: 1502 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1503 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1504 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1505 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1506 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1507 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1508 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1509 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1510 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1511 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1512 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1513 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1514 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1515 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1516 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1517 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1518 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1519 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1520 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1521 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1522 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1523 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1524 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1525 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1526 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1527 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1528 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1529 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1530 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1531 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1532 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1533 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1534 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1535 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1536 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1537 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1538 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1539 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1540 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1541 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1542 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1543 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1544 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1545 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1546 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1547 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1548 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1549 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1550 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1551 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1552 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1553 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1554 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1555 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1556 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1557 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1558 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1559 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1560 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1561 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1562 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1563 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1564 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1565 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1566 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1567 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1568 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1569 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1570 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1571 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1572 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1573 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1574 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1575 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1576 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1577 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1578 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1579 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1580 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1581 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1582 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1583 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1584 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1585 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1586 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1587 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1588 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1589 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1590 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1591 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1592 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1593 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1594 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1595 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1596 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1597 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1598 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1599 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1600 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1601 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1602 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1603 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1604 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1605 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1606 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1607 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1608 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1609 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1610 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1611 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1612 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1613 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1614 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1615 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1616 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1617 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1618 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1619 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1620 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1621 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1622 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1623 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1624 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1625 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1626 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1627 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1628 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1629 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1630 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1631 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1632 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1633 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1634 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1635 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1636 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1637 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1638 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1639 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1640 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1641 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1642 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1643 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1644 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1645 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1646 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1647 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1648 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1649 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1650 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1651 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1652 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1653 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1654 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1655 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1656 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1657 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1658 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1659 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1660 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1661 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1662 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1663 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1664 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1665 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1666 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1667 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1668 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1669 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1670 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1671 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1672 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1673 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1674 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1675 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1676 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1677 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1678 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1679 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 1680 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1681 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1682 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1683 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1684 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1685 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1686 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1687 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1688 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1689 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1690 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1691 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1692 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1693 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1694 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1695 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1696 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1697 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1698 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1699 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1700 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1701 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1702 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1703 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1704 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1705 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1706 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1707 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1708 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1709 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1710 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1711 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1712 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1713 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1714 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1715 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1716 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1717 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1718 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1719 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1720 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1721 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1722 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1723 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1724 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1725 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1726 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1727 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1728 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1729 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1730 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1731 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1732 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1733 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1734 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1735 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1736 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1737 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1738 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1739 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1740 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1741 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1742 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1743 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1744 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1745 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1746 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1747 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1748 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1749 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1750 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1751 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1752 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1753 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1754 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1755 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1756 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1757 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1758 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1759 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1760 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1761 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1762 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1763 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1764 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1765 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1766 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1767 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1768 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1769 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1770 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1771 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1772 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1773 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1774 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1775 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1776 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1777 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1778 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1779 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1780 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1781 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1782 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1783 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1784 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1785 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1786 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1787 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1788 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1789 Train loss 0.803, Test loss 0.794 Train R2 0.688, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1790 Train loss 0.803, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1791 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1792 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1793 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1794 Train loss 0.802, Test loss 0.794 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1795 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1796 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1797 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1798 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1799 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1800 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1801 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1802 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1803 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1804 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1805 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1806 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1807 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1808 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1809 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1810 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1811 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1812 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1813 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1814 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1815 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1816 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1817 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1818 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1819 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1820 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1821 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1822 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1823 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1824 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1825 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1826 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1827 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1828 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1829 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1830 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1831 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1832 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1833 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1834 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1835 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1836 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1837 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1838 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1839 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1840 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1841 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1842 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1843 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1844 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1845 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1846 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1847 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1848 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1849 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1850 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1851 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1852 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1853 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1854 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1855 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1856 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1857 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1858 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1859 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1860 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1861 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1862 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1863 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1864 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1865 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1866 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1867 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1868 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1869 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1870 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1871 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1872 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1873 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1874 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1875 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1876 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1877 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1878 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1879 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1880 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1881 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1882 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1883 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1884 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1885 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1886 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1887 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1888 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1889 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1890 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1891 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1892 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1893 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.2s\n",
      "Epoch: 1894 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1895 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1896 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1897 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1898 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1899 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1900 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1901 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1902 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1903 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1904 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1905 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1906 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1907 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1908 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1909 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1910 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1911 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1912 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1913 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1914 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1915 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1916 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1917 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1918 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1919 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1920 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1921 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1922 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1923 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1924 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1925 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1926 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1927 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1928 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1929 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1930 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1931 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1932 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1933 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1934 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1935 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1936 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1937 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1938 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1939 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1940 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1941 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1942 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1943 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1944 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 1945 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1946 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1947 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1948 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1949 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1950 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1951 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1952 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1953 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1954 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1955 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1956 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1957 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1958 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1959 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1960 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1961 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1962 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1963 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1964 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1965 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1966 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1967 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1968 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1969 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1970 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 1971 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1972 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1973 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1974 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1975 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1976 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1977 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1978 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1979 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1980 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1981 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1982 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1983 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1984 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1985 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1986 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1987 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1988 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1989 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1990 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1991 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1992 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1993 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 1994 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1995 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 1996 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 1997 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 1998 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 1999 Train loss 0.802, Test loss 0.793 Train R2 0.689, Test R2 0.694 \n"
     ]
    }
   ],
   "source": [
    "#TEST with model trainer\n",
    "import omegaconf\n",
    "\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.util.model_trainer import ModelTrainerOverriden\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "import mbrl.util.common\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 1\n",
    "device = \"cpu\"\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "optim_lr=learningRate\n",
    "model_wd=0.\n",
    "model_batch_size=dataset_size\n",
    "validation_ratio=test_split_ratio\n",
    "num_epochs= epochs\n",
    "\n",
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "#Seed\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# torch_generator = torch.Generator(device=device)\n",
    "# if seed is not None:\n",
    "#     torch_generator.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#Dynamics model\n",
    "from src.model.simple import Simple\n",
    "from src.model.gaussian_process import MultiOutputGP\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    LinearRegression(in_size, out_size, device),\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "#Load replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    dataset_size,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=dataset_size)\n",
    "\n",
    "dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    model_batch_size,\n",
    "    validation_ratio,\n",
    "    ensemble_size=len(dynamics_model),\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,\n",
    ")\n",
    "\n",
    "if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "    dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "train_losses, test_losses, train_metrics, test_metrics = model_trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=num_epochs,\n",
    "    evaluate=True,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_model.model(train_x)\n",
    "    test_preds = dynamics_model.model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.simple import Simple\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.0001\n",
    "weight_decay = 0.00001\n",
    "optim_eps = 1e-8\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "NN_model = Simple(\n",
    "    in_size, \n",
    "    out_size, \n",
    "    device,\n",
    "    num_layers=2,\n",
    "    hid_size=100,\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                NN_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 5.084537029266357, R2 -0.35017168521881104\n",
      "Eval loss 5.101757526397705, R2 -0.34853339195251465\n",
      "epoch 1, loss 4.560205459594727, R2 -0.1813887506723404\n",
      "Eval loss 4.5604753494262695, R2 -0.17539280652999878\n",
      "epoch 2, loss 3.709505081176758, R2 0.00689114211127162\n",
      "Eval loss 3.7030155658721924, R2 0.018820814788341522\n",
      "epoch 3, loss 3.2644011974334717, R2 0.09219035506248474\n",
      "Eval loss 3.3018875122070312, R2 0.09712818264961243\n",
      "epoch 4, loss 3.147446393966675, R2 0.11825559288263321\n",
      "Eval loss 3.207768440246582, R2 0.11668994277715683\n",
      "epoch 5, loss 3.0691137313842773, R2 0.13322535157203674\n",
      "Eval loss 3.1312477588653564, R2 0.1318073719739914\n",
      "epoch 6, loss 2.986293315887451, R2 0.1494521051645279\n",
      "Eval loss 3.0499651432037354, R2 0.14798718690872192\n",
      "epoch 7, loss 2.894711494445801, R2 0.16709300875663757\n",
      "Eval loss 2.960143566131592, R2 0.1654273122549057\n",
      "epoch 8, loss 2.7952778339385986, R2 0.1864708811044693\n",
      "Eval loss 2.8612163066864014, R2 0.18521390855312347\n",
      "epoch 9, loss 2.6898558139801025, R2 0.20747973024845123\n",
      "Eval loss 2.7581300735473633, R2 0.20611706376075745\n",
      "epoch 10, loss 2.5802106857299805, R2 0.23122316598892212\n",
      "Eval loss 2.649510622024536, R2 0.23002900183200836\n",
      "epoch 11, loss 2.4673779010772705, R2 0.25639864802360535\n",
      "Eval loss 2.538316488265991, R2 0.25508758425712585\n",
      "epoch 12, loss 2.352557897567749, R2 0.2819962799549103\n",
      "Eval loss 2.425096273422241, R2 0.28046461939811707\n",
      "epoch 13, loss 2.2370450496673584, R2 0.30712053179740906\n",
      "Eval loss 2.311392307281494, R2 0.3051970899105072\n",
      "epoch 14, loss 2.1202681064605713, R2 0.3324419856071472\n",
      "Eval loss 2.1961774826049805, R2 0.3303031921386719\n",
      "epoch 15, loss 2.0033156871795654, R2 0.3578978180885315\n",
      "Eval loss 2.0811991691589355, R2 0.35506224632263184\n",
      "epoch 16, loss 1.88866126537323, R2 0.3828373849391937\n",
      "Eval loss 1.968503713607788, R2 0.37932834029197693\n",
      "epoch 17, loss 1.7790701389312744, R2 0.406717985868454\n",
      "Eval loss 1.8606743812561035, R2 0.4025222063064575\n",
      "epoch 18, loss 1.6756558418273926, R2 0.42923295497894287\n",
      "Eval loss 1.758486270904541, R2 0.4244026839733124\n",
      "epoch 19, loss 1.5787427425384521, R2 0.4505406618118286\n",
      "Eval loss 1.6621365547180176, R2 0.4452371597290039\n",
      "epoch 20, loss 1.4884896278381348, R2 0.4707970917224884\n",
      "Eval loss 1.5716981887817383, R2 0.4651724100112915\n",
      "epoch 21, loss 1.4049174785614014, R2 0.49007436633110046\n",
      "Eval loss 1.4871667623519897, R2 0.48424434661865234\n",
      "epoch 22, loss 1.3282674551010132, R2 0.5083236694335938\n",
      "Eval loss 1.4087761640548706, R2 0.5024963021278381\n",
      "epoch 23, loss 1.2586932182312012, R2 0.5254606008529663\n",
      "Eval loss 1.3368927240371704, R2 0.5198166370391846\n",
      "epoch 24, loss 1.1962165832519531, R2 0.5413904786109924\n",
      "Eval loss 1.271645188331604, R2 0.5361077785491943\n",
      "epoch 25, loss 1.1407041549682617, R2 0.5560182929039001\n",
      "Eval loss 1.2129992246627808, R2 0.5512500405311584\n",
      "epoch 26, loss 1.091849446296692, R2 0.5693175792694092\n",
      "Eval loss 1.1608140468597412, R2 0.5651830434799194\n",
      "epoch 27, loss 1.0492362976074219, R2 0.581325352191925\n",
      "Eval loss 1.1148563623428345, R2 0.5778842568397522\n",
      "epoch 28, loss 1.0123645067214966, R2 0.5921310782432556\n",
      "Eval loss 1.0747803449630737, R2 0.5893803238868713\n",
      "epoch 29, loss 0.9807354211807251, R2 0.6018247604370117\n",
      "Eval loss 1.040203332901001, R2 0.599713921546936\n",
      "epoch 30, loss 0.9538806080818176, R2 0.6104834079742432\n",
      "Eval loss 1.0107423067092896, R2 0.6089363098144531\n",
      "epoch 31, loss 0.931358814239502, R2 0.6181802153587341\n",
      "Eval loss 0.9859046339988708, R2 0.6171324253082275\n",
      "epoch 32, loss 0.9126355051994324, R2 0.625009298324585\n",
      "Eval loss 0.9651562571525574, R2 0.6243929862976074\n",
      "epoch 33, loss 0.8971784710884094, R2 0.6310735940933228\n",
      "Eval loss 0.9479002356529236, R2 0.6308495998382568\n",
      "epoch 34, loss 0.8844226002693176, R2 0.6364753842353821\n",
      "Eval loss 0.9334802031517029, R2 0.6366352438926697\n",
      "epoch 35, loss 0.8738373517990112, R2 0.6413170099258423\n",
      "Eval loss 0.9212787747383118, R2 0.6418774127960205\n",
      "epoch 36, loss 0.8649380207061768, R2 0.6456977725028992\n",
      "Eval loss 0.9108126759529114, R2 0.6466720104217529\n",
      "epoch 37, loss 0.8573553562164307, R2 0.6496858596801758\n",
      "Eval loss 0.90171217918396, R2 0.6510820388793945\n",
      "epoch 38, loss 0.8508023023605347, R2 0.6533300876617432\n",
      "Eval loss 0.8936733603477478, R2 0.6551592350006104\n",
      "epoch 39, loss 0.8450659513473511, R2 0.6566713452339172\n",
      "Eval loss 0.8864725828170776, R2 0.6589483618736267\n",
      "epoch 40, loss 0.8399957418441772, R2 0.6597296595573425\n",
      "Eval loss 0.8799671530723572, R2 0.6624661087989807\n",
      "epoch 41, loss 0.8354834914207458, R2 0.6625204682350159\n",
      "Eval loss 0.8740501403808594, R2 0.665729820728302\n",
      "epoch 42, loss 0.8314517140388489, R2 0.6650621294975281\n",
      "Eval loss 0.8686403632164001, R2 0.6687566637992859\n",
      "epoch 43, loss 0.8278369903564453, R2 0.6673728227615356\n",
      "Eval loss 0.8637017011642456, R2 0.6715531945228577\n",
      "epoch 44, loss 0.8245943784713745, R2 0.6694698929786682\n",
      "Eval loss 0.8591809272766113, R2 0.67413330078125\n",
      "epoch 45, loss 0.8216872215270996, R2 0.6713694334030151\n",
      "Eval loss 0.8550633788108826, R2 0.676501989364624\n",
      "epoch 46, loss 0.8190909028053284, R2 0.6730836629867554\n",
      "Eval loss 0.8513212203979492, R2 0.6786717772483826\n",
      "epoch 47, loss 0.8167744874954224, R2 0.6746264100074768\n",
      "Eval loss 0.8479408621788025, R2 0.6806450486183167\n",
      "epoch 48, loss 0.8147177696228027, R2 0.6760032773017883\n",
      "Eval loss 0.844918966293335, R2 0.6824226975440979\n",
      "epoch 49, loss 0.8128959536552429, R2 0.6772286891937256\n",
      "Eval loss 0.8422029614448547, R2 0.6840338110923767\n",
      "epoch 50, loss 0.8112974166870117, R2 0.678306519985199\n",
      "Eval loss 0.8397673964500427, R2 0.6854812502861023\n",
      "epoch 51, loss 0.8099042773246765, R2 0.6792430281639099\n",
      "Eval loss 0.8376345634460449, R2 0.6867546439170837\n",
      "epoch 52, loss 0.8086942434310913, R2 0.6800482273101807\n",
      "Eval loss 0.8357480764389038, R2 0.6878687143325806\n",
      "epoch 53, loss 0.8076481819152832, R2 0.6807330846786499\n",
      "Eval loss 0.8341148495674133, R2 0.688828706741333\n",
      "epoch 54, loss 0.8067436218261719, R2 0.6813127994537354\n",
      "Eval loss 0.8326800465583801, R2 0.6896539926528931\n",
      "epoch 55, loss 0.8059622049331665, R2 0.681800127029419\n",
      "Eval loss 0.8314375281333923, R2 0.6903569102287292\n",
      "epoch 56, loss 0.8052847981452942, R2 0.6822084784507751\n",
      "Eval loss 0.8303694725036621, R2 0.6909514665603638\n",
      "epoch 57, loss 0.8046892881393433, R2 0.6825586557388306\n",
      "Eval loss 0.8294623494148254, R2 0.6914503574371338\n",
      "epoch 58, loss 0.8041654825210571, R2 0.6828558444976807\n",
      "Eval loss 0.8286598920822144, R2 0.691871702671051\n",
      "epoch 59, loss 0.8037006258964539, R2 0.6831112504005432\n",
      "Eval loss 0.8279722332954407, R2 0.6922271251678467\n",
      "epoch 60, loss 0.8032819628715515, R2 0.6833344101905823\n",
      "Eval loss 0.827373743057251, R2 0.6925344467163086\n",
      "epoch 61, loss 0.8028998374938965, R2 0.6835334300994873\n",
      "Eval loss 0.8268473148345947, R2 0.6927990317344666\n",
      "epoch 62, loss 0.8025485873222351, R2 0.6837108731269836\n",
      "Eval loss 0.8263879418373108, R2 0.6930269002914429\n",
      "epoch 63, loss 0.8022276163101196, R2 0.6838697791099548\n",
      "Eval loss 0.8259795904159546, R2 0.6932252049446106\n",
      "epoch 64, loss 0.80193030834198, R2 0.6840140223503113\n",
      "Eval loss 0.8256165385246277, R2 0.6934028267860413\n",
      "epoch 65, loss 0.8016502857208252, R2 0.6841509938240051\n",
      "Eval loss 0.825285792350769, R2 0.6935612559318542\n",
      "epoch 66, loss 0.8013841509819031, R2 0.6842797994613647\n",
      "Eval loss 0.8249883055686951, R2 0.6937039494514465\n",
      "epoch 67, loss 0.801131010055542, R2 0.6844018697738647\n",
      "Eval loss 0.8247144818305969, R2 0.6938368082046509\n",
      "epoch 68, loss 0.8008893132209778, R2 0.68450528383255\n",
      "Eval loss 0.8244622945785522, R2 0.6939602494239807\n",
      "epoch 69, loss 0.800656795501709, R2 0.6846298575401306\n",
      "Eval loss 0.8242288827896118, R2 0.6940756440162659\n",
      "epoch 70, loss 0.8004330396652222, R2 0.6847383975982666\n",
      "Eval loss 0.8240092396736145, R2 0.6941824555397034\n",
      "epoch 71, loss 0.8002180457115173, R2 0.6848430633544922\n",
      "Eval loss 0.8238032460212708, R2 0.6942834258079529\n",
      "epoch 72, loss 0.8000113368034363, R2 0.684943437576294\n",
      "Eval loss 0.8236106038093567, R2 0.6943762898445129\n",
      "epoch 73, loss 0.7998135685920715, R2 0.6850375533103943\n",
      "Eval loss 0.8234264850616455, R2 0.6944596171379089\n",
      "epoch 74, loss 0.7996228933334351, R2 0.6851330399513245\n",
      "Eval loss 0.8232539296150208, R2 0.6945505738258362\n",
      "epoch 75, loss 0.7994394302368164, R2 0.6852236390113831\n",
      "Eval loss 0.8230921626091003, R2 0.6946313381195068\n",
      "epoch 76, loss 0.7992624044418335, R2 0.6853107810020447\n",
      "Eval loss 0.8229331374168396, R2 0.6947144865989685\n",
      "epoch 77, loss 0.7990907430648804, R2 0.6853967308998108\n",
      "Eval loss 0.8227853178977966, R2 0.6947886943817139\n",
      "epoch 78, loss 0.798922598361969, R2 0.6854824423789978\n",
      "Eval loss 0.8226464986801147, R2 0.6948635578155518\n",
      "epoch 79, loss 0.7987577319145203, R2 0.6855667233467102\n",
      "Eval loss 0.8225069046020508, R2 0.6949326992034912\n",
      "epoch 80, loss 0.7985991835594177, R2 0.6856439113616943\n",
      "Eval loss 0.8223769664764404, R2 0.6950041055679321\n",
      "epoch 81, loss 0.7984464764595032, R2 0.6857244968414307\n",
      "Eval loss 0.8222475051879883, R2 0.6950700283050537\n",
      "epoch 82, loss 0.7982980012893677, R2 0.6857953667640686\n",
      "Eval loss 0.8221321105957031, R2 0.6951344013214111\n",
      "epoch 83, loss 0.7981519103050232, R2 0.6858741641044617\n",
      "Eval loss 0.8220229148864746, R2 0.695195734500885\n",
      "epoch 84, loss 0.7980095744132996, R2 0.6859474182128906\n",
      "Eval loss 0.821915864944458, R2 0.6952556371688843\n",
      "epoch 85, loss 0.7978706359863281, R2 0.6860204339027405\n",
      "Eval loss 0.8218008875846863, R2 0.6953192353248596\n",
      "epoch 86, loss 0.7977367043495178, R2 0.6860899329185486\n",
      "Eval loss 0.8216924667358398, R2 0.6953760981559753\n",
      "epoch 87, loss 0.7976058125495911, R2 0.6861574053764343\n",
      "Eval loss 0.8215941190719604, R2 0.6954322457313538\n",
      "epoch 88, loss 0.7974777817726135, R2 0.6862225532531738\n",
      "Eval loss 0.8215129375457764, R2 0.6954757571220398\n",
      "epoch 89, loss 0.7973540425300598, R2 0.6862843036651611\n",
      "Eval loss 0.8214120864868164, R2 0.6955332159996033\n",
      "epoch 90, loss 0.797232449054718, R2 0.6863502264022827\n",
      "Eval loss 0.8213300704956055, R2 0.6955799460411072\n",
      "epoch 91, loss 0.7971129417419434, R2 0.6864116787910461\n",
      "Eval loss 0.8212599158287048, R2 0.6956212520599365\n",
      "epoch 92, loss 0.7969968318939209, R2 0.6864630579948425\n",
      "Eval loss 0.8211764693260193, R2 0.6956655383110046\n",
      "epoch 93, loss 0.7968811988830566, R2 0.6865294575691223\n",
      "Eval loss 0.8211173415184021, R2 0.6957072615623474\n",
      "epoch 94, loss 0.7967681884765625, R2 0.6865861415863037\n",
      "Eval loss 0.8210436701774597, R2 0.6957509517669678\n",
      "epoch 95, loss 0.7966594696044922, R2 0.6866323351860046\n",
      "Eval loss 0.8209852576255798, R2 0.6957688927650452\n",
      "epoch 96, loss 0.7965512871742249, R2 0.6866933703422546\n",
      "Eval loss 0.8208976984024048, R2 0.6958324909210205\n",
      "epoch 97, loss 0.7964451313018799, R2 0.6867444515228271\n",
      "Eval loss 0.8208137154579163, R2 0.6958823800086975\n",
      "epoch 98, loss 0.7963393330574036, R2 0.6867970824241638\n",
      "Eval loss 0.8207620978355408, R2 0.69591224193573\n",
      "epoch 99, loss 0.7962363362312317, R2 0.6868471503257751\n",
      "Eval loss 0.8206801414489746, R2 0.695962131023407\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    k = 0\n",
    "    l = batch_size\n",
    "    batch_loss = []\n",
    "    while l < train_x.shape[0]:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = NN_model(train_x)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, train_y)\n",
    "        batch_loss.append(loss.item())\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update k,l\n",
    "        k = l\n",
    "        l = min(l+batch_size, train_x.shape[0])\n",
    "\n",
    "    #Append train loss\n",
    "    train_losses.append(np.mean(batch_loss))\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = NN_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = NN_model(train_x)\n",
    "    test_preds = NN_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 44]) torch.Size([800, 44]) torch.Size([200, 44]) torch.Size([200, 44])\n"
     ]
    }
   ],
   "source": [
    "from src.model.gaussian_process import MultiOutputGP\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.1\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "mean = \"Linear\"\n",
    "kernel = \"Linear\"\n",
    "\n",
    "epochs = 50\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "GP_model = MultiOutputGP(in_size, out_size, device, mean, kernel)\n",
    "mll = gpytorch.mlls.SumMarginalLogLikelihood(GP_model.likelihood, GP_model.gp)\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                GP_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "dataset_size_gp = 1000\n",
    "gp_idx_split = int(test_split_ratio*dataset_size_gp)\n",
    "\n",
    "train_x_gp = train_x[gp_idx_split:dataset_size_gp, ...]\n",
    "train_y_gp = train_y[gp_idx_split:dataset_size_gp, ...]\n",
    "test_x_gp = test_x[:gp_idx_split, ...]\n",
    "test_y_gp = test_y[:gp_idx_split, ...]\n",
    "\n",
    "print(train_x_gp.shape, train_y_gp.shape, test_x_gp.shape, test_y_gp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.6525566577911377, R2 -442.6127014160156\n",
      "Eval loss 1.409150242805481, R2 0.5499072670936584\n",
      "epoch 1, loss 1.6070736646652222, R2 -370.25836181640625\n",
      "Eval loss 1.382038950920105, R2 0.5787969827651978\n",
      "epoch 2, loss 1.5723155736923218, R2 -303.2740173339844\n",
      "Eval loss 1.360946536064148, R2 0.6033591628074646\n",
      "epoch 3, loss 1.5456608533859253, R2 -246.81480407714844\n",
      "Eval loss 1.344666838645935, R2 0.6236982345581055\n",
      "epoch 4, loss 1.5247894525527954, R2 -199.2003936767578\n",
      "Eval loss 1.3321117162704468, R2 0.6399360299110413\n",
      "epoch 5, loss 1.508100986480713, R2 -158.91722106933594\n",
      "Eval loss 1.322338581085205, R2 0.6522696614265442\n",
      "epoch 6, loss 1.4944539070129395, R2 -124.80152130126953\n",
      "Eval loss 1.3146113157272339, R2 0.6609153151512146\n",
      "epoch 7, loss 1.483010172843933, R2 -96.12910461425781\n",
      "Eval loss 1.3082785606384277, R2 0.6662331819534302\n",
      "epoch 8, loss 1.4730991125106812, R2 -72.44343566894531\n",
      "Eval loss 1.3028409481048584, R2 0.6687151193618774\n",
      "epoch 9, loss 1.4642326831817627, R2 -53.42706298828125\n",
      "Eval loss 1.2981257438659668, R2 0.6689431667327881\n",
      "epoch 10, loss 1.4561302661895752, R2 -38.80351257324219\n",
      "Eval loss 1.2939013242721558, R2 0.6678144335746765\n",
      "epoch 11, loss 1.4485750198364258, R2 -28.196636199951172\n",
      "Eval loss 1.2900516986846924, R2 0.6662544012069702\n",
      "epoch 12, loss 1.4413737058639526, R2 -20.99057388305664\n",
      "Eval loss 1.286475658416748, R2 0.6651242971420288\n",
      "epoch 13, loss 1.4344271421432495, R2 -16.310503005981445\n",
      "Eval loss 1.283228874206543, R2 0.6649078130722046\n",
      "epoch 14, loss 1.427641749382019, R2 -13.175434112548828\n",
      "Eval loss 1.2802759408950806, R2 0.6657324433326721\n",
      "epoch 15, loss 1.4209481477737427, R2 -10.78738784790039\n",
      "Eval loss 1.2776182889938354, R2 0.6673374772071838\n",
      "epoch 16, loss 1.4142484664916992, R2 -8.736515998840332\n",
      "Eval loss 1.2751903533935547, R2 0.6693050861358643\n",
      "epoch 17, loss 1.4075227975845337, R2 -6.967267990112305\n",
      "Eval loss 1.272889494895935, R2 0.6711816787719727\n",
      "epoch 18, loss 1.4007548093795776, R2 -5.549572467803955\n",
      "Eval loss 1.2707120180130005, R2 0.6725630164146423\n",
      "epoch 19, loss 1.393979549407959, R2 -4.473941802978516\n",
      "Eval loss 1.268707513809204, R2 0.6731948852539062\n",
      "epoch 20, loss 1.3872807025909424, R2 -3.6485016345977783\n",
      "Eval loss 1.2669175863265991, R2 0.6731020212173462\n",
      "epoch 21, loss 1.3806785345077515, R2 -2.973177194595337\n",
      "Eval loss 1.2653746604919434, R2 0.6725804805755615\n",
      "epoch 22, loss 1.3741627931594849, R2 -2.4591078758239746\n",
      "Eval loss 1.2639826536178589, R2 0.6721362471580505\n",
      "epoch 23, loss 1.3676977157592773, R2 -2.154120922088623\n",
      "Eval loss 1.2626146078109741, R2 0.6721873879432678\n",
      "epoch 24, loss 1.3612651824951172, R2 -1.9979298114776611\n",
      "Eval loss 1.261228322982788, R2 0.6727672219276428\n",
      "epoch 25, loss 1.354844570159912, R2 -1.7037159204483032\n",
      "Eval loss 1.2599142789840698, R2 0.6735835671424866\n",
      "epoch 26, loss 1.3484059572219849, R2 -1.0864590406417847\n",
      "Eval loss 1.2586781978607178, R2 0.6742873787879944\n",
      "epoch 27, loss 1.3419257402420044, R2 -0.47039273381233215\n",
      "Eval loss 1.2575381994247437, R2 0.6746689677238464\n",
      "epoch 28, loss 1.3353824615478516, R2 -0.05845621973276138\n",
      "Eval loss 1.2564582824707031, R2 0.6746845841407776\n",
      "epoch 29, loss 1.3287948369979858, R2 0.18494068086147308\n",
      "Eval loss 1.2554266452789307, R2 0.674572765827179\n",
      "epoch 30, loss 1.3222228288650513, R2 0.23573073744773865\n",
      "Eval loss 1.254416823387146, R2 0.6747360825538635\n",
      "epoch 31, loss 1.3155977725982666, R2 0.1660180687904358\n",
      "Eval loss 1.253571629524231, R2 0.6749628186225891\n",
      "epoch 32, loss 1.3088501691818237, R2 0.10655531287193298\n",
      "Eval loss 1.2529793977737427, R2 0.6750012040138245\n",
      "epoch 33, loss 1.3021467924118042, R2 0.2974998354911804\n",
      "Eval loss 1.2524980306625366, R2 0.6748231649398804\n",
      "epoch 34, loss 1.295647144317627, R2 0.4608432650566101\n",
      "Eval loss 1.252460241317749, R2 0.674228847026825\n",
      "epoch 35, loss 1.2893348932266235, R2 0.4980684220790863\n",
      "Eval loss 1.252162218093872, R2 0.6742336750030518\n",
      "epoch 36, loss 1.2831858396530151, R2 0.45995962619781494\n",
      "Eval loss 1.2513391971588135, R2 0.674871027469635\n",
      "epoch 37, loss 1.2770949602127075, R2 0.5116732120513916\n",
      "Eval loss 1.2512761354446411, R2 0.6749265789985657\n",
      "epoch 38, loss 1.271125078201294, R2 0.5405891537666321\n",
      "Eval loss 1.251261830329895, R2 0.6750602126121521\n",
      "epoch 39, loss 1.2655284404754639, R2 0.5929330587387085\n",
      "Eval loss 1.2510515451431274, R2 0.6751936674118042\n",
      "epoch 40, loss 1.260231614112854, R2 0.6038808226585388\n",
      "Eval loss 1.2507433891296387, R2 0.6751765012741089\n",
      "epoch 41, loss 1.2550263404846191, R2 0.6279163956642151\n",
      "Eval loss 1.2508158683776855, R2 0.6751207113265991\n",
      "epoch 42, loss 1.2504777908325195, R2 0.6115236878395081\n",
      "Eval loss 1.2514760494232178, R2 0.6747332215309143\n",
      "epoch 43, loss 1.245993733406067, R2 0.6322929859161377\n",
      "Eval loss 1.2519159317016602, R2 0.6747274994850159\n",
      "epoch 44, loss 1.2418501377105713, R2 0.6314993500709534\n",
      "Eval loss 1.2524785995483398, R2 0.674299955368042\n",
      "epoch 45, loss 1.2381314039230347, R2 0.6267807483673096\n",
      "Eval loss 1.2531484365463257, R2 0.6738986372947693\n",
      "epoch 46, loss 1.2347809076309204, R2 0.6231033205986023\n",
      "Eval loss 1.2525051832199097, R2 0.6748554706573486\n",
      "epoch 47, loss 1.2315723896026611, R2 0.5648758411407471\n",
      "Eval loss 1.254289150238037, R2 0.6740771532058716\n",
      "epoch 48, loss 1.2297993898391724, R2 0.570385754108429\n",
      "Eval loss 1.2563202381134033, R2 0.6724655628204346\n",
      "epoch 49, loss 1.2301493883132935, R2 0.5751101970672607\n",
      "Eval loss 1.2551751136779785, R2 0.673427164554596\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    #Set training mode REQUIRED FOR GP\n",
    "    GP_model.gp.train()\n",
    "\n",
    "    #Set the training data\n",
    "    if epoch==0:\n",
    "        GP_model.set_train_data(train_x_gp, train_y_gp)\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = GP_model.forward()\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = -mll(outputs, GP_model.gp.train_targets)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_pred_output = GP_model.likelihood(*outputs)\n",
    "    train_pred_mean = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_pred_output], axis=-1\n",
    "    )\n",
    "    train_metric = metric(train_pred_mean, train_y_gp)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            GP_model.gp.eval()\n",
    "            GP_model.likelihood.eval()\n",
    "            preds = GP_model.forward(test_x_gp)\n",
    "            test_loss = -mll(preds, [test_y_gp[..., i] for i in range(len(GP_model.gp.models))])\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_pred_output = GP_model.likelihood(*preds)\n",
    "            test_pred_mean = torch.cat(\n",
    "                [pred.mean.unsqueeze(-1) for pred in test_pred_output], axis=-1\n",
    "            )\n",
    "            test_metric = metric(test_pred_mean, test_y_gp)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theauv/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = GP_model.likelihood(*GP_model(train_x_gp))\n",
    "    test_preds = GP_model.likelihood(*GP_model(test_x_gp))\n",
    "    train_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_preds], axis=-1\n",
    "    )\n",
    "    test_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in test_preds], axis=-1\n",
    "    )\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y_gp, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y_gp, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y_gp, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y_gp, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (200,)\n",
      "Lower train: -1.7761917114257812+-0.013998170383274555\n",
      "Upper train: 1.7761917114257812+-0.013998174108564854\n",
      "Lower test: -1.7772729396820068+-0.013850781135261059\n",
      "Lower test: 1.7772729396820068+-0.013850778341293335\n"
     ]
    }
   ],
   "source": [
    "#For a given output look at the confidence interval for each sample of the training and test sets\n",
    "\n",
    "gp_id = 0\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](train_x_gp))\n",
    "    test_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](test_x_gp))\n",
    "\n",
    "    train_lower, train_upper = train_pred.confidence_region()\n",
    "    mean_lower_train = np.mean(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    std_lower_train = np.std(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    mean_upper_train = np.mean(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    std_upper_train = np.std(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    test_lower, test_upper = test_pred.confidence_region()\n",
    "    mean_lower_test = np.mean(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    std_lower_test = np.std(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    mean_upper_test = np.mean(test_upper.numpy()-test_pred.mean.numpy())\n",
    "    std_upper_test = np.std(test_upper.numpy()-test_pred.mean.numpy())\n",
    "\n",
    "    print(train_pred.mean.numpy().shape, test_pred.mean.numpy().shape)\n",
    "    print(f\"Lower train: {mean_lower_train}+-{std_lower_train}\")\n",
    "    print(f\"Upper train: {mean_upper_train}+-{std_upper_train}\")\n",
    "    print(f\"Lower test: {mean_lower_test}+-{std_lower_test}\")\n",
    "    print(f\"Lower test: {mean_upper_test}+-{std_upper_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark: next_obs=obs (only dynamics, can't fake predict reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 43]) torch.Size([8000, 43])\n",
      "Train MSE: 1.071226716041565, Train R2: 0.545248806476593\n",
      "Test MSE: 1.049034833908081, Test R2: 0.5838119387626648\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "\n",
    "train_fake_preds = train_x[:, :-1]\n",
    "train_target = train_y[:, :-1]\n",
    "test_fake_preds = test_x[:, :-1]\n",
    "test_target = test_y[:, :-1]\n",
    "print(train_fake_preds.shape, train_target.shape)\n",
    "print(f\"Train MSE: {criterion(train_fake_preds, train_target)}, Train R2: {metric(train_fake_preds, train_target)}\")\n",
    "print(f\"Test MSE: {criterion(test_fake_preds, test_target)}, Test R2: {metric(test_fake_preds, test_target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = train_fake_preds\n",
    "    test_preds = test_fake_preds\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y[:, :-1], axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y[:, :-1], axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y[:, :-1], axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y[:, :-1], axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model: Reward and dynamics separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 43]) torch.Size([2000, 44]) torch.Size([2000, 43])\n"
     ]
    }
   ],
   "source": [
    "dynamics_train_x = train_x\n",
    "dynamics_train_y = train_y[:, :-1]\n",
    "dynamics_test_x = test_x\n",
    "dynamics_test_y = test_y[:, :-1]\n",
    "\n",
    "print(dynamics_train_x.shape, dynamics_train_y.shape, dynamics_test_x.shape, dynamics_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01 \n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size=dynamics_train_x.shape[-1]\n",
    "out_size=dynamics_train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "dynamics_lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(dynamics_lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 7.579585075378418, R2 -1.2256653308868408\n",
      "Eval loss 7.2239251136779785, R2 -1.0983457565307617\n",
      "epoch 1, loss 7.25657844543457, R2 -1.1373687982559204\n",
      "Eval loss 6.935920238494873, R2 -1.0201694965362549\n",
      "epoch 2, loss 6.962957859039307, R2 -1.0571080446243286\n",
      "Eval loss 6.674193859100342, R2 -0.9491091966629028\n",
      "epoch 3, loss 6.695937633514404, R2 -0.9841214418411255\n",
      "Eval loss 6.436247825622559, R2 -0.8844888210296631\n",
      "epoch 4, loss 6.452997207641602, R2 -0.917719841003418\n",
      "Eval loss 6.21981954574585, R2 -0.8256970047950745\n",
      "epoch 5, loss 6.231855392456055, R2 -0.8572792410850525\n",
      "Eval loss 6.022862434387207, R2 -0.7721801400184631\n",
      "epoch 6, loss 6.0304484367370605, R2 -0.8022348284721375\n",
      "Eval loss 5.843524932861328, R2 -0.7234376072883606\n",
      "epoch 7, loss 5.846907138824463, R2 -0.7520757913589478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss 5.680130958557129, R2 -0.67901611328125\n",
      "epoch 8, loss 5.679541110992432, R2 -0.7063394784927368\n",
      "Eval loss 5.531164646148682, R2 -0.6385055184364319\n",
      "epoch 9, loss 5.526818752288818, R2 -0.6646074056625366\n",
      "Eval loss 5.395254611968994, R2 -0.6015348434448242\n",
      "epoch 10, loss 5.387355327606201, R2 -0.6265006065368652\n",
      "Eval loss 5.2711591720581055, R2 -0.5677680373191833\n",
      "epoch 11, loss 5.259896755218506, R2 -0.5916762351989746\n",
      "Eval loss 5.1577558517456055, R2 -0.5369014143943787\n",
      "epoch 12, loss 5.143309593200684, R2 -0.5598240494728088\n",
      "Eval loss 5.054027080535889, R2 -0.5086598992347717\n",
      "epoch 13, loss 5.03656530380249, R2 -0.5306631326675415\n",
      "Eval loss 4.9590559005737305, R2 -0.4827946126461029\n",
      "epoch 14, loss 4.938735485076904, R2 -0.5039392709732056\n",
      "Eval loss 4.872008800506592, R2 -0.4590804874897003\n",
      "epoch 15, loss 4.8489789962768555, R2 -0.4794224500656128\n",
      "Eval loss 4.792133331298828, R2 -0.4373137354850769\n",
      "epoch 16, loss 4.766534328460693, R2 -0.45690447092056274\n",
      "Eval loss 4.71875, R2 -0.4173099994659424\n",
      "epoch 17, loss 4.6907124519348145, R2 -0.43619680404663086\n",
      "Eval loss 4.651241779327393, R2 -0.39890244603157043\n",
      "epoch 18, loss 4.620889663696289, R2 -0.4171290397644043\n",
      "Eval loss 4.589051246643066, R2 -0.38194018602371216\n",
      "epoch 19, loss 4.556501388549805, R2 -0.39954665303230286\n",
      "Eval loss 4.531675338745117, R2 -0.36628642678260803\n",
      "epoch 20, loss 4.497036933898926, R2 -0.3833100199699402\n",
      "Eval loss 4.478656768798828, R2 -0.351817786693573\n",
      "epoch 21, loss 4.442033767700195, R2 -0.36829277873039246\n",
      "Eval loss 4.429583549499512, R2 -0.33842235803604126\n",
      "epoch 22, loss 4.391074180603027, R2 -0.3543802797794342\n",
      "Eval loss 4.384083271026611, R2 -0.3259989321231842\n",
      "epoch 23, loss 4.343778133392334, R2 -0.3414691090583801\n",
      "Eval loss 4.3418169021606445, R2 -0.31445592641830444\n",
      "epoch 24, loss 4.299804210662842, R2 -0.32946544885635376\n",
      "Eval loss 4.302478790283203, R2 -0.30371055006980896\n",
      "epoch 25, loss 4.258841037750244, R2 -0.31828442215919495\n",
      "Eval loss 4.2657952308654785, R2 -0.2936878204345703\n",
      "epoch 26, loss 4.220608234405518, R2 -0.3078491985797882\n",
      "Eval loss 4.231513023376465, R2 -0.2843199372291565\n",
      "epoch 27, loss 4.184851169586182, R2 -0.2980903685092926\n",
      "Eval loss 4.199408531188965, R2 -0.27554550766944885\n",
      "epoch 28, loss 4.151340961456299, R2 -0.28894495964050293\n",
      "Eval loss 4.169275760650635, R2 -0.2673090696334839\n",
      "epoch 29, loss 4.11986780166626, R2 -0.2803560197353363\n",
      "Eval loss 4.14093017578125, R2 -0.2595602869987488\n",
      "epoch 30, loss 4.090244293212891, R2 -0.272271990776062\n",
      "Eval loss 4.114205837249756, R2 -0.2522537410259247\n",
      "epoch 31, loss 4.0622992515563965, R2 -0.2646462321281433\n",
      "Eval loss 4.088949680328369, R2 -0.24534842371940613\n",
      "epoch 32, loss 4.035878658294678, R2 -0.25743648409843445\n",
      "Eval loss 4.0650248527526855, R2 -0.23880696296691895\n",
      "epoch 33, loss 4.010842323303223, R2 -0.2506045401096344\n",
      "Eval loss 4.042308807373047, R2 -0.2325957715511322\n",
      "epoch 34, loss 3.9870636463165283, R2 -0.24411581456661224\n",
      "Eval loss 4.020689487457275, R2 -0.22668446600437164\n",
      "epoch 35, loss 3.964428663253784, R2 -0.23793897032737732\n",
      "Eval loss 4.000064849853516, R2 -0.2210453897714615\n",
      "epoch 36, loss 3.942833185195923, R2 -0.23204563558101654\n",
      "Eval loss 3.980343818664551, R2 -0.21565364301204681\n",
      "epoch 37, loss 3.9221837520599365, R2 -0.2264101505279541\n",
      "Eval loss 3.9614434242248535, R2 -0.21048665046691895\n",
      "epoch 38, loss 3.902395009994507, R2 -0.22100929915905\n",
      "Eval loss 3.943288803100586, R2 -0.20552411675453186\n",
      "epoch 39, loss 3.883390188217163, R2 -0.2158220410346985\n",
      "Eval loss 3.9258124828338623, R2 -0.20074747502803802\n",
      "epoch 40, loss 3.8650996685028076, R2 -0.21082931756973267\n",
      "Eval loss 3.9089531898498535, R2 -0.19614000618457794\n",
      "epoch 41, loss 3.8474600315093994, R2 -0.20601385831832886\n",
      "Eval loss 3.892655611038208, R2 -0.1916866898536682\n",
      "epoch 42, loss 3.830414295196533, R2 -0.20136015117168427\n",
      "Eval loss 3.8768692016601562, R2 -0.18737384676933289\n",
      "epoch 43, loss 3.8139114379882812, R2 -0.19685396552085876\n",
      "Eval loss 3.8615493774414062, R2 -0.1831890195608139\n",
      "epoch 44, loss 3.7979040145874023, R2 -0.1924825757741928\n",
      "Eval loss 3.8466548919677734, R2 -0.17912116646766663\n",
      "epoch 45, loss 3.7823498249053955, R2 -0.1882343888282776\n",
      "Eval loss 3.832148551940918, R2 -0.17516006529331207\n",
      "epoch 46, loss 3.7672107219696045, R2 -0.18409886956214905\n",
      "Eval loss 3.817997932434082, R2 -0.17129653692245483\n",
      "epoch 47, loss 3.7524514198303223, R2 -0.1800665557384491\n",
      "Eval loss 3.804171562194824, R2 -0.1675223708152771\n",
      "epoch 48, loss 3.7380406856536865, R2 -0.1761288344860077\n",
      "Eval loss 3.790642499923706, R2 -0.16383005678653717\n",
      "epoch 49, loss 3.723950147628784, R2 -0.17227789759635925\n",
      "Eval loss 3.777385950088501, R2 -0.16021279990673065\n",
      "epoch 50, loss 3.7101540565490723, R2 -0.1685066819190979\n",
      "Eval loss 3.7643799781799316, R2 -0.1566644310951233\n",
      "epoch 51, loss 3.6966285705566406, R2 -0.16480885446071625\n",
      "Eval loss 3.7516043186187744, R2 -0.15317942202091217\n",
      "epoch 52, loss 3.6833529472351074, R2 -0.1611785888671875\n",
      "Eval loss 3.7390406131744385, R2 -0.14975282549858093\n",
      "epoch 53, loss 3.6703073978424072, R2 -0.15761061012744904\n",
      "Eval loss 3.7266721725463867, R2 -0.1463799923658371\n",
      "epoch 54, loss 3.6574759483337402, R2 -0.1541002094745636\n",
      "Eval loss 3.7144837379455566, R2 -0.14305685460567474\n",
      "epoch 55, loss 3.644840955734253, R2 -0.15064305067062378\n",
      "Eval loss 3.702462911605835, R2 -0.1397797167301178\n",
      "epoch 56, loss 3.6323890686035156, R2 -0.1472352296113968\n",
      "Eval loss 3.690595865249634, R2 -0.13654513657093048\n",
      "epoch 57, loss 3.620107650756836, R2 -0.14387327432632446\n",
      "Eval loss 3.6788735389709473, R2 -0.13335011899471283\n",
      "epoch 58, loss 3.6079843044281006, R2 -0.14055386185646057\n",
      "Eval loss 3.6672842502593994, R2 -0.13019192218780518\n",
      "epoch 59, loss 3.596008777618408, R2 -0.13727420568466187\n",
      "Eval loss 3.6558196544647217, R2 -0.12706802785396576\n",
      "epoch 60, loss 3.5841715335845947, R2 -0.13403160870075226\n",
      "Eval loss 3.644472360610962, R2 -0.12397616356611252\n",
      "epoch 61, loss 3.572463035583496, R2 -0.13082362711429596\n",
      "Eval loss 3.6332333087921143, R2 -0.12091429531574249\n",
      "epoch 62, loss 3.5608763694763184, R2 -0.12764817476272583\n",
      "Eval loss 3.6220974922180176, R2 -0.11788053065538406\n",
      "epoch 63, loss 3.549403429031372, R2 -0.12450329959392548\n",
      "Eval loss 3.611058235168457, R2 -0.11487326771020889\n",
      "epoch 64, loss 3.5380377769470215, R2 -0.12138710170984268\n",
      "Eval loss 3.600109815597534, R2 -0.11189094930887222\n",
      "epoch 65, loss 3.5267744064331055, R2 -0.11829806119203568\n",
      "Eval loss 3.589246988296509, R2 -0.10893215984106064\n",
      "epoch 66, loss 3.5156068801879883, R2 -0.11523467302322388\n",
      "Eval loss 3.5784661769866943, R2 -0.10599570721387863\n",
      "epoch 67, loss 3.504530429840088, R2 -0.11219558864831924\n",
      "Eval loss 3.567763328552246, R2 -0.10308042913675308\n",
      "epoch 68, loss 3.4935414791107178, R2 -0.10917963087558746\n",
      "Eval loss 3.5571343898773193, R2 -0.1001853197813034\n",
      "epoch 69, loss 3.4826347827911377, R2 -0.10618563741445541\n",
      "Eval loss 3.5465755462646484, R2 -0.0973094254732132\n",
      "epoch 70, loss 3.4718070030212402, R2 -0.10321266949176788\n",
      "Eval loss 3.5360844135284424, R2 -0.09445192664861679\n",
      "epoch 71, loss 3.461055278778076, R2 -0.10025973618030548\n",
      "Eval loss 3.525658369064331, R2 -0.09161202609539032\n",
      "epoch 72, loss 3.450376033782959, R2 -0.09732603281736374\n",
      "Eval loss 3.5152950286865234, R2 -0.08878907561302185\n",
      "epoch 73, loss 3.4397666454315186, R2 -0.09441082924604416\n",
      "Eval loss 3.504991292953491, R2 -0.08598239719867706\n",
      "epoch 74, loss 3.429224967956543, R2 -0.09151344001293182\n",
      "Eval loss 3.494745969772339, R2 -0.08319142460823059\n",
      "epoch 75, loss 3.418747901916504, R2 -0.08863315731287003\n",
      "Eval loss 3.4845573902130127, R2 -0.08041566610336304\n",
      "epoch 76, loss 3.4083340167999268, R2 -0.08576950430870056\n",
      "Eval loss 3.4744229316711426, R2 -0.07765459269285202\n",
      "epoch 77, loss 3.3979804515838623, R2 -0.08292188495397568\n",
      "Eval loss 3.464341640472412, R2 -0.0749078243970871\n",
      "epoch 78, loss 3.3876867294311523, R2 -0.08008985966444016\n",
      "Eval loss 3.454312562942505, R2 -0.07217497378587723\n",
      "epoch 79, loss 3.3774502277374268, R2 -0.07727300375699997\n",
      "Eval loss 3.444333553314209, R2 -0.06945563852787018\n",
      "epoch 80, loss 3.367269515991211, R2 -0.07447083294391632\n",
      "Eval loss 3.434404134750366, R2 -0.06674947589635849\n",
      "epoch 81, loss 3.3571438789367676, R2 -0.07168304920196533\n",
      "Eval loss 3.424522876739502, R2 -0.06405624002218246\n",
      "epoch 82, loss 3.347071886062622, R2 -0.06890930235385895\n",
      "Eval loss 3.414689064025879, R2 -0.06137562170624733\n",
      "epoch 83, loss 3.3370518684387207, R2 -0.0661492869257927\n",
      "Eval loss 3.4049012660980225, R2 -0.058707404881715775\n",
      "epoch 84, loss 3.327082872390747, R2 -0.0634026899933815\n",
      "Eval loss 3.3951592445373535, R2 -0.05605130270123482\n",
      "epoch 85, loss 3.317164182662964, R2 -0.06066925451159477\n",
      "Eval loss 3.3854622840881348, R2 -0.05340714752674103\n",
      "epoch 86, loss 3.307295322418213, R2 -0.057948749512434006\n",
      "Eval loss 3.375809907913208, R2 -0.05077475309371948\n",
      "epoch 87, loss 3.2974746227264404, R2 -0.05524098128080368\n",
      "Eval loss 3.3662009239196777, R2 -0.04815387725830078\n",
      "epoch 88, loss 3.2877016067504883, R2 -0.05254567414522171\n",
      "Eval loss 3.356635093688965, R2 -0.045544419437646866\n",
      "epoch 89, loss 3.27797532081604, R2 -0.049862682819366455\n",
      "Eval loss 3.347111225128174, R2 -0.042946189641952515\n",
      "epoch 90, loss 3.268296241760254, R2 -0.04719183221459389\n",
      "Eval loss 3.337630033493042, R2 -0.04035905748605728\n",
      "epoch 91, loss 3.258661985397339, R2 -0.044532909989356995\n",
      "Eval loss 3.328190326690674, R2 -0.03778289631009102\n",
      "epoch 92, loss 3.2490735054016113, R2 -0.04188580438494682\n",
      "Eval loss 3.3187918663024902, R2 -0.03521757200360298\n",
      "epoch 93, loss 3.2395291328430176, R2 -0.03925034776329994\n",
      "Eval loss 3.309434175491333, R2 -0.032662976533174515\n",
      "epoch 94, loss 3.2300286293029785, R2 -0.0366264171898365\n",
      "Eval loss 3.3001163005828857, R2 -0.030119020491838455\n",
      "epoch 95, loss 3.2205727100372314, R2 -0.034013908356428146\n",
      "Eval loss 3.2908387184143066, R2 -0.0275855902582407\n",
      "epoch 96, loss 3.2111592292785645, R2 -0.03141266852617264\n",
      "Eval loss 3.2816009521484375, R2 -0.025062602013349533\n",
      "epoch 97, loss 3.2017884254455566, R2 -0.02882262133061886\n",
      "Eval loss 3.2724030017852783, R2 -0.022549906745553017\n",
      "epoch 98, loss 3.192460060119629, R2 -0.02624361775815487\n",
      "Eval loss 3.2632434368133545, R2 -0.020047491416335106\n",
      "epoch 99, loss 3.183173656463623, R2 -0.02367556095123291\n",
      "Eval loss 3.2541234493255615, R2 -0.017555266618728638\n",
      "epoch 100, loss 3.173928737640381, R2 -0.02111838385462761\n",
      "Eval loss 3.2450413703918457, R2 -0.015073106624186039\n",
      "epoch 101, loss 3.1647253036499023, R2 -0.01857198216021061\n",
      "Eval loss 3.2359979152679443, R2 -0.012601037509739399\n",
      "epoch 102, loss 3.1555628776550293, R2 -0.016036290675401688\n",
      "Eval loss 3.2269928455352783, R2 -0.010138923302292824\n",
      "epoch 103, loss 3.1464412212371826, R2 -0.01351122371852398\n",
      "Eval loss 3.2180254459381104, R2 -0.007686687167733908\n",
      "epoch 104, loss 3.1373589038848877, R2 -0.010996639728546143\n",
      "Eval loss 3.2090952396392822, R2 -0.005244296509772539\n",
      "epoch 105, loss 3.1283175945281982, R2 -0.00849253498017788\n",
      "Eval loss 3.200202703475952, R2 -0.00281168962828815\n",
      "epoch 106, loss 3.1193153858184814, R2 -0.0059988247230648994\n",
      "Eval loss 3.191347360610962, R2 -0.000388824671972543\n",
      "epoch 107, loss 3.1103532314300537, R2 -0.0035154472570866346\n",
      "Eval loss 3.1825296878814697, R2 0.0020244205370545387\n",
      "epoch 108, loss 3.1014299392700195, R2 -0.001042281510308385\n",
      "Eval loss 3.17374849319458, R2 0.004428031854331493\n",
      "epoch 109, loss 3.092545747756958, R2 0.0014206281630322337\n",
      "Eval loss 3.165003538131714, R2 0.006822055205702782\n",
      "epoch 110, loss 3.083699941635132, R2 0.0038734786212444305\n",
      "Eval loss 3.1562957763671875, R2 0.009206618182361126\n",
      "epoch 111, loss 3.0748932361602783, R2 0.006316227838397026\n",
      "Eval loss 3.1476235389709473, R2 0.01158169936388731\n",
      "epoch 112, loss 3.066124200820923, R2 0.008749001659452915\n",
      "Eval loss 3.1389882564544678, R2 0.013947349973022938\n",
      "epoch 113, loss 3.0573935508728027, R2 0.011171801015734673\n",
      "Eval loss 3.130388021469116, R2 0.016303643584251404\n",
      "epoch 114, loss 3.0487005710601807, R2 0.013584724627435207\n",
      "Eval loss 3.121824026107788, R2 0.018650619313120842\n",
      "epoch 115, loss 3.0400452613830566, R2 0.015987783670425415\n",
      "Eval loss 3.113295793533325, R2 0.020988326519727707\n",
      "epoch 116, loss 3.0314273834228516, R2 0.018381057307124138\n",
      "Eval loss 3.1048030853271484, R2 0.023316804319620132\n",
      "epoch 117, loss 3.0228466987609863, R2 0.020764613524079323\n",
      "Eval loss 3.0963451862335205, R2 0.02563607506453991\n",
      "epoch 118, loss 3.014302968978882, R2 0.0231384988874197\n",
      "Eval loss 3.087923049926758, R2 0.027946215122938156\n",
      "epoch 119, loss 3.005796194076538, R2 0.02550273761153221\n",
      "Eval loss 3.0795352458953857, R2 0.030247261747717857\n",
      "epoch 120, loss 2.997325897216797, R2 0.027857378125190735\n",
      "Eval loss 3.0711824893951416, R2 0.0325392447412014\n",
      "epoch 121, loss 2.988892078399658, R2 0.030202500522136688\n",
      "Eval loss 3.0628645420074463, R2 0.03482219949364662\n",
      "epoch 122, loss 2.980494260787964, R2 0.03253812715411186\n",
      "Eval loss 3.0545806884765625, R2 0.03709616884589195\n",
      "epoch 123, loss 2.972132921218872, R2 0.03486430272459984\n",
      "Eval loss 3.0463316440582275, R2 0.03936120495200157\n",
      "epoch 124, loss 2.9638073444366455, R2 0.037181101739406586\n",
      "Eval loss 3.038116931915283, R2 0.04161733761429787\n",
      "epoch 125, loss 2.955517292022705, R2 0.03948851674795151\n",
      "Eval loss 3.029935598373413, R2 0.043864618986845016\n",
      "epoch 126, loss 2.94726300239563, R2 0.04178665578365326\n",
      "Eval loss 3.0217888355255127, R2 0.04610306769609451\n",
      "epoch 127, loss 2.9390435218811035, R2 0.04407552257180214\n",
      "Eval loss 3.0136759281158447, R2 0.048332735896110535\n",
      "epoch 128, loss 2.9308595657348633, R2 0.04635516554117203\n",
      "Eval loss 3.0055959224700928, R2 0.05055365338921547\n",
      "epoch 129, loss 2.922710657119751, R2 0.048625648021698\n",
      "Eval loss 2.9975497722625732, R2 0.052765872329473495\n",
      "epoch 130, loss 2.9145967960357666, R2 0.05088699609041214\n",
      "Eval loss 2.989537000656128, R2 0.054969411343336105\n",
      "epoch 131, loss 2.906517267227173, R2 0.053139254450798035\n",
      "Eval loss 2.9815573692321777, R2 0.05716431885957718\n",
      "epoch 132, loss 2.898472309112549, R2 0.05538245290517807\n",
      "Eval loss 2.9736108779907227, R2 0.05935063585639\n",
      "epoch 133, loss 2.8904619216918945, R2 0.057616643607616425\n",
      "Eval loss 2.9656970500946045, R2 0.061528418213129044\n",
      "epoch 134, loss 2.8824851512908936, R2 0.059841908514499664\n",
      "Eval loss 2.9578163623809814, R2 0.06369765847921371\n",
      "epoch 135, loss 2.874542713165283, R2 0.062058232724666595\n",
      "Eval loss 2.949967861175537, R2 0.0658584013581276\n",
      "epoch 136, loss 2.8666341304779053, R2 0.0642656609416008\n",
      "Eval loss 2.9421517848968506, R2 0.06801068782806396\n",
      "epoch 137, loss 2.8587591648101807, R2 0.06646426767110825\n",
      "Eval loss 2.934368371963501, R2 0.07015460729598999\n",
      "epoch 138, loss 2.8509175777435303, R2 0.06865406036376953\n",
      "Eval loss 2.926616907119751, R2 0.07229010760784149\n",
      "epoch 139, loss 2.843109369277954, R2 0.07083509862422943\n",
      "Eval loss 2.918897867202759, R2 0.07441727817058563\n",
      "epoch 140, loss 2.8353347778320312, R2 0.07300739735364914\n",
      "Eval loss 2.911210298538208, R2 0.076536163687706\n",
      "epoch 141, loss 2.8275930881500244, R2 0.07517103850841522\n",
      "Eval loss 2.903554916381836, R2 0.07864676415920258\n",
      "epoch 142, loss 2.8198840618133545, R2 0.07732603698968887\n",
      "Eval loss 2.895930528640747, R2 0.08074913918972015\n",
      "epoch 143, loss 2.8122079372406006, R2 0.07947242259979248\n",
      "Eval loss 2.888338327407837, R2 0.08284330368041992\n",
      "epoch 144, loss 2.8045642375946045, R2 0.08161023259162903\n",
      "Eval loss 2.880777359008789, R2 0.08492930233478546\n",
      "epoch 145, loss 2.7969532012939453, R2 0.08373953402042389\n",
      "Eval loss 2.8732473850250244, R2 0.08700718730688095\n",
      "epoch 146, loss 2.789374351501465, R2 0.08586032688617706\n",
      "Eval loss 2.865748643875122, R2 0.08907696604728699\n",
      "epoch 147, loss 2.781827688217163, R2 0.08797265589237213\n",
      "Eval loss 2.8582804203033447, R2 0.09113867580890656\n",
      "epoch 148, loss 2.77431321144104, R2 0.09007660299539566\n",
      "Eval loss 2.850843667984009, R2 0.09319235384464264\n",
      "epoch 149, loss 2.7668302059173584, R2 0.09217215329408646\n",
      "Eval loss 2.843437433242798, R2 0.0952380895614624\n",
      "epoch 150, loss 2.7593791484832764, R2 0.09425939619541168\n",
      "Eval loss 2.836061477661133, R2 0.09727584570646286\n",
      "epoch 151, loss 2.7519595623016357, R2 0.09633829444646835\n",
      "Eval loss 2.828716278076172, R2 0.0993056520819664\n",
      "epoch 152, loss 2.7445714473724365, R2 0.09840895235538483\n",
      "Eval loss 2.8214008808135986, R2 0.10132758319377899\n",
      "epoch 153, loss 2.7372148036956787, R2 0.1004713624715805\n",
      "Eval loss 2.8141160011291504, R2 0.10334164649248123\n",
      "epoch 154, loss 2.729888916015625, R2 0.10252561420202255\n",
      "Eval loss 2.806861162185669, R2 0.10534790903329849\n",
      "epoch 155, loss 2.7225940227508545, R2 0.10457168519496918\n",
      "Eval loss 2.799635887145996, R2 0.10734639316797256\n",
      "epoch 156, loss 2.7153303623199463, R2 0.10660963505506516\n",
      "Eval loss 2.79244065284729, R2 0.10933706164360046\n",
      "epoch 157, loss 2.708096742630005, R2 0.10863948613405228\n",
      "Eval loss 2.7852749824523926, R2 0.11132007092237473\n",
      "epoch 158, loss 2.700894355773926, R2 0.11066130548715591\n",
      "Eval loss 2.7781388759613037, R2 0.11329533159732819\n",
      "epoch 159, loss 2.6937220096588135, R2 0.11267509311437607\n",
      "Eval loss 2.7710323333740234, R2 0.1152629405260086\n",
      "epoch 160, loss 2.686579942703247, R2 0.1146809309720993\n",
      "Eval loss 2.7639546394348145, R2 0.11722303181886673\n",
      "epoch 161, loss 2.6794681549072266, R2 0.11667880415916443\n",
      "Eval loss 2.756906270980835, R2 0.11917543411254883\n",
      "epoch 162, loss 2.6723861694335938, R2 0.1186688095331192\n",
      "Eval loss 2.7498865127563477, R2 0.12112029641866684\n",
      "epoch 163, loss 2.6653342247009277, R2 0.12065088003873825\n",
      "Eval loss 2.74289608001709, R2 0.12305761128664017\n",
      "epoch 164, loss 2.6583120822906494, R2 0.12262514233589172\n",
      "Eval loss 2.735934257507324, R2 0.12498743832111359\n",
      "epoch 165, loss 2.6513195037841797, R2 0.12459160387516022\n",
      "Eval loss 2.729001045227051, R2 0.12690982222557068\n",
      "epoch 166, loss 2.6443560123443604, R2 0.12655030190944672\n",
      "Eval loss 2.7220959663391113, R2 0.12882477045059204\n",
      "epoch 167, loss 2.6374220848083496, R2 0.12850123643875122\n",
      "Eval loss 2.715219497680664, R2 0.13073231279850006\n",
      "epoch 168, loss 2.6305174827575684, R2 0.1304444670677185\n",
      "Eval loss 2.70837140083313, R2 0.13263244926929474\n",
      "epoch 169, loss 2.6236419677734375, R2 0.13238006830215454\n",
      "Eval loss 2.7015511989593506, R2 0.13452531397342682\n",
      "epoch 170, loss 2.616795063018799, R2 0.13430801033973694\n",
      "Eval loss 2.694758892059326, R2 0.13641083240509033\n",
      "epoch 171, loss 2.6099770069122314, R2 0.13622833788394928\n",
      "Eval loss 2.6879944801330566, R2 0.13828909397125244\n",
      "epoch 172, loss 2.6031875610351562, R2 0.13814112544059753\n",
      "Eval loss 2.681257963180542, R2 0.14016012847423553\n",
      "epoch 173, loss 2.5964267253875732, R2 0.14004632830619812\n",
      "Eval loss 2.674548625946045, R2 0.14202389121055603\n",
      "epoch 174, loss 2.5896944999694824, R2 0.1419440656900406\n",
      "Eval loss 2.667867422103882, R2 0.14388051629066467\n",
      "epoch 175, loss 2.5829906463623047, R2 0.14383433759212494\n",
      "Eval loss 2.6612131595611572, R2 0.14572998881340027\n",
      "epoch 176, loss 2.5763142108917236, R2 0.14571714401245117\n",
      "Eval loss 2.65458607673645, R2 0.147572323679924\n",
      "epoch 177, loss 2.5696663856506348, R2 0.14759257435798645\n",
      "Eval loss 2.647986650466919, R2 0.14940756559371948\n",
      "epoch 178, loss 2.5630462169647217, R2 0.14946064352989197\n",
      "Eval loss 2.641413688659668, R2 0.15123577415943146\n",
      "epoch 179, loss 2.5564539432525635, R2 0.15132135152816772\n",
      "Eval loss 2.6348676681518555, R2 0.15305691957473755\n",
      "epoch 180, loss 2.54988956451416, R2 0.1531747281551361\n",
      "Eval loss 2.6283485889434814, R2 0.15487109124660492\n",
      "epoch 181, loss 2.5433521270751953, R2 0.15502086281776428\n",
      "Eval loss 2.621856212615967, R2 0.15667830407619476\n",
      "epoch 182, loss 2.5368423461914062, R2 0.15685975551605225\n",
      "Eval loss 2.615389823913574, R2 0.1584785133600235\n",
      "epoch 183, loss 2.5303597450256348, R2 0.15869140625\n",
      "Eval loss 2.60895037651062, R2 0.16027188301086426\n",
      "epoch 184, loss 2.523904323577881, R2 0.1605158895254135\n",
      "Eval loss 2.602537155151367, R2 0.16205833852291107\n",
      "epoch 185, loss 2.5174760818481445, R2 0.16233325004577637\n",
      "Eval loss 2.5961501598358154, R2 0.16383793950080872\n",
      "epoch 186, loss 2.5110745429992676, R2 0.1641434282064438\n",
      "Eval loss 2.5897886753082275, R2 0.16561073064804077\n",
      "epoch 187, loss 2.50469970703125, R2 0.1659465730190277\n",
      "Eval loss 2.583453416824341, R2 0.16737674176692963\n",
      "epoch 188, loss 2.4983513355255127, R2 0.16774266958236694\n",
      "Eval loss 2.5771443843841553, R2 0.1691359430551529\n",
      "epoch 189, loss 2.492029905319214, R2 0.1695316880941391\n",
      "Eval loss 2.5708606243133545, R2 0.17088843882083893\n",
      "epoch 190, loss 2.485734701156616, R2 0.17131373286247253\n",
      "Eval loss 2.564602851867676, R2 0.17263418436050415\n",
      "epoch 191, loss 2.479465961456299, R2 0.17308881878852844\n",
      "Eval loss 2.558370351791382, R2 0.1743733137845993\n",
      "epoch 192, loss 2.4732232093811035, R2 0.17485696077346802\n",
      "Eval loss 2.5521631240844727, R2 0.17610575258731842\n",
      "epoch 193, loss 2.4670064449310303, R2 0.17661818861961365\n",
      "Eval loss 2.5459811687469482, R2 0.17783156037330627\n",
      "epoch 194, loss 2.460815668106079, R2 0.1783725619316101\n",
      "Eval loss 2.5398247241973877, R2 0.17955079674720764\n",
      "epoch 195, loss 2.454650640487671, R2 0.1801200658082962\n",
      "Eval loss 2.5336928367614746, R2 0.18126347661018372\n",
      "epoch 196, loss 2.4485113620758057, R2 0.1818607598543167\n",
      "Eval loss 2.5275862216949463, R2 0.1829695999622345\n",
      "epoch 197, loss 2.4423978328704834, R2 0.18359465897083282\n",
      "Eval loss 2.5215041637420654, R2 0.18466922640800476\n",
      "epoch 198, loss 2.436309576034546, R2 0.18532182276248932\n",
      "Eval loss 2.515446901321411, R2 0.1863623559474945\n",
      "epoch 199, loss 2.4302468299865723, R2 0.187042236328125\n",
      "Eval loss 2.5094144344329834, R2 0.1880490481853485\n",
      "epoch 200, loss 2.424208879470825, R2 0.18875597417354584\n",
      "Eval loss 2.503406286239624, R2 0.18972928822040558\n",
      "epoch 201, loss 2.418196678161621, R2 0.19046302139759064\n",
      "Eval loss 2.497422695159912, R2 0.1914031207561493\n",
      "epoch 202, loss 2.4122090339660645, R2 0.19216342270374298\n",
      "Eval loss 2.4914631843566895, R2 0.19307059049606323\n",
      "epoch 203, loss 2.4062464237213135, R2 0.19385723769664764\n",
      "Eval loss 2.485527515411377, R2 0.1947317272424698\n",
      "epoch 204, loss 2.400308609008789, R2 0.19554446637630463\n",
      "Eval loss 2.479616403579712, R2 0.19638653099536896\n",
      "epoch 205, loss 2.394395351409912, R2 0.19722507894039154\n",
      "Eval loss 2.473729372024536, R2 0.19803504645824432\n",
      "epoch 206, loss 2.3885068893432617, R2 0.1988992542028427\n",
      "Eval loss 2.4678659439086914, R2 0.19967728853225708\n",
      "epoch 207, loss 2.3826425075531006, R2 0.20056688785552979\n",
      "Eval loss 2.4620261192321777, R2 0.2013133317232132\n",
      "epoch 208, loss 2.376802682876587, R2 0.20222800970077515\n",
      "Eval loss 2.456210136413574, R2 0.2029430866241455\n",
      "epoch 209, loss 2.3709871768951416, R2 0.20388275384902954\n",
      "Eval loss 2.4504175186157227, R2 0.20456668734550476\n",
      "epoch 210, loss 2.3651957511901855, R2 0.2055310606956482\n",
      "Eval loss 2.444648504257202, R2 0.20618414878845215\n",
      "epoch 211, loss 2.3594284057617188, R2 0.20717297494411469\n",
      "Eval loss 2.4389026165008545, R2 0.2077954262495041\n",
      "epoch 212, loss 2.353684663772583, R2 0.2088085263967514\n",
      "Eval loss 2.433180332183838, R2 0.20940063893795013\n",
      "epoch 213, loss 2.3479650020599365, R2 0.21043772995471954\n",
      "Eval loss 2.427480936050415, R2 0.21099980175495148\n",
      "epoch 214, loss 2.342268943786621, R2 0.21206064522266388\n",
      "Eval loss 2.421804428100586, R2 0.21259285509586334\n",
      "epoch 215, loss 2.336596727371216, R2 0.2136772722005844\n",
      "Eval loss 2.4161510467529297, R2 0.2141799032688141\n",
      "epoch 216, loss 2.3309476375579834, R2 0.21528765559196472\n",
      "Eval loss 2.410520315170288, R2 0.21576091647148132\n",
      "epoch 217, loss 2.325321912765503, R2 0.2168918401002884\n",
      "Eval loss 2.4049124717712402, R2 0.21733593940734863\n",
      "epoch 218, loss 2.3197195529937744, R2 0.21848978102207184\n",
      "Eval loss 2.399327039718628, R2 0.21890507638454437\n",
      "epoch 219, loss 2.3141403198242188, R2 0.22008159756660461\n",
      "Eval loss 2.3937642574310303, R2 0.220468208193779\n",
      "epoch 220, loss 2.308584213256836, R2 0.22166723012924194\n",
      "Eval loss 2.388223886489868, R2 0.22202545404434204\n",
      "epoch 221, loss 2.303050994873047, R2 0.2232467234134674\n",
      "Eval loss 2.3827059268951416, R2 0.22357682883739471\n",
      "epoch 222, loss 2.2975406646728516, R2 0.22482015192508698\n",
      "Eval loss 2.3772101402282715, R2 0.2251223474740982\n",
      "epoch 223, loss 2.292052984237671, R2 0.22638753056526184\n",
      "Eval loss 2.3717362880706787, R2 0.22666200995445251\n",
      "epoch 224, loss 2.286588191986084, R2 0.2279488742351532\n",
      "Eval loss 2.3662846088409424, R2 0.228195920586586\n",
      "epoch 225, loss 2.2811458110809326, R2 0.22950418293476105\n",
      "Eval loss 2.3608551025390625, R2 0.22972401976585388\n",
      "epoch 226, loss 2.2757256031036377, R2 0.23105348646640778\n",
      "Eval loss 2.3554470539093018, R2 0.23124632239341736\n",
      "epoch 227, loss 2.2703282833099365, R2 0.23259684443473816\n",
      "Eval loss 2.3500607013702393, R2 0.23276293277740479\n",
      "epoch 228, loss 2.2649528980255127, R2 0.2341342717409134\n",
      "Eval loss 2.344696521759033, R2 0.23427386581897736\n",
      "epoch 229, loss 2.259599447250366, R2 0.23566573858261108\n",
      "Eval loss 2.339353561401367, R2 0.2357790470123291\n",
      "epoch 230, loss 2.2542686462402344, R2 0.23719137907028198\n",
      "Eval loss 2.334031820297241, R2 0.23727858066558838\n",
      "epoch 231, loss 2.2489593029022217, R2 0.2387111634016037\n",
      "Eval loss 2.3287315368652344, R2 0.23877252638339996\n",
      "epoch 232, loss 2.2436718940734863, R2 0.24022507667541504\n",
      "Eval loss 2.3234527111053467, R2 0.24026080965995789\n",
      "epoch 233, loss 2.238406181335449, R2 0.24173322319984436\n",
      "Eval loss 2.318195104598999, R2 0.24174350500106812\n",
      "epoch 234, loss 2.2331621646881104, R2 0.2432355284690857\n",
      "Eval loss 2.3129584789276123, R2 0.24322068691253662\n",
      "epoch 235, loss 2.2279398441314697, R2 0.2447320818901062\n",
      "Eval loss 2.3077428340911865, R2 0.24469226598739624\n",
      "epoch 236, loss 2.2227389812469482, R2 0.24622291326522827\n",
      "Eval loss 2.3025479316711426, R2 0.24615836143493652\n",
      "epoch 237, loss 2.217559337615967, R2 0.2477080374956131\n",
      "Eval loss 2.2973740100860596, R2 0.24761894345283508\n",
      "epoch 238, loss 2.2124011516571045, R2 0.24918745458126068\n",
      "Eval loss 2.2922208309173584, R2 0.2490740269422531\n",
      "epoch 239, loss 2.207263708114624, R2 0.2506612241268158\n",
      "Eval loss 2.287088394165039, R2 0.2505236864089966\n",
      "epoch 240, loss 2.202147960662842, R2 0.25212931632995605\n",
      "Eval loss 2.2819762229919434, R2 0.2519679367542267\n",
      "epoch 241, loss 2.1970529556274414, R2 0.2535918354988098\n",
      "Eval loss 2.2768845558166504, R2 0.25340673327445984\n",
      "epoch 242, loss 2.1919784545898438, R2 0.2550487220287323\n",
      "Eval loss 2.271813154220581, R2 0.2548401951789856\n",
      "epoch 243, loss 2.186925172805786, R2 0.2565000653266907\n",
      "Eval loss 2.2667622566223145, R2 0.25626829266548157\n",
      "epoch 244, loss 2.1818923950195312, R2 0.25794586539268494\n",
      "Eval loss 2.2617313861846924, R2 0.25769099593162537\n",
      "epoch 245, loss 2.176880359649658, R2 0.25938618183135986\n",
      "Eval loss 2.256720542907715, R2 0.25910842418670654\n",
      "epoch 246, loss 2.171888828277588, R2 0.2608209252357483\n",
      "Eval loss 2.251729965209961, R2 0.2605205774307251\n",
      "epoch 247, loss 2.1669178009033203, R2 0.26225024461746216\n",
      "Eval loss 2.2467591762542725, R2 0.26192739605903625\n",
      "epoch 248, loss 2.1619670391082764, R2 0.2636740803718567\n",
      "Eval loss 2.2418081760406494, R2 0.26332902908325195\n",
      "epoch 249, loss 2.157036781311035, R2 0.26509249210357666\n",
      "Eval loss 2.2368767261505127, R2 0.2647254168987274\n",
      "epoch 250, loss 2.1521260738372803, R2 0.26650553941726685\n",
      "Eval loss 2.2319653034210205, R2 0.26611658930778503\n",
      "epoch 251, loss 2.147235870361328, R2 0.26791316270828247\n",
      "Eval loss 2.2270731925964355, R2 0.2675025761127472\n",
      "epoch 252, loss 2.1423654556274414, R2 0.2693154215812683\n",
      "Eval loss 2.222200632095337, R2 0.26888343691825867\n",
      "epoch 253, loss 2.1375153064727783, R2 0.27071237564086914\n",
      "Eval loss 2.2173473834991455, R2 0.2702591121196747\n",
      "epoch 254, loss 2.1326847076416016, R2 0.2721039950847626\n",
      "Eval loss 2.2125134468078613, R2 0.2716297209262848\n",
      "epoch 255, loss 2.127873659133911, R2 0.273490309715271\n",
      "Eval loss 2.2076988220214844, R2 0.2729952037334442\n",
      "epoch 256, loss 2.123082399368286, R2 0.2748713791370392\n",
      "Eval loss 2.2029037475585938, R2 0.27435559034347534\n",
      "epoch 257, loss 2.1183109283447266, R2 0.27624720335006714\n",
      "Eval loss 2.198127031326294, R2 0.27571094036102295\n",
      "epoch 258, loss 2.113558530807495, R2 0.27761781215667725\n",
      "Eval loss 2.1933696269989014, R2 0.27706125378608704\n",
      "epoch 259, loss 2.10882568359375, R2 0.2789832055568695\n",
      "Eval loss 2.188631057739258, R2 0.2784065306186676\n",
      "epoch 260, loss 2.104112148284912, R2 0.2803434133529663\n",
      "Eval loss 2.183911085128784, R2 0.27974680066108704\n",
      "epoch 261, loss 2.0994174480438232, R2 0.2816984951496124\n",
      "Eval loss 2.1792099475860596, R2 0.2810821831226349\n",
      "epoch 262, loss 2.0947422981262207, R2 0.2830484211444855\n",
      "Eval loss 2.174527883529663, R2 0.2824125587940216\n",
      "epoch 263, loss 2.0900862216949463, R2 0.28439319133758545\n",
      "Eval loss 2.1698641777038574, R2 0.28373798727989197\n",
      "epoch 264, loss 2.085448741912842, R2 0.2857329249382019\n",
      "Eval loss 2.1652188301086426, R2 0.28505855798721313\n",
      "epoch 265, loss 2.0808303356170654, R2 0.2870675325393677\n",
      "Eval loss 2.1605918407440186, R2 0.28637418150901794\n",
      "epoch 266, loss 2.076230764389038, R2 0.2883971333503723\n",
      "Eval loss 2.1559832096099854, R2 0.28768494725227356\n",
      "epoch 267, loss 2.0716497898101807, R2 0.28972169756889343\n",
      "Eval loss 2.151393175125122, R2 0.28899088501930237\n",
      "epoch 268, loss 2.067087173461914, R2 0.2910412549972534\n",
      "Eval loss 2.1468207836151123, R2 0.29029199481010437\n",
      "epoch 269, loss 2.0625436305999756, R2 0.29235580563545227\n",
      "Eval loss 2.1422669887542725, R2 0.29158830642700195\n",
      "epoch 270, loss 2.0580179691314697, R2 0.29366540908813477\n",
      "Eval loss 2.137730836868286, R2 0.29287976026535034\n",
      "epoch 271, loss 2.053510904312134, R2 0.2949700355529785\n",
      "Eval loss 2.1332125663757324, R2 0.29416653513908386\n",
      "epoch 272, loss 2.0490224361419678, R2 0.29626980423927307\n",
      "Eval loss 2.1287126541137695, R2 0.29544851183891296\n",
      "epoch 273, loss 2.0445516109466553, R2 0.29756462574005127\n",
      "Eval loss 2.124229907989502, R2 0.2967257499694824\n",
      "epoch 274, loss 2.040099620819092, R2 0.2988545596599579\n",
      "Eval loss 2.119765281677246, R2 0.297998309135437\n",
      "epoch 275, loss 2.0356650352478027, R2 0.3001396358013153\n",
      "Eval loss 2.1153182983398438, R2 0.29926615953445435\n",
      "epoch 276, loss 2.0312485694885254, R2 0.3014198839664459\n",
      "Eval loss 2.1108884811401367, R2 0.3005293607711792\n",
      "epoch 277, loss 2.0268502235412598, R2 0.30269530415534973\n",
      "Eval loss 2.106476306915283, R2 0.3017878532409668\n",
      "epoch 278, loss 2.0224690437316895, R2 0.30396589636802673\n",
      "Eval loss 2.102081775665283, R2 0.3030417859554291\n",
      "epoch 279, loss 2.01810622215271, R2 0.3052317500114441\n",
      "Eval loss 2.0977041721343994, R2 0.3042910397052765\n",
      "epoch 280, loss 2.013760805130005, R2 0.30649280548095703\n",
      "Eval loss 2.093344211578369, R2 0.30553576350212097\n",
      "epoch 281, loss 2.0094330310821533, R2 0.3077491521835327\n",
      "Eval loss 2.089001178741455, R2 0.30677586793899536\n",
      "epoch 282, loss 2.0051229000091553, R2 0.30900076031684875\n",
      "Eval loss 2.0846753120422363, R2 0.3080114424228668\n",
      "epoch 283, loss 2.0008299350738525, R2 0.31024765968322754\n",
      "Eval loss 2.080366611480713, R2 0.30924245715141296\n",
      "epoch 284, loss 1.9965543746948242, R2 0.31148990988731384\n",
      "Eval loss 2.0760746002197266, R2 0.31046897172927856\n",
      "epoch 285, loss 1.9922960996627808, R2 0.3127274811267853\n",
      "Eval loss 2.0717997550964355, R2 0.3116909861564636\n",
      "epoch 286, loss 1.9880551099777222, R2 0.31396040320396423\n",
      "Eval loss 2.0675415992736816, R2 0.312908411026001\n",
      "epoch 287, loss 1.9838310480117798, R2 0.3151887059211731\n",
      "Eval loss 2.0632998943328857, R2 0.3141215145587921\n",
      "epoch 288, loss 1.9796239137649536, R2 0.31641238927841187\n",
      "Eval loss 2.059075117111206, R2 0.3153301179409027\n",
      "epoch 289, loss 1.9754341840744019, R2 0.3176315426826477\n",
      "Eval loss 2.0548665523529053, R2 0.3165343105792999\n",
      "epoch 290, loss 1.9712610244750977, R2 0.31884607672691345\n",
      "Eval loss 2.0506751537323, R2 0.3177340626716614\n",
      "epoch 291, loss 1.9671050310134888, R2 0.32005611062049866\n",
      "Eval loss 2.046499729156494, R2 0.3189294636249542\n",
      "epoch 292, loss 1.962965488433838, R2 0.32126158475875854\n",
      "Eval loss 2.0423409938812256, R2 0.3201204538345337\n",
      "epoch 293, loss 1.9588426351547241, R2 0.3224625587463379\n",
      "Eval loss 2.038198471069336, R2 0.32130709290504456\n",
      "epoch 294, loss 1.9547362327575684, R2 0.3236590325832367\n",
      "Eval loss 2.034071922302246, R2 0.3224894404411316\n",
      "epoch 295, loss 1.9506464004516602, R2 0.32485100626945496\n",
      "Eval loss 2.0299620628356934, R2 0.32366740703582764\n",
      "epoch 296, loss 1.94657301902771, R2 0.3260386288166046\n",
      "Eval loss 2.0258677005767822, R2 0.32484111189842224\n",
      "epoch 297, loss 1.9425162076950073, R2 0.32722169160842896\n",
      "Eval loss 2.02178955078125, R2 0.32601049542427063\n",
      "epoch 298, loss 1.9384756088256836, R2 0.3284004330635071\n",
      "Eval loss 2.0177276134490967, R2 0.32717567682266235\n",
      "epoch 299, loss 1.9344513416290283, R2 0.3295747637748718\n",
      "Eval loss 2.013681411743164, R2 0.32833656668663025\n",
      "epoch 300, loss 1.9304429292678833, R2 0.3307446837425232\n",
      "Eval loss 2.0096511840820312, R2 0.3294932246208191\n",
      "epoch 301, loss 1.9264506101608276, R2 0.33191028237342834\n",
      "Eval loss 2.00563645362854, R2 0.33064568042755127\n",
      "epoch 302, loss 1.9224746227264404, R2 0.3330715000629425\n",
      "Eval loss 2.0016374588012695, R2 0.3317939341068268\n",
      "epoch 303, loss 1.918514370918274, R2 0.33422842621803284\n",
      "Eval loss 1.9976540803909302, R2 0.33293798565864563\n",
      "epoch 304, loss 1.9145700931549072, R2 0.33538103103637695\n",
      "Eval loss 1.9936864376068115, R2 0.334077924489975\n",
      "epoch 305, loss 1.9106416702270508, R2 0.33652934432029724\n",
      "Eval loss 1.9897340536117554, R2 0.33521369099617004\n",
      "epoch 306, loss 1.906728982925415, R2 0.3376733958721161\n",
      "Eval loss 1.9857970476150513, R2 0.3363453149795532\n",
      "epoch 307, loss 1.9028319120407104, R2 0.3388131856918335\n",
      "Eval loss 1.9818757772445679, R2 0.3374728858470917\n",
      "epoch 308, loss 1.8989505767822266, R2 0.33994874358177185\n",
      "Eval loss 1.9779695272445679, R2 0.33859631419181824\n",
      "epoch 309, loss 1.8950846195220947, R2 0.34108009934425354\n",
      "Eval loss 1.9740785360336304, R2 0.3397156298160553\n",
      "epoch 310, loss 1.891234040260315, R2 0.34220725297927856\n",
      "Eval loss 1.9702024459838867, R2 0.34083092212677\n",
      "epoch 311, loss 1.8873988389968872, R2 0.3433302044868469\n",
      "Eval loss 1.9663417339324951, R2 0.34194216132164\n",
      "epoch 312, loss 1.8835792541503906, R2 0.3444490134716034\n",
      "Eval loss 1.962496042251587, R2 0.34304943680763245\n",
      "epoch 313, loss 1.8797749280929565, R2 0.345563679933548\n",
      "Eval loss 1.958665370941162, R2 0.344152569770813\n",
      "epoch 314, loss 1.8759855031967163, R2 0.3466741740703583\n",
      "Eval loss 1.954849362373352, R2 0.34525179862976074\n",
      "epoch 315, loss 1.8722114562988281, R2 0.34778058528900146\n",
      "Eval loss 1.9510483741760254, R2 0.34634706377983093\n",
      "epoch 316, loss 1.8684524297714233, R2 0.34888288378715515\n",
      "Eval loss 1.9472620487213135, R2 0.3474383056163788\n",
      "epoch 317, loss 1.8647081851959229, R2 0.3499811589717865\n",
      "Eval loss 1.9434905052185059, R2 0.34852564334869385\n",
      "epoch 318, loss 1.8609793186187744, R2 0.35107532143592834\n",
      "Eval loss 1.9397335052490234, R2 0.34960904717445374\n",
      "epoch 319, loss 1.8572648763656616, R2 0.35216543078422546\n",
      "Eval loss 1.9359912872314453, R2 0.35068854689598083\n",
      "epoch 320, loss 1.8535654544830322, R2 0.353251576423645\n",
      "Eval loss 1.9322634935379028, R2 0.35176408290863037\n",
      "epoch 321, loss 1.8498808145523071, R2 0.3543336093425751\n",
      "Eval loss 1.9285497665405273, R2 0.3528357744216919\n",
      "epoch 322, loss 1.8462107181549072, R2 0.35541173815727234\n",
      "Eval loss 1.9248510599136353, R2 0.3539036512374878\n",
      "epoch 323, loss 1.8425556421279907, R2 0.35648584365844727\n",
      "Eval loss 1.9211664199829102, R2 0.3549675941467285\n",
      "epoch 324, loss 1.8389146327972412, R2 0.35755598545074463\n",
      "Eval loss 1.9174959659576416, R2 0.3560277223587036\n",
      "epoch 325, loss 1.835288166999817, R2 0.3586221933364868\n",
      "Eval loss 1.9138396978378296, R2 0.3570840656757355\n",
      "epoch 326, loss 1.8316762447357178, R2 0.35968443751335144\n",
      "Eval loss 1.9101976156234741, R2 0.3581365644931793\n",
      "epoch 327, loss 1.8280786275863647, R2 0.36074280738830566\n",
      "Eval loss 1.9065697193145752, R2 0.35918524861335754\n",
      "epoch 328, loss 1.8244954347610474, R2 0.3617972731590271\n",
      "Eval loss 1.9029560089111328, R2 0.3602302372455597\n",
      "epoch 329, loss 1.8209264278411865, R2 0.3628478944301605\n",
      "Eval loss 1.8993561267852783, R2 0.3612714409828186\n",
      "epoch 330, loss 1.8173716068267822, R2 0.36389458179473877\n",
      "Eval loss 1.8957703113555908, R2 0.3623088598251343\n",
      "epoch 331, loss 1.813830852508545, R2 0.3649374544620514\n",
      "Eval loss 1.892198085784912, R2 0.3633425831794739\n",
      "epoch 332, loss 1.8103041648864746, R2 0.365976482629776\n",
      "Eval loss 1.8886399269104004, R2 0.3643725514411926\n",
      "epoch 333, loss 1.8067914247512817, R2 0.3670116662979126\n",
      "Eval loss 1.885095238685608, R2 0.3653988838195801\n",
      "epoch 334, loss 1.8032927513122559, R2 0.36804309487342834\n",
      "Eval loss 1.8815642595291138, R2 0.36642155051231384\n",
      "epoch 335, loss 1.7998077869415283, R2 0.36907070875167847\n",
      "Eval loss 1.8780468702316284, R2 0.36744049191474915\n",
      "epoch 336, loss 1.7963366508483887, R2 0.3700945973396301\n",
      "Eval loss 1.8745431900024414, R2 0.36845576763153076\n",
      "epoch 337, loss 1.792879343032837, R2 0.37111470103263855\n",
      "Eval loss 1.8710530996322632, R2 0.36946743726730347\n",
      "epoch 338, loss 1.789435625076294, R2 0.3721310794353485\n",
      "Eval loss 1.8675761222839355, R2 0.3704754710197449\n",
      "epoch 339, loss 1.7860058546066284, R2 0.37314367294311523\n",
      "Eval loss 1.8641126155853271, R2 0.37147992849349976\n",
      "epoch 340, loss 1.7825894355773926, R2 0.37415260076522827\n",
      "Eval loss 1.860662817955017, R2 0.37248069047927856\n",
      "epoch 341, loss 1.7791863679885864, R2 0.37515783309936523\n",
      "Eval loss 1.857226014137268, R2 0.3734779357910156\n",
      "epoch 342, loss 1.7757970094680786, R2 0.3761593997478485\n",
      "Eval loss 1.8538025617599487, R2 0.37447166442871094\n",
      "epoch 343, loss 1.7724207639694214, R2 0.3771573007106781\n",
      "Eval loss 1.8503921031951904, R2 0.37546178698539734\n",
      "epoch 344, loss 1.769058108329773, R2 0.3781515061855316\n",
      "Eval loss 1.8469948768615723, R2 0.376448392868042\n",
      "epoch 345, loss 1.7657088041305542, R2 0.3791421055793762\n",
      "Eval loss 1.843610405921936, R2 0.3774314820766449\n",
      "epoch 346, loss 1.7623724937438965, R2 0.3801291286945343\n",
      "Eval loss 1.8402390480041504, R2 0.37841105461120605\n",
      "epoch 347, loss 1.759049415588379, R2 0.3811125159263611\n",
      "Eval loss 1.8368808031082153, R2 0.37938711047172546\n",
      "epoch 348, loss 1.755739450454712, R2 0.38209232687950134\n",
      "Eval loss 1.8335354328155518, R2 0.3803596794605255\n",
      "epoch 349, loss 1.752442717552185, R2 0.3830684721469879\n",
      "Eval loss 1.8302031755447388, R2 0.38132885098457336\n",
      "epoch 350, loss 1.7491587400436401, R2 0.3840411901473999\n",
      "Eval loss 1.8268831968307495, R2 0.3822944462299347\n",
      "epoch 351, loss 1.745888113975525, R2 0.385010302066803\n",
      "Eval loss 1.8235763311386108, R2 0.3832567036151886\n",
      "epoch 352, loss 1.7426297664642334, R2 0.3859758675098419\n",
      "Eval loss 1.820281982421875, R2 0.38421550393104553\n",
      "epoch 353, loss 1.739384651184082, R2 0.3869379162788391\n",
      "Eval loss 1.8170000314712524, R2 0.38517090678215027\n",
      "epoch 354, loss 1.7361522912979126, R2 0.38789647817611694\n",
      "Eval loss 1.8137307167053223, R2 0.3861229121685028\n",
      "epoch 355, loss 1.732932448387146, R2 0.3888515532016754\n",
      "Eval loss 1.810474157333374, R2 0.38707152009010315\n",
      "epoch 356, loss 1.7297255992889404, R2 0.3898031413555145\n",
      "Eval loss 1.8072298765182495, R2 0.3880167305469513\n",
      "epoch 357, loss 1.7265310287475586, R2 0.39075127243995667\n",
      "Eval loss 1.8039982318878174, R2 0.3889586329460144\n",
      "epoch 358, loss 1.7233492136001587, R2 0.3916960060596466\n",
      "Eval loss 1.8007787466049194, R2 0.3898971378803253\n",
      "epoch 359, loss 1.7201799154281616, R2 0.3926372528076172\n",
      "Eval loss 1.7975714206695557, R2 0.3908323645591736\n",
      "epoch 360, loss 1.7170228958129883, R2 0.39357510209083557\n",
      "Eval loss 1.7943764925003052, R2 0.39176422357559204\n",
      "epoch 361, loss 1.7138786315917969, R2 0.39450955390930176\n",
      "Eval loss 1.7911938428878784, R2 0.39269283413887024\n",
      "epoch 362, loss 1.71074640750885, R2 0.39544060826301575\n",
      "Eval loss 1.7880232334136963, R2 0.3936181664466858\n",
      "epoch 363, loss 1.707626461982727, R2 0.3963682949542999\n",
      "Eval loss 1.7848647832870483, R2 0.39454013109207153\n",
      "epoch 364, loss 1.7045186758041382, R2 0.3972926139831543\n",
      "Eval loss 1.781718373298645, R2 0.3954589366912842\n",
      "epoch 365, loss 1.7014232873916626, R2 0.39821359515190125\n",
      "Eval loss 1.7785838842391968, R2 0.396374374628067\n",
      "epoch 366, loss 1.6983400583267212, R2 0.3991312086582184\n",
      "Eval loss 1.7754614353179932, R2 0.39728671312332153\n",
      "epoch 367, loss 1.6952685117721558, R2 0.4000455141067505\n",
      "Eval loss 1.7723506689071655, R2 0.39819571375846863\n",
      "epoch 368, loss 1.6922094821929932, R2 0.40095654129981995\n",
      "Eval loss 1.7692521810531616, R2 0.3991015553474426\n",
      "epoch 369, loss 1.689162254333496, R2 0.4018642008304596\n",
      "Eval loss 1.766165018081665, R2 0.40000417828559875\n",
      "epoch 370, loss 1.686126947402954, R2 0.40276864171028137\n",
      "Eval loss 1.7630901336669922, R2 0.4009036719799042\n",
      "epoch 371, loss 1.6831035614013672, R2 0.4036698043346405\n",
      "Eval loss 1.7600263357162476, R2 0.40179985761642456\n",
      "epoch 372, loss 1.6800919771194458, R2 0.404567688703537\n",
      "Eval loss 1.7569745779037476, R2 0.4026930034160614\n",
      "epoch 373, loss 1.6770920753479004, R2 0.4054623544216156\n",
      "Eval loss 1.7539342641830444, R2 0.40358299016952515\n",
      "epoch 374, loss 1.6741039752960205, R2 0.40635380148887634\n",
      "Eval loss 1.7509055137634277, R2 0.4044697880744934\n",
      "epoch 375, loss 1.6711275577545166, R2 0.4072420001029968\n",
      "Eval loss 1.747888445854187, R2 0.40535345673561096\n",
      "epoch 376, loss 1.6681628227233887, R2 0.4081270396709442\n",
      "Eval loss 1.744882583618164, R2 0.4062340259552002\n",
      "epoch 377, loss 1.6652095317840576, R2 0.40900886058807373\n",
      "Eval loss 1.7418882846832275, R2 0.4071115553379059\n",
      "epoch 378, loss 1.662267804145813, R2 0.40988749265670776\n",
      "Eval loss 1.7389051914215088, R2 0.4079858958721161\n",
      "epoch 379, loss 1.6593375205993652, R2 0.4107629656791687\n",
      "Eval loss 1.735933542251587, R2 0.4088572561740875\n",
      "epoch 380, loss 1.6564185619354248, R2 0.4116353392601013\n",
      "Eval loss 1.7329730987548828, R2 0.409725546836853\n",
      "epoch 381, loss 1.6535112857818604, R2 0.41250449419021606\n",
      "Eval loss 1.730023980140686, R2 0.4105907380580902\n",
      "epoch 382, loss 1.6506152153015137, R2 0.4133705198764801\n",
      "Eval loss 1.7270859479904175, R2 0.41145288944244385\n",
      "epoch 383, loss 1.6477303504943848, R2 0.414233535528183\n",
      "Eval loss 1.7241591215133667, R2 0.4123120903968811\n",
      "epoch 384, loss 1.6448566913604736, R2 0.41509345173835754\n",
      "Eval loss 1.721243143081665, R2 0.41316816210746765\n",
      "epoch 385, loss 1.6419942378997803, R2 0.41595014929771423\n",
      "Eval loss 1.7183382511138916, R2 0.41402125358581543\n",
      "epoch 386, loss 1.6391431093215942, R2 0.41680383682250977\n",
      "Eval loss 1.715444564819336, R2 0.414871484041214\n",
      "epoch 387, loss 1.6363028287887573, R2 0.41765448451042175\n",
      "Eval loss 1.7125617265701294, R2 0.41571861505508423\n",
      "epoch 388, loss 1.6334736347198486, R2 0.4185020923614502\n",
      "Eval loss 1.709689736366272, R2 0.4165628254413605\n",
      "epoch 389, loss 1.6306555271148682, R2 0.41934657096862793\n",
      "Eval loss 1.7068283557891846, R2 0.41740405559539795\n",
      "epoch 390, loss 1.6278481483459473, R2 0.4201880991458893\n",
      "Eval loss 1.703978180885315, R2 0.41824230551719666\n",
      "epoch 391, loss 1.6250519752502441, R2 0.4210266172885895\n",
      "Eval loss 1.7011384963989258, R2 0.41907772421836853\n",
      "epoch 392, loss 1.6222665309906006, R2 0.4218621253967285\n",
      "Eval loss 1.6983095407485962, R2 0.41991016268730164\n",
      "epoch 393, loss 1.6194918155670166, R2 0.4226946234703064\n",
      "Eval loss 1.6954914331436157, R2 0.42073968052864075\n",
      "epoch 394, loss 1.6167278289794922, R2 0.4235242009162903\n",
      "Eval loss 1.6926838159561157, R2 0.421566367149353\n",
      "epoch 395, loss 1.6139745712280273, R2 0.424350768327713\n",
      "Eval loss 1.6898868083953857, R2 0.42239007353782654\n",
      "epoch 396, loss 1.6112321615219116, R2 0.425174355506897\n",
      "Eval loss 1.6871002912521362, R2 0.4232109487056732\n",
      "epoch 397, loss 1.608500361442566, R2 0.4259950518608093\n",
      "Eval loss 1.6843242645263672, R2 0.4240289628505707\n",
      "epoch 398, loss 1.6057790517807007, R2 0.4268128275871277\n",
      "Eval loss 1.6815588474273682, R2 0.4248441755771637\n",
      "epoch 399, loss 1.6030683517456055, R2 0.4276275932788849\n",
      "Eval loss 1.6788034439086914, R2 0.42565643787384033\n",
      "epoch 400, loss 1.6003681421279907, R2 0.42843952775001526\n",
      "Eval loss 1.6760588884353638, R2 0.4264659583568573\n",
      "epoch 401, loss 1.5976784229278564, R2 0.42924854159355164\n",
      "Eval loss 1.6733243465423584, R2 0.4272726774215698\n",
      "epoch 402, loss 1.5949989557266235, R2 0.4300546944141388\n",
      "Eval loss 1.6706000566482544, R2 0.42807653546333313\n",
      "epoch 403, loss 1.592329978942871, R2 0.43085789680480957\n",
      "Eval loss 1.6678862571716309, R2 0.4288776218891144\n",
      "epoch 404, loss 1.5896711349487305, R2 0.4316583573818207\n",
      "Eval loss 1.6651824712753296, R2 0.4296759366989136\n",
      "epoch 405, loss 1.5870229005813599, R2 0.4324558973312378\n",
      "Eval loss 1.6624886989593506, R2 0.4304714500904083\n",
      "epoch 406, loss 1.5843844413757324, R2 0.43325063586235046\n",
      "Eval loss 1.659805178642273, R2 0.43126416206359863\n",
      "epoch 407, loss 1.581756591796875, R2 0.4340425431728363\n",
      "Eval loss 1.6571316719055176, R2 0.43205422163009644\n",
      "epoch 408, loss 1.5791383981704712, R2 0.4348316490650177\n",
      "Eval loss 1.6544684171676636, R2 0.4328414499759674\n",
      "epoch 409, loss 1.5765306949615479, R2 0.43561792373657227\n",
      "Eval loss 1.6518148183822632, R2 0.4336259663105011\n",
      "epoch 410, loss 1.5739330053329468, R2 0.4364013671875\n",
      "Eval loss 1.6491713523864746, R2 0.4344078004360199\n",
      "epoch 411, loss 1.5713452100753784, R2 0.43718209862709045\n",
      "Eval loss 1.6465376615524292, R2 0.43518686294555664\n",
      "epoch 412, loss 1.5687674283981323, R2 0.43796002864837646\n",
      "Eval loss 1.6439138650894165, R2 0.43596330285072327\n",
      "epoch 413, loss 1.566199541091919, R2 0.4387352168560028\n",
      "Eval loss 1.6412999629974365, R2 0.43673697113990784\n",
      "epoch 414, loss 1.5636416673660278, R2 0.4395076334476471\n",
      "Eval loss 1.6386958360671997, R2 0.43750807642936707\n",
      "epoch 415, loss 1.5610933303833008, R2 0.4402773082256317\n",
      "Eval loss 1.6361013650894165, R2 0.43827641010284424\n",
      "epoch 416, loss 1.558555245399475, R2 0.44104424118995667\n",
      "Eval loss 1.6335166692733765, R2 0.4390421211719513\n",
      "epoch 417, loss 1.556026577949524, R2 0.4418085217475891\n",
      "Eval loss 1.6309415102005005, R2 0.43980517983436584\n",
      "epoch 418, loss 1.5535080432891846, R2 0.4425700902938843\n",
      "Eval loss 1.6283761262893677, R2 0.4405655860900879\n",
      "epoch 419, loss 1.5509986877441406, R2 0.4433288872241974\n",
      "Eval loss 1.6258201599121094, R2 0.44132348895072937\n",
      "epoch 420, loss 1.5484992265701294, R2 0.4440850615501404\n",
      "Eval loss 1.6232736110687256, R2 0.4420786201953888\n",
      "epoch 421, loss 1.5460094213485718, R2 0.44483858346939087\n",
      "Eval loss 1.6207369565963745, R2 0.44283121824264526\n",
      "epoch 422, loss 1.5435290336608887, R2 0.44558942317962646\n",
      "Eval loss 1.6182092428207397, R2 0.4435811936855316\n",
      "epoch 423, loss 1.5410581827163696, R2 0.44633758068084717\n",
      "Eval loss 1.6156915426254272, R2 0.4443286061286926\n",
      "epoch 424, loss 1.5385968685150146, R2 0.447083055973053\n",
      "Eval loss 1.613182783126831, R2 0.4450734555721283\n",
      "epoch 425, loss 1.5361448526382446, R2 0.4478260278701782\n",
      "Eval loss 1.610683560371399, R2 0.445815771818161\n",
      "epoch 426, loss 1.5337023735046387, R2 0.4485663175582886\n",
      "Eval loss 1.608193278312683, R2 0.4465554356575012\n",
      "epoch 427, loss 1.5312693119049072, R2 0.4493040144443512\n",
      "Eval loss 1.6057125329971313, R2 0.44729262590408325\n",
      "epoch 428, loss 1.528845191001892, R2 0.4500390887260437\n",
      "Eval loss 1.6032413244247437, R2 0.44802728295326233\n",
      "epoch 429, loss 1.526430606842041, R2 0.4507715106010437\n",
      "Eval loss 1.6007790565490723, R2 0.44875937700271606\n",
      "epoch 430, loss 1.5240254402160645, R2 0.4515014886856079\n",
      "Eval loss 1.5983257293701172, R2 0.44948896765708923\n",
      "epoch 431, loss 1.5216293334960938, R2 0.4522288143634796\n",
      "Eval loss 1.5958819389343262, R2 0.4502160847187042\n",
      "epoch 432, loss 1.5192424058914185, R2 0.45295360684394836\n",
      "Eval loss 1.5934466123580933, R2 0.45094063878059387\n",
      "epoch 433, loss 1.5168644189834595, R2 0.4536758363246918\n",
      "Eval loss 1.5910207033157349, R2 0.4516627788543701\n",
      "epoch 434, loss 1.514495849609375, R2 0.4543955624103546\n",
      "Eval loss 1.5886039733886719, R2 0.4523823857307434\n",
      "epoch 435, loss 1.5121361017227173, R2 0.4551127254962921\n",
      "Eval loss 1.5861958265304565, R2 0.4530995786190033\n",
      "epoch 436, loss 1.5097852945327759, R2 0.45582741498947144\n",
      "Eval loss 1.5837968587875366, R2 0.4538142681121826\n",
      "epoch 437, loss 1.5074435472488403, R2 0.4565395414829254\n",
      "Eval loss 1.581406593322754, R2 0.45452654361724854\n",
      "epoch 438, loss 1.5051108598709106, R2 0.45724916458129883\n",
      "Eval loss 1.5790250301361084, R2 0.45523640513420105\n",
      "epoch 439, loss 1.5027868747711182, R2 0.45795634388923645\n",
      "Eval loss 1.5766526460647583, R2 0.4559437930583954\n",
      "epoch 440, loss 1.500472068786621, R2 0.4586610198020935\n",
      "Eval loss 1.5742888450622559, R2 0.4566487669944763\n",
      "epoch 441, loss 1.4981657266616821, R2 0.4593632221221924\n",
      "Eval loss 1.5719338655471802, R2 0.45735129714012146\n",
      "epoch 442, loss 1.495868444442749, R2 0.4600629508495331\n",
      "Eval loss 1.5695877075195312, R2 0.45805150270462036\n",
      "epoch 443, loss 1.4935797452926636, R2 0.46076029539108276\n",
      "Eval loss 1.5672500133514404, R2 0.45874929428100586\n",
      "epoch 444, loss 1.4912997484207153, R2 0.4614551365375519\n",
      "Eval loss 1.5649213790893555, R2 0.45944470167160034\n",
      "epoch 445, loss 1.4890285730361938, R2 0.46214765310287476\n",
      "Eval loss 1.5626006126403809, R2 0.4601376950740814\n",
      "epoch 446, loss 1.4867658615112305, R2 0.4628375768661499\n",
      "Eval loss 1.560288906097412, R2 0.46082839369773865\n",
      "epoch 447, loss 1.4845118522644043, R2 0.46352529525756836\n",
      "Eval loss 1.557985782623291, R2 0.46151667833328247\n",
      "epoch 448, loss 1.4822663068771362, R2 0.4642104208469391\n",
      "Eval loss 1.5556907653808594, R2 0.46220263838768005\n",
      "epoch 449, loss 1.4800292253494263, R2 0.46489325165748596\n",
      "Eval loss 1.553404450416565, R2 0.462886244058609\n",
      "epoch 450, loss 1.4778008460998535, R2 0.4655736982822418\n",
      "Eval loss 1.5511268377304077, R2 0.4635675549507141\n",
      "epoch 451, loss 1.4755809307098389, R2 0.4662517309188843\n",
      "Eval loss 1.5488572120666504, R2 0.464246541261673\n",
      "epoch 452, loss 1.4733693599700928, R2 0.4669274091720581\n",
      "Eval loss 1.5465961694717407, R2 0.4649232029914856\n",
      "epoch 453, loss 1.4711662530899048, R2 0.4676007926464081\n",
      "Eval loss 1.54434335231781, R2 0.46559759974479675\n",
      "epoch 454, loss 1.4689714908599854, R2 0.46827173233032227\n",
      "Eval loss 1.542098879814148, R2 0.46626970171928406\n",
      "epoch 455, loss 1.4667848348617554, R2 0.4689404368400574\n",
      "Eval loss 1.5398626327514648, R2 0.46693944931030273\n",
      "epoch 456, loss 1.4646068811416626, R2 0.46960675716400146\n",
      "Eval loss 1.5376346111297607, R2 0.4676070213317871\n",
      "epoch 457, loss 1.4624367952346802, R2 0.47027072310447693\n",
      "Eval loss 1.5354145765304565, R2 0.46827226877212524\n",
      "epoch 458, loss 1.4602751731872559, R2 0.4709324538707733\n",
      "Eval loss 1.533202886581421, R2 0.4689352810382843\n",
      "epoch 459, loss 1.4581215381622314, R2 0.47159186005592346\n",
      "Eval loss 1.5309993028640747, R2 0.4695959687232971\n",
      "epoch 460, loss 1.4559762477874756, R2 0.47224897146224976\n",
      "Eval loss 1.528803825378418, R2 0.4702545404434204\n",
      "epoch 461, loss 1.45383882522583, R2 0.4729038178920746\n",
      "Eval loss 1.5266164541244507, R2 0.470910906791687\n",
      "epoch 462, loss 1.4517097473144531, R2 0.47355639934539795\n",
      "Eval loss 1.5244371891021729, R2 0.4715648889541626\n",
      "epoch 463, loss 1.4495885372161865, R2 0.4742066562175751\n",
      "Eval loss 1.5222656726837158, R2 0.47221675515174866\n",
      "epoch 464, loss 1.4474754333496094, R2 0.4748547673225403\n",
      "Eval loss 1.5201021432876587, R2 0.47286632657051086\n",
      "epoch 465, loss 1.4453703165054321, R2 0.47550055384635925\n",
      "Eval loss 1.517946720123291, R2 0.4735139012336731\n",
      "epoch 466, loss 1.4432730674743652, R2 0.47614404559135437\n",
      "Eval loss 1.5157989263534546, R2 0.4741591513156891\n",
      "epoch 467, loss 1.4411838054656982, R2 0.4767853915691376\n",
      "Eval loss 1.5136593580245972, R2 0.4748022258281708\n",
      "epoch 468, loss 1.4391025304794312, R2 0.4774245023727417\n",
      "Eval loss 1.5115272998809814, R2 0.47544315457344055\n",
      "epoch 469, loss 1.4370288848876953, R2 0.47806140780448914\n",
      "Eval loss 1.509403109550476, R2 0.47608184814453125\n",
      "epoch 470, loss 1.4349629878997803, R2 0.47869616746902466\n",
      "Eval loss 1.5072869062423706, R2 0.4767184555530548\n",
      "epoch 471, loss 1.4329051971435547, R2 0.47932863235473633\n",
      "Eval loss 1.5051780939102173, R2 0.47735294699668884\n",
      "epoch 472, loss 1.4308547973632812, R2 0.4799589216709137\n",
      "Eval loss 1.5030772686004639, R2 0.4779852628707886\n",
      "epoch 473, loss 1.4288125038146973, R2 0.48058709502220154\n",
      "Eval loss 1.5009838342666626, R2 0.4786154329776764\n",
      "epoch 474, loss 1.4267776012420654, R2 0.4812130630016327\n",
      "Eval loss 1.4988983869552612, R2 0.4792434871196747\n",
      "epoch 475, loss 1.4247503280639648, R2 0.48183685541152954\n",
      "Eval loss 1.496820330619812, R2 0.47986945509910583\n",
      "epoch 476, loss 1.4227309226989746, R2 0.4824585020542145\n",
      "Eval loss 1.494749903678894, R2 0.4804932773113251\n",
      "epoch 477, loss 1.420719027519226, R2 0.48307809233665466\n",
      "Eval loss 1.4926869869232178, R2 0.48111504316329956\n",
      "epoch 478, loss 1.4187146425247192, R2 0.4836954176425934\n",
      "Eval loss 1.4906315803527832, R2 0.4817346930503845\n",
      "epoch 479, loss 1.4167178869247437, R2 0.48431068658828735\n",
      "Eval loss 1.4885838031768799, R2 0.48235228657722473\n",
      "epoch 480, loss 1.4147285223007202, R2 0.484923779964447\n",
      "Eval loss 1.4865432977676392, R2 0.4829677641391754\n",
      "epoch 481, loss 1.412746548652649, R2 0.4855348467826843\n",
      "Eval loss 1.4845103025436401, R2 0.4835812449455261\n",
      "epoch 482, loss 1.4107722043991089, R2 0.48614373803138733\n",
      "Eval loss 1.4824846982955933, R2 0.4841926097869873\n",
      "epoch 483, loss 1.408805251121521, R2 0.4867505729198456\n",
      "Eval loss 1.480466604232788, R2 0.4848019480705261\n",
      "epoch 484, loss 1.4068454504013062, R2 0.4873552918434143\n",
      "Eval loss 1.4784555435180664, R2 0.4854092299938202\n",
      "epoch 485, loss 1.404893159866333, R2 0.48795801401138306\n",
      "Eval loss 1.4764519929885864, R2 0.4860144853591919\n",
      "epoch 486, loss 1.402948260307312, R2 0.4885586202144623\n",
      "Eval loss 1.474455714225769, R2 0.48661768436431885\n",
      "epoch 487, loss 1.401010513305664, R2 0.489157110452652\n",
      "Eval loss 1.4724665880203247, R2 0.4872188866138458\n",
      "epoch 488, loss 1.3990799188613892, R2 0.4897536337375641\n",
      "Eval loss 1.470484733581543, R2 0.48781806230545044\n",
      "epoch 489, loss 1.397156834602356, R2 0.49034807085990906\n",
      "Eval loss 1.4685101509094238, R2 0.48841530084609985\n",
      "epoch 490, loss 1.3952407836914062, R2 0.4909404516220093\n",
      "Eval loss 1.4665427207946777, R2 0.4890104830265045\n",
      "epoch 491, loss 1.3933318853378296, R2 0.4915308356285095\n",
      "Eval loss 1.4645824432373047, R2 0.4896036982536316\n",
      "epoch 492, loss 1.3914300203323364, R2 0.4921192228794098\n",
      "Eval loss 1.4626291990280151, R2 0.4901949465274811\n",
      "epoch 493, loss 1.3895354270935059, R2 0.49270549416542053\n",
      "Eval loss 1.460682988166809, R2 0.4907841682434082\n",
      "epoch 494, loss 1.387648105621338, R2 0.49328985810279846\n",
      "Eval loss 1.4587438106536865, R2 0.4913715124130249\n",
      "epoch 495, loss 1.3857675790786743, R2 0.4938722252845764\n",
      "Eval loss 1.4568116664886475, R2 0.49195683002471924\n",
      "epoch 496, loss 1.3838938474655151, R2 0.49445250630378723\n",
      "Eval loss 1.4548866748809814, R2 0.49254024028778076\n",
      "epoch 497, loss 1.3820276260375977, R2 0.49503087997436523\n",
      "Eval loss 1.4529683589935303, R2 0.4931216537952423\n",
      "epoch 498, loss 1.3801679611206055, R2 0.49560728669166565\n",
      "Eval loss 1.4510571956634521, R2 0.49370113015174866\n",
      "epoch 499, loss 1.3783154487609863, R2 0.4961816966533661\n",
      "Eval loss 1.449152946472168, R2 0.4942786991596222\n",
      "epoch 500, loss 1.3764697313308716, R2 0.49675416946411133\n",
      "Eval loss 1.4472554922103882, R2 0.4948543608188629\n",
      "epoch 501, loss 1.3746309280395508, R2 0.4973246455192566\n",
      "Eval loss 1.4453648328781128, R2 0.4954281151294708\n",
      "epoch 502, loss 1.3727989196777344, R2 0.4978931248188019\n",
      "Eval loss 1.4434810876846313, R2 0.4959999918937683\n",
      "epoch 503, loss 1.370973825454712, R2 0.49845975637435913\n",
      "Eval loss 1.4416041374206543, R2 0.49656984210014343\n",
      "epoch 504, loss 1.3691555261611938, R2 0.49902451038360596\n",
      "Eval loss 1.439733862876892, R2 0.4971379041671753\n",
      "epoch 505, loss 1.367343783378601, R2 0.4995872676372528\n",
      "Eval loss 1.4378703832626343, R2 0.4977041184902191\n",
      "epoch 506, loss 1.3655388355255127, R2 0.5001481175422668\n",
      "Eval loss 1.4360138177871704, R2 0.49826839566230774\n",
      "epoch 507, loss 1.3637406826019287, R2 0.5007070302963257\n",
      "Eval loss 1.4341638088226318, R2 0.49883076548576355\n",
      "epoch 508, loss 1.3619493246078491, R2 0.5012640953063965\n",
      "Eval loss 1.4323203563690186, R2 0.4993913471698761\n",
      "epoch 509, loss 1.3601646423339844, R2 0.5018192529678345\n",
      "Eval loss 1.4304836988449097, R2 0.49994996190071106\n",
      "epoch 510, loss 1.3583863973617554, R2 0.5023724436759949\n",
      "Eval loss 1.428653359413147, R2 0.5005068182945251\n",
      "epoch 511, loss 1.356614589691162, R2 0.5029239058494568\n",
      "Eval loss 1.4268298149108887, R2 0.5010617971420288\n",
      "epoch 512, loss 1.3548496961593628, R2 0.5034733414649963\n",
      "Eval loss 1.4250127077102661, R2 0.5016149282455444\n",
      "epoch 513, loss 1.3530912399291992, R2 0.5040209293365479\n",
      "Eval loss 1.423202395439148, R2 0.5021663308143616\n",
      "epoch 514, loss 1.351339340209961, R2 0.5045667886734009\n",
      "Eval loss 1.421398401260376, R2 0.5027158260345459\n",
      "epoch 515, loss 1.3495938777923584, R2 0.5051107406616211\n",
      "Eval loss 1.4196008443832397, R2 0.503263533115387\n",
      "epoch 516, loss 1.3478549718856812, R2 0.5056528449058533\n",
      "Eval loss 1.4178097248077393, R2 0.50380939245224\n",
      "epoch 517, loss 1.34612238407135, R2 0.5061931014060974\n",
      "Eval loss 1.416025161743164, R2 0.5043534636497498\n",
      "epoch 518, loss 1.3443961143493652, R2 0.5067315101623535\n",
      "Eval loss 1.4142470359802246, R2 0.504895806312561\n",
      "epoch 519, loss 1.3426764011383057, R2 0.5072681307792664\n",
      "Eval loss 1.4124751091003418, R2 0.5054361820220947\n",
      "epoch 520, loss 1.3409631252288818, R2 0.5078029036521912\n",
      "Eval loss 1.4107096195220947, R2 0.5059749484062195\n",
      "epoch 521, loss 1.3392560482025146, R2 0.5083358883857727\n",
      "Eval loss 1.4089504480361938, R2 0.506511926651001\n",
      "epoch 522, loss 1.3375552892684937, R2 0.5088670253753662\n",
      "Eval loss 1.4071974754333496, R2 0.5070470571517944\n",
      "epoch 523, loss 1.3358608484268188, R2 0.509396493434906\n",
      "Eval loss 1.4054509401321411, R2 0.5075804591178894\n",
      "epoch 524, loss 1.3341724872589111, R2 0.5099239945411682\n",
      "Eval loss 1.4037106037139893, R2 0.5081121921539307\n",
      "epoch 525, loss 1.3324905633926392, R2 0.5104498267173767\n",
      "Eval loss 1.4019763469696045, R2 0.5086420178413391\n",
      "epoch 526, loss 1.3308148384094238, R2 0.5109739303588867\n",
      "Eval loss 1.4002485275268555, R2 0.5091702342033386\n",
      "epoch 527, loss 1.3291451930999756, R2 0.5114961862564087\n",
      "Eval loss 1.398526668548584, R2 0.5096966624259949\n",
      "epoch 528, loss 1.3274818658828735, R2 0.5120167136192322\n",
      "Eval loss 1.3968110084533691, R2 0.5102213025093079\n",
      "epoch 529, loss 1.325824499130249, R2 0.5125355124473572\n",
      "Eval loss 1.395101547241211, R2 0.5107443332672119\n",
      "epoch 530, loss 1.3241733312606812, R2 0.5130524635314941\n",
      "Eval loss 1.3933982849121094, R2 0.5112655758857727\n",
      "epoch 531, loss 1.3225281238555908, R2 0.5135677456855774\n",
      "Eval loss 1.3917008638381958, R2 0.511785089969635\n",
      "epoch 532, loss 1.3208891153335571, R2 0.5140813589096069\n",
      "Eval loss 1.3900096416473389, R2 0.5123029947280884\n",
      "epoch 533, loss 1.319256067276001, R2 0.5145931243896484\n",
      "Eval loss 1.38832426071167, R2 0.5128191709518433\n",
      "epoch 534, loss 1.317629098892212, R2 0.515103280544281\n",
      "Eval loss 1.3866451978683472, R2 0.5133335590362549\n",
      "epoch 535, loss 1.3160080909729004, R2 0.5156115889549255\n",
      "Eval loss 1.3849718570709229, R2 0.5138463377952576\n",
      "epoch 536, loss 1.3143930435180664, R2 0.5161182284355164\n",
      "Eval loss 1.3833045959472656, R2 0.5143575072288513\n",
      "epoch 537, loss 1.3127838373184204, R2 0.516623318195343\n",
      "Eval loss 1.3816431760787964, R2 0.5148668885231018\n",
      "epoch 538, loss 1.311180591583252, R2 0.5171265006065369\n",
      "Eval loss 1.3799875974655151, R2 0.5153747200965881\n",
      "epoch 539, loss 1.3095831871032715, R2 0.5176280736923218\n",
      "Eval loss 1.3783382177352905, R2 0.5158807635307312\n",
      "epoch 540, loss 1.307991862297058, R2 0.5181280374526978\n",
      "Eval loss 1.3766945600509644, R2 0.5163852572441101\n",
      "epoch 541, loss 1.3064061403274536, R2 0.5186262726783752\n",
      "Eval loss 1.375056505203247, R2 0.5168880224227905\n",
      "epoch 542, loss 1.304826259613037, R2 0.519122838973999\n",
      "Eval loss 1.3734244108200073, R2 0.517389178276062\n",
      "epoch 543, loss 1.3032522201538086, R2 0.5196177363395691\n",
      "Eval loss 1.3717981576919556, R2 0.5178887248039246\n",
      "epoch 544, loss 1.301684021949768, R2 0.5201109647750854\n",
      "Eval loss 1.3701777458190918, R2 0.5183866620063782\n",
      "epoch 545, loss 1.300121545791626, R2 0.5206025838851929\n",
      "Eval loss 1.368562936782837, R2 0.5188828706741333\n",
      "epoch 546, loss 1.2985646724700928, R2 0.5210925340652466\n",
      "Eval loss 1.3669540882110596, R2 0.5193775296211243\n",
      "epoch 547, loss 1.2970134019851685, R2 0.5215808153152466\n",
      "Eval loss 1.365350604057312, R2 0.5198706388473511\n",
      "epoch 548, loss 1.2954682111740112, R2 0.5220674872398376\n",
      "Eval loss 1.3637529611587524, R2 0.5203620791435242\n",
      "epoch 549, loss 1.2939285039901733, R2 0.522552490234375\n",
      "Eval loss 1.3621609210968018, R2 0.5208519101142883\n",
      "epoch 550, loss 1.2923942804336548, R2 0.523036003112793\n",
      "Eval loss 1.360574722290039, R2 0.5213401317596436\n",
      "epoch 551, loss 1.2908657789230347, R2 0.5235177874565125\n",
      "Eval loss 1.3589938879013062, R2 0.5218268632888794\n",
      "epoch 552, loss 1.2893427610397339, R2 0.5239980220794678\n",
      "Eval loss 1.3574187755584717, R2 0.5223119258880615\n",
      "epoch 553, loss 1.287825345993042, R2 0.5244766473770142\n",
      "Eval loss 1.3558491468429565, R2 0.5227953791618347\n",
      "epoch 554, loss 1.2863134145736694, R2 0.5249536633491516\n",
      "Eval loss 1.3542852401733398, R2 0.5232773423194885\n",
      "epoch 555, loss 1.2848072052001953, R2 0.5254290103912354\n",
      "Eval loss 1.352726697921753, R2 0.5237577557563782\n",
      "epoch 556, loss 1.283306360244751, R2 0.5259029269218445\n",
      "Eval loss 1.3511736392974854, R2 0.5242366194725037\n",
      "epoch 557, loss 1.281810998916626, R2 0.5263751745223999\n",
      "Eval loss 1.349626064300537, R2 0.5247138142585754\n",
      "epoch 558, loss 1.2803211212158203, R2 0.5268459320068359\n",
      "Eval loss 1.3480840921401978, R2 0.5251895785331726\n",
      "epoch 559, loss 1.2788366079330444, R2 0.5273150205612183\n",
      "Eval loss 1.3465473651885986, R2 0.5256637930870056\n",
      "epoch 560, loss 1.277357578277588, R2 0.5277826189994812\n",
      "Eval loss 1.3450162410736084, R2 0.5261364579200745\n",
      "epoch 561, loss 1.275883674621582, R2 0.5282486081123352\n",
      "Eval loss 1.3434903621673584, R2 0.5266075730323792\n",
      "epoch 562, loss 1.274415373802185, R2 0.5287131071090698\n",
      "Eval loss 1.3419699668884277, R2 0.5270771980285645\n",
      "epoch 563, loss 1.2729524374008179, R2 0.5291760563850403\n",
      "Eval loss 1.3404549360275269, R2 0.5275452136993408\n",
      "epoch 564, loss 1.2714948654174805, R2 0.5296374559402466\n",
      "Eval loss 1.3389451503753662, R2 0.5280117988586426\n",
      "epoch 565, loss 1.2700425386428833, R2 0.5300973057746887\n",
      "Eval loss 1.3374407291412354, R2 0.528476893901825\n",
      "epoch 566, loss 1.2685953378677368, R2 0.5305556654930115\n",
      "Eval loss 1.3359415531158447, R2 0.5289403796195984\n",
      "epoch 567, loss 1.2671535015106201, R2 0.5310124158859253\n",
      "Eval loss 1.3344476222991943, R2 0.5294024348258972\n",
      "epoch 568, loss 1.2657169103622437, R2 0.5314677357673645\n",
      "Eval loss 1.3329589366912842, R2 0.5298631191253662\n",
      "epoch 569, loss 1.2642854452133179, R2 0.5319215655326843\n",
      "Eval loss 1.3314756155014038, R2 0.5303221344947815\n",
      "epoch 570, loss 1.2628593444824219, R2 0.53237384557724\n",
      "Eval loss 1.3299973011016846, R2 0.5307797789573669\n",
      "epoch 571, loss 1.2614383697509766, R2 0.532824695110321\n",
      "Eval loss 1.3285243511199951, R2 0.5312359929084778\n",
      "epoch 572, loss 1.2600224018096924, R2 0.5332740545272827\n",
      "Eval loss 1.3270565271377563, R2 0.5316905975341797\n",
      "epoch 573, loss 1.2586116790771484, R2 0.5337218642234802\n",
      "Eval loss 1.3255937099456787, R2 0.5321438312530518\n",
      "epoch 574, loss 1.2572060823440552, R2 0.5341681838035583\n",
      "Eval loss 1.3241361379623413, R2 0.5325956344604492\n",
      "epoch 575, loss 1.255805492401123, R2 0.5346130728721619\n",
      "Eval loss 1.3226836919784546, R2 0.5330458879470825\n",
      "epoch 576, loss 1.2544100284576416, R2 0.535056471824646\n",
      "Eval loss 1.321236252784729, R2 0.5334947109222412\n",
      "epoch 577, loss 1.2530195713043213, R2 0.5354984402656555\n",
      "Eval loss 1.319793939590454, R2 0.5339422225952148\n",
      "epoch 578, loss 1.2516342401504517, R2 0.5359388589859009\n",
      "Eval loss 1.3183566331863403, R2 0.5343880653381348\n",
      "epoch 579, loss 1.2502537965774536, R2 0.5363779664039612\n",
      "Eval loss 1.3169244527816772, R2 0.5348327159881592\n",
      "epoch 580, loss 1.2488784790039062, R2 0.5368155837059021\n",
      "Eval loss 1.3154972791671753, R2 0.5352757573127747\n",
      "epoch 581, loss 1.2475080490112305, R2 0.5372516512870789\n",
      "Eval loss 1.314074993133545, R2 0.5357174873352051\n",
      "epoch 582, loss 1.2461426258087158, R2 0.5376864075660706\n",
      "Eval loss 1.3126577138900757, R2 0.5361577868461609\n",
      "epoch 583, loss 1.2447822093963623, R2 0.5381197333335876\n",
      "Eval loss 1.311245322227478, R2 0.5365965962409973\n",
      "epoch 584, loss 1.2434264421463013, R2 0.5385515689849854\n",
      "Eval loss 1.3098379373550415, R2 0.5370340943336487\n",
      "epoch 585, loss 1.242075800895691, R2 0.5389819741249084\n",
      "Eval loss 1.308435320854187, R2 0.5374701619148254\n",
      "epoch 586, loss 1.240729808807373, R2 0.5394109487533569\n",
      "Eval loss 1.3070378303527832, R2 0.5379048585891724\n",
      "epoch 587, loss 1.2393888235092163, R2 0.5398386716842651\n",
      "Eval loss 1.3056448698043823, R2 0.5383381843566895\n",
      "epoch 588, loss 1.2380527257919312, R2 0.5402648448944092\n",
      "Eval loss 1.3042571544647217, R2 0.5387700796127319\n",
      "epoch 589, loss 1.2367212772369385, R2 0.5406897068023682\n",
      "Eval loss 1.3028740882873535, R2 0.5392005443572998\n",
      "epoch 590, loss 1.2353945970535278, R2 0.5411131381988525\n",
      "Eval loss 1.3014957904815674, R2 0.5396297574043274\n",
      "epoch 591, loss 1.2340728044509888, R2 0.5415351390838623\n",
      "Eval loss 1.3001223802566528, R2 0.5400574803352356\n",
      "epoch 592, loss 1.2327557802200317, R2 0.5419557690620422\n",
      "Eval loss 1.2987537384033203, R2 0.5404838919639587\n",
      "epoch 593, loss 1.2314435243606567, R2 0.5423750281333923\n",
      "Eval loss 1.2973898649215698, R2 0.5409089922904968\n",
      "epoch 594, loss 1.2301359176635742, R2 0.5427929162979126\n",
      "Eval loss 1.2960307598114014, R2 0.5413326621055603\n",
      "epoch 595, loss 1.2288329601287842, R2 0.5432094931602478\n",
      "Eval loss 1.2946761846542358, R2 0.5417550802230835\n",
      "epoch 596, loss 1.2275347709655762, R2 0.5436246395111084\n",
      "Eval loss 1.293326497077942, R2 0.5421760082244873\n",
      "epoch 597, loss 1.2262412309646606, R2 0.5440384745597839\n",
      "Eval loss 1.2919814586639404, R2 0.5425956845283508\n",
      "epoch 598, loss 1.2249523401260376, R2 0.5444509387016296\n",
      "Eval loss 1.290640950202942, R2 0.5430140495300293\n",
      "epoch 599, loss 1.2236679792404175, R2 0.5448619723320007\n",
      "Eval loss 1.2893054485321045, R2 0.5434310436248779\n",
      "epoch 600, loss 1.2223883867263794, R2 0.5452718138694763\n",
      "Eval loss 1.287974238395691, R2 0.5438467264175415\n",
      "epoch 601, loss 1.2211132049560547, R2 0.5456802248954773\n",
      "Eval loss 1.2866476774215698, R2 0.54426109790802\n",
      "epoch 602, loss 1.219842791557312, R2 0.5460873246192932\n",
      "Eval loss 1.2853258848190308, R2 0.5446741580963135\n",
      "epoch 603, loss 1.2185766696929932, R2 0.5464931130409241\n",
      "Eval loss 1.284008502960205, R2 0.5450859069824219\n",
      "epoch 604, loss 1.2173153162002563, R2 0.5468974709510803\n",
      "Eval loss 1.2826956510543823, R2 0.5454962849617004\n",
      "epoch 605, loss 1.216058373451233, R2 0.5473006367683411\n",
      "Eval loss 1.2813875675201416, R2 0.5459054708480835\n",
      "epoch 606, loss 1.2148059606552124, R2 0.5477024912834167\n",
      "Eval loss 1.2800838947296143, R2 0.5463132262229919\n",
      "epoch 607, loss 1.2135581970214844, R2 0.5481029748916626\n",
      "Eval loss 1.2787847518920898, R2 0.5467197895050049\n",
      "epoch 608, loss 1.212314486503601, R2 0.5485022068023682\n",
      "Eval loss 1.2774900197982788, R2 0.5471250414848328\n",
      "epoch 609, loss 1.2110754251480103, R2 0.5489001274108887\n",
      "Eval loss 1.2761998176574707, R2 0.5475290417671204\n",
      "epoch 610, loss 1.2098408937454224, R2 0.5492967367172241\n",
      "Eval loss 1.2749139070510864, R2 0.5479317307472229\n",
      "epoch 611, loss 1.2086106538772583, R2 0.5496920347213745\n",
      "Eval loss 1.2736327648162842, R2 0.5483331680297852\n",
      "epoch 612, loss 1.2073848247528076, R2 0.5500860810279846\n",
      "Eval loss 1.2723557949066162, R2 0.5487332344055176\n",
      "epoch 613, loss 1.2061632871627808, R2 0.5504788160324097\n",
      "Eval loss 1.2710832357406616, R2 0.5491321682929993\n",
      "epoch 614, loss 1.2049463987350464, R2 0.5508702993392944\n",
      "Eval loss 1.26981520652771, R2 0.5495297908782959\n",
      "epoch 615, loss 1.2037334442138672, R2 0.5512605309486389\n",
      "Eval loss 1.2685514688491821, R2 0.5499261617660522\n",
      "epoch 616, loss 1.2025251388549805, R2 0.5516495108604431\n",
      "Eval loss 1.2672920227050781, R2 0.5503213405609131\n",
      "epoch 617, loss 1.201321005821228, R2 0.5520371794700623\n",
      "Eval loss 1.2660369873046875, R2 0.5507151484489441\n",
      "epoch 618, loss 1.200121283531189, R2 0.5524235367774963\n",
      "Eval loss 1.2647861242294312, R2 0.5511077642440796\n",
      "epoch 619, loss 1.1989257335662842, R2 0.5528088212013245\n",
      "Eval loss 1.2635397911071777, R2 0.5514991879463196\n",
      "epoch 620, loss 1.1977344751358032, R2 0.553192675113678\n",
      "Eval loss 1.2622976303100586, R2 0.5518893599510193\n",
      "epoch 621, loss 1.196547269821167, R2 0.5535754561424255\n",
      "Eval loss 1.2610596418380737, R2 0.5522782802581787\n",
      "epoch 622, loss 1.1953644752502441, R2 0.553956925868988\n",
      "Eval loss 1.2598261833190918, R2 0.5526659488677979\n",
      "epoch 623, loss 1.1941858530044556, R2 0.5543370842933655\n",
      "Eval loss 1.258596658706665, R2 0.5530524849891663\n",
      "epoch 624, loss 1.1930114030838013, R2 0.5547160506248474\n",
      "Eval loss 1.2573715448379517, R2 0.5534377694129944\n",
      "epoch 625, loss 1.1918411254882812, R2 0.5550937652587891\n",
      "Eval loss 1.256150484085083, R2 0.5538218021392822\n",
      "epoch 626, loss 1.1906750202178955, R2 0.5554704070091248\n",
      "Eval loss 1.2549338340759277, R2 0.5542047023773193\n",
      "epoch 627, loss 1.189513087272644, R2 0.5558456778526306\n",
      "Eval loss 1.2537212371826172, R2 0.5545863509178162\n",
      "epoch 628, loss 1.1883552074432373, R2 0.5562198758125305\n",
      "Eval loss 1.2525126934051514, R2 0.5549668669700623\n",
      "epoch 629, loss 1.1872013807296753, R2 0.5565927624702454\n",
      "Eval loss 1.2513084411621094, R2 0.5553461313247681\n",
      "epoch 630, loss 1.186051607131958, R2 0.5569644570350647\n",
      "Eval loss 1.2501081228256226, R2 0.5557241439819336\n",
      "epoch 631, loss 1.1849061250686646, R2 0.5573349595069885\n",
      "Eval loss 1.2489120960235596, R2 0.5561010837554932\n",
      "epoch 632, loss 1.1837646961212158, R2 0.5577042698860168\n",
      "Eval loss 1.2477200031280518, R2 0.5564768314361572\n",
      "epoch 633, loss 1.1826272010803223, R2 0.5580724477767944\n",
      "Eval loss 1.2465322017669678, R2 0.5568513870239258\n",
      "epoch 634, loss 1.1814936399459839, R2 0.5584393739700317\n",
      "Eval loss 1.245348334312439, R2 0.557224690914154\n",
      "epoch 635, loss 1.1803641319274902, R2 0.5588052272796631\n",
      "Eval loss 1.2441686391830444, R2 0.5575969815254211\n",
      "epoch 636, loss 1.1792386770248413, R2 0.5591697692871094\n",
      "Eval loss 1.2429927587509155, R2 0.557968020439148\n",
      "epoch 637, loss 1.178117275238037, R2 0.5595331788063049\n",
      "Eval loss 1.2418209314346313, R2 0.5583378672599792\n",
      "epoch 638, loss 1.176999807357788, R2 0.5598954558372498\n",
      "Eval loss 1.2406532764434814, R2 0.5587065815925598\n",
      "epoch 639, loss 1.1758861541748047, R2 0.5602565407752991\n",
      "Eval loss 1.2394894361495972, R2 0.5590741634368896\n",
      "epoch 640, loss 1.1747766733169556, R2 0.5606164932250977\n",
      "Eval loss 1.238329529762268, R2 0.5594406127929688\n",
      "epoch 641, loss 1.1736708879470825, R2 0.560975193977356\n",
      "Eval loss 1.2371737957000732, R2 0.5598058700561523\n",
      "epoch 642, loss 1.1725690364837646, R2 0.5613328218460083\n",
      "Eval loss 1.2360217571258545, R2 0.5601701140403748\n",
      "epoch 643, loss 1.171471118927002, R2 0.5616893172264099\n",
      "Eval loss 1.23487389087677, R2 0.5605330467224121\n",
      "epoch 644, loss 1.1703771352767944, R2 0.5620446801185608\n",
      "Eval loss 1.2337298393249512, R2 0.5608949065208435\n",
      "epoch 645, loss 1.169286847114563, R2 0.5623988509178162\n",
      "Eval loss 1.232589602470398, R2 0.561255693435669\n",
      "epoch 646, loss 1.1682006120681763, R2 0.5627519488334656\n",
      "Eval loss 1.2314532995224, R2 0.5616153478622437\n",
      "epoch 647, loss 1.167117953300476, R2 0.5631038546562195\n",
      "Eval loss 1.230320692062378, R2 0.5619738101959229\n",
      "epoch 648, loss 1.1660393476486206, R2 0.5634546875953674\n",
      "Eval loss 1.2291922569274902, R2 0.5623311996459961\n",
      "epoch 649, loss 1.1649644374847412, R2 0.5638043880462646\n",
      "Eval loss 1.228067398071289, R2 0.5626875162124634\n",
      "epoch 650, loss 1.163893222808838, R2 0.5641530156135559\n",
      "Eval loss 1.226946473121643, R2 0.5630427002906799\n",
      "epoch 651, loss 1.1628257036209106, R2 0.5645005106925964\n",
      "Eval loss 1.2258293628692627, R2 0.5633967518806458\n",
      "epoch 652, loss 1.1617622375488281, R2 0.5648468136787415\n",
      "Eval loss 1.2247159481048584, R2 0.5637497305870056\n",
      "epoch 653, loss 1.1607023477554321, R2 0.5651920437812805\n",
      "Eval loss 1.2236064672470093, R2 0.5641016364097595\n",
      "epoch 654, loss 1.1596461534500122, R2 0.5655361413955688\n",
      "Eval loss 1.2225005626678467, R2 0.5644524693489075\n",
      "epoch 655, loss 1.158593773841858, R2 0.565879225730896\n",
      "Eval loss 1.2213985919952393, R2 0.5648021101951599\n",
      "epoch 656, loss 1.1575449705123901, R2 0.5662211775779724\n",
      "Eval loss 1.2203001976013184, R2 0.565150797367096\n",
      "epoch 657, loss 1.1564998626708984, R2 0.5665620565414429\n",
      "Eval loss 1.2192054986953735, R2 0.5654983520507812\n",
      "epoch 658, loss 1.1554584503173828, R2 0.5669018030166626\n",
      "Eval loss 1.2181146144866943, R2 0.5658447742462158\n",
      "epoch 659, loss 1.1544207334518433, R2 0.5672405362129211\n",
      "Eval loss 1.2170274257659912, R2 0.5661901831626892\n",
      "epoch 660, loss 1.1533865928649902, R2 0.567578136920929\n",
      "Eval loss 1.2159438133239746, R2 0.5665344595909119\n",
      "epoch 661, loss 1.1523560285568237, R2 0.5679147243499756\n",
      "Eval loss 1.2148640155792236, R2 0.5668778419494629\n",
      "epoch 662, loss 1.1513290405273438, R2 0.5682501792907715\n",
      "Eval loss 1.2137877941131592, R2 0.5672200322151184\n",
      "epoch 663, loss 1.1503057479858398, R2 0.5685845613479614\n",
      "Eval loss 1.2127151489257812, R2 0.567561149597168\n",
      "epoch 664, loss 1.149286150932312, R2 0.568917989730835\n",
      "Eval loss 1.2116461992263794, R2 0.5679012537002563\n",
      "epoch 665, loss 1.1482700109481812, R2 0.5692501664161682\n",
      "Eval loss 1.2105807065963745, R2 0.5682403445243835\n",
      "epoch 666, loss 1.1472572088241577, R2 0.5695815086364746\n",
      "Eval loss 1.2095190286636353, R2 0.56857830286026\n",
      "epoch 667, loss 1.1462483406066895, R2 0.5699116587638855\n",
      "Eval loss 1.2084609270095825, R2 0.5689152479171753\n",
      "epoch 668, loss 1.145242691040039, R2 0.5702407956123352\n",
      "Eval loss 1.2074062824249268, R2 0.5692511796951294\n",
      "epoch 669, loss 1.1442407369613647, R2 0.5705689191818237\n",
      "Eval loss 1.2063552141189575, R2 0.5695860981941223\n",
      "epoch 670, loss 1.1432422399520874, R2 0.5708960294723511\n",
      "Eval loss 1.2053076028823853, R2 0.569920003414154\n",
      "epoch 671, loss 1.1422470808029175, R2 0.5712219476699829\n",
      "Eval loss 1.2042635679244995, R2 0.5702527761459351\n",
      "epoch 672, loss 1.141255497932434, R2 0.5715469717979431\n",
      "Eval loss 1.2032231092453003, R2 0.5705845355987549\n",
      "epoch 673, loss 1.1402674913406372, R2 0.5718709230422974\n",
      "Eval loss 1.2021859884262085, R2 0.5709154009819031\n",
      "epoch 674, loss 1.1392827033996582, R2 0.5721939206123352\n",
      "Eval loss 1.2011525630950928, R2 0.5712451338768005\n",
      "epoch 675, loss 1.1383014917373657, R2 0.5725157260894775\n",
      "Eval loss 1.200122594833374, R2 0.5715739130973816\n",
      "epoch 676, loss 1.1373236179351807, R2 0.572836697101593\n",
      "Eval loss 1.1990959644317627, R2 0.5719016790390015\n",
      "epoch 677, loss 1.1363492012023926, R2 0.5731565952301025\n",
      "Eval loss 1.198072910308838, R2 0.5722284317016602\n",
      "epoch 678, loss 1.135378122329712, R2 0.5734754204750061\n",
      "Eval loss 1.1970531940460205, R2 0.5725542306900024\n",
      "epoch 679, loss 1.1344103813171387, R2 0.573793351650238\n",
      "Eval loss 1.1960369348526, R2 0.572878897190094\n",
      "epoch 680, loss 1.133446216583252, R2 0.574110209941864\n",
      "Eval loss 1.195024013519287, R2 0.5732026696205139\n",
      "epoch 681, loss 1.132485270500183, R2 0.5744260549545288\n",
      "Eval loss 1.194014549255371, R2 0.5735254287719727\n",
      "epoch 682, loss 1.1315276622772217, R2 0.5747408866882324\n",
      "Eval loss 1.1930084228515625, R2 0.573847234249115\n",
      "epoch 683, loss 1.1305733919143677, R2 0.5750548243522644\n",
      "Eval loss 1.1920058727264404, R2 0.5741679668426514\n",
      "epoch 684, loss 1.129622459411621, R2 0.5753676295280457\n",
      "Eval loss 1.1910063028335571, R2 0.5744878053665161\n",
      "epoch 685, loss 1.1286747455596924, R2 0.5756796002388\n",
      "Eval loss 1.1900103092193604, R2 0.5748066306114197\n",
      "epoch 686, loss 1.127730369567871, R2 0.5759904980659485\n",
      "Eval loss 1.1890177726745605, R2 0.5751245021820068\n",
      "epoch 687, loss 1.1267892122268677, R2 0.5763004422187805\n",
      "Eval loss 1.188028335571289, R2 0.5754413604736328\n",
      "epoch 688, loss 1.1258515119552612, R2 0.5766094326972961\n",
      "Eval loss 1.1870423555374146, R2 0.5757573246955872\n",
      "epoch 689, loss 1.124916911125183, R2 0.5769174695014954\n",
      "Eval loss 1.186059594154358, R2 0.5760722160339355\n",
      "epoch 690, loss 1.1239855289459229, R2 0.5772244930267334\n",
      "Eval loss 1.1850801706314087, R2 0.5763862133026123\n",
      "epoch 691, loss 1.1230573654174805, R2 0.5775305032730103\n",
      "Eval loss 1.1841039657592773, R2 0.5766992568969727\n",
      "epoch 692, loss 1.122132420539856, R2 0.5778356790542603\n",
      "Eval loss 1.1831308603286743, R2 0.5770113468170166\n",
      "epoch 693, loss 1.1212108135223389, R2 0.5781398415565491\n",
      "Eval loss 1.1821612119674683, R2 0.5773224234580994\n",
      "epoch 694, loss 1.1202921867370605, R2 0.5784429907798767\n",
      "Eval loss 1.1811946630477905, R2 0.5776326656341553\n",
      "epoch 695, loss 1.1193767786026, R2 0.5787453055381775\n",
      "Eval loss 1.1802315711975098, R2 0.57794189453125\n",
      "epoch 696, loss 1.118464708328247, R2 0.5790465474128723\n",
      "Eval loss 1.1792714595794678, R2 0.5782501697540283\n",
      "epoch 697, loss 1.1175557374954224, R2 0.5793469548225403\n",
      "Eval loss 1.1783145666122437, R2 0.578557550907135\n",
      "epoch 698, loss 1.1166497468948364, R2 0.5796463489532471\n",
      "Eval loss 1.177361011505127, R2 0.5788639783859253\n",
      "epoch 699, loss 1.1157468557357788, R2 0.5799448490142822\n",
      "Eval loss 1.176410436630249, R2 0.5791694521903992\n",
      "epoch 700, loss 1.1148474216461182, R2 0.580242395401001\n",
      "Eval loss 1.175463080406189, R2 0.5794740319252014\n",
      "epoch 701, loss 1.1139507293701172, R2 0.5805389881134033\n",
      "Eval loss 1.1745189428329468, R2 0.579777717590332\n",
      "epoch 702, loss 1.113057255744934, R2 0.5808347463607788\n",
      "Eval loss 1.173577904701233, R2 0.5800804495811462\n",
      "epoch 703, loss 1.1121670007705688, R2 0.5811294913291931\n",
      "Eval loss 1.1726398468017578, R2 0.580382227897644\n",
      "epoch 704, loss 1.1112794876098633, R2 0.5814234018325806\n",
      "Eval loss 1.1717051267623901, R2 0.5806831121444702\n",
      "epoch 705, loss 1.1103953123092651, R2 0.5817162990570068\n",
      "Eval loss 1.1707735061645508, R2 0.5809831023216248\n",
      "epoch 706, loss 1.1095141172409058, R2 0.5820083022117615\n",
      "Eval loss 1.1698448657989502, R2 0.5812822580337524\n",
      "epoch 707, loss 1.1086360216140747, R2 0.5822994112968445\n",
      "Eval loss 1.168919324874878, R2 0.5815803408622742\n",
      "epoch 708, loss 1.1077606678009033, R2 0.5825895667076111\n",
      "Eval loss 1.167996883392334, R2 0.5818777084350586\n",
      "epoch 709, loss 1.1068886518478394, R2 0.5828788876533508\n",
      "Eval loss 1.1670775413513184, R2 0.5821740031242371\n",
      "epoch 710, loss 1.1060194969177246, R2 0.583167314529419\n",
      "Eval loss 1.166161060333252, R2 0.5824694633483887\n",
      "epoch 711, loss 1.1051533222198486, R2 0.5834548473358154\n",
      "Eval loss 1.1652477979660034, R2 0.5827640295028687\n",
      "epoch 712, loss 1.1042900085449219, R2 0.5837413668632507\n",
      "Eval loss 1.1643375158309937, R2 0.583057701587677\n",
      "epoch 713, loss 1.103429913520813, R2 0.584027111530304\n",
      "Eval loss 1.1634302139282227, R2 0.5833505392074585\n",
      "epoch 714, loss 1.1025726795196533, R2 0.5843119621276855\n",
      "Eval loss 1.16252601146698, R2 0.5836424827575684\n",
      "epoch 715, loss 1.1017181873321533, R2 0.5845959186553955\n",
      "Eval loss 1.1616246700286865, R2 0.5839335322380066\n",
      "epoch 716, loss 1.1008667945861816, R2 0.5848790407180786\n",
      "Eval loss 1.1607263088226318, R2 0.5842235684394836\n",
      "epoch 717, loss 1.1000183820724487, R2 0.5851611495018005\n",
      "Eval loss 1.1598308086395264, R2 0.5845129489898682\n",
      "epoch 718, loss 1.0991727113723755, R2 0.5854424834251404\n",
      "Eval loss 1.1589386463165283, R2 0.5848013162612915\n",
      "epoch 719, loss 1.0983299016952515, R2 0.5857229232788086\n",
      "Eval loss 1.1580491065979004, R2 0.585088849067688\n",
      "epoch 720, loss 1.0974901914596558, R2 0.5860024094581604\n",
      "Eval loss 1.1571625471115112, R2 0.5853756070137024\n",
      "epoch 721, loss 1.0966532230377197, R2 0.5862811207771301\n",
      "Eval loss 1.1562788486480713, R2 0.5856614112854004\n",
      "epoch 722, loss 1.0958192348480225, R2 0.586558997631073\n",
      "Eval loss 1.1553981304168701, R2 0.5859463214874268\n",
      "epoch 723, loss 1.0949878692626953, R2 0.5868359804153442\n",
      "Eval loss 1.1545205116271973, R2 0.586230456829071\n",
      "epoch 724, loss 1.094159483909607, R2 0.5871121287345886\n",
      "Eval loss 1.1536455154418945, R2 0.5865137577056885\n",
      "epoch 725, loss 1.0933340787887573, R2 0.5873873233795166\n",
      "Eval loss 1.1527734994888306, R2 0.5867961645126343\n",
      "epoch 726, loss 1.0925112962722778, R2 0.5876617431640625\n",
      "Eval loss 1.1519043445587158, R2 0.5870776772499084\n",
      "epoch 727, loss 1.0916913747787476, R2 0.5879352688789368\n",
      "Eval loss 1.1510381698608398, R2 0.5873584151268005\n",
      "epoch 728, loss 1.0908743143081665, R2 0.5882080793380737\n",
      "Eval loss 1.150174617767334, R2 0.5876383185386658\n",
      "epoch 729, loss 1.0900599956512451, R2 0.5884799361228943\n",
      "Eval loss 1.1493139266967773, R2 0.5879173874855042\n",
      "epoch 730, loss 1.0892484188079834, R2 0.588750958442688\n",
      "Eval loss 1.1484562158584595, R2 0.5881955027580261\n",
      "epoch 731, loss 1.088439702987671, R2 0.5890211462974548\n",
      "Eval loss 1.1476012468338013, R2 0.5884729027748108\n",
      "epoch 732, loss 1.087633490562439, R2 0.5892905592918396\n",
      "Eval loss 1.1467491388320923, R2 0.5887494087219238\n",
      "epoch 733, loss 1.0868302583694458, R2 0.5895590782165527\n",
      "Eval loss 1.1458996534347534, R2 0.58902508020401\n",
      "epoch 734, loss 1.0860296487808228, R2 0.5898268222808838\n",
      "Eval loss 1.1450531482696533, R2 0.5893000364303589\n",
      "epoch 735, loss 1.085231900215149, R2 0.5900936722755432\n",
      "Eval loss 1.1442092657089233, R2 0.5895740985870361\n",
      "epoch 736, loss 1.0844367742538452, R2 0.5903596878051758\n",
      "Eval loss 1.1433682441711426, R2 0.5898472666740417\n",
      "epoch 737, loss 1.0836443901062012, R2 0.590624988079071\n",
      "Eval loss 1.1425299644470215, R2 0.5901197195053101\n",
      "epoch 738, loss 1.0828546285629272, R2 0.5908893942832947\n",
      "Eval loss 1.14169442653656, R2 0.5903913378715515\n",
      "epoch 739, loss 1.082067608833313, R2 0.5911529660224915\n",
      "Eval loss 1.1408616304397583, R2 0.5906621217727661\n",
      "epoch 740, loss 1.0812832117080688, R2 0.5914158225059509\n",
      "Eval loss 1.1400315761566162, R2 0.5909321904182434\n",
      "epoch 741, loss 1.080501675605774, R2 0.5916778445243835\n",
      "Eval loss 1.1392042636871338, R2 0.5912013649940491\n",
      "epoch 742, loss 1.07972252368927, R2 0.5919389724731445\n",
      "Eval loss 1.138379454612732, R2 0.5914697647094727\n",
      "epoch 743, loss 1.0789461135864258, R2 0.5921993851661682\n",
      "Eval loss 1.1375573873519897, R2 0.5917373299598694\n",
      "epoch 744, loss 1.0781723260879517, R2 0.592458963394165\n",
      "Eval loss 1.1367381811141968, R2 0.592004120349884\n",
      "epoch 745, loss 1.0774012804031372, R2 0.5927177667617798\n",
      "Eval loss 1.1359214782714844, R2 0.5922701358795166\n",
      "epoch 746, loss 1.0766326189041138, R2 0.5929757952690125\n",
      "Eval loss 1.1351075172424316, R2 0.5925353169441223\n",
      "epoch 747, loss 1.07586669921875, R2 0.593233048915863\n",
      "Eval loss 1.1342962980270386, R2 0.5927997827529907\n",
      "epoch 748, loss 1.0751034021377563, R2 0.5934894680976868\n",
      "Eval loss 1.1334877014160156, R2 0.5930634140968323\n",
      "epoch 749, loss 1.0743426084518433, R2 0.5937450528144836\n",
      "Eval loss 1.1326817274093628, R2 0.593326210975647\n",
      "epoch 750, loss 1.0735844373703003, R2 0.5939999222755432\n",
      "Eval loss 1.1318782567977905, R2 0.5935883522033691\n",
      "epoch 751, loss 1.0728288888931274, R2 0.5942539572715759\n",
      "Eval loss 1.131077527999878, R2 0.5938495993614197\n",
      "epoch 752, loss 1.0720757246017456, R2 0.5945072770118713\n",
      "Eval loss 1.130279302597046, R2 0.5941100716590881\n",
      "epoch 753, loss 1.071325421333313, R2 0.5947598218917847\n",
      "Eval loss 1.129483699798584, R2 0.5943699479103088\n",
      "epoch 754, loss 1.0705772638320923, R2 0.5950115323066711\n",
      "Eval loss 1.1286907196044922, R2 0.5946289300918579\n",
      "epoch 755, loss 1.0698319673538208, R2 0.5952625870704651\n",
      "Eval loss 1.127900242805481, R2 0.5948870778083801\n",
      "epoch 756, loss 1.0690890550613403, R2 0.5955127477645874\n",
      "Eval loss 1.1271125078201294, R2 0.595144510269165\n",
      "epoch 757, loss 1.0683485269546509, R2 0.5957622528076172\n",
      "Eval loss 1.1263271570205688, R2 0.5954011678695679\n",
      "epoch 758, loss 1.0676106214523315, R2 0.5960109829902649\n",
      "Eval loss 1.1255444288253784, R2 0.5956571102142334\n",
      "epoch 759, loss 1.0668752193450928, R2 0.5962588787078857\n",
      "Eval loss 1.1247642040252686, R2 0.5959123373031616\n",
      "epoch 760, loss 1.066142201423645, R2 0.5965060591697693\n",
      "Eval loss 1.1239864826202393, R2 0.596166729927063\n",
      "epoch 761, loss 1.065411925315857, R2 0.596752405166626\n",
      "Eval loss 1.1232112646102905, R2 0.5964203476905823\n",
      "epoch 762, loss 1.0646836757659912, R2 0.5969981551170349\n",
      "Eval loss 1.1224387884140015, R2 0.5966733694076538\n",
      "epoch 763, loss 1.0639581680297852, R2 0.597243070602417\n",
      "Eval loss 1.1216684579849243, R2 0.5969254970550537\n",
      "epoch 764, loss 1.0632350444793701, R2 0.597487211227417\n",
      "Eval loss 1.1209009885787964, R2 0.5971769094467163\n",
      "epoch 765, loss 1.0625141859054565, R2 0.5977306962013245\n",
      "Eval loss 1.12013578414917, R2 0.5974276065826416\n",
      "epoch 766, loss 1.061795949935913, R2 0.5979734063148499\n",
      "Eval loss 1.119373083114624, R2 0.5976775288581848\n",
      "epoch 767, loss 1.0610800981521606, R2 0.5982153415679932\n",
      "Eval loss 1.1186128854751587, R2 0.5979267358779907\n",
      "epoch 768, loss 1.0603666305541992, R2 0.5984565615653992\n",
      "Eval loss 1.1178550720214844, R2 0.5981752276420593\n",
      "epoch 769, loss 1.0596556663513184, R2 0.5986970663070679\n",
      "Eval loss 1.1170997619628906, R2 0.5984230041503906\n",
      "epoch 770, loss 1.058946967124939, R2 0.5989368557929993\n",
      "Eval loss 1.116346836090088, R2 0.5986700057983398\n",
      "epoch 771, loss 1.0582407712936401, R2 0.5991758704185486\n",
      "Eval loss 1.1155964136123657, R2 0.5989162921905518\n",
      "epoch 772, loss 1.0575367212295532, R2 0.5994142293930054\n",
      "Eval loss 1.1148483753204346, R2 0.5991618037223816\n",
      "epoch 773, loss 1.0568351745605469, R2 0.5996517539024353\n",
      "Eval loss 1.114102840423584, R2 0.5994067192077637\n",
      "epoch 774, loss 1.056135892868042, R2 0.5998886227607727\n",
      "Eval loss 1.1133595705032349, R2 0.5996508598327637\n",
      "epoch 775, loss 1.0554389953613281, R2 0.6001248359680176\n",
      "Eval loss 1.1126186847686768, R2 0.5998942255973816\n",
      "epoch 776, loss 1.0547446012496948, R2 0.6003602743148804\n",
      "Eval loss 1.1118803024291992, R2 0.600136935710907\n",
      "epoch 777, loss 1.0540523529052734, R2 0.6005949974060059\n",
      "Eval loss 1.1111443042755127, R2 0.6003788709640503\n",
      "epoch 778, loss 1.0533623695373535, R2 0.600829005241394\n",
      "Eval loss 1.1104105710983276, R2 0.6006202101707458\n",
      "epoch 779, loss 1.0526747703552246, R2 0.6010623574256897\n",
      "Eval loss 1.1096793413162231, R2 0.6008607149124146\n",
      "epoch 780, loss 1.0519895553588867, R2 0.6012949347496033\n",
      "Eval loss 1.1089502573013306, R2 0.6011006236076355\n",
      "epoch 781, loss 1.0513064861297607, R2 0.6015268564224243\n",
      "Eval loss 1.1082236766815186, R2 0.6013398170471191\n",
      "epoch 782, loss 1.0506259202957153, R2 0.6017580032348633\n",
      "Eval loss 1.1074994802474976, R2 0.6015781760215759\n",
      "epoch 783, loss 1.0499472618103027, R2 0.6019884943962097\n",
      "Eval loss 1.1067774295806885, R2 0.6018161177635193\n",
      "epoch 784, loss 1.0492712259292603, R2 0.6022183299064636\n",
      "Eval loss 1.1060577630996704, R2 0.6020531058311462\n",
      "epoch 785, loss 1.0485972166061401, R2 0.6024474501609802\n",
      "Eval loss 1.1053403615951538, R2 0.6022894978523254\n",
      "epoch 786, loss 1.0479257106781006, R2 0.6026758551597595\n",
      "Eval loss 1.1046253442764282, R2 0.6025251746177673\n",
      "epoch 787, loss 1.0472562313079834, R2 0.6029036045074463\n",
      "Eval loss 1.103912591934204, R2 0.6027601957321167\n",
      "epoch 788, loss 1.0465890169143677, R2 0.6031306385993958\n",
      "Eval loss 1.103202223777771, R2 0.6029945015907288\n",
      "epoch 789, loss 1.0459240674972534, R2 0.6033570170402527\n",
      "Eval loss 1.1024940013885498, R2 0.6032280325889587\n",
      "epoch 790, loss 1.045261263847351, R2 0.6035826802253723\n",
      "Eval loss 1.1017881631851196, R2 0.603460967540741\n",
      "epoch 791, loss 1.0446007251739502, R2 0.6038076877593994\n",
      "Eval loss 1.1010844707489014, R2 0.6036933064460754\n",
      "epoch 792, loss 1.0439424514770508, R2 0.604032039642334\n",
      "Eval loss 1.1003830432891846, R2 0.6039249300956726\n",
      "epoch 793, loss 1.0432862043380737, R2 0.6042556166648865\n",
      "Eval loss 1.0996838808059692, R2 0.6041558384895325\n",
      "epoch 794, loss 1.0426322221755981, R2 0.6044787168502808\n",
      "Eval loss 1.0989868640899658, R2 0.6043860912322998\n",
      "epoch 795, loss 1.0419803857803345, R2 0.604701042175293\n",
      "Eval loss 1.0982922315597534, R2 0.6046156883239746\n",
      "epoch 796, loss 1.0413308143615723, R2 0.6049226522445679\n",
      "Eval loss 1.097599744796753, R2 0.6048446297645569\n",
      "epoch 797, loss 1.0406832695007324, R2 0.6051435470581055\n",
      "Eval loss 1.096909523010254, R2 0.6050727963447571\n",
      "epoch 798, loss 1.040037989616394, R2 0.6053639650344849\n",
      "Eval loss 1.0962213277816772, R2 0.6053003668785095\n",
      "epoch 799, loss 1.0393948554992676, R2 0.6055836081504822\n",
      "Eval loss 1.0955355167388916, R2 0.6055272817611694\n",
      "epoch 800, loss 1.038753867149353, R2 0.605802595615387\n",
      "Eval loss 1.0948518514633179, R2 0.6057535409927368\n",
      "epoch 801, loss 1.0381147861480713, R2 0.6060209274291992\n",
      "Eval loss 1.0941704511642456, R2 0.6059792041778564\n",
      "epoch 802, loss 1.037477970123291, R2 0.6062386631965637\n",
      "Eval loss 1.0934909582138062, R2 0.6062042117118835\n",
      "epoch 803, loss 1.0368434190750122, R2 0.6064556241035461\n",
      "Eval loss 1.0928137302398682, R2 0.6064283847808838\n",
      "epoch 804, loss 1.0362107753753662, R2 0.6066721081733704\n",
      "Eval loss 1.0921387672424316, R2 0.6066520810127258\n",
      "epoch 805, loss 1.0355802774429321, R2 0.6068877577781677\n",
      "Eval loss 1.0914658308029175, R2 0.6068751215934753\n",
      "epoch 806, loss 1.0349518060684204, R2 0.6071028709411621\n",
      "Eval loss 1.0907952785491943, R2 0.6070975065231323\n",
      "epoch 807, loss 1.0343254804611206, R2 0.6073173880577087\n",
      "Eval loss 1.090126633644104, R2 0.6073191165924072\n",
      "epoch 808, loss 1.0337011814117432, R2 0.6075311303138733\n",
      "Eval loss 1.089460015296936, R2 0.6075401902198792\n",
      "epoch 809, loss 1.0330791473388672, R2 0.6077443361282349\n",
      "Eval loss 1.088795781135559, R2 0.6077607274055481\n",
      "epoch 810, loss 1.0324589014053345, R2 0.6079568862915039\n",
      "Eval loss 1.0881335735321045, R2 0.607980489730835\n",
      "epoch 811, loss 1.0318408012390137, R2 0.6081687808036804\n",
      "Eval loss 1.0874733924865723, R2 0.6081996560096741\n",
      "epoch 812, loss 1.0312248468399048, R2 0.6083800196647644\n",
      "Eval loss 1.086815357208252, R2 0.6084181070327759\n",
      "epoch 813, loss 1.0306106805801392, R2 0.6085907220840454\n",
      "Eval loss 1.086159348487854, R2 0.6086360812187195\n",
      "epoch 814, loss 1.029998779296875, R2 0.6088007092475891\n",
      "Eval loss 1.085505485534668, R2 0.6088533401489258\n",
      "epoch 815, loss 1.0293887853622437, R2 0.6090101003646851\n",
      "Eval loss 1.0848536491394043, R2 0.6090699434280396\n",
      "epoch 816, loss 1.0287809371948242, R2 0.6092188358306885\n",
      "Eval loss 1.084203839302063, R2 0.6092860102653503\n",
      "epoch 817, loss 1.0281749963760376, R2 0.6094269752502441\n",
      "Eval loss 1.0835561752319336, R2 0.6095014214515686\n",
      "epoch 818, loss 1.0275710821151733, R2 0.6096344590187073\n",
      "Eval loss 1.0829106569290161, R2 0.6097161769866943\n",
      "epoch 819, loss 1.0269689559936523, R2 0.6098414659500122\n",
      "Eval loss 1.082266926765442, R2 0.6099303364753723\n",
      "epoch 820, loss 1.0263689756393433, R2 0.6100478172302246\n",
      "Eval loss 1.0816253423690796, R2 0.6101438999176025\n",
      "epoch 821, loss 1.0257710218429565, R2 0.6102535128593445\n",
      "Eval loss 1.0809857845306396, R2 0.610356867313385\n",
      "epoch 822, loss 1.0251750946044922, R2 0.610458493232727\n",
      "Eval loss 1.0803483724594116, R2 0.610569179058075\n",
      "epoch 823, loss 1.024580955505371, R2 0.6106628775596619\n",
      "Eval loss 1.0797127485275269, R2 0.6107808351516724\n",
      "epoch 824, loss 1.0239888429641724, R2 0.6108667850494385\n",
      "Eval loss 1.0790793895721436, R2 0.6109919548034668\n",
      "epoch 825, loss 1.0233986377716064, R2 0.6110700964927673\n",
      "Eval loss 1.0784478187561035, R2 0.6112024784088135\n",
      "epoch 826, loss 1.0228103399276733, R2 0.6112726926803589\n",
      "Eval loss 1.0778182744979858, R2 0.6114123463630676\n",
      "epoch 827, loss 1.0222240686416626, R2 0.6114747524261475\n",
      "Eval loss 1.0771907567977905, R2 0.6116216778755188\n",
      "epoch 828, loss 1.0216397047042847, R2 0.6116762161254883\n",
      "Eval loss 1.0765652656555176, R2 0.6118302941322327\n",
      "epoch 829, loss 1.0210572481155396, R2 0.6118770837783813\n",
      "Eval loss 1.0759416818618774, R2 0.6120384335517883\n",
      "epoch 830, loss 1.0204766988754272, R2 0.6120772957801819\n",
      "Eval loss 1.0753201246261597, R2 0.6122459173202515\n",
      "epoch 831, loss 1.0198979377746582, R2 0.6122769117355347\n",
      "Eval loss 1.0747004747390747, R2 0.6124528646469116\n",
      "epoch 832, loss 1.0193212032318115, R2 0.612476110458374\n",
      "Eval loss 1.0740827322006226, R2 0.6126590967178345\n",
      "epoch 833, loss 1.0187464952468872, R2 0.6126745939254761\n",
      "Eval loss 1.0734670162200928, R2 0.6128648519515991\n",
      "epoch 834, loss 1.018173336982727, R2 0.6128724217414856\n",
      "Eval loss 1.0728530883789062, R2 0.613070011138916\n",
      "epoch 835, loss 1.0176023244857788, R2 0.6130697727203369\n",
      "Eval loss 1.072241187095642, R2 0.6132745146751404\n",
      "epoch 836, loss 1.0170331001281738, R2 0.6132664680480957\n",
      "Eval loss 1.0716311931610107, R2 0.6134784817695618\n",
      "epoch 837, loss 1.016465663909912, R2 0.6134626865386963\n",
      "Eval loss 1.0710232257843018, R2 0.6136818528175354\n",
      "epoch 838, loss 1.0159001350402832, R2 0.6136581897735596\n",
      "Eval loss 1.0704171657562256, R2 0.6138846278190613\n",
      "epoch 839, loss 1.015336513519287, R2 0.6138532757759094\n",
      "Eval loss 1.0698126554489136, R2 0.6140868663787842\n",
      "epoch 840, loss 1.0147746801376343, R2 0.6140477061271667\n",
      "Eval loss 1.069210410118103, R2 0.6142885684967041\n",
      "epoch 841, loss 1.0142147541046143, R2 0.6142414808273315\n",
      "Eval loss 1.0686099529266357, R2 0.6144896149635315\n",
      "epoch 842, loss 1.013656497001648, R2 0.6144347786903381\n",
      "Eval loss 1.0680114030838013, R2 0.6146901249885559\n",
      "epoch 843, loss 1.0131001472473145, R2 0.6146275401115417\n",
      "Eval loss 1.06741464138031, R2 0.6148900985717773\n",
      "epoch 844, loss 1.0125455856323242, R2 0.6148197054862976\n",
      "Eval loss 1.0668197870254517, R2 0.6150893568992615\n",
      "epoch 845, loss 1.0119929313659668, R2 0.6150112748146057\n",
      "Eval loss 1.0662267208099365, R2 0.6152881979942322\n",
      "epoch 846, loss 1.0114420652389526, R2 0.6152023077011108\n",
      "Eval loss 1.0656355619430542, R2 0.6154863834381104\n",
      "epoch 847, loss 1.0108928680419922, R2 0.6153927445411682\n",
      "Eval loss 1.0650463104248047, R2 0.6156841516494751\n",
      "epoch 848, loss 1.0103455781936646, R2 0.6155826449394226\n",
      "Eval loss 1.064458966255188, R2 0.6158811450004578\n",
      "epoch 849, loss 1.0097999572753906, R2 0.6157718896865845\n",
      "Eval loss 1.0638734102249146, R2 0.616077721118927\n",
      "epoch 850, loss 1.00925612449646, R2 0.6159606575965881\n",
      "Eval loss 1.0632895231246948, R2 0.6162737607955933\n",
      "epoch 851, loss 1.008714199066162, R2 0.6161489486694336\n",
      "Eval loss 1.062707543373108, R2 0.616469144821167\n",
      "epoch 852, loss 1.0081738233566284, R2 0.6163365840911865\n",
      "Eval loss 1.0621273517608643, R2 0.6166640520095825\n",
      "epoch 853, loss 1.007635474205017, R2 0.6165237426757812\n",
      "Eval loss 1.0615490674972534, R2 0.6168583035469055\n",
      "epoch 854, loss 1.00709867477417, R2 0.6167103052139282\n",
      "Eval loss 1.0609725713729858, R2 0.6170520782470703\n",
      "epoch 855, loss 1.0065637826919556, R2 0.616896390914917\n",
      "Eval loss 1.060397744178772, R2 0.6172453165054321\n",
      "epoch 856, loss 1.0060304403305054, R2 0.6170818209648132\n",
      "Eval loss 1.059824824333191, R2 0.6174379587173462\n",
      "epoch 857, loss 1.0054988861083984, R2 0.6172667145729065\n",
      "Eval loss 1.0592536926269531, R2 0.617630124092102\n",
      "epoch 858, loss 1.0049691200256348, R2 0.6174511313438416\n",
      "Eval loss 1.0586843490600586, R2 0.6178216934204102\n",
      "epoch 859, loss 1.0044410228729248, R2 0.6176349520683289\n",
      "Eval loss 1.0581166744232178, R2 0.6180127859115601\n",
      "epoch 860, loss 1.003914713859558, R2 0.6178184151649475\n",
      "Eval loss 1.0575506687164307, R2 0.6182032823562622\n",
      "epoch 861, loss 1.0033899545669556, R2 0.6180011630058289\n",
      "Eval loss 1.056986689567566, R2 0.6183931827545166\n",
      "epoch 862, loss 1.0028669834136963, R2 0.618183434009552\n",
      "Eval loss 1.0564243793487549, R2 0.6185826659202576\n",
      "epoch 863, loss 1.0023458003997803, R2 0.6183651685714722\n",
      "Eval loss 1.055863618850708, R2 0.6187715530395508\n",
      "epoch 864, loss 1.0018261671066284, R2 0.6185463070869446\n",
      "Eval loss 1.0553048849105835, R2 0.618959903717041\n",
      "epoch 865, loss 1.0013083219528198, R2 0.6187269687652588\n",
      "Eval loss 1.0547477006912231, R2 0.6191478371620178\n",
      "epoch 866, loss 1.0007920265197754, R2 0.6189071536064148\n",
      "Eval loss 1.0541924238204956, R2 0.6193350553512573\n",
      "epoch 867, loss 1.0002774000167847, R2 0.6190868020057678\n",
      "Eval loss 1.0536385774612427, R2 0.6195218563079834\n",
      "epoch 868, loss 0.9997645616531372, R2 0.6192658543586731\n",
      "Eval loss 1.0530866384506226, R2 0.6197081208229065\n",
      "epoch 869, loss 0.9992533326148987, R2 0.6194444894790649\n",
      "Eval loss 1.0525364875793457, R2 0.6198938488960266\n",
      "epoch 870, loss 0.9987438321113586, R2 0.6196224689483643\n",
      "Eval loss 1.051987886428833, R2 0.620078980922699\n",
      "epoch 871, loss 0.998235821723938, R2 0.6197999715805054\n",
      "Eval loss 1.051440954208374, R2 0.6202636957168579\n",
      "epoch 872, loss 0.997729480266571, R2 0.6199769973754883\n",
      "Eval loss 1.0508958101272583, R2 0.6204478740692139\n",
      "epoch 873, loss 0.997224748134613, R2 0.6201534867286682\n",
      "Eval loss 1.0503524541854858, R2 0.6206315159797668\n",
      "epoch 874, loss 0.9967218637466431, R2 0.6203294992446899\n",
      "Eval loss 1.049810528755188, R2 0.6208146810531616\n",
      "epoch 875, loss 0.9962204098701477, R2 0.6205049753189087\n",
      "Eval loss 1.049270510673523, R2 0.6209972500801086\n",
      "epoch 876, loss 0.9957205653190613, R2 0.6206799149513245\n",
      "Eval loss 1.0487319231033325, R2 0.6211794018745422\n",
      "epoch 877, loss 0.9952223896980286, R2 0.620854377746582\n",
      "Eval loss 1.048195242881775, R2 0.6213609576225281\n",
      "epoch 878, loss 0.9947258234024048, R2 0.6210283637046814\n",
      "Eval loss 1.0476601123809814, R2 0.6215420365333557\n",
      "epoch 879, loss 0.9942307472229004, R2 0.6212018132209778\n",
      "Eval loss 1.0471265316009521, R2 0.6217226386070251\n",
      "epoch 880, loss 0.9937372803688049, R2 0.6213747262954712\n",
      "Eval loss 1.0465946197509766, R2 0.6219027042388916\n",
      "epoch 881, loss 0.993245542049408, R2 0.6215471625328064\n",
      "Eval loss 1.0460644960403442, R2 0.6220822334289551\n",
      "epoch 882, loss 0.9927552938461304, R2 0.6217191219329834\n",
      "Eval loss 1.045535922050476, R2 0.6222612857818604\n",
      "epoch 883, loss 0.9922667145729065, R2 0.621890664100647\n",
      "Eval loss 1.045008897781372, R2 0.6224399209022522\n",
      "epoch 884, loss 0.991779625415802, R2 0.622061550617218\n",
      "Eval loss 1.0444836616516113, R2 0.6226179599761963\n",
      "epoch 885, loss 0.9912940859794617, R2 0.6222319602966309\n",
      "Eval loss 1.0439599752426147, R2 0.6227955222129822\n",
      "epoch 886, loss 0.9908100366592407, R2 0.6224019527435303\n",
      "Eval loss 1.0434378385543823, R2 0.6229726076126099\n",
      "epoch 887, loss 0.9903275966644287, R2 0.6225714683532715\n",
      "Eval loss 1.0429173707962036, R2 0.6231492757797241\n",
      "epoch 888, loss 0.9898468255996704, R2 0.6227404475212097\n",
      "Eval loss 1.042398452758789, R2 0.6233252882957458\n",
      "epoch 889, loss 0.9893674850463867, R2 0.6229089498519897\n",
      "Eval loss 1.0418812036514282, R2 0.6235008239746094\n",
      "epoch 890, loss 0.9888896942138672, R2 0.6230769157409668\n",
      "Eval loss 1.041365385055542, R2 0.6236759424209595\n",
      "epoch 891, loss 0.9884133338928223, R2 0.6232444643974304\n",
      "Eval loss 1.040851354598999, R2 0.6238505840301514\n",
      "epoch 892, loss 0.9879387021064758, R2 0.6234114170074463\n",
      "Eval loss 1.0403387546539307, R2 0.6240246891975403\n",
      "epoch 893, loss 0.987465500831604, R2 0.6235780119895935\n",
      "Eval loss 1.0398277044296265, R2 0.624198317527771\n",
      "epoch 894, loss 0.9869938492774963, R2 0.6237440705299377\n",
      "Eval loss 1.039318323135376, R2 0.6243715286254883\n",
      "epoch 895, loss 0.9865235090255737, R2 0.6239096522331238\n",
      "Eval loss 1.0388103723526, R2 0.6245442032814026\n",
      "epoch 896, loss 0.9860548973083496, R2 0.6240748167037964\n",
      "Eval loss 1.038304090499878, R2 0.6247164011001587\n",
      "epoch 897, loss 0.9855876564979553, R2 0.624239444732666\n",
      "Eval loss 1.0377994775772095, R2 0.6248880624771118\n",
      "epoch 898, loss 0.9851219058036804, R2 0.6244035959243774\n",
      "Eval loss 1.037296175956726, R2 0.6250593662261963\n",
      "epoch 899, loss 0.9846578240394592, R2 0.6245673894882202\n",
      "Eval loss 1.0367945432662964, R2 0.6252301335334778\n",
      "epoch 900, loss 0.9841950535774231, R2 0.6247305870056152\n",
      "Eval loss 1.0362943410873413, R2 0.6254004240036011\n",
      "epoch 901, loss 0.9837337136268616, R2 0.6248933672904968\n",
      "Eval loss 1.03579580783844, R2 0.6255702376365662\n",
      "epoch 902, loss 0.983273983001709, R2 0.6250556111335754\n",
      "Eval loss 1.0352987051010132, R2 0.625739574432373\n",
      "epoch 903, loss 0.9828157424926758, R2 0.6252174377441406\n",
      "Eval loss 1.0348031520843506, R2 0.6259084343910217\n",
      "epoch 904, loss 0.9823588132858276, R2 0.6253787875175476\n",
      "Eval loss 1.0343090295791626, R2 0.626076877117157\n",
      "epoch 905, loss 0.9819034337997437, R2 0.6255397200584412\n",
      "Eval loss 1.0338164567947388, R2 0.626244843006134\n",
      "epoch 906, loss 0.9814494848251343, R2 0.6257001757621765\n",
      "Eval loss 1.033325433731079, R2 0.6264123320579529\n",
      "epoch 907, loss 0.9809969067573547, R2 0.6258601546287537\n",
      "Eval loss 1.032835841178894, R2 0.6265792846679688\n",
      "epoch 908, loss 0.9805458784103394, R2 0.6260196566581726\n",
      "Eval loss 1.0323477983474731, R2 0.626745879650116\n",
      "epoch 909, loss 0.9800962805747986, R2 0.6261786818504333\n",
      "Eval loss 1.0318611860275269, R2 0.626911997795105\n",
      "epoch 910, loss 0.9796480536460876, R2 0.6263372898101807\n",
      "Eval loss 1.0313761234283447, R2 0.6270776391029358\n",
      "epoch 911, loss 0.9792013168334961, R2 0.6264954805374146\n",
      "Eval loss 1.0308924913406372, R2 0.6272428631782532\n",
      "epoch 912, loss 0.9787560105323792, R2 0.6266531944274902\n",
      "Eval loss 1.0304102897644043, R2 0.6274074912071228\n",
      "epoch 913, loss 0.9783119559288025, R2 0.6268104314804077\n",
      "Eval loss 1.029929518699646, R2 0.6275717616081238\n",
      "epoch 914, loss 0.9778695702552795, R2 0.626967191696167\n",
      "Eval loss 1.0294504165649414, R2 0.6277356147766113\n",
      "epoch 915, loss 0.9774283170700073, R2 0.6271235942840576\n",
      "Eval loss 1.0289725065231323, R2 0.6278989911079407\n",
      "epoch 916, loss 0.9769885540008545, R2 0.6272794604301453\n",
      "Eval loss 1.028496265411377, R2 0.6280618906021118\n",
      "epoch 917, loss 0.9765503406524658, R2 0.6274349093437195\n",
      "Eval loss 1.0280213356018066, R2 0.6282243132591248\n",
      "epoch 918, loss 0.9761133790016174, R2 0.627590000629425\n",
      "Eval loss 1.0275479555130005, R2 0.6283863186836243\n",
      "epoch 919, loss 0.9756777882575989, R2 0.6277444958686829\n",
      "Eval loss 1.0270758867263794, R2 0.6285479068756104\n",
      "epoch 920, loss 0.9752435684204102, R2 0.6278985738754272\n",
      "Eval loss 1.0266053676605225, R2 0.6287090182304382\n",
      "epoch 921, loss 0.9748108386993408, R2 0.6280523538589478\n",
      "Eval loss 1.0261362791061401, R2 0.6288697123527527\n",
      "epoch 922, loss 0.9743793606758118, R2 0.6282055974006653\n",
      "Eval loss 1.0256683826446533, R2 0.6290299892425537\n",
      "epoch 923, loss 0.9739493131637573, R2 0.6283584237098694\n",
      "Eval loss 1.0252022743225098, R2 0.6291897892951965\n",
      "epoch 924, loss 0.9735206365585327, R2 0.6285108327865601\n",
      "Eval loss 1.0247372388839722, R2 0.6293491721153259\n",
      "epoch 925, loss 0.9730932116508484, R2 0.6286627054214478\n",
      "Eval loss 1.0242737531661987, R2 0.6295080184936523\n",
      "epoch 926, loss 0.9726673364639282, R2 0.6288142800331116\n",
      "Eval loss 1.0238116979599, R2 0.6296665072441101\n",
      "epoch 927, loss 0.9722426533699036, R2 0.6289653182029724\n",
      "Eval loss 1.0233510732650757, R2 0.6298246383666992\n",
      "epoch 928, loss 0.9718192219734192, R2 0.6291160583496094\n",
      "Eval loss 1.022891640663147, R2 0.6299821734428406\n",
      "epoch 929, loss 0.9713971614837646, R2 0.6292662024497986\n",
      "Eval loss 1.0224336385726929, R2 0.6301394701004028\n",
      "epoch 930, loss 0.9709766507148743, R2 0.6294160485267639\n",
      "Eval loss 1.0219773054122925, R2 0.6302961707115173\n",
      "epoch 931, loss 0.9705573320388794, R2 0.629565417766571\n",
      "Eval loss 1.0215219259262085, R2 0.6304525136947632\n",
      "epoch 932, loss 0.9701393246650696, R2 0.6297143697738647\n",
      "Eval loss 1.0210683345794678, R2 0.6306083798408508\n",
      "epoch 933, loss 0.9697227478027344, R2 0.6298628449440002\n",
      "Eval loss 1.0206159353256226, R2 0.6307638883590698\n",
      "epoch 934, loss 0.9693072438240051, R2 0.6300109624862671\n",
      "Eval loss 1.0201647281646729, R2 0.6309189200401306\n",
      "epoch 935, loss 0.9688932299613953, R2 0.6301587224006653\n",
      "Eval loss 1.0197150707244873, R2 0.631073534488678\n",
      "epoch 936, loss 0.9684803485870361, R2 0.6303060054779053\n",
      "Eval loss 1.0192666053771973, R2 0.6312277317047119\n",
      "epoch 937, loss 0.9680689573287964, R2 0.6304528117179871\n",
      "Eval loss 1.0188196897506714, R2 0.6313815116882324\n",
      "epoch 938, loss 0.9676588177680969, R2 0.630599319934845\n",
      "Eval loss 1.0183740854263306, R2 0.6315348148345947\n",
      "epoch 939, loss 0.9672499895095825, R2 0.6307452321052551\n",
      "Eval loss 1.0179296731948853, R2 0.6316878199577332\n",
      "epoch 940, loss 0.9668422937393188, R2 0.6308909058570862\n",
      "Eval loss 1.0174866914749146, R2 0.6318403482437134\n",
      "epoch 941, loss 0.9664361476898193, R2 0.631036102771759\n",
      "Eval loss 1.017045021057129, R2 0.6319924592971802\n",
      "epoch 942, loss 0.9660310745239258, R2 0.6311808228492737\n",
      "Eval loss 1.0166047811508179, R2 0.6321441531181335\n",
      "epoch 943, loss 0.9656273722648621, R2 0.6313252449035645\n",
      "Eval loss 1.0161657333374023, R2 0.6322954297065735\n",
      "epoch 944, loss 0.9652247428894043, R2 0.631469190120697\n",
      "Eval loss 1.0157279968261719, R2 0.6324462294578552\n",
      "epoch 945, loss 0.9648236036300659, R2 0.6316128373146057\n",
      "Eval loss 1.0152915716171265, R2 0.6325967311859131\n",
      "epoch 946, loss 0.9644235968589783, R2 0.6317559480667114\n",
      "Eval loss 1.0148566961288452, R2 0.6327468156814575\n",
      "epoch 947, loss 0.9640249013900757, R2 0.6318987011909485\n",
      "Eval loss 1.0144230127334595, R2 0.6328964233398438\n",
      "epoch 948, loss 0.9636273384094238, R2 0.6320410370826721\n",
      "Eval loss 1.0139904022216797, R2 0.6330457329750061\n",
      "epoch 949, loss 0.9632312655448914, R2 0.6321830153465271\n",
      "Eval loss 1.0135592222213745, R2 0.6331945061683655\n",
      "epoch 950, loss 0.9628362059593201, R2 0.6323245763778687\n",
      "Eval loss 1.0131292343139648, R2 0.6333429217338562\n",
      "epoch 951, loss 0.9624423980712891, R2 0.632465660572052\n",
      "Eval loss 1.0127005577087402, R2 0.6334909200668335\n",
      "epoch 952, loss 0.9620499610900879, R2 0.6326064467430115\n",
      "Eval loss 1.0122733116149902, R2 0.6336386203765869\n",
      "epoch 953, loss 0.9616585969924927, R2 0.6327467560768127\n",
      "Eval loss 1.0118472576141357, R2 0.6337857842445374\n",
      "epoch 954, loss 0.9612685441970825, R2 0.6328868269920349\n",
      "Eval loss 1.0114226341247559, R2 0.6339326500892639\n",
      "epoch 955, loss 0.9608797430992126, R2 0.6330263614654541\n",
      "Eval loss 1.010999083518982, R2 0.634079098701477\n",
      "epoch 956, loss 0.9604920148849487, R2 0.6331655383110046\n",
      "Eval loss 1.0105767250061035, R2 0.634225070476532\n",
      "epoch 957, loss 0.9601057171821594, R2 0.6333042979240417\n",
      "Eval loss 1.0101559162139893, R2 0.634370744228363\n",
      "epoch 958, loss 0.9597203731536865, R2 0.6334426999092102\n",
      "Eval loss 1.0097360610961914, R2 0.6345158815383911\n",
      "epoch 959, loss 0.9593364596366882, R2 0.63358074426651\n",
      "Eval loss 1.0093176364898682, R2 0.6346607804298401\n",
      "epoch 960, loss 0.9589536786079407, R2 0.6337183117866516\n",
      "Eval loss 1.0089002847671509, R2 0.6348052024841309\n",
      "epoch 961, loss 0.9585720300674438, R2 0.6338555812835693\n",
      "Eval loss 1.0084843635559082, R2 0.6349493265151978\n",
      "epoch 962, loss 0.9581915736198425, R2 0.6339924335479736\n",
      "Eval loss 1.008069634437561, R2 0.6350929737091064\n",
      "epoch 963, loss 0.9578123092651367, R2 0.6341288685798645\n",
      "Eval loss 1.007656216621399, R2 0.6352362036705017\n",
      "epoch 964, loss 0.9574342370033264, R2 0.6342649459838867\n",
      "Eval loss 1.0072438716888428, R2 0.6353791356086731\n",
      "epoch 965, loss 0.9570574164390564, R2 0.6344007253646851\n",
      "Eval loss 1.0068327188491821, R2 0.6355217099189758\n",
      "epoch 966, loss 0.9566816091537476, R2 0.6345359683036804\n",
      "Eval loss 1.006422996520996, R2 0.6356638073921204\n",
      "epoch 967, loss 0.9563071131706238, R2 0.6346709132194519\n",
      "Eval loss 1.006014347076416, R2 0.635805606842041\n",
      "epoch 968, loss 0.955933690071106, R2 0.6348055005073547\n",
      "Eval loss 1.0056068897247314, R2 0.6359469294548035\n",
      "epoch 969, loss 0.9555615782737732, R2 0.6349396109580994\n",
      "Eval loss 1.005200743675232, R2 0.6360878944396973\n",
      "epoch 970, loss 0.9551904201507568, R2 0.6350734829902649\n",
      "Eval loss 1.004795789718628, R2 0.6362285017967224\n",
      "epoch 971, loss 0.9548205137252808, R2 0.6352069973945618\n",
      "Eval loss 1.0043919086456299, R2 0.6363687515258789\n",
      "epoch 972, loss 0.9544517397880554, R2 0.6353400349617004\n",
      "Eval loss 1.003989338874817, R2 0.6365085244178772\n",
      "epoch 973, loss 0.9540840983390808, R2 0.6354727149009705\n",
      "Eval loss 1.0035879611968994, R2 0.6366480588912964\n",
      "epoch 974, loss 0.9537177681922913, R2 0.635604977607727\n",
      "Eval loss 1.003187656402588, R2 0.6367871165275574\n",
      "epoch 975, loss 0.9533523917198181, R2 0.6357370018959045\n",
      "Eval loss 1.0027886629104614, R2 0.6369258165359497\n",
      "epoch 976, loss 0.9529882073402405, R2 0.6358685493469238\n",
      "Eval loss 1.0023908615112305, R2 0.6370642185211182\n",
      "epoch 977, loss 0.9526251554489136, R2 0.6359997987747192\n",
      "Eval loss 1.0019941329956055, R2 0.6372022032737732\n",
      "epoch 978, loss 0.9522630572319031, R2 0.636130690574646\n",
      "Eval loss 1.0015987157821655, R2 0.6373398303985596\n",
      "epoch 979, loss 0.9519022703170776, R2 0.6362611055374146\n",
      "Eval loss 1.0012043714523315, R2 0.6374770402908325\n",
      "epoch 980, loss 0.9515424966812134, R2 0.636391282081604\n",
      "Eval loss 1.0008111000061035, R2 0.6376139521598816\n",
      "epoch 981, loss 0.9511839747428894, R2 0.63652104139328\n",
      "Eval loss 1.0004191398620605, R2 0.6377503871917725\n",
      "epoch 982, loss 0.9508264660835266, R2 0.6366504430770874\n",
      "Eval loss 1.000028371810913, R2 0.6378865242004395\n",
      "epoch 983, loss 0.9504700899124146, R2 0.6367795467376709\n",
      "Eval loss 0.9996386170387268, R2 0.6380223035812378\n",
      "epoch 984, loss 0.9501148462295532, R2 0.6369081735610962\n",
      "Eval loss 0.9992501735687256, R2 0.6381577253341675\n",
      "epoch 985, loss 0.9497605562210083, R2 0.6370365023612976\n",
      "Eval loss 0.998862624168396, R2 0.6382927298545837\n",
      "epoch 986, loss 0.9494075179100037, R2 0.6371644735336304\n",
      "Eval loss 0.9984763860702515, R2 0.6384274959564209\n",
      "epoch 987, loss 0.9490554928779602, R2 0.6372921466827393\n",
      "Eval loss 0.9980912804603577, R2 0.6385618448257446\n",
      "epoch 988, loss 0.9487046003341675, R2 0.6374193429946899\n",
      "Eval loss 0.9977073073387146, R2 0.6386957168579102\n",
      "epoch 989, loss 0.9483547210693359, R2 0.6375463008880615\n",
      "Eval loss 0.9973244667053223, R2 0.6388294100761414\n",
      "epoch 990, loss 0.9480059742927551, R2 0.6376728415489197\n",
      "Eval loss 0.9969426989555359, R2 0.6389626264572144\n",
      "epoch 991, loss 0.9476582407951355, R2 0.6377990245819092\n",
      "Eval loss 0.996562123298645, R2 0.6390955448150635\n",
      "epoch 992, loss 0.9473115801811218, R2 0.63792484998703\n",
      "Eval loss 0.9961826205253601, R2 0.6392280459403992\n",
      "epoch 993, loss 0.9469662308692932, R2 0.6380504369735718\n",
      "Eval loss 0.9958041310310364, R2 0.639360249042511\n",
      "epoch 994, loss 0.9466216564178467, R2 0.6381755471229553\n",
      "Eval loss 0.9954269528388977, R2 0.6394920945167542\n",
      "epoch 995, loss 0.9462781548500061, R2 0.638300359249115\n",
      "Eval loss 0.9950507879257202, R2 0.6396236419677734\n",
      "epoch 996, loss 0.945935845375061, R2 0.6384248733520508\n",
      "Eval loss 0.9946756958961487, R2 0.6397547125816345\n",
      "epoch 997, loss 0.9455944895744324, R2 0.6385489702224731\n",
      "Eval loss 0.9943017959594727, R2 0.6398854851722717\n",
      "epoch 998, loss 0.945254385471344, R2 0.6386726498603821\n",
      "Eval loss 0.9939289689064026, R2 0.6400159597396851\n",
      "epoch 999, loss 0.9449151754379272, R2 0.6387961506843567\n",
      "Eval loss 0.9935570359230042, R2 0.6401460766792297\n",
      "epoch 1000, loss 0.9445768594741821, R2 0.6389192342758179\n",
      "Eval loss 0.9931864142417908, R2 0.640275776386261\n",
      "epoch 1001, loss 0.9442398548126221, R2 0.6390419602394104\n",
      "Eval loss 0.9928167462348938, R2 0.6404052376747131\n",
      "epoch 1002, loss 0.9439036846160889, R2 0.6391644477844238\n",
      "Eval loss 0.9924483895301819, R2 0.6405342817306519\n",
      "epoch 1003, loss 0.9435685873031616, R2 0.6392865180969238\n",
      "Eval loss 0.9920808672904968, R2 0.6406629681587219\n",
      "epoch 1004, loss 0.9432345628738403, R2 0.6394082307815552\n",
      "Eval loss 0.991714596748352, R2 0.6407913565635681\n",
      "epoch 1005, loss 0.9429015517234802, R2 0.6395297050476074\n",
      "Eval loss 0.9913492798805237, R2 0.6409194469451904\n",
      "epoch 1006, loss 0.9425694942474365, R2 0.6396507620811462\n",
      "Eval loss 0.9909849166870117, R2 0.6410470604896545\n",
      "epoch 1007, loss 0.9422385692596436, R2 0.6397714614868164\n",
      "Eval loss 0.99062180519104, R2 0.6411744952201843\n",
      "epoch 1008, loss 0.941908597946167, R2 0.6398918628692627\n",
      "Eval loss 0.9902597069740295, R2 0.6413015127182007\n",
      "epoch 1009, loss 0.9415796399116516, R2 0.6400119662284851\n",
      "Eval loss 0.9898988008499146, R2 0.6414281725883484\n",
      "epoch 1010, loss 0.9412516355514526, R2 0.6401317715644836\n",
      "Eval loss 0.9895386695861816, R2 0.6415545344352722\n",
      "epoch 1011, loss 0.9409247040748596, R2 0.6402512192726135\n",
      "Eval loss 0.9891796708106995, R2 0.6416805386543274\n",
      "epoch 1012, loss 0.940598726272583, R2 0.6403701901435852\n",
      "Eval loss 0.9888218641281128, R2 0.6418062448501587\n",
      "epoch 1013, loss 0.9402738213539124, R2 0.6404889822006226\n",
      "Eval loss 0.9884650111198425, R2 0.6419315338134766\n",
      "epoch 1014, loss 0.9399498701095581, R2 0.6406074166297913\n",
      "Eval loss 0.9881091713905334, R2 0.6420566439628601\n",
      "epoch 1015, loss 0.9396268129348755, R2 0.6407254934310913\n",
      "Eval loss 0.9877544641494751, R2 0.6421813368797302\n",
      "epoch 1016, loss 0.9393048882484436, R2 0.6408433318138123\n",
      "Eval loss 0.9874005913734436, R2 0.6423056721687317\n",
      "epoch 1017, loss 0.9389838576316833, R2 0.6409607529640198\n",
      "Eval loss 0.9870479702949524, R2 0.6424296498298645\n",
      "epoch 1018, loss 0.938663899898529, R2 0.6410778760910034\n",
      "Eval loss 0.9866962432861328, R2 0.6425533890724182\n",
      "epoch 1019, loss 0.9383448362350464, R2 0.6411947011947632\n",
      "Eval loss 0.9863455891609192, R2 0.6426767110824585\n",
      "epoch 1020, loss 0.9380267262458801, R2 0.6413112282752991\n",
      "Eval loss 0.9859958291053772, R2 0.6427997946739197\n",
      "epoch 1021, loss 0.9377096891403198, R2 0.6414273977279663\n",
      "Eval loss 0.9856471419334412, R2 0.6429225206375122\n",
      "epoch 1022, loss 0.9373935461044312, R2 0.6415433287620544\n",
      "Eval loss 0.9852995872497559, R2 0.6430450081825256\n",
      "epoch 1023, loss 0.9370782375335693, R2 0.6416588425636292\n",
      "Eval loss 0.9849529266357422, R2 0.6431670784950256\n",
      "epoch 1024, loss 0.9367640018463135, R2 0.64177405834198\n",
      "Eval loss 0.9846073985099792, R2 0.643288791179657\n",
      "epoch 1025, loss 0.9364507794380188, R2 0.6418889760971069\n",
      "Eval loss 0.9842627048492432, R2 0.6434102058410645\n",
      "epoch 1026, loss 0.9361385107040405, R2 0.64200359582901\n",
      "Eval loss 0.9839191436767578, R2 0.6435313820838928\n",
      "epoch 1027, loss 0.9358271956443787, R2 0.6421178579330444\n",
      "Eval loss 0.9835764765739441, R2 0.6436522006988525\n",
      "epoch 1028, loss 0.9355167150497437, R2 0.6422318816184998\n",
      "Eval loss 0.9832348227500916, R2 0.6437726020812988\n",
      "epoch 1029, loss 0.9352073073387146, R2 0.6423454880714417\n",
      "Eval loss 0.982894241809845, R2 0.643892765045166\n",
      "epoch 1030, loss 0.9348987340927124, R2 0.6424589157104492\n",
      "Eval loss 0.9825544953346252, R2 0.6440126895904541\n",
      "epoch 1031, loss 0.9345909953117371, R2 0.6425718665122986\n",
      "Eval loss 0.9822158217430115, R2 0.6441321969032288\n",
      "epoch 1032, loss 0.934284508228302, R2 0.6426845788955688\n",
      "Eval loss 0.9818781614303589, R2 0.6442514657974243\n",
      "epoch 1033, loss 0.9339787364006042, R2 0.64279705286026\n",
      "Eval loss 0.9815413951873779, R2 0.6443703174591064\n",
      "epoch 1034, loss 0.9336738586425781, R2 0.6429091095924377\n",
      "Eval loss 0.9812056422233582, R2 0.6444889903068542\n",
      "epoch 1035, loss 0.9333699941635132, R2 0.6430209279060364\n",
      "Eval loss 0.9808708429336548, R2 0.6446073055267334\n",
      "epoch 1036, loss 0.9330670237541199, R2 0.6431325078010559\n",
      "Eval loss 0.9805370569229126, R2 0.6447252631187439\n",
      "epoch 1037, loss 0.932765007019043, R2 0.6432437300682068\n",
      "Eval loss 0.9802042245864868, R2 0.6448429226875305\n",
      "epoch 1038, loss 0.9324638247489929, R2 0.6433545351028442\n",
      "Eval loss 0.9798722863197327, R2 0.644960343837738\n",
      "epoch 1039, loss 0.9321637153625488, R2 0.6434651613235474\n",
      "Eval loss 0.9795412421226501, R2 0.6450773477554321\n",
      "epoch 1040, loss 0.9318642616271973, R2 0.6435754299163818\n",
      "Eval loss 0.9792113304138184, R2 0.6451941132545471\n",
      "epoch 1041, loss 0.9315659403800964, R2 0.643685519695282\n",
      "Eval loss 0.9788822531700134, R2 0.6453105807304382\n",
      "epoch 1042, loss 0.9312684535980225, R2 0.6437951922416687\n",
      "Eval loss 0.9785541296005249, R2 0.6454267501831055\n",
      "epoch 1043, loss 0.9309718608856201, R2 0.6439046263694763\n",
      "Eval loss 0.9782270193099976, R2 0.645542562007904\n",
      "epoch 1044, loss 0.9306761622428894, R2 0.6440137624740601\n",
      "Eval loss 0.9779008030891418, R2 0.6456580758094788\n",
      "epoch 1045, loss 0.9303812384605408, R2 0.6441225409507751\n",
      "Eval loss 0.9775754809379578, R2 0.6457733511924744\n",
      "epoch 1046, loss 0.9300873875617981, R2 0.6442310214042664\n",
      "Eval loss 0.9772511720657349, R2 0.6458882689476013\n",
      "epoch 1047, loss 0.9297943115234375, R2 0.6443392634391785\n",
      "Eval loss 0.9769278764724731, R2 0.6460029482841492\n",
      "epoch 1048, loss 0.9295021891593933, R2 0.6444472670555115\n",
      "Eval loss 0.9766053557395935, R2 0.6461173295974731\n",
      "epoch 1049, loss 0.9292109608650208, R2 0.6445547342300415\n",
      "Eval loss 0.9762837290763855, R2 0.6462313532829285\n",
      "epoch 1050, loss 0.9289204478263855, R2 0.6446621417999268\n",
      "Eval loss 0.9759631156921387, R2 0.6463450789451599\n",
      "epoch 1051, loss 0.9286310076713562, R2 0.6447691917419434\n",
      "Eval loss 0.9756434559822083, R2 0.6464585065841675\n",
      "epoch 1052, loss 0.9283422827720642, R2 0.6448760032653809\n",
      "Eval loss 0.9753245115280151, R2 0.646571695804596\n",
      "epoch 1053, loss 0.9280545115470886, R2 0.6449824571609497\n",
      "Eval loss 0.9750065207481384, R2 0.6466845870018005\n",
      "epoch 1054, loss 0.9277676343917847, R2 0.6450886130332947\n",
      "Eval loss 0.9746896624565125, R2 0.6467971205711365\n",
      "epoch 1055, loss 0.927481472492218, R2 0.6451945304870605\n",
      "Eval loss 0.9743736386299133, R2 0.6469094157218933\n",
      "epoch 1056, loss 0.9271963834762573, R2 0.6453001499176025\n",
      "Eval loss 0.9740585088729858, R2 0.647021472454071\n",
      "epoch 1057, loss 0.9269120693206787, R2 0.6454054713249207\n",
      "Eval loss 0.9737442135810852, R2 0.6471331119537354\n",
      "epoch 1058, loss 0.9266285300254822, R2 0.6455104947090149\n",
      "Eval loss 0.9734307527542114, R2 0.6472445130348206\n",
      "epoch 1059, loss 0.926345944404602, R2 0.6456152200698853\n",
      "Eval loss 0.9731183648109436, R2 0.6473556160926819\n",
      "epoch 1060, loss 0.926064133644104, R2 0.6457197070121765\n",
      "Eval loss 0.9728066921234131, R2 0.6474664807319641\n",
      "epoch 1061, loss 0.9257832765579224, R2 0.6458239555358887\n",
      "Eval loss 0.972495973110199, R2 0.6475770473480225\n",
      "epoch 1062, loss 0.9255030751228333, R2 0.6459278464317322\n",
      "Eval loss 0.9721862077713013, R2 0.6476872563362122\n",
      "epoch 1063, loss 0.9252238273620605, R2 0.6460314989089966\n",
      "Eval loss 0.9718772768974304, R2 0.6477972269058228\n",
      "epoch 1064, loss 0.9249454736709595, R2 0.6461348533630371\n",
      "Eval loss 0.9715692400932312, R2 0.6479068994522095\n",
      "epoch 1065, loss 0.9246678948402405, R2 0.6462379097938538\n",
      "Eval loss 0.9712619781494141, R2 0.6480163335800171\n",
      "epoch 1066, loss 0.9243910908699036, R2 0.6463406682014465\n",
      "Eval loss 0.9709557890892029, R2 0.6481254696846008\n",
      "epoch 1067, loss 0.9241151809692383, R2 0.6464431881904602\n",
      "Eval loss 0.9706502556800842, R2 0.6482343673706055\n",
      "epoch 1068, loss 0.9238401055335999, R2 0.6465454697608948\n",
      "Eval loss 0.9703457355499268, R2 0.6483428478240967\n",
      "epoch 1069, loss 0.9235658645629883, R2 0.6466473937034607\n",
      "Eval loss 0.9700419902801514, R2 0.6484511494636536\n",
      "epoch 1070, loss 0.923292338848114, R2 0.6467490792274475\n",
      "Eval loss 0.9697392582893372, R2 0.6485591530799866\n",
      "epoch 1071, loss 0.9230198264122009, R2 0.6468505859375\n",
      "Eval loss 0.969437301158905, R2 0.6486669182777405\n",
      "epoch 1072, loss 0.922747790813446, R2 0.6469516754150391\n",
      "Eval loss 0.9691361784934998, R2 0.6487743258476257\n",
      "epoch 1073, loss 0.9224769473075867, R2 0.6470525860786438\n",
      "Eval loss 0.9688359498977661, R2 0.6488814949989319\n",
      "epoch 1074, loss 0.9222066402435303, R2 0.6471531987190247\n",
      "Eval loss 0.9685364365577698, R2 0.6489883661270142\n",
      "epoch 1075, loss 0.9219373464584351, R2 0.6472535133361816\n",
      "Eval loss 0.9682379364967346, R2 0.6490949988365173\n",
      "epoch 1076, loss 0.9216685891151428, R2 0.6473536491394043\n",
      "Eval loss 0.9679402112960815, R2 0.6492013335227966\n",
      "epoch 1077, loss 0.9214009642601013, R2 0.6474533677101135\n",
      "Eval loss 0.9676433205604553, R2 0.649307370185852\n",
      "epoch 1078, loss 0.9211338758468628, R2 0.6475529074668884\n",
      "Eval loss 0.967347264289856, R2 0.6494132280349731\n",
      "epoch 1079, loss 0.9208677411079407, R2 0.6476522088050842\n",
      "Eval loss 0.967052161693573, R2 0.6495187282562256\n",
      "epoch 1080, loss 0.9206023812294006, R2 0.6477512121200562\n",
      "Eval loss 0.9667577147483826, R2 0.6496240496635437\n",
      "epoch 1081, loss 0.9203377366065979, R2 0.647849977016449\n",
      "Eval loss 0.9664642214775085, R2 0.6497290134429932\n",
      "epoch 1082, loss 0.9200738668441772, R2 0.6479483842849731\n",
      "Eval loss 0.9661715030670166, R2 0.6498336791992188\n",
      "epoch 1083, loss 0.9198108911514282, R2 0.6480465531349182\n",
      "Eval loss 0.9658797383308411, R2 0.64993816614151\n",
      "epoch 1084, loss 0.9195486307144165, R2 0.648144543170929\n",
      "Eval loss 0.9655886888504028, R2 0.6500422954559326\n",
      "epoch 1085, loss 0.9192870855331421, R2 0.6482422351837158\n",
      "Eval loss 0.9652985334396362, R2 0.6501462459564209\n",
      "epoch 1086, loss 0.9190263152122498, R2 0.6483396291732788\n",
      "Eval loss 0.9650089740753174, R2 0.6502498388290405\n",
      "epoch 1087, loss 0.918766438961029, R2 0.6484367847442627\n",
      "Eval loss 0.9647204875946045, R2 0.6503532528877258\n",
      "epoch 1088, loss 0.9185072779655457, R2 0.6485337018966675\n",
      "Eval loss 0.9644326567649841, R2 0.6504563689231873\n",
      "epoch 1089, loss 0.9182490110397339, R2 0.6486303210258484\n",
      "Eval loss 0.9641457200050354, R2 0.6505591869354248\n",
      "epoch 1090, loss 0.9179912805557251, R2 0.648726761341095\n",
      "Eval loss 0.9638595581054688, R2 0.6506617665290833\n",
      "epoch 1091, loss 0.9177343845367432, R2 0.6488228440284729\n",
      "Eval loss 0.9635742902755737, R2 0.6507641077041626\n",
      "epoch 1092, loss 0.9174783825874329, R2 0.6489187479019165\n",
      "Eval loss 0.9632896780967712, R2 0.6508661508560181\n",
      "epoch 1093, loss 0.9172230362892151, R2 0.6490143537521362\n",
      "Eval loss 0.9630059003829956, R2 0.6509680151939392\n",
      "epoch 1094, loss 0.9169684052467346, R2 0.6491096615791321\n",
      "Eval loss 0.9627229571342468, R2 0.6510695815086365\n",
      "epoch 1095, loss 0.9167146682739258, R2 0.6492047905921936\n",
      "Eval loss 0.9624407887458801, R2 0.6511707901954651\n",
      "epoch 1096, loss 0.9164615869522095, R2 0.6492996215820312\n",
      "Eval loss 0.9621595144271851, R2 0.6512718796730042\n",
      "epoch 1097, loss 0.9162092804908752, R2 0.6493942141532898\n",
      "Eval loss 0.9618788957595825, R2 0.6513726711273193\n",
      "epoch 1098, loss 0.9159576892852783, R2 0.6494886875152588\n",
      "Eval loss 0.9615991115570068, R2 0.6514731645584106\n",
      "epoch 1099, loss 0.9157067537307739, R2 0.6495826840400696\n",
      "Eval loss 0.9613201022148132, R2 0.6515734195709229\n",
      "epoch 1100, loss 0.9154567718505859, R2 0.6496766209602356\n",
      "Eval loss 0.9610419869422913, R2 0.6516733765602112\n",
      "epoch 1101, loss 0.9152075052261353, R2 0.6497701406478882\n",
      "Eval loss 0.9607645273208618, R2 0.6517730951309204\n",
      "epoch 1102, loss 0.9149587750434875, R2 0.6498635411262512\n",
      "Eval loss 0.9604879021644592, R2 0.6518725752830505\n",
      "epoch 1103, loss 0.9147108197212219, R2 0.6499566435813904\n",
      "Eval loss 0.9602120518684387, R2 0.6519718766212463\n",
      "epoch 1104, loss 0.9144637584686279, R2 0.6500494480133057\n",
      "Eval loss 0.9599369764328003, R2 0.6520708799362183\n",
      "epoch 1105, loss 0.9142172932624817, R2 0.6501421332359314\n",
      "Eval loss 0.9596626162528992, R2 0.6521695852279663\n",
      "epoch 1106, loss 0.9139716625213623, R2 0.6502344608306885\n",
      "Eval loss 0.9593889713287354, R2 0.6522680521011353\n",
      "epoch 1107, loss 0.9137266278266907, R2 0.6503266096115112\n",
      "Eval loss 0.9591161608695984, R2 0.6523663401603699\n",
      "epoch 1108, loss 0.9134824872016907, R2 0.6504184603691101\n",
      "Eval loss 0.9588441848754883, R2 0.6524643301963806\n",
      "epoch 1109, loss 0.9132389426231384, R2 0.6505100131034851\n",
      "Eval loss 0.9585729241371155, R2 0.6525620222091675\n",
      "epoch 1110, loss 0.9129961729049683, R2 0.6506015062332153\n",
      "Eval loss 0.9583024382591248, R2 0.65265953540802\n",
      "epoch 1111, loss 0.9127539992332458, R2 0.6506925821304321\n",
      "Eval loss 0.9580326080322266, R2 0.6527567505836487\n",
      "epoch 1112, loss 0.9125127196311951, R2 0.6507834792137146\n",
      "Eval loss 0.9577636122703552, R2 0.652853786945343\n",
      "epoch 1113, loss 0.9122719764709473, R2 0.6508741974830627\n",
      "Eval loss 0.957495391368866, R2 0.6529505252838135\n",
      "epoch 1114, loss 0.9120319485664368, R2 0.650964617729187\n",
      "Eval loss 0.9572278261184692, R2 0.6530470252037048\n",
      "epoch 1115, loss 0.9117926955223083, R2 0.6510547995567322\n",
      "Eval loss 0.9569610357284546, R2 0.6531432867050171\n",
      "epoch 1116, loss 0.911554217338562, R2 0.6511447429656982\n",
      "Eval loss 0.9566949605941772, R2 0.6532393097877502\n",
      "epoch 1117, loss 0.9113163352012634, R2 0.6512344479560852\n",
      "Eval loss 0.956429660320282, R2 0.6533351540565491\n",
      "epoch 1118, loss 0.9110792279243469, R2 0.6513239145278931\n",
      "Eval loss 0.9561651349067688, R2 0.6534306406974792\n",
      "epoch 1119, loss 0.9108426570892334, R2 0.6514131426811218\n",
      "Eval loss 0.9559013247489929, R2 0.6535259485244751\n",
      "epoch 1120, loss 0.9106070399284363, R2 0.6515021324157715\n",
      "Eval loss 0.9556383490562439, R2 0.6536210179328918\n",
      "epoch 1121, loss 0.9103718996047974, R2 0.6515909433364868\n",
      "Eval loss 0.9553760290145874, R2 0.6537157893180847\n",
      "epoch 1122, loss 0.9101375341415405, R2 0.651679515838623\n",
      "Eval loss 0.9551143646240234, R2 0.6538103818893433\n",
      "epoch 1123, loss 0.9099038243293762, R2 0.6517677903175354\n",
      "Eval loss 0.9548534750938416, R2 0.6539047360420227\n",
      "epoch 1124, loss 0.9096707701683044, R2 0.6518558263778687\n",
      "Eval loss 0.954593300819397, R2 0.6539987921714783\n",
      "epoch 1125, loss 0.9094384908676147, R2 0.6519437432289124\n",
      "Eval loss 0.9543337821960449, R2 0.6540926694869995\n",
      "epoch 1126, loss 0.9092068672180176, R2 0.6520312428474426\n",
      "Eval loss 0.9540752172470093, R2 0.6541862487792969\n",
      "epoch 1127, loss 0.9089758396148682, R2 0.6521186232566833\n",
      "Eval loss 0.9538172483444214, R2 0.6542796492576599\n",
      "epoch 1128, loss 0.908745527267456, R2 0.652205765247345\n",
      "Eval loss 0.953559935092926, R2 0.6543728113174438\n",
      "epoch 1129, loss 0.908515989780426, R2 0.6522926688194275\n",
      "Eval loss 0.9533032178878784, R2 0.6544656753540039\n",
      "epoch 1130, loss 0.9082870483398438, R2 0.6523793935775757\n",
      "Eval loss 0.9530473947525024, R2 0.6545584201812744\n",
      "epoch 1131, loss 0.9080587029457092, R2 0.6524658799171448\n",
      "Eval loss 0.9527923464775085, R2 0.6546508073806763\n",
      "epoch 1132, loss 0.9078310132026672, R2 0.6525521278381348\n",
      "Eval loss 0.9525377750396729, R2 0.6547431349754333\n",
      "epoch 1133, loss 0.9076040983200073, R2 0.6526380777359009\n",
      "Eval loss 0.9522841572761536, R2 0.654835045337677\n",
      "epoch 1134, loss 0.9073778986930847, R2 0.6527238488197327\n",
      "Eval loss 0.9520310759544373, R2 0.6549268364906311\n",
      "epoch 1135, loss 0.9071522355079651, R2 0.6528093814849854\n",
      "Eval loss 0.9517785906791687, R2 0.6550183892250061\n",
      "epoch 1136, loss 0.9069273471832275, R2 0.6528947949409485\n",
      "Eval loss 0.9515270590782166, R2 0.6551096439361572\n",
      "epoch 1137, loss 0.9067029356956482, R2 0.652979850769043\n",
      "Eval loss 0.9512760639190674, R2 0.655200719833374\n",
      "epoch 1138, loss 0.9064792990684509, R2 0.6530647873878479\n",
      "Eval loss 0.9510257840156555, R2 0.6552915573120117\n",
      "epoch 1139, loss 0.9062562584877014, R2 0.653149425983429\n",
      "Eval loss 0.9507763385772705, R2 0.6553821563720703\n",
      "epoch 1140, loss 0.906033992767334, R2 0.6532338261604309\n",
      "Eval loss 0.9505274295806885, R2 0.6554725766181946\n",
      "epoch 1141, loss 0.90581214427948, R2 0.6533181071281433\n",
      "Eval loss 0.950279176235199, R2 0.6555626392364502\n",
      "epoch 1142, loss 0.9055911898612976, R2 0.6534020900726318\n",
      "Eval loss 0.9500316381454468, R2 0.655652642250061\n",
      "epoch 1143, loss 0.9053707122802734, R2 0.6534858345985413\n",
      "Eval loss 0.9497848749160767, R2 0.6557424068450928\n",
      "epoch 1144, loss 0.9051510095596313, R2 0.6535694003105164\n",
      "Eval loss 0.9495387077331543, R2 0.6558318138122559\n",
      "epoch 1145, loss 0.9049316644668579, R2 0.6536527872085571\n",
      "Eval loss 0.9492932558059692, R2 0.6559211015701294\n",
      "epoch 1146, loss 0.9047132730484009, R2 0.653735876083374\n",
      "Eval loss 0.9490483999252319, R2 0.656010091304779\n",
      "epoch 1147, loss 0.9044954776763916, R2 0.6538188457489014\n",
      "Eval loss 0.9488043189048767, R2 0.6560989022254944\n",
      "epoch 1148, loss 0.9042781591415405, R2 0.6539014577865601\n",
      "Eval loss 0.948560893535614, R2 0.6561875343322754\n",
      "epoch 1149, loss 0.904061496257782, R2 0.6539838910102844\n",
      "Eval loss 0.9483180642127991, R2 0.6562758684158325\n",
      "epoch 1150, loss 0.9038455486297607, R2 0.6540662050247192\n",
      "Eval loss 0.9480759501457214, R2 0.6563640236854553\n",
      "epoch 1151, loss 0.903630256652832, R2 0.6541482210159302\n",
      "Eval loss 0.9478344917297363, R2 0.6564520001411438\n",
      "epoch 1152, loss 0.9034155011177063, R2 0.6542300581932068\n",
      "Eval loss 0.9475937485694885, R2 0.6565396785736084\n",
      "epoch 1153, loss 0.9032014012336731, R2 0.6543116569519043\n",
      "Eval loss 0.9473534822463989, R2 0.6566271781921387\n",
      "epoch 1154, loss 0.9029878377914429, R2 0.6543931365013123\n",
      "Eval loss 0.947114109992981, R2 0.6567144393920898\n",
      "epoch 1155, loss 0.9027750492095947, R2 0.6544743180274963\n",
      "Eval loss 0.9468753337860107, R2 0.6568015217781067\n",
      "epoch 1156, loss 0.9025627970695496, R2 0.6545552611351013\n",
      "Eval loss 0.946636974811554, R2 0.6568883657455444\n",
      "epoch 1157, loss 0.9023510813713074, R2 0.654636025428772\n",
      "Eval loss 0.9463995099067688, R2 0.6569749712944031\n",
      "epoch 1158, loss 0.9021400809288025, R2 0.6547165513038635\n",
      "Eval loss 0.9461628198623657, R2 0.6570614576339722\n",
      "epoch 1159, loss 0.9019296765327454, R2 0.6547969579696655\n",
      "Eval loss 0.9459264278411865, R2 0.6571475863456726\n",
      "epoch 1160, loss 0.901719868183136, R2 0.6548770070075989\n",
      "Eval loss 0.945690929889679, R2 0.6572335958480835\n",
      "epoch 1161, loss 0.9015106558799744, R2 0.6549569964408875\n",
      "Eval loss 0.9454560279846191, R2 0.6573193669319153\n",
      "epoch 1162, loss 0.9013020396232605, R2 0.6550367474555969\n",
      "Eval loss 0.9452217221260071, R2 0.657404899597168\n",
      "epoch 1163, loss 0.9010941386222839, R2 0.6551162600517273\n",
      "Eval loss 0.9449880719184875, R2 0.6574902534484863\n",
      "epoch 1164, loss 0.9008868336677551, R2 0.6551955938339233\n",
      "Eval loss 0.9447550177574158, R2 0.6575753688812256\n",
      "epoch 1165, loss 0.900679886341095, R2 0.6552747488021851\n",
      "Eval loss 0.9445227384567261, R2 0.6576603651046753\n",
      "epoch 1166, loss 0.9004736542701721, R2 0.6553536057472229\n",
      "Eval loss 0.9442910552024841, R2 0.6577450037002563\n",
      "epoch 1167, loss 0.9002681970596313, R2 0.6554322838783264\n",
      "Eval loss 0.9440599679946899, R2 0.6578295230865479\n",
      "epoch 1168, loss 0.9000630378723145, R2 0.6555107235908508\n",
      "Eval loss 0.9438294172286987, R2 0.6579138040542603\n",
      "epoch 1169, loss 0.8998586535453796, R2 0.6555891036987305\n",
      "Eval loss 0.9435995817184448, R2 0.6579979062080383\n",
      "epoch 1170, loss 0.8996548056602478, R2 0.6556671261787415\n",
      "Eval loss 0.9433702826499939, R2 0.6580817699432373\n",
      "epoch 1171, loss 0.899451494216919, R2 0.6557450294494629\n",
      "Eval loss 0.9431416988372803, R2 0.658165454864502\n",
      "epoch 1172, loss 0.8992488980293274, R2 0.65582275390625\n",
      "Eval loss 0.9429137110710144, R2 0.6582489609718323\n",
      "epoch 1173, loss 0.8990468978881836, R2 0.6559001803398132\n",
      "Eval loss 0.9426864385604858, R2 0.6583322286605835\n",
      "epoch 1174, loss 0.8988453149795532, R2 0.6559774875640869\n",
      "Eval loss 0.9424596428871155, R2 0.6584152579307556\n",
      "epoch 1175, loss 0.8986444473266602, R2 0.6560545563697815\n",
      "Eval loss 0.9422335624694824, R2 0.6584980487823486\n",
      "epoch 1176, loss 0.8984440565109253, R2 0.6561314463615417\n",
      "Eval loss 0.9420080184936523, R2 0.6585807204246521\n",
      "epoch 1177, loss 0.8982443809509277, R2 0.6562080979347229\n",
      "Eval loss 0.94178307056427, R2 0.6586632132530212\n",
      "epoch 1178, loss 0.8980451226234436, R2 0.6562846302986145\n",
      "Eval loss 0.941558837890625, R2 0.6587454080581665\n",
      "epoch 1179, loss 0.8978464603424072, R2 0.6563608646392822\n",
      "Eval loss 0.9413352012634277, R2 0.6588274240493774\n",
      "epoch 1180, loss 0.8976484537124634, R2 0.6564369797706604\n",
      "Eval loss 0.9411121010780334, R2 0.658909261226654\n",
      "epoch 1181, loss 0.8974509239196777, R2 0.6565129160881042\n",
      "Eval loss 0.9408895373344421, R2 0.6589908599853516\n",
      "epoch 1182, loss 0.8972539901733398, R2 0.6565886735916138\n",
      "Eval loss 0.9406676888465881, R2 0.6590723395347595\n",
      "epoch 1183, loss 0.8970577120780945, R2 0.6566641330718994\n",
      "Eval loss 0.9404464960098267, R2 0.6591535806655884\n",
      "epoch 1184, loss 0.8968618512153625, R2 0.6567394137382507\n",
      "Eval loss 0.9402258396148682, R2 0.6592345237731934\n",
      "epoch 1185, loss 0.8966665863990784, R2 0.6568145155906677\n",
      "Eval loss 0.9400058388710022, R2 0.6593154072761536\n",
      "epoch 1186, loss 0.8964719176292419, R2 0.6568893790245056\n",
      "Eval loss 0.9397862553596497, R2 0.6593959927558899\n",
      "epoch 1187, loss 0.8962777853012085, R2 0.6569641828536987\n",
      "Eval loss 0.9395673871040344, R2 0.6594764590263367\n",
      "epoch 1188, loss 0.8960843086242676, R2 0.657038688659668\n",
      "Eval loss 0.9393491148948669, R2 0.6595567464828491\n",
      "epoch 1189, loss 0.8958912491798401, R2 0.6571130752563477\n",
      "Eval loss 0.9391313791275024, R2 0.6596367359161377\n",
      "epoch 1190, loss 0.8956987857818604, R2 0.657187283039093\n",
      "Eval loss 0.9389142394065857, R2 0.6597166061401367\n",
      "epoch 1191, loss 0.8955069184303284, R2 0.6572611331939697\n",
      "Eval loss 0.9386976957321167, R2 0.6597962379455566\n",
      "epoch 1192, loss 0.8953155875205994, R2 0.6573349833488464\n",
      "Eval loss 0.9384818077087402, R2 0.6598756313323975\n",
      "epoch 1193, loss 0.8951247334480286, R2 0.657408595085144\n",
      "Eval loss 0.9382665157318115, R2 0.6599549055099487\n",
      "epoch 1194, loss 0.8949344754219055, R2 0.6574819684028625\n",
      "Eval loss 0.9380515813827515, R2 0.6600340604782104\n",
      "epoch 1195, loss 0.8947447538375854, R2 0.6575551629066467\n",
      "Eval loss 0.9378376007080078, R2 0.6601128578186035\n",
      "epoch 1196, loss 0.8945555686950684, R2 0.6576281785964966\n",
      "Eval loss 0.937623918056488, R2 0.660191535949707\n",
      "epoch 1197, loss 0.8943669199943542, R2 0.6577010154724121\n",
      "Eval loss 0.9374107718467712, R2 0.660270094871521\n",
      "epoch 1198, loss 0.8941788077354431, R2 0.6577736735343933\n",
      "Eval loss 0.9371984004974365, R2 0.6603482961654663\n",
      "epoch 1199, loss 0.8939912915229797, R2 0.6578461527824402\n",
      "Eval loss 0.9369863867759705, R2 0.6604264378547668\n",
      "epoch 1200, loss 0.8938042521476746, R2 0.6579183340072632\n",
      "Eval loss 0.9367750883102417, R2 0.6605042815208435\n",
      "epoch 1201, loss 0.8936177492141724, R2 0.6579905152320862\n",
      "Eval loss 0.9365643262863159, R2 0.6605820655822754\n",
      "epoch 1202, loss 0.8934317827224731, R2 0.6580623388290405\n",
      "Eval loss 0.9363541007041931, R2 0.6606595516204834\n",
      "epoch 1203, loss 0.8932462930679321, R2 0.6581341028213501\n",
      "Eval loss 0.9361444115638733, R2 0.6607369184494019\n",
      "epoch 1204, loss 0.8930613994598389, R2 0.6582056283950806\n",
      "Eval loss 0.9359354376792908, R2 0.6608140468597412\n",
      "epoch 1205, loss 0.8928769826889038, R2 0.6582770943641663\n",
      "Eval loss 0.9357268214225769, R2 0.6608909964561462\n",
      "epoch 1206, loss 0.8926931619644165, R2 0.6583482027053833\n",
      "Eval loss 0.9355189204216003, R2 0.6609678268432617\n",
      "epoch 1207, loss 0.8925098180770874, R2 0.6584191918373108\n",
      "Eval loss 0.9353116154670715, R2 0.6610443592071533\n",
      "epoch 1208, loss 0.892326831817627, R2 0.658490002155304\n",
      "Eval loss 0.9351046681404114, R2 0.6611207723617554\n",
      "epoch 1209, loss 0.8921446204185486, R2 0.658560574054718\n",
      "Eval loss 0.9348984360694885, R2 0.6611970663070679\n",
      "epoch 1210, loss 0.8919627666473389, R2 0.6586310863494873\n",
      "Eval loss 0.9346925616264343, R2 0.6612730026245117\n",
      "epoch 1211, loss 0.8917816281318665, R2 0.6587013602256775\n",
      "Eval loss 0.9344874620437622, R2 0.6613489389419556\n",
      "epoch 1212, loss 0.8916008472442627, R2 0.6587714552879333\n",
      "Eval loss 0.9342828989028931, R2 0.6614245176315308\n",
      "epoch 1213, loss 0.8914206027984619, R2 0.6588413119316101\n",
      "Eval loss 0.9340786933898926, R2 0.661500096321106\n",
      "epoch 1214, loss 0.8912408351898193, R2 0.6589109897613525\n",
      "Eval loss 0.9338752031326294, R2 0.6615753173828125\n",
      "epoch 1215, loss 0.8910616040229797, R2 0.6589805483818054\n",
      "Eval loss 0.9336722493171692, R2 0.6616504788398743\n",
      "epoch 1216, loss 0.8908829092979431, R2 0.6590499877929688\n",
      "Eval loss 0.9334696531295776, R2 0.6617254018783569\n",
      "epoch 1217, loss 0.8907046914100647, R2 0.6591192483901978\n",
      "Eval loss 0.9332678318023682, R2 0.6618001461029053\n",
      "epoch 1218, loss 0.8905270099639893, R2 0.6591882705688477\n",
      "Eval loss 0.9330664277076721, R2 0.6618747115135193\n",
      "epoch 1219, loss 0.8903497457504272, R2 0.6592570543289185\n",
      "Eval loss 0.932865560054779, R2 0.6619491577148438\n",
      "epoch 1220, loss 0.8901729583740234, R2 0.6593257188796997\n",
      "Eval loss 0.932665228843689, R2 0.6620233058929443\n",
      "epoch 1221, loss 0.8899968266487122, R2 0.6593942046165466\n",
      "Eval loss 0.9324654936790466, R2 0.6620973348617554\n",
      "epoch 1222, loss 0.8898210525512695, R2 0.659462571144104\n",
      "Eval loss 0.9322661757469177, R2 0.6621711850166321\n",
      "epoch 1223, loss 0.8896458745002747, R2 0.6595306992530823\n",
      "Eval loss 0.9320675134658813, R2 0.6622448563575745\n",
      "epoch 1224, loss 0.8894711136817932, R2 0.659598708152771\n",
      "Eval loss 0.931869387626648, R2 0.6623183488845825\n",
      "epoch 1225, loss 0.8892968893051147, R2 0.6596665382385254\n",
      "Eval loss 0.9316716194152832, R2 0.6623916625976562\n",
      "epoch 1226, loss 0.889123260974884, R2 0.6597341299057007\n",
      "Eval loss 0.9314745664596558, R2 0.6624648571014404\n",
      "epoch 1227, loss 0.8889499306678772, R2 0.6598016023635864\n",
      "Eval loss 0.9312779903411865, R2 0.6625377535820007\n",
      "epoch 1228, loss 0.8887771368026733, R2 0.6598688364028931\n",
      "Eval loss 0.9310817718505859, R2 0.6626105904579163\n",
      "epoch 1229, loss 0.8886048197746277, R2 0.6599359512329102\n",
      "Eval loss 0.9308862686157227, R2 0.6626831889152527\n",
      "epoch 1230, loss 0.8884331583976746, R2 0.6600029468536377\n",
      "Eval loss 0.930691123008728, R2 0.6627556681632996\n",
      "epoch 1231, loss 0.8882617950439453, R2 0.6600697636604309\n",
      "Eval loss 0.9304967522621155, R2 0.6628278493881226\n",
      "epoch 1232, loss 0.8880910277366638, R2 0.660136342048645\n",
      "Eval loss 0.9303026795387268, R2 0.662899911403656\n",
      "epoch 1233, loss 0.887920618057251, R2 0.6602028012275696\n",
      "Eval loss 0.9301092028617859, R2 0.6629718542098999\n",
      "epoch 1234, loss 0.8877507448196411, R2 0.6602690815925598\n",
      "Eval loss 0.929916262626648, R2 0.6630435585975647\n",
      "epoch 1235, loss 0.8875814080238342, R2 0.6603351831436157\n",
      "Eval loss 0.9297236800193787, R2 0.6631151437759399\n",
      "epoch 1236, loss 0.887412428855896, R2 0.6604011654853821\n",
      "Eval loss 0.9295316338539124, R2 0.6631865501403809\n",
      "epoch 1237, loss 0.8872439861297607, R2 0.6604669094085693\n",
      "Eval loss 0.9293403029441833, R2 0.6632577776908875\n",
      "epoch 1238, loss 0.8870761394500732, R2 0.660532534122467\n",
      "Eval loss 0.9291492700576782, R2 0.6633288860321045\n",
      "epoch 1239, loss 0.8869085907936096, R2 0.6605979204177856\n",
      "Eval loss 0.9289587736129761, R2 0.6633996963500977\n",
      "epoch 1240, loss 0.8867416381835938, R2 0.6606631875038147\n",
      "Eval loss 0.9287689924240112, R2 0.6634703874588013\n",
      "epoch 1241, loss 0.8865750432014465, R2 0.660728394985199\n",
      "Eval loss 0.9285793900489807, R2 0.6635410189628601\n",
      "epoch 1242, loss 0.8864089846611023, R2 0.6607932448387146\n",
      "Eval loss 0.9283904433250427, R2 0.6636112928390503\n",
      "epoch 1243, loss 0.8862433433532715, R2 0.6608580946922302\n",
      "Eval loss 0.9282020330429077, R2 0.6636815071105957\n",
      "epoch 1244, loss 0.8860781192779541, R2 0.6609227657318115\n",
      "Eval loss 0.9280139803886414, R2 0.6637515425682068\n",
      "epoch 1245, loss 0.8859134912490845, R2 0.660987138748169\n",
      "Eval loss 0.9278266429901123, R2 0.6638214588165283\n",
      "epoch 1246, loss 0.8857492804527283, R2 0.6610514521598816\n",
      "Eval loss 0.9276397228240967, R2 0.663891077041626\n",
      "epoch 1247, loss 0.8855855464935303, R2 0.6611155271530151\n",
      "Eval loss 0.9274532198905945, R2 0.6639606356620789\n",
      "epoch 1248, loss 0.8854221701622009, R2 0.6611795425415039\n",
      "Eval loss 0.9272672533988953, R2 0.6640300154685974\n",
      "epoch 1249, loss 0.8852592706680298, R2 0.6612433791160583\n",
      "Eval loss 0.9270817637443542, R2 0.6640991568565369\n",
      "epoch 1250, loss 0.8850968480110168, R2 0.6613069772720337\n",
      "Eval loss 0.9268968105316162, R2 0.6641682386398315\n",
      "epoch 1251, loss 0.8849349617958069, R2 0.6613705158233643\n",
      "Eval loss 0.9267122149467468, R2 0.6642371416091919\n",
      "epoch 1252, loss 0.8847734332084656, R2 0.6614338159561157\n",
      "Eval loss 0.9265281558036804, R2 0.6643058657646179\n",
      "epoch 1253, loss 0.8846124410629272, R2 0.6614970564842224\n",
      "Eval loss 0.926344633102417, R2 0.6643743515014648\n",
      "epoch 1254, loss 0.8844518661499023, R2 0.6615599989891052\n",
      "Eval loss 0.9261615872383118, R2 0.6644427180290222\n",
      "epoch 1255, loss 0.8842915892601013, R2 0.6616228818893433\n",
      "Eval loss 0.9259790182113647, R2 0.66451096534729\n",
      "epoch 1256, loss 0.8841319680213928, R2 0.6616855263710022\n",
      "Eval loss 0.9257969856262207, R2 0.6645789742469788\n",
      "epoch 1257, loss 0.8839726448059082, R2 0.6617481112480164\n",
      "Eval loss 0.9256152510643005, R2 0.6646468043327332\n",
      "epoch 1258, loss 0.8838137984275818, R2 0.6618105173110962\n",
      "Eval loss 0.9254340529441833, R2 0.6647145748138428\n",
      "epoch 1259, loss 0.8836555480957031, R2 0.6618727445602417\n",
      "Eval loss 0.9252534508705139, R2 0.6647821664810181\n",
      "epoch 1260, loss 0.8834976553916931, R2 0.6619347929954529\n",
      "Eval loss 0.9250732064247131, R2 0.6648496389389038\n",
      "epoch 1261, loss 0.8833400011062622, R2 0.6619966626167297\n",
      "Eval loss 0.9248935580253601, R2 0.6649168729782104\n",
      "epoch 1262, loss 0.883182942867279, R2 0.6620584726333618\n",
      "Eval loss 0.924714207649231, R2 0.664983868598938\n",
      "epoch 1263, loss 0.8830264210700989, R2 0.6621200442314148\n",
      "Eval loss 0.9245354533195496, R2 0.665050745010376\n",
      "epoch 1264, loss 0.8828702569007874, R2 0.6621814966201782\n",
      "Eval loss 0.9243571758270264, R2 0.6651175618171692\n",
      "epoch 1265, loss 0.8827145099639893, R2 0.6622428297996521\n",
      "Eval loss 0.9241793155670166, R2 0.6651841402053833\n",
      "epoch 1266, loss 0.8825592398643494, R2 0.6623039841651917\n",
      "Eval loss 0.9240019917488098, R2 0.6652505993843079\n",
      "epoch 1267, loss 0.8824042677879333, R2 0.6623649001121521\n",
      "Eval loss 0.9238250255584717, R2 0.6653168797492981\n",
      "epoch 1268, loss 0.8822498321533203, R2 0.6624257564544678\n",
      "Eval loss 0.9236484169960022, R2 0.665382981300354\n",
      "epoch 1269, loss 0.8820959329605103, R2 0.6624864339828491\n",
      "Eval loss 0.9234724640846252, R2 0.6654490232467651\n",
      "epoch 1270, loss 0.8819422125816345, R2 0.6625469326972961\n",
      "Eval loss 0.9232970476150513, R2 0.6655147671699524\n",
      "epoch 1271, loss 0.8817890882492065, R2 0.6626074314117432\n",
      "Eval loss 0.9231219291687012, R2 0.6655804514884949\n",
      "epoch 1272, loss 0.881636381149292, R2 0.6626676321029663\n",
      "Eval loss 0.9229472279548645, R2 0.665645956993103\n",
      "epoch 1273, loss 0.8814840316772461, R2 0.6627275943756104\n",
      "Eval loss 0.9227730631828308, R2 0.6657114028930664\n",
      "epoch 1274, loss 0.8813320398330688, R2 0.6627876162528992\n",
      "Eval loss 0.9225993156433105, R2 0.6657764911651611\n",
      "epoch 1275, loss 0.8811805844306946, R2 0.6628474593162537\n",
      "Eval loss 0.9224260449409485, R2 0.6658415794372559\n",
      "epoch 1276, loss 0.8810295462608337, R2 0.662907063961029\n",
      "Eval loss 0.9222532510757446, R2 0.6659064888954163\n",
      "epoch 1277, loss 0.8808788061141968, R2 0.6629664897918701\n",
      "Eval loss 0.9220808744430542, R2 0.6659711599349976\n",
      "epoch 1278, loss 0.8807287216186523, R2 0.6630258560180664\n",
      "Eval loss 0.9219088554382324, R2 0.6660358309745789\n",
      "epoch 1279, loss 0.880578875541687, R2 0.6630850434303284\n",
      "Eval loss 0.9217374920845032, R2 0.6661001443862915\n",
      "epoch 1280, loss 0.8804295063018799, R2 0.663144052028656\n",
      "Eval loss 0.9215664863586426, R2 0.6661644577980042\n",
      "epoch 1281, loss 0.8802804350852966, R2 0.6632030010223389\n",
      "Eval loss 0.9213957786560059, R2 0.6662285327911377\n",
      "epoch 1282, loss 0.8801319003105164, R2 0.6632616519927979\n",
      "Eval loss 0.9212256669998169, R2 0.6662925481796265\n",
      "epoch 1283, loss 0.8799838423728943, R2 0.6633203029632568\n",
      "Eval loss 0.9210559725761414, R2 0.6663564443588257\n",
      "epoch 1284, loss 0.8798359632492065, R2 0.6633787155151367\n",
      "Eval loss 0.9208866953849792, R2 0.6664201021194458\n",
      "epoch 1285, loss 0.879688560962677, R2 0.6634371280670166\n",
      "Eval loss 0.9207178354263306, R2 0.6664835810661316\n",
      "epoch 1286, loss 0.8795416951179504, R2 0.6634952425956726\n",
      "Eval loss 0.9205493927001953, R2 0.6665470004081726\n",
      "epoch 1287, loss 0.8793951869010925, R2 0.6635532975196838\n",
      "Eval loss 0.9203815460205078, R2 0.6666101217269897\n",
      "epoch 1288, loss 0.879249095916748, R2 0.6636111736297607\n",
      "Eval loss 0.9202139973640442, R2 0.6666732430458069\n",
      "epoch 1289, loss 0.8791033029556274, R2 0.6636688709259033\n",
      "Eval loss 0.920046865940094, R2 0.6667361855506897\n",
      "epoch 1290, loss 0.8789580464363098, R2 0.6637265682220459\n",
      "Eval loss 0.9198801517486572, R2 0.6667989492416382\n",
      "epoch 1291, loss 0.8788130283355713, R2 0.6637839674949646\n",
      "Eval loss 0.9197140336036682, R2 0.6668615937232971\n",
      "epoch 1292, loss 0.8786686062812805, R2 0.663841187953949\n",
      "Eval loss 0.9195482134819031, R2 0.6669241189956665\n",
      "epoch 1293, loss 0.8785243630409241, R2 0.6638984084129333\n",
      "Eval loss 0.9193829298019409, R2 0.6669864058494568\n",
      "epoch 1294, loss 0.8783807158470154, R2 0.6639554500579834\n",
      "Eval loss 0.9192179441452026, R2 0.6670486330986023\n",
      "epoch 1295, loss 0.8782373070716858, R2 0.6640123128890991\n",
      "Eval loss 0.9190534353256226, R2 0.6671106219291687\n",
      "epoch 1296, loss 0.8780943155288696, R2 0.6640691757202148\n",
      "Eval loss 0.9188892841339111, R2 0.6671725511550903\n",
      "epoch 1297, loss 0.8779518604278564, R2 0.6641256809234619\n",
      "Eval loss 0.9187256693840027, R2 0.6672343015670776\n",
      "epoch 1298, loss 0.8778097033500671, R2 0.6641820669174194\n",
      "Eval loss 0.9185624718666077, R2 0.6672958731651306\n",
      "epoch 1299, loss 0.8776679039001465, R2 0.664238452911377\n",
      "Eval loss 0.9183995127677917, R2 0.667357325553894\n",
      "epoch 1300, loss 0.8775265216827393, R2 0.6642946004867554\n",
      "Eval loss 0.9182372093200684, R2 0.6674186587333679\n",
      "epoch 1301, loss 0.8773855566978455, R2 0.6643506288528442\n",
      "Eval loss 0.9180752038955688, R2 0.6674798727035522\n",
      "epoch 1302, loss 0.8772448897361755, R2 0.6644065976142883\n",
      "Eval loss 0.9179136753082275, R2 0.6675407886505127\n",
      "epoch 1303, loss 0.877104640007019, R2 0.6644623279571533\n",
      "Eval loss 0.9177525639533997, R2 0.6676017642021179\n",
      "epoch 1304, loss 0.8769647479057312, R2 0.6645179390907288\n",
      "Eval loss 0.9175917506217957, R2 0.6676624417304993\n",
      "epoch 1305, loss 0.8768253922462463, R2 0.6645734310150146\n",
      "Eval loss 0.9174314141273499, R2 0.6677230596542358\n",
      "epoch 1306, loss 0.8766863942146301, R2 0.6646288633346558\n",
      "Eval loss 0.9172714352607727, R2 0.6677834987640381\n",
      "epoch 1307, loss 0.8765476942062378, R2 0.664683997631073\n",
      "Eval loss 0.9171119928359985, R2 0.6678438782691956\n",
      "epoch 1308, loss 0.8764093518257141, R2 0.6647391319274902\n",
      "Eval loss 0.9169529676437378, R2 0.6679039597511292\n",
      "epoch 1309, loss 0.8762713670730591, R2 0.6647940278053284\n",
      "Eval loss 0.9167941212654114, R2 0.6679641008377075\n",
      "epoch 1310, loss 0.8761337995529175, R2 0.664848804473877\n",
      "Eval loss 0.9166359305381775, R2 0.6680240035057068\n",
      "epoch 1311, loss 0.8759967088699341, R2 0.6649035215377808\n",
      "Eval loss 0.9164780378341675, R2 0.6680837869644165\n",
      "epoch 1312, loss 0.8758598566055298, R2 0.6649580597877502\n",
      "Eval loss 0.9163205623626709, R2 0.6681434512138367\n",
      "epoch 1313, loss 0.8757234811782837, R2 0.6650124192237854\n",
      "Eval loss 0.9161635041236877, R2 0.6682027578353882\n",
      "epoch 1314, loss 0.8755874037742615, R2 0.6650667190551758\n",
      "Eval loss 0.9160069227218628, R2 0.668262243270874\n",
      "epoch 1315, loss 0.8754517436027527, R2 0.6651209592819214\n",
      "Eval loss 0.9158506393432617, R2 0.6683213710784912\n",
      "epoch 1316, loss 0.875316321849823, R2 0.6651749014854431\n",
      "Eval loss 0.9156947731971741, R2 0.6683804988861084\n",
      "epoch 1317, loss 0.8751814961433411, R2 0.6652287244796753\n",
      "Eval loss 0.9155392646789551, R2 0.6684393286705017\n",
      "epoch 1318, loss 0.8750468492507935, R2 0.6652825474739075\n",
      "Eval loss 0.9153841733932495, R2 0.6684980988502502\n",
      "epoch 1319, loss 0.8749126195907593, R2 0.6653361320495605\n",
      "Eval loss 0.9152294397354126, R2 0.668556809425354\n",
      "epoch 1320, loss 0.8747788071632385, R2 0.6653895974159241\n",
      "Eval loss 0.9150752425193787, R2 0.6686153411865234\n",
      "epoch 1321, loss 0.8746452331542969, R2 0.665442943572998\n",
      "Eval loss 0.9149213433265686, R2 0.6686736941337585\n",
      "epoch 1322, loss 0.8745121955871582, R2 0.6654961705207825\n",
      "Eval loss 0.9147678017616272, R2 0.6687319278717041\n",
      "epoch 1323, loss 0.8743793368339539, R2 0.6655492782592773\n",
      "Eval loss 0.914614737033844, R2 0.6687900424003601\n",
      "epoch 1324, loss 0.8742468953132629, R2 0.6656021475791931\n",
      "Eval loss 0.9144620299339294, R2 0.6688480377197266\n",
      "epoch 1325, loss 0.8741149306297302, R2 0.6656548976898193\n",
      "Eval loss 0.9143096804618835, R2 0.6689058542251587\n",
      "epoch 1326, loss 0.8739832639694214, R2 0.6657077074050903\n",
      "Eval loss 0.9141576886177063, R2 0.6689635515213013\n",
      "epoch 1327, loss 0.8738519549369812, R2 0.6657602787017822\n",
      "Eval loss 0.9140061736106873, R2 0.6690210700035095\n",
      "epoch 1328, loss 0.8737209439277649, R2 0.6658126711845398\n",
      "Eval loss 0.9138550162315369, R2 0.6690785884857178\n",
      "epoch 1329, loss 0.8735902905464172, R2 0.6658650040626526\n",
      "Eval loss 0.9137042164802551, R2 0.6691358685493469\n",
      "epoch 1330, loss 0.8734601140022278, R2 0.665917158126831\n",
      "Eval loss 0.9135539531707764, R2 0.6691930294036865\n",
      "epoch 1331, loss 0.8733302354812622, R2 0.6659692525863647\n",
      "Eval loss 0.9134038686752319, R2 0.6692501306533813\n",
      "epoch 1332, loss 0.8732006549835205, R2 0.6660211086273193\n",
      "Eval loss 0.9132542014122009, R2 0.6693069934844971\n",
      "epoch 1333, loss 0.8730714917182922, R2 0.6660729050636292\n",
      "Eval loss 0.9131050109863281, R2 0.669363796710968\n",
      "epoch 1334, loss 0.8729425668716431, R2 0.6661246418952942\n",
      "Eval loss 0.9129562377929688, R2 0.6694204807281494\n",
      "epoch 1335, loss 0.8728141188621521, R2 0.6661762595176697\n",
      "Eval loss 0.9128075838088989, R2 0.6694769263267517\n",
      "epoch 1336, loss 0.8726858496665955, R2 0.6662275791168213\n",
      "Eval loss 0.9126595258712769, R2 0.669533371925354\n",
      "epoch 1337, loss 0.8725581169128418, R2 0.6662788987159729\n",
      "Eval loss 0.9125118255615234, R2 0.6695895791053772\n",
      "epoch 1338, loss 0.872430682182312, R2 0.666330099105835\n",
      "Eval loss 0.9123643636703491, R2 0.6696457266807556\n",
      "epoch 1339, loss 0.8723036050796509, R2 0.6663811206817627\n",
      "Eval loss 0.9122173190116882, R2 0.6697016954421997\n",
      "epoch 1340, loss 0.8721767663955688, R2 0.6664320230484009\n",
      "Eval loss 0.9120707511901855, R2 0.669757604598999\n",
      "epoch 1341, loss 0.8720503449440002, R2 0.6664828658103943\n",
      "Eval loss 0.9119244813919067, R2 0.669813334941864\n",
      "epoch 1342, loss 0.8719242215156555, R2 0.6665334701538086\n",
      "Eval loss 0.9117786884307861, R2 0.6698688864707947\n",
      "epoch 1343, loss 0.8717985153198242, R2 0.6665840744972229\n",
      "Eval loss 0.9116331934928894, R2 0.6699243783950806\n",
      "epoch 1344, loss 0.8716731667518616, R2 0.6666344404220581\n",
      "Eval loss 0.9114879369735718, R2 0.6699797511100769\n",
      "epoch 1345, loss 0.8715481758117676, R2 0.6666847467422485\n",
      "Eval loss 0.9113432168960571, R2 0.6700350046157837\n",
      "epoch 1346, loss 0.8714233040809631, R2 0.6667349338531494\n",
      "Eval loss 0.9111986756324768, R2 0.6700901389122009\n",
      "epoch 1347, loss 0.8712989687919617, R2 0.6667850613594055\n",
      "Eval loss 0.911054790019989, R2 0.6701450943946838\n",
      "epoch 1348, loss 0.8711748719215393, R2 0.6668350100517273\n",
      "Eval loss 0.910910964012146, R2 0.6701998710632324\n",
      "epoch 1349, loss 0.8710511326789856, R2 0.6668847799301147\n",
      "Eval loss 0.9107676148414612, R2 0.6702545881271362\n",
      "epoch 1350, loss 0.8709276914596558, R2 0.6669344902038574\n",
      "Eval loss 0.9106248021125793, R2 0.6703091859817505\n",
      "epoch 1351, loss 0.8708046674728394, R2 0.6669840216636658\n",
      "Eval loss 0.9104821681976318, R2 0.6703636050224304\n",
      "epoch 1352, loss 0.8706819415092468, R2 0.6670335531234741\n",
      "Eval loss 0.9103399515151978, R2 0.6704180240631104\n",
      "epoch 1353, loss 0.870559573173523, R2 0.6670827865600586\n",
      "Eval loss 0.9101980328559875, R2 0.670472264289856\n",
      "epoch 1354, loss 0.870437502861023, R2 0.6671320199966431\n",
      "Eval loss 0.9100565314292908, R2 0.670526385307312\n",
      "epoch 1355, loss 0.8703157901763916, R2 0.667181134223938\n",
      "Eval loss 0.9099153280258179, R2 0.6705803275108337\n",
      "epoch 1356, loss 0.8701943159103394, R2 0.6672301292419434\n",
      "Eval loss 0.9097745418548584, R2 0.6706340909004211\n",
      "epoch 1357, loss 0.8700731992721558, R2 0.6672789454460144\n",
      "Eval loss 0.9096341133117676, R2 0.6706878542900085\n",
      "epoch 1358, loss 0.8699524998664856, R2 0.6673277020454407\n",
      "Eval loss 0.9094938039779663, R2 0.6707414388656616\n",
      "epoch 1359, loss 0.8698320984840393, R2 0.6673763990402222\n",
      "Eval loss 0.9093540906906128, R2 0.6707949638366699\n",
      "epoch 1360, loss 0.8697120547294617, R2 0.6674248576164246\n",
      "Eval loss 0.9092147350311279, R2 0.6708482503890991\n",
      "epoch 1361, loss 0.8695921301841736, R2 0.6674732565879822\n",
      "Eval loss 0.9090756773948669, R2 0.6709014773368835\n",
      "epoch 1362, loss 0.8694727420806885, R2 0.6675214767456055\n",
      "Eval loss 0.9089369773864746, R2 0.670954704284668\n",
      "epoch 1363, loss 0.8693535327911377, R2 0.667569637298584\n",
      "Eval loss 0.9087985754013062, R2 0.6710076332092285\n",
      "epoch 1364, loss 0.8692347407341003, R2 0.6676177382469177\n",
      "Eval loss 0.9086605906486511, R2 0.6710605025291443\n",
      "epoch 1365, loss 0.8691163063049316, R2 0.6676655411720276\n",
      "Eval loss 0.9085229635238647, R2 0.6711131930351257\n",
      "epoch 1366, loss 0.868998110294342, R2 0.6677134037017822\n",
      "Eval loss 0.9083855152130127, R2 0.6711658835411072\n",
      "epoch 1367, loss 0.8688800930976868, R2 0.6677610278129578\n",
      "Eval loss 0.9082486629486084, R2 0.6712183952331543\n",
      "epoch 1368, loss 0.8687625527381897, R2 0.6678085923194885\n",
      "Eval loss 0.908112108707428, R2 0.6712707281112671\n",
      "epoch 1369, loss 0.8686453700065613, R2 0.6678560376167297\n",
      "Eval loss 0.9079756736755371, R2 0.6713230013847351\n",
      "epoch 1370, loss 0.8685283660888672, R2 0.667903482913971\n",
      "Eval loss 0.9078397750854492, R2 0.6713751554489136\n",
      "epoch 1371, loss 0.8684117197990417, R2 0.6679506897926331\n",
      "Eval loss 0.9077041149139404, R2 0.6714271903038025\n",
      "epoch 1372, loss 0.868295431137085, R2 0.6679977774620056\n",
      "Eval loss 0.9075688719749451, R2 0.6714791059494019\n",
      "epoch 1373, loss 0.868179440498352, R2 0.6680446863174438\n",
      "Eval loss 0.9074338674545288, R2 0.6715309023857117\n",
      "epoch 1374, loss 0.868063747882843, R2 0.6680916547775269\n",
      "Eval loss 0.9072992205619812, R2 0.6715825796127319\n",
      "epoch 1375, loss 0.8679484128952026, R2 0.6681384444236755\n",
      "Eval loss 0.907164990901947, R2 0.6716341376304626\n",
      "epoch 1376, loss 0.8678333163261414, R2 0.6681851148605347\n",
      "Eval loss 0.9070310592651367, R2 0.6716855764389038\n",
      "epoch 1377, loss 0.8677185773849487, R2 0.6682316660881042\n",
      "Eval loss 0.9068974256515503, R2 0.6717368960380554\n",
      "epoch 1378, loss 0.8676040768623352, R2 0.6682780981063843\n",
      "Eval loss 0.9067641496658325, R2 0.6717880964279175\n",
      "epoch 1379, loss 0.8674899935722351, R2 0.6683244705200195\n",
      "Eval loss 0.906631350517273, R2 0.67183917760849\n",
      "epoch 1380, loss 0.8673760890960693, R2 0.6683706641197205\n",
      "Eval loss 0.9064986109733582, R2 0.6718901991844177\n",
      "epoch 1381, loss 0.8672625422477722, R2 0.6684167385101318\n",
      "Eval loss 0.9063663482666016, R2 0.6719409823417664\n",
      "epoch 1382, loss 0.8671493530273438, R2 0.6684626936912537\n",
      "Eval loss 0.9062344431877136, R2 0.671991765499115\n",
      "epoch 1383, loss 0.8670363426208496, R2 0.6685086488723755\n",
      "Eval loss 0.9061028361320496, R2 0.6720424294471741\n",
      "epoch 1384, loss 0.8669236898422241, R2 0.6685543656349182\n",
      "Eval loss 0.9059714674949646, R2 0.6720929741859436\n",
      "epoch 1385, loss 0.8668113946914673, R2 0.6686000823974609\n",
      "Eval loss 0.9058405756950378, R2 0.672143280506134\n",
      "epoch 1386, loss 0.8666993379592896, R2 0.6686456203460693\n",
      "Eval loss 0.9057098627090454, R2 0.6721936464309692\n",
      "epoch 1387, loss 0.8665875792503357, R2 0.6686910390853882\n",
      "Eval loss 0.9055795669555664, R2 0.6722438335418701\n",
      "epoch 1388, loss 0.8664761185646057, R2 0.6687363386154175\n",
      "Eval loss 0.9054495096206665, R2 0.6722938418388367\n",
      "epoch 1389, loss 0.8663650155067444, R2 0.6687816381454468\n",
      "Eval loss 0.90531986951828, R2 0.6723437905311584\n",
      "epoch 1390, loss 0.8662541508674622, R2 0.668826699256897\n",
      "Eval loss 0.9051905870437622, R2 0.6723936200141907\n",
      "epoch 1391, loss 0.866143524646759, R2 0.6688717007637024\n",
      "Eval loss 0.9050616025924683, R2 0.6724433302879333\n",
      "epoch 1392, loss 0.8660332560539246, R2 0.6689165830612183\n",
      "Eval loss 0.9049327969551086, R2 0.6724929213523865\n",
      "epoch 1393, loss 0.8659233450889587, R2 0.6689613461494446\n",
      "Eval loss 0.9048044085502625, R2 0.67254239320755\n",
      "epoch 1394, loss 0.8658136129379272, R2 0.6690061092376709\n",
      "Eval loss 0.9046764373779297, R2 0.6725917458534241\n",
      "epoch 1395, loss 0.8657042384147644, R2 0.6690506339073181\n",
      "Eval loss 0.9045485258102417, R2 0.6726410388946533\n",
      "epoch 1396, loss 0.8655950427055359, R2 0.6690951585769653\n",
      "Eval loss 0.9044211506843567, R2 0.6726902723312378\n",
      "epoch 1397, loss 0.8654862642288208, R2 0.6691395044326782\n",
      "Eval loss 0.904293954372406, R2 0.6727392673492432\n",
      "epoch 1398, loss 0.8653777241706848, R2 0.6691837906837463\n",
      "Eval loss 0.9041671752929688, R2 0.6727882027626038\n",
      "epoch 1399, loss 0.8652695417404175, R2 0.6692278981208801\n",
      "Eval loss 0.9040406942367554, R2 0.6728370189666748\n",
      "epoch 1400, loss 0.8651615977287292, R2 0.6692719459533691\n",
      "Eval loss 0.9039145112037659, R2 0.6728857755661011\n",
      "epoch 1401, loss 0.8650539517402649, R2 0.6693159341812134\n",
      "Eval loss 0.9037886261940002, R2 0.6729344129562378\n",
      "epoch 1402, loss 0.8649464845657349, R2 0.6693596839904785\n",
      "Eval loss 0.903663158416748, R2 0.6729828715324402\n",
      "epoch 1403, loss 0.8648393750190735, R2 0.6694034934043884\n",
      "Eval loss 0.9035378098487854, R2 0.6730312705039978\n",
      "epoch 1404, loss 0.864732563495636, R2 0.6694470643997192\n",
      "Eval loss 0.9034128189086914, R2 0.6730795502662659\n",
      "epoch 1405, loss 0.8646261096000671, R2 0.66949063539505\n",
      "Eval loss 0.9032881259918213, R2 0.6731277108192444\n",
      "epoch 1406, loss 0.8645197749137878, R2 0.6695340871810913\n",
      "Eval loss 0.9031639099121094, R2 0.6731757521629333\n",
      "epoch 1407, loss 0.8644137978553772, R2 0.669577419757843\n",
      "Eval loss 0.903039813041687, R2 0.6732236742973328\n",
      "epoch 1408, loss 0.8643081188201904, R2 0.6696205735206604\n",
      "Eval loss 0.9029160737991333, R2 0.6732715964317322\n",
      "epoch 1409, loss 0.8642027378082275, R2 0.6696637272834778\n",
      "Eval loss 0.9027926921844482, R2 0.6733193397521973\n",
      "epoch 1410, loss 0.864097535610199, R2 0.6697067022323608\n",
      "Eval loss 0.9026694893836975, R2 0.6733669638633728\n",
      "epoch 1411, loss 0.8639927506446838, R2 0.6697496175765991\n",
      "Eval loss 0.9025467038154602, R2 0.6734144687652588\n",
      "epoch 1412, loss 0.8638880848884583, R2 0.6697924137115479\n",
      "Eval loss 0.9024242162704468, R2 0.6734619140625\n",
      "epoch 1413, loss 0.8637837767601013, R2 0.6698351502418518\n",
      "Eval loss 0.9023019671440125, R2 0.6735091805458069\n",
      "epoch 1414, loss 0.8636797666549683, R2 0.6698777675628662\n",
      "Eval loss 0.9021801352500916, R2 0.6735564470291138\n",
      "epoch 1415, loss 0.8635760545730591, R2 0.6699202060699463\n",
      "Eval loss 0.902058482170105, R2 0.6736035346984863\n",
      "epoch 1416, loss 0.863472580909729, R2 0.6699626445770264\n",
      "Eval loss 0.9019371271133423, R2 0.6736505031585693\n",
      "epoch 1417, loss 0.8633692860603333, R2 0.6700049042701721\n",
      "Eval loss 0.9018161296844482, R2 0.6736973524093628\n",
      "epoch 1418, loss 0.8632663488388062, R2 0.6700471043586731\n",
      "Eval loss 0.9016954302787781, R2 0.6737442016601562\n",
      "epoch 1419, loss 0.8631637096405029, R2 0.6700891852378845\n",
      "Eval loss 0.901574969291687, R2 0.6737908720970154\n",
      "epoch 1420, loss 0.863061249256134, R2 0.670131266117096\n",
      "Eval loss 0.9014549255371094, R2 0.6738374829292297\n",
      "epoch 1421, loss 0.8629591464996338, R2 0.6701731085777283\n",
      "Eval loss 0.9013350009918213, R2 0.6738839149475098\n",
      "epoch 1422, loss 0.8628572821617126, R2 0.6702148914337158\n",
      "Eval loss 0.9012154936790466, R2 0.6739303469657898\n",
      "epoch 1423, loss 0.8627556562423706, R2 0.6702566146850586\n",
      "Eval loss 0.9010962843894958, R2 0.6739766001701355\n",
      "epoch 1424, loss 0.8626542687416077, R2 0.6702982187271118\n",
      "Eval loss 0.9009773135185242, R2 0.6740227341651917\n",
      "epoch 1425, loss 0.8625532388687134, R2 0.6703397035598755\n",
      "Eval loss 0.9008585810661316, R2 0.6740688681602478\n",
      "epoch 1426, loss 0.862452507019043, R2 0.6703811287879944\n",
      "Eval loss 0.9007402062416077, R2 0.6741148233413696\n",
      "epoch 1427, loss 0.8623519539833069, R2 0.6704224348068237\n",
      "Eval loss 0.9006220698356628, R2 0.6741605997085571\n",
      "epoch 1428, loss 0.8622516393661499, R2 0.6704636216163635\n",
      "Eval loss 0.9005042910575867, R2 0.6742063760757446\n",
      "epoch 1429, loss 0.8621516823768616, R2 0.6705047488212585\n",
      "Eval loss 0.9003868103027344, R2 0.6742519736289978\n",
      "epoch 1430, loss 0.8620518445968628, R2 0.670545756816864\n",
      "Eval loss 0.900269627571106, R2 0.674297571182251\n",
      "epoch 1431, loss 0.8619524240493774, R2 0.6705865859985352\n",
      "Eval loss 0.9001526236534119, R2 0.6743430495262146\n",
      "epoch 1432, loss 0.8618531823158264, R2 0.6706275343894958\n",
      "Eval loss 0.9000359773635864, R2 0.6743883490562439\n",
      "epoch 1433, loss 0.8617541790008545, R2 0.6706682443618774\n",
      "Eval loss 0.8999195098876953, R2 0.6744336485862732\n",
      "epoch 1434, loss 0.8616555333137512, R2 0.6707088351249695\n",
      "Eval loss 0.8998034000396729, R2 0.6744787693023682\n",
      "epoch 1435, loss 0.8615570664405823, R2 0.6707494258880615\n",
      "Eval loss 0.8996877074241638, R2 0.6745237708091736\n",
      "epoch 1436, loss 0.8614587783813477, R2 0.6707897782325745\n",
      "Eval loss 0.8995720148086548, R2 0.674568772315979\n",
      "epoch 1437, loss 0.8613608479499817, R2 0.6708301901817322\n",
      "Eval loss 0.8994568586349487, R2 0.6746135950088501\n",
      "epoch 1438, loss 0.8612631559371948, R2 0.6708704233169556\n",
      "Eval loss 0.8993418216705322, R2 0.6746584177017212\n",
      "epoch 1439, loss 0.8611657619476318, R2 0.6709104776382446\n",
      "Eval loss 0.8992272019386292, R2 0.6747030019760132\n",
      "epoch 1440, loss 0.861068606376648, R2 0.6709505915641785\n",
      "Eval loss 0.8991128206253052, R2 0.6747475266456604\n",
      "epoch 1441, loss 0.8609716296195984, R2 0.670990526676178\n",
      "Eval loss 0.8989987373352051, R2 0.6747919917106628\n",
      "epoch 1442, loss 0.8608750104904175, R2 0.6710303425788879\n",
      "Eval loss 0.8988847136497498, R2 0.674836277961731\n",
      "epoch 1443, loss 0.8607785105705261, R2 0.6710701584815979\n",
      "Eval loss 0.8987712860107422, R2 0.6748805642127991\n",
      "epoch 1444, loss 0.8606824278831482, R2 0.6711098551750183\n",
      "Eval loss 0.8986578583717346, R2 0.6749247312545776\n",
      "epoch 1445, loss 0.8605864644050598, R2 0.6711494326591492\n",
      "Eval loss 0.8985448479652405, R2 0.6749687790870667\n",
      "epoch 1446, loss 0.8604907989501953, R2 0.6711889505386353\n",
      "Eval loss 0.8984320759773254, R2 0.6750127077102661\n",
      "epoch 1447, loss 0.8603953719139099, R2 0.6712283492088318\n",
      "Eval loss 0.8983194828033447, R2 0.6750566363334656\n",
      "epoch 1448, loss 0.8603002429008484, R2 0.6712676882743835\n",
      "Eval loss 0.8982073068618774, R2 0.6751003861427307\n",
      "epoch 1449, loss 0.8602052927017212, R2 0.671306848526001\n",
      "Eval loss 0.8980953693389893, R2 0.6751441359519958\n",
      "epoch 1450, loss 0.8601106405258179, R2 0.6713460087776184\n",
      "Eval loss 0.8979836702346802, R2 0.6751876473426819\n",
      "epoch 1451, loss 0.8600161671638489, R2 0.6713849902153015\n",
      "Eval loss 0.897872269153595, R2 0.6752311587333679\n",
      "epoch 1452, loss 0.8599219918251038, R2 0.6714239120483398\n",
      "Eval loss 0.8977611064910889, R2 0.6752744913101196\n",
      "epoch 1453, loss 0.8598280549049377, R2 0.6714627742767334\n",
      "Eval loss 0.8976502418518066, R2 0.6753178238868713\n",
      "epoch 1454, loss 0.8597343564033508, R2 0.6715015769004822\n",
      "Eval loss 0.8975396752357483, R2 0.6753610372543335\n",
      "epoch 1455, loss 0.8596407771110535, R2 0.6715402603149414\n",
      "Eval loss 0.897429347038269, R2 0.6754040122032166\n",
      "epoch 1456, loss 0.8595476746559143, R2 0.6715787649154663\n",
      "Eval loss 0.8973193168640137, R2 0.6754470467567444\n",
      "epoch 1457, loss 0.8594547510147095, R2 0.6716172695159912\n",
      "Eval loss 0.8972094655036926, R2 0.6754899621009827\n",
      "epoch 1458, loss 0.8593619465827942, R2 0.6716556549072266\n",
      "Eval loss 0.8970999121665955, R2 0.6755327582359314\n",
      "epoch 1459, loss 0.8592694401741028, R2 0.6716939210891724\n",
      "Eval loss 0.8969906568527222, R2 0.6755754947662354\n",
      "epoch 1460, loss 0.8591772317886353, R2 0.6717321872711182\n",
      "Eval loss 0.8968816995620728, R2 0.6756181120872498\n",
      "epoch 1461, loss 0.859085202217102, R2 0.6717703342437744\n",
      "Eval loss 0.8967729210853577, R2 0.6756606698036194\n",
      "epoch 1462, loss 0.8589934706687927, R2 0.6718083024024963\n",
      "Eval loss 0.8966644406318665, R2 0.6757031083106995\n",
      "epoch 1463, loss 0.8589019179344177, R2 0.6718462705612183\n",
      "Eval loss 0.8965561389923096, R2 0.67574542760849\n",
      "epoch 1464, loss 0.8588106632232666, R2 0.6718841195106506\n",
      "Eval loss 0.8964481949806213, R2 0.6757876873016357\n",
      "epoch 1465, loss 0.8587194681167603, R2 0.6719219088554382\n",
      "Eval loss 0.8963404893875122, R2 0.6758297681808472\n",
      "epoch 1466, loss 0.8586287498474121, R2 0.6719595789909363\n",
      "Eval loss 0.8962330222129822, R2 0.6758719086647034\n",
      "epoch 1467, loss 0.8585380911827087, R2 0.6719971299171448\n",
      "Eval loss 0.8961257934570312, R2 0.6759138703346252\n",
      "epoch 1468, loss 0.858447790145874, R2 0.6720346808433533\n",
      "Eval loss 0.896018922328949, R2 0.6759557723999023\n",
      "epoch 1469, loss 0.8583576679229736, R2 0.6720721125602722\n",
      "Eval loss 0.895912230014801, R2 0.6759975552558899\n",
      "epoch 1470, loss 0.8582677841186523, R2 0.6721094846725464\n",
      "Eval loss 0.8958057761192322, R2 0.6760392189025879\n",
      "epoch 1471, loss 0.8581780791282654, R2 0.6721466183662415\n",
      "Eval loss 0.8956995010375977, R2 0.6760808825492859\n",
      "epoch 1472, loss 0.8580886721611023, R2 0.6721838116645813\n",
      "Eval loss 0.8955936431884766, R2 0.6761223077774048\n",
      "epoch 1473, loss 0.8579994440078735, R2 0.6722208261489868\n",
      "Eval loss 0.8954880237579346, R2 0.6761637330055237\n",
      "epoch 1474, loss 0.8579104542732239, R2 0.6722579002380371\n",
      "Eval loss 0.8953826427459717, R2 0.6762050986289978\n",
      "epoch 1475, loss 0.8578217625617981, R2 0.6722946763038635\n",
      "Eval loss 0.8952774405479431, R2 0.6762464046478271\n",
      "epoch 1476, loss 0.8577331900596619, R2 0.6723315119743347\n",
      "Eval loss 0.8951725363731384, R2 0.6762874722480774\n",
      "epoch 1477, loss 0.8576449751853943, R2 0.6723682880401611\n",
      "Eval loss 0.8950677514076233, R2 0.6763285398483276\n",
      "epoch 1478, loss 0.8575568795204163, R2 0.6724050045013428\n",
      "Eval loss 0.8949635028839111, R2 0.6763694882392883\n",
      "epoch 1479, loss 0.8574690222740173, R2 0.6724415421485901\n",
      "Eval loss 0.894859254360199, R2 0.676410436630249\n",
      "epoch 1480, loss 0.8573814630508423, R2 0.6724779605865479\n",
      "Eval loss 0.8947552442550659, R2 0.6764511466026306\n",
      "epoch 1481, loss 0.8572940826416016, R2 0.6725143790245056\n",
      "Eval loss 0.8946517109870911, R2 0.676491916179657\n",
      "epoch 1482, loss 0.8572068214416504, R2 0.672550618648529\n",
      "Eval loss 0.894548237323761, R2 0.676532506942749\n",
      "epoch 1483, loss 0.8571199178695679, R2 0.6725868582725525\n",
      "Eval loss 0.8944450616836548, R2 0.6765730381011963\n",
      "epoch 1484, loss 0.8570332527160645, R2 0.6726229786872864\n",
      "Eval loss 0.8943421244621277, R2 0.676613450050354\n",
      "epoch 1485, loss 0.8569467663764954, R2 0.6726589798927307\n",
      "Eval loss 0.8942394852638245, R2 0.6766538619995117\n",
      "epoch 1486, loss 0.8568604588508606, R2 0.672694981098175\n",
      "Eval loss 0.8941370844841003, R2 0.6766940951347351\n",
      "epoch 1487, loss 0.8567744493484497, R2 0.6727308630943298\n",
      "Eval loss 0.8940348029136658, R2 0.676734209060669\n",
      "epoch 1488, loss 0.8566886782646179, R2 0.6727666854858398\n",
      "Eval loss 0.8939329385757446, R2 0.6767743229866028\n",
      "epoch 1489, loss 0.8566030263900757, R2 0.6728023886680603\n",
      "Eval loss 0.893831193447113, R2 0.6768143177032471\n",
      "epoch 1490, loss 0.8565176129341125, R2 0.6728379726409912\n",
      "Eval loss 0.8937296271324158, R2 0.6768543124198914\n",
      "epoch 1491, loss 0.8564324975013733, R2 0.6728735566139221\n",
      "Eval loss 0.8936284780502319, R2 0.6768940687179565\n",
      "epoch 1492, loss 0.8563475608825684, R2 0.6729090213775635\n",
      "Eval loss 0.8935274481773376, R2 0.6769338250160217\n",
      "epoch 1493, loss 0.8562628030776978, R2 0.6729444265365601\n",
      "Eval loss 0.893426775932312, R2 0.6769734025001526\n",
      "epoch 1494, loss 0.856178343296051, R2 0.6729796528816223\n",
      "Eval loss 0.8933262228965759, R2 0.6770130395889282\n",
      "epoch 1495, loss 0.8560941219329834, R2 0.6730148792266846\n",
      "Eval loss 0.893225908279419, R2 0.6770525574684143\n",
      "epoch 1496, loss 0.8560099005699158, R2 0.673050045967102\n",
      "Eval loss 0.8931258916854858, R2 0.6770919561386108\n",
      "epoch 1497, loss 0.8559260368347168, R2 0.6730850338935852\n",
      "Eval loss 0.8930261731147766, R2 0.6771312355995178\n",
      "epoch 1498, loss 0.8558424115180969, R2 0.6731200218200684\n",
      "Eval loss 0.8929265737533569, R2 0.6771703958511353\n",
      "epoch 1499, loss 0.8557589054107666, R2 0.6731549501419067\n",
      "Eval loss 0.8928272128105164, R2 0.6772096157073975\n",
      "epoch 1500, loss 0.8556756973266602, R2 0.6731897592544556\n",
      "Eval loss 0.8927280902862549, R2 0.6772486567497253\n",
      "epoch 1501, loss 0.8555927276611328, R2 0.6732245683670044\n",
      "Eval loss 0.8926292657852173, R2 0.6772876977920532\n",
      "epoch 1502, loss 0.855509877204895, R2 0.6732591390609741\n",
      "Eval loss 0.892530620098114, R2 0.6773264408111572\n",
      "epoch 1503, loss 0.8554273247718811, R2 0.6732937097549438\n",
      "Eval loss 0.8924322128295898, R2 0.6773653030395508\n",
      "epoch 1504, loss 0.8553449511528015, R2 0.6733282208442688\n",
      "Eval loss 0.8923341035842896, R2 0.67740398645401\n",
      "epoch 1505, loss 0.855262815952301, R2 0.673362672328949\n",
      "Eval loss 0.8922361731529236, R2 0.6774426102638245\n",
      "epoch 1506, loss 0.8551807999610901, R2 0.6733970046043396\n",
      "Eval loss 0.8921384215354919, R2 0.6774812340736389\n",
      "epoch 1507, loss 0.8550990223884583, R2 0.6734312176704407\n",
      "Eval loss 0.8920409679412842, R2 0.6775196194648743\n",
      "epoch 1508, loss 0.8550174236297607, R2 0.6734654903411865\n",
      "Eval loss 0.8919438719749451, R2 0.6775580644607544\n",
      "epoch 1509, loss 0.8549362421035767, R2 0.6734995245933533\n",
      "Eval loss 0.8918468356132507, R2 0.677596390247345\n",
      "epoch 1510, loss 0.8548550009727478, R2 0.67353355884552\n",
      "Eval loss 0.8917499780654907, R2 0.677634596824646\n",
      "epoch 1511, loss 0.8547741770744324, R2 0.6735674738883972\n",
      "Eval loss 0.8916535377502441, R2 0.6776727437973022\n",
      "epoch 1512, loss 0.8546934127807617, R2 0.6736013293266296\n",
      "Eval loss 0.8915570378303528, R2 0.6777108311653137\n",
      "epoch 1513, loss 0.8546129465103149, R2 0.6736350655555725\n",
      "Eval loss 0.8914609551429749, R2 0.6777487397193909\n",
      "epoch 1514, loss 0.8545325398445129, R2 0.6736688017845154\n",
      "Eval loss 0.8913651704788208, R2 0.677786648273468\n",
      "epoch 1515, loss 0.8544524908065796, R2 0.6737024188041687\n",
      "Eval loss 0.8912694454193115, R2 0.6778244376182556\n",
      "epoch 1516, loss 0.8543725609779358, R2 0.6737359762191772\n",
      "Eval loss 0.8911740779876709, R2 0.6778622269630432\n",
      "epoch 1517, loss 0.8542928695678711, R2 0.6737694144248962\n",
      "Eval loss 0.8910788297653198, R2 0.6778998374938965\n",
      "epoch 1518, loss 0.8542134165763855, R2 0.6738028526306152\n",
      "Eval loss 0.8909839391708374, R2 0.677937388420105\n",
      "epoch 1519, loss 0.8541340827941895, R2 0.6738361716270447\n",
      "Eval loss 0.8908891677856445, R2 0.6779749393463135\n",
      "epoch 1520, loss 0.8540549874305725, R2 0.6738694310188293\n",
      "Eval loss 0.8907946944236755, R2 0.6780122518539429\n",
      "epoch 1521, loss 0.8539760112762451, R2 0.6739025712013245\n",
      "Eval loss 0.8907003998756409, R2 0.6780495643615723\n",
      "epoch 1522, loss 0.8538973331451416, R2 0.6739356517791748\n",
      "Eval loss 0.8906062841415405, R2 0.6780868172645569\n",
      "epoch 1523, loss 0.8538188338279724, R2 0.6739686727523804\n",
      "Eval loss 0.8905123472213745, R2 0.678123950958252\n",
      "epoch 1524, loss 0.8537406325340271, R2 0.6740015745162964\n",
      "Eval loss 0.8904187083244324, R2 0.678161084651947\n",
      "epoch 1525, loss 0.8536624908447266, R2 0.6740344762802124\n",
      "Eval loss 0.8903251886367798, R2 0.6781980991363525\n",
      "epoch 1526, loss 0.8535845875740051, R2 0.6740672588348389\n",
      "Eval loss 0.8902320861816406, R2 0.6782350540161133\n",
      "epoch 1527, loss 0.8535069227218628, R2 0.6740999221801758\n",
      "Eval loss 0.890139102935791, R2 0.6782718300819397\n",
      "epoch 1528, loss 0.8534294366836548, R2 0.6741325855255127\n",
      "Eval loss 0.8900463581085205, R2 0.6783085465431213\n",
      "epoch 1529, loss 0.8533521294593811, R2 0.6741651296615601\n",
      "Eval loss 0.8899537324905396, R2 0.6783453226089478\n",
      "epoch 1530, loss 0.8532750010490417, R2 0.6741976141929626\n",
      "Eval loss 0.8898613452911377, R2 0.6783819198608398\n",
      "epoch 1531, loss 0.8531981110572815, R2 0.6742300987243652\n",
      "Eval loss 0.8897693753242493, R2 0.6784184575080872\n",
      "epoch 1532, loss 0.8531213402748108, R2 0.6742624044418335\n",
      "Eval loss 0.8896773457527161, R2 0.6784549355506897\n",
      "epoch 1533, loss 0.8530448079109192, R2 0.6742945909500122\n",
      "Eval loss 0.889585554599762, R2 0.6784912943840027\n",
      "epoch 1534, loss 0.8529684543609619, R2 0.6743268966674805\n",
      "Eval loss 0.8894941806793213, R2 0.6785275936126709\n",
      "epoch 1535, loss 0.852892279624939, R2 0.6743589639663696\n",
      "Eval loss 0.8894028663635254, R2 0.6785638332366943\n",
      "epoch 1536, loss 0.8528163433074951, R2 0.6743910908699036\n",
      "Eval loss 0.8893119692802429, R2 0.6785999536514282\n",
      "epoch 1537, loss 0.8527406454086304, R2 0.6744230389595032\n",
      "Eval loss 0.8892210125923157, R2 0.6786360144615173\n",
      "epoch 1538, loss 0.8526650667190552, R2 0.6744548678398132\n",
      "Eval loss 0.8891303539276123, R2 0.6786720156669617\n",
      "epoch 1539, loss 0.8525896668434143, R2 0.6744866967201233\n",
      "Eval loss 0.8890399932861328, R2 0.6787078976631165\n",
      "epoch 1540, loss 0.8525145053863525, R2 0.6745185256004333\n",
      "Eval loss 0.8889497518539429, R2 0.6787437796592712\n",
      "epoch 1541, loss 0.8524395227432251, R2 0.6745501756668091\n",
      "Eval loss 0.888859748840332, R2 0.6787795424461365\n",
      "epoch 1542, loss 0.8523646593093872, R2 0.6745817065238953\n",
      "Eval loss 0.8887698650360107, R2 0.6788151860237122\n",
      "epoch 1543, loss 0.8522899746894836, R2 0.6746132373809814\n",
      "Eval loss 0.8886803984642029, R2 0.6788507699966431\n",
      "epoch 1544, loss 0.852215588092804, R2 0.6746446490287781\n",
      "Eval loss 0.8885910511016846, R2 0.678886353969574\n",
      "epoch 1545, loss 0.8521413803100586, R2 0.6746761202812195\n",
      "Eval loss 0.8885018229484558, R2 0.6789217591285706\n",
      "epoch 1546, loss 0.852067232131958, R2 0.6747074723243713\n",
      "Eval loss 0.8884127736091614, R2 0.6789571642875671\n",
      "epoch 1547, loss 0.8519933819770813, R2 0.6747387051582336\n",
      "Eval loss 0.8883240222930908, R2 0.6789924502372742\n",
      "epoch 1548, loss 0.8519197106361389, R2 0.674769937992096\n",
      "Eval loss 0.8882354497909546, R2 0.6790277361869812\n",
      "epoch 1549, loss 0.8518462181091309, R2 0.6748009324073792\n",
      "Eval loss 0.8881471753120422, R2 0.6790628433227539\n",
      "epoch 1550, loss 0.8517729043960571, R2 0.6748319864273071\n",
      "Eval loss 0.8880590200424194, R2 0.6790979504585266\n",
      "epoch 1551, loss 0.8516996502876282, R2 0.6748629212379456\n",
      "Eval loss 0.8879711031913757, R2 0.679132878780365\n",
      "epoch 1552, loss 0.8516268134117126, R2 0.6748939156532288\n",
      "Eval loss 0.8878833651542664, R2 0.6791678667068481\n",
      "epoch 1553, loss 0.8515539765357971, R2 0.6749247312545776\n",
      "Eval loss 0.8877958059310913, R2 0.679202675819397\n",
      "epoch 1554, loss 0.8514813780784607, R2 0.674955427646637\n",
      "Eval loss 0.8877083659172058, R2 0.679237425327301\n",
      "epoch 1555, loss 0.8514089584350586, R2 0.6749861240386963\n",
      "Eval loss 0.8876213431358337, R2 0.6792722344398499\n",
      "epoch 1556, loss 0.851336658000946, R2 0.6750167012214661\n",
      "Eval loss 0.8875343203544617, R2 0.6793068647384644\n",
      "epoch 1557, loss 0.8512647151947021, R2 0.6750473380088806\n",
      "Eval loss 0.8874476552009583, R2 0.6793414354324341\n",
      "epoch 1558, loss 0.851192831993103, R2 0.6750777363777161\n",
      "Eval loss 0.8873611688613892, R2 0.6793758869171143\n",
      "epoch 1559, loss 0.851121187210083, R2 0.6751081943511963\n",
      "Eval loss 0.8872748613357544, R2 0.6794103384017944\n",
      "epoch 1560, loss 0.8510496020317078, R2 0.6751384735107422\n",
      "Eval loss 0.8871886730194092, R2 0.6794446706771851\n",
      "epoch 1561, loss 0.8509781956672668, R2 0.6751687526702881\n",
      "Eval loss 0.8871027231216431, R2 0.6794789433479309\n",
      "epoch 1562, loss 0.8509071469306946, R2 0.6751989126205444\n",
      "Eval loss 0.887017011642456, R2 0.6795130968093872\n",
      "epoch 1563, loss 0.8508360981941223, R2 0.6752290725708008\n",
      "Eval loss 0.8869314789772034, R2 0.6795473098754883\n",
      "epoch 1564, loss 0.8507653474807739, R2 0.6752591729164124\n",
      "Eval loss 0.886846125125885, R2 0.679581344127655\n",
      "epoch 1565, loss 0.8506946563720703, R2 0.6752891540527344\n",
      "Eval loss 0.8867610096931458, R2 0.6796151399612427\n",
      "epoch 1566, loss 0.8506242632865906, R2 0.6753190755844116\n",
      "Eval loss 0.8866760730743408, R2 0.6796492338180542\n",
      "epoch 1567, loss 0.8505539894104004, R2 0.6753488779067993\n",
      "Eval loss 0.886591374874115, R2 0.6796830296516418\n",
      "epoch 1568, loss 0.8504838347434998, R2 0.6753787994384766\n",
      "Eval loss 0.8865067362785339, R2 0.6797167658805847\n",
      "epoch 1569, loss 0.8504140377044678, R2 0.6754084825515747\n",
      "Eval loss 0.8864223957061768, R2 0.6797505021095276\n",
      "epoch 1570, loss 0.850344181060791, R2 0.6754381656646729\n",
      "Eval loss 0.8863382339477539, R2 0.6797840595245361\n",
      "epoch 1571, loss 0.8502746820449829, R2 0.6754676699638367\n",
      "Eval loss 0.8862542510032654, R2 0.6798176765441895\n",
      "epoch 1572, loss 0.8502053022384644, R2 0.6754972338676453\n",
      "Eval loss 0.886170506477356, R2 0.6798511147499084\n",
      "epoch 1573, loss 0.8501361012458801, R2 0.6755267381668091\n",
      "Eval loss 0.8860868215560913, R2 0.6798845529556274\n",
      "epoch 1574, loss 0.8500669598579407, R2 0.6755560636520386\n",
      "Eval loss 0.8860034346580505, R2 0.6799178719520569\n",
      "epoch 1575, loss 0.8499981164932251, R2 0.6755853891372681\n",
      "Eval loss 0.8859202265739441, R2 0.6799511313438416\n",
      "epoch 1576, loss 0.8499293327331543, R2 0.6756146550178528\n",
      "Eval loss 0.885837197303772, R2 0.6799843311309814\n",
      "epoch 1577, loss 0.8498609066009521, R2 0.6756438612937927\n",
      "Eval loss 0.8857543468475342, R2 0.6800175309181213\n",
      "epoch 1578, loss 0.8497925400733948, R2 0.6756729483604431\n",
      "Eval loss 0.8856717944145203, R2 0.6800506114959717\n",
      "epoch 1579, loss 0.8497241735458374, R2 0.6757020354270935\n",
      "Eval loss 0.8855891823768616, R2 0.6800835728645325\n",
      "epoch 1580, loss 0.8496562242507935, R2 0.6757310628890991\n",
      "Eval loss 0.8855069875717163, R2 0.6801164746284485\n",
      "epoch 1581, loss 0.8495883941650391, R2 0.6757599115371704\n",
      "Eval loss 0.8854249715805054, R2 0.6801493167877197\n",
      "epoch 1582, loss 0.8495206832885742, R2 0.6757888197898865\n",
      "Eval loss 0.8853430151939392, R2 0.6801820993423462\n",
      "epoch 1583, loss 0.8494531512260437, R2 0.6758175492286682\n",
      "Eval loss 0.8852614164352417, R2 0.6802148222923279\n",
      "epoch 1584, loss 0.8493857979774475, R2 0.67584627866745\n",
      "Eval loss 0.885179877281189, R2 0.68024742603302\n",
      "epoch 1585, loss 0.8493186831474304, R2 0.6758750081062317\n",
      "Eval loss 0.8850984573364258, R2 0.6802800297737122\n",
      "epoch 1586, loss 0.8492515683174133, R2 0.6759035587310791\n",
      "Eval loss 0.8850173354148865, R2 0.6803126335144043\n",
      "epoch 1587, loss 0.8491847515106201, R2 0.6759321689605713\n",
      "Eval loss 0.8849363923072815, R2 0.6803450584411621\n",
      "epoch 1588, loss 0.8491179943084717, R2 0.6759606003761292\n",
      "Eval loss 0.8848556280136108, R2 0.6803773641586304\n",
      "epoch 1589, loss 0.8490514159202576, R2 0.6759890913963318\n",
      "Eval loss 0.8847751021385193, R2 0.6804097294807434\n",
      "epoch 1590, loss 0.8489850759506226, R2 0.6760173439979553\n",
      "Eval loss 0.8846945762634277, R2 0.6804419755935669\n",
      "epoch 1591, loss 0.8489189743995667, R2 0.6760455965995789\n",
      "Eval loss 0.8846143484115601, R2 0.6804741024971008\n",
      "epoch 1592, loss 0.848852813243866, R2 0.6760738492012024\n",
      "Eval loss 0.8845343589782715, R2 0.68050616979599\n",
      "epoch 1593, loss 0.8487870693206787, R2 0.6761019825935364\n",
      "Eval loss 0.884454607963562, R2 0.6805382370948792\n",
      "epoch 1594, loss 0.8487212657928467, R2 0.6761300563812256\n",
      "Eval loss 0.8843747973442078, R2 0.6805701851844788\n",
      "epoch 1595, loss 0.8486557006835938, R2 0.6761581301689148\n",
      "Eval loss 0.8842953443527222, R2 0.6806021928787231\n",
      "epoch 1596, loss 0.8485903739929199, R2 0.6761861443519592\n",
      "Eval loss 0.8842160105705261, R2 0.6806339621543884\n",
      "epoch 1597, loss 0.8485250473022461, R2 0.6762139797210693\n",
      "Eval loss 0.8841369152069092, R2 0.6806657314300537\n",
      "epoch 1598, loss 0.8484600186347961, R2 0.6762418746948242\n",
      "Eval loss 0.8840579390525818, R2 0.6806974411010742\n",
      "epoch 1599, loss 0.8483951687812805, R2 0.6762695908546448\n",
      "Eval loss 0.883979082107544, R2 0.6807291507720947\n",
      "epoch 1600, loss 0.8483303785324097, R2 0.6762973666191101\n",
      "Eval loss 0.8839006423950195, R2 0.6807606816291809\n",
      "epoch 1601, loss 0.8482658267021179, R2 0.6763250827789307\n",
      "Eval loss 0.8838221430778503, R2 0.6807922124862671\n",
      "epoch 1602, loss 0.8482013940811157, R2 0.6763526201248169\n",
      "Eval loss 0.8837438225746155, R2 0.6808236241340637\n",
      "epoch 1603, loss 0.8481371998786926, R2 0.6763801574707031\n",
      "Eval loss 0.8836658596992493, R2 0.6808550357818604\n",
      "epoch 1604, loss 0.8480730652809143, R2 0.6764076352119446\n",
      "Eval loss 0.8835879564285278, R2 0.6808862686157227\n",
      "epoch 1605, loss 0.8480091094970703, R2 0.6764350533485413\n",
      "Eval loss 0.883510172367096, R2 0.6809175610542297\n",
      "epoch 1606, loss 0.8479453325271606, R2 0.6764624118804932\n",
      "Eval loss 0.8834325671195984, R2 0.6809487342834473\n",
      "epoch 1607, loss 0.8478816151618958, R2 0.6764896512031555\n",
      "Eval loss 0.8833553791046143, R2 0.6809799075126648\n",
      "epoch 1608, loss 0.84781813621521, R2 0.6765168905258179\n",
      "Eval loss 0.8832780718803406, R2 0.681010901927948\n",
      "epoch 1609, loss 0.8477548360824585, R2 0.6765440702438354\n",
      "Eval loss 0.8832011222839355, R2 0.6810418963432312\n",
      "epoch 1610, loss 0.8476915955543518, R2 0.6765711307525635\n",
      "Eval loss 0.8831242918968201, R2 0.6810728311538696\n",
      "epoch 1611, loss 0.847628653049469, R2 0.6765981912612915\n",
      "Eval loss 0.8830475807189941, R2 0.6811036467552185\n",
      "epoch 1612, loss 0.847565770149231, R2 0.6766251921653748\n",
      "Eval loss 0.8829711079597473, R2 0.6811344623565674\n",
      "epoch 1613, loss 0.8475030660629272, R2 0.6766520738601685\n",
      "Eval loss 0.8828948140144348, R2 0.6811652779579163\n",
      "epoch 1614, loss 0.8474404811859131, R2 0.6766789555549622\n",
      "Eval loss 0.8828186988830566, R2 0.6811959147453308\n",
      "epoch 1615, loss 0.8473781943321228, R2 0.6767057180404663\n",
      "Eval loss 0.882742702960968, R2 0.6812264323234558\n",
      "epoch 1616, loss 0.8473158478736877, R2 0.6767325401306152\n",
      "Eval loss 0.8826667666435242, R2 0.6812569499015808\n",
      "epoch 1617, loss 0.8472537994384766, R2 0.6767591834068298\n",
      "Eval loss 0.882591187953949, R2 0.6812874674797058\n",
      "epoch 1618, loss 0.8471918702125549, R2 0.6767858266830444\n",
      "Eval loss 0.8825157880783081, R2 0.6813178658485413\n",
      "epoch 1619, loss 0.8471300601959229, R2 0.6768124103546143\n",
      "Eval loss 0.8824403882026672, R2 0.6813482046127319\n",
      "epoch 1620, loss 0.8470684289932251, R2 0.6768388748168945\n",
      "Eval loss 0.8823652863502502, R2 0.6813784837722778\n",
      "epoch 1621, loss 0.8470069169998169, R2 0.67686527967453\n",
      "Eval loss 0.8822903633117676, R2 0.6814087629318237\n",
      "epoch 1622, loss 0.8469454646110535, R2 0.6768916845321655\n",
      "Eval loss 0.8822155594825745, R2 0.6814388632774353\n",
      "epoch 1623, loss 0.8468844294548035, R2 0.6769180297851562\n",
      "Eval loss 0.8821408748626709, R2 0.6814689636230469\n",
      "epoch 1624, loss 0.8468233346939087, R2 0.6769443154335022\n",
      "Eval loss 0.8820664882659912, R2 0.6814990043640137\n",
      "epoch 1625, loss 0.8467623591423035, R2 0.6769704818725586\n",
      "Eval loss 0.8819921612739563, R2 0.6815290451049805\n",
      "epoch 1626, loss 0.8467016816139221, R2 0.676996648311615\n",
      "Eval loss 0.8819180727005005, R2 0.6815589666366577\n",
      "epoch 1627, loss 0.8466410636901855, R2 0.6770227551460266\n",
      "Eval loss 0.8818441033363342, R2 0.6815887093544006\n",
      "epoch 1628, loss 0.8465806841850281, R2 0.6770487427711487\n",
      "Eval loss 0.8817703723907471, R2 0.6816185712814331\n",
      "epoch 1629, loss 0.8465203642845154, R2 0.6770747900009155\n",
      "Eval loss 0.8816967010498047, R2 0.681648313999176\n",
      "epoch 1630, loss 0.846460223197937, R2 0.6771007180213928\n",
      "Eval loss 0.8816232085227966, R2 0.6816779971122742\n",
      "epoch 1631, loss 0.846400260925293, R2 0.6771265268325806\n",
      "Eval loss 0.8815498948097229, R2 0.6817075610160828\n",
      "epoch 1632, loss 0.8463402986526489, R2 0.6771523952484131\n",
      "Eval loss 0.8814768195152283, R2 0.6817371249198914\n",
      "epoch 1633, loss 0.8462806344032288, R2 0.6771780252456665\n",
      "Eval loss 0.8814038634300232, R2 0.6817665696144104\n",
      "epoch 1634, loss 0.8462210893630981, R2 0.6772037744522095\n",
      "Eval loss 0.8813309669494629, R2 0.6817960143089294\n",
      "epoch 1635, loss 0.8461616039276123, R2 0.6772294640541077\n",
      "Eval loss 0.8812583684921265, R2 0.6818253993988037\n",
      "epoch 1636, loss 0.8461024761199951, R2 0.6772549748420715\n",
      "Eval loss 0.8811858892440796, R2 0.6818546652793884\n",
      "epoch 1637, loss 0.8460432291030884, R2 0.6772804856300354\n",
      "Eval loss 0.8811135292053223, R2 0.6818839311599731\n",
      "epoch 1638, loss 0.8459842801094055, R2 0.6773059964179993\n",
      "Eval loss 0.881041407585144, R2 0.6819131374359131\n",
      "epoch 1639, loss 0.845925509929657, R2 0.6773313283920288\n",
      "Eval loss 0.8809694051742554, R2 0.6819422245025635\n",
      "epoch 1640, loss 0.8458667397499084, R2 0.6773567199707031\n",
      "Eval loss 0.8808976411819458, R2 0.6819713115692139\n",
      "epoch 1641, loss 0.8458081483840942, R2 0.6773820519447327\n",
      "Eval loss 0.8808258771896362, R2 0.6820003390312195\n",
      "epoch 1642, loss 0.8457497358322144, R2 0.6774072647094727\n",
      "Eval loss 0.8807543516159058, R2 0.6820292472839355\n",
      "epoch 1643, loss 0.8456915020942688, R2 0.6774324178695679\n",
      "Eval loss 0.8806829452514648, R2 0.6820581555366516\n",
      "epoch 1644, loss 0.8456332683563232, R2 0.6774575710296631\n",
      "Eval loss 0.8806117177009583, R2 0.6820869445800781\n",
      "epoch 1645, loss 0.8455753922462463, R2 0.6774826645851135\n",
      "Eval loss 0.880540668964386, R2 0.6821157336235046\n",
      "epoch 1646, loss 0.8455174565315247, R2 0.6775076389312744\n",
      "Eval loss 0.8804698586463928, R2 0.6821444034576416\n",
      "epoch 1647, loss 0.8454597592353821, R2 0.6775326132774353\n",
      "Eval loss 0.8803991675376892, R2 0.6821730136871338\n",
      "epoch 1648, loss 0.8454022407531738, R2 0.6775574684143066\n",
      "Eval loss 0.8803284764289856, R2 0.682201623916626\n",
      "epoch 1649, loss 0.8453448414802551, R2 0.6775823831558228\n",
      "Eval loss 0.8802582025527954, R2 0.6822302341461182\n",
      "epoch 1650, loss 0.8452874422073364, R2 0.6776071786880493\n",
      "Eval loss 0.8801878690719604, R2 0.682258665561676\n",
      "epoch 1651, loss 0.8452302813529968, R2 0.6776319146156311\n",
      "Eval loss 0.8801178336143494, R2 0.6822870373725891\n",
      "epoch 1652, loss 0.8451732993125916, R2 0.6776565909385681\n",
      "Eval loss 0.8800478577613831, R2 0.6823154091835022\n",
      "epoch 1653, loss 0.845116376876831, R2 0.6776812076568604\n",
      "Eval loss 0.8799780011177063, R2 0.6823437213897705\n",
      "epoch 1654, loss 0.8450597524642944, R2 0.6777058243751526\n",
      "Eval loss 0.8799084424972534, R2 0.6823719143867493\n",
      "epoch 1655, loss 0.845003068447113, R2 0.6777303218841553\n",
      "Eval loss 0.8798389434814453, R2 0.6824001669883728\n",
      "epoch 1656, loss 0.844946563243866, R2 0.6777548789978027\n",
      "Eval loss 0.8797696232795715, R2 0.6824283003807068\n",
      "epoch 1657, loss 0.8448902368545532, R2 0.6777791976928711\n",
      "Eval loss 0.8797004222869873, R2 0.682456374168396\n",
      "epoch 1658, loss 0.8448341488838196, R2 0.6778035163879395\n",
      "Eval loss 0.8796313405036926, R2 0.6824843287467957\n",
      "epoch 1659, loss 0.8447780013084412, R2 0.6778278946876526\n",
      "Eval loss 0.879562497138977, R2 0.6825123429298401\n",
      "epoch 1660, loss 0.8447220921516418, R2 0.677852213382721\n",
      "Eval loss 0.8794938325881958, R2 0.6825401782989502\n",
      "epoch 1661, loss 0.8446663618087769, R2 0.677876353263855\n",
      "Eval loss 0.8794252276420593, R2 0.6825680732727051\n",
      "epoch 1662, loss 0.8446106314659119, R2 0.6779004335403442\n",
      "Eval loss 0.879356861114502, R2 0.6825958490371704\n",
      "epoch 1663, loss 0.8445552587509155, R2 0.677924633026123\n",
      "Eval loss 0.8792884945869446, R2 0.682623565196991\n",
      "epoch 1664, loss 0.8444996476173401, R2 0.6779486536979675\n",
      "Eval loss 0.8792204856872559, R2 0.6826512813568115\n",
      "epoch 1665, loss 0.8444445133209229, R2 0.6779726147651672\n",
      "Eval loss 0.8791524171829224, R2 0.6826788187026978\n",
      "epoch 1666, loss 0.8443893790245056, R2 0.6779965162277222\n",
      "Eval loss 0.8790846467018127, R2 0.6827064156532288\n",
      "epoch 1667, loss 0.8443344831466675, R2 0.6780204772949219\n",
      "Eval loss 0.8790170550346375, R2 0.682733952999115\n",
      "epoch 1668, loss 0.8442795276641846, R2 0.678044319152832\n",
      "Eval loss 0.8789494633674622, R2 0.6827613711357117\n",
      "epoch 1669, loss 0.844224750995636, R2 0.6780680418014526\n",
      "Eval loss 0.8788821697235107, R2 0.6827887892723083\n",
      "epoch 1670, loss 0.8441702127456665, R2 0.678091824054718\n",
      "Eval loss 0.8788148760795593, R2 0.6828160285949707\n",
      "epoch 1671, loss 0.8441157341003418, R2 0.6781154274940491\n",
      "Eval loss 0.878747820854187, R2 0.6828433871269226\n",
      "epoch 1672, loss 0.8440614938735962, R2 0.6781390905380249\n",
      "Eval loss 0.8786808848381042, R2 0.6828705668449402\n",
      "epoch 1673, loss 0.8440072536468506, R2 0.678162693977356\n",
      "Eval loss 0.8786141276359558, R2 0.6828978061676025\n",
      "epoch 1674, loss 0.8439531326293945, R2 0.6781861782073975\n",
      "Eval loss 0.8785474896430969, R2 0.6829248666763306\n",
      "epoch 1675, loss 0.8438992500305176, R2 0.678209662437439\n",
      "Eval loss 0.8784809112548828, R2 0.6829519271850586\n",
      "epoch 1676, loss 0.8438453674316406, R2 0.6782330274581909\n",
      "Eval loss 0.8784146308898926, R2 0.6829789876937866\n",
      "epoch 1677, loss 0.8437917828559875, R2 0.6782564520835876\n",
      "Eval loss 0.8783484697341919, R2 0.6830059289932251\n",
      "epoch 1678, loss 0.8437381982803345, R2 0.6782797574996948\n",
      "Eval loss 0.8782824277877808, R2 0.683032751083374\n",
      "epoch 1679, loss 0.8436847925186157, R2 0.6783030033111572\n",
      "Eval loss 0.878216564655304, R2 0.6830596327781677\n",
      "epoch 1680, loss 0.8436315655708313, R2 0.6783262491226196\n",
      "Eval loss 0.8781508207321167, R2 0.6830863952636719\n",
      "epoch 1681, loss 0.8435782790184021, R2 0.6783494353294373\n",
      "Eval loss 0.878085196018219, R2 0.6831130981445312\n",
      "epoch 1682, loss 0.843525230884552, R2 0.6783725023269653\n",
      "Eval loss 0.8780196905136108, R2 0.6831398606300354\n",
      "epoch 1683, loss 0.8434723615646362, R2 0.6783955097198486\n",
      "Eval loss 0.8779544234275818, R2 0.6831664443016052\n",
      "epoch 1684, loss 0.84341961145401, R2 0.6784185171127319\n",
      "Eval loss 0.8778891563415527, R2 0.683193027973175\n",
      "epoch 1685, loss 0.8433669209480286, R2 0.6784414052963257\n",
      "Eval loss 0.8778241276741028, R2 0.6832194924354553\n",
      "epoch 1686, loss 0.8433144092559814, R2 0.6784644722938538\n",
      "Eval loss 0.8777591586112976, R2 0.6832460165023804\n",
      "epoch 1687, loss 0.8432620167732239, R2 0.6784873604774475\n",
      "Eval loss 0.8776944279670715, R2 0.6832723617553711\n",
      "epoch 1688, loss 0.8432096838951111, R2 0.6785101294517517\n",
      "Eval loss 0.8776298761367798, R2 0.6832987666130066\n",
      "epoch 1689, loss 0.8431575298309326, R2 0.6785328388214111\n",
      "Eval loss 0.8775653839111328, R2 0.6833250522613525\n",
      "epoch 1690, loss 0.8431055545806885, R2 0.6785555481910706\n",
      "Eval loss 0.8775010704994202, R2 0.6833512783050537\n",
      "epoch 1691, loss 0.8430535793304443, R2 0.6785781979560852\n",
      "Eval loss 0.8774369359016418, R2 0.6833774447441101\n",
      "epoch 1692, loss 0.8430018424987793, R2 0.6786008477210999\n",
      "Eval loss 0.8773728013038635, R2 0.6834036707878113\n",
      "epoch 1693, loss 0.8429502248764038, R2 0.678623378276825\n",
      "Eval loss 0.8773088455200195, R2 0.6834296584129333\n",
      "epoch 1694, loss 0.8428986072540283, R2 0.67864590883255\n",
      "Eval loss 0.8772450685501099, R2 0.683455765247345\n",
      "epoch 1695, loss 0.8428472280502319, R2 0.6786683797836304\n",
      "Eval loss 0.8771815299987793, R2 0.6834818124771118\n",
      "epoch 1696, loss 0.8427959680557251, R2 0.6786907911300659\n",
      "Eval loss 0.8771179914474487, R2 0.6835076808929443\n",
      "epoch 1697, loss 0.8427447080612183, R2 0.6787131428718567\n",
      "Eval loss 0.8770546913146973, R2 0.6835335493087769\n",
      "epoch 1698, loss 0.8426937460899353, R2 0.6787354946136475\n",
      "Eval loss 0.8769914507865906, R2 0.6835594177246094\n",
      "epoch 1699, loss 0.8426427841186523, R2 0.6787577867507935\n",
      "Eval loss 0.8769282102584839, R2 0.6835851669311523\n",
      "epoch 1700, loss 0.8425919413566589, R2 0.6787799596786499\n",
      "Eval loss 0.8768653869628906, R2 0.6836109161376953\n",
      "epoch 1701, loss 0.8425412178039551, R2 0.6788021326065063\n",
      "Eval loss 0.8768025040626526, R2 0.6836365461349487\n",
      "epoch 1702, loss 0.8424907326698303, R2 0.678824245929718\n",
      "Eval loss 0.8767397999763489, R2 0.6836622357368469\n",
      "epoch 1703, loss 0.8424403071403503, R2 0.6788462996482849\n",
      "Eval loss 0.8766773343086243, R2 0.6836877465248108\n",
      "epoch 1704, loss 0.8423898816108704, R2 0.678868293762207\n",
      "Eval loss 0.8766149878501892, R2 0.6837133169174194\n",
      "epoch 1705, loss 0.8423397541046143, R2 0.6788902878761292\n",
      "Eval loss 0.8765525817871094, R2 0.6837387681007385\n",
      "epoch 1706, loss 0.8422896862030029, R2 0.6789122223854065\n",
      "Eval loss 0.8764905333518982, R2 0.6837641596794128\n",
      "epoch 1707, loss 0.8422396183013916, R2 0.6789340972900391\n",
      "Eval loss 0.876428484916687, R2 0.6837896108627319\n",
      "epoch 1708, loss 0.8421898484230042, R2 0.6789559721946716\n",
      "Eval loss 0.8763666152954102, R2 0.6838148236274719\n",
      "epoch 1709, loss 0.8421400785446167, R2 0.6789777278900146\n",
      "Eval loss 0.8763049840927124, R2 0.6838400959968567\n",
      "epoch 1710, loss 0.8420904874801636, R2 0.6789994835853577\n",
      "Eval loss 0.8762433528900146, R2 0.6838653683662415\n",
      "epoch 1711, loss 0.8420410752296448, R2 0.6790212392807007\n",
      "Eval loss 0.8761818408966064, R2 0.6838905215263367\n",
      "epoch 1712, loss 0.841991662979126, R2 0.6790428161621094\n",
      "Eval loss 0.8761204481124878, R2 0.6839156746864319\n",
      "epoch 1713, loss 0.8419424295425415, R2 0.6790643930435181\n",
      "Eval loss 0.8760592937469482, R2 0.6839407086372375\n",
      "epoch 1714, loss 0.8418932557106018, R2 0.6790859699249268\n",
      "Eval loss 0.8759981989860535, R2 0.6839656829833984\n",
      "epoch 1715, loss 0.8418442010879517, R2 0.6791075468063354\n",
      "Eval loss 0.8759374022483826, R2 0.6839907169342041\n",
      "epoch 1716, loss 0.8417954444885254, R2 0.6791290044784546\n",
      "Eval loss 0.8758764266967773, R2 0.6840155720710754\n",
      "epoch 1717, loss 0.8417465686798096, R2 0.679150402545929\n",
      "Eval loss 0.8758158683776855, R2 0.6840404272079468\n",
      "epoch 1718, loss 0.8416978716850281, R2 0.6791717410087585\n",
      "Eval loss 0.8757553696632385, R2 0.6840652823448181\n",
      "epoch 1719, loss 0.8416492342948914, R2 0.6791930794715881\n",
      "Eval loss 0.8756949305534363, R2 0.6840900778770447\n",
      "epoch 1720, loss 0.8416008353233337, R2 0.6792144179344177\n",
      "Eval loss 0.8756346106529236, R2 0.6841147541999817\n",
      "epoch 1721, loss 0.8415524959564209, R2 0.679235577583313\n",
      "Eval loss 0.8755744695663452, R2 0.6841394305229187\n",
      "epoch 1722, loss 0.8415043354034424, R2 0.679256796836853\n",
      "Eval loss 0.8755144476890564, R2 0.6841641068458557\n",
      "epoch 1723, loss 0.8414561152458191, R2 0.6792778968811035\n",
      "Eval loss 0.8754546046257019, R2 0.6841886639595032\n",
      "epoch 1724, loss 0.8414080739021301, R2 0.679298996925354\n",
      "Eval loss 0.8753948211669922, R2 0.6842131018638611\n",
      "epoch 1725, loss 0.8413602709770203, R2 0.6793200373649597\n",
      "Eval loss 0.8753350973129272, R2 0.6842376589775085\n",
      "epoch 1726, loss 0.8413125276565552, R2 0.6793410181999207\n",
      "Eval loss 0.8752756118774414, R2 0.6842620968818665\n",
      "epoch 1727, loss 0.8412649035453796, R2 0.6793619990348816\n",
      "Eval loss 0.8752163052558899, R2 0.6842864751815796\n",
      "epoch 1728, loss 0.8412172794342041, R2 0.6793829202651978\n",
      "Eval loss 0.8751569986343384, R2 0.684310793876648\n",
      "epoch 1729, loss 0.8411698937416077, R2 0.6794037818908691\n",
      "Eval loss 0.875097930431366, R2 0.6843349933624268\n",
      "epoch 1730, loss 0.8411226272583008, R2 0.6794245839118958\n",
      "Eval loss 0.8750388622283936, R2 0.6843592524528503\n",
      "epoch 1731, loss 0.8410754203796387, R2 0.6794453263282776\n",
      "Eval loss 0.8749800324440002, R2 0.6843833923339844\n",
      "epoch 1732, loss 0.8410283327102661, R2 0.6794661283493042\n",
      "Eval loss 0.8749212622642517, R2 0.6844075918197632\n",
      "epoch 1733, loss 0.8409813046455383, R2 0.6794868111610413\n",
      "Eval loss 0.8748626708984375, R2 0.6844315528869629\n",
      "epoch 1734, loss 0.8409343957901001, R2 0.6795074343681335\n",
      "Eval loss 0.8748041391372681, R2 0.6844556927680969\n",
      "epoch 1735, loss 0.840887725353241, R2 0.6795280575752258\n",
      "Eval loss 0.8747457265853882, R2 0.6844797730445862\n",
      "epoch 1736, loss 0.8408409953117371, R2 0.6795485615730286\n",
      "Eval loss 0.8746874928474426, R2 0.6845036149024963\n",
      "epoch 1737, loss 0.8407945036888123, R2 0.6795690655708313\n",
      "Eval loss 0.8746293783187866, R2 0.6845275163650513\n",
      "epoch 1738, loss 0.8407480120658875, R2 0.6795895099639893\n",
      "Eval loss 0.8745713829994202, R2 0.6845513582229614\n",
      "epoch 1739, loss 0.8407018780708313, R2 0.6796099543571472\n",
      "Eval loss 0.8745136260986328, R2 0.6845752000808716\n",
      "epoch 1740, loss 0.840655505657196, R2 0.6796303391456604\n",
      "Eval loss 0.8744558691978455, R2 0.684598982334137\n",
      "epoch 1741, loss 0.8406093716621399, R2 0.6796506643295288\n",
      "Eval loss 0.8743982315063477, R2 0.6846226453781128\n",
      "epoch 1742, loss 0.8405634164810181, R2 0.6796709895133972\n",
      "Eval loss 0.8743406534194946, R2 0.6846462488174438\n",
      "epoch 1743, loss 0.840517520904541, R2 0.6796912550926208\n",
      "Eval loss 0.8742832541465759, R2 0.6846698522567749\n",
      "epoch 1744, loss 0.8404716849327087, R2 0.6797114014625549\n",
      "Eval loss 0.8742260336875916, R2 0.684693455696106\n",
      "epoch 1745, loss 0.8404260277748108, R2 0.679731547832489\n",
      "Eval loss 0.874168872833252, R2 0.6847169995307922\n",
      "epoch 1746, loss 0.8403804302215576, R2 0.6797517538070679\n",
      "Eval loss 0.8741118311882019, R2 0.6847404837608337\n",
      "epoch 1747, loss 0.8403350114822388, R2 0.6797717809677124\n",
      "Eval loss 0.874055027961731, R2 0.6847638487815857\n",
      "epoch 1748, loss 0.8402897119522095, R2 0.6797918081283569\n",
      "Eval loss 0.8739981651306152, R2 0.6847872138023376\n",
      "epoch 1749, loss 0.840244472026825, R2 0.6798117756843567\n",
      "Eval loss 0.8739416599273682, R2 0.6848106384277344\n",
      "epoch 1750, loss 0.8401992917060852, R2 0.6798316836357117\n",
      "Eval loss 0.8738849759101868, R2 0.6848339438438416\n",
      "epoch 1751, loss 0.840154230594635, R2 0.6798515915870667\n",
      "Eval loss 0.873828649520874, R2 0.6848571300506592\n",
      "epoch 1752, loss 0.8401093482971191, R2 0.6798714399337769\n",
      "Eval loss 0.8737723231315613, R2 0.6848803758621216\n",
      "epoch 1753, loss 0.840064525604248, R2 0.6798913478851318\n",
      "Eval loss 0.8737162351608276, R2 0.6849035024642944\n",
      "epoch 1754, loss 0.840019702911377, R2 0.6799110174179077\n",
      "Eval loss 0.873660147190094, R2 0.6849266290664673\n",
      "epoch 1755, loss 0.839975118637085, R2 0.6799307465553284\n",
      "Eval loss 0.8736041188240051, R2 0.6849496960639954\n",
      "epoch 1756, loss 0.8399305939674377, R2 0.6799505352973938\n",
      "Eval loss 0.8735484480857849, R2 0.6849727034568787\n",
      "epoch 1757, loss 0.8398861885070801, R2 0.6799702048301697\n",
      "Eval loss 0.8734927177429199, R2 0.6849956512451172\n",
      "epoch 1758, loss 0.8398419618606567, R2 0.679989755153656\n",
      "Eval loss 0.8734371662139893, R2 0.6850184798240662\n",
      "epoch 1759, loss 0.8397976756095886, R2 0.6800092458724976\n",
      "Eval loss 0.8733815550804138, R2 0.6850414872169495\n",
      "epoch 1760, loss 0.8397536277770996, R2 0.6800287961959839\n",
      "Eval loss 0.873326301574707, R2 0.6850642561912537\n",
      "epoch 1761, loss 0.8397095799446106, R2 0.6800482273101807\n",
      "Eval loss 0.8732710480690002, R2 0.6850870251655579\n",
      "epoch 1762, loss 0.8396657109260559, R2 0.6800677180290222\n",
      "Eval loss 0.8732160329818726, R2 0.6851098537445068\n",
      "epoch 1763, loss 0.839621901512146, R2 0.6800870895385742\n",
      "Eval loss 0.8731609582901001, R2 0.6851325631141663\n",
      "epoch 1764, loss 0.8395781517028809, R2 0.680106520652771\n",
      "Eval loss 0.8731061220169067, R2 0.6851551532745361\n",
      "epoch 1765, loss 0.8395346403121948, R2 0.6801258325576782\n",
      "Eval loss 0.8730514049530029, R2 0.6851778030395508\n",
      "epoch 1766, loss 0.839491069316864, R2 0.6801450252532959\n",
      "Eval loss 0.8729966282844543, R2 0.6852003335952759\n",
      "epoch 1767, loss 0.8394477367401123, R2 0.6801642775535583\n",
      "Eval loss 0.8729422092437744, R2 0.685222864151001\n",
      "epoch 1768, loss 0.8394044041633606, R2 0.680183470249176\n",
      "Eval loss 0.8728877902030945, R2 0.6852453947067261\n",
      "epoch 1769, loss 0.8393613696098328, R2 0.6802026033401489\n",
      "Eval loss 0.8728334903717041, R2 0.6852678060531616\n",
      "epoch 1770, loss 0.8393182158470154, R2 0.6802217364311218\n",
      "Eval loss 0.8727792501449585, R2 0.6852902173995972\n",
      "epoch 1771, loss 0.8392752408981323, R2 0.68024080991745\n",
      "Eval loss 0.8727253079414368, R2 0.6853125095367432\n",
      "epoch 1772, loss 0.8392323851585388, R2 0.6802597641944885\n",
      "Eval loss 0.8726713061332703, R2 0.6853348612785339\n",
      "epoch 1773, loss 0.8391895890235901, R2 0.6802787780761719\n",
      "Eval loss 0.8726173639297485, R2 0.6853570938110352\n",
      "epoch 1774, loss 0.8391469120979309, R2 0.6802977919578552\n",
      "Eval loss 0.8725637793540955, R2 0.6853792667388916\n",
      "epoch 1775, loss 0.8391042947769165, R2 0.6803166270256042\n",
      "Eval loss 0.8725100755691528, R2 0.6854014992713928\n",
      "epoch 1776, loss 0.8390617966651917, R2 0.6803355813026428\n",
      "Eval loss 0.8724565505981445, R2 0.6854236721992493\n",
      "epoch 1777, loss 0.8390194177627563, R2 0.6803543567657471\n",
      "Eval loss 0.8724031448364258, R2 0.6854457259178162\n",
      "epoch 1778, loss 0.8389770984649658, R2 0.6803731918334961\n",
      "Eval loss 0.8723499178886414, R2 0.6854677200317383\n",
      "epoch 1779, loss 0.8389349579811096, R2 0.6803919076919556\n",
      "Eval loss 0.8722966909408569, R2 0.6854897141456604\n",
      "epoch 1780, loss 0.8388928174972534, R2 0.6804105043411255\n",
      "Eval loss 0.8722436428070068, R2 0.6855117678642273\n",
      "epoch 1781, loss 0.8388508558273315, R2 0.680429220199585\n",
      "Eval loss 0.8721907734870911, R2 0.6855335831642151\n",
      "epoch 1782, loss 0.8388089537620544, R2 0.6804478764533997\n",
      "Eval loss 0.8721379041671753, R2 0.6855555772781372\n",
      "epoch 1783, loss 0.8387671709060669, R2 0.6804664134979248\n",
      "Eval loss 0.8720852136611938, R2 0.685577392578125\n",
      "epoch 1784, loss 0.8387254476547241, R2 0.6804850101470947\n",
      "Eval loss 0.872032642364502, R2 0.6855990886688232\n",
      "epoch 1785, loss 0.8386838436126709, R2 0.6805035471916199\n",
      "Eval loss 0.8719801306724548, R2 0.6856208443641663\n",
      "epoch 1786, loss 0.8386422395706177, R2 0.6805220246315002\n",
      "Eval loss 0.8719276785850525, R2 0.6856425404548645\n",
      "epoch 1787, loss 0.8386008143424988, R2 0.6805403828620911\n",
      "Eval loss 0.8718754649162292, R2 0.6856642365455627\n",
      "epoch 1788, loss 0.8385595679283142, R2 0.6805588006973267\n",
      "Eval loss 0.8718231916427612, R2 0.6856858730316162\n",
      "epoch 1789, loss 0.8385183215141296, R2 0.6805771589279175\n",
      "Eval loss 0.8717712759971619, R2 0.6857073307037354\n",
      "epoch 1790, loss 0.8384771943092346, R2 0.6805954575538635\n",
      "Eval loss 0.8717193007469177, R2 0.6857289671897888\n",
      "epoch 1791, loss 0.8384362459182739, R2 0.6806138157844543\n",
      "Eval loss 0.8716673254966736, R2 0.6857504844665527\n",
      "epoch 1792, loss 0.8383951783180237, R2 0.6806319355964661\n",
      "Eval loss 0.8716157078742981, R2 0.6857718825340271\n",
      "epoch 1793, loss 0.8383542895317078, R2 0.6806502342224121\n",
      "Eval loss 0.8715641498565674, R2 0.6857933402061462\n",
      "epoch 1794, loss 0.8383135795593262, R2 0.680668294429779\n",
      "Eval loss 0.8715125322341919, R2 0.6858146786689758\n",
      "epoch 1795, loss 0.8382729887962341, R2 0.6806864142417908\n",
      "Eval loss 0.8714610934257507, R2 0.6858360171318054\n",
      "epoch 1796, loss 0.8382323980331421, R2 0.6807045340538025\n",
      "Eval loss 0.8714098930358887, R2 0.6858572363853455\n",
      "epoch 1797, loss 0.8381918668746948, R2 0.6807225942611694\n",
      "Eval loss 0.8713586330413818, R2 0.6858785152435303\n",
      "epoch 1798, loss 0.8381515145301819, R2 0.6807405352592468\n",
      "Eval loss 0.8713076114654541, R2 0.6858997344970703\n",
      "epoch 1799, loss 0.838111162185669, R2 0.680758535861969\n",
      "Eval loss 0.8712565302848816, R2 0.6859208941459656\n",
      "epoch 1800, loss 0.8380710482597351, R2 0.6807764172554016\n",
      "Eval loss 0.8712056875228882, R2 0.6859419941902161\n",
      "epoch 1801, loss 0.8380310535430908, R2 0.680794358253479\n",
      "Eval loss 0.8711549639701843, R2 0.6859630942344666\n",
      "epoch 1802, loss 0.837990939617157, R2 0.6808121204376221\n",
      "Eval loss 0.8711043000221252, R2 0.6859841346740723\n",
      "epoch 1803, loss 0.8379510641098022, R2 0.6808299422264099\n",
      "Eval loss 0.8710535764694214, R2 0.6860051155090332\n",
      "epoch 1804, loss 0.8379111289978027, R2 0.6808477640151978\n",
      "Eval loss 0.871003270149231, R2 0.6860261559486389\n",
      "epoch 1805, loss 0.8378714323043823, R2 0.6808655261993408\n",
      "Eval loss 0.870952844619751, R2 0.6860470175743103\n",
      "epoch 1806, loss 0.8378317356109619, R2 0.6808832287788391\n",
      "Eval loss 0.8709025979042053, R2 0.6860678791999817\n",
      "epoch 1807, loss 0.8377923369407654, R2 0.6809009313583374\n",
      "Eval loss 0.8708524703979492, R2 0.6860887408256531\n",
      "epoch 1808, loss 0.8377528190612793, R2 0.6809185147285461\n",
      "Eval loss 0.8708025217056274, R2 0.6861094832420349\n",
      "epoch 1809, loss 0.8377134799957275, R2 0.6809360384941101\n",
      "Eval loss 0.8707525730133057, R2 0.6861302852630615\n",
      "epoch 1810, loss 0.8376742601394653, R2 0.6809536218643188\n",
      "Eval loss 0.8707026839256287, R2 0.6861510276794434\n",
      "epoch 1811, loss 0.8376349806785583, R2 0.6809712052345276\n",
      "Eval loss 0.870652973651886, R2 0.6861717104911804\n",
      "epoch 1812, loss 0.8375958204269409, R2 0.6809886693954468\n",
      "Eval loss 0.8706033825874329, R2 0.6861923336982727\n",
      "epoch 1813, loss 0.8375568389892578, R2 0.681006133556366\n",
      "Eval loss 0.8705537915229797, R2 0.686212956905365\n",
      "epoch 1814, loss 0.8375179767608643, R2 0.6810234189033508\n",
      "Eval loss 0.8705043792724609, R2 0.6862334609031677\n",
      "epoch 1815, loss 0.8374791145324707, R2 0.6810408234596252\n",
      "Eval loss 0.8704550266265869, R2 0.6862540245056152\n",
      "epoch 1816, loss 0.8374404311180115, R2 0.6810582876205444\n",
      "Eval loss 0.870405912399292, R2 0.686274528503418\n",
      "epoch 1817, loss 0.8374016880989075, R2 0.6810754537582397\n",
      "Eval loss 0.8703566789627075, R2 0.6862949132919312\n",
      "epoch 1818, loss 0.8373631834983826, R2 0.6810927987098694\n",
      "Eval loss 0.8703076839447021, R2 0.6863152980804443\n",
      "epoch 1819, loss 0.8373247385025024, R2 0.6811100244522095\n",
      "Eval loss 0.8702588081359863, R2 0.6863356232643127\n",
      "epoch 1820, loss 0.8372863531112671, R2 0.68112713098526\n",
      "Eval loss 0.8702100515365601, R2 0.6863559484481812\n",
      "epoch 1821, loss 0.8372480273246765, R2 0.6811442971229553\n",
      "Eval loss 0.8701613545417786, R2 0.6863762736320496\n",
      "epoch 1822, loss 0.8372098207473755, R2 0.6811614632606506\n",
      "Eval loss 0.8701127171516418, R2 0.6863964796066284\n",
      "epoch 1823, loss 0.8371716737747192, R2 0.6811785101890564\n",
      "Eval loss 0.8700641393661499, R2 0.6864166855812073\n",
      "epoch 1824, loss 0.8371336460113525, R2 0.6811956167221069\n",
      "Eval loss 0.8700157999992371, R2 0.6864369511604309\n",
      "epoch 1825, loss 0.8370956778526306, R2 0.6812124848365784\n",
      "Eval loss 0.8699674606323242, R2 0.6864569783210754\n",
      "epoch 1826, loss 0.8370577692985535, R2 0.6812295317649841\n",
      "Eval loss 0.8699193596839905, R2 0.6864771246910095\n",
      "epoch 1827, loss 0.8370199799537659, R2 0.6812464594841003\n",
      "Eval loss 0.869871199131012, R2 0.686497151851654\n",
      "epoch 1828, loss 0.8369823694229126, R2 0.6812633872032166\n",
      "Eval loss 0.869823157787323, R2 0.6865171194076538\n",
      "epoch 1829, loss 0.8369447588920593, R2 0.6812801957130432\n",
      "Eval loss 0.8697751760482788, R2 0.6865371465682983\n",
      "epoch 1830, loss 0.836907148361206, R2 0.6812970042228699\n",
      "Eval loss 0.8697274923324585, R2 0.6865571141242981\n",
      "epoch 1831, loss 0.8368697166442871, R2 0.6813138723373413\n",
      "Eval loss 0.8696796894073486, R2 0.6865769624710083\n",
      "epoch 1832, loss 0.8368324041366577, R2 0.681330680847168\n",
      "Eval loss 0.8696320652961731, R2 0.6865968704223633\n",
      "epoch 1833, loss 0.8367952108383179, R2 0.6813473105430603\n",
      "Eval loss 0.8695845603942871, R2 0.6866166591644287\n",
      "epoch 1834, loss 0.836758017539978, R2 0.6813639998435974\n",
      "Eval loss 0.8695370554924011, R2 0.6866363883018494\n",
      "epoch 1835, loss 0.8367209434509277, R2 0.6813806891441345\n",
      "Eval loss 0.869489848613739, R2 0.6866561770439148\n",
      "epoch 1836, loss 0.8366837501525879, R2 0.6813972592353821\n",
      "Eval loss 0.8694425821304321, R2 0.6866758465766907\n",
      "epoch 1837, loss 0.8366469740867615, R2 0.6814138889312744\n",
      "Eval loss 0.8693955540657043, R2 0.6866955161094666\n",
      "epoch 1838, loss 0.8366100788116455, R2 0.681430459022522\n",
      "Eval loss 0.8693484663963318, R2 0.6867152452468872\n",
      "epoch 1839, loss 0.8365732431411743, R2 0.6814469695091248\n",
      "Eval loss 0.8693014979362488, R2 0.6867347955703735\n",
      "epoch 1840, loss 0.836536705493927, R2 0.6814634203910828\n",
      "Eval loss 0.8692547082901001, R2 0.6867542862892151\n",
      "epoch 1841, loss 0.8364999890327454, R2 0.6814798712730408\n",
      "Eval loss 0.8692078590393066, R2 0.6867738962173462\n",
      "epoch 1842, loss 0.836463451385498, R2 0.681496262550354\n",
      "Eval loss 0.8691613078117371, R2 0.686793327331543\n",
      "epoch 1843, loss 0.8364270329475403, R2 0.6815125942230225\n",
      "Eval loss 0.8691146373748779, R2 0.6868128776550293\n",
      "epoch 1844, loss 0.8363907933235168, R2 0.6815290451049805\n",
      "Eval loss 0.8690683245658875, R2 0.6868321895599365\n",
      "epoch 1845, loss 0.8363544940948486, R2 0.6815453171730042\n",
      "Eval loss 0.8690218925476074, R2 0.6868515610694885\n",
      "epoch 1846, loss 0.83631831407547, R2 0.6815615296363831\n",
      "Eval loss 0.8689756393432617, R2 0.6868708729743958\n",
      "epoch 1847, loss 0.8362821340560913, R2 0.6815778613090515\n",
      "Eval loss 0.868929386138916, R2 0.686890184879303\n",
      "epoch 1848, loss 0.8362461924552917, R2 0.6815940737724304\n",
      "Eval loss 0.8688833713531494, R2 0.6869094967842102\n",
      "epoch 1849, loss 0.8362101912498474, R2 0.6816101670265198\n",
      "Eval loss 0.8688374161720276, R2 0.6869286894798279\n",
      "epoch 1850, loss 0.8361744284629822, R2 0.6816263198852539\n",
      "Eval loss 0.8687915205955505, R2 0.6869478225708008\n",
      "epoch 1851, loss 0.8361386060714722, R2 0.681642472743988\n",
      "Eval loss 0.868745744228363, R2 0.6869670748710632\n",
      "epoch 1852, loss 0.8361029028892517, R2 0.6816585659980774\n",
      "Eval loss 0.8687000274658203, R2 0.6869861483573914\n",
      "epoch 1853, loss 0.8360673785209656, R2 0.6816746592521667\n",
      "Eval loss 0.8686543703079224, R2 0.6870052218437195\n",
      "epoch 1854, loss 0.8360317945480347, R2 0.6816905736923218\n",
      "Eval loss 0.868608832359314, R2 0.6870242953300476\n",
      "epoch 1855, loss 0.8359962701797485, R2 0.6817066073417664\n",
      "Eval loss 0.8685634136199951, R2 0.6870432496070862\n",
      "epoch 1856, loss 0.8359609246253967, R2 0.6817225217819214\n",
      "Eval loss 0.868518054485321, R2 0.6870622634887695\n",
      "epoch 1857, loss 0.8359256982803345, R2 0.6817384362220764\n",
      "Eval loss 0.8684729337692261, R2 0.6870811581611633\n",
      "epoch 1858, loss 0.8358904719352722, R2 0.6817542910575867\n",
      "Eval loss 0.8684276938438416, R2 0.6871000528335571\n",
      "epoch 1859, loss 0.8358553647994995, R2 0.6817701458930969\n",
      "Eval loss 0.8683826327323914, R2 0.6871189475059509\n",
      "epoch 1860, loss 0.8358203172683716, R2 0.6817858815193176\n",
      "Eval loss 0.8683377504348755, R2 0.6871377825737\n",
      "epoch 1861, loss 0.8357852697372437, R2 0.6818016171455383\n",
      "Eval loss 0.8682926893234253, R2 0.6871565580368042\n",
      "epoch 1862, loss 0.8357503414154053, R2 0.6818174123764038\n",
      "Eval loss 0.8682481050491333, R2 0.6871753334999084\n",
      "epoch 1863, loss 0.8357155919075012, R2 0.6818330883979797\n",
      "Eval loss 0.8682034015655518, R2 0.6871940493583679\n",
      "epoch 1864, loss 0.8356807827949524, R2 0.6818488240242004\n",
      "Eval loss 0.8681586980819702, R2 0.6872127652168274\n",
      "epoch 1865, loss 0.8356461524963379, R2 0.6818644404411316\n",
      "Eval loss 0.868114173412323, R2 0.6872313618659973\n",
      "epoch 1866, loss 0.8356115818023682, R2 0.6818801164627075\n",
      "Eval loss 0.8680697679519653, R2 0.687250018119812\n",
      "epoch 1867, loss 0.8355770111083984, R2 0.6818956732749939\n",
      "Eval loss 0.8680254220962524, R2 0.6872685551643372\n",
      "epoch 1868, loss 0.8355426788330078, R2 0.6819112300872803\n",
      "Eval loss 0.8679813146591187, R2 0.6872870922088623\n",
      "epoch 1869, loss 0.8355082869529724, R2 0.6819267272949219\n",
      "Eval loss 0.8679371476173401, R2 0.6873056888580322\n",
      "epoch 1870, loss 0.8354740142822266, R2 0.6819421648979187\n",
      "Eval loss 0.8678929805755615, R2 0.6873241066932678\n",
      "epoch 1871, loss 0.8354398608207703, R2 0.6819576025009155\n",
      "Eval loss 0.8678489923477173, R2 0.6873425245285034\n",
      "epoch 1872, loss 0.835405707359314, R2 0.6819730401039124\n",
      "Eval loss 0.8678051233291626, R2 0.687360942363739\n",
      "epoch 1873, loss 0.8353715538978577, R2 0.6819884181022644\n",
      "Eval loss 0.8677613735198975, R2 0.6873793601989746\n",
      "epoch 1874, loss 0.83533775806427, R2 0.6820037961006165\n",
      "Eval loss 0.8677176833152771, R2 0.6873976588249207\n",
      "epoch 1875, loss 0.8353038430213928, R2 0.6820191740989685\n",
      "Eval loss 0.8676740527153015, R2 0.6874158978462219\n",
      "epoch 1876, loss 0.8352699875831604, R2 0.6820344924926758\n",
      "Eval loss 0.8676304221153259, R2 0.687434196472168\n",
      "epoch 1877, loss 0.8352361917495728, R2 0.6820496916770935\n",
      "Eval loss 0.8675870299339294, R2 0.6874524354934692\n",
      "epoch 1878, loss 0.8352025747299194, R2 0.6820648312568665\n",
      "Eval loss 0.8675436973571777, R2 0.6874706149101257\n",
      "epoch 1879, loss 0.8351690769195557, R2 0.6820800304412842\n",
      "Eval loss 0.867500364780426, R2 0.6874887943267822\n",
      "epoch 1880, loss 0.8351355195045471, R2 0.6820952892303467\n",
      "Eval loss 0.8674573302268982, R2 0.687506914138794\n",
      "epoch 1881, loss 0.8351020812988281, R2 0.6821103692054749\n",
      "Eval loss 0.8674142360687256, R2 0.6875250339508057\n",
      "epoch 1882, loss 0.8350687623023987, R2 0.6821255087852478\n",
      "Eval loss 0.8673712015151978, R2 0.6875430941581726\n",
      "epoch 1883, loss 0.8350354433059692, R2 0.682140588760376\n",
      "Eval loss 0.8673282265663147, R2 0.6875610947608948\n",
      "epoch 1884, loss 0.8350021839141846, R2 0.6821556091308594\n",
      "Eval loss 0.867285430431366, R2 0.6875791549682617\n",
      "epoch 1885, loss 0.8349690437316895, R2 0.682170569896698\n",
      "Eval loss 0.8672425746917725, R2 0.6875970959663391\n",
      "epoch 1886, loss 0.8349359631538391, R2 0.6821855902671814\n",
      "Eval loss 0.8671999573707581, R2 0.6876150369644165\n",
      "epoch 1887, loss 0.8349030017852783, R2 0.6822006106376648\n",
      "Eval loss 0.8671573400497437, R2 0.6876329183578491\n",
      "epoch 1888, loss 0.8348701000213623, R2 0.6822154521942139\n",
      "Eval loss 0.8671149015426636, R2 0.687650740146637\n",
      "epoch 1889, loss 0.8348371982574463, R2 0.6822303533554077\n",
      "Eval loss 0.8670724630355835, R2 0.6876686215400696\n",
      "epoch 1890, loss 0.8348045349121094, R2 0.6822451949119568\n",
      "Eval loss 0.867030143737793, R2 0.6876864433288574\n",
      "epoch 1891, loss 0.8347718119621277, R2 0.6822600364685059\n",
      "Eval loss 0.866987943649292, R2 0.6877041459083557\n",
      "epoch 1892, loss 0.8347392082214355, R2 0.6822748184204102\n",
      "Eval loss 0.8669458627700806, R2 0.687721848487854\n",
      "epoch 1893, loss 0.8347066044807434, R2 0.6822895407676697\n",
      "Eval loss 0.8669037222862244, R2 0.6877395510673523\n",
      "epoch 1894, loss 0.8346741199493408, R2 0.682304322719574\n",
      "Eval loss 0.8668617606163025, R2 0.6877572536468506\n",
      "epoch 1895, loss 0.834641695022583, R2 0.6823190450668335\n",
      "Eval loss 0.8668198585510254, R2 0.6877748370170593\n",
      "epoch 1896, loss 0.8346093893051147, R2 0.6823336482048035\n",
      "Eval loss 0.8667779564857483, R2 0.6877924203872681\n",
      "epoch 1897, loss 0.8345771431922913, R2 0.6823483109474182\n",
      "Eval loss 0.8667362928390503, R2 0.6878100037574768\n",
      "epoch 1898, loss 0.8345448970794678, R2 0.6823628544807434\n",
      "Eval loss 0.8666945695877075, R2 0.6878275871276855\n",
      "epoch 1899, loss 0.8345128297805786, R2 0.6823774576187134\n",
      "Eval loss 0.8666530847549438, R2 0.68784499168396\n",
      "epoch 1900, loss 0.8344807624816895, R2 0.6823921203613281\n",
      "Eval loss 0.8666115403175354, R2 0.6878624558448792\n",
      "epoch 1901, loss 0.8344487547874451, R2 0.6824066042900085\n",
      "Eval loss 0.866570234298706, R2 0.6878799200057983\n",
      "epoch 1902, loss 0.8344168066978455, R2 0.6824210286140442\n",
      "Eval loss 0.8665289878845215, R2 0.6878973245620728\n",
      "epoch 1903, loss 0.8343849778175354, R2 0.6824355125427246\n",
      "Eval loss 0.8664876222610474, R2 0.6879147291183472\n",
      "epoch 1904, loss 0.8343533873558044, R2 0.6824499368667603\n",
      "Eval loss 0.8664463758468628, R2 0.687932014465332\n",
      "epoch 1905, loss 0.8343215584754944, R2 0.6824643611907959\n",
      "Eval loss 0.8664054274559021, R2 0.6879492998123169\n",
      "epoch 1906, loss 0.8342899680137634, R2 0.6824787259101868\n",
      "Eval loss 0.8663643598556519, R2 0.6879665851593018\n",
      "epoch 1907, loss 0.8342583775520325, R2 0.6824930906295776\n",
      "Eval loss 0.8663234710693359, R2 0.6879838705062866\n",
      "epoch 1908, loss 0.8342269062995911, R2 0.682507336139679\n",
      "Eval loss 0.8662827014923096, R2 0.6880010366439819\n",
      "epoch 1909, loss 0.8341954946517944, R2 0.6825217604637146\n",
      "Eval loss 0.866241991519928, R2 0.6880182027816772\n",
      "epoch 1910, loss 0.8341641426086426, R2 0.6825359463691711\n",
      "Eval loss 0.8662012815475464, R2 0.6880353689193726\n",
      "epoch 1911, loss 0.8341329097747803, R2 0.6825501918792725\n",
      "Eval loss 0.8661606907844543, R2 0.6880525350570679\n",
      "epoch 1912, loss 0.8341017365455627, R2 0.682564377784729\n",
      "Eval loss 0.8661202788352966, R2 0.6880695223808289\n",
      "epoch 1913, loss 0.8340705037117004, R2 0.6825785636901855\n",
      "Eval loss 0.8660798668861389, R2 0.6880865693092346\n",
      "epoch 1914, loss 0.8340395092964172, R2 0.6825926303863525\n",
      "Eval loss 0.8660394549369812, R2 0.6881036162376404\n",
      "epoch 1915, loss 0.8340083360671997, R2 0.6826067566871643\n",
      "Eval loss 0.8659991025924683, R2 0.6881205439567566\n",
      "epoch 1916, loss 0.8339775800704956, R2 0.6826209425926208\n",
      "Eval loss 0.8659589290618896, R2 0.6881375908851624\n",
      "epoch 1917, loss 0.8339467644691467, R2 0.6826349496841431\n",
      "Eval loss 0.8659188747406006, R2 0.6881544589996338\n",
      "epoch 1918, loss 0.8339158892631531, R2 0.6826490163803101\n",
      "Eval loss 0.8658788204193115, R2 0.6881713271141052\n",
      "epoch 1919, loss 0.8338851928710938, R2 0.6826629042625427\n",
      "Eval loss 0.8658389449119568, R2 0.6881881356239319\n",
      "epoch 1920, loss 0.8338545560836792, R2 0.6826769113540649\n",
      "Eval loss 0.865799069404602, R2 0.6882050037384033\n",
      "epoch 1921, loss 0.8338239192962646, R2 0.6826907992362976\n",
      "Eval loss 0.8657592535018921, R2 0.68822181224823\n",
      "epoch 1922, loss 0.8337934017181396, R2 0.682704746723175\n",
      "Eval loss 0.8657195568084717, R2 0.6882385611534119\n",
      "epoch 1923, loss 0.8337628841400146, R2 0.6827185750007629\n",
      "Eval loss 0.8656799793243408, R2 0.6882553696632385\n",
      "epoch 1924, loss 0.833732545375824, R2 0.6827324628829956\n",
      "Eval loss 0.8656404614448547, R2 0.6882720589637756\n",
      "epoch 1925, loss 0.8337022066116333, R2 0.6827462911605835\n",
      "Eval loss 0.8656009435653687, R2 0.688288688659668\n",
      "epoch 1926, loss 0.8336719870567322, R2 0.6827601790428162\n",
      "Eval loss 0.8655614852905273, R2 0.6883053183555603\n",
      "epoch 1927, loss 0.8336418867111206, R2 0.6827738285064697\n",
      "Eval loss 0.8655221462249756, R2 0.6883219480514526\n",
      "epoch 1928, loss 0.8336117267608643, R2 0.6827875971794128\n",
      "Eval loss 0.8654829859733582, R2 0.6883385181427002\n",
      "epoch 1929, loss 0.8335816860198975, R2 0.6828013062477112\n",
      "Eval loss 0.865443766117096, R2 0.6883550882339478\n",
      "epoch 1930, loss 0.8335515856742859, R2 0.6828149557113647\n",
      "Eval loss 0.8654047846794128, R2 0.6883715391159058\n",
      "epoch 1931, loss 0.8335217237472534, R2 0.6828287243843079\n",
      "Eval loss 0.865365743637085, R2 0.6883880496025085\n",
      "epoch 1932, loss 0.8334918022155762, R2 0.6828423142433167\n",
      "Eval loss 0.8653268814086914, R2 0.6884045004844666\n",
      "epoch 1933, loss 0.8334619998931885, R2 0.6828559041023254\n",
      "Eval loss 0.8652878999710083, R2 0.6884210109710693\n",
      "epoch 1934, loss 0.8334324359893799, R2 0.6828694939613342\n",
      "Eval loss 0.8652492761611938, R2 0.6884373426437378\n",
      "epoch 1935, loss 0.833402693271637, R2 0.6828829646110535\n",
      "Eval loss 0.8652104139328003, R2 0.6884536743164062\n",
      "epoch 1936, loss 0.8333731889724731, R2 0.6828965544700623\n",
      "Eval loss 0.8651718497276306, R2 0.6884700655937195\n",
      "epoch 1937, loss 0.8333435654640198, R2 0.6829100251197815\n",
      "Eval loss 0.8651333451271057, R2 0.6884863376617432\n",
      "epoch 1938, loss 0.8333141207695007, R2 0.6829234957695007\n",
      "Eval loss 0.8650948405265808, R2 0.6885026693344116\n",
      "epoch 1939, loss 0.8332846760749817, R2 0.6829369068145752\n",
      "Eval loss 0.8650563955307007, R2 0.6885188817977905\n",
      "epoch 1940, loss 0.8332554697990417, R2 0.6829503178596497\n",
      "Eval loss 0.8650181889533997, R2 0.6885350346565247\n",
      "epoch 1941, loss 0.833226203918457, R2 0.6829637289047241\n",
      "Eval loss 0.8649799227714539, R2 0.6885512471199036\n",
      "epoch 1942, loss 0.8331969380378723, R2 0.6829770803451538\n",
      "Eval loss 0.8649416565895081, R2 0.6885673403739929\n",
      "epoch 1943, loss 0.8331676721572876, R2 0.6829904317855835\n",
      "Eval loss 0.8649036884307861, R2 0.6885835528373718\n",
      "epoch 1944, loss 0.8331387042999268, R2 0.6830036640167236\n",
      "Eval loss 0.8648655414581299, R2 0.6885996460914612\n",
      "epoch 1945, loss 0.8331097364425659, R2 0.6830169558525085\n",
      "Eval loss 0.8648277521133423, R2 0.6886156797409058\n",
      "epoch 1946, loss 0.8330807685852051, R2 0.6830302476882935\n",
      "Eval loss 0.8647897839546204, R2 0.6886317133903503\n",
      "epoch 1947, loss 0.8330518007278442, R2 0.6830434799194336\n",
      "Eval loss 0.8647519946098328, R2 0.6886478066444397\n",
      "epoch 1948, loss 0.8330230712890625, R2 0.6830565929412842\n",
      "Eval loss 0.8647143244743347, R2 0.6886637210845947\n",
      "epoch 1949, loss 0.8329941630363464, R2 0.6830698251724243\n",
      "Eval loss 0.8646765947341919, R2 0.6886796951293945\n",
      "epoch 1950, loss 0.8329654932022095, R2 0.6830828785896301\n",
      "Eval loss 0.8646391034126282, R2 0.6886956095695496\n",
      "epoch 1951, loss 0.8329369425773621, R2 0.6830959916114807\n",
      "Eval loss 0.8646015524864197, R2 0.688711404800415\n",
      "epoch 1952, loss 0.8329084515571594, R2 0.6831091046333313\n",
      "Eval loss 0.8645641207695007, R2 0.6887273788452148\n",
      "epoch 1953, loss 0.832879900932312, R2 0.6831221580505371\n",
      "Eval loss 0.8645268678665161, R2 0.6887431740760803\n",
      "epoch 1954, loss 0.8328514099121094, R2 0.6831352114677429\n",
      "Eval loss 0.8644894361495972, R2 0.6887589693069458\n",
      "epoch 1955, loss 0.8328230381011963, R2 0.6831481456756592\n",
      "Eval loss 0.8644522428512573, R2 0.6887747049331665\n",
      "epoch 1956, loss 0.8327946662902832, R2 0.683161199092865\n",
      "Eval loss 0.864415168762207, R2 0.6887904405593872\n",
      "epoch 1957, loss 0.8327664136886597, R2 0.6831741333007812\n",
      "Eval loss 0.8643780946731567, R2 0.6888062357902527\n",
      "epoch 1958, loss 0.8327381610870361, R2 0.6831870675086975\n",
      "Eval loss 0.864341139793396, R2 0.6888218522071838\n",
      "epoch 1959, loss 0.8327101469039917, R2 0.6832000613212585\n",
      "Eval loss 0.8643041253089905, R2 0.6888375282287598\n",
      "epoch 1960, loss 0.8326820731163025, R2 0.6832128167152405\n",
      "Eval loss 0.8642672896385193, R2 0.6888532638549805\n",
      "epoch 1961, loss 0.8326540589332581, R2 0.683225691318512\n",
      "Eval loss 0.8642305731773376, R2 0.6888687610626221\n",
      "epoch 1962, loss 0.8326261043548584, R2 0.6832384467124939\n",
      "Eval loss 0.864193856716156, R2 0.6888843178749084\n",
      "epoch 1963, loss 0.832598090171814, R2 0.6832513213157654\n",
      "Eval loss 0.8641572594642639, R2 0.6888999342918396\n",
      "epoch 1964, loss 0.8325703144073486, R2 0.6832640767097473\n",
      "Eval loss 0.8641207218170166, R2 0.6889154314994812\n",
      "epoch 1965, loss 0.8325425982475281, R2 0.6832767724990845\n",
      "Eval loss 0.8640841245651245, R2 0.6889309287071228\n",
      "epoch 1966, loss 0.8325148820877075, R2 0.6832895278930664\n",
      "Eval loss 0.8640477657318115, R2 0.6889463663101196\n",
      "epoch 1967, loss 0.832487165927887, R2 0.6833022236824036\n",
      "Eval loss 0.8640113472938538, R2 0.6889618039131165\n",
      "epoch 1968, loss 0.8324596881866455, R2 0.683314859867096\n",
      "Eval loss 0.8639751076698303, R2 0.6889772415161133\n",
      "epoch 1969, loss 0.8324320316314697, R2 0.6833274960517883\n",
      "Eval loss 0.8639389276504517, R2 0.6889925003051758\n",
      "epoch 1970, loss 0.832404613494873, R2 0.6833401322364807\n",
      "Eval loss 0.8639028072357178, R2 0.6890079379081726\n",
      "epoch 1971, loss 0.8323771953582764, R2 0.6833528280258179\n",
      "Eval loss 0.8638666272163391, R2 0.6890231966972351\n",
      "epoch 1972, loss 0.832349956035614, R2 0.6833653450012207\n",
      "Eval loss 0.8638306856155396, R2 0.6890384554862976\n",
      "epoch 1973, loss 0.8323224782943726, R2 0.6833778619766235\n",
      "Eval loss 0.8637946844100952, R2 0.6890537142753601\n",
      "epoch 1974, loss 0.8322953581809998, R2 0.6833904385566711\n",
      "Eval loss 0.86375892162323, R2 0.6890690326690674\n",
      "epoch 1975, loss 0.8322681784629822, R2 0.6834028959274292\n",
      "Eval loss 0.86372309923172, R2 0.6890842914581299\n",
      "epoch 1976, loss 0.8322409987449646, R2 0.6834153532981873\n",
      "Eval loss 0.8636873364448547, R2 0.6890994906425476\n",
      "epoch 1977, loss 0.8322139978408813, R2 0.6834278106689453\n",
      "Eval loss 0.8636518120765686, R2 0.6891145706176758\n",
      "epoch 1978, loss 0.8321870565414429, R2 0.6834401488304138\n",
      "Eval loss 0.8636161088943481, R2 0.6891297101974487\n",
      "epoch 1979, loss 0.8321600556373596, R2 0.6834526062011719\n",
      "Eval loss 0.8635806441307068, R2 0.6891447901725769\n",
      "epoch 1980, loss 0.8321331739425659, R2 0.6834649443626404\n",
      "Eval loss 0.8635452389717102, R2 0.6891598105430603\n",
      "epoch 1981, loss 0.8321062922477722, R2 0.6834772825241089\n",
      "Eval loss 0.8635098338127136, R2 0.6891749501228333\n",
      "epoch 1982, loss 0.8320795893669128, R2 0.6834896206855774\n",
      "Eval loss 0.8634745478630066, R2 0.6891899704933167\n",
      "epoch 1983, loss 0.8320528864860535, R2 0.6835018396377563\n",
      "Eval loss 0.8634393215179443, R2 0.6892049908638\n",
      "epoch 1984, loss 0.8320261836051941, R2 0.6835141181945801\n",
      "Eval loss 0.8634040951728821, R2 0.6892198920249939\n",
      "epoch 1985, loss 0.831999659538269, R2 0.6835263967514038\n",
      "Eval loss 0.8633689880371094, R2 0.6892349123954773\n",
      "epoch 1986, loss 0.831973135471344, R2 0.683538556098938\n",
      "Eval loss 0.8633340001106262, R2 0.6892497539520264\n",
      "epoch 1987, loss 0.831946611404419, R2 0.6835507750511169\n",
      "Eval loss 0.8632990717887878, R2 0.6892645359039307\n",
      "epoch 1988, loss 0.8319201469421387, R2 0.6835629940032959\n",
      "Eval loss 0.8632641434669495, R2 0.6892794966697693\n",
      "epoch 1989, loss 0.831893801689148, R2 0.6835750937461853\n",
      "Eval loss 0.8632292747497559, R2 0.6892943382263184\n",
      "epoch 1990, loss 0.8318675756454468, R2 0.6835872530937195\n",
      "Eval loss 0.863194465637207, R2 0.6893091201782227\n",
      "epoch 1991, loss 0.831841230392456, R2 0.6835993528366089\n",
      "Eval loss 0.8631597757339478, R2 0.6893238425254822\n",
      "epoch 1992, loss 0.8318150639533997, R2 0.6836113333702087\n",
      "Eval loss 0.863125205039978, R2 0.6893386840820312\n",
      "epoch 1993, loss 0.831788957118988, R2 0.6836234927177429\n",
      "Eval loss 0.8630906343460083, R2 0.6893534064292908\n",
      "epoch 1994, loss 0.8317629098892212, R2 0.6836354732513428\n",
      "Eval loss 0.8630561232566833, R2 0.6893680691719055\n",
      "epoch 1995, loss 0.8317368030548096, R2 0.6836475133895874\n",
      "Eval loss 0.8630216121673584, R2 0.6893827319145203\n",
      "epoch 1996, loss 0.8317108750343323, R2 0.6836594343185425\n",
      "Eval loss 0.8629873991012573, R2 0.689397394657135\n",
      "epoch 1997, loss 0.831684947013855, R2 0.6836713552474976\n",
      "Eval loss 0.8629530072212219, R2 0.6894119381904602\n",
      "epoch 1998, loss 0.8316591382026672, R2 0.6836833357810974\n",
      "Eval loss 0.8629187941551208, R2 0.689426600933075\n",
      "epoch 1999, loss 0.8316333293914795, R2 0.6836952567100525\n",
      "Eval loss 0.8628846406936646, R2 0.6894411444664001\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(dynamics_train_x.shape[0])\n",
    "    dynamics_train_x = dynamics_train_x[idx, :]\n",
    "    dynamics_train_y = dynamics_train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    dynamics_test_x = dynamics_test_x[idx, :]\n",
    "    dynamics_test_y = dynamics_test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = dynamics_lr_model(dynamics_train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, dynamics_train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, dynamics_train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = dynamics_lr_model(dynamics_test_x)\n",
    "            test_loss = criterion(preds, dynamics_test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, dynamics_test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_lr_model(dynamics_train_x)\n",
    "    test_preds = dynamics_lr_model(dynamics_test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - dynamics_train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - dynamics_train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - dynamics_test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - dynamics_test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 1]) torch.Size([2000, 44]) torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "reward_train_x = train_x\n",
    "reward_train_y = train_y[:, -1][:, None]\n",
    "reward_test_x = test_x\n",
    "reward_test_y = test_y[:, -1][:, None]\n",
    "\n",
    "print(reward_train_x.shape, reward_train_y.shape, reward_test_x.shape, reward_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "epochs = 10000\n",
    "eval_epoch_freq = 1\n",
    "in_size=reward_train_x.shape[-1]\n",
    "out_size=reward_train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "reward_lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(reward_lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.3613812923431396, R2 -55.587257385253906\n",
      "Eval loss 1.2107487916946411, R2 -45.63677978515625\n",
      "epoch 1, loss 1.2832679748535156, R2 -52.34037399291992\n",
      "Eval loss 1.1628497838974, R2 -43.79173278808594\n",
      "epoch 2, loss 1.2275830507278442, R2 -50.02577209472656\n",
      "Eval loss 1.127724289894104, R2 -42.438724517822266\n",
      "epoch 3, loss 1.186045527458191, R2 -48.29924011230469\n",
      "Eval loss 1.1004862785339355, R2 -41.38956069946289\n",
      "epoch 4, loss 1.1534751653671265, R2 -46.9454345703125\n",
      "Eval loss 1.0781232118606567, R2 -40.52817916870117\n",
      "epoch 5, loss 1.1266310214996338, R2 -45.829593658447266\n",
      "Eval loss 1.0587831735610962, R2 -39.783199310302734\n",
      "epoch 6, loss 1.1034841537475586, R2 -44.86748504638672\n",
      "Eval loss 1.0413341522216797, R2 -39.11109161376953\n",
      "epoch 7, loss 1.082763671875, R2 -44.006160736083984\n",
      "Eval loss 1.0250871181488037, R2 -38.485286712646484\n",
      "epoch 8, loss 1.0636727809906006, R2 -43.212684631347656\n",
      "Eval loss 1.0096269845962524, R2 -37.889766693115234\n",
      "epoch 9, loss 1.0457109212875366, R2 -42.466068267822266\n",
      "Eval loss 0.9947048425674438, R2 -37.31498336791992\n",
      "epoch 10, loss 1.028563141822815, R2 -41.75328826904297\n",
      "Eval loss 0.9801738262176514, R2 -36.75528335571289\n",
      "epoch 11, loss 1.0120301246643066, R2 -41.06610107421875\n",
      "Eval loss 0.9659491777420044, R2 -36.20734405517578\n",
      "epoch 12, loss 0.9959854483604431, R2 -40.39917755126953\n",
      "Eval loss 0.9519816040992737, R2 -35.66933822631836\n",
      "epoch 13, loss 0.9803481698036194, R2 -39.74919509887695\n",
      "Eval loss 0.9382440447807312, R2 -35.14017105102539\n",
      "epoch 14, loss 0.9650651216506958, R2 -39.113914489746094\n",
      "Eval loss 0.9247222542762756, R2 -34.619327545166016\n",
      "epoch 15, loss 0.9501017332077026, R2 -38.49198532104492\n",
      "Eval loss 0.9114083051681519, R2 -34.10648727416992\n",
      "epoch 16, loss 0.9354340434074402, R2 -37.88230514526367\n",
      "Eval loss 0.8982983827590942, R2 -33.60151672363281\n",
      "epoch 17, loss 0.9210456013679504, R2 -37.284236907958984\n",
      "Eval loss 0.8853904008865356, R2 -33.10432434082031\n",
      "epoch 18, loss 0.906923770904541, R2 -36.69724655151367\n",
      "Eval loss 0.8726829290390015, R2 -32.61481475830078\n",
      "epoch 19, loss 0.8930593132972717, R2 -36.12094497680664\n",
      "Eval loss 0.8601751923561096, R2 -32.13302230834961\n",
      "epoch 20, loss 0.8794444799423218, R2 -35.55503845214844\n",
      "Eval loss 0.8478655815124512, R2 -31.658889770507812\n",
      "epoch 21, loss 0.8660728931427002, R2 -34.999237060546875\n",
      "Eval loss 0.8357527852058411, R2 -31.192306518554688\n",
      "epoch 22, loss 0.8529385924339294, R2 -34.453285217285156\n",
      "Eval loss 0.823835015296936, R2 -30.733257293701172\n",
      "epoch 23, loss 0.8400366306304932, R2 -33.91701126098633\n",
      "Eval loss 0.8121103644371033, R2 -30.281597137451172\n",
      "epoch 24, loss 0.8273617029190063, R2 -33.39016342163086\n",
      "Eval loss 0.800576388835907, R2 -29.837377548217773\n",
      "epoch 25, loss 0.8149095177650452, R2 -32.872589111328125\n",
      "Eval loss 0.7892307639122009, R2 -29.40032958984375\n",
      "epoch 26, loss 0.8026756048202515, R2 -32.36406326293945\n",
      "Eval loss 0.7780709266662598, R2 -28.970489501953125\n",
      "epoch 27, loss 0.7906553745269775, R2 -31.864429473876953\n",
      "Eval loss 0.767094075679779, R2 -28.54765510559082\n",
      "epoch 28, loss 0.7788450121879578, R2 -31.373519897460938\n",
      "Eval loss 0.7562974095344543, R2 -28.13178825378418\n",
      "epoch 29, loss 0.7672402262687683, R2 -30.89115333557129\n",
      "Eval loss 0.7456778883934021, R2 -27.72274398803711\n",
      "epoch 30, loss 0.7558372020721436, R2 -30.41715431213379\n",
      "Eval loss 0.735232949256897, R2 -27.32040786743164\n",
      "epoch 31, loss 0.7446320056915283, R2 -29.951417922973633\n",
      "Eval loss 0.7249593734741211, R2 -26.92466926574707\n",
      "epoch 32, loss 0.7336207032203674, R2 -29.493694305419922\n",
      "Eval loss 0.7148542404174805, R2 -26.535423278808594\n",
      "epoch 33, loss 0.7227998971939087, R2 -29.043935775756836\n",
      "Eval loss 0.7049148082733154, R2 -26.15257453918457\n",
      "epoch 34, loss 0.7121658325195312, R2 -28.601917266845703\n",
      "Eval loss 0.6951379179954529, R2 -25.77597999572754\n",
      "epoch 35, loss 0.7017146944999695, R2 -28.16751480102539\n",
      "Eval loss 0.6855210065841675, R2 -25.4055233001709\n",
      "epoch 36, loss 0.6914435029029846, R2 -27.740581512451172\n",
      "Eval loss 0.6760610938072205, R2 -25.041168212890625\n",
      "epoch 37, loss 0.6813483834266663, R2 -27.320959091186523\n",
      "Eval loss 0.6667553186416626, R2 -24.682727813720703\n",
      "epoch 38, loss 0.6714262962341309, R2 -26.908544540405273\n",
      "Eval loss 0.6576009392738342, R2 -24.330087661743164\n",
      "epoch 39, loss 0.6616737842559814, R2 -26.503145217895508\n",
      "Eval loss 0.6485952734947205, R2 -23.983192443847656\n",
      "epoch 40, loss 0.6520876884460449, R2 -26.104707717895508\n",
      "Eval loss 0.6397356986999512, R2 -23.641939163208008\n",
      "epoch 41, loss 0.6426650285720825, R2 -25.713045120239258\n",
      "Eval loss 0.6310195326805115, R2 -23.306215286254883\n",
      "epoch 42, loss 0.6334025859832764, R2 -25.32805824279785\n",
      "Eval loss 0.622444212436676, R2 -22.975894927978516\n",
      "epoch 43, loss 0.6242973804473877, R2 -24.949548721313477\n",
      "Eval loss 0.6140071153640747, R2 -22.650922775268555\n",
      "epoch 44, loss 0.615346372127533, R2 -24.577499389648438\n",
      "Eval loss 0.6057060360908508, R2 -22.331144332885742\n",
      "epoch 45, loss 0.6065468192100525, R2 -24.21175193786621\n",
      "Eval loss 0.5975381731987, R2 -22.01653289794922\n",
      "epoch 46, loss 0.5978958010673523, R2 -23.852188110351562\n",
      "Eval loss 0.5895013213157654, R2 -21.70697021484375\n",
      "epoch 47, loss 0.5893905758857727, R2 -23.498619079589844\n",
      "Eval loss 0.5815930962562561, R2 -21.402372360229492\n",
      "epoch 48, loss 0.5810284614562988, R2 -23.151039123535156\n",
      "Eval loss 0.5738112926483154, R2 -21.102619171142578\n",
      "epoch 49, loss 0.5728066563606262, R2 -22.80929183959961\n",
      "Eval loss 0.5661534667015076, R2 -20.807621002197266\n",
      "epoch 50, loss 0.5647225975990295, R2 -22.473285675048828\n",
      "Eval loss 0.5586175322532654, R2 -20.517372131347656\n",
      "epoch 51, loss 0.5567737817764282, R2 -22.142864227294922\n",
      "Eval loss 0.5512012839317322, R2 -20.231693267822266\n",
      "epoch 52, loss 0.5489577054977417, R2 -21.81800079345703\n",
      "Eval loss 0.5439024567604065, R2 -19.950544357299805\n",
      "epoch 53, loss 0.5412719249725342, R2 -21.4985408782959\n",
      "Eval loss 0.5367192625999451, R2 -19.673858642578125\n",
      "epoch 54, loss 0.5337138772010803, R2 -21.18438148498535\n",
      "Eval loss 0.5296493768692017, R2 -19.40152931213379\n",
      "epoch 55, loss 0.5262813568115234, R2 -20.87543487548828\n",
      "Eval loss 0.5226908922195435, R2 -19.133508682250977\n",
      "epoch 56, loss 0.5189720392227173, R2 -20.571622848510742\n",
      "Eval loss 0.5158418416976929, R2 -18.869678497314453\n",
      "epoch 57, loss 0.5117835402488708, R2 -20.27280616760254\n",
      "Eval loss 0.5091002583503723, R2 -18.609994888305664\n",
      "epoch 58, loss 0.5047138333320618, R2 -19.97896385192871\n",
      "Eval loss 0.5024643540382385, R2 -18.35439109802246\n",
      "epoch 59, loss 0.49776050448417664, R2 -19.6899356842041\n",
      "Eval loss 0.49593210220336914, R2 -18.10277557373047\n",
      "epoch 60, loss 0.49092167615890503, R2 -19.405691146850586\n",
      "Eval loss 0.4895017743110657, R2 -17.8551025390625\n",
      "epoch 61, loss 0.4841949939727783, R2 -19.12607192993164\n",
      "Eval loss 0.48317158222198486, R2 -17.611265182495117\n",
      "epoch 62, loss 0.4775785803794861, R2 -18.851058959960938\n",
      "Eval loss 0.47693967819213867, R2 -17.37122344970703\n",
      "epoch 63, loss 0.4710703492164612, R2 -18.58053207397461\n",
      "Eval loss 0.4708045423030853, R2 -17.134899139404297\n",
      "epoch 64, loss 0.464668333530426, R2 -18.314407348632812\n",
      "Eval loss 0.46476420760154724, R2 -16.9022274017334\n",
      "epoch 65, loss 0.4583705961704254, R2 -18.052640914916992\n",
      "Eval loss 0.4588172733783722, R2 -16.67316246032715\n",
      "epoch 66, loss 0.4521753489971161, R2 -17.795141220092773\n",
      "Eval loss 0.4529619514942169, R2 -16.447607040405273\n",
      "epoch 67, loss 0.44608059525489807, R2 -17.54180145263672\n",
      "Eval loss 0.44719672203063965, R2 -16.225542068481445\n",
      "epoch 68, loss 0.4400846064090729, R2 -17.29258155822754\n",
      "Eval loss 0.4415200352668762, R2 -16.006877899169922\n",
      "epoch 69, loss 0.4341855049133301, R2 -17.047380447387695\n",
      "Eval loss 0.4359302818775177, R2 -15.791574478149414\n",
      "epoch 70, loss 0.42838165163993835, R2 -16.80613136291504\n",
      "Eval loss 0.4304260313510895, R2 -15.579561233520508\n",
      "epoch 71, loss 0.42267125844955444, R2 -16.568761825561523\n",
      "Eval loss 0.42500582337379456, R2 -15.370786666870117\n",
      "epoch 72, loss 0.41705265641212463, R2 -16.335229873657227\n",
      "Eval loss 0.4196680784225464, R2 -15.165176391601562\n",
      "epoch 73, loss 0.4115241467952728, R2 -16.10542106628418\n",
      "Eval loss 0.4144115149974823, R2 -14.962695121765137\n",
      "epoch 74, loss 0.40608423948287964, R2 -15.879316329956055\n",
      "Eval loss 0.4092347025871277, R2 -14.763276100158691\n",
      "epoch 75, loss 0.4007312059402466, R2 -15.656801223754883\n",
      "Eval loss 0.4041362702846527, R2 -14.566898345947266\n",
      "epoch 76, loss 0.39546361565589905, R2 -15.437870025634766\n",
      "Eval loss 0.39911481738090515, R2 -14.373490333557129\n",
      "epoch 77, loss 0.3902798593044281, R2 -15.222400665283203\n",
      "Eval loss 0.3941691517829895, R2 -14.182989120483398\n",
      "epoch 78, loss 0.38517847657203674, R2 -15.010351181030273\n",
      "Eval loss 0.38929784297943115, R2 -13.995347023010254\n",
      "epoch 79, loss 0.3801579475402832, R2 -14.801663398742676\n",
      "Eval loss 0.38449975848197937, R2 -13.810524940490723\n",
      "epoch 80, loss 0.37521693110466003, R2 -14.596283912658691\n",
      "Eval loss 0.3797735273838043, R2 -13.6284761428833\n",
      "epoch 81, loss 0.37035390734672546, R2 -14.394152641296387\n",
      "Eval loss 0.3751179873943329, R2 -13.449140548706055\n",
      "epoch 82, loss 0.36556756496429443, R2 -14.195202827453613\n",
      "Eval loss 0.37053197622299194, R2 -13.272497177124023\n",
      "epoch 83, loss 0.3608565628528595, R2 -13.999385833740234\n",
      "Eval loss 0.366014301776886, R2 -13.098485946655273\n",
      "epoch 84, loss 0.3562195301055908, R2 -13.806642532348633\n",
      "Eval loss 0.3615637421607971, R2 -12.927054405212402\n",
      "epoch 85, loss 0.3516552150249481, R2 -13.616921424865723\n",
      "Eval loss 0.3571792542934418, R2 -12.758164405822754\n",
      "epoch 86, loss 0.3471622169017792, R2 -13.430166244506836\n",
      "Eval loss 0.3528597056865692, R2 -12.591804504394531\n",
      "epoch 87, loss 0.34273943305015564, R2 -13.246318817138672\n",
      "Eval loss 0.3486039936542511, R2 -12.42786693572998\n",
      "epoch 88, loss 0.3383854925632477, R2 -13.065351486206055\n",
      "Eval loss 0.34441104531288147, R2 -12.26635456085205\n",
      "epoch 89, loss 0.33409929275512695, R2 -12.887191772460938\n",
      "Eval loss 0.340279757976532, R2 -12.107202529907227\n",
      "epoch 90, loss 0.32987961173057556, R2 -12.711782455444336\n",
      "Eval loss 0.336209237575531, R2 -11.950425148010254\n",
      "epoch 91, loss 0.3257253170013428, R2 -12.539117813110352\n",
      "Eval loss 0.3321983218193054, R2 -11.795929908752441\n",
      "epoch 92, loss 0.3216351866722107, R2 -12.369115829467773\n",
      "Eval loss 0.3282461166381836, R2 -11.643698692321777\n",
      "epoch 93, loss 0.31760814785957336, R2 -12.201702117919922\n",
      "Eval loss 0.32435160875320435, R2 -11.49368667602539\n",
      "epoch 94, loss 0.3136431574821472, R2 -12.036897659301758\n",
      "Eval loss 0.32051387429237366, R2 -11.345853805541992\n",
      "epoch 95, loss 0.3097390830516815, R2 -11.874629020690918\n",
      "Eval loss 0.3167319595813751, R2 -11.200188636779785\n",
      "epoch 96, loss 0.3058948814868927, R2 -11.714836120605469\n",
      "Eval loss 0.3130049407482147, R2 -11.056624412536621\n",
      "epoch 97, loss 0.3021095097064972, R2 -11.557497024536133\n",
      "Eval loss 0.30933183431625366, R2 -10.915140151977539\n",
      "epoch 98, loss 0.2983819246292114, R2 -11.402565002441406\n",
      "Eval loss 0.30571189522743225, R2 -10.77569580078125\n",
      "epoch 99, loss 0.2947112023830414, R2 -11.249983787536621\n",
      "Eval loss 0.3021441698074341, R2 -10.638275146484375\n",
      "epoch 100, loss 0.2910963296890259, R2 -11.099727630615234\n",
      "Eval loss 0.2986277937889099, R2 -10.502828598022461\n",
      "epoch 101, loss 0.2875363528728485, R2 -10.951737403869629\n",
      "Eval loss 0.2951619625091553, R2 -10.369327545166016\n",
      "epoch 102, loss 0.2840302586555481, R2 -10.806018829345703\n",
      "Eval loss 0.29174578189849854, R2 -10.237740516662598\n",
      "epoch 103, loss 0.28057724237442017, R2 -10.662489891052246\n",
      "Eval loss 0.28837850689888, R2 -10.108036041259766\n",
      "epoch 104, loss 0.2771763205528259, R2 -10.521127700805664\n",
      "Eval loss 0.2850593328475952, R2 -9.980191230773926\n",
      "epoch 105, loss 0.27382662892341614, R2 -10.381890296936035\n",
      "Eval loss 0.2817875146865845, R2 -9.854151725769043\n",
      "epoch 106, loss 0.27052727341651917, R2 -10.244752883911133\n",
      "Eval loss 0.27856212854385376, R2 -9.729926109313965\n",
      "epoch 107, loss 0.26727741956710815, R2 -10.109658241271973\n",
      "Eval loss 0.2753826677799225, R2 -9.607453346252441\n",
      "epoch 108, loss 0.26407623291015625, R2 -9.976608276367188\n",
      "Eval loss 0.27224817872047424, R2 -9.486712455749512\n",
      "epoch 109, loss 0.2609228789806366, R2 -9.845536231994629\n",
      "Eval loss 0.2691579759120941, R2 -9.367685317993164\n",
      "epoch 110, loss 0.25781652331352234, R2 -9.71641731262207\n",
      "Eval loss 0.26611143350601196, R2 -9.250325202941895\n",
      "epoch 111, loss 0.25475645065307617, R2 -9.589221000671387\n",
      "Eval loss 0.2631078064441681, R2 -9.13463020324707\n",
      "epoch 112, loss 0.25174176692962646, R2 -9.463903427124023\n",
      "Eval loss 0.2601463496685028, R2 -9.020564079284668\n",
      "epoch 113, loss 0.2487717866897583, R2 -9.340459823608398\n",
      "Eval loss 0.25722649693489075, R2 -8.908093452453613\n",
      "epoch 114, loss 0.2458457052707672, R2 -9.218836784362793\n",
      "Eval loss 0.2543475031852722, R2 -8.797203063964844\n",
      "epoch 115, loss 0.24296286702156067, R2 -9.099005699157715\n",
      "Eval loss 0.2515086829662323, R2 -8.687854766845703\n",
      "epoch 116, loss 0.2401224672794342, R2 -8.980944633483887\n",
      "Eval loss 0.24870949983596802, R2 -8.580033302307129\n",
      "epoch 117, loss 0.2373238503932953, R2 -8.864614486694336\n",
      "Eval loss 0.24594923853874207, R2 -8.473702430725098\n",
      "epoch 118, loss 0.23456625640392303, R2 -8.749991416931152\n",
      "Eval loss 0.2432272881269455, R2 -8.36885929107666\n",
      "epoch 119, loss 0.2318490594625473, R2 -8.637048721313477\n",
      "Eval loss 0.24054314196109772, R2 -8.26546573638916\n",
      "epoch 120, loss 0.22917158901691437, R2 -8.525747299194336\n",
      "Eval loss 0.23789608478546143, R2 -8.163503646850586\n",
      "epoch 121, loss 0.22653314471244812, R2 -8.41609001159668\n",
      "Eval loss 0.23528552055358887, R2 -8.062950134277344\n",
      "epoch 122, loss 0.22393307089805603, R2 -8.308012008666992\n",
      "Eval loss 0.23271098732948303, R2 -7.963784217834473\n",
      "epoch 123, loss 0.22137083113193512, R2 -8.201513290405273\n",
      "Eval loss 0.2301718145608902, R2 -7.865975379943848\n",
      "epoch 124, loss 0.2188456505537033, R2 -8.096551895141602\n",
      "Eval loss 0.22766754031181335, R2 -7.769514083862305\n",
      "epoch 125, loss 0.2163570374250412, R2 -7.993101119995117\n",
      "Eval loss 0.22519750893115997, R2 -7.674365997314453\n",
      "epoch 126, loss 0.21390432119369507, R2 -7.891151428222656\n",
      "Eval loss 0.22276125848293304, R2 -7.580533981323242\n",
      "epoch 127, loss 0.21148698031902313, R2 -7.790680885314941\n",
      "Eval loss 0.2203582525253296, R2 -7.487964630126953\n",
      "epoch 128, loss 0.20910437405109406, R2 -7.691641807556152\n",
      "Eval loss 0.21798793971538544, R2 -7.396671295166016\n",
      "epoch 129, loss 0.20675598084926605, R2 -7.59403133392334\n",
      "Eval loss 0.21564985811710358, R2 -7.306605339050293\n",
      "epoch 130, loss 0.2044411599636078, R2 -7.4978132247924805\n",
      "Eval loss 0.21334347128868103, R2 -7.2177629470825195\n",
      "epoch 131, loss 0.20215944945812225, R2 -7.402960777282715\n",
      "Eval loss 0.2110683172941208, R2 -7.130124092102051\n",
      "epoch 132, loss 0.1999102681875229, R2 -7.309473991394043\n",
      "Eval loss 0.20882385969161987, R2 -7.043672561645508\n",
      "epoch 133, loss 0.1976930797100067, R2 -7.21732234954834\n",
      "Eval loss 0.20660968124866486, R2 -6.958384990692139\n",
      "epoch 134, loss 0.19550740718841553, R2 -7.12646484375\n",
      "Eval loss 0.20442529022693634, R2 -6.874239921569824\n",
      "epoch 135, loss 0.19335272908210754, R2 -7.0369110107421875\n",
      "Eval loss 0.20227020978927612, R2 -6.7912397384643555\n",
      "epoch 136, loss 0.191228449344635, R2 -6.948613166809082\n",
      "Eval loss 0.20014400780200958, R2 -6.709341049194336\n",
      "epoch 137, loss 0.18913421034812927, R2 -6.8615641593933105\n",
      "Eval loss 0.19804628193378448, R2 -6.628536224365234\n",
      "epoch 138, loss 0.18706947565078735, R2 -6.775740623474121\n",
      "Eval loss 0.19597651064395905, R2 -6.548809051513672\n",
      "epoch 139, loss 0.18503375351428986, R2 -6.691121578216553\n",
      "Eval loss 0.1939343363046646, R2 -6.470146656036377\n",
      "epoch 140, loss 0.18302658200263977, R2 -6.607693672180176\n",
      "Eval loss 0.19191931188106537, R2 -6.392533779144287\n",
      "epoch 141, loss 0.18104751408100128, R2 -6.5254316329956055\n",
      "Eval loss 0.18993104994297028, R2 -6.315941333770752\n",
      "epoch 142, loss 0.17909611761569977, R2 -6.444310188293457\n",
      "Eval loss 0.18796908855438232, R2 -6.240371227264404\n",
      "epoch 143, loss 0.17717190086841583, R2 -6.364330768585205\n",
      "Eval loss 0.18603307008743286, R2 -6.165793418884277\n",
      "epoch 144, loss 0.17527447640895844, R2 -6.285469055175781\n",
      "Eval loss 0.18412259221076965, R2 -6.092207908630371\n",
      "epoch 145, loss 0.17340341210365295, R2 -6.207696437835693\n",
      "Eval loss 0.18223726749420166, R2 -6.019589900970459\n",
      "epoch 146, loss 0.17155826091766357, R2 -6.130998611450195\n",
      "Eval loss 0.18037672340869904, R2 -5.947925567626953\n",
      "epoch 147, loss 0.16973862051963806, R2 -6.055369853973389\n",
      "Eval loss 0.17854057252407074, R2 -5.877192497253418\n",
      "epoch 148, loss 0.16794408857822418, R2 -5.980772018432617\n",
      "Eval loss 0.17672845721244812, R2 -5.807395935058594\n",
      "epoch 149, loss 0.16617430746555328, R2 -5.907210826873779\n",
      "Eval loss 0.17494001984596252, R2 -5.7385029792785645\n",
      "epoch 150, loss 0.16442880034446716, R2 -5.834655284881592\n",
      "Eval loss 0.1731749027967453, R2 -5.6705145835876465\n",
      "epoch 151, loss 0.16270728409290314, R2 -5.763101100921631\n",
      "Eval loss 0.171432763338089, R2 -5.603405952453613\n",
      "epoch 152, loss 0.16100932657718658, R2 -5.692516803741455\n",
      "Eval loss 0.16971322894096375, R2 -5.537168979644775\n",
      "epoch 153, loss 0.15933454036712646, R2 -5.622908592224121\n",
      "Eval loss 0.1680159866809845, R2 -5.471798896789551\n",
      "epoch 154, loss 0.15768258273601532, R2 -5.554237365722656\n",
      "Eval loss 0.16634070873260498, R2 -5.407267093658447\n",
      "epoch 155, loss 0.15605314075946808, R2 -5.486514091491699\n",
      "Eval loss 0.16468706727027893, R2 -5.343572616577148\n",
      "epoch 156, loss 0.1544457972049713, R2 -5.419705390930176\n",
      "Eval loss 0.1630547195672989, R2 -5.280694007873535\n",
      "epoch 157, loss 0.15286020934581757, R2 -5.353792667388916\n",
      "Eval loss 0.16144338250160217, R2 -5.218629360198975\n",
      "epoch 158, loss 0.15129609405994415, R2 -5.288782596588135\n",
      "Eval loss 0.1598527431488037, R2 -5.157362461090088\n",
      "epoch 159, loss 0.14975306391716003, R2 -5.224642753601074\n",
      "Eval loss 0.15828247368335724, R2 -5.096877574920654\n",
      "epoch 160, loss 0.14823083579540253, R2 -5.16137170791626\n",
      "Eval loss 0.15673227608203888, R2 -5.0371599197387695\n",
      "epoch 161, loss 0.14672905206680298, R2 -5.098946571350098\n",
      "Eval loss 0.15520186722278595, R2 -4.978210926055908\n",
      "epoch 162, loss 0.14524739980697632, R2 -5.037360191345215\n",
      "Eval loss 0.15369097888469696, R2 -4.920012950897217\n",
      "epoch 163, loss 0.14378559589385986, R2 -4.976598739624023\n",
      "Eval loss 0.15219928324222565, R2 -4.862557411193848\n",
      "epoch 164, loss 0.14234332740306854, R2 -4.916648864746094\n",
      "Eval loss 0.15072649717330933, R2 -4.8058295249938965\n",
      "epoch 165, loss 0.1409202516078949, R2 -4.85750150680542\n",
      "Eval loss 0.1492723971605301, R2 -4.749815464019775\n",
      "epoch 166, loss 0.13951614499092102, R2 -4.799136161804199\n",
      "Eval loss 0.14783667027950287, R2 -4.694516181945801\n",
      "epoch 167, loss 0.13813064992427826, R2 -4.741546630859375\n",
      "Eval loss 0.14641904830932617, R2 -4.639907360076904\n",
      "epoch 168, loss 0.13676351308822632, R2 -4.684718608856201\n",
      "Eval loss 0.1450193077325821, R2 -4.585988998413086\n",
      "epoch 169, loss 0.1354144662618637, R2 -4.628645420074463\n",
      "Eval loss 0.14363715052604675, R2 -4.532751560211182\n",
      "epoch 170, loss 0.13408319652080536, R2 -4.573309898376465\n",
      "Eval loss 0.14227232336997986, R2 -4.480178356170654\n",
      "epoch 171, loss 0.13276948034763336, R2 -4.518704414367676\n",
      "Eval loss 0.1409245878458023, R2 -4.428261756896973\n",
      "epoch 172, loss 0.13147301971912384, R2 -4.464808464050293\n",
      "Eval loss 0.1395936906337738, R2 -4.377001762390137\n",
      "epoch 173, loss 0.1301935762166977, R2 -4.4116339683532715\n",
      "Eval loss 0.13827940821647644, R2 -4.326380252838135\n",
      "epoch 174, loss 0.12893086671829224, R2 -4.359148025512695\n",
      "Eval loss 0.13698148727416992, R2 -4.276384353637695\n",
      "epoch 175, loss 0.12768463790416718, R2 -4.307346820831299\n",
      "Eval loss 0.13569970428943634, R2 -4.2270097732543945\n",
      "epoch 176, loss 0.1264546662569046, R2 -4.256222248077393\n",
      "Eval loss 0.1344338059425354, R2 -4.178244113922119\n",
      "epoch 177, loss 0.12524068355560303, R2 -4.205761909484863\n",
      "Eval loss 0.13318359851837158, R2 -4.130090236663818\n",
      "epoch 178, loss 0.12404248863458633, R2 -4.155952453613281\n",
      "Eval loss 0.13194884359836578, R2 -4.0825300216674805\n",
      "epoch 179, loss 0.12285979092121124, R2 -4.106797695159912\n",
      "Eval loss 0.1307292878627777, R2 -4.035552978515625\n",
      "epoch 180, loss 0.12169239670038223, R2 -4.058262348175049\n",
      "Eval loss 0.12952476739883423, R2 -3.9891605377197266\n",
      "epoch 181, loss 0.1205400675535202, R2 -4.010375499725342\n",
      "Eval loss 0.12833504378795624, R2 -3.943328857421875\n",
      "epoch 182, loss 0.11940257996320724, R2 -3.9630932807922363\n",
      "Eval loss 0.12715990841388702, R2 -3.898064136505127\n",
      "epoch 183, loss 0.11827969551086426, R2 -3.916414737701416\n",
      "Eval loss 0.12599916756153107, R2 -3.853353977203369\n",
      "epoch 184, loss 0.11717123538255692, R2 -3.870345115661621\n",
      "Eval loss 0.12485259026288986, R2 -3.8091859817504883\n",
      "epoch 185, loss 0.11607693135738373, R2 -3.8248605728149414\n",
      "Eval loss 0.12372003495693207, R2 -3.765566825866699\n",
      "epoch 186, loss 0.11499661952257156, R2 -3.7799549102783203\n",
      "Eval loss 0.12260124087333679, R2 -3.722470760345459\n",
      "epoch 187, loss 0.1139300987124443, R2 -3.7356209754943848\n",
      "Eval loss 0.1214960366487503, R2 -3.6799020767211914\n",
      "epoch 188, loss 0.11287713050842285, R2 -3.69185733795166\n",
      "Eval loss 0.12040426582098007, R2 -3.637848377227783\n",
      "epoch 189, loss 0.11183753609657288, R2 -3.6486458778381348\n",
      "Eval loss 0.1193256825208664, R2 -3.596299648284912\n",
      "epoch 190, loss 0.11081112176179886, R2 -3.6059813499450684\n",
      "Eval loss 0.11826013028621674, R2 -3.555258274078369\n",
      "epoch 191, loss 0.10979767888784409, R2 -3.5638551712036133\n",
      "Eval loss 0.11720744520425797, R2 -3.514711380004883\n",
      "epoch 192, loss 0.10879702121019363, R2 -3.522263526916504\n",
      "Eval loss 0.11616743355989456, R2 -3.474647045135498\n",
      "epoch 193, loss 0.10780897736549377, R2 -3.4811902046203613\n",
      "Eval loss 0.1151399165391922, R2 -3.435068130493164\n",
      "epoch 194, loss 0.10683334618806839, R2 -3.440640926361084\n",
      "Eval loss 0.11412472277879715, R2 -3.3959641456604004\n",
      "epoch 195, loss 0.10586994886398315, R2 -3.400595188140869\n",
      "Eval loss 0.11312168091535568, R2 -3.3573293685913086\n",
      "epoch 196, loss 0.10491864383220673, R2 -3.361052989959717\n",
      "Eval loss 0.11213062703609467, R2 -3.319152355194092\n",
      "epoch 197, loss 0.10397919267416, R2 -3.3220038414001465\n",
      "Eval loss 0.11115139722824097, R2 -3.281437397003174\n",
      "epoch 198, loss 0.10305146872997284, R2 -3.2834434509277344\n",
      "Eval loss 0.11018383502960205, R2 -3.244163990020752\n",
      "epoch 199, loss 0.10213528573513031, R2 -3.245361328125\n",
      "Eval loss 0.109227754175663, R2 -3.2073402404785156\n",
      "epoch 200, loss 0.10123048722743988, R2 -3.2077507972717285\n",
      "Eval loss 0.10828302800655365, R2 -3.1709494590759277\n",
      "epoch 201, loss 0.10033690184354782, R2 -3.170609474182129\n",
      "Eval loss 0.10734949260950089, R2 -3.1349868774414062\n",
      "epoch 202, loss 0.0994543731212616, R2 -3.133922576904297\n",
      "Eval loss 0.10642697662115097, R2 -3.099454879760742\n",
      "epoch 203, loss 0.09858275204896927, R2 -3.097696304321289\n",
      "Eval loss 0.10551536083221436, R2 -3.0643391609191895\n",
      "epoch 204, loss 0.09772184491157532, R2 -3.061910629272461\n",
      "Eval loss 0.10461447387933731, R2 -3.0296378135681152\n",
      "epoch 205, loss 0.09687153995037079, R2 -3.026566505432129\n",
      "Eval loss 0.10372418165206909, R2 -2.9953484535217285\n",
      "epoch 206, loss 0.09603168070316315, R2 -2.9916584491729736\n",
      "Eval loss 0.10284434258937836, R2 -2.961458206176758\n",
      "epoch 207, loss 0.09520208835601807, R2 -2.9571754932403564\n",
      "Eval loss 0.10197480767965317, R2 -2.9279608726501465\n",
      "epoch 208, loss 0.09438265860080719, R2 -2.923110246658325\n",
      "Eval loss 0.10111543536186218, R2 -2.8948588371276855\n",
      "epoch 209, loss 0.0935732051730156, R2 -2.8894705772399902\n",
      "Eval loss 0.10026608407497406, R2 -2.8621437549591064\n",
      "epoch 210, loss 0.09277361631393433, R2 -2.856229782104492\n",
      "Eval loss 0.09942663460969925, R2 -2.829810380935669\n",
      "epoch 211, loss 0.09198375046253204, R2 -2.823399543762207\n",
      "Eval loss 0.09859694540500641, R2 -2.7978527545928955\n",
      "epoch 212, loss 0.09120345860719681, R2 -2.790963649749756\n",
      "Eval loss 0.09777690470218658, R2 -2.7662620544433594\n",
      "epoch 213, loss 0.09043261408805847, R2 -2.7589263916015625\n",
      "Eval loss 0.09696634858846664, R2 -2.73504376411438\n",
      "epoch 214, loss 0.08967109769582748, R2 -2.7272706031799316\n",
      "Eval loss 0.09616517275571823, R2 -2.70418119430542\n",
      "epoch 215, loss 0.08891873806715012, R2 -2.6960015296936035\n",
      "Eval loss 0.0953732430934906, R2 -2.6736791133880615\n",
      "epoch 216, loss 0.0881754457950592, R2 -2.6651010513305664\n",
      "Eval loss 0.0945904478430748, R2 -2.643524408340454\n",
      "epoch 217, loss 0.087441086769104, R2 -2.634577989578247\n",
      "Eval loss 0.09381666779518127, R2 -2.6137211322784424\n",
      "epoch 218, loss 0.08671550452709198, R2 -2.6044230461120605\n",
      "Eval loss 0.09305174648761749, R2 -2.5842573642730713\n",
      "epoch 219, loss 0.08599862456321716, R2 -2.5746240615844727\n",
      "Eval loss 0.09229563176631927, R2 -2.5551304817199707\n",
      "epoch 220, loss 0.08529030531644821, R2 -2.5451762676239014\n",
      "Eval loss 0.0915481448173523, R2 -2.5263359546661377\n",
      "epoch 221, loss 0.08459042012691498, R2 -2.5160861015319824\n",
      "Eval loss 0.09080920368432999, R2 -2.497875928878784\n",
      "epoch 222, loss 0.08389884978532791, R2 -2.487346649169922\n",
      "Eval loss 0.0900786966085434, R2 -2.4697353839874268\n",
      "epoch 223, loss 0.08321550488471985, R2 -2.4589385986328125\n",
      "Eval loss 0.08935648947954178, R2 -2.441917896270752\n",
      "epoch 224, loss 0.08254025876522064, R2 -2.4308700561523438\n",
      "Eval loss 0.08864250034093857, R2 -2.4144158363342285\n",
      "epoch 225, loss 0.08187299221754074, R2 -2.403137683868408\n",
      "Eval loss 0.08793661743402481, R2 -2.3872246742248535\n",
      "epoch 226, loss 0.08121359348297119, R2 -2.3757290840148926\n",
      "Eval loss 0.08723871409893036, R2 -2.360344409942627\n",
      "epoch 227, loss 0.08056196570396423, R2 -2.348640203475952\n",
      "Eval loss 0.08654870837926865, R2 -2.3337628841400146\n",
      "epoch 228, loss 0.07991799712181091, R2 -2.3218719959259033\n",
      "Eval loss 0.08586649596691132, R2 -2.307487726211548\n",
      "epoch 229, loss 0.07928159087896347, R2 -2.295419216156006\n",
      "Eval loss 0.08519195765256882, R2 -2.281503438949585\n",
      "epoch 230, loss 0.07865263521671295, R2 -2.2692768573760986\n",
      "Eval loss 0.08452500402927399, R2 -2.2558131217956543\n",
      "epoch 231, loss 0.078031025826931, R2 -2.243441343307495\n",
      "Eval loss 0.08386554569005966, R2 -2.230412483215332\n",
      "epoch 232, loss 0.07741668075323105, R2 -2.2179083824157715\n",
      "Eval loss 0.08321347832679749, R2 -2.2052955627441406\n",
      "epoch 233, loss 0.07680947333574295, R2 -2.192667007446289\n",
      "Eval loss 0.0825687050819397, R2 -2.1804611682891846\n",
      "epoch 234, loss 0.07620933651924133, R2 -2.167719841003418\n",
      "Eval loss 0.08193112909793854, R2 -2.155900716781616\n",
      "epoch 235, loss 0.07561615109443665, R2 -2.1430654525756836\n",
      "Eval loss 0.08130066841840744, R2 -2.1316168308258057\n",
      "epoch 236, loss 0.07502984255552292, R2 -2.1186940670013428\n",
      "Eval loss 0.08067721873521805, R2 -2.107603073120117\n",
      "epoch 237, loss 0.0744502991437912, R2 -2.0946028232574463\n",
      "Eval loss 0.0800606831908226, R2 -2.0838541984558105\n",
      "epoch 238, loss 0.07387744635343552, R2 -2.0707952976226807\n",
      "Eval loss 0.07945099472999573, R2 -2.060370445251465\n",
      "epoch 239, loss 0.07331118732690811, R2 -2.0472571849823\n",
      "Eval loss 0.07884807884693146, R2 -2.0371429920196533\n",
      "epoch 240, loss 0.07275144010782242, R2 -2.0239906311035156\n",
      "Eval loss 0.07825180143117905, R2 -2.01417875289917\n",
      "epoch 241, loss 0.07219810038805008, R2 -2.0009875297546387\n",
      "Eval loss 0.07766211032867432, R2 -1.991464376449585\n",
      "epoch 242, loss 0.07165110111236572, R2 -1.9782538414001465\n",
      "Eval loss 0.0770789161324501, R2 -1.9690003395080566\n",
      "epoch 243, loss 0.07111035287380219, R2 -1.9557762145996094\n",
      "Eval loss 0.07650212943553925, R2 -1.9467813968658447\n",
      "epoch 244, loss 0.0705757737159729, R2 -1.9335567951202393\n",
      "Eval loss 0.07593168318271637, R2 -1.9248077869415283\n",
      "epoch 245, loss 0.0700472742319107, R2 -1.9115891456604004\n",
      "Eval loss 0.07536749541759491, R2 -1.9030780792236328\n",
      "epoch 246, loss 0.06952477246522903, R2 -1.8898708820343018\n",
      "Eval loss 0.0748094692826271, R2 -1.8815827369689941\n",
      "epoch 247, loss 0.06900820136070251, R2 -1.8684008121490479\n",
      "Eval loss 0.07425753772258759, R2 -1.860321283340454\n",
      "epoch 248, loss 0.06849747151136398, R2 -1.847172737121582\n",
      "Eval loss 0.07371163368225098, R2 -1.83929443359375\n",
      "epoch 249, loss 0.06799252331256866, R2 -1.826181173324585\n",
      "Eval loss 0.07317166030406952, R2 -1.818495273590088\n",
      "epoch 250, loss 0.0674932524561882, R2 -1.8054258823394775\n",
      "Eval loss 0.07263756543397903, R2 -1.7979199886322021\n",
      "epoch 251, loss 0.06699962168931961, R2 -1.7849109172821045\n",
      "Eval loss 0.07210925221443176, R2 -1.7775723934173584\n",
      "epoch 252, loss 0.06651151180267334, R2 -1.764617919921875\n",
      "Eval loss 0.07158667594194412, R2 -1.7574434280395508\n",
      "epoch 253, loss 0.0660289004445076, R2 -1.7445611953735352\n",
      "Eval loss 0.07106974720954895, R2 -1.7375319004058838\n",
      "epoch 254, loss 0.06555166840553284, R2 -1.724724531173706\n",
      "Eval loss 0.07055839896202087, R2 -1.7178349494934082\n",
      "epoch 255, loss 0.06507977843284607, R2 -1.7051100730895996\n",
      "Eval loss 0.07005256414413452, R2 -1.6983511447906494\n",
      "epoch 256, loss 0.06461314111948013, R2 -1.6857128143310547\n",
      "Eval loss 0.06955216079950333, R2 -1.6790766716003418\n",
      "epoch 257, loss 0.06415171176195145, R2 -1.6665329933166504\n",
      "Eval loss 0.06905713677406311, R2 -1.6600072383880615\n",
      "epoch 258, loss 0.06369540095329285, R2 -1.6475670337677002\n",
      "Eval loss 0.0685674250125885, R2 -1.6411442756652832\n",
      "epoch 259, loss 0.06324415653944016, R2 -1.6288104057312012\n",
      "Eval loss 0.06808295100927353, R2 -1.6224827766418457\n",
      "epoch 260, loss 0.0627978965640068, R2 -1.6102619171142578\n",
      "Eval loss 0.06760367006063461, R2 -1.6040220260620117\n",
      "epoch 261, loss 0.062356576323509216, R2 -1.5919146537780762\n",
      "Eval loss 0.06712948530912399, R2 -1.5857563018798828\n",
      "epoch 262, loss 0.06192011386156082, R2 -1.5737743377685547\n",
      "Eval loss 0.06666035205125809, R2 -1.5676865577697754\n",
      "epoch 263, loss 0.06148845702409744, R2 -1.5558295249938965\n",
      "Eval loss 0.06619621813297272, R2 -1.5498101711273193\n",
      "epoch 264, loss 0.061061546206474304, R2 -1.5380871295928955\n",
      "Eval loss 0.06573702394962311, R2 -1.532120704650879\n",
      "epoch 265, loss 0.06063932180404663, R2 -1.5205366611480713\n",
      "Eval loss 0.0652826800942421, R2 -1.5146205425262451\n",
      "epoch 266, loss 0.06022170931100845, R2 -1.503175973892212\n",
      "Eval loss 0.0648331418633461, R2 -1.4973037242889404\n",
      "epoch 267, loss 0.05980867147445679, R2 -1.4860115051269531\n",
      "Eval loss 0.06438835710287094, R2 -1.480173110961914\n",
      "epoch 268, loss 0.05940013751387596, R2 -1.4690303802490234\n",
      "Eval loss 0.06394827365875244, R2 -1.4632186889648438\n",
      "epoch 269, loss 0.0589960478246212, R2 -1.4522333145141602\n",
      "Eval loss 0.06351282447576523, R2 -1.4464483261108398\n",
      "epoch 270, loss 0.05859634652733803, R2 -1.4356184005737305\n",
      "Eval loss 0.06308194249868393, R2 -1.4298498630523682\n",
      "epoch 271, loss 0.05820099264383316, R2 -1.4191834926605225\n",
      "Eval loss 0.06265559047460556, R2 -1.4134280681610107\n",
      "epoch 272, loss 0.05780990421772003, R2 -1.4029300212860107\n",
      "Eval loss 0.062233712524175644, R2 -1.3971784114837646\n",
      "epoch 273, loss 0.05742305889725685, R2 -1.386847972869873\n",
      "Eval loss 0.06181623786687851, R2 -1.3810970783233643\n",
      "epoch 274, loss 0.05704038217663765, R2 -1.3709440231323242\n",
      "Eval loss 0.06140312924981117, R2 -1.3651831150054932\n",
      "epoch 275, loss 0.056661829352378845, R2 -1.3552091121673584\n",
      "Eval loss 0.060994334518909454, R2 -1.3494396209716797\n",
      "epoch 276, loss 0.05628734454512596, R2 -1.339643955230713\n",
      "Eval loss 0.06058979034423828, R2 -1.3338541984558105\n",
      "epoch 277, loss 0.055916883051395416, R2 -1.324244499206543\n",
      "Eval loss 0.06018945947289467, R2 -1.3184359073638916\n",
      "epoch 278, loss 0.05555038899183273, R2 -1.3090107440948486\n",
      "Eval loss 0.05979328602552414, R2 -1.3031730651855469\n",
      "epoch 279, loss 0.05518781393766403, R2 -1.2939398288726807\n",
      "Eval loss 0.05940120667219162, R2 -1.288071870803833\n",
      "epoch 280, loss 0.054829102009534836, R2 -1.2790296077728271\n",
      "Eval loss 0.05901319161057472, R2 -1.273125171661377\n",
      "epoch 281, loss 0.05447422340512276, R2 -1.2642781734466553\n",
      "Eval loss 0.05862918868660927, R2 -1.258335828781128\n",
      "epoch 282, loss 0.05412311851978302, R2 -1.2496862411499023\n",
      "Eval loss 0.05824914574623108, R2 -1.2436957359313965\n",
      "epoch 283, loss 0.05377574265003204, R2 -1.235245704650879\n",
      "Eval loss 0.05787300318479538, R2 -1.2292084693908691\n",
      "epoch 284, loss 0.05343203991651535, R2 -1.2209594249725342\n",
      "Eval loss 0.05750073492527008, R2 -1.2148683071136475\n",
      "epoch 285, loss 0.05309198051691055, R2 -1.2068235874176025\n",
      "Eval loss 0.0571322925388813, R2 -1.2006769180297852\n",
      "epoch 286, loss 0.05275552347302437, R2 -1.1928391456604004\n",
      "Eval loss 0.05676763132214546, R2 -1.186629295349121\n",
      "epoch 287, loss 0.05242259427905083, R2 -1.1790013313293457\n",
      "Eval loss 0.056406691670417786, R2 -1.1727275848388672\n",
      "epoch 288, loss 0.05209318548440933, R2 -1.1653084754943848\n",
      "Eval loss 0.056049447506666183, R2 -1.1589655876159668\n",
      "epoch 289, loss 0.05176723375916481, R2 -1.1517598628997803\n",
      "Eval loss 0.05569584667682648, R2 -1.1453452110290527\n",
      "epoch 290, loss 0.05144469812512398, R2 -1.1383554935455322\n",
      "Eval loss 0.055345844477415085, R2 -1.1318628787994385\n",
      "epoch 291, loss 0.05112554132938385, R2 -1.1250858306884766\n",
      "Eval loss 0.054999396204948425, R2 -1.1185169219970703\n",
      "epoch 292, loss 0.050809722393751144, R2 -1.1119599342346191\n",
      "Eval loss 0.05465647950768471, R2 -1.1053106784820557\n",
      "epoch 293, loss 0.05049720034003258, R2 -1.0989668369293213\n",
      "Eval loss 0.054317034780979156, R2 -1.0922348499298096\n",
      "epoch 294, loss 0.05018793046474457, R2 -1.0861117839813232\n",
      "Eval loss 0.05398101732134819, R2 -1.0792932510375977\n",
      "epoch 295, loss 0.04988187551498413, R2 -1.0733928680419922\n",
      "Eval loss 0.05364840850234032, R2 -1.0664794445037842\n",
      "epoch 296, loss 0.04957900196313858, R2 -1.0608031749725342\n",
      "Eval loss 0.05331914499402046, R2 -1.0537991523742676\n",
      "epoch 297, loss 0.049279261380434036, R2 -1.0483427047729492\n",
      "Eval loss 0.052993204444646835, R2 -1.04124116897583\n",
      "epoch 298, loss 0.048982635140419006, R2 -1.036013126373291\n",
      "Eval loss 0.052670545876026154, R2 -1.0288145542144775\n",
      "epoch 299, loss 0.048689063638448715, R2 -1.0238125324249268\n",
      "Eval loss 0.052351128309965134, R2 -1.016510248184204\n",
      "epoch 300, loss 0.048398517072200775, R2 -1.0117368698120117\n",
      "Eval loss 0.052034903317689896, R2 -1.0043284893035889\n",
      "epoch 301, loss 0.0481109619140625, R2 -0.9997830390930176\n",
      "Eval loss 0.051721859723329544, R2 -0.9922726154327393\n",
      "epoch 302, loss 0.047826360911130905, R2 -0.9879528284072876\n",
      "Eval loss 0.0514119453728199, R2 -0.9803345203399658\n",
      "epoch 303, loss 0.0475446842610836, R2 -0.9762445688247681\n",
      "Eval loss 0.051105111837387085, R2 -0.9685150384902954\n",
      "epoch 304, loss 0.04726588726043701, R2 -0.9646579027175903\n",
      "Eval loss 0.050801340490579605, R2 -0.9568153619766235\n",
      "epoch 305, loss 0.04698994755744934, R2 -0.9531863927841187\n",
      "Eval loss 0.050500594079494476, R2 -0.9452308416366577\n",
      "epoch 306, loss 0.04671681299805641, R2 -0.9418319463729858\n",
      "Eval loss 0.05020284652709961, R2 -0.9337613582611084\n",
      "epoch 307, loss 0.04644646868109703, R2 -0.9305959939956665\n",
      "Eval loss 0.04990803822875023, R2 -0.9224051237106323\n",
      "epoch 308, loss 0.04617886617779732, R2 -0.9194735288619995\n",
      "Eval loss 0.04961615800857544, R2 -0.9111621379852295\n",
      "epoch 309, loss 0.04591398313641548, R2 -0.9084633588790894\n",
      "Eval loss 0.04932716488838196, R2 -0.9000303745269775\n",
      "epoch 310, loss 0.04565178602933884, R2 -0.8975642919540405\n",
      "Eval loss 0.0490410290658474, R2 -0.8890093564987183\n",
      "epoch 311, loss 0.045392245054244995, R2 -0.8867744207382202\n",
      "Eval loss 0.04875770956277847, R2 -0.8780961036682129\n",
      "epoch 312, loss 0.04513532668352127, R2 -0.8760982751846313\n",
      "Eval loss 0.048477187752723694, R2 -0.8672890663146973\n",
      "epoch 313, loss 0.04488100856542587, R2 -0.8655266761779785\n",
      "Eval loss 0.04819942265748978, R2 -0.8565919399261475\n",
      "epoch 314, loss 0.04462923854589462, R2 -0.8550609350204468\n",
      "Eval loss 0.047924384474754333, R2 -0.8459968566894531\n",
      "epoch 315, loss 0.044380005449056625, R2 -0.8447012901306152\n",
      "Eval loss 0.04765203222632408, R2 -0.8355059623718262\n",
      "epoch 316, loss 0.044133260846138, R2 -0.8344446420669556\n",
      "Eval loss 0.04738235846161842, R2 -0.8251179456710815\n",
      "epoch 317, loss 0.04388900846242905, R2 -0.8242925405502319\n",
      "Eval loss 0.04711531475186348, R2 -0.8148317337036133\n",
      "epoch 318, loss 0.04364718124270439, R2 -0.8142414093017578\n",
      "Eval loss 0.046850886195898056, R2 -0.8046468496322632\n",
      "epoch 319, loss 0.043407782912254333, R2 -0.8042881488800049\n",
      "Eval loss 0.04658902436494827, R2 -0.7945595979690552\n",
      "epoch 320, loss 0.043170761317014694, R2 -0.794438362121582\n",
      "Eval loss 0.046329714357852936, R2 -0.7845717668533325\n",
      "epoch 321, loss 0.04293610155582428, R2 -0.784684419631958\n",
      "Eval loss 0.04607292637228966, R2 -0.7746795415878296\n",
      "epoch 322, loss 0.0427037738263607, R2 -0.7750276327133179\n",
      "Eval loss 0.04581862688064575, R2 -0.7648851871490479\n",
      "epoch 323, loss 0.04247376322746277, R2 -0.7654669284820557\n",
      "Eval loss 0.04556678980588913, R2 -0.7551858425140381\n",
      "epoch 324, loss 0.0422460101544857, R2 -0.7560001611709595\n",
      "Eval loss 0.0453173890709877, R2 -0.7455791234970093\n",
      "epoch 325, loss 0.042020514607429504, R2 -0.746625542640686\n",
      "Eval loss 0.045070402324199677, R2 -0.7360644340515137\n",
      "epoch 326, loss 0.041797250509262085, R2 -0.737346887588501\n",
      "Eval loss 0.04482579603791237, R2 -0.7266428470611572\n",
      "epoch 327, loss 0.041576188057661057, R2 -0.7281582355499268\n",
      "Eval loss 0.04458354413509369, R2 -0.7173106670379639\n",
      "epoch 328, loss 0.04135730490088463, R2 -0.7190601825714111\n",
      "Eval loss 0.044343624264001846, R2 -0.7080686092376709\n",
      "epoch 329, loss 0.04114056006073952, R2 -0.7100504636764526\n",
      "Eval loss 0.04410600662231445, R2 -0.6989169120788574\n",
      "epoch 330, loss 0.040925949811935425, R2 -0.7011293172836304\n",
      "Eval loss 0.04387066513299942, R2 -0.6898518800735474\n",
      "epoch 331, loss 0.04071343317627907, R2 -0.6922953128814697\n",
      "Eval loss 0.043637581169605255, R2 -0.6808736324310303\n",
      "epoch 332, loss 0.04050299897789955, R2 -0.6835484504699707\n",
      "Eval loss 0.04340672120451927, R2 -0.671981692314148\n",
      "epoch 333, loss 0.04029461741447449, R2 -0.6748883724212646\n",
      "Eval loss 0.04317807778716087, R2 -0.6631749868392944\n",
      "epoch 334, loss 0.04008826985955238, R2 -0.6663113832473755\n",
      "Eval loss 0.04295160248875618, R2 -0.6544514894485474\n",
      "epoch 335, loss 0.03988393023610115, R2 -0.6578178405761719\n",
      "Eval loss 0.042727287858724594, R2 -0.645810604095459\n",
      "epoch 336, loss 0.0396815724670887, R2 -0.6494065523147583\n",
      "Eval loss 0.04250510409474373, R2 -0.6372513771057129\n",
      "epoch 337, loss 0.03948117420077324, R2 -0.6410763263702393\n",
      "Eval loss 0.042285021394491196, R2 -0.6287754774093628\n",
      "epoch 338, loss 0.039282724261283875, R2 -0.6328279972076416\n",
      "Eval loss 0.042067043483257294, R2 -0.6203790903091431\n",
      "epoch 339, loss 0.03908618912100792, R2 -0.6246588230133057\n",
      "Eval loss 0.04185111075639725, R2 -0.6120617389678955\n",
      "epoch 340, loss 0.03889154642820358, R2 -0.616567850112915\n",
      "Eval loss 0.041637226939201355, R2 -0.6038216352462769\n",
      "epoch 341, loss 0.03869878500699997, R2 -0.6085554361343384\n",
      "Eval loss 0.04142536595463753, R2 -0.5956604480743408\n",
      "epoch 342, loss 0.03850787878036499, R2 -0.600621223449707\n",
      "Eval loss 0.04121549427509308, R2 -0.5875784158706665\n",
      "epoch 343, loss 0.038318801671266556, R2 -0.5927616357803345\n",
      "Eval loss 0.04100760444998741, R2 -0.5795692205429077\n",
      "epoch 344, loss 0.03813154622912407, R2 -0.5849781036376953\n",
      "Eval loss 0.04080166295170784, R2 -0.5716370344161987\n",
      "epoch 345, loss 0.037946078926324844, R2 -0.5772684812545776\n",
      "Eval loss 0.04059765487909317, R2 -0.5637785196304321\n",
      "epoch 346, loss 0.03776238113641739, R2 -0.5696333646774292\n",
      "Eval loss 0.04039556160569191, R2 -0.5559943914413452\n",
      "epoch 347, loss 0.03758044168353081, R2 -0.5620713233947754\n",
      "Eval loss 0.04019535705447197, R2 -0.5482823848724365\n",
      "epoch 348, loss 0.03740023449063301, R2 -0.5545798540115356\n",
      "Eval loss 0.039997026324272156, R2 -0.5406427383422852\n",
      "epoch 349, loss 0.03722174093127251, R2 -0.5471601486206055\n",
      "Eval loss 0.03980053961277008, R2 -0.5330740213394165\n",
      "epoch 350, loss 0.0370449498295784, R2 -0.5398110151290894\n",
      "Eval loss 0.03960588946938515, R2 -0.5255770683288574\n",
      "epoch 351, loss 0.036869827657938004, R2 -0.5325329303741455\n",
      "Eval loss 0.03941305726766586, R2 -0.5181493759155273\n",
      "epoch 352, loss 0.03669637441635132, R2 -0.5253236293792725\n",
      "Eval loss 0.039222005754709244, R2 -0.510790228843689\n",
      "epoch 353, loss 0.03652454912662506, R2 -0.518181562423706\n",
      "Eval loss 0.03903272747993469, R2 -0.5034990310668945\n",
      "epoch 354, loss 0.03635435551404953, R2 -0.5111052989959717\n",
      "Eval loss 0.038845211267471313, R2 -0.49627745151519775\n",
      "epoch 355, loss 0.03618576005101204, R2 -0.5040980577468872\n",
      "Eval loss 0.03865942358970642, R2 -0.48912107944488525\n",
      "epoch 356, loss 0.03601875528693199, R2 -0.4971578121185303\n",
      "Eval loss 0.03847534954547882, R2 -0.48203039169311523\n",
      "epoch 357, loss 0.035853322595357895, R2 -0.49028122425079346\n",
      "Eval loss 0.038292985409498215, R2 -0.4750058650970459\n",
      "epoch 358, loss 0.03568943962454796, R2 -0.48346877098083496\n",
      "Eval loss 0.03811229392886162, R2 -0.4680461883544922\n",
      "epoch 359, loss 0.03552709147334099, R2 -0.4767211675643921\n",
      "Eval loss 0.037933263927698135, R2 -0.46115028858184814\n",
      "epoch 360, loss 0.03536626324057579, R2 -0.47003626823425293\n",
      "Eval loss 0.03775588423013687, R2 -0.45431697368621826\n",
      "epoch 361, loss 0.03520693629980087, R2 -0.4634131193161011\n",
      "Eval loss 0.03758012875914574, R2 -0.44754600524902344\n",
      "epoch 362, loss 0.03504909574985504, R2 -0.4568527936935425\n",
      "Eval loss 0.037405986338853836, R2 -0.44083917140960693\n",
      "epoch 363, loss 0.034892722964286804, R2 -0.45035111904144287\n",
      "Eval loss 0.03723343834280968, R2 -0.43419313430786133\n",
      "epoch 364, loss 0.03473780304193497, R2 -0.4439135789871216\n",
      "Eval loss 0.03706246241927147, R2 -0.4276069402694702\n",
      "epoch 365, loss 0.03458432853221893, R2 -0.4375342130661011\n",
      "Eval loss 0.03689304739236832, R2 -0.4210808277130127\n",
      "epoch 366, loss 0.03443227335810661, R2 -0.43121349811553955\n",
      "Eval loss 0.036725182086229324, R2 -0.41461610794067383\n",
      "epoch 367, loss 0.03428162261843681, R2 -0.4249502420425415\n",
      "Eval loss 0.036558836698532104, R2 -0.40820789337158203\n",
      "epoch 368, loss 0.03413236886262894, R2 -0.418748140335083\n",
      "Eval loss 0.03639400750398636, R2 -0.40185797214508057\n",
      "epoch 369, loss 0.03398449346423149, R2 -0.41260015964508057\n",
      "Eval loss 0.036230675876140594, R2 -0.3955674171447754\n",
      "epoch 370, loss 0.033837977796792984, R2 -0.40651094913482666\n",
      "Eval loss 0.03606882318854332, R2 -0.3893343210220337\n",
      "epoch 371, loss 0.03369281068444252, R2 -0.40047693252563477\n",
      "Eval loss 0.03590843081474304, R2 -0.38315582275390625\n",
      "epoch 372, loss 0.033548977226018906, R2 -0.394498348236084\n",
      "Eval loss 0.03574949502944946, R2 -0.3770325183868408\n",
      "epoch 373, loss 0.03340646252036095, R2 -0.38857507705688477\n",
      "Eval loss 0.03559198975563049, R2 -0.3709656000137329\n",
      "epoch 374, loss 0.03326525539159775, R2 -0.38270556926727295\n",
      "Eval loss 0.035435907542705536, R2 -0.36495471000671387\n",
      "epoch 375, loss 0.03312533721327782, R2 -0.3768879175186157\n",
      "Eval loss 0.0352812297642231, R2 -0.35899579524993896\n",
      "epoch 376, loss 0.03298669680953026, R2 -0.37112700939178467\n",
      "Eval loss 0.035127945244312286, R2 -0.3530910015106201\n",
      "epoch 377, loss 0.03284931927919388, R2 -0.3654162883758545\n",
      "Eval loss 0.03497603163123131, R2 -0.3472398519515991\n",
      "epoch 378, loss 0.03271319344639778, R2 -0.3597588539123535\n",
      "Eval loss 0.03482547774910927, R2 -0.3414410352706909\n",
      "epoch 379, loss 0.03257830813527107, R2 -0.3541518449783325\n",
      "Eval loss 0.03467627614736557, R2 -0.33569443225860596\n",
      "epoch 380, loss 0.03244464099407196, R2 -0.3485950231552124\n",
      "Eval loss 0.03452841192483902, R2 -0.3299992084503174\n",
      "epoch 381, loss 0.032312195748090744, R2 -0.34309065341949463\n",
      "Eval loss 0.03438186272978783, R2 -0.32435309886932373\n",
      "epoch 382, loss 0.03218093886971474, R2 -0.3376343250274658\n",
      "Eval loss 0.03423662111163139, R2 -0.3187582492828369\n",
      "epoch 383, loss 0.03205087408423424, R2 -0.33222639560699463\n",
      "Eval loss 0.03409267216920853, R2 -0.3132137060165405\n",
      "epoch 384, loss 0.031921979039907455, R2 -0.32687127590179443\n",
      "Eval loss 0.03395000472664833, R2 -0.3077183961868286\n",
      "epoch 385, loss 0.03179424628615379, R2 -0.321560263633728\n",
      "Eval loss 0.03380860388278961, R2 -0.3022722005844116\n",
      "epoch 386, loss 0.031667664647102356, R2 -0.31630003452301025\n",
      "Eval loss 0.03366846218705177, R2 -0.2968754768371582\n",
      "epoch 387, loss 0.03154221549630165, R2 -0.31108570098876953\n",
      "Eval loss 0.033529557287693024, R2 -0.29152393341064453\n",
      "epoch 388, loss 0.03141790255904198, R2 -0.3059183359146118\n",
      "Eval loss 0.03339188173413277, R2 -0.28621959686279297\n",
      "epoch 389, loss 0.031294696033000946, R2 -0.30079710483551025\n",
      "Eval loss 0.03325542062520981, R2 -0.2809644937515259\n",
      "epoch 390, loss 0.03117259033024311, R2 -0.29572176933288574\n",
      "Eval loss 0.03312016278505325, R2 -0.2757537364959717\n",
      "epoch 391, loss 0.031051576137542725, R2 -0.290691614151001\n",
      "Eval loss 0.032986100763082504, R2 -0.2705901861190796\n",
      "epoch 392, loss 0.030931642279028893, R2 -0.2857060432434082\n",
      "Eval loss 0.03285321593284607, R2 -0.2654712200164795\n",
      "epoch 393, loss 0.03081277199089527, R2 -0.28076350688934326\n",
      "Eval loss 0.03272149711847305, R2 -0.2603968381881714\n",
      "epoch 394, loss 0.03069496713578701, R2 -0.27586841583251953\n",
      "Eval loss 0.03259093314409256, R2 -0.25536811351776123\n",
      "epoch 395, loss 0.03057820349931717, R2 -0.2710157632827759\n",
      "Eval loss 0.03246152028441429, R2 -0.2503836154937744\n",
      "epoch 396, loss 0.03046247735619545, R2 -0.2662038803100586\n",
      "Eval loss 0.03233323246240616, R2 -0.24544203281402588\n",
      "epoch 397, loss 0.03034777007997036, R2 -0.2614368200302124\n",
      "Eval loss 0.03220606967806816, R2 -0.24054455757141113\n",
      "epoch 398, loss 0.03023407980799675, R2 -0.25671112537384033\n",
      "Eval loss 0.032080017030239105, R2 -0.2356884479522705\n",
      "epoch 399, loss 0.030121397227048874, R2 -0.2520277500152588\n",
      "Eval loss 0.031955063343048096, R2 -0.23087537288665771\n",
      "epoch 400, loss 0.03000970371067524, R2 -0.2473851442337036\n",
      "Eval loss 0.03183119744062424, R2 -0.22610414028167725\n",
      "epoch 401, loss 0.0298989936709404, R2 -0.24278342723846436\n",
      "Eval loss 0.031708408147096634, R2 -0.22137451171875\n",
      "epoch 402, loss 0.029789261519908905, R2 -0.23822104930877686\n",
      "Eval loss 0.03158668801188469, R2 -0.21668589115142822\n",
      "epoch 403, loss 0.029680486768484116, R2 -0.23370087146759033\n",
      "Eval loss 0.03146601840853691, R2 -0.21203792095184326\n",
      "epoch 404, loss 0.02957266755402088, R2 -0.22921931743621826\n",
      "Eval loss 0.0313463918864727, R2 -0.20743036270141602\n",
      "epoch 405, loss 0.02946579083800316, R2 -0.22477686405181885\n",
      "Eval loss 0.03122781217098236, R2 -0.20286238193511963\n",
      "epoch 406, loss 0.029359852895140648, R2 -0.22037339210510254\n",
      "Eval loss 0.031110243871808052, R2 -0.1983342170715332\n",
      "epoch 407, loss 0.029254833236336708, R2 -0.21600854396820068\n",
      "Eval loss 0.03099370002746582, R2 -0.19384467601776123\n",
      "epoch 408, loss 0.02915072999894619, R2 -0.21167981624603271\n",
      "Eval loss 0.030878150835633278, R2 -0.18939244747161865\n",
      "epoch 409, loss 0.029047537595033646, R2 -0.207391619682312\n",
      "Eval loss 0.030763601884245872, R2 -0.1849818229675293\n",
      "epoch 410, loss 0.028945239260792732, R2 -0.20313799381256104\n",
      "Eval loss 0.03065003827214241, R2 -0.18060815334320068\n",
      "epoch 411, loss 0.028843827545642853, R2 -0.19892430305480957\n",
      "Eval loss 0.0305374413728714, R2 -0.1762700080871582\n",
      "epoch 412, loss 0.02874329499900341, R2 -0.19474554061889648\n",
      "Eval loss 0.030425814911723137, R2 -0.17197072505950928\n",
      "epoch 413, loss 0.02864363044500351, R2 -0.19060170650482178\n",
      "Eval loss 0.03031514398753643, R2 -0.1677076816558838\n",
      "epoch 414, loss 0.028544830158352852, R2 -0.18649613857269287\n",
      "Eval loss 0.030205419287085533, R2 -0.16348063945770264\n",
      "epoch 415, loss 0.028446882963180542, R2 -0.18242478370666504\n",
      "Eval loss 0.03009662963449955, R2 -0.15929114818572998\n",
      "epoch 416, loss 0.028349779546260834, R2 -0.1783885955810547\n",
      "Eval loss 0.02998877316713333, R2 -0.15513527393341064\n",
      "epoch 417, loss 0.02825350873172283, R2 -0.1743859052658081\n",
      "Eval loss 0.029881827533245087, R2 -0.1510162353515625\n",
      "epoch 418, loss 0.028158066794276237, R2 -0.1704195737838745\n",
      "Eval loss 0.029775794595479965, R2 -0.14693224430084229\n",
      "epoch 419, loss 0.028063444420695305, R2 -0.16648566722869873\n",
      "Eval loss 0.029670661315321922, R2 -0.14288365840911865\n",
      "epoch 420, loss 0.02796962857246399, R2 -0.16258692741394043\n",
      "Eval loss 0.02956642583012581, R2 -0.13886821269989014\n",
      "epoch 421, loss 0.02787662111222744, R2 -0.1587216854095459\n",
      "Eval loss 0.029463065788149834, R2 -0.13488590717315674\n",
      "epoch 422, loss 0.027784405276179314, R2 -0.15488815307617188\n",
      "Eval loss 0.029360583052039146, R2 -0.13093841075897217\n",
      "epoch 423, loss 0.027692975476384163, R2 -0.15108823776245117\n",
      "Eval loss 0.029258964583277702, R2 -0.12702453136444092\n",
      "epoch 424, loss 0.02760232612490654, R2 -0.14731991291046143\n",
      "Eval loss 0.029158204793930054, R2 -0.12314295768737793\n",
      "epoch 425, loss 0.027512449771165848, R2 -0.14358413219451904\n",
      "Eval loss 0.029058294370770454, R2 -0.1192941665649414\n",
      "epoch 426, loss 0.02742333523929119, R2 -0.13988006114959717\n",
      "Eval loss 0.028959224000573158, R2 -0.11547911167144775\n",
      "epoch 427, loss 0.027334976941347122, R2 -0.13620686531066895\n",
      "Eval loss 0.028860989958047867, R2 -0.1116943359375\n",
      "epoch 428, loss 0.027247367426753044, R2 -0.13256430625915527\n",
      "Eval loss 0.028763579204678535, R2 -0.10794341564178467\n",
      "epoch 429, loss 0.02716049924492836, R2 -0.12895560264587402\n",
      "Eval loss 0.028666986152529716, R2 -0.10422277450561523\n",
      "epoch 430, loss 0.027074364945292473, R2 -0.12537360191345215\n",
      "Eval loss 0.028571197763085365, R2 -0.10053300857543945\n",
      "epoch 431, loss 0.026988955214619637, R2 -0.12182343006134033\n",
      "Eval loss 0.028476214036345482, R2 -0.09687304496765137\n",
      "epoch 432, loss 0.026904266327619553, R2 -0.11830437183380127\n",
      "Eval loss 0.02838202752172947, R2 -0.09324610233306885\n",
      "epoch 433, loss 0.026820292696356773, R2 -0.11481356620788574\n",
      "Eval loss 0.028288617730140686, R2 -0.08964836597442627\n",
      "epoch 434, loss 0.026737025007605553, R2 -0.11135232448577881\n",
      "Eval loss 0.02819599211215973, R2 -0.08607959747314453\n",
      "epoch 435, loss 0.026654450222849846, R2 -0.10792040824890137\n",
      "Eval loss 0.028104137629270554, R2 -0.08254170417785645\n",
      "epoch 436, loss 0.02657257206737995, R2 -0.1045156717300415\n",
      "Eval loss 0.028013046830892563, R2 -0.0790330171585083\n",
      "epoch 437, loss 0.026491377502679825, R2 -0.10114216804504395\n",
      "Eval loss 0.027922702953219414, R2 -0.0755530595779419\n",
      "epoch 438, loss 0.026410860940814018, R2 -0.09779572486877441\n",
      "Eval loss 0.02783311903476715, R2 -0.07210302352905273\n",
      "epoch 439, loss 0.026331016793847084, R2 -0.09447658061981201\n",
      "Eval loss 0.027744272723793983, R2 -0.06868040561676025\n",
      "epoch 440, loss 0.026251839473843575, R2 -0.09118545055389404\n",
      "Eval loss 0.027656160295009613, R2 -0.06528615951538086\n",
      "epoch 441, loss 0.026173317804932594, R2 -0.0879216194152832\n",
      "Eval loss 0.027568776160478592, R2 -0.06192028522491455\n",
      "epoch 442, loss 0.026095449924468994, R2 -0.08468496799468994\n",
      "Eval loss 0.027482112869620323, R2 -0.05858206748962402\n",
      "epoch 443, loss 0.026018226519227028, R2 -0.08147406578063965\n",
      "Eval loss 0.02739616483449936, R2 -0.05527079105377197\n",
      "epoch 444, loss 0.025941643863916397, R2 -0.0782918930053711\n",
      "Eval loss 0.027310924604535103, R2 -0.05198860168457031\n",
      "epoch 445, loss 0.025865698233246803, R2 -0.0751338005065918\n",
      "Eval loss 0.02722637727856636, R2 -0.04873168468475342\n",
      "epoch 446, loss 0.025790374726057053, R2 -0.07200396060943604\n",
      "Eval loss 0.02714253030717373, R2 -0.045502305030822754\n",
      "epoch 447, loss 0.025715677067637444, R2 -0.06889939308166504\n",
      "Eval loss 0.027059372514486313, R2 -0.042298197746276855\n",
      "epoch 448, loss 0.02564159408211708, R2 -0.06581997871398926\n",
      "Eval loss 0.02697688899934292, R2 -0.03912198543548584\n",
      "epoch 449, loss 0.025568120181560516, R2 -0.06276607513427734\n",
      "Eval loss 0.026895087212324142, R2 -0.035970091819763184\n",
      "epoch 450, loss 0.025495249778032303, R2 -0.05973672866821289\n",
      "Eval loss 0.026813950389623642, R2 -0.032845377922058105\n",
      "epoch 451, loss 0.02542297914624214, R2 -0.05673205852508545\n",
      "Eval loss 0.02673347294330597, R2 -0.02974522113800049\n",
      "epoch 452, loss 0.025351300835609436, R2 -0.05375373363494873\n",
      "Eval loss 0.026653651148080826, R2 -0.02667057514190674\n",
      "epoch 453, loss 0.025280211120843887, R2 -0.050798773765563965\n",
      "Eval loss 0.026574479416012764, R2 -0.02362060546875\n",
      "epoch 454, loss 0.0252096988260746, R2 -0.047867536544799805\n",
      "Eval loss 0.026495948433876038, R2 -0.02059626579284668\n",
      "epoch 455, loss 0.025139760226011276, R2 -0.04496049880981445\n",
      "Eval loss 0.026418056339025497, R2 -0.017595648765563965\n",
      "epoch 456, loss 0.025070397183299065, R2 -0.04207730293273926\n",
      "Eval loss 0.026340799406170845, R2 -0.014620184898376465\n",
      "epoch 457, loss 0.02500159665942192, R2 -0.039217472076416016\n",
      "Eval loss 0.026264164596796036, R2 -0.011668205261230469\n",
      "epoch 458, loss 0.024933353066444397, R2 -0.03638160228729248\n",
      "Eval loss 0.02618815004825592, R2 -0.008740544319152832\n",
      "epoch 459, loss 0.024865664541721344, R2 -0.03356766700744629\n",
      "Eval loss 0.026112746447324753, R2 -0.00583493709564209\n",
      "epoch 460, loss 0.024798529222607613, R2 -0.03077709674835205\n",
      "Eval loss 0.026037953794002533, R2 -0.0029534101486206055\n",
      "epoch 461, loss 0.02473193220794201, R2 -0.02800929546356201\n",
      "Eval loss 0.025963764637708664, R2 -9.715557098388672e-05\n",
      "epoch 462, loss 0.024665875360369682, R2 -0.02526223659515381\n",
      "Eval loss 0.0258901696652174, R2 0.0027379989624023438\n",
      "epoch 463, loss 0.024600351229310036, R2 -0.02253901958465576\n",
      "Eval loss 0.02581716887652874, R2 0.0055495500564575195\n",
      "epoch 464, loss 0.02453535422682762, R2 -0.01983797550201416\n",
      "Eval loss 0.02574474923312664, R2 0.00833970308303833\n",
      "epoch 465, loss 0.02447088435292244, R2 -0.017157912254333496\n",
      "Eval loss 0.02567291259765625, R2 0.011106729507446289\n",
      "epoch 466, loss 0.024406928569078445, R2 -0.014499902725219727\n",
      "Eval loss 0.025601651519536972, R2 0.013851642608642578\n",
      "epoch 467, loss 0.024343492463231087, R2 -0.011862039566040039\n",
      "Eval loss 0.02553095668554306, R2 0.01657390594482422\n",
      "epoch 468, loss 0.02428055927157402, R2 -0.009247183799743652\n",
      "Eval loss 0.025460828095674515, R2 0.019275426864624023\n",
      "epoch 469, loss 0.024218132719397545, R2 -0.006652355194091797\n",
      "Eval loss 0.02539125829935074, R2 0.021955490112304688\n",
      "epoch 470, loss 0.024156203493475914, R2 -0.004077315330505371\n",
      "Eval loss 0.025322241708636284, R2 0.024613380432128906\n",
      "epoch 471, loss 0.024094771593809128, R2 -0.0015246868133544922\n",
      "Eval loss 0.025253776460886, R2 0.027250587940216064\n",
      "epoch 472, loss 0.02403382770717144, R2 0.0010094642639160156\n",
      "Eval loss 0.025185853242874146, R2 0.029867470264434814\n",
      "epoch 473, loss 0.0239733699709177, R2 0.0035215020179748535\n",
      "Eval loss 0.025118468329310417, R2 0.03246277570724487\n",
      "epoch 474, loss 0.023913390934467316, R2 0.006014585494995117\n",
      "Eval loss 0.025051617994904518, R2 0.0350380539894104\n",
      "epoch 475, loss 0.02385389246046543, R2 0.008487999439239502\n",
      "Eval loss 0.0249853003770113, R2 0.03759258985519409\n",
      "epoch 476, loss 0.023794865235686302, R2 0.010941267013549805\n",
      "Eval loss 0.024919502437114716, R2 0.0401267409324646\n",
      "epoch 477, loss 0.02373630367219448, R2 0.013375699520111084\n",
      "Eval loss 0.024854226037859917, R2 0.04264116287231445\n",
      "epoch 478, loss 0.023678205907344818, R2 0.015790283679962158\n",
      "Eval loss 0.024789461866021156, R2 0.045136094093322754\n",
      "epoch 479, loss 0.023620568215847015, R2 0.018186092376708984\n",
      "Eval loss 0.024725208058953285, R2 0.04761159420013428\n",
      "epoch 480, loss 0.023563383147120476, R2 0.020563960075378418\n",
      "Eval loss 0.024661460891366005, R2 0.050067365169525146\n",
      "epoch 481, loss 0.02350665256381035, R2 0.022921383380889893\n",
      "Eval loss 0.02459821291267872, R2 0.05250328779220581\n",
      "epoch 482, loss 0.02345036342740059, R2 0.0252608060836792\n",
      "Eval loss 0.024535465985536575, R2 0.05491918325424194\n",
      "epoch 483, loss 0.023394517600536346, R2 0.027581453323364258\n",
      "Eval loss 0.02447320520877838, R2 0.057317912578582764\n",
      "epoch 484, loss 0.02333911322057247, R2 0.029884397983551025\n",
      "Eval loss 0.024411432445049286, R2 0.05969679355621338\n",
      "epoch 485, loss 0.02328414097428322, R2 0.03217029571533203\n",
      "Eval loss 0.02435014210641384, R2 0.062058210372924805\n",
      "epoch 486, loss 0.023229598999023438, R2 0.03443711996078491\n",
      "Eval loss 0.024289332330226898, R2 0.06440001726150513\n",
      "epoch 487, loss 0.02317548543214798, R2 0.03668642044067383\n",
      "Eval loss 0.02422899566590786, R2 0.06672406196594238\n",
      "epoch 488, loss 0.0231217909604311, R2 0.038918256759643555\n",
      "Eval loss 0.024169130250811577, R2 0.06903082132339478\n",
      "epoch 489, loss 0.023068521171808243, R2 0.04113340377807617\n",
      "Eval loss 0.024109728634357452, R2 0.07131868600845337\n",
      "epoch 490, loss 0.02301565743982792, R2 0.043329834938049316\n",
      "Eval loss 0.024050787091255188, R2 0.073589026927948\n",
      "epoch 491, loss 0.022963210940361023, R2 0.04550981521606445\n",
      "Eval loss 0.023992303758859634, R2 0.07584148645401001\n",
      "epoch 492, loss 0.02291116863489151, R2 0.04767298698425293\n",
      "Eval loss 0.023934276774525642, R2 0.07807713747024536\n",
      "epoch 493, loss 0.02285953052341938, R2 0.04981964826583862\n",
      "Eval loss 0.023876694962382317, R2 0.0802951455116272\n",
      "epoch 494, loss 0.022808294743299484, R2 0.05194908380508423\n",
      "Eval loss 0.023819558322429657, R2 0.0824965238571167\n",
      "epoch 495, loss 0.022757451981306076, R2 0.05406332015991211\n",
      "Eval loss 0.023762863129377365, R2 0.0846790075302124\n",
      "epoch 496, loss 0.022707000374794006, R2 0.05615973472595215\n",
      "Eval loss 0.023706603795289993, R2 0.08684659004211426\n",
      "epoch 497, loss 0.022656939923763275, R2 0.058240294456481934\n",
      "Eval loss 0.02365078218281269, R2 0.08899682760238647\n",
      "epoch 498, loss 0.022607263177633286, R2 0.060305118560791016\n",
      "Eval loss 0.02359538897871971, R2 0.09112995862960815\n",
      "epoch 499, loss 0.022557971999049187, R2 0.06235402822494507\n",
      "Eval loss 0.023540416732430458, R2 0.09324795007705688\n",
      "epoch 500, loss 0.02250906080007553, R2 0.06438738107681274\n",
      "Eval loss 0.02348586916923523, R2 0.09534907341003418\n",
      "epoch 501, loss 0.022460520267486572, R2 0.06640440225601196\n",
      "Eval loss 0.023431740701198578, R2 0.09743380546569824\n",
      "epoch 502, loss 0.02241235412657261, R2 0.06840676069259644\n",
      "Eval loss 0.023378025740385056, R2 0.09950309991836548\n",
      "epoch 503, loss 0.022364553064107895, R2 0.07039391994476318\n",
      "Eval loss 0.023324718698859215, R2 0.10155642032623291\n",
      "epoch 504, loss 0.02231711894273758, R2 0.07236498594284058\n",
      "Eval loss 0.023271819576621056, R2 0.10359346866607666\n",
      "epoch 505, loss 0.022270048037171364, R2 0.07432246208190918\n",
      "Eval loss 0.02321932464838028, R2 0.10561555624008179\n",
      "epoch 506, loss 0.02222333662211895, R2 0.0762631893157959\n",
      "Eval loss 0.023167230188846588, R2 0.10762298107147217\n",
      "epoch 507, loss 0.02217697910964489, R2 0.07819008827209473\n",
      "Eval loss 0.023115532472729683, R2 0.10961461067199707\n",
      "epoch 508, loss 0.022130971774458885, R2 0.08010274171829224\n",
      "Eval loss 0.023064224049448967, R2 0.11159062385559082\n",
      "epoch 509, loss 0.022085318341851234, R2 0.08200037479400635\n",
      "Eval loss 0.02301330305635929, R2 0.1135517954826355\n",
      "epoch 510, loss 0.022040005773305893, R2 0.08388376235961914\n",
      "Eval loss 0.022962773218750954, R2 0.11549818515777588\n",
      "epoch 511, loss 0.02199503779411316, R2 0.08575356006622314\n",
      "Eval loss 0.02291262522339821, R2 0.11743009090423584\n",
      "epoch 512, loss 0.021950412541627884, R2 0.08760756254196167\n",
      "Eval loss 0.02286285161972046, R2 0.11934703588485718\n",
      "epoch 513, loss 0.02190612256526947, R2 0.08944880962371826\n",
      "Eval loss 0.022813457995653152, R2 0.12124967575073242\n",
      "epoch 514, loss 0.02186216413974762, R2 0.09127575159072876\n",
      "Eval loss 0.022764435037970543, R2 0.1231377124786377\n",
      "epoch 515, loss 0.02181853912770748, R2 0.09308964014053345\n",
      "Eval loss 0.02271578274667263, R2 0.12501204013824463\n",
      "epoch 516, loss 0.02177524007856846, R2 0.09489023685455322\n",
      "Eval loss 0.02266749180853367, R2 0.12687236070632935\n",
      "epoch 517, loss 0.021732261404395103, R2 0.09667551517486572\n",
      "Eval loss 0.022619569674134254, R2 0.12871778011322021\n",
      "epoch 518, loss 0.02168961614370346, R2 0.09844815731048584\n",
      "Eval loss 0.022572003304958344, R2 0.13055026531219482\n",
      "epoch 519, loss 0.02164728380739689, R2 0.10020774602890015\n",
      "Eval loss 0.022524792701005936, R2 0.13236922025680542\n",
      "epoch 520, loss 0.021605266258120537, R2 0.10195362567901611\n",
      "Eval loss 0.02247793599963188, R2 0.13417387008666992\n",
      "epoch 521, loss 0.021563563495874405, R2 0.103687584400177\n",
      "Eval loss 0.02243143506348133, R2 0.1359642744064331\n",
      "epoch 522, loss 0.021522173658013344, R2 0.10540807247161865\n",
      "Eval loss 0.02238527312874794, R2 0.13774287700653076\n",
      "epoch 523, loss 0.021481091156601906, R2 0.10711628198623657\n",
      "Eval loss 0.022339457646012306, R2 0.13950759172439575\n",
      "epoch 524, loss 0.02144031599164009, R2 0.10881030559539795\n",
      "Eval loss 0.02229398675262928, R2 0.14125961065292358\n",
      "epoch 525, loss 0.021399838849902153, R2 0.110493004322052\n",
      "Eval loss 0.022248851135373116, R2 0.142997145652771\n",
      "epoch 526, loss 0.02135966718196869, R2 0.1121639609336853\n",
      "Eval loss 0.022204050794243813, R2 0.14472311735153198\n",
      "epoch 527, loss 0.021319789811968803, R2 0.11382037401199341\n",
      "Eval loss 0.022159583866596222, R2 0.1464361548423767\n",
      "epoch 528, loss 0.021280204877257347, R2 0.11546605825424194\n",
      "Eval loss 0.022115441039204597, R2 0.14813578128814697\n",
      "epoch 529, loss 0.021240917965769768, R2 0.11709880828857422\n",
      "Eval loss 0.022071627900004387, R2 0.1498236060142517\n",
      "epoch 530, loss 0.02120191603899002, R2 0.11871993541717529\n",
      "Eval loss 0.022028140723705292, R2 0.1514992117881775\n",
      "epoch 531, loss 0.021163204684853554, R2 0.12032902240753174\n",
      "Eval loss 0.021984977647662163, R2 0.15316182374954224\n",
      "epoch 532, loss 0.02112477645277977, R2 0.12192624807357788\n",
      "Eval loss 0.021942123770713806, R2 0.1548120379447937\n",
      "epoch 533, loss 0.02108662948012352, R2 0.12351357936859131\n",
      "Eval loss 0.021899592131376266, R2 0.1564508080482483\n",
      "epoch 534, loss 0.021048767492175102, R2 0.125085711479187\n",
      "Eval loss 0.0218573696911335, R2 0.15807712078094482\n",
      "epoch 535, loss 0.021011177450418472, R2 0.12664955854415894\n",
      "Eval loss 0.021815458312630653, R2 0.15969127416610718\n",
      "epoch 536, loss 0.020973864942789078, R2 0.12819933891296387\n",
      "Eval loss 0.02177385613322258, R2 0.16129428148269653\n",
      "epoch 537, loss 0.02093682438135147, R2 0.1297386884689331\n",
      "Eval loss 0.021732553839683533, R2 0.16288518905639648\n",
      "epoch 538, loss 0.020900052040815353, R2 0.1312674880027771\n",
      "Eval loss 0.02169155888259411, R2 0.16446423530578613\n",
      "epoch 539, loss 0.020863547921180725, R2 0.13278478384017944\n",
      "Eval loss 0.021650858223438263, R2 0.16603201627731323\n",
      "epoch 540, loss 0.020827312022447586, R2 0.1342906951904297\n",
      "Eval loss 0.02161046303808689, R2 0.1675877571105957\n",
      "epoch 541, loss 0.02079133875668049, R2 0.1357859969139099\n",
      "Eval loss 0.02157035656273365, R2 0.16913306713104248\n",
      "epoch 542, loss 0.020755628123879433, R2 0.13727033138275146\n",
      "Eval loss 0.02153054066002369, R2 0.17066580057144165\n",
      "epoch 543, loss 0.02072017267346382, R2 0.138744056224823\n",
      "Eval loss 0.02149101532995701, R2 0.17218828201293945\n",
      "epoch 544, loss 0.020684974268078804, R2 0.1402079463005066\n",
      "Eval loss 0.021451778709888458, R2 0.17369985580444336\n",
      "epoch 545, loss 0.02065003290772438, R2 0.14165979623794556\n",
      "Eval loss 0.021412821486592293, R2 0.1752004623413086\n",
      "epoch 546, loss 0.020615341141819954, R2 0.1431015133857727\n",
      "Eval loss 0.02137414924800396, R2 0.17669004201889038\n",
      "epoch 547, loss 0.020580900833010674, R2 0.14453333616256714\n",
      "Eval loss 0.02133575826883316, R2 0.17816907167434692\n",
      "epoch 548, loss 0.02054670639336109, R2 0.14595437049865723\n",
      "Eval loss 0.021297644823789597, R2 0.1796373724937439\n",
      "epoch 549, loss 0.020512759685516357, R2 0.14736539125442505\n",
      "Eval loss 0.02125980146229267, R2 0.18109506368637085\n",
      "epoch 550, loss 0.020479056984186172, R2 0.1487671136856079\n",
      "Eval loss 0.02122223936021328, R2 0.18254196643829346\n",
      "epoch 551, loss 0.02044559456408024, R2 0.15015721321105957\n",
      "Eval loss 0.02118493989109993, R2 0.18397796154022217\n",
      "epoch 552, loss 0.020412372425198555, R2 0.1515389084815979\n",
      "Eval loss 0.02114791050553322, R2 0.18540501594543457\n",
      "epoch 553, loss 0.020379386842250824, R2 0.15290945768356323\n",
      "Eval loss 0.021111147478222847, R2 0.1868213415145874\n",
      "epoch 554, loss 0.020346637815237045, R2 0.1542704701423645\n",
      "Eval loss 0.021074648946523666, R2 0.18822622299194336\n",
      "epoch 555, loss 0.02031412161886692, R2 0.15562206506729126\n",
      "Eval loss 0.02103840932250023, R2 0.1896228790283203\n",
      "epoch 556, loss 0.02028183825314045, R2 0.15696394443511963\n",
      "Eval loss 0.021002434194087982, R2 0.19100838899612427\n",
      "epoch 557, loss 0.020249782130122185, R2 0.1582963466644287\n",
      "Eval loss 0.020966708660125732, R2 0.1923844814300537\n",
      "epoch 558, loss 0.020217953249812126, R2 0.15961939096450806\n",
      "Eval loss 0.020931242033839226, R2 0.19375061988830566\n",
      "epoch 559, loss 0.020186349749565125, R2 0.16093295812606812\n",
      "Eval loss 0.020896028727293015, R2 0.19510674476623535\n",
      "epoch 560, loss 0.02015497349202633, R2 0.16223740577697754\n",
      "Eval loss 0.02086106315255165, R2 0.19645452499389648\n",
      "epoch 561, loss 0.020123817026615143, R2 0.16353249549865723\n",
      "Eval loss 0.020826345309615135, R2 0.19779109954833984\n",
      "epoch 562, loss 0.020092878490686417, R2 0.16481822729110718\n",
      "Eval loss 0.020791877061128616, R2 0.19911879301071167\n",
      "epoch 563, loss 0.0200621597468853, R2 0.1660950779914856\n",
      "Eval loss 0.02075764909386635, R2 0.2004370093345642\n",
      "epoch 564, loss 0.020031658932566643, R2 0.167363703250885\n",
      "Eval loss 0.02072366699576378, R2 0.2017461657524109\n",
      "epoch 565, loss 0.02000136859714985, R2 0.16862189769744873\n",
      "Eval loss 0.02068992331624031, R2 0.20304566621780396\n",
      "epoch 566, loss 0.019971292465925217, R2 0.16987180709838867\n",
      "Eval loss 0.020656419917941093, R2 0.20433694124221802\n",
      "epoch 567, loss 0.019941428676247597, R2 0.17111444473266602\n",
      "Eval loss 0.02062314935028553, R2 0.2056182622909546\n",
      "epoch 568, loss 0.01991177350282669, R2 0.1723460555076599\n",
      "Eval loss 0.02059011720120907, R2 0.20689088106155396\n",
      "epoch 569, loss 0.0198823269456625, R2 0.17357003688812256\n",
      "Eval loss 0.020557312294840813, R2 0.20815354585647583\n",
      "epoch 570, loss 0.019853081554174423, R2 0.17478591203689575\n",
      "Eval loss 0.02052474021911621, R2 0.20940864086151123\n",
      "epoch 571, loss 0.019824042916297913, R2 0.1759929060935974\n",
      "Eval loss 0.020492397248744965, R2 0.21065419912338257\n",
      "epoch 572, loss 0.01979520544409752, R2 0.1771920919418335\n",
      "Eval loss 0.020460281521081924, R2 0.211891770362854\n",
      "epoch 573, loss 0.01976657286286354, R2 0.17838144302368164\n",
      "Eval loss 0.020428387448191643, R2 0.21312028169631958\n",
      "epoch 574, loss 0.019738133996725082, R2 0.1795635223388672\n",
      "Eval loss 0.02039671689271927, R2 0.21434062719345093\n",
      "epoch 575, loss 0.019709894433617592, R2 0.1807381510734558\n",
      "Eval loss 0.020365267992019653, R2 0.21555107831954956\n",
      "epoch 576, loss 0.01968185044825077, R2 0.18190354108810425\n",
      "Eval loss 0.020334037020802498, R2 0.21675407886505127\n",
      "epoch 577, loss 0.01965399831533432, R2 0.18306076526641846\n",
      "Eval loss 0.020303023979067802, R2 0.21794915199279785\n",
      "epoch 578, loss 0.01962633803486824, R2 0.18421149253845215\n",
      "Eval loss 0.020272230729460716, R2 0.2191346287727356\n",
      "epoch 579, loss 0.01959886960685253, R2 0.18535250425338745\n",
      "Eval loss 0.020241647958755493, R2 0.22031259536743164\n",
      "epoch 580, loss 0.019571593031287193, R2 0.18648630380630493\n",
      "Eval loss 0.020211275666952133, R2 0.2214832305908203\n",
      "epoch 581, loss 0.01954449899494648, R2 0.18761324882507324\n",
      "Eval loss 0.020181115716695786, R2 0.22264444828033447\n",
      "epoch 582, loss 0.01951759308576584, R2 0.1887316107749939\n",
      "Eval loss 0.020151162520051003, R2 0.2237984538078308\n",
      "epoch 583, loss 0.01949087530374527, R2 0.1898411512374878\n",
      "Eval loss 0.020121416077017784, R2 0.22494405508041382\n",
      "epoch 584, loss 0.019464334473013878, R2 0.19094431400299072\n",
      "Eval loss 0.02009187452495098, R2 0.22608166933059692\n",
      "epoch 585, loss 0.01943797618150711, R2 0.19204014539718628\n",
      "Eval loss 0.020062537863850594, R2 0.22721219062805176\n",
      "epoch 586, loss 0.019411800429224968, R2 0.19312822818756104\n",
      "Eval loss 0.020033404231071472, R2 0.2283344268798828\n",
      "epoch 587, loss 0.01938580349087715, R2 0.19420933723449707\n",
      "Eval loss 0.02000446431338787, R2 0.2294490933418274\n",
      "epoch 588, loss 0.019359981641173363, R2 0.19528186321258545\n",
      "Eval loss 0.019975731149315834, R2 0.2305554747581482\n",
      "epoch 589, loss 0.019334333017468452, R2 0.19634795188903809\n",
      "Eval loss 0.019947189837694168, R2 0.2316552996635437\n",
      "epoch 590, loss 0.01930886320769787, R2 0.19740664958953857\n",
      "Eval loss 0.01991884596645832, R2 0.23274773359298706\n",
      "epoch 591, loss 0.019283562898635864, R2 0.1984584927558899\n",
      "Eval loss 0.019890692085027695, R2 0.23383104801177979\n",
      "epoch 592, loss 0.01925843581557274, R2 0.19950270652770996\n",
      "Eval loss 0.01986273191869259, R2 0.23490852117538452\n",
      "epoch 593, loss 0.019233478233218193, R2 0.20054036378860474\n",
      "Eval loss 0.019834963604807854, R2 0.23597770929336548\n",
      "epoch 594, loss 0.01920868270099163, R2 0.201571524143219\n",
      "Eval loss 0.01980738341808319, R2 0.23704028129577637\n",
      "epoch 595, loss 0.019184062257409096, R2 0.2025941014289856\n",
      "Eval loss 0.01977999322116375, R2 0.23809504508972168\n",
      "epoch 596, loss 0.019159607589244843, R2 0.20361065864562988\n",
      "Eval loss 0.019752783700823784, R2 0.23914384841918945\n",
      "epoch 597, loss 0.019135311245918274, R2 0.20462125539779663\n",
      "Eval loss 0.01972576230764389, R2 0.24018466472625732\n",
      "epoch 598, loss 0.019111182540655136, R2 0.20562416315078735\n",
      "Eval loss 0.019698921591043472, R2 0.24121832847595215\n",
      "epoch 599, loss 0.019087214022874832, R2 0.2066197395324707\n",
      "Eval loss 0.01967226341366768, R2 0.2422446608543396\n",
      "epoch 600, loss 0.019063405692577362, R2 0.20760983228683472\n",
      "Eval loss 0.01964578591287136, R2 0.24326521158218384\n",
      "epoch 601, loss 0.019039759412407875, R2 0.2085922360420227\n",
      "Eval loss 0.01961948536336422, R2 0.24427765607833862\n",
      "epoch 602, loss 0.019016264006495476, R2 0.20956909656524658\n",
      "Eval loss 0.019593363627791405, R2 0.2452843189239502\n",
      "epoch 603, loss 0.01899293065071106, R2 0.21053946018218994\n",
      "Eval loss 0.019567416980862617, R2 0.24628394842147827\n",
      "epoch 604, loss 0.01896975189447403, R2 0.21150219440460205\n",
      "Eval loss 0.01954164355993271, R2 0.24727648496627808\n",
      "epoch 605, loss 0.018946725875139236, R2 0.2124595046043396\n",
      "Eval loss 0.01951604336500168, R2 0.24826282262802124\n",
      "epoch 606, loss 0.01892385259270668, R2 0.21340978145599365\n",
      "Eval loss 0.019490614533424377, R2 0.24924206733703613\n",
      "epoch 607, loss 0.018901130184531212, R2 0.2143552303314209\n",
      "Eval loss 0.019465357065200806, R2 0.25021499395370483\n",
      "epoch 608, loss 0.01887856051325798, R2 0.21529263257980347\n",
      "Eval loss 0.019440265372395515, R2 0.2511812448501587\n",
      "epoch 609, loss 0.018856137990951538, R2 0.21622538566589355\n",
      "Eval loss 0.019415345042943954, R2 0.252142071723938\n",
      "epoch 610, loss 0.018833864480257034, R2 0.2171512246131897\n",
      "Eval loss 0.019390586763620377, R2 0.25309503078460693\n",
      "epoch 611, loss 0.01881173811852932, R2 0.218070387840271\n",
      "Eval loss 0.01936599425971508, R2 0.25404298305511475\n",
      "epoch 612, loss 0.018789755180478096, R2 0.2189839482307434\n",
      "Eval loss 0.019341565668582916, R2 0.2549828290939331\n",
      "epoch 613, loss 0.018767915666103363, R2 0.2198924422264099\n",
      "Eval loss 0.019317299127578735, R2 0.255918025970459\n",
      "epoch 614, loss 0.01874622143805027, R2 0.22079318761825562\n",
      "Eval loss 0.01929319277405739, R2 0.2568466067314148\n",
      "epoch 615, loss 0.018724670633673668, R2 0.22168946266174316\n",
      "Eval loss 0.019269244745373726, R2 0.25776898860931396\n",
      "epoch 616, loss 0.01870325766503811, R2 0.2225794792175293\n",
      "Eval loss 0.019245458766818047, R2 0.25868523120880127\n",
      "epoch 617, loss 0.01868198625743389, R2 0.22346371412277222\n",
      "Eval loss 0.019221825525164604, R2 0.2595958113670349\n",
      "epoch 618, loss 0.018660852685570717, R2 0.22434264421463013\n",
      "Eval loss 0.019198346883058548, R2 0.26050013303756714\n",
      "epoch 619, loss 0.018639855086803436, R2 0.22521495819091797\n",
      "Eval loss 0.019175026565790176, R2 0.2613980174064636\n",
      "epoch 620, loss 0.0186189953237772, R2 0.22608298063278198\n",
      "Eval loss 0.019151857122778893, R2 0.2622908353805542\n",
      "epoch 621, loss 0.018598271533846855, R2 0.22694337368011475\n",
      "Eval loss 0.019128844141960144, R2 0.2631770968437195\n",
      "epoch 622, loss 0.018577679991722107, R2 0.22779929637908936\n",
      "Eval loss 0.019105974584817886, R2 0.26405781507492065\n",
      "epoch 623, loss 0.018557224422693253, R2 0.2286495566368103\n",
      "Eval loss 0.019083259627223015, R2 0.26493293046951294\n",
      "epoch 624, loss 0.018536899238824844, R2 0.2294941544532776\n",
      "Eval loss 0.019060691818594933, R2 0.2658017873764038\n",
      "epoch 625, loss 0.01851670630276203, R2 0.23033350706100464\n",
      "Eval loss 0.01903827115893364, R2 0.26666587591171265\n",
      "epoch 626, loss 0.018496643751859665, R2 0.23116719722747803\n",
      "Eval loss 0.019015995785593987, R2 0.2675243616104126\n",
      "epoch 627, loss 0.018476707860827446, R2 0.2319963574409485\n",
      "Eval loss 0.018993865698575974, R2 0.26837629079818726\n",
      "epoch 628, loss 0.018456902354955673, R2 0.2328193187713623\n",
      "Eval loss 0.01897188276052475, R2 0.2692224383354187\n",
      "epoch 629, loss 0.0184372216463089, R2 0.23363763093948364\n",
      "Eval loss 0.01895003952085972, R2 0.27006447315216064\n",
      "epoch 630, loss 0.01841767132282257, R2 0.23444998264312744\n",
      "Eval loss 0.01892833597958088, R2 0.2709004878997803\n",
      "epoch 631, loss 0.018398240208625793, R2 0.23525762557983398\n",
      "Eval loss 0.01890677399933338, R2 0.2717309594154358\n",
      "epoch 632, loss 0.018378937616944313, R2 0.2360602617263794\n",
      "Eval loss 0.018885351717472076, R2 0.2725563645362854\n",
      "epoch 633, loss 0.018359752371907234, R2 0.23685717582702637\n",
      "Eval loss 0.018864067271351814, R2 0.2733762264251709\n",
      "epoch 634, loss 0.018340693786740303, R2 0.2376498579978943\n",
      "Eval loss 0.018842922523617744, R2 0.2741907238960266\n",
      "epoch 635, loss 0.018321754410862923, R2 0.23843687772750854\n",
      "Eval loss 0.01882190816104412, R2 0.2749994993209839\n",
      "epoch 636, loss 0.018302934244275093, R2 0.23921936750411987\n",
      "Eval loss 0.01880103349685669, R2 0.2758033871650696\n",
      "epoch 637, loss 0.018284237012267113, R2 0.2399963140487671\n",
      "Eval loss 0.018780291080474854, R2 0.276602566242218\n",
      "epoch 638, loss 0.018265657126903534, R2 0.24076884984970093\n",
      "Eval loss 0.018759682774543762, R2 0.2773969769477844\n",
      "epoch 639, loss 0.018247192725539207, R2 0.24153614044189453\n",
      "Eval loss 0.018739202991127968, R2 0.27818524837493896\n",
      "epoch 640, loss 0.018228843808174133, R2 0.24229902029037476\n",
      "Eval loss 0.018718857318162918, R2 0.27897000312805176\n",
      "epoch 641, loss 0.01821061410009861, R2 0.24305659532546997\n",
      "Eval loss 0.018698638305068016, R2 0.2797481417655945\n",
      "epoch 642, loss 0.01819249615073204, R2 0.24380964040756226\n",
      "Eval loss 0.01867854967713356, R2 0.2805219888687134\n",
      "epoch 643, loss 0.018174489960074425, R2 0.24455809593200684\n",
      "Eval loss 0.0186585895717144, R2 0.2812909483909607\n",
      "epoch 644, loss 0.01815660111606121, R2 0.24530261754989624\n",
      "Eval loss 0.01863875985145569, R2 0.282054603099823\n",
      "epoch 645, loss 0.0181388221681118, R2 0.24604111909866333\n",
      "Eval loss 0.018619049340486526, R2 0.2828136682510376\n",
      "epoch 646, loss 0.018121154978871346, R2 0.24677526950836182\n",
      "Eval loss 0.018599465489387512, R2 0.28356772661209106\n",
      "epoch 647, loss 0.018103597685694695, R2 0.24750453233718872\n",
      "Eval loss 0.018580008298158646, R2 0.28431767225265503\n",
      "epoch 648, loss 0.0180861484259367, R2 0.24823004007339478\n",
      "Eval loss 0.01856066845357418, R2 0.28506237268447876\n",
      "epoch 649, loss 0.018068809062242508, R2 0.2489507794380188\n",
      "Eval loss 0.018541453406214714, R2 0.2858027219772339\n",
      "epoch 650, loss 0.018051577731966972, R2 0.24966681003570557\n",
      "Eval loss 0.0185223575681448, R2 0.2865384817123413\n",
      "epoch 651, loss 0.018034450709819794, R2 0.25037896633148193\n",
      "Eval loss 0.018503384664654732, R2 0.28726911544799805\n",
      "epoch 652, loss 0.01801743172109127, R2 0.25108617544174194\n",
      "Eval loss 0.018484532833099365, R2 0.28799527883529663\n",
      "epoch 653, loss 0.018000517040491104, R2 0.2517896890640259\n",
      "Eval loss 0.0184657983481884, R2 0.28871697187423706\n",
      "epoch 654, loss 0.017983706668019295, R2 0.2524878978729248\n",
      "Eval loss 0.01844717562198639, R2 0.28943419456481934\n",
      "epoch 655, loss 0.01796700246632099, R2 0.25318318605422974\n",
      "Eval loss 0.018428673967719078, R2 0.2901473045349121\n",
      "epoch 656, loss 0.017950396984815598, R2 0.25387293100357056\n",
      "Eval loss 0.01841028779745102, R2 0.290855348110199\n",
      "epoch 657, loss 0.01793389767408371, R2 0.254558801651001\n",
      "Eval loss 0.018392013385891914, R2 0.29155904054641724\n",
      "epoch 658, loss 0.017917495220899582, R2 0.2552403211593628\n",
      "Eval loss 0.018373852595686913, R2 0.2922587990760803\n",
      "epoch 659, loss 0.017901195213198662, R2 0.25591784715652466\n",
      "Eval loss 0.018355805426836014, R2 0.2929536700248718\n",
      "epoch 660, loss 0.01788499392569065, R2 0.2565914988517761\n",
      "Eval loss 0.018337873741984367, R2 0.29364466667175293\n",
      "epoch 661, loss 0.0178688932210207, R2 0.2572605609893799\n",
      "Eval loss 0.018320050090551376, R2 0.2943311929702759\n",
      "epoch 662, loss 0.017852889373898506, R2 0.25792592763900757\n",
      "Eval loss 0.01830233633518219, R2 0.295012891292572\n",
      "epoch 663, loss 0.017836982384324074, R2 0.25858694314956665\n",
      "Eval loss 0.018284732475876808, R2 0.2956913709640503\n",
      "epoch 664, loss 0.01782117411494255, R2 0.25924378633499146\n",
      "Eval loss 0.01826723851263523, R2 0.2963652014732361\n",
      "epoch 665, loss 0.01780546084046364, R2 0.2598974108695984\n",
      "Eval loss 0.01824984699487686, R2 0.2970349192619324\n",
      "epoch 666, loss 0.017789840698242188, R2 0.2605471611022949\n",
      "Eval loss 0.018232567235827446, R2 0.2977009415626526\n",
      "epoch 667, loss 0.017774319276213646, R2 0.26119160652160645\n",
      "Eval loss 0.018215393647551537, R2 0.2983618378639221\n",
      "epoch 668, loss 0.017758887261152267, R2 0.2618333101272583\n",
      "Eval loss 0.018198322504758835, R2 0.29901957511901855\n",
      "epoch 669, loss 0.0177435465157032, R2 0.26247137784957886\n",
      "Eval loss 0.01818135753273964, R2 0.29967302083969116\n",
      "epoch 670, loss 0.017728304490447044, R2 0.26310425996780396\n",
      "Eval loss 0.01816449873149395, R2 0.30032265186309814\n",
      "epoch 671, loss 0.017713148146867752, R2 0.26373398303985596\n",
      "Eval loss 0.01814773865044117, R2 0.30096858739852905\n",
      "epoch 672, loss 0.01769808679819107, R2 0.264360249042511\n",
      "Eval loss 0.018131084740161896, R2 0.3016093373298645\n",
      "epoch 673, loss 0.017683114856481552, R2 0.26498281955718994\n",
      "Eval loss 0.01811452955007553, R2 0.30224788188934326\n",
      "epoch 674, loss 0.017668232321739197, R2 0.2656019330024719\n",
      "Eval loss 0.018098074942827225, R2 0.30288100242614746\n",
      "epoch 675, loss 0.017653439193964005, R2 0.26621633768081665\n",
      "Eval loss 0.018081722781062126, R2 0.3035113215446472\n",
      "epoch 676, loss 0.017638731747865677, R2 0.26682817935943604\n",
      "Eval loss 0.01806546375155449, R2 0.30413734912872314\n",
      "epoch 677, loss 0.017624111846089363, R2 0.26743561029434204\n",
      "Eval loss 0.01804930903017521, R2 0.3047596216201782\n",
      "epoch 678, loss 0.017609579488635063, R2 0.26803988218307495\n",
      "Eval loss 0.01803325116634369, R2 0.30537837743759155\n",
      "epoch 679, loss 0.017595134675502777, R2 0.26864027976989746\n",
      "Eval loss 0.01801728457212448, R2 0.3059927821159363\n",
      "epoch 680, loss 0.017580771818757057, R2 0.26923656463623047\n",
      "Eval loss 0.018001418560743332, R2 0.30660390853881836\n",
      "epoch 681, loss 0.0175664983689785, R2 0.2698301672935486\n",
      "Eval loss 0.017985649406909943, R2 0.3072117567062378\n",
      "epoch 682, loss 0.01755230687558651, R2 0.2704206705093384\n",
      "Eval loss 0.017969971522688866, R2 0.3078152537345886\n",
      "epoch 683, loss 0.017538201063871384, R2 0.27100682258605957\n",
      "Eval loss 0.01795439049601555, R2 0.30841583013534546\n",
      "epoch 684, loss 0.017524175345897675, R2 0.27158862352371216\n",
      "Eval loss 0.017938898876309395, R2 0.3090125322341919\n",
      "epoch 685, loss 0.01751023158431053, R2 0.27216869592666626\n",
      "Eval loss 0.017923502251505852, R2 0.30960559844970703\n",
      "epoch 686, loss 0.017496371641755104, R2 0.2727447748184204\n",
      "Eval loss 0.01790819689631462, R2 0.310194730758667\n",
      "epoch 687, loss 0.017482591792941093, R2 0.273317813873291\n",
      "Eval loss 0.017892984673380852, R2 0.3107813000679016\n",
      "epoch 688, loss 0.0174688920378685, R2 0.273887038230896\n",
      "Eval loss 0.017877859994769096, R2 0.3113632798194885\n",
      "epoch 689, loss 0.017455274239182472, R2 0.2744530439376831\n",
      "Eval loss 0.017862824723124504, R2 0.31194281578063965\n",
      "epoch 690, loss 0.017441732808947563, R2 0.2750158905982971\n",
      "Eval loss 0.017847880721092224, R2 0.31251823902130127\n",
      "epoch 691, loss 0.01742827333509922, R2 0.27557533979415894\n",
      "Eval loss 0.017833024263381958, R2 0.31309086084365845\n",
      "epoch 692, loss 0.017414890229701996, R2 0.27613186836242676\n",
      "Eval loss 0.017818257212638855, R2 0.3136597275733948\n",
      "epoch 693, loss 0.01740158535540104, R2 0.2766849398612976\n",
      "Eval loss 0.017803575843572617, R2 0.3142252564430237\n",
      "epoch 694, loss 0.01738835498690605, R2 0.27723485231399536\n",
      "Eval loss 0.017788978293538094, R2 0.31478726863861084\n",
      "epoch 695, loss 0.017375202849507332, R2 0.27778130769729614\n",
      "Eval loss 0.017774472013115883, R2 0.3153461217880249\n",
      "epoch 696, loss 0.01736212521791458, R2 0.27832484245300293\n",
      "Eval loss 0.01776004396378994, R2 0.31590163707733154\n",
      "epoch 697, loss 0.0173491220921278, R2 0.27886533737182617\n",
      "Eval loss 0.01774570904672146, R2 0.3164539933204651\n",
      "epoch 698, loss 0.017336200922727585, R2 0.2794024348258972\n",
      "Eval loss 0.017731452360749245, R2 0.31700289249420166\n",
      "epoch 699, loss 0.017323344945907593, R2 0.2799367904663086\n",
      "Eval loss 0.017717279493808746, R2 0.31754887104034424\n",
      "epoch 700, loss 0.01731056720018387, R2 0.2804679274559021\n",
      "Eval loss 0.017703190445899963, R2 0.3180919885635376\n",
      "epoch 701, loss 0.017297860234975815, R2 0.28099673986434937\n",
      "Eval loss 0.017689181491732597, R2 0.31863152980804443\n",
      "epoch 702, loss 0.01728522591292858, R2 0.2815212607383728\n",
      "Eval loss 0.017675256356596947, R2 0.3191683292388916\n",
      "epoch 703, loss 0.017272666096687317, R2 0.2820435166358948\n",
      "Eval loss 0.017661409452557564, R2 0.31970155239105225\n",
      "epoch 704, loss 0.017260177060961723, R2 0.28256309032440186\n",
      "Eval loss 0.017647644504904747, R2 0.3202311396598816\n",
      "epoch 705, loss 0.017247755080461502, R2 0.28307968378067017\n",
      "Eval loss 0.017633961513638496, R2 0.32075822353363037\n",
      "epoch 706, loss 0.0172354094684124, R2 0.2835926413536072\n",
      "Eval loss 0.017620353028178215, R2 0.3212823271751404\n",
      "epoch 707, loss 0.01722313091158867, R2 0.28410255908966064\n",
      "Eval loss 0.0176068264991045, R2 0.3218032121658325\n",
      "epoch 708, loss 0.01721091940999031, R2 0.2846105694770813\n",
      "Eval loss 0.017593376338481903, R2 0.32232147455215454\n",
      "epoch 709, loss 0.017198780551552773, R2 0.28511446714401245\n",
      "Eval loss 0.017580004408955574, R2 0.3228369951248169\n",
      "epoch 710, loss 0.017186706885695457, R2 0.28561633825302124\n",
      "Eval loss 0.017566708847880363, R2 0.3233490586280823\n",
      "epoch 711, loss 0.017174704000353813, R2 0.286116361618042\n",
      "Eval loss 0.01755348965525627, R2 0.32385826110839844\n",
      "epoch 712, loss 0.017162766307592392, R2 0.28661149740219116\n",
      "Eval loss 0.01754034496843815, R2 0.3243640065193176\n",
      "epoch 713, loss 0.017150895670056343, R2 0.2871059775352478\n",
      "Eval loss 0.017527274787425995, R2 0.3248676657676697\n",
      "epoch 714, loss 0.017139092087745667, R2 0.2875955104827881\n",
      "Eval loss 0.01751428097486496, R2 0.32536840438842773\n",
      "epoch 715, loss 0.017127351835370064, R2 0.2880834937095642\n",
      "Eval loss 0.017501359805464745, R2 0.32586604356765747\n",
      "epoch 716, loss 0.017115680500864983, R2 0.2885686159133911\n",
      "Eval loss 0.017488515004515648, R2 0.326360821723938\n",
      "epoch 717, loss 0.017104074358940125, R2 0.289050817489624\n",
      "Eval loss 0.017475740984082222, R2 0.3268525004386902\n",
      "epoch 718, loss 0.01709252968430519, R2 0.2895318269729614\n",
      "Eval loss 0.017463039606809616, R2 0.32734233140945435\n",
      "epoch 719, loss 0.01708105206489563, R2 0.29000794887542725\n",
      "Eval loss 0.01745040901005268, R2 0.32782822847366333\n",
      "epoch 720, loss 0.017069635912775993, R2 0.29048341512680054\n",
      "Eval loss 0.017437851056456566, R2 0.3283119797706604\n",
      "epoch 721, loss 0.01705828309059143, R2 0.2909550666809082\n",
      "Eval loss 0.01742536388337612, R2 0.32879334688186646\n",
      "epoch 722, loss 0.017046991735696793, R2 0.2914237380027771\n",
      "Eval loss 0.017412947490811348, R2 0.32927167415618896\n",
      "epoch 723, loss 0.017035765573382378, R2 0.29189062118530273\n",
      "Eval loss 0.017400600016117096, R2 0.32974720001220703\n",
      "epoch 724, loss 0.01702459715306759, R2 0.2923545837402344\n",
      "Eval loss 0.017388319596648216, R2 0.33022069931030273\n",
      "epoch 725, loss 0.017013493925333023, R2 0.2928161025047302\n",
      "Eval loss 0.017376109957695007, R2 0.3306901454925537\n",
      "epoch 726, loss 0.017002448439598083, R2 0.2932758927345276\n",
      "Eval loss 0.01736396923661232, R2 0.3311578035354614\n",
      "epoch 727, loss 0.01699146442115307, R2 0.2937317490577698\n",
      "Eval loss 0.017351895570755005, R2 0.331622838973999\n",
      "epoch 728, loss 0.01698054000735283, R2 0.29418569803237915\n",
      "Eval loss 0.017339888960123062, R2 0.33208513259887695\n",
      "epoch 729, loss 0.016969675198197365, R2 0.29463815689086914\n",
      "Eval loss 0.01732795126736164, R2 0.33254557847976685\n",
      "epoch 730, loss 0.016958871856331825, R2 0.29508674144744873\n",
      "Eval loss 0.017316076904535294, R2 0.3330029845237732\n",
      "epoch 731, loss 0.016948124393820763, R2 0.2955341935157776\n",
      "Eval loss 0.01730426773428917, R2 0.3334580063819885\n",
      "epoch 732, loss 0.016937434673309326, R2 0.2959775924682617\n",
      "Eval loss 0.017292527481913567, R2 0.33390986919403076\n",
      "epoch 733, loss 0.016926802694797516, R2 0.2964203953742981\n",
      "Eval loss 0.01728084869682789, R2 0.3343595266342163\n",
      "epoch 734, loss 0.016916226595640182, R2 0.29685908555984497\n",
      "Eval loss 0.017269236966967583, R2 0.3348071575164795\n",
      "epoch 735, loss 0.016905711963772774, R2 0.2972961664199829\n",
      "Eval loss 0.0172576867043972, R2 0.3352521061897278\n",
      "epoch 736, loss 0.016895251348614693, R2 0.2977309226989746\n",
      "Eval loss 0.017246201634407043, R2 0.33569443225860596\n",
      "epoch 737, loss 0.016884848475456238, R2 0.29816389083862305\n",
      "Eval loss 0.01723477989435196, R2 0.33613407611846924\n",
      "epoch 738, loss 0.01687449961900711, R2 0.29859375953674316\n",
      "Eval loss 0.0172234196215868, R2 0.33657240867614746\n",
      "epoch 739, loss 0.01686420850455761, R2 0.2990211248397827\n",
      "Eval loss 0.017212122678756714, R2 0.33700722455978394\n",
      "epoch 740, loss 0.016853969544172287, R2 0.29944711923599243\n",
      "Eval loss 0.017200887203216553, R2 0.3374401330947876\n",
      "epoch 741, loss 0.01684378646314144, R2 0.29987019300460815\n",
      "Eval loss 0.017189713194966316, R2 0.33787041902542114\n",
      "epoch 742, loss 0.016833655536174774, R2 0.3002908229827881\n",
      "Eval loss 0.017178596928715706, R2 0.3382985591888428\n",
      "epoch 743, loss 0.016823580488562584, R2 0.3007100820541382\n",
      "Eval loss 0.01716754399240017, R2 0.3387244939804077\n",
      "epoch 744, loss 0.01681355945765972, R2 0.3011266589164734\n",
      "Eval loss 0.017156550660729408, R2 0.3391479253768921\n",
      "epoch 745, loss 0.016803590580821037, R2 0.30154097080230713\n",
      "Eval loss 0.017145615071058273, R2 0.3395687937736511\n",
      "epoch 746, loss 0.01679367385804653, R2 0.30195313692092896\n",
      "Eval loss 0.017134740948677063, R2 0.3399878144264221\n",
      "epoch 747, loss 0.016783809289336205, R2 0.30236297845840454\n",
      "Eval loss 0.01712392456829548, R2 0.34040409326553345\n",
      "epoch 748, loss 0.016773998737335205, R2 0.3027716875076294\n",
      "Eval loss 0.01711316406726837, R2 0.34081852436065674\n",
      "epoch 749, loss 0.016764238476753235, R2 0.3031768798828125\n",
      "Eval loss 0.017102466896176338, R2 0.34123098850250244\n",
      "epoch 750, loss 0.016754526644945145, R2 0.30358123779296875\n",
      "Eval loss 0.017091825604438782, R2 0.34164053201675415\n",
      "epoch 751, loss 0.016744868829846382, R2 0.3039817810058594\n",
      "Eval loss 0.017081238329410553, R2 0.3420485258102417\n",
      "epoch 752, loss 0.01673526130616665, R2 0.3043811321258545\n",
      "Eval loss 0.0170707069337368, R2 0.3424539566040039\n",
      "epoch 753, loss 0.016725702211260796, R2 0.30477869510650635\n",
      "Eval loss 0.017060233280062675, R2 0.3428577780723572\n",
      "epoch 754, loss 0.01671619340777397, R2 0.30517393350601196\n",
      "Eval loss 0.017049815505743027, R2 0.34325963258743286\n",
      "epoch 755, loss 0.016706734895706177, R2 0.30556708574295044\n",
      "Eval loss 0.017039455473423004, R2 0.34365832805633545\n",
      "epoch 756, loss 0.016697324812412262, R2 0.30595844984054565\n",
      "Eval loss 0.01702914759516716, R2 0.3440547585487366\n",
      "epoch 757, loss 0.016687963157892227, R2 0.306347131729126\n",
      "Eval loss 0.017018893733620644, R2 0.34445011615753174\n",
      "epoch 758, loss 0.016678649932146072, R2 0.30673426389694214\n",
      "Eval loss 0.017008697614073753, R2 0.34484291076660156\n",
      "epoch 759, loss 0.01666938327252865, R2 0.3071191906929016\n",
      "Eval loss 0.01699855364859104, R2 0.3452332019805908\n",
      "epoch 760, loss 0.016660168766975403, R2 0.3075028657913208\n",
      "Eval loss 0.01698845997452736, R2 0.3456227779388428\n",
      "epoch 761, loss 0.01665099710226059, R2 0.30788367986679077\n",
      "Eval loss 0.016978424042463303, R2 0.3460085988044739\n",
      "epoch 762, loss 0.016641873866319656, R2 0.3082628846168518\n",
      "Eval loss 0.016968436539173126, R2 0.346393883228302\n",
      "epoch 763, loss 0.016632799059152603, R2 0.3086400628089905\n",
      "Eval loss 0.016958506777882576, R2 0.34677577018737793\n",
      "epoch 764, loss 0.01662376895546913, R2 0.3090156316757202\n",
      "Eval loss 0.016948625445365906, R2 0.34715622663497925\n",
      "epoch 765, loss 0.01661478355526924, R2 0.3093891143798828\n",
      "Eval loss 0.016938796266913414, R2 0.3475353717803955\n",
      "epoch 766, loss 0.01660584844648838, R2 0.30976057052612305\n",
      "Eval loss 0.01692901737987995, R2 0.34791189432144165\n",
      "epoch 767, loss 0.016596956178545952, R2 0.3101308345794678\n",
      "Eval loss 0.016919290646910667, R2 0.34828656911849976\n",
      "epoch 768, loss 0.016588106751441956, R2 0.3104979991912842\n",
      "Eval loss 0.016909614205360413, R2 0.3486594557762146\n",
      "epoch 769, loss 0.01657930389046669, R2 0.3108636736869812\n",
      "Eval loss 0.016899986192584038, R2 0.34903013706207275\n",
      "epoch 770, loss 0.016570543870329857, R2 0.3112284541130066\n",
      "Eval loss 0.01689041033387184, R2 0.34939903020858765\n",
      "epoch 771, loss 0.016561828553676605, R2 0.3115907311439514\n",
      "Eval loss 0.016880882903933525, R2 0.34976619482040405\n",
      "epoch 772, loss 0.016553159803152084, R2 0.31195104122161865\n",
      "Eval loss 0.01687140390276909, R2 0.35013145208358765\n",
      "epoch 773, loss 0.016544530168175697, R2 0.31230902671813965\n",
      "Eval loss 0.01686197705566883, R2 0.35049402713775635\n",
      "epoch 774, loss 0.01653594896197319, R2 0.3126659393310547\n",
      "Eval loss 0.016852594912052155, R2 0.3508554697036743\n",
      "epoch 775, loss 0.016527406871318817, R2 0.31302040815353394\n",
      "Eval loss 0.016843261197209358, R2 0.3512149453163147\n",
      "epoch 776, loss 0.016518907621502876, R2 0.3133741021156311\n",
      "Eval loss 0.016833975911140442, R2 0.3515729308128357\n",
      "epoch 777, loss 0.016510453075170517, R2 0.3137255907058716\n",
      "Eval loss 0.016824740916490555, R2 0.3519287109375\n",
      "epoch 778, loss 0.01650203950703144, R2 0.31407594680786133\n",
      "Eval loss 0.0168155487626791, R2 0.3522828221321106\n",
      "epoch 779, loss 0.016493666917085648, R2 0.31442368030548096\n",
      "Eval loss 0.016806406900286674, R2 0.35263514518737793\n",
      "epoch 780, loss 0.016485337167978287, R2 0.3147704005241394\n",
      "Eval loss 0.01679730787873268, R2 0.3529850244522095\n",
      "epoch 781, loss 0.01647704467177391, R2 0.31511420011520386\n",
      "Eval loss 0.016788257285952568, R2 0.3533341884613037\n",
      "epoch 782, loss 0.016468796879053116, R2 0.31545698642730713\n",
      "Eval loss 0.016779251396656036, R2 0.35368072986602783\n",
      "epoch 783, loss 0.016460590064525604, R2 0.3157981038093567\n",
      "Eval loss 0.016770292073488235, R2 0.35402584075927734\n",
      "epoch 784, loss 0.016452422365546227, R2 0.3161378502845764\n",
      "Eval loss 0.016761377453804016, R2 0.3543694019317627\n",
      "epoch 785, loss 0.016444293782114983, R2 0.3164757490158081\n",
      "Eval loss 0.01675250567495823, R2 0.35471075773239136\n",
      "epoch 786, loss 0.016436204314231873, R2 0.3168123960494995\n",
      "Eval loss 0.016743682324886322, R2 0.35505062341690063\n",
      "epoch 787, loss 0.016428157687187195, R2 0.3171464204788208\n",
      "Eval loss 0.016734901815652847, R2 0.35538941621780396\n",
      "epoch 788, loss 0.0164201483130455, R2 0.31747889518737793\n",
      "Eval loss 0.016726166009902954, R2 0.3557257056236267\n",
      "epoch 789, loss 0.016412178054451942, R2 0.31781065464019775\n",
      "Eval loss 0.016717469319701195, R2 0.35606086254119873\n",
      "epoch 790, loss 0.016404246911406517, R2 0.31814074516296387\n",
      "Eval loss 0.016708819195628166, R2 0.3563939332962036\n",
      "epoch 791, loss 0.016396353021264076, R2 0.31846803426742554\n",
      "Eval loss 0.01670021377503872, R2 0.3567253351211548\n",
      "epoch 792, loss 0.01638850010931492, R2 0.3187946081161499\n",
      "Eval loss 0.016691649332642555, R2 0.3570554852485657\n",
      "epoch 793, loss 0.016380682587623596, R2 0.3191196322441101\n",
      "Eval loss 0.016683127731084824, R2 0.3573830723762512\n",
      "epoch 794, loss 0.016372904181480408, R2 0.3194429278373718\n",
      "Eval loss 0.016674648970365524, R2 0.35770976543426514\n",
      "epoch 795, loss 0.016365161165595055, R2 0.3197649121284485\n",
      "Eval loss 0.01666620746254921, R2 0.3580358028411865\n",
      "epoch 796, loss 0.016357455402612686, R2 0.32008522748947144\n",
      "Eval loss 0.016657814383506775, R2 0.3583587408065796\n",
      "epoch 797, loss 0.01634978875517845, R2 0.32040417194366455\n",
      "Eval loss 0.016649458557367325, R2 0.3586800694465637\n",
      "epoch 798, loss 0.016342157498002052, R2 0.320720911026001\n",
      "Eval loss 0.016641143709421158, R2 0.3590010404586792\n",
      "epoch 799, loss 0.016334563493728638, R2 0.32103651762008667\n",
      "Eval loss 0.016632867977023125, R2 0.359319269657135\n",
      "epoch 800, loss 0.01632700115442276, R2 0.3213515281677246\n",
      "Eval loss 0.016624636948108673, R2 0.35963666439056396\n",
      "epoch 801, loss 0.016319481655955315, R2 0.32166391611099243\n",
      "Eval loss 0.016616443172097206, R2 0.3599517345428467\n",
      "epoch 802, loss 0.016311993822455406, R2 0.3219746947288513\n",
      "Eval loss 0.01660829223692417, R2 0.3602663278579712\n",
      "epoch 803, loss 0.016304543241858482, R2 0.32228440046310425\n",
      "Eval loss 0.01660018041729927, R2 0.36057859659194946\n",
      "epoch 804, loss 0.016297126188874245, R2 0.32259225845336914\n",
      "Eval loss 0.016592102125287056, R2 0.36088991165161133\n",
      "epoch 805, loss 0.016289742663502693, R2 0.3228997588157654\n",
      "Eval loss 0.016584068536758423, R2 0.36119914054870605\n",
      "epoch 806, loss 0.016282398253679276, R2 0.3232048749923706\n",
      "Eval loss 0.016576072201132774, R2 0.3615071773529053\n",
      "epoch 807, loss 0.016275085508823395, R2 0.32350820302963257\n",
      "Eval loss 0.01656811498105526, R2 0.3618133068084717\n",
      "epoch 808, loss 0.01626780815422535, R2 0.32381153106689453\n",
      "Eval loss 0.01656019501388073, R2 0.36211830377578735\n",
      "epoch 809, loss 0.01626056618988514, R2 0.32411229610443115\n",
      "Eval loss 0.016552316024899483, R2 0.36242222785949707\n",
      "epoch 810, loss 0.016253354027867317, R2 0.32441216707229614\n",
      "Eval loss 0.01654447242617607, R2 0.36272478103637695\n",
      "epoch 811, loss 0.01624617539346218, R2 0.324710488319397\n",
      "Eval loss 0.016536667943000793, R2 0.36302512884140015\n",
      "epoch 812, loss 0.01623903401196003, R2 0.3250073194503784\n",
      "Eval loss 0.01652889885008335, R2 0.36332499980926514\n",
      "epoch 813, loss 0.016231926158070564, R2 0.32530301809310913\n",
      "Eval loss 0.016521167010068893, R2 0.3636220097541809\n",
      "epoch 814, loss 0.016224848106503487, R2 0.32559722661972046\n",
      "Eval loss 0.01651347242295742, R2 0.363918662071228\n",
      "epoch 815, loss 0.016217801719903946, R2 0.3258898854255676\n",
      "Eval loss 0.016505815088748932, R2 0.36421316862106323\n",
      "epoch 816, loss 0.01621078886091709, R2 0.3261815905570984\n",
      "Eval loss 0.01649819128215313, R2 0.3645070195198059\n",
      "epoch 817, loss 0.016203811392188072, R2 0.32647138833999634\n",
      "Eval loss 0.01649060845375061, R2 0.3647991418838501\n",
      "epoch 818, loss 0.01619686186313629, R2 0.32676053047180176\n",
      "Eval loss 0.016483057290315628, R2 0.36509019136428833\n",
      "epoch 819, loss 0.016189947724342346, R2 0.3270477056503296\n",
      "Eval loss 0.01647554151713848, R2 0.3653795123100281\n",
      "epoch 820, loss 0.01618306152522564, R2 0.32733410596847534\n",
      "Eval loss 0.01646806299686432, R2 0.36566776037216187\n",
      "epoch 821, loss 0.016176210716366768, R2 0.32761865854263306\n",
      "Eval loss 0.016460619866847992, R2 0.36595410108566284\n",
      "epoch 822, loss 0.016169385984539986, R2 0.32790255546569824\n",
      "Eval loss 0.01645321026444435, R2 0.3662394881248474\n",
      "epoch 823, loss 0.016162598505616188, R2 0.32818537950515747\n",
      "Eval loss 0.016445834189653397, R2 0.3665233850479126\n",
      "epoch 824, loss 0.01615583710372448, R2 0.32846570014953613\n",
      "Eval loss 0.016438495367765427, R2 0.36680668592453003\n",
      "epoch 825, loss 0.016149107366800308, R2 0.3287452459335327\n",
      "Eval loss 0.016431190073490143, R2 0.36708784103393555\n",
      "epoch 826, loss 0.016142409294843674, R2 0.32902371883392334\n",
      "Eval loss 0.016423918306827545, R2 0.36736780405044556\n",
      "epoch 827, loss 0.016135739162564278, R2 0.3293011784553528\n",
      "Eval loss 0.016416680067777634, R2 0.36764681339263916\n",
      "epoch 828, loss 0.01612910069525242, R2 0.32957684993743896\n",
      "Eval loss 0.01640947349369526, R2 0.3679240345954895\n",
      "epoch 829, loss 0.016122492030262947, R2 0.32985132932662964\n",
      "Eval loss 0.01640230230987072, R2 0.368200421333313\n",
      "epoch 830, loss 0.016115911304950714, R2 0.330125093460083\n",
      "Eval loss 0.016395164653658867, R2 0.36847519874572754\n",
      "epoch 831, loss 0.016109362244606018, R2 0.3303973078727722\n",
      "Eval loss 0.0163880567997694, R2 0.3687494993209839\n",
      "epoch 832, loss 0.01610284112393856, R2 0.330668568611145\n",
      "Eval loss 0.01638098433613777, R2 0.36902135610580444\n",
      "epoch 833, loss 0.01609634980559349, R2 0.3309383988380432\n",
      "Eval loss 0.01637394353747368, R2 0.3692929744720459\n",
      "epoch 834, loss 0.01608988642692566, R2 0.3312068581581116\n",
      "Eval loss 0.016366934403777122, R2 0.36956334114074707\n",
      "epoch 835, loss 0.016083452850580215, R2 0.33147406578063965\n",
      "Eval loss 0.016359956935048103, R2 0.36983227729797363\n",
      "epoch 836, loss 0.01607704721391201, R2 0.33174049854278564\n",
      "Eval loss 0.01635301299393177, R2 0.37009936571121216\n",
      "epoch 837, loss 0.016070667654275894, R2 0.33200567960739136\n",
      "Eval loss 0.016346098855137825, R2 0.37036556005477905\n",
      "epoch 838, loss 0.016064319759607315, R2 0.332269549369812\n",
      "Eval loss 0.016339214518666267, R2 0.37063068151474\n",
      "epoch 839, loss 0.016057996079325676, R2 0.3325323462486267\n",
      "Eval loss 0.016332363709807396, R2 0.3708946108818054\n",
      "epoch 840, loss 0.016051704064011574, R2 0.3327938914299011\n",
      "Eval loss 0.016325542703270912, R2 0.37115728855133057\n",
      "epoch 841, loss 0.01604543626308441, R2 0.33305442333221436\n",
      "Eval loss 0.016318753361701965, R2 0.37141913175582886\n",
      "epoch 842, loss 0.016039196401834488, R2 0.33331406116485596\n",
      "Eval loss 0.016311993822455406, R2 0.37167876958847046\n",
      "epoch 843, loss 0.016032986342906952, R2 0.3335722088813782\n",
      "Eval loss 0.016305265948176384, R2 0.37193799018859863\n",
      "epoch 844, loss 0.016026800498366356, R2 0.3338291049003601\n",
      "Eval loss 0.01629856787621975, R2 0.3721959590911865\n",
      "epoch 845, loss 0.016020642593503, R2 0.33408498764038086\n",
      "Eval loss 0.016291901469230652, R2 0.3724527955055237\n",
      "epoch 846, loss 0.01601451076567173, R2 0.3343396782875061\n",
      "Eval loss 0.016285261139273643, R2 0.37270891666412354\n",
      "epoch 847, loss 0.01600840501487255, R2 0.3345943093299866\n",
      "Eval loss 0.016278650611639023, R2 0.37296319007873535\n",
      "epoch 848, loss 0.01600232720375061, R2 0.3348463177680969\n",
      "Eval loss 0.01627206988632679, R2 0.37321698665618896\n",
      "epoch 849, loss 0.01599627546966076, R2 0.3350980877876282\n",
      "Eval loss 0.016265518963336945, R2 0.37346863746643066\n",
      "epoch 850, loss 0.0159902460873127, R2 0.33534830808639526\n",
      "Eval loss 0.016258995980024338, R2 0.3737204670906067\n",
      "epoch 851, loss 0.015984246507287025, R2 0.33559781312942505\n",
      "Eval loss 0.01625250279903412, R2 0.37397074699401855\n",
      "epoch 852, loss 0.015978271141648293, R2 0.3358462452888489\n",
      "Eval loss 0.01624603569507599, R2 0.37421977519989014\n",
      "epoch 853, loss 0.01597232185304165, R2 0.336093544960022\n",
      "Eval loss 0.016239600256085396, R2 0.374467670917511\n",
      "epoch 854, loss 0.015966396778821945, R2 0.33634018898010254\n",
      "Eval loss 0.01623319275677204, R2 0.374714732170105\n",
      "epoch 855, loss 0.01596049964427948, R2 0.3365851640701294\n",
      "Eval loss 0.016226811334490776, R2 0.374960720539093\n",
      "epoch 856, loss 0.015954624861478806, R2 0.33682936429977417\n",
      "Eval loss 0.0162204597145319, R2 0.375204861164093\n",
      "epoch 857, loss 0.015948772430419922, R2 0.33707302808761597\n",
      "Eval loss 0.01621413230895996, R2 0.37544888257980347\n",
      "epoch 858, loss 0.015942949801683426, R2 0.3373146653175354\n",
      "Eval loss 0.01620783656835556, R2 0.3756912350654602\n",
      "epoch 859, loss 0.01593714952468872, R2 0.33755552768707275\n",
      "Eval loss 0.0162015650421381, R2 0.37593281269073486\n",
      "epoch 860, loss 0.015931373462080956, R2 0.33779555559158325\n",
      "Eval loss 0.016195323318243027, R2 0.37617337703704834\n",
      "epoch 861, loss 0.01592562161386013, R2 0.338035523891449\n",
      "Eval loss 0.016189105808734894, R2 0.37641286849975586\n",
      "epoch 862, loss 0.015919893980026245, R2 0.33827298879623413\n",
      "Eval loss 0.016182916238904, R2 0.37665146589279175\n",
      "epoch 863, loss 0.01591418869793415, R2 0.33851009607315063\n",
      "Eval loss 0.016176754608750343, R2 0.3768885135650635\n",
      "epoch 864, loss 0.015908509492874146, R2 0.33874595165252686\n",
      "Eval loss 0.016170615330338478, R2 0.377124547958374\n",
      "epoch 865, loss 0.01590285263955593, R2 0.33898109197616577\n",
      "Eval loss 0.01616450771689415, R2 0.3773602247238159\n",
      "epoch 866, loss 0.015897218137979507, R2 0.3392152786254883\n",
      "Eval loss 0.016158422455191612, R2 0.3775944113731384\n",
      "epoch 867, loss 0.015891611576080322, R2 0.33944833278656006\n",
      "Eval loss 0.016152363270521164, R2 0.3778281807899475\n",
      "epoch 868, loss 0.01588602364063263, R2 0.3396812081336975\n",
      "Eval loss 0.016146332025527954, R2 0.37805992364883423\n",
      "epoch 869, loss 0.015880459919571877, R2 0.3399120569229126\n",
      "Eval loss 0.016140323132276535, R2 0.3782917261123657\n",
      "epoch 870, loss 0.015874918550252914, R2 0.34014201164245605\n",
      "Eval loss 0.016134342178702354, R2 0.37852197885513306\n",
      "epoch 871, loss 0.015869401395320892, R2 0.3403717279434204\n",
      "Eval loss 0.016128387302160263, R2 0.3787515163421631\n",
      "epoch 872, loss 0.01586390659213066, R2 0.3405998945236206\n",
      "Eval loss 0.016122452914714813, R2 0.3789801001548767\n",
      "epoch 873, loss 0.01585843227803707, R2 0.3408272862434387\n",
      "Eval loss 0.01611654832959175, R2 0.3792075514793396\n",
      "epoch 874, loss 0.01585298217833042, R2 0.3410542607307434\n",
      "Eval loss 0.01611066795885563, R2 0.37943387031555176\n",
      "epoch 875, loss 0.015847552567720413, R2 0.34127968549728394\n",
      "Eval loss 0.016104809939861298, R2 0.3796592950820923\n",
      "epoch 876, loss 0.015842147171497345, R2 0.3415045738220215\n",
      "Eval loss 0.016098976135253906, R2 0.37988442182540894\n",
      "epoch 877, loss 0.01583676040172577, R2 0.34172892570495605\n",
      "Eval loss 0.016093168407678604, R2 0.3801077604293823\n",
      "epoch 878, loss 0.015831397846341133, R2 0.34195077419281006\n",
      "Eval loss 0.016087383031845093, R2 0.3803311586380005\n",
      "epoch 879, loss 0.015826057642698288, R2 0.34217339754104614\n",
      "Eval loss 0.01608162373304367, R2 0.3805524706840515\n",
      "epoch 880, loss 0.015820737928152084, R2 0.3423948884010315\n",
      "Eval loss 0.01607588678598404, R2 0.3807739019393921\n",
      "epoch 881, loss 0.015815436840057373, R2 0.3426152467727661\n",
      "Eval loss 0.016070175915956497, R2 0.38099396228790283\n",
      "epoch 882, loss 0.01581016182899475, R2 0.3428347110748291\n",
      "Eval loss 0.016064485535025597, R2 0.38121259212493896\n",
      "epoch 883, loss 0.015804903581738472, R2 0.3430524468421936\n",
      "Eval loss 0.016058819368481636, R2 0.38143086433410645\n",
      "epoch 884, loss 0.015799669548869133, R2 0.3432702422142029\n",
      "Eval loss 0.016053177416324615, R2 0.3816488981246948\n",
      "epoch 885, loss 0.015794454142451286, R2 0.3434865474700928\n",
      "Eval loss 0.016047557815909386, R2 0.38186460733413696\n",
      "epoch 886, loss 0.01578926108777523, R2 0.34370267391204834\n",
      "Eval loss 0.016041962429881096, R2 0.38208073377609253\n",
      "epoch 887, loss 0.015784084796905518, R2 0.3439183831214905\n",
      "Eval loss 0.0160363856703043, R2 0.3822949528694153\n",
      "epoch 888, loss 0.015778932720422745, R2 0.3441317081451416\n",
      "Eval loss 0.01603083685040474, R2 0.3825092911720276\n",
      "epoch 889, loss 0.015773799270391464, R2 0.3443452715873718\n",
      "Eval loss 0.016025306656956673, R2 0.38272207975387573\n",
      "epoch 890, loss 0.015768688172101974, R2 0.3445580005645752\n",
      "Eval loss 0.016019800677895546, R2 0.3829341530799866\n",
      "epoch 891, loss 0.015763595700263977, R2 0.34477031230926514\n",
      "Eval loss 0.01601431705057621, R2 0.3831455707550049\n",
      "epoch 892, loss 0.01575852371752262, R2 0.3449798822402954\n",
      "Eval loss 0.016008855774998665, R2 0.3833557367324829\n",
      "epoch 893, loss 0.015753470361232758, R2 0.3451905846595764\n",
      "Eval loss 0.01600341498851776, R2 0.3835657238960266\n",
      "epoch 894, loss 0.015748435631394386, R2 0.34539979696273804\n",
      "Eval loss 0.01599799655377865, R2 0.3837738633155823\n",
      "epoch 895, loss 0.015743421390652657, R2 0.3456079959869385\n",
      "Eval loss 0.015992598608136177, R2 0.38398194313049316\n",
      "epoch 896, loss 0.01573842577636242, R2 0.3458154797554016\n",
      "Eval loss 0.015987224876880646, R2 0.38418859243392944\n",
      "epoch 897, loss 0.01573345437645912, R2 0.3460225462913513\n",
      "Eval loss 0.015981871634721756, R2 0.3843953013420105\n",
      "epoch 898, loss 0.015728497877717018, R2 0.34622830152511597\n",
      "Eval loss 0.01597653701901436, R2 0.38460075855255127\n",
      "epoch 899, loss 0.015723560005426407, R2 0.3464335799217224\n",
      "Eval loss 0.0159712266176939, R2 0.3848053812980652\n",
      "epoch 900, loss 0.015718642622232437, R2 0.34663820266723633\n",
      "Eval loss 0.015965936705470085, R2 0.3850085735321045\n",
      "epoch 901, loss 0.01571374386548996, R2 0.34684163331985474\n",
      "Eval loss 0.01596066728234291, R2 0.38521212339401245\n",
      "epoch 902, loss 0.015708861872553825, R2 0.34704452753067017\n",
      "Eval loss 0.015955420210957527, R2 0.3854137063026428\n",
      "epoch 903, loss 0.015704000368714333, R2 0.34724658727645874\n",
      "Eval loss 0.015950189903378487, R2 0.3856157064437866\n",
      "epoch 904, loss 0.015699157491326332, R2 0.3474476933479309\n",
      "Eval loss 0.015944981947541237, R2 0.3858162760734558\n",
      "epoch 905, loss 0.015694333240389824, R2 0.347648024559021\n",
      "Eval loss 0.01593979448080063, R2 0.3860158920288086\n",
      "epoch 906, loss 0.01568952575325966, R2 0.3478490114212036\n",
      "Eval loss 0.015934627503156662, R2 0.3862147331237793\n",
      "epoch 907, loss 0.015684736892580986, R2 0.34804725646972656\n",
      "Eval loss 0.015929479151964188, R2 0.38641321659088135\n",
      "epoch 908, loss 0.015679966658353806, R2 0.3482457995414734\n",
      "Eval loss 0.015924355015158653, R2 0.3866109848022461\n",
      "epoch 909, loss 0.015675216913223267, R2 0.348443865776062\n",
      "Eval loss 0.01591924950480461, R2 0.3868076801300049\n",
      "epoch 910, loss 0.01567048206925392, R2 0.3486398458480835\n",
      "Eval loss 0.015914158895611763, R2 0.38700318336486816\n",
      "epoch 911, loss 0.01566576398909092, R2 0.34883612394332886\n",
      "Eval loss 0.015909092500805855, R2 0.38719892501831055\n",
      "epoch 912, loss 0.01566106453537941, R2 0.3490315079689026\n",
      "Eval loss 0.01590404286980629, R2 0.3873928189277649\n",
      "epoch 913, loss 0.015656381845474243, R2 0.3492256999015808\n",
      "Eval loss 0.015899015590548515, R2 0.3875862956047058\n",
      "epoch 914, loss 0.015651719644665718, R2 0.3494197130203247\n",
      "Eval loss 0.015894003212451935, R2 0.3877795338630676\n",
      "epoch 915, loss 0.015647072345018387, R2 0.34961289167404175\n",
      "Eval loss 0.015889013186097145, R2 0.3879719376564026\n",
      "epoch 916, loss 0.0156424418091774, R2 0.3498053550720215\n",
      "Eval loss 0.015884041786193848, R2 0.3881634473800659\n",
      "epoch 917, loss 0.015637829899787903, R2 0.34999704360961914\n",
      "Eval loss 0.015879089012742043, R2 0.38835424184799194\n",
      "epoch 918, loss 0.0156332328915596, R2 0.35018813610076904\n",
      "Eval loss 0.01587415672838688, R2 0.38854384422302246\n",
      "epoch 919, loss 0.01562865450978279, R2 0.35037922859191895\n",
      "Eval loss 0.01586924120783806, R2 0.38873374462127686\n",
      "epoch 920, loss 0.01562409196048975, R2 0.3505680561065674\n",
      "Eval loss 0.01586434431374073, R2 0.3889227509498596\n",
      "epoch 921, loss 0.015619546175003052, R2 0.35075700283050537\n",
      "Eval loss 0.015859466046094894, R2 0.38911008834838867\n",
      "epoch 922, loss 0.01561501994729042, R2 0.3509451746940613\n",
      "Eval loss 0.01585460640490055, R2 0.3892969489097595\n",
      "epoch 923, loss 0.01561050582677126, R2 0.35113340616226196\n",
      "Eval loss 0.01584976352751255, R2 0.3894834518432617\n",
      "epoch 924, loss 0.01560601033270359, R2 0.35131943225860596\n",
      "Eval loss 0.01584494113922119, R2 0.3896697759628296\n",
      "epoch 925, loss 0.015601529739797115, R2 0.3515058755874634\n",
      "Eval loss 0.015840135514736176, R2 0.38985490798950195\n",
      "epoch 926, loss 0.015597066842019558, R2 0.35169219970703125\n",
      "Eval loss 0.015835346654057503, R2 0.39003896713256836\n",
      "epoch 927, loss 0.015592620708048344, R2 0.3518766164779663\n",
      "Eval loss 0.01583057828247547, R2 0.39022302627563477\n",
      "epoch 928, loss 0.015588190406560898, R2 0.35206055641174316\n",
      "Eval loss 0.015825822949409485, R2 0.3904055953025818\n",
      "epoch 929, loss 0.015583775006234646, R2 0.35224390029907227\n",
      "Eval loss 0.01582108996808529, R2 0.39058828353881836\n",
      "epoch 930, loss 0.015579374507069588, R2 0.35242682695388794\n",
      "Eval loss 0.015816371887922287, R2 0.39077019691467285\n",
      "epoch 931, loss 0.015574988909065723, R2 0.3526090979576111\n",
      "Eval loss 0.015811672434210777, R2 0.3909512162208557\n",
      "epoch 932, loss 0.015570621937513351, R2 0.3527906537055969\n",
      "Eval loss 0.01580698788166046, R2 0.39113128185272217\n",
      "epoch 933, loss 0.015566268004477024, R2 0.352971613407135\n",
      "Eval loss 0.015802323818206787, R2 0.39131081104278564\n",
      "epoch 934, loss 0.015561931766569614, R2 0.3531518578529358\n",
      "Eval loss 0.015797672793269157, R2 0.39148998260498047\n",
      "epoch 935, loss 0.015557609498500824, R2 0.35333168506622314\n",
      "Eval loss 0.01579304225742817, R2 0.3916686177253723\n",
      "epoch 936, loss 0.015553303062915802, R2 0.3535104990005493\n",
      "Eval loss 0.015788430348038673, R2 0.3918459415435791\n",
      "epoch 937, loss 0.015549011528491974, R2 0.35368889570236206\n",
      "Eval loss 0.015783829614520073, R2 0.39202332496643066\n",
      "epoch 938, loss 0.015544733963906765, R2 0.35386669635772705\n",
      "Eval loss 0.015779247507452965, R2 0.392200231552124\n",
      "epoch 939, loss 0.015540474094450474, R2 0.35404372215270996\n",
      "Eval loss 0.0157746821641922, R2 0.392375648021698\n",
      "epoch 940, loss 0.015536226332187653, R2 0.3542209267616272\n",
      "Eval loss 0.015770135447382927, R2 0.3925510048866272\n",
      "epoch 941, loss 0.0155319944024086, R2 0.3543969988822937\n",
      "Eval loss 0.0157656017690897, R2 0.3927256464958191\n",
      "epoch 942, loss 0.01552777923643589, R2 0.3545715808868408\n",
      "Eval loss 0.015761086717247963, R2 0.3928990364074707\n",
      "epoch 943, loss 0.01552357617765665, R2 0.35474610328674316\n",
      "Eval loss 0.01575658656656742, R2 0.39307326078414917\n",
      "epoch 944, loss 0.01551938895136118, R2 0.3549203872680664\n",
      "Eval loss 0.015752103179693222, R2 0.3932453989982605\n",
      "epoch 945, loss 0.015515215694904327, R2 0.35509443283081055\n",
      "Eval loss 0.015747636556625366, R2 0.3934176564216614\n",
      "epoch 946, loss 0.01551105733960867, R2 0.35526710748672485\n",
      "Eval loss 0.015743182972073555, R2 0.3935895562171936\n",
      "epoch 947, loss 0.015506911091506481, R2 0.3554392457008362\n",
      "Eval loss 0.015738746151328087, R2 0.3937602639198303\n",
      "epoch 948, loss 0.015502780675888062, R2 0.3556104898452759\n",
      "Eval loss 0.015734326094388962, R2 0.3939305543899536\n",
      "epoch 949, loss 0.015498665161430836, R2 0.35578155517578125\n",
      "Eval loss 0.01572991907596588, R2 0.39410024881362915\n",
      "epoch 950, loss 0.015494560822844505, R2 0.3559523820877075\n",
      "Eval loss 0.015725528821349144, R2 0.3942689895629883\n",
      "epoch 951, loss 0.015490475110709667, R2 0.3561221957206726\n",
      "Eval loss 0.0157211571931839, R2 0.39443743228912354\n",
      "epoch 952, loss 0.015486400574445724, R2 0.35629117488861084\n",
      "Eval loss 0.01571679674088955, R2 0.3946055769920349\n",
      "epoch 953, loss 0.015482339076697826, R2 0.3564603924751282\n",
      "Eval loss 0.015712453052401543, R2 0.39477264881134033\n",
      "epoch 954, loss 0.015478293411433697, R2 0.3566283583641052\n",
      "Eval loss 0.01570812612771988, R2 0.3949398994445801\n",
      "epoch 955, loss 0.015474258922040462, R2 0.3567960262298584\n",
      "Eval loss 0.01570381037890911, R2 0.39510577917099\n",
      "epoch 956, loss 0.015470241196453571, R2 0.3569636344909668\n",
      "Eval loss 0.015699513256549835, R2 0.3952711224555969\n",
      "epoch 957, loss 0.015466234646737576, R2 0.3571295738220215\n",
      "Eval loss 0.015695229172706604, R2 0.39543598890304565\n",
      "epoch 958, loss 0.015462242066860199, R2 0.3572957515716553\n",
      "Eval loss 0.015690958127379417, R2 0.3956007957458496\n",
      "epoch 959, loss 0.015458262525498867, R2 0.35746097564697266\n",
      "Eval loss 0.015686703845858574, R2 0.3957642912864685\n",
      "epoch 960, loss 0.01545429602265358, R2 0.35762643814086914\n",
      "Eval loss 0.015682466328144073, R2 0.3959276080131531\n",
      "epoch 961, loss 0.015450343489646912, R2 0.3577907085418701\n",
      "Eval loss 0.01567823812365532, R2 0.3960907459259033\n",
      "epoch 962, loss 0.015446405857801437, R2 0.3579537868499756\n",
      "Eval loss 0.015674026682972908, R2 0.39625298976898193\n",
      "epoch 963, loss 0.015442476607859135, R2 0.35811710357666016\n",
      "Eval loss 0.01566983200609684, R2 0.39641427993774414\n",
      "epoch 964, loss 0.0154385631904006, R2 0.3582797646522522\n",
      "Eval loss 0.015665648505091667, R2 0.3965764045715332\n",
      "epoch 965, loss 0.01543466281145811, R2 0.35844188928604126\n",
      "Eval loss 0.015661481767892838, R2 0.3967358469963074\n",
      "epoch 966, loss 0.015430774539709091, R2 0.3586031198501587\n",
      "Eval loss 0.015657326206564903, R2 0.3968963027000427\n",
      "epoch 967, loss 0.01542690023779869, R2 0.3587645888328552\n",
      "Eval loss 0.015653187409043312, R2 0.39705556631088257\n",
      "epoch 968, loss 0.01542303804308176, R2 0.3589250445365906\n",
      "Eval loss 0.015649061650037766, R2 0.3972146511077881\n",
      "epoch 969, loss 0.0154191879555583, R2 0.3590851426124573\n",
      "Eval loss 0.015644945204257965, R2 0.3973734974861145\n",
      "epoch 970, loss 0.01541534997522831, R2 0.3592448830604553\n",
      "Eval loss 0.015640847384929657, R2 0.39753085374832153\n",
      "epoch 971, loss 0.015411525964736938, R2 0.3594036102294922\n",
      "Eval loss 0.015636764466762543, R2 0.3976881504058838\n",
      "epoch 972, loss 0.015407713130116463, R2 0.3595626950263977\n",
      "Eval loss 0.015632692724466324, R2 0.3978453278541565\n",
      "epoch 973, loss 0.015403912402689457, R2 0.35972005128860474\n",
      "Eval loss 0.015628632158041, R2 0.39800119400024414\n",
      "epoch 974, loss 0.01540012564510107, R2 0.35987770557403564\n",
      "Eval loss 0.015624589286744595, R2 0.3981574773788452\n",
      "epoch 975, loss 0.015396349132061005, R2 0.3600344657897949\n",
      "Eval loss 0.015620558522641659, R2 0.39831221103668213\n",
      "epoch 976, loss 0.015392584726214409, R2 0.3601909279823303\n",
      "Eval loss 0.015616538934409618, R2 0.39846718311309814\n",
      "epoch 977, loss 0.015388835221529007, R2 0.36034679412841797\n",
      "Eval loss 0.015612537041306496, R2 0.398621141910553\n",
      "epoch 978, loss 0.0153850968927145, R2 0.36050236225128174\n",
      "Eval loss 0.015608544461429119, R2 0.3987753391265869\n",
      "epoch 979, loss 0.01538136787712574, R2 0.36065739393234253\n",
      "Eval loss 0.015604565851390362, R2 0.3989284038543701\n",
      "epoch 980, loss 0.015377650037407875, R2 0.36081165075302124\n",
      "Eval loss 0.015600601211190224, R2 0.3990814685821533\n",
      "epoch 981, loss 0.015373948030173779, R2 0.360965371131897\n",
      "Eval loss 0.015596646815538406, R2 0.39923322200775146\n",
      "epoch 982, loss 0.015370254404842854, R2 0.36111927032470703\n",
      "Eval loss 0.015592707321047783, R2 0.39938533306121826\n",
      "epoch 983, loss 0.015366574749350548, R2 0.36127203702926636\n",
      "Eval loss 0.015588779002428055, R2 0.39953649044036865\n",
      "epoch 984, loss 0.015362904407083988, R2 0.3614252209663391\n",
      "Eval loss 0.015584864653646946, R2 0.3996874690055847\n",
      "epoch 985, loss 0.015359247103333473, R2 0.3615766167640686\n",
      "Eval loss 0.015580965206027031, R2 0.3998376727104187\n",
      "epoch 986, loss 0.015355600975453854, R2 0.3617281913757324\n",
      "Eval loss 0.015577075071632862, R2 0.3999871611595154\n",
      "epoch 987, loss 0.01535196602344513, R2 0.36187928915023804\n",
      "Eval loss 0.015573197044432163, R2 0.4001368284225464\n",
      "epoch 988, loss 0.015348343178629875, R2 0.3620300889015198\n",
      "Eval loss 0.015569333918392658, R2 0.4002853035926819\n",
      "epoch 989, loss 0.015344730578362942, R2 0.36218005418777466\n",
      "Eval loss 0.0155654801055789, R2 0.4004339575767517\n",
      "epoch 990, loss 0.015341130085289478, R2 0.36232990026474\n",
      "Eval loss 0.015561641193926334, R2 0.4005820155143738\n",
      "epoch 991, loss 0.015337539836764336, R2 0.36247891187667847\n",
      "Eval loss 0.015557813458144665, R2 0.4007294178009033\n",
      "epoch 992, loss 0.015333962626755238, R2 0.3626278042793274\n",
      "Eval loss 0.01555399689823389, R2 0.4008760452270508\n",
      "epoch 993, loss 0.015330392867326736, R2 0.36277639865875244\n",
      "Eval loss 0.01555019523948431, R2 0.4010230302810669\n",
      "epoch 994, loss 0.015326837077736855, R2 0.36292439699172974\n",
      "Eval loss 0.015546402893960476, R2 0.40116894245147705\n",
      "epoch 995, loss 0.015323291532695293, R2 0.3630717396736145\n",
      "Eval loss 0.015542625449597836, R2 0.4013146162033081\n",
      "epoch 996, loss 0.015319757163524628, R2 0.3632180690765381\n",
      "Eval loss 0.015538855455815792, R2 0.40145933628082275\n",
      "epoch 997, loss 0.015316233038902283, R2 0.363364577293396\n",
      "Eval loss 0.015535100363194942, R2 0.4016042947769165\n",
      "epoch 998, loss 0.015312718227505684, R2 0.3635106682777405\n",
      "Eval loss 0.015531356446444988, R2 0.4017481803894043\n",
      "epoch 999, loss 0.01530921645462513, R2 0.363656222820282\n",
      "Eval loss 0.01552762370556593, R2 0.4018924832344055\n",
      "epoch 1000, loss 0.015305723063647747, R2 0.36380141973495483\n",
      "Eval loss 0.015523904003202915, R2 0.40203577280044556\n",
      "epoch 1001, loss 0.015302243642508984, R2 0.3639460802078247\n",
      "Eval loss 0.015520191751420498, R2 0.40217870473861694\n",
      "epoch 1002, loss 0.015298771671950817, R2 0.3640909790992737\n",
      "Eval loss 0.015516494400799274, R2 0.4023209810256958\n",
      "epoch 1003, loss 0.015295309014618397, R2 0.3642345070838928\n",
      "Eval loss 0.015512808226048946, R2 0.4024631381034851\n",
      "epoch 1004, loss 0.015291860327124596, R2 0.36437803506851196\n",
      "Eval loss 0.015509132295846939, R2 0.4026045799255371\n",
      "epoch 1005, loss 0.01528842095285654, R2 0.3645203709602356\n",
      "Eval loss 0.015505469404160976, R2 0.4027453064918518\n",
      "epoch 1006, loss 0.015284989960491657, R2 0.36466336250305176\n",
      "Eval loss 0.01550181582570076, R2 0.40288639068603516\n",
      "epoch 1007, loss 0.015281571075320244, R2 0.3648058772087097\n",
      "Eval loss 0.015498174354434013, R2 0.4030264616012573\n",
      "epoch 1008, loss 0.015278162434697151, R2 0.3649477958679199\n",
      "Eval loss 0.015494543127715588, R2 0.40316635370254517\n",
      "epoch 1009, loss 0.015274761244654655, R2 0.36508917808532715\n",
      "Eval loss 0.015490924008190632, R2 0.4033055901527405\n",
      "epoch 1010, loss 0.01527137216180563, R2 0.36522984504699707\n",
      "Eval loss 0.015487314201891422, R2 0.40344512462615967\n",
      "epoch 1011, loss 0.015267995186150074, R2 0.36536961793899536\n",
      "Eval loss 0.015483716502785683, R2 0.40358370542526245\n",
      "epoch 1012, loss 0.01526462472975254, R2 0.36550968885421753\n",
      "Eval loss 0.015480129048228264, R2 0.4037213921546936\n",
      "epoch 1013, loss 0.015261264517903328, R2 0.36564934253692627\n",
      "Eval loss 0.015476553700864315, R2 0.4038592576980591\n",
      "epoch 1014, loss 0.01525791548192501, R2 0.3657887578010559\n",
      "Eval loss 0.015472987666726112, R2 0.403997004032135\n",
      "epoch 1015, loss 0.015254574827849865, R2 0.36592745780944824\n",
      "Eval loss 0.015469430945813656, R2 0.40413379669189453\n",
      "epoch 1016, loss 0.01525124441832304, R2 0.3660658597946167\n",
      "Eval loss 0.015465888194739819, R2 0.4042699337005615\n",
      "epoch 1017, loss 0.015247924253344536, R2 0.3662039041519165\n",
      "Eval loss 0.015462354756891727, R2 0.4044065475463867\n",
      "epoch 1018, loss 0.015244614332914352, R2 0.36634206771850586\n",
      "Eval loss 0.015458830632269382, R2 0.40454214811325073\n",
      "epoch 1019, loss 0.01524131279438734, R2 0.3664792776107788\n",
      "Eval loss 0.015455315820872784, R2 0.40467751026153564\n",
      "epoch 1020, loss 0.015238020569086075, R2 0.3666157126426697\n",
      "Eval loss 0.015451814979314804, R2 0.4048120379447937\n",
      "epoch 1021, loss 0.01523473858833313, R2 0.36675214767456055\n",
      "Eval loss 0.015448324382305145, R2 0.404946506023407\n",
      "epoch 1022, loss 0.015231464058160782, R2 0.3668878674507141\n",
      "Eval loss 0.015444841235876083, R2 0.4050806164741516\n",
      "epoch 1023, loss 0.015228201635181904, R2 0.36702388525009155\n",
      "Eval loss 0.015441366471350193, R2 0.4052150249481201\n",
      "epoch 1024, loss 0.015224945731461048, R2 0.367159366607666\n",
      "Eval loss 0.015437906607985497, R2 0.40534812211990356\n",
      "epoch 1025, loss 0.015221702866256237, R2 0.3672938346862793\n",
      "Eval loss 0.015434454195201397, R2 0.4054814577102661\n",
      "epoch 1026, loss 0.015218465588986874, R2 0.3674289584159851\n",
      "Eval loss 0.015431012026965618, R2 0.40561366081237793\n",
      "epoch 1027, loss 0.01521524041891098, R2 0.36756259202957153\n",
      "Eval loss 0.01542758196592331, R2 0.40574580430984497\n",
      "epoch 1028, loss 0.015212022699415684, R2 0.36769574880599976\n",
      "Eval loss 0.015424159355461597, R2 0.40587764978408813\n",
      "epoch 1029, loss 0.015208813361823559, R2 0.3678293228149414\n",
      "Eval loss 0.015420747920870781, R2 0.4060091972351074\n",
      "epoch 1030, loss 0.01520561520010233, R2 0.36796247959136963\n",
      "Eval loss 0.015417344868183136, R2 0.4061397910118103\n",
      "epoch 1031, loss 0.015202425420284271, R2 0.3680950999259949\n",
      "Eval loss 0.015413952991366386, R2 0.40627044439315796\n",
      "epoch 1032, loss 0.01519924309104681, R2 0.3682275414466858\n",
      "Eval loss 0.015410570427775383, R2 0.40640074014663696\n",
      "epoch 1033, loss 0.015196071937680244, R2 0.36835920810699463\n",
      "Eval loss 0.01540719997137785, R2 0.40653055906295776\n",
      "epoch 1034, loss 0.015192908234894276, R2 0.36849069595336914\n",
      "Eval loss 0.015403835102915764, R2 0.40666013956069946\n",
      "epoch 1035, loss 0.015189751982688904, R2 0.3686218857765198\n",
      "Eval loss 0.015400480479001999, R2 0.40678972005844116\n",
      "epoch 1036, loss 0.015186605975031853, R2 0.3687528371810913\n",
      "Eval loss 0.015397136099636555, R2 0.4069182276725769\n",
      "epoch 1037, loss 0.015183470211923122, R2 0.36888378858566284\n",
      "Eval loss 0.015393799170851707, R2 0.40704745054244995\n",
      "epoch 1038, loss 0.01518034003674984, R2 0.36901330947875977\n",
      "Eval loss 0.01539047434926033, R2 0.4071754813194275\n",
      "epoch 1039, loss 0.015177219174802303, R2 0.3691437840461731\n",
      "Eval loss 0.015387158840894699, R2 0.40730249881744385\n",
      "epoch 1040, loss 0.015174110420048237, R2 0.36927181482315063\n",
      "Eval loss 0.01538385171443224, R2 0.4074302315711975\n",
      "epoch 1041, loss 0.015171006321907043, R2 0.3694012761116028\n",
      "Eval loss 0.015380553901195526, R2 0.4075568914413452\n",
      "epoch 1042, loss 0.015167911536991596, R2 0.369529664516449\n",
      "Eval loss 0.015377266332507133, R2 0.4076840877532959\n",
      "epoch 1043, loss 0.015164823271334171, R2 0.3696582317352295\n",
      "Eval loss 0.015373987145721912, R2 0.407809853553772\n",
      "epoch 1044, loss 0.015161747112870216, R2 0.36978650093078613\n",
      "Eval loss 0.015370714478194714, R2 0.40793609619140625\n",
      "epoch 1045, loss 0.015158677473664284, R2 0.36991435289382935\n",
      "Eval loss 0.015367453917860985, R2 0.40806204080581665\n",
      "epoch 1046, loss 0.015155617147684097, R2 0.3700407147407532\n",
      "Eval loss 0.015364201739430428, R2 0.4081871509552002\n",
      "epoch 1047, loss 0.015152563340961933, R2 0.3701682686805725\n",
      "Eval loss 0.015360958874225616, R2 0.4083118438720703\n",
      "epoch 1048, loss 0.01514951791614294, R2 0.3702942132949829\n",
      "Eval loss 0.015357722528278828, R2 0.4084363579750061\n",
      "epoch 1049, loss 0.01514648087322712, R2 0.3704206943511963\n",
      "Eval loss 0.015354498289525509, R2 0.40856093168258667\n",
      "epoch 1050, loss 0.015143453143537045, R2 0.37054651975631714\n",
      "Eval loss 0.015351282432675362, R2 0.40868496894836426\n",
      "epoch 1051, loss 0.015140432864427567, R2 0.37067264318466187\n",
      "Eval loss 0.015348073095083237, R2 0.40880805253982544\n",
      "epoch 1052, loss 0.01513742096722126, R2 0.3707970976829529\n",
      "Eval loss 0.015344872139394283, R2 0.4089322090148926\n",
      "epoch 1053, loss 0.01513441652059555, R2 0.370922327041626\n",
      "Eval loss 0.0153416832908988, R2 0.40905487537384033\n",
      "epoch 1054, loss 0.015131420455873013, R2 0.3710467219352722\n",
      "Eval loss 0.01533849909901619, R2 0.40917718410491943\n",
      "epoch 1055, loss 0.015128430910408497, R2 0.37117135524749756\n",
      "Eval loss 0.01533532701432705, R2 0.40929973125457764\n",
      "epoch 1056, loss 0.015125450678169727, R2 0.3712947964668274\n",
      "Eval loss 0.01533216331154108, R2 0.409421443939209\n",
      "epoch 1057, loss 0.01512247882783413, R2 0.37141817808151245\n",
      "Eval loss 0.015329006128013134, R2 0.40954285860061646\n",
      "epoch 1058, loss 0.015119514428079128, R2 0.3715415596961975\n",
      "Eval loss 0.015325856395065784, R2 0.4096641540527344\n",
      "epoch 1059, loss 0.015116558410227299, R2 0.3716650605201721\n",
      "Eval loss 0.01532271783798933, R2 0.40978556871414185\n",
      "epoch 1060, loss 0.015113607980310917, R2 0.37178707122802734\n",
      "Eval loss 0.015319585800170898, R2 0.4099053740501404\n",
      "epoch 1061, loss 0.015110667794942856, R2 0.37190908193588257\n",
      "Eval loss 0.015316463075578213, R2 0.410025954246521\n",
      "epoch 1062, loss 0.015107733197510242, R2 0.37203067541122437\n",
      "Eval loss 0.015313348732888699, R2 0.4101454019546509\n",
      "epoch 1063, loss 0.0151048069819808, R2 0.3721526861190796\n",
      "Eval loss 0.015310242772102356, R2 0.41026556491851807\n",
      "epoch 1064, loss 0.015101888217031956, R2 0.37227481603622437\n",
      "Eval loss 0.015307145193219185, R2 0.4103850722312927\n",
      "epoch 1065, loss 0.015098975971341133, R2 0.37239503860473633\n",
      "Eval loss 0.015304054133594036, R2 0.4105042815208435\n",
      "epoch 1066, loss 0.015096073038876057, R2 0.3725161552429199\n",
      "Eval loss 0.015300973318517208, R2 0.4106224775314331\n",
      "epoch 1067, loss 0.015093176625669003, R2 0.3726363182067871\n",
      "Eval loss 0.015297897160053253, R2 0.4107409715652466\n",
      "epoch 1068, loss 0.015090285800397396, R2 0.37275707721710205\n",
      "Eval loss 0.015294832177460194, R2 0.4108593463897705\n",
      "epoch 1069, loss 0.015087406150996685, R2 0.37287598848342896\n",
      "Eval loss 0.015291772782802582, R2 0.41097718477249146\n",
      "epoch 1070, loss 0.015084533020853996, R2 0.37299561500549316\n",
      "Eval loss 0.01528872363269329, R2 0.41109412908554077\n",
      "epoch 1071, loss 0.015081667341291904, R2 0.3731153607368469\n",
      "Eval loss 0.015285681933164597, R2 0.41121166944503784\n",
      "epoch 1072, loss 0.01507880724966526, R2 0.373233437538147\n",
      "Eval loss 0.01528264582157135, R2 0.4113285541534424\n",
      "epoch 1073, loss 0.015075954608619213, R2 0.3733518123626709\n",
      "Eval loss 0.015279620885848999, R2 0.41144508123397827\n",
      "epoch 1074, loss 0.015073109418153763, R2 0.3734702467918396\n",
      "Eval loss 0.015276601538062096, R2 0.41156142950057983\n",
      "epoch 1075, loss 0.01507027167826891, R2 0.3735882043838501\n",
      "Eval loss 0.015273588709533215, R2 0.4116777777671814\n",
      "epoch 1076, loss 0.015067440457642078, R2 0.3737061023712158\n",
      "Eval loss 0.015270587056875229, R2 0.41179293394088745\n",
      "epoch 1077, loss 0.015064617618918419, R2 0.37382322549819946\n",
      "Eval loss 0.015267591923475266, R2 0.41190826892852783\n",
      "epoch 1078, loss 0.015061800368130207, R2 0.37394052743911743\n",
      "Eval loss 0.015264604240655899, R2 0.412023663520813\n",
      "epoch 1079, loss 0.015058991499245167, R2 0.3740572929382324\n",
      "Eval loss 0.015261621214449406, R2 0.41213810443878174\n",
      "epoch 1080, loss 0.015056188218295574, R2 0.3741735816001892\n",
      "Eval loss 0.015258649364113808, R2 0.4122527241706848\n",
      "epoch 1081, loss 0.015053391456604004, R2 0.374289870262146\n",
      "Eval loss 0.015255684964358807, R2 0.4123672842979431\n",
      "epoch 1082, loss 0.01505060400813818, R2 0.3744059205055237\n",
      "Eval loss 0.015252725221216679, R2 0.4124807119369507\n",
      "epoch 1083, loss 0.015047823078930378, R2 0.37452131509780884\n",
      "Eval loss 0.015249774791300297, R2 0.41259491443634033\n",
      "epoch 1084, loss 0.015045047737658024, R2 0.37463682889938354\n",
      "Eval loss 0.015246831811964512, R2 0.4127082824707031\n",
      "epoch 1085, loss 0.015042277984321117, R2 0.37475156784057617\n",
      "Eval loss 0.015243894420564175, R2 0.4128214120864868\n",
      "epoch 1086, loss 0.015039517544209957, R2 0.3748667240142822\n",
      "Eval loss 0.015240969136357307, R2 0.41293394565582275\n",
      "epoch 1087, loss 0.015036762692034245, R2 0.37498122453689575\n",
      "Eval loss 0.015238044783473015, R2 0.4130462408065796\n",
      "epoch 1088, loss 0.015034014359116554, R2 0.3750952482223511\n",
      "Eval loss 0.015235130675137043, R2 0.4131588339805603\n",
      "epoch 1089, loss 0.015031273476779461, R2 0.37520939111709595\n",
      "Eval loss 0.015232222154736519, R2 0.41327136754989624\n",
      "epoch 1090, loss 0.015028540045022964, R2 0.3753228187561035\n",
      "Eval loss 0.015229322947561741, R2 0.41338253021240234\n",
      "epoch 1091, loss 0.015025812201201916, R2 0.3754361867904663\n",
      "Eval loss 0.01522643119096756, R2 0.4134935736656189\n",
      "epoch 1092, loss 0.015023089945316315, R2 0.37555015087127686\n",
      "Eval loss 0.015223545022308826, R2 0.413605272769928\n",
      "epoch 1093, loss 0.01502037700265646, R2 0.37566208839416504\n",
      "Eval loss 0.015220667235553265, R2 0.4137159585952759\n",
      "epoch 1094, loss 0.015017667785286903, R2 0.3757747411727905\n",
      "Eval loss 0.01521779503673315, R2 0.4138270616531372\n",
      "epoch 1095, loss 0.015014966949820518, R2 0.375887393951416\n",
      "Eval loss 0.015214930288493633, R2 0.41393691301345825\n",
      "epoch 1096, loss 0.015012271702289581, R2 0.37599921226501465\n",
      "Eval loss 0.015212072059512138, R2 0.4140470027923584\n",
      "epoch 1097, loss 0.015009581111371517, R2 0.3761110305786133\n",
      "Eval loss 0.01520922314375639, R2 0.4141569137573242\n",
      "epoch 1098, loss 0.0150068998336792, R2 0.37622249126434326\n",
      "Eval loss 0.015206378884613514, R2 0.4142659902572632\n",
      "epoch 1099, loss 0.015004225075244904, R2 0.3763335347175598\n",
      "Eval loss 0.01520354114472866, R2 0.4143754839897156\n",
      "epoch 1100, loss 0.015001555904746056, R2 0.37644505500793457\n",
      "Eval loss 0.015200710855424404, R2 0.4144846200942993\n",
      "epoch 1101, loss 0.01499889139086008, R2 0.3765549659729004\n",
      "Eval loss 0.015197886154055595, R2 0.41459327936172485\n",
      "epoch 1102, loss 0.014996233396232128, R2 0.37666571140289307\n",
      "Eval loss 0.015195072628557682, R2 0.41470181941986084\n",
      "epoch 1103, loss 0.014993584714829922, R2 0.37677597999572754\n",
      "Eval loss 0.015192262828350067, R2 0.4148097038269043\n",
      "epoch 1104, loss 0.014990939758718014, R2 0.3768863081932068\n",
      "Eval loss 0.015189457684755325, R2 0.4149182438850403\n",
      "epoch 1105, loss 0.014988300390541553, R2 0.3769954442977905\n",
      "Eval loss 0.015186660923063755, R2 0.4150254726409912\n",
      "epoch 1106, loss 0.01498566847294569, R2 0.37710481882095337\n",
      "Eval loss 0.015183870680630207, R2 0.41513311862945557\n",
      "epoch 1107, loss 0.014983043074607849, R2 0.3772137761116028\n",
      "Eval loss 0.015181087888777256, R2 0.41524070501327515\n",
      "epoch 1108, loss 0.014980423264205456, R2 0.3773234486579895\n",
      "Eval loss 0.015178310684859753, R2 0.41534745693206787\n",
      "epoch 1109, loss 0.014977808110415936, R2 0.37743157148361206\n",
      "Eval loss 0.015175539068877697, R2 0.4154542088508606\n",
      "epoch 1110, loss 0.014975199475884438, R2 0.3775401711463928\n",
      "Eval loss 0.015172774903476238, R2 0.41556084156036377\n",
      "epoch 1111, loss 0.014972598291933537, R2 0.3776482939720154\n",
      "Eval loss 0.015170020051300526, R2 0.41566699743270874\n",
      "epoch 1112, loss 0.014970003627240658, R2 0.37775593996047974\n",
      "Eval loss 0.015167267993092537, R2 0.4157731533050537\n",
      "epoch 1113, loss 0.014967414550483227, R2 0.3778635263442993\n",
      "Eval loss 0.015164521522819996, R2 0.41587865352630615\n",
      "epoch 1114, loss 0.014964829199016094, R2 0.37797099351882935\n",
      "Eval loss 0.015161784365773201, R2 0.41598373651504517\n",
      "epoch 1115, loss 0.014962253160774708, R2 0.3780781030654907\n",
      "Eval loss 0.01515905186533928, R2 0.41608911752700806\n",
      "epoch 1116, loss 0.014959679916501045, R2 0.37818509340286255\n",
      "Eval loss 0.015156326815485954, R2 0.4161939024925232\n",
      "epoch 1117, loss 0.014957113191485405, R2 0.3782915472984314\n",
      "Eval loss 0.015153605490922928, R2 0.4162992835044861\n",
      "epoch 1118, loss 0.014954553917050362, R2 0.37839871644973755\n",
      "Eval loss 0.015150894410908222, R2 0.4164033532142639\n",
      "epoch 1119, loss 0.014951999299228191, R2 0.37850427627563477\n",
      "Eval loss 0.01514818798750639, R2 0.4165083169937134\n",
      "epoch 1120, loss 0.014949450269341469, R2 0.37861067056655884\n",
      "Eval loss 0.01514548622071743, R2 0.41661185026168823\n",
      "epoch 1121, loss 0.01494690589606762, R2 0.37871599197387695\n",
      "Eval loss 0.015142790041863918, R2 0.4167153239250183\n",
      "epoch 1122, loss 0.014944369904696941, R2 0.3788214325904846\n",
      "Eval loss 0.015140102244913578, R2 0.41681885719299316\n",
      "epoch 1123, loss 0.014941838569939137, R2 0.37892723083496094\n",
      "Eval loss 0.015137420035898685, R2 0.41692203283309937\n",
      "epoch 1124, loss 0.014939313754439354, R2 0.3790323734283447\n",
      "Eval loss 0.01513474341481924, R2 0.41702526807785034\n",
      "epoch 1125, loss 0.014936791732907295, R2 0.37913626432418823\n",
      "Eval loss 0.015132074244320393, R2 0.4171282649040222\n",
      "epoch 1126, loss 0.014934277161955833, R2 0.37924134731292725\n",
      "Eval loss 0.015129407867789268, R2 0.41723114252090454\n",
      "epoch 1127, loss 0.01493176817893982, R2 0.3793454170227051\n",
      "Eval loss 0.01512675080448389, R2 0.41733312606811523\n",
      "epoch 1128, loss 0.014929263852536678, R2 0.3794493079185486\n",
      "Eval loss 0.015124095603823662, R2 0.4174359440803528\n",
      "epoch 1129, loss 0.014926766976714134, R2 0.37955331802368164\n",
      "Eval loss 0.01512144971638918, R2 0.41753822565078735\n",
      "epoch 1130, loss 0.014924274757504463, R2 0.3796573281288147\n",
      "Eval loss 0.01511880848556757, R2 0.41763925552368164\n",
      "epoch 1131, loss 0.014921785332262516, R2 0.379760205745697\n",
      "Eval loss 0.015116174705326557, R2 0.41774052381515503\n",
      "epoch 1132, loss 0.014919305220246315, R2 0.3798636794090271\n",
      "Eval loss 0.015113546513020992, R2 0.41784214973449707\n",
      "epoch 1133, loss 0.014916828833520412, R2 0.37996697425842285\n",
      "Eval loss 0.015110922046005726, R2 0.41794353723526\n",
      "epoch 1134, loss 0.014914358034729958, R2 0.3800694942474365\n",
      "Eval loss 0.015108304098248482, R2 0.41804420948028564\n",
      "epoch 1135, loss 0.014911893755197525, R2 0.380171537399292\n",
      "Eval loss 0.015105691738426685, R2 0.4181448221206665\n",
      "epoch 1136, loss 0.014909432269632816, R2 0.38027364015579224\n",
      "Eval loss 0.015103086829185486, R2 0.4182446599006653\n",
      "epoch 1137, loss 0.014906978234648705, R2 0.38037586212158203\n",
      "Eval loss 0.015100485645234585, R2 0.4183448553085327\n",
      "epoch 1138, loss 0.014904527924954891, R2 0.3804780840873718\n",
      "Eval loss 0.015097892843186855, R2 0.4184451103210449\n",
      "epoch 1139, loss 0.0149020841345191, R2 0.380578875541687\n",
      "Eval loss 0.01509530283510685, R2 0.41854482889175415\n",
      "epoch 1140, loss 0.014899646863341331, R2 0.3806803822517395\n",
      "Eval loss 0.015092720277607441, R2 0.4186443090438843\n",
      "epoch 1141, loss 0.014897213317453861, R2 0.38078176975250244\n",
      "Eval loss 0.015090142376720905, R2 0.4187433123588562\n",
      "epoch 1142, loss 0.014894787222146988, R2 0.3808826208114624\n",
      "Eval loss 0.015087570063769817, R2 0.4188428521156311\n",
      "epoch 1143, loss 0.014892362989485264, R2 0.3809831738471985\n",
      "Eval loss 0.015085003338754177, R2 0.4189414381980896\n",
      "epoch 1144, loss 0.014889944344758987, R2 0.38108426332473755\n",
      "Eval loss 0.01508244127035141, R2 0.4190402626991272\n",
      "epoch 1145, loss 0.014887530356645584, R2 0.3811842203140259\n",
      "Eval loss 0.015079885721206665, R2 0.41913872957229614\n",
      "epoch 1146, loss 0.014885123819112778, R2 0.381284236907959\n",
      "Eval loss 0.015077335759997368, R2 0.419236958026886\n",
      "epoch 1147, loss 0.014882720075547695, R2 0.3813841938972473\n",
      "Eval loss 0.015074790455400944, R2 0.41933465003967285\n",
      "epoch 1148, loss 0.01488032378256321, R2 0.38148361444473267\n",
      "Eval loss 0.015072250738739967, R2 0.4194326400756836\n",
      "epoch 1149, loss 0.014877931214869022, R2 0.381583034992218\n",
      "Eval loss 0.015069717541337013, R2 0.4195300340652466\n",
      "epoch 1150, loss 0.014875544235110283, R2 0.38168203830718994\n",
      "Eval loss 0.015067188069224358, R2 0.4196277856826782\n",
      "epoch 1151, loss 0.014873161911964417, R2 0.3817814588546753\n",
      "Eval loss 0.015064665116369724, R2 0.4197251796722412\n",
      "epoch 1152, loss 0.014870783314108849, R2 0.3818807601928711\n",
      "Eval loss 0.015062146820127964, R2 0.41982197761535645\n",
      "epoch 1153, loss 0.014868412166833878, R2 0.38197946548461914\n",
      "Eval loss 0.015059635043144226, R2 0.41991889476776123\n",
      "epoch 1154, loss 0.01486604381352663, R2 0.38207733631134033\n",
      "Eval loss 0.015057125128805637, R2 0.4200153946876526\n",
      "epoch 1155, loss 0.014863681979477406, R2 0.3821755051612854\n",
      "Eval loss 0.01505462545901537, R2 0.4201117157936096\n",
      "epoch 1156, loss 0.014861326664686203, R2 0.3822726607322693\n",
      "Eval loss 0.015052125789225101, R2 0.4202079772949219\n",
      "epoch 1157, loss 0.014858971349895, R2 0.38237130641937256\n",
      "Eval loss 0.015049634501338005, R2 0.4203035831451416\n",
      "epoch 1158, loss 0.014856623485684395, R2 0.3824685215950012\n",
      "Eval loss 0.015047147870063782, R2 0.4203997254371643\n",
      "epoch 1159, loss 0.014854281209409237, R2 0.38256585597991943\n",
      "Eval loss 0.015044664964079857, R2 0.42049503326416016\n",
      "epoch 1160, loss 0.014851940795779228, R2 0.38266414403915405\n",
      "Eval loss 0.015042189508676529, R2 0.42059123516082764\n",
      "epoch 1161, loss 0.014849609695374966, R2 0.3827604651451111\n",
      "Eval loss 0.015039718709886074, R2 0.42068570852279663\n",
      "epoch 1162, loss 0.014847281388938427, R2 0.38285720348358154\n",
      "Eval loss 0.015037252567708492, R2 0.42078107595443726\n",
      "epoch 1163, loss 0.014844956807792187, R2 0.3829536437988281\n",
      "Eval loss 0.015034790150821209, R2 0.4208753705024719\n",
      "epoch 1164, loss 0.014842637814581394, R2 0.3830506205558777\n",
      "Eval loss 0.015032334253191948, R2 0.42096996307373047\n",
      "epoch 1165, loss 0.01484032440930605, R2 0.3831467628479004\n",
      "Eval loss 0.01502988301217556, R2 0.421064555644989\n",
      "epoch 1166, loss 0.014838015660643578, R2 0.3832421898841858\n",
      "Eval loss 0.01502743735909462, R2 0.4211586117744446\n",
      "epoch 1167, loss 0.014835708774626255, R2 0.3833388686180115\n",
      "Eval loss 0.015024995431303978, R2 0.4212530255317688\n",
      "epoch 1168, loss 0.014833410270512104, R2 0.3834341764450073\n",
      "Eval loss 0.015022559091448784, R2 0.4213464856147766\n",
      "epoch 1169, loss 0.014831112697720528, R2 0.38352930545806885\n",
      "Eval loss 0.015020128339529037, R2 0.42144012451171875\n",
      "epoch 1170, loss 0.014828822575509548, R2 0.3836243152618408\n",
      "Eval loss 0.015017700381577015, R2 0.4215337038040161\n",
      "epoch 1171, loss 0.014826535247266293, R2 0.38371938467025757\n",
      "Eval loss 0.01501527987420559, R2 0.4216269254684448\n",
      "epoch 1172, loss 0.01482425443828106, R2 0.38381415605545044\n",
      "Eval loss 0.015012861229479313, R2 0.4217202663421631\n",
      "epoch 1173, loss 0.01482197642326355, R2 0.38390904664993286\n",
      "Eval loss 0.015010450035333633, R2 0.4218132495880127\n",
      "epoch 1174, loss 0.014819703064858913, R2 0.3840041160583496\n",
      "Eval loss 0.015008043497800827, R2 0.4219059944152832\n",
      "epoch 1175, loss 0.014817435294389725, R2 0.3840973973274231\n",
      "Eval loss 0.015005640685558319, R2 0.4219985008239746\n",
      "epoch 1176, loss 0.014815172180533409, R2 0.38419169187545776\n",
      "Eval loss 0.015003243461251259, R2 0.42209088802337646\n",
      "epoch 1177, loss 0.014812912791967392, R2 0.38428616523742676\n",
      "Eval loss 0.015000849030911922, R2 0.42218291759490967\n",
      "epoch 1178, loss 0.014810655266046524, R2 0.38438040018081665\n",
      "Eval loss 0.014998462982475758, R2 0.422275185585022\n",
      "epoch 1179, loss 0.014808407984673977, R2 0.38447266817092896\n",
      "Eval loss 0.014996077865362167, R2 0.4223669171333313\n",
      "epoch 1180, loss 0.014806161634624004, R2 0.3845660090446472\n",
      "Eval loss 0.01499369740486145, R2 0.42245912551879883\n",
      "epoch 1181, loss 0.01480391900986433, R2 0.38465964794158936\n",
      "Eval loss 0.014991325326263905, R2 0.42254960536956787\n",
      "epoch 1182, loss 0.014801680110394955, R2 0.38475245237350464\n",
      "Eval loss 0.014988955110311508, R2 0.4226410388946533\n",
      "epoch 1183, loss 0.01479944959282875, R2 0.3848457932472229\n",
      "Eval loss 0.014986591413617134, R2 0.4227319359779358\n",
      "epoch 1184, loss 0.014797219075262547, R2 0.3849378824234009\n",
      "Eval loss 0.01498422771692276, R2 0.42282313108444214\n",
      "epoch 1185, loss 0.01479499600827694, R2 0.38503050804138184\n",
      "Eval loss 0.014981873333454132, R2 0.42291367053985596\n",
      "epoch 1186, loss 0.014792775735259056, R2 0.3851225972175598\n",
      "Eval loss 0.014979522675275803, R2 0.4230045676231384\n",
      "epoch 1187, loss 0.01479056105017662, R2 0.3852148652076721\n",
      "Eval loss 0.014977176673710346, R2 0.4230947494506836\n",
      "epoch 1188, loss 0.014788350090384483, R2 0.3853071331977844\n",
      "Eval loss 0.014974833466112614, R2 0.42318516969680786\n",
      "epoch 1189, loss 0.014786142855882645, R2 0.3853990435600281\n",
      "Eval loss 0.014972495846450329, R2 0.4232754111289978\n",
      "epoch 1190, loss 0.014783939346671104, R2 0.3854901194572449\n",
      "Eval loss 0.014970161952078342, R2 0.42336511611938477\n",
      "epoch 1191, loss 0.014781741425395012, R2 0.38558125495910645\n",
      "Eval loss 0.014967834576964378, R2 0.4234544634819031\n",
      "epoch 1192, loss 0.014779547229409218, R2 0.3856724500656128\n",
      "Eval loss 0.014965509064495564, R2 0.423544704914093\n",
      "epoch 1193, loss 0.014777355827391148, R2 0.3857635259628296\n",
      "Eval loss 0.014963188208639622, R2 0.4236339330673218\n",
      "epoch 1194, loss 0.014775170013308525, R2 0.38585442304611206\n",
      "Eval loss 0.014960872940719128, R2 0.42372262477874756\n",
      "epoch 1195, loss 0.014772987924516201, R2 0.3859456777572632\n",
      "Eval loss 0.014958562329411507, R2 0.4238124489784241\n",
      "epoch 1196, loss 0.01477081049233675, R2 0.38603639602661133\n",
      "Eval loss 0.01495625451207161, R2 0.423900842666626\n",
      "epoch 1197, loss 0.014768636785447598, R2 0.3861267566680908\n",
      "Eval loss 0.014953953213989735, R2 0.423989474773407\n",
      "epoch 1198, loss 0.014766467735171318, R2 0.38621610403060913\n",
      "Eval loss 0.014951655641198158, R2 0.42407816648483276\n",
      "epoch 1199, loss 0.014764302410185337, R2 0.38630592823028564\n",
      "Eval loss 0.01494936179369688, R2 0.4241660237312317\n",
      "epoch 1200, loss 0.014762140810489655, R2 0.3863963484764099\n",
      "Eval loss 0.014947071671485901, R2 0.4242541790008545\n",
      "epoch 1201, loss 0.01475998293608427, R2 0.3864856958389282\n",
      "Eval loss 0.014944786205887794, R2 0.42434293031692505\n",
      "epoch 1202, loss 0.014757830649614334, R2 0.3865753412246704\n",
      "Eval loss 0.014942505396902561, R2 0.42443007230758667\n",
      "epoch 1203, loss 0.014755680225789547, R2 0.3866647481918335\n",
      "Eval loss 0.014940227381885052, R2 0.42451798915863037\n",
      "epoch 1204, loss 0.014753536321222782, R2 0.3867536187171936\n",
      "Eval loss 0.01493795681744814, R2 0.4246057868003845\n",
      "epoch 1205, loss 0.014751396141946316, R2 0.3868427872657776\n",
      "Eval loss 0.01493568904697895, R2 0.4246929883956909\n",
      "epoch 1206, loss 0.014749257825314999, R2 0.3869326114654541\n",
      "Eval loss 0.014933424070477486, R2 0.4247802495956421\n",
      "epoch 1207, loss 0.01474712509661913, R2 0.38702017068862915\n",
      "Eval loss 0.014931163750588894, R2 0.4248674511909485\n",
      "epoch 1208, loss 0.014744995161890984, R2 0.3871086835861206\n",
      "Eval loss 0.01492890901863575, R2 0.4249541759490967\n",
      "epoch 1209, loss 0.014742868021130562, R2 0.3871970772743225\n",
      "Eval loss 0.014926656149327755, R2 0.42504096031188965\n",
      "epoch 1210, loss 0.014740747399628162, R2 0.3872852325439453\n",
      "Eval loss 0.014924408867955208, R2 0.42512768507003784\n",
      "epoch 1211, loss 0.014738630503416061, R2 0.3873733878135681\n",
      "Eval loss 0.01492216531187296, R2 0.42521393299102783\n",
      "epoch 1212, loss 0.014736517332494259, R2 0.3874610662460327\n",
      "Eval loss 0.014919925481081009, R2 0.4253001809120178\n",
      "epoch 1213, loss 0.014734405092895031, R2 0.3875492215156555\n",
      "Eval loss 0.014917690306901932, R2 0.42538630962371826\n",
      "epoch 1214, loss 0.0147323003038764, R2 0.38763630390167236\n",
      "Eval loss 0.014915460720658302, R2 0.4254720211029053\n",
      "epoch 1215, loss 0.014730199240148067, R2 0.3877238631248474\n",
      "Eval loss 0.014913232997059822, R2 0.4255579710006714\n",
      "epoch 1216, loss 0.01472809910774231, R2 0.38781118392944336\n",
      "Eval loss 0.01491100899875164, R2 0.4256439805030823\n",
      "epoch 1217, loss 0.014726003631949425, R2 0.38789862394332886\n",
      "Eval loss 0.014908790588378906, R2 0.4257287383079529\n",
      "epoch 1218, loss 0.014723913744091988, R2 0.38798511028289795\n",
      "Eval loss 0.014906574040651321, R2 0.425814151763916\n",
      "epoch 1219, loss 0.0147218257188797, R2 0.38807213306427\n",
      "Eval loss 0.014904364012181759, R2 0.4258994460105896\n",
      "epoch 1220, loss 0.014719742350280285, R2 0.38815832138061523\n",
      "Eval loss 0.014902153983712196, R2 0.4259852170944214\n",
      "epoch 1221, loss 0.014717663638293743, R2 0.3882449269294739\n",
      "Eval loss 0.014899952337145805, R2 0.4260697364807129\n",
      "epoch 1222, loss 0.01471558678895235, R2 0.3883310556411743\n",
      "Eval loss 0.014897751621901989, R2 0.42615413665771484\n",
      "epoch 1223, loss 0.01471351645886898, R2 0.38841712474823\n",
      "Eval loss 0.014895554631948471, R2 0.4262389540672302\n",
      "epoch 1224, loss 0.014711445197463036, R2 0.38850319385528564\n",
      "Eval loss 0.01489336509257555, R2 0.4263231158256531\n",
      "epoch 1225, loss 0.014709381386637688, R2 0.3885895609855652\n",
      "Eval loss 0.014891175553202629, R2 0.42640745639801025\n",
      "epoch 1226, loss 0.014707320369780064, R2 0.38867485523223877\n",
      "Eval loss 0.014888991601765156, R2 0.42649155855178833\n",
      "epoch 1227, loss 0.014705262146890163, R2 0.38876116275787354\n",
      "Eval loss 0.014886812306940556, R2 0.42657536268234253\n",
      "epoch 1228, loss 0.014703208580613136, R2 0.3888455629348755\n",
      "Eval loss 0.01488463580608368, R2 0.42665916681289673\n",
      "epoch 1229, loss 0.014701157808303833, R2 0.38893139362335205\n",
      "Eval loss 0.014882462099194527, R2 0.4267430901527405\n",
      "epoch 1230, loss 0.014699110761284828, R2 0.38901591300964355\n",
      "Eval loss 0.014880294911563396, R2 0.4268268942832947\n",
      "epoch 1231, loss 0.014697069302201271, R2 0.38910073041915894\n",
      "Eval loss 0.014878127723932266, R2 0.4269101619720459\n",
      "epoch 1232, loss 0.014695027843117714, R2 0.3891856074333191\n",
      "Eval loss 0.014875965192914009, R2 0.4269930124282837\n",
      "epoch 1233, loss 0.014692991971969604, R2 0.3892710208892822\n",
      "Eval loss 0.014873809181153774, R2 0.4270765781402588\n",
      "epoch 1234, loss 0.014690959826111794, R2 0.3893548846244812\n",
      "Eval loss 0.014871655963361263, R2 0.42715978622436523\n",
      "epoch 1235, loss 0.014688932336866856, R2 0.3894389867782593\n",
      "Eval loss 0.01486950647085905, R2 0.42724210023880005\n",
      "epoch 1236, loss 0.014686906710267067, R2 0.38952314853668213\n",
      "Eval loss 0.014867358841001987, R2 0.42732518911361694\n",
      "epoch 1237, loss 0.014684885740280151, R2 0.38960713148117065\n",
      "Eval loss 0.014865215867757797, R2 0.4274073839187622\n",
      "epoch 1238, loss 0.01468286570161581, R2 0.3896911144256592\n",
      "Eval loss 0.01486307755112648, R2 0.4274899363517761\n",
      "epoch 1239, loss 0.014680853113532066, R2 0.3897753953933716\n",
      "Eval loss 0.014860942028462887, R2 0.42757248878479004\n",
      "epoch 1240, loss 0.014678839594125748, R2 0.3898584842681885\n",
      "Eval loss 0.014858809299767017, R2 0.4276541471481323\n",
      "epoch 1241, loss 0.014676832593977451, R2 0.389941930770874\n",
      "Eval loss 0.014856682159006596, R2 0.42773646116256714\n",
      "epoch 1242, loss 0.01467482652515173, R2 0.3900254964828491\n",
      "Eval loss 0.014854555949568748, R2 0.4278181791305542\n",
      "epoch 1243, loss 0.01467282697558403, R2 0.39010900259017944\n",
      "Eval loss 0.014852436259388924, R2 0.4278998374938965\n",
      "epoch 1244, loss 0.014670828357338905, R2 0.390192449092865\n",
      "Eval loss 0.014850319363176823, R2 0.4279811978340149\n",
      "epoch 1245, loss 0.014668834395706654, R2 0.39027512073516846\n",
      "Eval loss 0.014848205260932446, R2 0.4280627965927124\n",
      "epoch 1246, loss 0.0146668441593647, R2 0.3903570771217346\n",
      "Eval loss 0.014846093952655792, R2 0.4281437397003174\n",
      "epoch 1247, loss 0.014664855785667896, R2 0.390439510345459\n",
      "Eval loss 0.014843988232314587, R2 0.42822521924972534\n",
      "epoch 1248, loss 0.01466287299990654, R2 0.3905221223831177\n",
      "Eval loss 0.01484188437461853, R2 0.4283062219619751\n",
      "epoch 1249, loss 0.014660892076790333, R2 0.39060527086257935\n",
      "Eval loss 0.014839783310890198, R2 0.4283871650695801\n",
      "epoch 1250, loss 0.014658915810286999, R2 0.3906872272491455\n",
      "Eval loss 0.014837688766419888, R2 0.4284675717353821\n",
      "epoch 1251, loss 0.014656939543783665, R2 0.39076876640319824\n",
      "Eval loss 0.014835595153272152, R2 0.4285486936569214\n",
      "epoch 1252, loss 0.014654969796538353, R2 0.39085084199905396\n",
      "Eval loss 0.014833505265414715, R2 0.4286288022994995\n",
      "epoch 1253, loss 0.01465300191193819, R2 0.3909330368041992\n",
      "Eval loss 0.014831420965492725, R2 0.42870914936065674\n",
      "epoch 1254, loss 0.014651038683950901, R2 0.3910142183303833\n",
      "Eval loss 0.014829336665570736, R2 0.4287893772125244\n",
      "epoch 1255, loss 0.014649076387286186, R2 0.3910956382751465\n",
      "Eval loss 0.014827258884906769, R2 0.4288696050643921\n",
      "epoch 1256, loss 0.014647118747234344, R2 0.39117681980133057\n",
      "Eval loss 0.014825182966887951, R2 0.428949773311615\n",
      "epoch 1257, loss 0.014645165763795376, R2 0.391258180141449\n",
      "Eval loss 0.014823110774159431, R2 0.42902904748916626\n",
      "epoch 1258, loss 0.014643214643001556, R2 0.3913393020629883\n",
      "Eval loss 0.014821041375398636, R2 0.42910897731781006\n",
      "epoch 1259, loss 0.01464126631617546, R2 0.39142030477523804\n",
      "Eval loss 0.014818976633250713, R2 0.4291887879371643\n",
      "epoch 1260, loss 0.014639321714639664, R2 0.39150112867355347\n",
      "Eval loss 0.014816914685070515, R2 0.4292682409286499\n",
      "epoch 1261, loss 0.01463737990707159, R2 0.3915814161300659\n",
      "Eval loss 0.014814856462180614, R2 0.4293471574783325\n",
      "epoch 1262, loss 0.014635440893471241, R2 0.39166295528411865\n",
      "Eval loss 0.014812801033258438, R2 0.42942649126052856\n",
      "epoch 1263, loss 0.01463350746780634, R2 0.3917429447174072\n",
      "Eval loss 0.014810750260949135, R2 0.4295051693916321\n",
      "epoch 1264, loss 0.014631574973464012, R2 0.3918231129646301\n",
      "Eval loss 0.014808698557317257, R2 0.42958468198776245\n",
      "epoch 1265, loss 0.014629644341766834, R2 0.3919033408164978\n",
      "Eval loss 0.014806654304265976, R2 0.4296633005142212\n",
      "epoch 1266, loss 0.014627720229327679, R2 0.3919833302497864\n",
      "Eval loss 0.01480461098253727, R2 0.4297417998313904\n",
      "epoch 1267, loss 0.014625797979533672, R2 0.3920632004737854\n",
      "Eval loss 0.014802572317421436, R2 0.42982035875320435\n",
      "epoch 1268, loss 0.01462387852370739, R2 0.392143189907074\n",
      "Eval loss 0.014800538308918476, R2 0.42989856004714966\n",
      "epoch 1269, loss 0.014621960930526257, R2 0.39222270250320435\n",
      "Eval loss 0.014798504300415516, R2 0.42997753620147705\n",
      "epoch 1270, loss 0.014620047993957996, R2 0.3923032283782959\n",
      "Eval loss 0.014796476811170578, R2 0.43005532026290894\n",
      "epoch 1271, loss 0.014618136920034885, R2 0.39238184690475464\n",
      "Eval loss 0.01479445118457079, R2 0.4301331639289856\n",
      "epoch 1272, loss 0.014616232365369797, R2 0.39246124029159546\n",
      "Eval loss 0.01479242742061615, R2 0.43021130561828613\n",
      "epoch 1273, loss 0.014614328742027283, R2 0.39254051446914673\n",
      "Eval loss 0.014790408313274384, R2 0.43028873205184937\n",
      "epoch 1274, loss 0.014612426981329918, R2 0.3926190137863159\n",
      "Eval loss 0.014788390137255192, R2 0.4303666353225708\n",
      "epoch 1275, loss 0.014610528945922852, R2 0.39269787073135376\n",
      "Eval loss 0.014786376617848873, R2 0.4304441809654236\n",
      "epoch 1276, loss 0.014608633704483509, R2 0.3927772045135498\n",
      "Eval loss 0.014784368686378002, R2 0.43052172660827637\n",
      "epoch 1277, loss 0.014606742188334465, R2 0.3928554654121399\n",
      "Eval loss 0.014782360754907131, R2 0.4305986762046814\n",
      "epoch 1278, loss 0.01460485253483057, R2 0.39293479919433594\n",
      "Eval loss 0.014780359342694283, R2 0.4306756258010864\n",
      "epoch 1279, loss 0.014602969400584698, R2 0.3930128812789917\n",
      "Eval loss 0.01477835699915886, R2 0.4307534694671631\n",
      "epoch 1280, loss 0.014601083472371101, R2 0.39309048652648926\n",
      "Eval loss 0.014776360243558884, R2 0.430830180644989\n",
      "epoch 1281, loss 0.014599204994738102, R2 0.393168568611145\n",
      "Eval loss 0.014774365350604057, R2 0.43090736865997314\n",
      "epoch 1282, loss 0.014597328379750252, R2 0.39324676990509033\n",
      "Eval loss 0.014772373251616955, R2 0.43098390102386475\n",
      "epoch 1283, loss 0.014595454558730125, R2 0.3933252692222595\n",
      "Eval loss 0.014770385809242725, R2 0.4310603141784668\n",
      "epoch 1284, loss 0.014593584463000298, R2 0.39340221881866455\n",
      "Eval loss 0.01476839929819107, R2 0.43113696575164795\n",
      "epoch 1285, loss 0.01459171436727047, R2 0.39347994327545166\n",
      "Eval loss 0.014766418375074863, R2 0.43121278285980225\n",
      "epoch 1286, loss 0.014589851722121239, R2 0.3935573697090149\n",
      "Eval loss 0.014764441177248955, R2 0.43128877878189087\n",
      "epoch 1287, loss 0.014587988145649433, R2 0.3936348557472229\n",
      "Eval loss 0.014762463979423046, R2 0.43136513233184814\n",
      "epoch 1288, loss 0.0145861292257905, R2 0.39371228218078613\n",
      "Eval loss 0.014760492369532585, R2 0.4314415454864502\n",
      "epoch 1289, loss 0.014584274031221867, R2 0.3937894105911255\n",
      "Eval loss 0.01475851982831955, R2 0.4315175414085388\n",
      "epoch 1290, loss 0.014582418836653233, R2 0.3938659429550171\n",
      "Eval loss 0.014756552875041962, R2 0.43159300088882446\n",
      "epoch 1291, loss 0.014580566436052322, R2 0.3939434885978699\n",
      "Eval loss 0.014754590578377247, R2 0.43166905641555786\n",
      "epoch 1292, loss 0.014578720554709435, R2 0.39402061700820923\n",
      "Eval loss 0.014752629213035107, R2 0.4317442774772644\n",
      "epoch 1293, loss 0.014576875604689121, R2 0.39409691095352173\n",
      "Eval loss 0.014750671572983265, R2 0.4318196773529053\n",
      "epoch 1294, loss 0.014575033448636532, R2 0.3941733241081238\n",
      "Eval loss 0.014748717658221722, R2 0.4318951368331909\n",
      "epoch 1295, loss 0.014573195949196815, R2 0.39425021409988403\n",
      "Eval loss 0.014746763743460178, R2 0.43196988105773926\n",
      "epoch 1296, loss 0.0145713584497571, R2 0.39432621002197266\n",
      "Eval loss 0.014744814485311508, R2 0.43204545974731445\n",
      "epoch 1297, loss 0.014569525606930256, R2 0.3944022059440613\n",
      "Eval loss 0.014742868952453136, R2 0.4321202039718628\n",
      "epoch 1298, loss 0.014567694626748562, R2 0.39447832107543945\n",
      "Eval loss 0.014740927144885063, R2 0.4321950674057007\n",
      "epoch 1299, loss 0.014565865509212017, R2 0.39455437660217285\n",
      "Eval loss 0.01473898533731699, R2 0.43226999044418335\n",
      "epoch 1300, loss 0.01456404011696577, R2 0.39463043212890625\n",
      "Eval loss 0.014737049117684364, R2 0.432344913482666\n",
      "epoch 1301, loss 0.014562218450009823, R2 0.39470595121383667\n",
      "Eval loss 0.014735114760696888, R2 0.4324185848236084\n",
      "epoch 1302, loss 0.01456039771437645, R2 0.3947816491127014\n",
      "Eval loss 0.014733181335031986, R2 0.43249356746673584\n",
      "epoch 1303, loss 0.014558582566678524, R2 0.3948572874069214\n",
      "Eval loss 0.014731253497302532, R2 0.4325683116912842\n",
      "epoch 1304, loss 0.014556768350303173, R2 0.3949331045150757\n",
      "Eval loss 0.014729326590895653, R2 0.43264156579971313\n",
      "epoch 1305, loss 0.014554955996572971, R2 0.395008385181427\n",
      "Eval loss 0.014727403409779072, R2 0.4327159523963928\n",
      "epoch 1306, loss 0.014553148299455643, R2 0.39508354663848877\n",
      "Eval loss 0.01472548395395279, R2 0.43278956413269043\n",
      "epoch 1307, loss 0.014551342464983463, R2 0.39515864849090576\n",
      "Eval loss 0.014723566360771656, R2 0.4328637719154358\n",
      "epoch 1308, loss 0.014549538493156433, R2 0.39523303508758545\n",
      "Eval loss 0.014721652492880821, R2 0.43293750286102295\n",
      "epoch 1309, loss 0.014547739177942276, R2 0.39530783891677856\n",
      "Eval loss 0.014719739556312561, R2 0.43301111459732056\n",
      "epoch 1310, loss 0.014545940794050694, R2 0.39538276195526123\n",
      "Eval loss 0.0147178303450346, R2 0.4330843687057495\n",
      "epoch 1311, loss 0.014544145204126835, R2 0.395457923412323\n",
      "Eval loss 0.014715924859046936, R2 0.43315809965133667\n",
      "epoch 1312, loss 0.0145423524081707, R2 0.39553189277648926\n",
      "Eval loss 0.014714021235704422, R2 0.4332318902015686\n",
      "epoch 1313, loss 0.014540562406182289, R2 0.39560627937316895\n",
      "Eval loss 0.014712121337652206, R2 0.43330442905426025\n",
      "epoch 1314, loss 0.014538776129484177, R2 0.3956805467605591\n",
      "Eval loss 0.01471022330224514, R2 0.43337738513946533\n",
      "epoch 1315, loss 0.014536992646753788, R2 0.39575427770614624\n",
      "Eval loss 0.014708327129483223, R2 0.43345075845718384\n",
      "epoch 1316, loss 0.014535211026668549, R2 0.3958289623260498\n",
      "Eval loss 0.014706436544656754, R2 0.43352359533309937\n",
      "epoch 1317, loss 0.014533431269228458, R2 0.3959023356437683\n",
      "Eval loss 0.014704544097185135, R2 0.433596134185791\n",
      "epoch 1318, loss 0.014531653374433517, R2 0.3959771990776062\n",
      "Eval loss 0.014702660031616688, R2 0.4336690306663513\n",
      "epoch 1319, loss 0.01452988013625145, R2 0.3960508704185486\n",
      "Eval loss 0.014700775034725666, R2 0.43374180793762207\n",
      "epoch 1320, loss 0.014528109692037106, R2 0.3961239457130432\n",
      "Eval loss 0.014698891900479794, R2 0.4338141679763794\n",
      "epoch 1321, loss 0.014526338316500187, R2 0.3961975574493408\n",
      "Eval loss 0.014697013422846794, R2 0.4338868260383606\n",
      "epoch 1322, loss 0.014524574391543865, R2 0.3962706923484802\n",
      "Eval loss 0.014695139601826668, R2 0.4339587092399597\n",
      "epoch 1323, loss 0.014522811397910118, R2 0.3963441848754883\n",
      "Eval loss 0.014693264849483967, R2 0.4340308904647827\n",
      "epoch 1324, loss 0.014521047472953796, R2 0.396418035030365\n",
      "Eval loss 0.014691393822431564, R2 0.43410301208496094\n",
      "epoch 1325, loss 0.014519290998578072, R2 0.39649027585983276\n",
      "Eval loss 0.01468952652066946, R2 0.43417489528656006\n",
      "epoch 1326, loss 0.014517535455524921, R2 0.3965632915496826\n",
      "Eval loss 0.014687659218907356, R2 0.4342470169067383\n",
      "epoch 1327, loss 0.01451578177511692, R2 0.3966361880302429\n",
      "Eval loss 0.014685796573758125, R2 0.4343186020851135\n",
      "epoch 1328, loss 0.014514029957354069, R2 0.3967095613479614\n",
      "Eval loss 0.014683935791254044, R2 0.43439042568206787\n",
      "epoch 1329, loss 0.014512283727526665, R2 0.3967817425727844\n",
      "Eval loss 0.014682079665362835, R2 0.43446147441864014\n",
      "epoch 1330, loss 0.014510535635054111, R2 0.3968542218208313\n",
      "Eval loss 0.014680223539471626, R2 0.43453341722488403\n",
      "epoch 1331, loss 0.01450879406183958, R2 0.3969266414642334\n",
      "Eval loss 0.014678371138870716, R2 0.434604287147522\n",
      "epoch 1332, loss 0.014507051557302475, R2 0.39699864387512207\n",
      "Eval loss 0.01467651966959238, R2 0.43467646837234497\n",
      "epoch 1333, loss 0.014505312778055668, R2 0.39707130193710327\n",
      "Eval loss 0.014674673788249493, R2 0.4347468614578247\n",
      "epoch 1334, loss 0.014503579586744308, R2 0.3971439599990845\n",
      "Eval loss 0.014672830700874329, R2 0.4348180294036865\n",
      "epoch 1335, loss 0.014501843601465225, R2 0.3972155451774597\n",
      "Eval loss 0.01467098854482174, R2 0.43488919734954834\n",
      "epoch 1336, loss 0.01450011320412159, R2 0.39728742837905884\n",
      "Eval loss 0.014669148251414299, R2 0.43496036529541016\n",
      "epoch 1337, loss 0.014498385600745678, R2 0.3973592519760132\n",
      "Eval loss 0.014667311683297157, R2 0.435030460357666\n",
      "epoch 1338, loss 0.01449665892869234, R2 0.3974313735961914\n",
      "Eval loss 0.014665476977825165, R2 0.43510162830352783\n",
      "epoch 1339, loss 0.014494935050606728, R2 0.3975026607513428\n",
      "Eval loss 0.014663646928966045, R2 0.43517178297042847\n",
      "epoch 1340, loss 0.014493214897811413, R2 0.39757418632507324\n",
      "Eval loss 0.014661815017461777, R2 0.43524235486984253\n",
      "epoch 1341, loss 0.014491495676338673, R2 0.39764559268951416\n",
      "Eval loss 0.014659988693892956, R2 0.4353122115135193\n",
      "epoch 1342, loss 0.014489780180156231, R2 0.39771711826324463\n",
      "Eval loss 0.01465816330164671, R2 0.4353832006454468\n",
      "epoch 1343, loss 0.014488065615296364, R2 0.39778822660446167\n",
      "Eval loss 0.01465634349733591, R2 0.4354531168937683\n",
      "epoch 1344, loss 0.014486354775726795, R2 0.3978593349456787\n",
      "Eval loss 0.014654523693025112, R2 0.4355233311653137\n",
      "epoch 1345, loss 0.014484645798802376, R2 0.397930383682251\n",
      "Eval loss 0.014652705751359463, R2 0.4355933666229248\n",
      "epoch 1346, loss 0.014482938684523106, R2 0.3980013132095337\n",
      "Eval loss 0.014650891534984112, R2 0.43566328287124634\n",
      "epoch 1347, loss 0.014481235295534134, R2 0.39807212352752686\n",
      "Eval loss 0.014649080112576485, R2 0.43573254346847534\n",
      "epoch 1348, loss 0.014479533769190311, R2 0.3981434106826782\n",
      "Eval loss 0.014647270552814007, R2 0.4358029365539551\n",
      "epoch 1349, loss 0.014477833174169064, R2 0.39821356534957886\n",
      "Eval loss 0.014645463787019253, R2 0.43587249517440796\n",
      "epoch 1350, loss 0.014476136304438114, R2 0.39828425645828247\n",
      "Eval loss 0.014643657952547073, R2 0.4359413981437683\n",
      "epoch 1351, loss 0.01447444036602974, R2 0.39835453033447266\n",
      "Eval loss 0.014641855843365192, R2 0.4360111355781555\n",
      "epoch 1352, loss 0.014472750015556812, R2 0.39842504262924194\n",
      "Eval loss 0.014640056528151035, R2 0.43608009815216064\n",
      "epoch 1353, loss 0.014471059665083885, R2 0.398495078086853\n",
      "Eval loss 0.014638259075582027, R2 0.4361500144004822\n",
      "epoch 1354, loss 0.014469372108578682, R2 0.39856523275375366\n",
      "Eval loss 0.014636464416980743, R2 0.43621885776519775\n",
      "epoch 1355, loss 0.014467686414718628, R2 0.39863526821136475\n",
      "Eval loss 0.014634674414992332, R2 0.43628793954849243\n",
      "epoch 1356, loss 0.014466002583503723, R2 0.39870530366897583\n",
      "Eval loss 0.014632881619036198, R2 0.43635666370391846\n",
      "epoch 1357, loss 0.014464323408901691, R2 0.39877527952194214\n",
      "Eval loss 0.01463109441101551, R2 0.43642550706863403\n",
      "epoch 1358, loss 0.01446264237165451, R2 0.39884495735168457\n",
      "Eval loss 0.014629309996962547, R2 0.4364945888519287\n",
      "epoch 1359, loss 0.014460967853665352, R2 0.3989143371582031\n",
      "Eval loss 0.014627527445554733, R2 0.43656325340270996\n",
      "epoch 1360, loss 0.014459294266998768, R2 0.39898431301116943\n",
      "Eval loss 0.01462574489414692, R2 0.43663156032562256\n",
      "epoch 1361, loss 0.014457620680332184, R2 0.39905351400375366\n",
      "Eval loss 0.014623967930674553, R2 0.43670034408569336\n",
      "epoch 1362, loss 0.014455951750278473, R2 0.39912325143814087\n",
      "Eval loss 0.014622190967202187, R2 0.4367685914039612\n",
      "epoch 1363, loss 0.014454285614192486, R2 0.39919209480285645\n",
      "Eval loss 0.014620417729020119, R2 0.43683677911758423\n",
      "epoch 1364, loss 0.014452620409429073, R2 0.3992615342140198\n",
      "Eval loss 0.0146186463534832, R2 0.4369053244590759\n",
      "epoch 1365, loss 0.014450957998633385, R2 0.3993307948112488\n",
      "Eval loss 0.014616877771914005, R2 0.436972975730896\n",
      "epoch 1366, loss 0.01444929651916027, R2 0.39939969778060913\n",
      "Eval loss 0.014615111984312534, R2 0.4370417594909668\n",
      "epoch 1367, loss 0.014447640627622604, R2 0.3994685411453247\n",
      "Eval loss 0.014613348059356213, R2 0.43710923194885254\n",
      "epoch 1368, loss 0.014445984736084938, R2 0.39953750371932983\n",
      "Eval loss 0.014611586928367615, R2 0.43717676401138306\n",
      "epoch 1369, loss 0.014444328844547272, R2 0.39960670471191406\n",
      "Eval loss 0.014609826728701591, R2 0.4372450113296509\n",
      "epoch 1370, loss 0.014442678540945053, R2 0.3996749520301819\n",
      "Eval loss 0.014608068391680717, R2 0.43731260299682617\n",
      "epoch 1371, loss 0.014441031031310558, R2 0.3997432589530945\n",
      "Eval loss 0.014606313779950142, R2 0.43738019466400146\n",
      "epoch 1372, loss 0.014439383521676064, R2 0.3998126983642578\n",
      "Eval loss 0.014604560099542141, R2 0.4374474287033081\n",
      "epoch 1373, loss 0.014437737874686718, R2 0.39988088607788086\n",
      "Eval loss 0.014602811075747013, R2 0.4375154376029968\n",
      "epoch 1374, loss 0.014436095021665096, R2 0.3999484181404114\n",
      "Eval loss 0.01460106298327446, R2 0.4375821352005005\n",
      "epoch 1375, loss 0.014434456825256348, R2 0.400016725063324\n",
      "Eval loss 0.014599316753447056, R2 0.4376491904258728\n",
      "epoch 1376, loss 0.014432817697525024, R2 0.40008485317230225\n",
      "Eval loss 0.014597572386264801, R2 0.4377167224884033\n",
      "epoch 1377, loss 0.01443118043243885, R2 0.4001534581184387\n",
      "Eval loss 0.014595831744372845, R2 0.43778425455093384\n",
      "epoch 1378, loss 0.014429546892642975, R2 0.4002205729484558\n",
      "Eval loss 0.014594092965126038, R2 0.43785107135772705\n",
      "epoch 1379, loss 0.014427914284169674, R2 0.400288462638855\n",
      "Eval loss 0.014592355117201805, R2 0.43791788816452026\n",
      "epoch 1380, loss 0.014426284469664097, R2 0.4003561735153198\n",
      "Eval loss 0.014590621925890446, R2 0.4379844665527344\n",
      "epoch 1381, loss 0.014424657449126244, R2 0.40042436122894287\n",
      "Eval loss 0.014588886871933937, R2 0.4380514621734619\n",
      "epoch 1382, loss 0.014423033222556114, R2 0.40049153566360474\n",
      "Eval loss 0.014587157405912876, R2 0.4381181001663208\n",
      "epoch 1383, loss 0.01442140992730856, R2 0.40055882930755615\n",
      "Eval loss 0.01458542887121439, R2 0.43818432092666626\n",
      "epoch 1384, loss 0.014419788494706154, R2 0.40062642097473145\n",
      "Eval loss 0.014583705924451351, R2 0.43825066089630127\n",
      "epoch 1385, loss 0.014418171718716621, R2 0.400693416595459\n",
      "Eval loss 0.014581980183720589, R2 0.4383171796798706\n",
      "epoch 1386, loss 0.014416552148759365, R2 0.40076112747192383\n",
      "Eval loss 0.0145802590996027, R2 0.4383837580680847\n",
      "epoch 1387, loss 0.014414939098060131, R2 0.40082836151123047\n",
      "Eval loss 0.014578538946807384, R2 0.43845003843307495\n",
      "epoch 1388, loss 0.014413325116038322, R2 0.4008948802947998\n",
      "Eval loss 0.014576822519302368, R2 0.4385164976119995\n",
      "epoch 1389, loss 0.014411715790629387, R2 0.4009617567062378\n",
      "Eval loss 0.014575108885765076, R2 0.4385817050933838\n",
      "epoch 1390, loss 0.0144101083278656, R2 0.401028573513031\n",
      "Eval loss 0.014573394320905209, R2 0.4386478662490845\n",
      "epoch 1391, loss 0.014408501796424389, R2 0.4010953903198242\n",
      "Eval loss 0.01457168348133564, R2 0.4387139081954956\n",
      "epoch 1392, loss 0.014406897127628326, R2 0.4011620283126831\n",
      "Eval loss 0.01456997450441122, R2 0.43877995014190674\n",
      "epoch 1393, loss 0.014405294321477413, R2 0.40122848749160767\n",
      "Eval loss 0.01456826739013195, R2 0.43884533643722534\n",
      "epoch 1394, loss 0.014403694309294224, R2 0.4012957811355591\n",
      "Eval loss 0.014566564932465553, R2 0.4389116168022156\n",
      "epoch 1395, loss 0.014402098022401333, R2 0.4013621211051941\n",
      "Eval loss 0.014564862474799156, R2 0.4389770030975342\n",
      "epoch 1396, loss 0.014400500804185867, R2 0.4014279246330261\n",
      "Eval loss 0.014563162811100483, R2 0.4390428066253662\n",
      "epoch 1397, loss 0.014398905448615551, R2 0.40149420499801636\n",
      "Eval loss 0.014561464078724384, R2 0.43910759687423706\n",
      "epoch 1398, loss 0.014397313818335533, R2 0.40156036615371704\n",
      "Eval loss 0.014559770002961159, R2 0.43917304277420044\n",
      "epoch 1399, loss 0.01439572498202324, R2 0.4016268253326416\n",
      "Eval loss 0.014558076858520508, R2 0.43923836946487427\n",
      "epoch 1400, loss 0.014394138008356094, R2 0.4016929864883423\n",
      "Eval loss 0.014556382782757282, R2 0.4393031597137451\n",
      "epoch 1401, loss 0.01439255103468895, R2 0.4017583727836609\n",
      "Eval loss 0.014554694294929504, R2 0.4393683671951294\n",
      "epoch 1402, loss 0.014390966854989529, R2 0.40182387828826904\n",
      "Eval loss 0.014553006738424301, R2 0.4394335150718689\n",
      "epoch 1403, loss 0.014389386400580406, R2 0.4018900990486145\n",
      "Eval loss 0.014551321975886822, R2 0.4394984245300293\n",
      "epoch 1404, loss 0.014387805946171284, R2 0.40195560455322266\n",
      "Eval loss 0.014549639075994492, R2 0.4395632743835449\n",
      "epoch 1405, loss 0.014386228285729885, R2 0.40202099084854126\n",
      "Eval loss 0.014547956176102161, R2 0.43962806463241577\n",
      "epoch 1406, loss 0.01438465341925621, R2 0.40208685398101807\n",
      "Eval loss 0.014546277932822704, R2 0.4396923780441284\n",
      "epoch 1407, loss 0.01438307948410511, R2 0.402152419090271\n",
      "Eval loss 0.014544601552188396, R2 0.4397572875022888\n",
      "epoch 1408, loss 0.01438150741159916, R2 0.40221738815307617\n",
      "Eval loss 0.014542926102876663, R2 0.43982183933258057\n",
      "epoch 1409, loss 0.014379936270415783, R2 0.40228271484375\n",
      "Eval loss 0.01454125251621008, R2 0.43988680839538574\n",
      "epoch 1410, loss 0.014378370717167854, R2 0.40234798192977905\n",
      "Eval loss 0.014539582654833794, R2 0.4399506449699402\n",
      "epoch 1411, loss 0.014376804232597351, R2 0.40241289138793945\n",
      "Eval loss 0.014537912793457508, R2 0.4400147795677185\n",
      "epoch 1412, loss 0.014375240541994572, R2 0.4024777412414551\n",
      "Eval loss 0.014536245726048946, R2 0.4400794506072998\n",
      "epoch 1413, loss 0.014373677782714367, R2 0.4025428295135498\n",
      "Eval loss 0.014534582383930683, R2 0.4401432275772095\n",
      "epoch 1414, loss 0.014372117817401886, R2 0.40260785818099976\n",
      "Eval loss 0.014532919973134995, R2 0.44020742177963257\n",
      "epoch 1415, loss 0.014370559714734554, R2 0.4026726484298706\n",
      "Eval loss 0.014531257562339306, R2 0.440271258354187\n",
      "epoch 1416, loss 0.014369006268680096, R2 0.402737021446228\n",
      "Eval loss 0.014529598876833916, R2 0.4403349757194519\n",
      "epoch 1417, loss 0.014367450028657913, R2 0.4028019309043884\n",
      "Eval loss 0.014527942053973675, R2 0.4403986930847168\n",
      "epoch 1418, loss 0.014365898445248604, R2 0.40286678075790405\n",
      "Eval loss 0.014526288956403732, R2 0.44046300649642944\n",
      "epoch 1419, loss 0.014364348724484444, R2 0.4029306173324585\n",
      "Eval loss 0.014524633064866066, R2 0.44052642583847046\n",
      "epoch 1420, loss 0.014362799935042858, R2 0.4029950499534607\n",
      "Eval loss 0.014522983692586422, R2 0.440589964389801\n",
      "epoch 1421, loss 0.014361253008246422, R2 0.40305912494659424\n",
      "Eval loss 0.014521335251629353, R2 0.44065314531326294\n",
      "epoch 1422, loss 0.014359707944095135, R2 0.40312355756759644\n",
      "Eval loss 0.014519687741994858, R2 0.4407169222831726\n",
      "epoch 1423, loss 0.014358165673911572, R2 0.4031878113746643\n",
      "Eval loss 0.014518043957650661, R2 0.44078028202056885\n",
      "epoch 1424, loss 0.014356625266373158, R2 0.40325164794921875\n",
      "Eval loss 0.014516400173306465, R2 0.44084322452545166\n",
      "epoch 1425, loss 0.014355085790157318, R2 0.4033156633377075\n",
      "Eval loss 0.014514759182929993, R2 0.4409067630767822\n",
      "epoch 1426, loss 0.014353549107909203, R2 0.4033800959587097\n",
      "Eval loss 0.014513119123876095, R2 0.44097012281417847\n",
      "epoch 1427, loss 0.014352013356983662, R2 0.4034441113471985\n",
      "Eval loss 0.014511480927467346, R2 0.4410330057144165\n",
      "epoch 1428, loss 0.01435048133134842, R2 0.403506875038147\n",
      "Eval loss 0.014509845525026321, R2 0.44109588861465454\n",
      "epoch 1429, loss 0.014348948374390602, R2 0.4035707712173462\n",
      "Eval loss 0.01450821291655302, R2 0.44115906953811646\n",
      "epoch 1430, loss 0.014347420074045658, R2 0.4036346673965454\n",
      "Eval loss 0.014506582170724869, R2 0.4412214159965515\n",
      "epoch 1431, loss 0.014345891773700714, R2 0.403698205947876\n",
      "Eval loss 0.014504952356219292, R2 0.4412843585014343\n",
      "epoch 1432, loss 0.014344367198646069, R2 0.40376120805740356\n",
      "Eval loss 0.014503324404358864, R2 0.4413473606109619\n",
      "epoch 1433, loss 0.014342842623591423, R2 0.4038251042366028\n",
      "Eval loss 0.014501701109111309, R2 0.44140976667404175\n",
      "epoch 1434, loss 0.014341321773827076, R2 0.4038883447647095\n",
      "Eval loss 0.014500074088573456, R2 0.4414721131324768\n",
      "epoch 1435, loss 0.01433979906141758, R2 0.40395087003707886\n",
      "Eval loss 0.0144984545186162, R2 0.4415348172187805\n",
      "epoch 1436, loss 0.014338282868266106, R2 0.40401411056518555\n",
      "Eval loss 0.014496834017336369, R2 0.4415974020957947\n",
      "epoch 1437, loss 0.014336765743792057, R2 0.4040771722793579\n",
      "Eval loss 0.014495217241346836, R2 0.44165968894958496\n",
      "epoch 1438, loss 0.014335250481963158, R2 0.4041401147842407\n",
      "Eval loss 0.014493599534034729, R2 0.44172149896621704\n",
      "epoch 1439, loss 0.014333738014101982, R2 0.40420299768447876\n",
      "Eval loss 0.01449198741465807, R2 0.441783607006073\n",
      "epoch 1440, loss 0.014332225546240807, R2 0.40426623821258545\n",
      "Eval loss 0.014490371569991112, R2 0.4418463110923767\n",
      "epoch 1441, loss 0.01433071680366993, R2 0.4043285846710205\n",
      "Eval loss 0.01448876317590475, R2 0.4419081211090088\n",
      "epoch 1442, loss 0.014329210855066776, R2 0.40439116954803467\n",
      "Eval loss 0.014487153850495815, R2 0.4419701099395752\n",
      "epoch 1443, loss 0.014327703975141048, R2 0.4044538140296936\n",
      "Eval loss 0.014485545456409454, R2 0.4420325756072998\n",
      "epoch 1444, loss 0.014326201751828194, R2 0.4045158624649048\n",
      "Eval loss 0.014483942650258541, R2 0.4420934319496155\n",
      "epoch 1445, loss 0.014324699528515339, R2 0.40457868576049805\n",
      "Eval loss 0.014482338912785053, R2 0.4421554207801819\n",
      "epoch 1446, loss 0.014323199167847633, R2 0.40464067459106445\n",
      "Eval loss 0.014480737037956715, R2 0.4422169327735901\n",
      "epoch 1447, loss 0.014321698807179928, R2 0.40470361709594727\n",
      "Eval loss 0.014479137025773525, R2 0.4422788619995117\n",
      "epoch 1448, loss 0.01432020403444767, R2 0.40476518869400024\n",
      "Eval loss 0.014477540738880634, R2 0.4423403739929199\n",
      "epoch 1449, loss 0.014318708330392838, R2 0.4048283100128174\n",
      "Eval loss 0.014475944451987743, R2 0.4424017071723938\n",
      "epoch 1450, loss 0.014317215420305729, R2 0.4048898220062256\n",
      "Eval loss 0.014474350027740002, R2 0.4424634575843811\n",
      "epoch 1451, loss 0.01431572437286377, R2 0.40495234727859497\n",
      "Eval loss 0.014472758397459984, R2 0.44252490997314453\n",
      "epoch 1452, loss 0.01431423332542181, R2 0.40501391887664795\n",
      "Eval loss 0.014471166767179966, R2 0.44258588552474976\n",
      "epoch 1453, loss 0.014312745071947575, R2 0.4050763249397278\n",
      "Eval loss 0.014469579793512821, R2 0.442646861076355\n",
      "epoch 1454, loss 0.014311259612441063, R2 0.4051373600959778\n",
      "Eval loss 0.014467992819845676, R2 0.4427078366279602\n",
      "epoch 1455, loss 0.0143097760155797, R2 0.40519899129867554\n",
      "Eval loss 0.014466407708823681, R2 0.44276905059814453\n",
      "epoch 1456, loss 0.014308294281363487, R2 0.4052608013153076\n",
      "Eval loss 0.014464824460446835, R2 0.44282984733581543\n",
      "epoch 1457, loss 0.014306813478469849, R2 0.4053221344947815\n",
      "Eval loss 0.014463243074715137, R2 0.4428912401199341\n",
      "epoch 1458, loss 0.01430533453822136, R2 0.40538346767425537\n",
      "Eval loss 0.01446166355162859, R2 0.44295215606689453\n",
      "epoch 1459, loss 0.014303854666650295, R2 0.4054449200630188\n",
      "Eval loss 0.014460086822509766, R2 0.4430123567581177\n",
      "epoch 1460, loss 0.014302381314337254, R2 0.4055063724517822\n",
      "Eval loss 0.014458510093390942, R2 0.4430732727050781\n",
      "epoch 1461, loss 0.014300906099379063, R2 0.4055677056312561\n",
      "Eval loss 0.014456936158239841, R2 0.44313424825668335\n",
      "epoch 1462, loss 0.014299432747066021, R2 0.40562891960144043\n",
      "Eval loss 0.014455363154411316, R2 0.4431946277618408\n",
      "epoch 1463, loss 0.014297964051365852, R2 0.4056905508041382\n",
      "Eval loss 0.01445379201322794, R2 0.4432551860809326\n",
      "epoch 1464, loss 0.014296494424343109, R2 0.4057510495185852\n",
      "Eval loss 0.014452222734689713, R2 0.44331562519073486\n",
      "epoch 1465, loss 0.01429502572864294, R2 0.40581172704696655\n",
      "Eval loss 0.01445065625011921, R2 0.4433756470680237\n",
      "epoch 1466, loss 0.014293561689555645, R2 0.4058729410171509\n",
      "Eval loss 0.014449091628193855, R2 0.44343602657318115\n",
      "epoch 1467, loss 0.014292098581790924, R2 0.4059339761734009\n",
      "Eval loss 0.01444752886891365, R2 0.44349610805511475\n",
      "epoch 1468, loss 0.014290636405348778, R2 0.4059947729110718\n",
      "Eval loss 0.014445965178310871, R2 0.44355666637420654\n",
      "epoch 1469, loss 0.014289177022874355, R2 0.406055212020874\n",
      "Eval loss 0.01444440521299839, R2 0.4436163902282715\n",
      "epoch 1470, loss 0.014287716709077358, R2 0.40611588954925537\n",
      "Eval loss 0.014442848041653633, R2 0.4436767101287842\n",
      "epoch 1471, loss 0.014286261051893234, R2 0.4061766266822815\n",
      "Eval loss 0.014441290870308876, R2 0.44373655319213867\n",
      "epoch 1472, loss 0.01428480539470911, R2 0.4062369465827942\n",
      "Eval loss 0.014439733698964119, R2 0.4437963366508484\n",
      "epoch 1473, loss 0.014283349737524986, R2 0.40629762411117554\n",
      "Eval loss 0.01443818025290966, R2 0.44385671615600586\n",
      "epoch 1474, loss 0.01428189966827631, R2 0.40635770559310913\n",
      "Eval loss 0.0144366305321455, R2 0.443916380405426\n",
      "epoch 1475, loss 0.014280447736382484, R2 0.4064182639122009\n",
      "Eval loss 0.01443508081138134, R2 0.44397544860839844\n",
      "epoch 1476, loss 0.014279000461101532, R2 0.40647822618484497\n",
      "Eval loss 0.014433532953262329, R2 0.44403523206710815\n",
      "epoch 1477, loss 0.01427755132317543, R2 0.4065384268760681\n",
      "Eval loss 0.014431986957788467, R2 0.444095253944397\n",
      "epoch 1478, loss 0.014276107773184776, R2 0.40659862756729126\n",
      "Eval loss 0.014430442824959755, R2 0.44415438175201416\n",
      "epoch 1479, loss 0.014274664223194122, R2 0.40665924549102783\n",
      "Eval loss 0.014428896829485893, R2 0.4442141056060791\n",
      "epoch 1480, loss 0.014273220673203468, R2 0.4067184329032898\n",
      "Eval loss 0.014427357353270054, R2 0.44427353143692017\n",
      "epoch 1481, loss 0.014271783642470837, R2 0.4067782163619995\n",
      "Eval loss 0.014425817877054214, R2 0.4443323612213135\n",
      "epoch 1482, loss 0.014270341955125332, R2 0.4068382978439331\n",
      "Eval loss 0.014424280263483524, R2 0.4443916082382202\n",
      "epoch 1483, loss 0.01426890678703785, R2 0.40689796209335327\n",
      "Eval loss 0.014422742649912834, R2 0.44445115327835083\n",
      "epoch 1484, loss 0.014267470687627792, R2 0.4069575071334839\n",
      "Eval loss 0.014421207830309868, R2 0.44451045989990234\n",
      "epoch 1485, loss 0.014266034588217735, R2 0.40701717138290405\n",
      "Eval loss 0.014419675804674625, R2 0.44456911087036133\n",
      "epoch 1486, loss 0.0142646050080657, R2 0.4070764183998108\n",
      "Eval loss 0.014418141916394234, R2 0.4446285367012024\n",
      "epoch 1487, loss 0.014263171702623367, R2 0.4071364998817444\n",
      "Eval loss 0.014416614547371864, R2 0.4446868896484375\n",
      "epoch 1488, loss 0.014261743985116482, R2 0.40719568729400635\n",
      "Eval loss 0.01441508624702692, R2 0.4447462558746338\n",
      "epoch 1489, loss 0.014260316267609596, R2 0.4072554111480713\n",
      "Eval loss 0.014413558878004551, R2 0.44480520486831665\n",
      "epoch 1490, loss 0.01425889041274786, R2 0.40731412172317505\n",
      "Eval loss 0.01441203523427248, R2 0.4448637366294861\n",
      "epoch 1491, loss 0.014257465489208698, R2 0.40737318992614746\n",
      "Eval loss 0.014410512521862984, R2 0.4449225664138794\n",
      "epoch 1492, loss 0.01425604335963726, R2 0.4074326157569885\n",
      "Eval loss 0.014408990740776062, R2 0.44498103857040405\n",
      "epoch 1493, loss 0.014254623092710972, R2 0.4074920415878296\n",
      "Eval loss 0.014407471753656864, R2 0.44503921270370483\n",
      "epoch 1494, loss 0.014253202825784683, R2 0.407550573348999\n",
      "Eval loss 0.014405953697860241, R2 0.4450978636741638\n",
      "epoch 1495, loss 0.014251784421503544, R2 0.4076097011566162\n",
      "Eval loss 0.014404437504708767, R2 0.4451565742492676\n",
      "epoch 1496, loss 0.014250367879867554, R2 0.40766894817352295\n",
      "Eval loss 0.014402920380234718, R2 0.44521480798721313\n",
      "epoch 1497, loss 0.014248953200876713, R2 0.4077271819114685\n",
      "Eval loss 0.014401408843696117, R2 0.445273220539093\n",
      "epoch 1498, loss 0.014247541315853596, R2 0.4077858328819275\n",
      "Eval loss 0.014399896375834942, R2 0.4453308582305908\n",
      "epoch 1499, loss 0.014246128499507904, R2 0.407844603061676\n",
      "Eval loss 0.01439838670194149, R2 0.4453889727592468\n",
      "epoch 1500, loss 0.014244718477129936, R2 0.4079035520553589\n",
      "Eval loss 0.014396877959370613, R2 0.44544756412506104\n",
      "epoch 1501, loss 0.014243310317397118, R2 0.40796172618865967\n",
      "Eval loss 0.014395371079444885, R2 0.44550514221191406\n",
      "epoch 1502, loss 0.014241902157664299, R2 0.4080209732055664\n",
      "Eval loss 0.014393865130841732, R2 0.4455631375312805\n",
      "epoch 1503, loss 0.014240496791899204, R2 0.408078670501709\n",
      "Eval loss 0.014392362907528877, R2 0.4456213712692261\n",
      "epoch 1504, loss 0.014239094220101833, R2 0.4081372022628784\n",
      "Eval loss 0.014390860684216022, R2 0.445679247379303\n",
      "epoch 1505, loss 0.014237691648304462, R2 0.408195436000824\n",
      "Eval loss 0.014389360323548317, R2 0.44573718309402466\n",
      "epoch 1506, loss 0.01423629093915224, R2 0.4082534909248352\n",
      "Eval loss 0.014387860894203186, R2 0.4457947611808777\n",
      "epoch 1507, loss 0.014234891161322594, R2 0.4083118438720703\n",
      "Eval loss 0.014386364258825779, R2 0.4458523988723755\n",
      "epoch 1508, loss 0.014233493246138096, R2 0.408369779586792\n",
      "Eval loss 0.014384868554770947, R2 0.44590967893600464\n",
      "epoch 1509, loss 0.014232096262276173, R2 0.4084285497665405\n",
      "Eval loss 0.014383373782038689, R2 0.445967435836792\n",
      "epoch 1510, loss 0.014230702072381973, R2 0.40848618745803833\n",
      "Eval loss 0.01438188087195158, R2 0.4460252523422241\n",
      "epoch 1511, loss 0.014229308813810349, R2 0.4085438847541809\n",
      "Eval loss 0.01438039168715477, R2 0.44608229398727417\n",
      "epoch 1512, loss 0.014227918349206448, R2 0.4086012840270996\n",
      "Eval loss 0.01437890063971281, R2 0.446139931678772\n",
      "epoch 1513, loss 0.014226526953279972, R2 0.40865933895111084\n",
      "Eval loss 0.014377414248883724, R2 0.44619715213775635\n",
      "epoch 1514, loss 0.014225139282643795, R2 0.40871697664260864\n",
      "Eval loss 0.014375927858054638, R2 0.4462549090385437\n",
      "epoch 1515, loss 0.014223752543330193, R2 0.4087752103805542\n",
      "Eval loss 0.014374444261193275, R2 0.446311891078949\n",
      "epoch 1516, loss 0.014222366735339165, R2 0.40883225202560425\n",
      "Eval loss 0.01437295787036419, R2 0.446368932723999\n",
      "epoch 1517, loss 0.014220981858670712, R2 0.4088892936706543\n",
      "Eval loss 0.014371477998793125, R2 0.44642579555511475\n",
      "epoch 1518, loss 0.014219598844647408, R2 0.40894728899002075\n",
      "Eval loss 0.014369999058544636, R2 0.44648295640945435\n",
      "epoch 1519, loss 0.014218216761946678, R2 0.40900474786758423\n",
      "Eval loss 0.014368521049618721, R2 0.44653987884521484\n",
      "epoch 1520, loss 0.014216838404536247, R2 0.40906262397766113\n",
      "Eval loss 0.014367040246725082, R2 0.44659674167633057\n",
      "epoch 1521, loss 0.014215460047125816, R2 0.4091193675994873\n",
      "Eval loss 0.014365565031766891, R2 0.4466530680656433\n",
      "epoch 1522, loss 0.01421408448368311, R2 0.4091763496398926\n",
      "Eval loss 0.014364093542098999, R2 0.44671058654785156\n",
      "epoch 1523, loss 0.014212708920240402, R2 0.4092339277267456\n",
      "Eval loss 0.014362621121108532, R2 0.4467669725418091\n",
      "epoch 1524, loss 0.014211336150765419, R2 0.40929096937179565\n",
      "Eval loss 0.014361148700118065, R2 0.4468236565589905\n",
      "epoch 1525, loss 0.014209962449967861, R2 0.4093478322029114\n",
      "Eval loss 0.014359680004417896, R2 0.446880578994751\n",
      "epoch 1526, loss 0.014208591543138027, R2 0.4094048738479614\n",
      "Eval loss 0.014358213171362877, R2 0.4469367265701294\n",
      "epoch 1527, loss 0.014207221567630768, R2 0.4094617962837219\n",
      "Eval loss 0.014356746338307858, R2 0.4469931125640869\n",
      "epoch 1528, loss 0.014205854386091232, R2 0.40951859951019287\n",
      "Eval loss 0.014355280436575413, R2 0.44704973697662354\n",
      "epoch 1529, loss 0.014204489067196846, R2 0.4095754027366638\n",
      "Eval loss 0.014353816397488117, R2 0.4471058249473572\n",
      "epoch 1530, loss 0.01420312374830246, R2 0.40963226556777954\n",
      "Eval loss 0.01435235608369112, R2 0.4471626877784729\n",
      "epoch 1531, loss 0.014201760292053223, R2 0.40968894958496094\n",
      "Eval loss 0.014350894838571548, R2 0.447218656539917\n",
      "epoch 1532, loss 0.014200398698449135, R2 0.4097450375556946\n",
      "Eval loss 0.01434943825006485, R2 0.44727444648742676\n",
      "epoch 1533, loss 0.014199038967490196, R2 0.4098021388053894\n",
      "Eval loss 0.014347979798913002, R2 0.44733142852783203\n",
      "epoch 1534, loss 0.014197679236531258, R2 0.409858763217926\n",
      "Eval loss 0.014346523210406303, R2 0.44738703966140747\n",
      "epoch 1535, loss 0.014196319505572319, R2 0.4099149703979492\n",
      "Eval loss 0.014345068484544754, R2 0.44744306802749634\n",
      "epoch 1536, loss 0.014194965362548828, R2 0.4099712371826172\n",
      "Eval loss 0.014343616552650928, R2 0.4474988579750061\n",
      "epoch 1537, loss 0.014193608425557613, R2 0.4100276231765747\n",
      "Eval loss 0.014342165552079678, R2 0.44755488634109497\n",
      "epoch 1538, loss 0.014192257076501846, R2 0.4100845456123352\n",
      "Eval loss 0.014340712688863277, R2 0.4476110339164734\n",
      "epoch 1539, loss 0.014190902933478355, R2 0.41014009714126587\n",
      "Eval loss 0.0143392663449049, R2 0.4476667046546936\n",
      "epoch 1540, loss 0.014189555309712887, R2 0.4101966619491577\n",
      "Eval loss 0.014337820000946522, R2 0.4477219581604004\n",
      "epoch 1541, loss 0.014188204891979694, R2 0.4102524518966675\n",
      "Eval loss 0.014336374588310719, R2 0.44777780771255493\n",
      "epoch 1542, loss 0.014186856336891651, R2 0.410308837890625\n",
      "Eval loss 0.014334931038320065, R2 0.4478335380554199\n",
      "epoch 1543, loss 0.014185510575771332, R2 0.41036421060562134\n",
      "Eval loss 0.014333488419651985, R2 0.4478892683982849\n",
      "epoch 1544, loss 0.014184166677296162, R2 0.4104202389717102\n",
      "Eval loss 0.01433204673230648, R2 0.447944700717926\n",
      "epoch 1545, loss 0.014182822778820992, R2 0.4104759097099304\n",
      "Eval loss 0.014330608770251274, R2 0.44799989461898804\n",
      "epoch 1546, loss 0.01418148074299097, R2 0.41053158044815063\n",
      "Eval loss 0.014329168945550919, R2 0.44805532693862915\n",
      "epoch 1547, loss 0.014180139638483524, R2 0.4105876684188843\n",
      "Eval loss 0.014327731914818287, R2 0.4481111764907837\n",
      "epoch 1548, loss 0.014178799465298653, R2 0.41064316034317017\n",
      "Eval loss 0.014326297678053379, R2 0.44816577434539795\n",
      "epoch 1549, loss 0.01417746301740408, R2 0.41069912910461426\n",
      "Eval loss 0.014324861578643322, R2 0.44822144508361816\n",
      "epoch 1550, loss 0.014176124706864357, R2 0.41075438261032104\n",
      "Eval loss 0.014323431067168713, R2 0.44827622175216675\n",
      "epoch 1551, loss 0.014174791052937508, R2 0.41081053018569946\n",
      "Eval loss 0.014321999624371529, R2 0.44833165407180786\n",
      "epoch 1552, loss 0.014173457399010658, R2 0.41086524724960327\n",
      "Eval loss 0.014320570975542068, R2 0.448386549949646\n",
      "epoch 1553, loss 0.014172124676406384, R2 0.4109213948249817\n",
      "Eval loss 0.014319141395390034, R2 0.44844162464141846\n",
      "epoch 1554, loss 0.014170794747769833, R2 0.41097593307495117\n",
      "Eval loss 0.014317715540528297, R2 0.4484966993331909\n",
      "epoch 1555, loss 0.014169463887810707, R2 0.4110317826271057\n",
      "Eval loss 0.014316289685666561, R2 0.4485512971878052\n",
      "epoch 1556, loss 0.01416813675314188, R2 0.4110865592956543\n",
      "Eval loss 0.0143148647621274, R2 0.4486066699028015\n",
      "epoch 1557, loss 0.014166809618473053, R2 0.41114169359207153\n",
      "Eval loss 0.01431344449520111, R2 0.448661208152771\n",
      "epoch 1558, loss 0.0141654834151268, R2 0.41119664907455444\n",
      "Eval loss 0.014312021434307098, R2 0.4487156867980957\n",
      "epoch 1559, loss 0.014164160937070847, R2 0.41125184297561646\n",
      "Eval loss 0.014310602098703384, R2 0.44877082109451294\n",
      "epoch 1560, loss 0.014162835665047169, R2 0.4113067388534546\n",
      "Eval loss 0.01430918462574482, R2 0.4488254189491272\n",
      "epoch 1561, loss 0.014161515980958939, R2 0.4113617539405823\n",
      "Eval loss 0.014307769015431404, R2 0.4488799571990967\n",
      "epoch 1562, loss 0.014160198159515858, R2 0.41141635179519653\n",
      "Eval loss 0.014306352473795414, R2 0.4489343762397766\n",
      "epoch 1563, loss 0.014158876612782478, R2 0.4114718437194824\n",
      "Eval loss 0.014304938726127148, R2 0.4489888548851013\n",
      "epoch 1564, loss 0.014157561585307121, R2 0.41152578592300415\n",
      "Eval loss 0.014303524978458881, R2 0.44904327392578125\n",
      "epoch 1565, loss 0.014156244695186615, R2 0.41158074140548706\n",
      "Eval loss 0.014302113093435764, R2 0.44909751415252686\n",
      "epoch 1566, loss 0.014154929667711258, R2 0.4116353392601013\n",
      "Eval loss 0.014300704002380371, R2 0.4491519331932068\n",
      "epoch 1567, loss 0.01415361650288105, R2 0.4116901159286499\n",
      "Eval loss 0.014299295842647552, R2 0.4492063522338867\n",
      "epoch 1568, loss 0.014152305200695992, R2 0.41174501180648804\n",
      "Eval loss 0.014297889545559883, R2 0.4492606520652771\n",
      "epoch 1569, loss 0.014150992967188358, R2 0.4117991328239441\n",
      "Eval loss 0.014296484179794788, R2 0.4493143558502197\n",
      "epoch 1570, loss 0.014149684458971024, R2 0.4118533730506897\n",
      "Eval loss 0.014295077882707119, R2 0.449368953704834\n",
      "epoch 1571, loss 0.014148378744721413, R2 0.4119073152542114\n",
      "Eval loss 0.014293676242232323, R2 0.44942229986190796\n",
      "epoch 1572, loss 0.014147070236504078, R2 0.4119618535041809\n",
      "Eval loss 0.014292274601757526, R2 0.44947683811187744\n",
      "epoch 1573, loss 0.014145766384899616, R2 0.41201627254486084\n",
      "Eval loss 0.01429087482392788, R2 0.44953060150146484\n",
      "epoch 1574, loss 0.014144462533295155, R2 0.41207045316696167\n",
      "Eval loss 0.014289475977420807, R2 0.44958460330963135\n",
      "epoch 1575, loss 0.014143159613013268, R2 0.41212475299835205\n",
      "Eval loss 0.014288078993558884, R2 0.4496379494667053\n",
      "epoch 1576, loss 0.014141859486699104, R2 0.41217881441116333\n",
      "Eval loss 0.014286681078374386, R2 0.4496920704841614\n",
      "epoch 1577, loss 0.014140558429062366, R2 0.41223347187042236\n",
      "Eval loss 0.014285285025835037, R2 0.44974589347839355\n",
      "epoch 1578, loss 0.014139260165393353, R2 0.41228723526000977\n",
      "Eval loss 0.014283893629908562, R2 0.44979947805404663\n",
      "epoch 1579, loss 0.014137961901724339, R2 0.4123406410217285\n",
      "Eval loss 0.014282501302659512, R2 0.44985276460647583\n",
      "epoch 1580, loss 0.014136666432023048, R2 0.4123947024345398\n",
      "Eval loss 0.014281111769378185, R2 0.4499066472053528\n",
      "epoch 1581, loss 0.014135372824966908, R2 0.4124490022659302\n",
      "Eval loss 0.01427972037345171, R2 0.44996052980422974\n",
      "epoch 1582, loss 0.014134079217910767, R2 0.4125022292137146\n",
      "Eval loss 0.014278333634138107, R2 0.45001381635665894\n",
      "epoch 1583, loss 0.0141327865421772, R2 0.4125557541847229\n",
      "Eval loss 0.014276946894824505, R2 0.4500672221183777\n",
      "epoch 1584, loss 0.014131496660411358, R2 0.41260939836502075\n",
      "Eval loss 0.014275562018156052, R2 0.4501206874847412\n",
      "epoch 1585, loss 0.01413020584732294, R2 0.4126630425453186\n",
      "Eval loss 0.014274178072810173, R2 0.45017367601394653\n",
      "epoch 1586, loss 0.014128916896879673, R2 0.4127166271209717\n",
      "Eval loss 0.014272796921432018, R2 0.4502267837524414\n",
      "epoch 1587, loss 0.014127629809081554, R2 0.4127700924873352\n",
      "Eval loss 0.01427141297608614, R2 0.45028018951416016\n",
      "epoch 1588, loss 0.01412634551525116, R2 0.41282403469085693\n",
      "Eval loss 0.01427003275603056, R2 0.45033353567123413\n",
      "epoch 1589, loss 0.01412506029009819, R2 0.41287708282470703\n",
      "Eval loss 0.014268655329942703, R2 0.45038658380508423\n",
      "epoch 1590, loss 0.01412377879023552, R2 0.4129301905632019\n",
      "Eval loss 0.014267278835177422, R2 0.4504396319389343\n",
      "epoch 1591, loss 0.014122496359050274, R2 0.4129834771156311\n",
      "Eval loss 0.01426590047776699, R2 0.4504922032356262\n",
      "epoch 1592, loss 0.014121214859187603, R2 0.41303694248199463\n",
      "Eval loss 0.014264527708292007, R2 0.45054543018341064\n",
      "epoch 1593, loss 0.014119936153292656, R2 0.4130895137786865\n",
      "Eval loss 0.01426315400749445, R2 0.45059800148010254\n",
      "epoch 1594, loss 0.014118658378720284, R2 0.4131430387496948\n",
      "Eval loss 0.014261781238019466, R2 0.45065152645111084\n",
      "epoch 1595, loss 0.01411738246679306, R2 0.41319626569747925\n",
      "Eval loss 0.014260411262512207, R2 0.4507041573524475\n",
      "epoch 1596, loss 0.014116107486188412, R2 0.41324901580810547\n",
      "Eval loss 0.014259041287004948, R2 0.45075660943984985\n",
      "epoch 1597, loss 0.014114831574261189, R2 0.41330188512802124\n",
      "Eval loss 0.014257674105465412, R2 0.45080941915512085\n",
      "epoch 1598, loss 0.014113559387624264, R2 0.41335493326187134\n",
      "Eval loss 0.014256307855248451, R2 0.4508618712425232\n",
      "epoch 1599, loss 0.014112287200987339, R2 0.41340839862823486\n",
      "Eval loss 0.01425494160503149, R2 0.4509151577949524\n",
      "epoch 1600, loss 0.014111017808318138, R2 0.41346079111099243\n",
      "Eval loss 0.014253577217459679, R2 0.4509672522544861\n",
      "epoch 1601, loss 0.014109747484326363, R2 0.41351318359375\n",
      "Eval loss 0.01425221562385559, R2 0.4510197043418884\n",
      "epoch 1602, loss 0.014108480885624886, R2 0.41356605291366577\n",
      "Eval loss 0.01425085123628378, R2 0.45107221603393555\n",
      "epoch 1603, loss 0.014107214286923409, R2 0.4136194586753845\n",
      "Eval loss 0.014249492436647415, R2 0.4511244297027588\n",
      "epoch 1604, loss 0.014105947688221931, R2 0.4136715531349182\n",
      "Eval loss 0.014248133637011051, R2 0.45117706060409546\n",
      "epoch 1605, loss 0.014104684814810753, R2 0.4137241840362549\n",
      "Eval loss 0.014246775768697262, R2 0.45122891664505005\n",
      "epoch 1606, loss 0.014103421941399574, R2 0.41377687454223633\n",
      "Eval loss 0.014245418831706047, R2 0.45128148794174194\n",
      "epoch 1607, loss 0.014102159067988396, R2 0.41382938623428345\n",
      "Eval loss 0.014244063757359982, R2 0.4513338804244995\n",
      "epoch 1608, loss 0.014100899919867516, R2 0.41388100385665894\n",
      "Eval loss 0.01424271147698164, R2 0.4513857960700989\n",
      "epoch 1609, loss 0.014099640771746635, R2 0.41393351554870605\n",
      "Eval loss 0.014241360127925873, R2 0.4514380097389221\n",
      "epoch 1610, loss 0.01409838255494833, R2 0.4139857888221741\n",
      "Eval loss 0.014240005053579807, R2 0.4514903426170349\n",
      "epoch 1611, loss 0.014097126200795174, R2 0.41403764486312866\n",
      "Eval loss 0.014238656498491764, R2 0.4515419602394104\n",
      "epoch 1612, loss 0.014095868915319443, R2 0.4140908122062683\n",
      "Eval loss 0.014237308874726295, R2 0.45159369707107544\n",
      "epoch 1613, loss 0.014094616286456585, R2 0.414142906665802\n",
      "Eval loss 0.014235960319638252, R2 0.45164597034454346\n",
      "epoch 1614, loss 0.014093363657593727, R2 0.41419440507888794\n",
      "Eval loss 0.014234615489840508, R2 0.45169728994369507\n",
      "epoch 1615, loss 0.014092110097408295, R2 0.4142465591430664\n",
      "Eval loss 0.014233267866075039, R2 0.4517490267753601\n",
      "epoch 1616, loss 0.01409086026251316, R2 0.4142986536026001\n",
      "Eval loss 0.014231925830245018, R2 0.4518013596534729\n",
      "epoch 1617, loss 0.014089611358940601, R2 0.41435056924819946\n",
      "Eval loss 0.014230581000447273, R2 0.45185303688049316\n",
      "epoch 1618, loss 0.014088363386690617, R2 0.4144027829170227\n",
      "Eval loss 0.01422924268990755, R2 0.4519042372703552\n",
      "epoch 1619, loss 0.014087115414440632, R2 0.4144541621208191\n",
      "Eval loss 0.01422790065407753, R2 0.45195645093917847\n",
      "epoch 1620, loss 0.014085870236158371, R2 0.4145064353942871\n",
      "Eval loss 0.014226560480892658, R2 0.4520081877708435\n",
      "epoch 1621, loss 0.01408462505787611, R2 0.4145578145980835\n",
      "Eval loss 0.014225225895643234, R2 0.45205944776535034\n",
      "epoch 1622, loss 0.014083382673561573, R2 0.41460931301116943\n",
      "Eval loss 0.014223889447748661, R2 0.45211076736450195\n",
      "epoch 1623, loss 0.014082140289247036, R2 0.4146609306335449\n",
      "Eval loss 0.014222553931176662, R2 0.45216190814971924\n",
      "epoch 1624, loss 0.014080898836255074, R2 0.414712131023407\n",
      "Eval loss 0.014221219345927238, R2 0.45221394300460815\n",
      "epoch 1625, loss 0.01407965924590826, R2 0.414764404296875\n",
      "Eval loss 0.014219887554645538, R2 0.45226508378982544\n",
      "epoch 1626, loss 0.014078421518206596, R2 0.4148154854774475\n",
      "Eval loss 0.014218557626008987, R2 0.4523163437843323\n",
      "epoch 1627, loss 0.014077182859182358, R2 0.41486698389053345\n",
      "Eval loss 0.014217225834727287, R2 0.45236778259277344\n",
      "epoch 1628, loss 0.014075946994125843, R2 0.41491907835006714\n",
      "Eval loss 0.014215897768735886, R2 0.4524187445640564\n",
      "epoch 1629, loss 0.014074711129069328, R2 0.4149697422981262\n",
      "Eval loss 0.01421456877142191, R2 0.45246946811676025\n",
      "epoch 1630, loss 0.014073478989303112, R2 0.4150209426879883\n",
      "Eval loss 0.014213243499398232, R2 0.45252084732055664\n",
      "epoch 1631, loss 0.014072245918214321, R2 0.4150720238685608\n",
      "Eval loss 0.014211917296051979, R2 0.4525716304779053\n",
      "epoch 1632, loss 0.01407101284712553, R2 0.4151236414909363\n",
      "Eval loss 0.014210592955350876, R2 0.45262306928634644\n",
      "epoch 1633, loss 0.014069784432649612, R2 0.41517454385757446\n",
      "Eval loss 0.014209272339940071, R2 0.4526737928390503\n",
      "epoch 1634, loss 0.01406855508685112, R2 0.4152263402938843\n",
      "Eval loss 0.014207950793206692, R2 0.4527243971824646\n",
      "epoch 1635, loss 0.014067327603697777, R2 0.41527682542800903\n",
      "Eval loss 0.014206631109118462, R2 0.4527755379676819\n",
      "epoch 1636, loss 0.014066100120544434, R2 0.41532766819000244\n",
      "Eval loss 0.014205310493707657, R2 0.45282673835754395\n",
      "epoch 1637, loss 0.01406487450003624, R2 0.41537898778915405\n",
      "Eval loss 0.01420399360358715, R2 0.45287710428237915\n",
      "epoch 1638, loss 0.014063650742173195, R2 0.41542965173721313\n",
      "Eval loss 0.01420267578214407, R2 0.45292770862579346\n",
      "epoch 1639, loss 0.01406242698431015, R2 0.415480375289917\n",
      "Eval loss 0.014201361685991287, R2 0.4529781937599182\n",
      "epoch 1640, loss 0.014061205089092255, R2 0.41553133726119995\n",
      "Eval loss 0.014200047589838505, R2 0.4530296325683594\n",
      "epoch 1641, loss 0.014059985056519508, R2 0.4155818223953247\n",
      "Eval loss 0.014198733493685722, R2 0.45307958126068115\n",
      "epoch 1642, loss 0.014058763161301613, R2 0.4156322479248047\n",
      "Eval loss 0.014197422191500664, R2 0.45312994718551636\n",
      "epoch 1643, loss 0.014057544991374016, R2 0.41568344831466675\n",
      "Eval loss 0.01419611182063818, R2 0.453180730342865\n",
      "epoch 1644, loss 0.014056327752768993, R2 0.4157344102859497\n",
      "Eval loss 0.014194801449775696, R2 0.45323091745376587\n",
      "epoch 1645, loss 0.01405511237680912, R2 0.41578489542007446\n",
      "Eval loss 0.014193493872880936, R2 0.4532812833786011\n",
      "epoch 1646, loss 0.014053897000849247, R2 0.415834903717041\n",
      "Eval loss 0.014192188158631325, R2 0.4533318877220154\n",
      "epoch 1647, loss 0.014052682556211948, R2 0.4158855676651001\n",
      "Eval loss 0.01419088151305914, R2 0.45338189601898193\n",
      "epoch 1648, loss 0.014051469974219799, R2 0.415935754776001\n",
      "Eval loss 0.014189575798809528, R2 0.45343250036239624\n",
      "epoch 1649, loss 0.014050258323550224, R2 0.4159862995147705\n",
      "Eval loss 0.014188272878527641, R2 0.4534825086593628\n",
      "epoch 1650, loss 0.014049047604203224, R2 0.4160364866256714\n",
      "Eval loss 0.014186971820890903, R2 0.4535328149795532\n",
      "epoch 1651, loss 0.014047837816178799, R2 0.41608673334121704\n",
      "Eval loss 0.014185669831931591, R2 0.453582763671875\n",
      "epoch 1652, loss 0.014046629890799522, R2 0.416137158870697\n",
      "Eval loss 0.014184369705617428, R2 0.45363301038742065\n",
      "epoch 1653, loss 0.01404542289674282, R2 0.41618770360946655\n",
      "Eval loss 0.01418307051062584, R2 0.45368289947509766\n",
      "epoch 1654, loss 0.014044216834008694, R2 0.4162370562553406\n",
      "Eval loss 0.014181774109601974, R2 0.45373302698135376\n",
      "epoch 1655, loss 0.014043012633919716, R2 0.41628730297088623\n",
      "Eval loss 0.014180476777255535, R2 0.4537826180458069\n",
      "epoch 1656, loss 0.014041808433830738, R2 0.4163375496864319\n",
      "Eval loss 0.014179183170199394, R2 0.45383280515670776\n",
      "epoch 1657, loss 0.014040605165064335, R2 0.4163873791694641\n",
      "Eval loss 0.014177887700498104, R2 0.45388269424438477\n",
      "epoch 1658, loss 0.014039404690265656, R2 0.4164370894432068\n",
      "Eval loss 0.014176595956087112, R2 0.4539324641227722\n",
      "epoch 1659, loss 0.014038204215466976, R2 0.4164871573448181\n",
      "Eval loss 0.01417530421167612, R2 0.4539823532104492\n",
      "epoch 1660, loss 0.014037004671990871, R2 0.41653722524642944\n",
      "Eval loss 0.014174013398587704, R2 0.45403146743774414\n",
      "epoch 1661, loss 0.014035805128514767, R2 0.4165869355201721\n",
      "Eval loss 0.014172724448144436, R2 0.4540812373161316\n",
      "epoch 1662, loss 0.01403461117297411, R2 0.41663670539855957\n",
      "Eval loss 0.014171436429023743, R2 0.4541313648223877\n",
      "epoch 1663, loss 0.014033414423465729, R2 0.4166862368583679\n",
      "Eval loss 0.01417014840990305, R2 0.4541807770729065\n",
      "epoch 1664, loss 0.014032217673957348, R2 0.4167367219924927\n",
      "Eval loss 0.01416886318475008, R2 0.4542301297187805\n",
      "epoch 1665, loss 0.014031026512384415, R2 0.41678571701049805\n",
      "Eval loss 0.014167578890919685, R2 0.45427995920181274\n",
      "epoch 1666, loss 0.014029832556843758, R2 0.4168349504470825\n",
      "Eval loss 0.01416629459708929, R2 0.454329252243042\n",
      "epoch 1667, loss 0.01402864046394825, R2 0.41688525676727295\n",
      "Eval loss 0.01416501123458147, R2 0.4543783664703369\n",
      "epoch 1668, loss 0.014027449302375317, R2 0.4169345498085022\n",
      "Eval loss 0.014163731597363949, R2 0.45442765951156616\n",
      "epoch 1669, loss 0.014026260934770107, R2 0.41698378324508667\n",
      "Eval loss 0.014162451960146427, R2 0.45447713136672974\n",
      "epoch 1670, loss 0.014025075361132622, R2 0.41703367233276367\n",
      "Eval loss 0.01416117325425148, R2 0.4545263648033142\n",
      "epoch 1671, loss 0.014023886993527412, R2 0.4170824885368347\n",
      "Eval loss 0.014159892685711384, R2 0.4545760154724121\n",
      "epoch 1672, loss 0.014022700488567352, R2 0.4171321392059326\n",
      "Eval loss 0.014158617705106735, R2 0.45462512969970703\n",
      "epoch 1673, loss 0.014021516777575016, R2 0.41718077659606934\n",
      "Eval loss 0.014157342724502087, R2 0.4546743631362915\n",
      "epoch 1674, loss 0.014020332135260105, R2 0.4172300100326538\n",
      "Eval loss 0.014156067743897438, R2 0.4547234773635864\n",
      "epoch 1675, loss 0.014019149355590343, R2 0.41727936267852783\n",
      "Eval loss 0.014154794625937939, R2 0.4547722339630127\n",
      "epoch 1676, loss 0.014017967507243156, R2 0.41732853651046753\n",
      "Eval loss 0.01415352150797844, R2 0.4548211097717285\n",
      "epoch 1677, loss 0.01401678565889597, R2 0.41737765073776245\n",
      "Eval loss 0.014152251183986664, R2 0.45487022399902344\n",
      "epoch 1678, loss 0.014015606604516506, R2 0.4174271821975708\n",
      "Eval loss 0.014150981791317463, R2 0.45491939783096313\n",
      "epoch 1679, loss 0.014014431275427341, R2 0.4174753427505493\n",
      "Eval loss 0.014149712398648262, R2 0.45496833324432373\n",
      "epoch 1680, loss 0.014013251289725304, R2 0.41752439737319946\n",
      "Eval loss 0.01414844486862421, R2 0.45501697063446045\n",
      "epoch 1681, loss 0.014012075960636139, R2 0.41757357120513916\n",
      "Eval loss 0.014147178269922733, R2 0.45506560802459717\n",
      "epoch 1682, loss 0.014010901562869549, R2 0.41762202978134155\n",
      "Eval loss 0.01414591446518898, R2 0.455114483833313\n",
      "epoch 1683, loss 0.014009727165102959, R2 0.4176708459854126\n",
      "Eval loss 0.014144649729132652, R2 0.45516300201416016\n",
      "epoch 1684, loss 0.014008553698658943, R2 0.4177201986312866\n",
      "Eval loss 0.014143385924398899, R2 0.45521169900894165\n",
      "epoch 1685, loss 0.014007382094860077, R2 0.41776829957962036\n",
      "Eval loss 0.014142122119665146, R2 0.455260694026947\n",
      "epoch 1686, loss 0.01400621049106121, R2 0.41781777143478394\n",
      "Eval loss 0.014140862971544266, R2 0.4553084969520569\n",
      "epoch 1687, loss 0.014005041681230068, R2 0.4178659915924072\n",
      "Eval loss 0.014139602892100811, R2 0.45535755157470703\n",
      "epoch 1688, loss 0.0140038738027215, R2 0.4179146885871887\n",
      "Eval loss 0.01413834560662508, R2 0.4554055333137512\n",
      "epoch 1689, loss 0.014002706855535507, R2 0.4179626703262329\n",
      "Eval loss 0.0141370864585042, R2 0.45545464754104614\n",
      "epoch 1690, loss 0.014001538045704365, R2 0.41801124811172485\n",
      "Eval loss 0.014135831035673618, R2 0.4555025100708008\n",
      "epoch 1691, loss 0.01400037296116352, R2 0.41805964708328247\n",
      "Eval loss 0.014134573750197887, R2 0.45555031299591064\n",
      "epoch 1692, loss 0.013999209739267826, R2 0.4181082248687744\n",
      "Eval loss 0.01413332112133503, R2 0.4555993676185608\n",
      "epoch 1693, loss 0.013998044654726982, R2 0.4181564450263977\n",
      "Eval loss 0.014132068492472172, R2 0.4556472897529602\n",
      "epoch 1694, loss 0.013996884226799011, R2 0.418204665184021\n",
      "Eval loss 0.014130815863609314, R2 0.4556962251663208\n",
      "epoch 1695, loss 0.013995722867548466, R2 0.4182531237602234\n",
      "Eval loss 0.014129565097391605, R2 0.45574402809143066\n",
      "epoch 1696, loss 0.013994564302265644, R2 0.4183012843132019\n",
      "Eval loss 0.014128315262496471, R2 0.4557918906211853\n",
      "epoch 1697, loss 0.013993403874337673, R2 0.41834938526153564\n",
      "Eval loss 0.014127065427601337, R2 0.45583999156951904\n",
      "epoch 1698, loss 0.013992245309054852, R2 0.4183982014656067\n",
      "Eval loss 0.014125819317996502, R2 0.45588815212249756\n",
      "epoch 1699, loss 0.013991087675094604, R2 0.4184455871582031\n",
      "Eval loss 0.014124571345746517, R2 0.455936074256897\n",
      "epoch 1700, loss 0.013989930972456932, R2 0.4184938669204712\n",
      "Eval loss 0.014123326167464256, R2 0.4559841752052307\n",
      "epoch 1701, loss 0.013988776132464409, R2 0.4185420870780945\n",
      "Eval loss 0.01412208192050457, R2 0.456032931804657\n",
      "epoch 1702, loss 0.013987623155117035, R2 0.4185900092124939\n",
      "Eval loss 0.014120838604867458, R2 0.45608019828796387\n",
      "epoch 1703, loss 0.013986471109092236, R2 0.4186375141143799\n",
      "Eval loss 0.014119595289230347, R2 0.45612823963165283\n",
      "epoch 1704, loss 0.013985318131744862, R2 0.4186859726905823\n",
      "Eval loss 0.014118356630206108, R2 0.45617544651031494\n",
      "epoch 1705, loss 0.013984168879687786, R2 0.41873282194137573\n",
      "Eval loss 0.014117114245891571, R2 0.45622366666793823\n",
      "epoch 1706, loss 0.013983016833662987, R2 0.4187811017036438\n",
      "Eval loss 0.014115876518189907, R2 0.4562711715698242\n",
      "epoch 1707, loss 0.013981870375573635, R2 0.41882872581481934\n",
      "Eval loss 0.014114639721810818, R2 0.4563189744949341\n",
      "epoch 1708, loss 0.013980722054839134, R2 0.4188764691352844\n",
      "Eval loss 0.014113401994109154, R2 0.45636647939682007\n",
      "epoch 1709, loss 0.013979576528072357, R2 0.41892409324645996\n",
      "Eval loss 0.014112165197730064, R2 0.4564139246940613\n",
      "epoch 1710, loss 0.01397843100130558, R2 0.4189717173576355\n",
      "Eval loss 0.014110931195318699, R2 0.4564617872238159\n",
      "epoch 1711, loss 0.013977284543216228, R2 0.41901934146881104\n",
      "Eval loss 0.014109697192907333, R2 0.4565093517303467\n",
      "epoch 1712, loss 0.0139761408790946, R2 0.4190669059753418\n",
      "Eval loss 0.014108465984463692, R2 0.4565567374229431\n",
      "epoch 1713, loss 0.013975000008940697, R2 0.41911429166793823\n",
      "Eval loss 0.0141072329133749, R2 0.45660459995269775\n",
      "epoch 1714, loss 0.013973858207464218, R2 0.4191617965698242\n",
      "Eval loss 0.014106002636253834, R2 0.45665180683135986\n",
      "epoch 1715, loss 0.013972718268632889, R2 0.41920918226242065\n",
      "Eval loss 0.014104772359132767, R2 0.4566988945007324\n",
      "epoch 1716, loss 0.01397157832980156, R2 0.419256329536438\n",
      "Eval loss 0.014103544875979424, R2 0.456745982170105\n",
      "epoch 1717, loss 0.013970439322292805, R2 0.4193044900894165\n",
      "Eval loss 0.014102316461503506, R2 0.45679348707199097\n",
      "epoch 1718, loss 0.013969301246106625, R2 0.4193517565727234\n",
      "Eval loss 0.014101091772317886, R2 0.4568411111831665\n",
      "epoch 1719, loss 0.013968165032565594, R2 0.4193984270095825\n",
      "Eval loss 0.014099868014454842, R2 0.4568881392478943\n",
      "epoch 1720, loss 0.013967030681669712, R2 0.41944634914398193\n",
      "Eval loss 0.014098643325269222, R2 0.4569352865219116\n",
      "epoch 1721, loss 0.01396589633077383, R2 0.4194927215576172\n",
      "Eval loss 0.014097419567406178, R2 0.45698243379592896\n",
      "epoch 1722, loss 0.013964763842523098, R2 0.4195399880409241\n",
      "Eval loss 0.014096198603510857, R2 0.4570295810699463\n",
      "epoch 1723, loss 0.013963630422949791, R2 0.4195868968963623\n",
      "Eval loss 0.014094977639615536, R2 0.4570763111114502\n",
      "epoch 1724, loss 0.013962497934699059, R2 0.4196341633796692\n",
      "Eval loss 0.014093756675720215, R2 0.4571235179901123\n",
      "epoch 1725, loss 0.01396136824041605, R2 0.4196811318397522\n",
      "Eval loss 0.014092537574470043, R2 0.45716995000839233\n",
      "epoch 1726, loss 0.013960238546133041, R2 0.4197279214859009\n",
      "Eval loss 0.01409132219851017, R2 0.45721668004989624\n",
      "epoch 1727, loss 0.013959111645817757, R2 0.4197753071784973\n",
      "Eval loss 0.014090104959905148, R2 0.4572640061378479\n",
      "epoch 1728, loss 0.013957983814179897, R2 0.41982215642929077\n",
      "Eval loss 0.014088887721300125, R2 0.45731121301651\n",
      "epoch 1729, loss 0.013956856913864613, R2 0.4198686480522156\n",
      "Eval loss 0.014087673276662827, R2 0.4573577046394348\n",
      "epoch 1730, loss 0.013955732807517052, R2 0.41991519927978516\n",
      "Eval loss 0.014086458832025528, R2 0.45740413665771484\n",
      "epoch 1731, loss 0.013954607769846916, R2 0.41996192932128906\n",
      "Eval loss 0.014085247181355953, R2 0.4574512839317322\n",
      "epoch 1732, loss 0.013953483663499355, R2 0.4200092554092407\n",
      "Eval loss 0.014084035530686378, R2 0.457497775554657\n",
      "epoch 1733, loss 0.013952362351119518, R2 0.4200552701950073\n",
      "Eval loss 0.014082824811339378, R2 0.4575444459915161\n",
      "epoch 1734, loss 0.013951241038739681, R2 0.4201018810272217\n",
      "Eval loss 0.014081615954637527, R2 0.45759081840515137\n",
      "epoch 1735, loss 0.013950119726359844, R2 0.420149028301239\n",
      "Eval loss 0.014080408029258251, R2 0.4576375484466553\n",
      "epoch 1736, loss 0.013949000276625156, R2 0.4201948642730713\n",
      "Eval loss 0.01407920103520155, R2 0.4576840400695801\n",
      "epoch 1737, loss 0.013947882689535618, R2 0.42024165391921997\n",
      "Eval loss 0.014077993109822273, R2 0.4577307105064392\n",
      "epoch 1738, loss 0.01394676510244608, R2 0.42028796672821045\n",
      "Eval loss 0.014076788909733295, R2 0.45777708292007446\n",
      "epoch 1739, loss 0.013945648446679115, R2 0.4203343391418457\n",
      "Eval loss 0.014075584709644318, R2 0.4578234553337097\n",
      "epoch 1740, loss 0.013944532722234726, R2 0.42038094997406006\n",
      "Eval loss 0.014074381440877914, R2 0.45786964893341064\n",
      "epoch 1741, loss 0.013943418860435486, R2 0.42042702436447144\n",
      "Eval loss 0.014073177240788937, R2 0.4579155445098877\n",
      "epoch 1742, loss 0.013942304998636246, R2 0.4204733371734619\n",
      "Eval loss 0.014071976765990257, R2 0.4579622745513916\n",
      "epoch 1743, loss 0.01394119206815958, R2 0.4205201268196106\n",
      "Eval loss 0.014070777222514153, R2 0.458008348941803\n",
      "epoch 1744, loss 0.013940081931650639, R2 0.4205659031867981\n",
      "Eval loss 0.014069577679038048, R2 0.4580550193786621\n",
      "epoch 1745, loss 0.013938969932496548, R2 0.4206121563911438\n",
      "Eval loss 0.014068379066884518, R2 0.45810115337371826\n",
      "epoch 1746, loss 0.01393786072731018, R2 0.4206582307815552\n",
      "Eval loss 0.014067182317376137, R2 0.4581472873687744\n",
      "epoch 1747, loss 0.013936751522123814, R2 0.4207049012184143\n",
      "Eval loss 0.01406598649919033, R2 0.45819300413131714\n",
      "epoch 1748, loss 0.013935643248260021, R2 0.4207500219345093\n",
      "Eval loss 0.014064791612327099, R2 0.45823901891708374\n",
      "epoch 1749, loss 0.013934538699686527, R2 0.42079615592956543\n",
      "Eval loss 0.014063597656786442, R2 0.45828521251678467\n",
      "epoch 1750, loss 0.01393343135714531, R2 0.42084217071533203\n",
      "Eval loss 0.01406240463256836, R2 0.4583311676979065\n",
      "epoch 1751, loss 0.013932325877249241, R2 0.4208884835243225\n",
      "Eval loss 0.014061212539672852, R2 0.45837676525115967\n",
      "epoch 1752, loss 0.013931222259998322, R2 0.4209340214729309\n",
      "Eval loss 0.01406001951545477, R2 0.45842254161834717\n",
      "epoch 1753, loss 0.013930119574069977, R2 0.42098039388656616\n",
      "Eval loss 0.01405883114784956, R2 0.45846879482269287\n",
      "epoch 1754, loss 0.013929018750786781, R2 0.4210261106491089\n",
      "Eval loss 0.01405764278024435, R2 0.45851457118988037\n",
      "epoch 1755, loss 0.013927917927503586, R2 0.4210713505744934\n",
      "Eval loss 0.014056453481316566, R2 0.4585602283477783\n",
      "epoch 1756, loss 0.013926818035542965, R2 0.42111706733703613\n",
      "Eval loss 0.014055266976356506, R2 0.4586057662963867\n",
      "epoch 1757, loss 0.013925718143582344, R2 0.42116278409957886\n",
      "Eval loss 0.014054082334041595, R2 0.4586520791053772\n",
      "epoch 1758, loss 0.013924620114266872, R2 0.4212084412574768\n",
      "Eval loss 0.014052895829081535, R2 0.45869725942611694\n",
      "epoch 1759, loss 0.013923523016273975, R2 0.4212534427642822\n",
      "Eval loss 0.014051711186766624, R2 0.4587429165840149\n",
      "epoch 1760, loss 0.013922425918281078, R2 0.42130017280578613\n",
      "Eval loss 0.014050528407096863, R2 0.4587884545326233\n",
      "epoch 1761, loss 0.013921331614255905, R2 0.4213458299636841\n",
      "Eval loss 0.014049344696104527, R2 0.45883452892303467\n",
      "epoch 1762, loss 0.013920237310230732, R2 0.4213905930519104\n",
      "Eval loss 0.014048166573047638, R2 0.458879292011261\n",
      "epoch 1763, loss 0.013919143937528133, R2 0.421436071395874\n",
      "Eval loss 0.014046985656023026, R2 0.4589252471923828\n",
      "epoch 1764, loss 0.01391805149614811, R2 0.4214816093444824\n",
      "Eval loss 0.014045805670320988, R2 0.4589703679084778\n",
      "epoch 1765, loss 0.01391695998609066, R2 0.42152684926986694\n",
      "Eval loss 0.014044627547264099, R2 0.459015429019928\n",
      "epoch 1766, loss 0.013915867544710636, R2 0.42157238721847534\n",
      "Eval loss 0.014043450355529785, R2 0.45906126499176025\n",
      "epoch 1767, loss 0.01391478069126606, R2 0.4216175675392151\n",
      "Eval loss 0.014042273163795471, R2 0.4591064453125\n",
      "epoch 1768, loss 0.013913691975176334, R2 0.4216628670692444\n",
      "Eval loss 0.014041099697351456, R2 0.4591515064239502\n",
      "epoch 1769, loss 0.013912604190409184, R2 0.4217078685760498\n",
      "Eval loss 0.014039924368262291, R2 0.4591965675354004\n",
      "epoch 1770, loss 0.013911517336964607, R2 0.42175304889678955\n",
      "Eval loss 0.01403875183314085, R2 0.4592421054840088\n",
      "epoch 1771, loss 0.013910431414842606, R2 0.42179781198501587\n",
      "Eval loss 0.01403757929801941, R2 0.45928722620010376\n",
      "epoch 1772, loss 0.013909346424043179, R2 0.42184311151504517\n",
      "Eval loss 0.014036407694220543, R2 0.45933252573013306\n",
      "epoch 1773, loss 0.013908262364566326, R2 0.42188799381256104\n",
      "Eval loss 0.014035237021744251, R2 0.4593774676322937\n",
      "epoch 1774, loss 0.013907179236412048, R2 0.4219333529472351\n",
      "Eval loss 0.014034068211913109, R2 0.4594224691390991\n",
      "epoch 1775, loss 0.013906095176935196, R2 0.4219784140586853\n",
      "Eval loss 0.014032901264727116, R2 0.4594672918319702\n",
      "epoch 1776, loss 0.013905015774071217, R2 0.42202383279800415\n",
      "Eval loss 0.014031734317541122, R2 0.4595118761062622\n",
      "epoch 1777, loss 0.013903936371207237, R2 0.42206889390945435\n",
      "Eval loss 0.01403056737035513, R2 0.4595569968223572\n",
      "epoch 1778, loss 0.013902856037020683, R2 0.4221130609512329\n",
      "Eval loss 0.014029400423169136, R2 0.4596022367477417\n",
      "epoch 1779, loss 0.013901777565479279, R2 0.42215806245803833\n",
      "Eval loss 0.014028236269950867, R2 0.45964711904525757\n",
      "epoch 1780, loss 0.013900700025260448, R2 0.4222027063369751\n",
      "Eval loss 0.014027071185410023, R2 0.45969200134277344\n",
      "epoch 1781, loss 0.013899623416364193, R2 0.42224740982055664\n",
      "Eval loss 0.014025910757482052, R2 0.45973700284957886\n",
      "epoch 1782, loss 0.013898546807467937, R2 0.42229217290878296\n",
      "Eval loss 0.014024747535586357, R2 0.4597814679145813\n",
      "epoch 1783, loss 0.013897472992539406, R2 0.42233699560165405\n",
      "Eval loss 0.014023588970303535, R2 0.4598262906074524\n",
      "epoch 1784, loss 0.0138963982462883, R2 0.42238110303878784\n",
      "Eval loss 0.014022428542375565, R2 0.4598708152770996\n",
      "epoch 1785, loss 0.013895326294004917, R2 0.4224262237548828\n",
      "Eval loss 0.014021269977092743, R2 0.45991528034210205\n",
      "epoch 1786, loss 0.01389425527304411, R2 0.42247074842453003\n",
      "Eval loss 0.014020111411809921, R2 0.45995980501174927\n",
      "epoch 1787, loss 0.013893184252083302, R2 0.42251527309417725\n",
      "Eval loss 0.014018956571817398, R2 0.4600047469139099\n",
      "epoch 1788, loss 0.013892112299799919, R2 0.42255979776382446\n",
      "Eval loss 0.014017798937857151, R2 0.46004897356033325\n",
      "epoch 1789, loss 0.013891044072806835, R2 0.4226042628288269\n",
      "Eval loss 0.014016645029187202, R2 0.46009325981140137\n",
      "epoch 1790, loss 0.013889976777136326, R2 0.42264842987060547\n",
      "Eval loss 0.014015491120517254, R2 0.4601372480392456\n",
      "epoch 1791, loss 0.013888910412788391, R2 0.4226927161216736\n",
      "Eval loss 0.014014337211847305, R2 0.46018218994140625\n",
      "epoch 1792, loss 0.013887841254472733, R2 0.42273736000061035\n",
      "Eval loss 0.014013185165822506, R2 0.46022671461105347\n",
      "epoch 1793, loss 0.013886776752769947, R2 0.42278212308883667\n",
      "Eval loss 0.014012034982442856, R2 0.4602710008621216\n",
      "epoch 1794, loss 0.013885711319744587, R2 0.42282623052597046\n",
      "Eval loss 0.014010883867740631, R2 0.4603155255317688\n",
      "epoch 1795, loss 0.01388464868068695, R2 0.42286986112594604\n",
      "Eval loss 0.01400973554700613, R2 0.4603601098060608\n",
      "epoch 1796, loss 0.01388358511030674, R2 0.4229142665863037\n",
      "Eval loss 0.014008588157594204, R2 0.4604039788246155\n",
      "epoch 1797, loss 0.013882522471249104, R2 0.42295825481414795\n",
      "Eval loss 0.014007442630827427, R2 0.46044808626174927\n",
      "epoch 1798, loss 0.013881461694836617, R2 0.4230028986930847\n",
      "Eval loss 0.014006293378770351, R2 0.46049219369888306\n",
      "epoch 1799, loss 0.013880401849746704, R2 0.42304641008377075\n",
      "Eval loss 0.014005147852003574, R2 0.46053630113601685\n",
      "epoch 1800, loss 0.013879342935979366, R2 0.4230905771255493\n",
      "Eval loss 0.014004003256559372, R2 0.4605810046195984\n",
      "epoch 1801, loss 0.013878284953534603, R2 0.42313438653945923\n",
      "Eval loss 0.01400285866111517, R2 0.46062445640563965\n",
      "epoch 1802, loss 0.01387722697108984, R2 0.42317837476730347\n",
      "Eval loss 0.014001717790961266, R2 0.46066874265670776\n",
      "epoch 1803, loss 0.013876168988645077, R2 0.423223078250885\n",
      "Eval loss 0.014000577852129936, R2 0.4607123136520386\n",
      "epoch 1804, loss 0.013875114731490612, R2 0.4232667088508606\n",
      "Eval loss 0.013999435119330883, R2 0.4607563614845276\n",
      "epoch 1805, loss 0.013874058611690998, R2 0.42331022024154663\n",
      "Eval loss 0.013998295180499554, R2 0.4607999324798584\n",
      "epoch 1806, loss 0.013873004354536533, R2 0.42335426807403564\n",
      "Eval loss 0.013997157104313374, R2 0.46084439754486084\n",
      "epoch 1807, loss 0.013871951960027218, R2 0.4233981966972351\n",
      "Eval loss 0.013996017165482044, R2 0.4608880281448364\n",
      "epoch 1808, loss 0.013870899565517902, R2 0.4234423041343689\n",
      "Eval loss 0.013994880951941013, R2 0.4609319567680359\n",
      "epoch 1809, loss 0.013869849033653736, R2 0.42348504066467285\n",
      "Eval loss 0.013993746601045132, R2 0.4609757661819458\n",
      "epoch 1810, loss 0.013868797570466995, R2 0.4235287308692932\n",
      "Eval loss 0.013992611318826675, R2 0.46101921796798706\n",
      "epoch 1811, loss 0.013867748901247978, R2 0.423572838306427\n",
      "Eval loss 0.01399147603660822, R2 0.46106308698654175\n",
      "epoch 1812, loss 0.013866701163351536, R2 0.4236156940460205\n",
      "Eval loss 0.013990343548357487, R2 0.46110671758651733\n",
      "epoch 1813, loss 0.013865654356777668, R2 0.42366015911102295\n",
      "Eval loss 0.013989211060106754, R2 0.4611503481864929\n",
      "epoch 1814, loss 0.013864605687558651, R2 0.42370373010635376\n",
      "Eval loss 0.013988079503178596, R2 0.46119409799575806\n",
      "epoch 1815, loss 0.013863560743629932, R2 0.4237469434738159\n",
      "Eval loss 0.013986948877573013, R2 0.4612371325492859\n",
      "epoch 1816, loss 0.01386251300573349, R2 0.42378997802734375\n",
      "Eval loss 0.013985819183290005, R2 0.4612811803817749\n",
      "epoch 1817, loss 0.013861468993127346, R2 0.42383337020874023\n",
      "Eval loss 0.013984691351652145, R2 0.4613247513771057\n",
      "epoch 1818, loss 0.013860425911843777, R2 0.42387670278549194\n",
      "Eval loss 0.013983562588691711, R2 0.4613679051399231\n",
      "epoch 1819, loss 0.013859382830560207, R2 0.42392081022262573\n",
      "Eval loss 0.013982436619699001, R2 0.4614109992980957\n",
      "epoch 1820, loss 0.013858342543244362, R2 0.42396390438079834\n",
      "Eval loss 0.013981309719383717, R2 0.4614548087120056\n",
      "epoch 1821, loss 0.013857301324605942, R2 0.4240071177482605\n",
      "Eval loss 0.013980184681713581, R2 0.46149784326553345\n",
      "epoch 1822, loss 0.013856261968612671, R2 0.42405015230178833\n",
      "Eval loss 0.01397906243801117, R2 0.4615411162376404\n",
      "epoch 1823, loss 0.013855221681296825, R2 0.42409318685531616\n",
      "Eval loss 0.013977938331663609, R2 0.4615848660469055\n",
      "epoch 1824, loss 0.013854184187948704, R2 0.42413634061813354\n",
      "Eval loss 0.013976815156638622, R2 0.4616277813911438\n",
      "epoch 1825, loss 0.013853146694600582, R2 0.4241794943809509\n",
      "Eval loss 0.01397569291293621, R2 0.4616708755493164\n",
      "epoch 1826, loss 0.01385211106389761, R2 0.4242228865623474\n",
      "Eval loss 0.013974574394524097, R2 0.4617142677307129\n",
      "epoch 1827, loss 0.013851074501872063, R2 0.42426562309265137\n",
      "Eval loss 0.013973454013466835, R2 0.46175694465637207\n",
      "epoch 1828, loss 0.013850039802491665, R2 0.42430877685546875\n",
      "Eval loss 0.013972333632409573, R2 0.46180039644241333\n",
      "epoch 1829, loss 0.013849006965756416, R2 0.4243515729904175\n",
      "Eval loss 0.013971217907965183, R2 0.46184343099594116\n",
      "epoch 1830, loss 0.013847974129021168, R2 0.42439430952072144\n",
      "Eval loss 0.013970102183520794, R2 0.461885929107666\n",
      "epoch 1831, loss 0.01384694129228592, R2 0.42443740367889404\n",
      "Eval loss 0.01396898552775383, R2 0.4619297385215759\n",
      "epoch 1832, loss 0.013845909386873245, R2 0.424480140209198\n",
      "Eval loss 0.013967870734632015, R2 0.461972177028656\n",
      "epoch 1833, loss 0.01384487934410572, R2 0.42452293634414673\n",
      "Eval loss 0.013966755010187626, R2 0.46201545000076294\n",
      "epoch 1834, loss 0.013843848370015621, R2 0.42456597089767456\n",
      "Eval loss 0.01396564394235611, R2 0.4620581269264221\n",
      "epoch 1835, loss 0.01384282112121582, R2 0.4246084690093994\n",
      "Eval loss 0.013964531011879444, R2 0.46210068464279175\n",
      "epoch 1836, loss 0.01384179387241602, R2 0.4246519207954407\n",
      "Eval loss 0.013963418081402779, R2 0.46214383840560913\n",
      "epoch 1837, loss 0.01384076476097107, R2 0.42469483613967896\n",
      "Eval loss 0.013962307944893837, R2 0.46218663454055786\n",
      "epoch 1838, loss 0.013839739374816418, R2 0.42473673820495605\n",
      "Eval loss 0.01396119873970747, R2 0.4622296690940857\n",
      "epoch 1839, loss 0.01383871491998434, R2 0.42477917671203613\n",
      "Eval loss 0.013960089534521103, R2 0.46227192878723145\n",
      "epoch 1840, loss 0.013837690465152264, R2 0.42482155561447144\n",
      "Eval loss 0.013958982191979885, R2 0.4623147249221802\n",
      "epoch 1841, loss 0.013836666010320187, R2 0.4248652458190918\n",
      "Eval loss 0.013957873918116093, R2 0.4623571038246155\n",
      "epoch 1842, loss 0.013835642486810684, R2 0.42490702867507935\n",
      "Eval loss 0.013956769369542599, R2 0.4623996615409851\n",
      "epoch 1843, loss 0.013834621757268906, R2 0.4249494671821594\n",
      "Eval loss 0.01395566388964653, R2 0.4624425172805786\n",
      "epoch 1844, loss 0.013833600096404552, R2 0.42499178647994995\n",
      "Eval loss 0.013954561203718185, R2 0.4624847173690796\n",
      "epoch 1845, loss 0.013832580298185349, R2 0.4250343441963196\n",
      "Eval loss 0.013953457586467266, R2 0.46252816915512085\n",
      "epoch 1846, loss 0.01383156143128872, R2 0.4250767230987549\n",
      "Eval loss 0.013952353969216347, R2 0.4625697135925293\n",
      "epoch 1847, loss 0.013830541633069515, R2 0.4251190423965454\n",
      "Eval loss 0.013951252214610577, R2 0.4626128077507019\n",
      "epoch 1848, loss 0.013829522766172886, R2 0.4251619577407837\n",
      "Eval loss 0.013950151391327381, R2 0.46265488862991333\n",
      "epoch 1849, loss 0.01382850669324398, R2 0.42520368099212646\n",
      "Eval loss 0.01394905149936676, R2 0.46269690990448\n",
      "epoch 1850, loss 0.01382749155163765, R2 0.42524582147598267\n",
      "Eval loss 0.013947952538728714, R2 0.46274006366729736\n",
      "epoch 1851, loss 0.013826475478708744, R2 0.4252879023551941\n",
      "Eval loss 0.013946854509413242, R2 0.4627818465232849\n",
      "epoch 1852, loss 0.013825460337102413, R2 0.4253302812576294\n",
      "Eval loss 0.01394575648009777, R2 0.4628238081932068\n",
      "epoch 1853, loss 0.013824447989463806, R2 0.4253721833229065\n",
      "Eval loss 0.013944661244750023, R2 0.4628661870956421\n",
      "epoch 1854, loss 0.013823435641825199, R2 0.42541444301605225\n",
      "Eval loss 0.0139435650780797, R2 0.4629085659980774\n",
      "epoch 1855, loss 0.013822425156831741, R2 0.42545628547668457\n",
      "Eval loss 0.013942470774054527, R2 0.46295106410980225\n",
      "epoch 1856, loss 0.01382141187787056, R2 0.4254985451698303\n",
      "Eval loss 0.01394137553870678, R2 0.46299272775650024\n",
      "epoch 1857, loss 0.013820401392877102, R2 0.42554038763046265\n",
      "Eval loss 0.013940284959971905, R2 0.46303510665893555\n",
      "epoch 1858, loss 0.013819391839206219, R2 0.4255830645561218\n",
      "Eval loss 0.01393919251859188, R2 0.4630771279335022\n",
      "epoch 1859, loss 0.01381838321685791, R2 0.4256247878074646\n",
      "Eval loss 0.013938101008534431, R2 0.4631190299987793\n",
      "epoch 1860, loss 0.013817375525832176, R2 0.4256661534309387\n",
      "Eval loss 0.013937010429799557, R2 0.46316105127334595\n",
      "epoch 1861, loss 0.013816368766129017, R2 0.42570799589157104\n",
      "Eval loss 0.013935920782387257, R2 0.4632030129432678\n",
      "epoch 1862, loss 0.013815361075103283, R2 0.42575007677078247\n",
      "Eval loss 0.013934832066297531, R2 0.46324479579925537\n",
      "epoch 1863, loss 0.013814357109367847, R2 0.42579180002212524\n",
      "Eval loss 0.01393374428153038, R2 0.4632868766784668\n",
      "epoch 1864, loss 0.013813352212309837, R2 0.425833523273468\n",
      "Eval loss 0.013932658359408379, R2 0.4633287191390991\n",
      "epoch 1865, loss 0.013812349177896976, R2 0.42587506771087646\n",
      "Eval loss 0.013931574299931526, R2 0.463370144367218\n",
      "epoch 1866, loss 0.01381134707480669, R2 0.42591673135757446\n",
      "Eval loss 0.013930486515164375, R2 0.463412344455719\n",
      "epoch 1867, loss 0.01381034404039383, R2 0.4259588122367859\n",
      "Eval loss 0.013929403387010098, R2 0.4634542465209961\n",
      "epoch 1868, loss 0.013809343799948692, R2 0.4259999990463257\n",
      "Eval loss 0.01392832025885582, R2 0.46349549293518066\n",
      "epoch 1869, loss 0.013808340765535831, R2 0.4260416626930237\n",
      "Eval loss 0.013927236199378967, R2 0.46353739500045776\n",
      "epoch 1870, loss 0.013807342387735844, R2 0.42608320713043213\n",
      "Eval loss 0.013926154933869839, R2 0.46357887983322144\n",
      "epoch 1871, loss 0.013806343078613281, R2 0.4261254668235779\n",
      "Eval loss 0.01392507553100586, R2 0.4636204242706299\n",
      "epoch 1872, loss 0.013805347494781017, R2 0.42616701126098633\n",
      "Eval loss 0.013923995196819305, R2 0.46366220712661743\n",
      "epoch 1873, loss 0.013804348185658455, R2 0.4262080192565918\n",
      "Eval loss 0.013922915793955326, R2 0.463703989982605\n",
      "epoch 1874, loss 0.01380335446447134, R2 0.4262491464614868\n",
      "Eval loss 0.013921836391091347, R2 0.4637451767921448\n",
      "epoch 1875, loss 0.013802357017993927, R2 0.42629045248031616\n",
      "Eval loss 0.013920758850872517, R2 0.4637867212295532\n",
      "epoch 1876, loss 0.013801361434161663, R2 0.4263319969177246\n",
      "Eval loss 0.01391968410462141, R2 0.46382826566696167\n",
      "epoch 1877, loss 0.013800366781651974, R2 0.4263731837272644\n",
      "Eval loss 0.01391860842704773, R2 0.4638700485229492\n",
      "epoch 1878, loss 0.013799373991787434, R2 0.4264140725135803\n",
      "Eval loss 0.013917533680796623, R2 0.463911235332489\n",
      "epoch 1879, loss 0.013798383064568043, R2 0.42645561695098877\n",
      "Eval loss 0.013916458934545517, R2 0.4639526605606079\n",
      "epoch 1880, loss 0.013797391206026077, R2 0.42649680376052856\n",
      "Eval loss 0.013915386982262135, R2 0.46399396657943726\n",
      "epoch 1881, loss 0.013796399347484112, R2 0.42653822898864746\n",
      "Eval loss 0.013914314098656178, R2 0.4640357494354248\n",
      "epoch 1882, loss 0.013795409351587296, R2 0.4265797734260559\n",
      "Eval loss 0.01391324307769537, R2 0.4640762209892273\n",
      "epoch 1883, loss 0.013794420287013054, R2 0.4266203045845032\n",
      "Eval loss 0.013912172056734562, R2 0.46411746740341187\n",
      "epoch 1884, loss 0.013793431222438812, R2 0.4266616106033325\n",
      "Eval loss 0.013911102898418903, R2 0.46415913105010986\n",
      "epoch 1885, loss 0.01379244402050972, R2 0.42670243978500366\n",
      "Eval loss 0.013910033740103245, R2 0.464199960231781\n",
      "epoch 1886, loss 0.013791457749903202, R2 0.4267432689666748\n",
      "Eval loss 0.013908966444432735, R2 0.4642411470413208\n",
      "epoch 1887, loss 0.013790472410619259, R2 0.4267842173576355\n",
      "Eval loss 0.013907897286117077, R2 0.46428245306015015\n",
      "epoch 1888, loss 0.013789486140012741, R2 0.4268261194229126\n",
      "Eval loss 0.013906831853091717, R2 0.4643235206604004\n",
      "epoch 1889, loss 0.013788502663373947, R2 0.4268662929534912\n",
      "Eval loss 0.013905766420066357, R2 0.46436452865600586\n",
      "epoch 1890, loss 0.013787517324090004, R2 0.4269072413444519\n",
      "Eval loss 0.013904700987040997, R2 0.46440553665161133\n",
      "epoch 1891, loss 0.013786537572741508, R2 0.4269479513168335\n",
      "Eval loss 0.013903639279305935, R2 0.464446485042572\n",
      "epoch 1892, loss 0.01378555316478014, R2 0.4269888997077942\n",
      "Eval loss 0.01390257477760315, R2 0.4644871950149536\n",
      "epoch 1893, loss 0.013784571550786495, R2 0.4270298480987549\n",
      "Eval loss 0.013901512138545513, R2 0.46452808380126953\n",
      "epoch 1894, loss 0.013783591799438, R2 0.42707037925720215\n",
      "Eval loss 0.013900449499487877, R2 0.4645690321922302\n",
      "epoch 1895, loss 0.013782612048089504, R2 0.4271111488342285\n",
      "Eval loss 0.013899392448365688, R2 0.4646100401878357\n",
      "epoch 1896, loss 0.013781633228063583, R2 0.42715197801589966\n",
      "Eval loss 0.013898331671953201, R2 0.4646512269973755\n",
      "epoch 1897, loss 0.013780655339360237, R2 0.4271925091743469\n",
      "Eval loss 0.01389726996421814, R2 0.46469181776046753\n",
      "epoch 1898, loss 0.013779677450656891, R2 0.42723309993743896\n",
      "Eval loss 0.0138962147757411, R2 0.46473264694213867\n",
      "epoch 1899, loss 0.01377870049327612, R2 0.427273690700531\n",
      "Eval loss 0.013895156793296337, R2 0.46477335691452026\n",
      "epoch 1900, loss 0.013777727261185646, R2 0.4273141622543335\n",
      "Eval loss 0.013894101604819298, R2 0.4648135304450989\n",
      "epoch 1901, loss 0.013776750303804874, R2 0.42735493183135986\n",
      "Eval loss 0.013893045485019684, R2 0.46485424041748047\n",
      "epoch 1902, loss 0.013775777071714401, R2 0.4273959994316101\n",
      "Eval loss 0.013891990296542645, R2 0.4648953080177307\n",
      "epoch 1903, loss 0.013774803839623928, R2 0.42743587493896484\n",
      "Eval loss 0.013890936970710754, R2 0.46493542194366455\n",
      "epoch 1904, loss 0.01377382967621088, R2 0.427476167678833\n",
      "Eval loss 0.01388988271355629, R2 0.4649766683578491\n",
      "epoch 1905, loss 0.013772859238088131, R2 0.42751652002334595\n",
      "Eval loss 0.013888832181692123, R2 0.4650164842605591\n",
      "epoch 1906, loss 0.013771888799965382, R2 0.42755740880966187\n",
      "Eval loss 0.013887779787182808, R2 0.46505749225616455\n",
      "epoch 1907, loss 0.013770916499197483, R2 0.4275974631309509\n",
      "Eval loss 0.013886730186641216, R2 0.4650977849960327\n",
      "epoch 1908, loss 0.013769946992397308, R2 0.42763739824295044\n",
      "Eval loss 0.01388567965477705, R2 0.46513795852661133\n",
      "epoch 1909, loss 0.013768980279564857, R2 0.4276779294013977\n",
      "Eval loss 0.013884630054235458, R2 0.4651786684989929\n",
      "epoch 1910, loss 0.013768010772764683, R2 0.42771875858306885\n",
      "Eval loss 0.01388358324766159, R2 0.46521902084350586\n",
      "epoch 1911, loss 0.013767044991254807, R2 0.42775821685791016\n",
      "Eval loss 0.013882536441087723, R2 0.465259313583374\n",
      "epoch 1912, loss 0.01376607920974493, R2 0.4277985095977783\n",
      "Eval loss 0.013881489634513855, R2 0.46529948711395264\n",
      "epoch 1913, loss 0.013765111565589905, R2 0.4278385639190674\n",
      "Eval loss 0.013880442827939987, R2 0.46533966064453125\n",
      "epoch 1914, loss 0.013764147646725178, R2 0.4278789758682251\n",
      "Eval loss 0.013879396952688694, R2 0.4653799533843994\n",
      "epoch 1915, loss 0.013763181865215302, R2 0.42791932821273804\n",
      "Eval loss 0.013878353871405125, R2 0.4654202461242676\n",
      "epoch 1916, loss 0.013762220740318298, R2 0.42795872688293457\n",
      "Eval loss 0.013877310790121555, R2 0.46546030044555664\n",
      "epoch 1917, loss 0.013761254958808422, R2 0.42799919843673706\n",
      "Eval loss 0.01387626864016056, R2 0.4655005931854248\n",
      "epoch 1918, loss 0.013760294765233994, R2 0.42803841829299927\n",
      "Eval loss 0.01387522742152214, R2 0.4655410647392273\n",
      "epoch 1919, loss 0.01375933364033699, R2 0.42807871103286743\n",
      "Eval loss 0.01387418620288372, R2 0.46558094024658203\n",
      "epoch 1920, loss 0.013758374378085136, R2 0.42811858654022217\n",
      "Eval loss 0.013873145915567875, R2 0.46562105417251587\n",
      "epoch 1921, loss 0.013757414184510708, R2 0.42815905809402466\n",
      "Eval loss 0.013872106559574604, R2 0.4656614065170288\n",
      "epoch 1922, loss 0.013756454922258854, R2 0.4281991124153137\n",
      "Eval loss 0.013871068134903908, R2 0.4657011032104492\n",
      "epoch 1923, loss 0.01375549752265215, R2 0.4282383322715759\n",
      "Eval loss 0.013870029710233212, R2 0.4657409191131592\n",
      "epoch 1924, loss 0.01375453919172287, R2 0.42827796936035156\n",
      "Eval loss 0.013868995010852814, R2 0.46578049659729004\n",
      "epoch 1925, loss 0.013753583654761314, R2 0.4283179044723511\n",
      "Eval loss 0.013867959380149841, R2 0.46582096815109253\n",
      "epoch 1926, loss 0.013752628117799759, R2 0.42835742235183716\n",
      "Eval loss 0.013866924680769444, R2 0.46586084365844727\n",
      "epoch 1927, loss 0.013751673512160778, R2 0.42839711904525757\n",
      "Eval loss 0.01386589091271162, R2 0.465900182723999\n",
      "epoch 1928, loss 0.013750718906521797, R2 0.4284369945526123\n",
      "Eval loss 0.013864854350686073, R2 0.46594053506851196\n",
      "epoch 1929, loss 0.013749765232205391, R2 0.4284764528274536\n",
      "Eval loss 0.0138638224452734, R2 0.4659801721572876\n",
      "epoch 1930, loss 0.01374881248921156, R2 0.4285160303115845\n",
      "Eval loss 0.0138627914711833, R2 0.46601957082748413\n",
      "epoch 1931, loss 0.013747860677540302, R2 0.42855560779571533\n",
      "Eval loss 0.013861759565770626, R2 0.4660596251487732\n",
      "epoch 1932, loss 0.01374690979719162, R2 0.4285951256752014\n",
      "Eval loss 0.013860728591680527, R2 0.4660990238189697\n",
      "epoch 1933, loss 0.013745957985520363, R2 0.4286348819732666\n",
      "Eval loss 0.013859700411558151, R2 0.46613895893096924\n",
      "epoch 1934, loss 0.01374500896781683, R2 0.4286739230155945\n",
      "Eval loss 0.013858670368790627, R2 0.4661787748336792\n",
      "epoch 1935, loss 0.013744060881435871, R2 0.42871373891830444\n",
      "Eval loss 0.013857642188668251, R2 0.4662185311317444\n",
      "epoch 1936, loss 0.013743111863732338, R2 0.4287528395652771\n",
      "Eval loss 0.0138566168025136, R2 0.46625757217407227\n",
      "epoch 1937, loss 0.013742164708673954, R2 0.42879289388656616\n",
      "Eval loss 0.013855588622391224, R2 0.46629685163497925\n",
      "epoch 1938, loss 0.013741218484938145, R2 0.42883169651031494\n",
      "Eval loss 0.013854564167559147, R2 0.4663369655609131\n",
      "epoch 1939, loss 0.013740272261202335, R2 0.428871214389801\n",
      "Eval loss 0.01385353971272707, R2 0.4663759469985962\n",
      "epoch 1940, loss 0.0137393269687891, R2 0.4289103150367737\n",
      "Eval loss 0.013852515257894993, R2 0.4664157032966614\n",
      "epoch 1941, loss 0.01373838260769844, R2 0.42894959449768066\n",
      "Eval loss 0.013851492665708065, R2 0.4664551019668579\n",
      "epoch 1942, loss 0.01373744010925293, R2 0.4289892911911011\n",
      "Eval loss 0.013850468210875988, R2 0.4664945602416992\n",
      "epoch 1943, loss 0.01373649574816227, R2 0.4290279746055603\n",
      "Eval loss 0.01384944748133421, R2 0.466533899307251\n",
      "epoch 1944, loss 0.013735553249716759, R2 0.4290670156478882\n",
      "Eval loss 0.013848426751792431, R2 0.46657317876815796\n",
      "epoch 1945, loss 0.013734611682593822, R2 0.42910706996917725\n",
      "Eval loss 0.013847406022250652, R2 0.4666123390197754\n",
      "epoch 1946, loss 0.01373367104679346, R2 0.4291459321975708\n",
      "Eval loss 0.013846388086676598, R2 0.46665143966674805\n",
      "epoch 1947, loss 0.0137327304109931, R2 0.4291844964027405\n",
      "Eval loss 0.013845368288457394, R2 0.46669065952301025\n",
      "epoch 1948, loss 0.013731791637837887, R2 0.42922353744506836\n",
      "Eval loss 0.013844352215528488, R2 0.466729998588562\n",
      "epoch 1949, loss 0.013730852864682674, R2 0.42926257848739624\n",
      "Eval loss 0.013843334279954433, R2 0.4667692184448242\n",
      "epoch 1950, loss 0.013729915022850037, R2 0.42930155992507935\n",
      "Eval loss 0.013842319138348103, R2 0.46680861711502075\n",
      "epoch 1951, loss 0.013728978112339973, R2 0.429340660572052\n",
      "Eval loss 0.013841303065419197, R2 0.46684759855270386\n",
      "epoch 1952, loss 0.01372804306447506, R2 0.42937934398651123\n",
      "Eval loss 0.01384028885513544, R2 0.46688711643218994\n",
      "epoch 1953, loss 0.013727107085287571, R2 0.4294184446334839\n",
      "Eval loss 0.01383927557617426, R2 0.4669256806373596\n",
      "epoch 1954, loss 0.013726171106100082, R2 0.4294576644897461\n",
      "Eval loss 0.013838260434567928, R2 0.46696507930755615\n",
      "epoch 1955, loss 0.013725236989557743, R2 0.42949599027633667\n",
      "Eval loss 0.013837248086929321, R2 0.46700364351272583\n",
      "epoch 1956, loss 0.013724301941692829, R2 0.4295348525047302\n",
      "Eval loss 0.013836236670613289, R2 0.46704286336898804\n",
      "epoch 1957, loss 0.013723370619118214, R2 0.42957359552383423\n",
      "Eval loss 0.013835227116942406, R2 0.4670816659927368\n",
      "epoch 1958, loss 0.013722436502575874, R2 0.42961257696151733\n",
      "Eval loss 0.013834217563271523, R2 0.4671206474304199\n",
      "epoch 1959, loss 0.013721507973968983, R2 0.4296509623527527\n",
      "Eval loss 0.01383320800960064, R2 0.4671593904495239\n",
      "epoch 1960, loss 0.013720577582716942, R2 0.4296896457672119\n",
      "Eval loss 0.013832198455929756, R2 0.46719831228256226\n",
      "epoch 1961, loss 0.013719646260142326, R2 0.4297283887863159\n",
      "Eval loss 0.013831190764904022, R2 0.46723711490631104\n",
      "epoch 1962, loss 0.013718717731535435, R2 0.42976659536361694\n",
      "Eval loss 0.013830184936523438, R2 0.46727555990219116\n",
      "epoch 1963, loss 0.013717789202928543, R2 0.4298055171966553\n",
      "Eval loss 0.013829180039465427, R2 0.46731454133987427\n",
      "epoch 1964, loss 0.013716861605644226, R2 0.4298441410064697\n",
      "Eval loss 0.013828174211084843, R2 0.4673534631729126\n",
      "epoch 1965, loss 0.013715934939682484, R2 0.4298831820487976\n",
      "Eval loss 0.013827168382704258, R2 0.4673920273780823\n",
      "epoch 1966, loss 0.013715008273720741, R2 0.42992115020751953\n",
      "Eval loss 0.013826164416968822, R2 0.46743059158325195\n",
      "epoch 1967, loss 0.013714082539081573, R2 0.42995965480804443\n",
      "Eval loss 0.013825161382555962, R2 0.4674690365791321\n",
      "epoch 1968, loss 0.01371315959841013, R2 0.429997980594635\n",
      "Eval loss 0.0138241583481431, R2 0.4675079584121704\n",
      "epoch 1969, loss 0.013712234795093536, R2 0.43003642559051514\n",
      "Eval loss 0.013823158107697964, R2 0.46754634380340576\n",
      "epoch 1970, loss 0.013711310923099518, R2 0.4300748109817505\n",
      "Eval loss 0.013822155073285103, R2 0.46758514642715454\n",
      "epoch 1971, loss 0.013710388913750648, R2 0.43011313676834106\n",
      "Eval loss 0.01382115576416254, R2 0.46762382984161377\n",
      "epoch 1972, loss 0.013709467835724354, R2 0.43015146255493164\n",
      "Eval loss 0.013820159249007702, R2 0.4676620364189148\n",
      "epoch 1973, loss 0.01370854489505291, R2 0.4301900267601013\n",
      "Eval loss 0.01381915993988514, R2 0.467700719833374\n",
      "epoch 1974, loss 0.01370762474834919, R2 0.430228054523468\n",
      "Eval loss 0.013818161562085152, R2 0.4677388072013855\n",
      "epoch 1975, loss 0.013706705532968044, R2 0.43026626110076904\n",
      "Eval loss 0.013817165046930313, R2 0.4677770137786865\n",
      "epoch 1976, loss 0.013705785386264324, R2 0.43030470609664917\n",
      "Eval loss 0.013816168531775475, R2 0.4678155779838562\n",
      "epoch 1977, loss 0.013704867102205753, R2 0.4303432106971741\n",
      "Eval loss 0.01381517294794321, R2 0.46785426139831543\n",
      "epoch 1978, loss 0.013703949749469757, R2 0.43038082122802734\n",
      "Eval loss 0.013814178295433521, R2 0.4678924083709717\n",
      "epoch 1979, loss 0.013703033328056335, R2 0.4304189085960388\n",
      "Eval loss 0.013813186436891556, R2 0.4679306149482727\n",
      "epoch 1980, loss 0.013702116906642914, R2 0.4304569959640503\n",
      "Eval loss 0.013812192715704441, R2 0.4679687023162842\n",
      "epoch 1981, loss 0.013701200485229492, R2 0.4304947257041931\n",
      "Eval loss 0.013811200857162476, R2 0.46800726652145386\n",
      "epoch 1982, loss 0.01370028592646122, R2 0.43053311109542847\n",
      "Eval loss 0.013810207135975361, R2 0.46804553270339966\n",
      "epoch 1983, loss 0.013699372299015522, R2 0.4305712580680847\n",
      "Eval loss 0.013809217140078545, R2 0.4680832028388977\n",
      "epoch 1984, loss 0.013698459602892399, R2 0.43060922622680664\n",
      "Eval loss 0.013808226212859154, R2 0.4681220054626465\n",
      "epoch 1985, loss 0.013697545044124126, R2 0.43064701557159424\n",
      "Eval loss 0.013807236216962337, R2 0.4681597948074341\n",
      "epoch 1986, loss 0.013696634210646152, R2 0.4306856393814087\n",
      "Eval loss 0.013806249015033245, R2 0.4681980013847351\n",
      "epoch 1987, loss 0.013695723377168179, R2 0.4307229518890381\n",
      "Eval loss 0.013805262744426727, R2 0.46823567152023315\n",
      "epoch 1988, loss 0.013694812543690205, R2 0.43076062202453613\n",
      "Eval loss 0.013804273679852486, R2 0.4682740569114685\n",
      "epoch 1989, loss 0.01369390357285738, R2 0.43079859018325806\n",
      "Eval loss 0.013803288340568542, R2 0.4683119058609009\n",
      "epoch 1990, loss 0.01369299367070198, R2 0.4308367371559143\n",
      "Eval loss 0.013802302069962025, R2 0.4683495759963989\n",
      "epoch 1991, loss 0.01369208563119173, R2 0.4308736324310303\n",
      "Eval loss 0.01380131859332323, R2 0.46838825941085815\n",
      "epoch 1992, loss 0.013691178523004055, R2 0.4309118390083313\n",
      "Eval loss 0.013800333254039288, R2 0.4684256911277771\n",
      "epoch 1993, loss 0.01369027141481638, R2 0.4309495687484741\n",
      "Eval loss 0.013799350708723068, R2 0.4684635400772095\n",
      "epoch 1994, loss 0.013689365237951279, R2 0.43098700046539307\n",
      "Eval loss 0.013798368163406849, R2 0.46850156784057617\n",
      "epoch 1995, loss 0.013688458129763603, R2 0.43102455139160156\n",
      "Eval loss 0.01379738561809063, R2 0.46853911876678467\n",
      "epoch 1996, loss 0.013687554746866226, R2 0.4310622811317444\n",
      "Eval loss 0.01379640493541956, R2 0.4685770273208618\n",
      "epoch 1997, loss 0.013686650432646275, R2 0.4311002492904663\n",
      "Eval loss 0.013795423321425915, R2 0.4686148166656494\n",
      "epoch 1998, loss 0.013685745187103748, R2 0.43113768100738525\n",
      "Eval loss 0.013794445432722569, R2 0.4686526656150818\n",
      "epoch 1999, loss 0.013684842735528946, R2 0.4311748147010803\n",
      "Eval loss 0.013793464750051498, R2 0.4686899781227112\n",
      "epoch 2000, loss 0.013683941215276718, R2 0.4312132000923157\n",
      "Eval loss 0.013792486861348152, R2 0.46872806549072266\n",
      "epoch 2001, loss 0.01368303969502449, R2 0.4312499761581421\n",
      "Eval loss 0.013791510835289955, R2 0.4687652587890625\n",
      "epoch 2002, loss 0.013682138174772263, R2 0.43128758668899536\n",
      "Eval loss 0.013790532946586609, R2 0.4688032269477844\n",
      "epoch 2003, loss 0.013681239448487759, R2 0.4313247799873352\n",
      "Eval loss 0.013789557851850986, R2 0.4688407778739929\n",
      "epoch 2004, loss 0.013680340722203255, R2 0.43136268854141235\n",
      "Eval loss 0.013788582757115364, R2 0.4688783288002014\n",
      "epoch 2005, loss 0.013679440133273602, R2 0.431399405002594\n",
      "Eval loss 0.013787606731057167, R2 0.46891576051712036\n",
      "epoch 2006, loss 0.013678542338311672, R2 0.4314368963241577\n",
      "Eval loss 0.01378663256764412, R2 0.46895313262939453\n",
      "epoch 2007, loss 0.013677643612027168, R2 0.4314742684364319\n",
      "Eval loss 0.013785659335553646, R2 0.468991219997406\n",
      "epoch 2008, loss 0.013676749542355537, R2 0.43151146173477173\n",
      "Eval loss 0.013784687966108322, R2 0.4690283536911011\n",
      "epoch 2009, loss 0.013675852678716183, R2 0.4315488934516907\n",
      "Eval loss 0.013783717527985573, R2 0.46906542778015137\n",
      "epoch 2010, loss 0.013674959540367126, R2 0.4315863847732544\n",
      "Eval loss 0.013782747089862823, R2 0.46910279989242554\n",
      "epoch 2011, loss 0.013674062676727772, R2 0.43162328004837036\n",
      "Eval loss 0.013781776651740074, R2 0.4691401720046997\n",
      "epoch 2012, loss 0.013673169538378716, R2 0.4316602349281311\n",
      "Eval loss 0.01378080528229475, R2 0.4691780209541321\n",
      "epoch 2013, loss 0.013672275468707085, R2 0.43169736862182617\n",
      "Eval loss 0.013779837638139725, R2 0.46921485662460327\n",
      "epoch 2014, loss 0.013671383261680603, R2 0.43173450231552124\n",
      "Eval loss 0.013778870925307274, R2 0.46925240755081177\n",
      "epoch 2015, loss 0.013670491054654121, R2 0.4317721128463745\n",
      "Eval loss 0.013777904212474823, R2 0.4692896604537964\n",
      "epoch 2016, loss 0.013669600710272789, R2 0.4318087697029114\n",
      "Eval loss 0.013776935636997223, R2 0.4693269729614258\n",
      "epoch 2017, loss 0.013668708503246307, R2 0.4318462014198303\n",
      "Eval loss 0.013775970786809921, R2 0.4693641662597656\n",
      "epoch 2018, loss 0.013667820952832699, R2 0.4318832755088806\n",
      "Eval loss 0.013775006867945194, R2 0.46940094232559204\n",
      "epoch 2019, loss 0.013666931539773941, R2 0.43191951513290405\n",
      "Eval loss 0.013774041086435318, R2 0.4694383144378662\n",
      "epoch 2020, loss 0.013666042126715183, R2 0.4319564700126648\n",
      "Eval loss 0.013773078098893166, R2 0.4694756865501404\n",
      "epoch 2021, loss 0.013665153644979, R2 0.43199360370635986\n",
      "Eval loss 0.013772116042673588, R2 0.46951228380203247\n",
      "epoch 2022, loss 0.013664268888533115, R2 0.43203020095825195\n",
      "Eval loss 0.013771153055131435, R2 0.46954935789108276\n",
      "epoch 2023, loss 0.013663380406796932, R2 0.4320670962333679\n",
      "Eval loss 0.013770190067589283, R2 0.46958649158477783\n",
      "epoch 2024, loss 0.013662495650351048, R2 0.43210387229919434\n",
      "Eval loss 0.013769229874014854, R2 0.46962422132492065\n",
      "epoch 2025, loss 0.013661609031260014, R2 0.43214112520217896\n",
      "Eval loss 0.013768271543085575, R2 0.46966052055358887\n",
      "epoch 2026, loss 0.013660725206136703, R2 0.43217748403549194\n",
      "Eval loss 0.013767312280833721, R2 0.4696974754333496\n",
      "epoch 2027, loss 0.013659841381013393, R2 0.43221473693847656\n",
      "Eval loss 0.013766353949904442, R2 0.46973472833633423\n",
      "epoch 2028, loss 0.013658958487212658, R2 0.4322514533996582\n",
      "Eval loss 0.013765396550297737, R2 0.469771146774292\n",
      "epoch 2029, loss 0.013658077456057072, R2 0.43228811025619507\n",
      "Eval loss 0.013764440082013607, R2 0.46980828046798706\n",
      "epoch 2030, loss 0.013657194562256336, R2 0.43232446908950806\n",
      "Eval loss 0.013763482682406902, R2 0.4698448181152344\n",
      "epoch 2031, loss 0.01365631353110075, R2 0.432361364364624\n",
      "Eval loss 0.013762527145445347, R2 0.4698817729949951\n",
      "epoch 2032, loss 0.013655433431267738, R2 0.43239742517471313\n",
      "Eval loss 0.01376157347112894, R2 0.4699183702468872\n",
      "epoch 2033, loss 0.013654553331434727, R2 0.4324338436126709\n",
      "Eval loss 0.013760616071522236, R2 0.4699552655220032\n",
      "epoch 2034, loss 0.013653675094246864, R2 0.43247056007385254\n",
      "Eval loss 0.013759664259850979, R2 0.46999192237854004\n",
      "epoch 2035, loss 0.013652797788381577, R2 0.4325076937675476\n",
      "Eval loss 0.013758713379502296, R2 0.47002851963043213\n",
      "epoch 2036, loss 0.013651919551193714, R2 0.4325435161590576\n",
      "Eval loss 0.013757758773863316, R2 0.4700655937194824\n",
      "epoch 2037, loss 0.013651040382683277, R2 0.43258005380630493\n",
      "Eval loss 0.013756808824837208, R2 0.4701021909713745\n",
      "epoch 2038, loss 0.013650165870785713, R2 0.4326164126396179\n",
      "Eval loss 0.01375585701316595, R2 0.4701387286186218\n",
      "epoch 2039, loss 0.013649290427565575, R2 0.432653546333313\n",
      "Eval loss 0.013754907995462418, R2 0.47017574310302734\n",
      "epoch 2040, loss 0.013648414984345436, R2 0.43268918991088867\n",
      "Eval loss 0.013753958977758884, R2 0.4702119827270508\n",
      "epoch 2041, loss 0.013647540472447872, R2 0.43272554874420166\n",
      "Eval loss 0.013753009960055351, R2 0.4702486991882324\n",
      "epoch 2042, loss 0.013646667823195457, R2 0.43276160955429077\n",
      "Eval loss 0.013752061873674393, R2 0.4702853560447693\n",
      "epoch 2043, loss 0.013645793311297894, R2 0.43279796838760376\n",
      "Eval loss 0.013751113787293434, R2 0.47032129764556885\n",
      "epoch 2044, loss 0.013644920662045479, R2 0.4328346252441406\n",
      "Eval loss 0.0137501684948802, R2 0.4703580141067505\n",
      "epoch 2045, loss 0.013644048944115639, R2 0.43287068605422974\n",
      "Eval loss 0.013749223202466965, R2 0.47039443254470825\n",
      "epoch 2046, loss 0.013643177226185799, R2 0.4329069256782532\n",
      "Eval loss 0.013748278841376305, R2 0.4704306125640869\n",
      "epoch 2047, loss 0.013642307370901108, R2 0.4329432249069214\n",
      "Eval loss 0.01374733354896307, R2 0.4704675078392029\n",
      "epoch 2048, loss 0.013641437515616417, R2 0.4329790472984314\n",
      "Eval loss 0.013746388256549835, R2 0.4705032706260681\n",
      "epoch 2049, loss 0.013640567660331726, R2 0.4330153465270996\n",
      "Eval loss 0.013745448552072048, R2 0.47053998708724976\n",
      "epoch 2050, loss 0.013639699667692184, R2 0.4330514073371887\n",
      "Eval loss 0.013744505122303963, R2 0.4705764651298523\n",
      "epoch 2051, loss 0.013638831675052643, R2 0.4330875277519226\n",
      "Eval loss 0.013743563555181026, R2 0.470612108707428\n",
      "epoch 2052, loss 0.01363796554505825, R2 0.43312370777130127\n",
      "Eval loss 0.013742621056735516, R2 0.47064870595932007\n",
      "epoch 2053, loss 0.013637098483741283, R2 0.4331597685813904\n",
      "Eval loss 0.013741683214902878, R2 0.47068482637405396\n",
      "epoch 2054, loss 0.013636231422424316, R2 0.4331957697868347\n",
      "Eval loss 0.01374074351042509, R2 0.47072136402130127\n",
      "epoch 2055, loss 0.013635366223752499, R2 0.43323153257369995\n",
      "Eval loss 0.013739805668592453, R2 0.4707568883895874\n",
      "epoch 2056, loss 0.013634501956403255, R2 0.4332674741744995\n",
      "Eval loss 0.01373886689543724, R2 0.47079330682754517\n",
      "epoch 2057, loss 0.013633638620376587, R2 0.4333033561706543\n",
      "Eval loss 0.013737929984927177, R2 0.47082942724227905\n",
      "epoch 2058, loss 0.013632775284349918, R2 0.4333392381668091\n",
      "Eval loss 0.01373699214309454, R2 0.4708653688430786\n",
      "epoch 2059, loss 0.013631912879645824, R2 0.43337488174438477\n",
      "Eval loss 0.0137360580265522, R2 0.4709011912345886\n",
      "epoch 2060, loss 0.013631051406264305, R2 0.43341147899627686\n",
      "Eval loss 0.013735122978687286, R2 0.4709378480911255\n",
      "epoch 2061, loss 0.013630191795527935, R2 0.43344664573669434\n",
      "Eval loss 0.013734186999499798, R2 0.47097402811050415\n",
      "epoch 2062, loss 0.013629329390823841, R2 0.43348228931427\n",
      "Eval loss 0.013733254745602608, R2 0.47100919485092163\n",
      "epoch 2063, loss 0.013628470711410046, R2 0.4335181713104248\n",
      "Eval loss 0.013732322491705418, R2 0.4710453748703003\n",
      "epoch 2064, loss 0.013627609238028526, R2 0.4335539937019348\n",
      "Eval loss 0.013731389306485653, R2 0.47108137607574463\n",
      "epoch 2065, loss 0.01362675242125988, R2 0.43358975648880005\n",
      "Eval loss 0.013730457983911037, R2 0.47111719846725464\n",
      "epoch 2066, loss 0.01362589281052351, R2 0.43362587690353394\n",
      "Eval loss 0.013729525730013847, R2 0.4711533188819885\n",
      "epoch 2067, loss 0.013625035993754864, R2 0.43366092443466187\n",
      "Eval loss 0.013728595338761806, R2 0.4711889624595642\n",
      "epoch 2068, loss 0.013624178245663643, R2 0.4336966276168823\n",
      "Eval loss 0.01372766587883234, R2 0.47122490406036377\n",
      "epoch 2069, loss 0.013623322360217571, R2 0.43373286724090576\n",
      "Eval loss 0.013726736418902874, R2 0.47126060724258423\n",
      "epoch 2070, loss 0.0136224664747715, R2 0.43376797437667847\n",
      "Eval loss 0.013725809752941132, R2 0.47129642963409424\n",
      "epoch 2071, loss 0.013621613383293152, R2 0.4338037371635437\n",
      "Eval loss 0.013724882155656815, R2 0.47133201360702515\n",
      "epoch 2072, loss 0.013620758429169655, R2 0.4338393211364746\n",
      "Eval loss 0.013723955489695072, R2 0.4713675379753113\n",
      "epoch 2073, loss 0.013619904406368732, R2 0.433874249458313\n",
      "Eval loss 0.013723027892410755, R2 0.47140324115753174\n",
      "epoch 2074, loss 0.013619051314890385, R2 0.433910071849823\n",
      "Eval loss 0.013722101226449013, R2 0.47143882513046265\n",
      "epoch 2075, loss 0.013618199154734612, R2 0.4339451193809509\n",
      "Eval loss 0.013721178285777569, R2 0.4714743494987488\n",
      "epoch 2076, loss 0.013617347925901413, R2 0.4339805245399475\n",
      "Eval loss 0.013720252551138401, R2 0.47151046991348267\n",
      "epoch 2077, loss 0.013616496697068214, R2 0.4340158700942993\n",
      "Eval loss 0.013719329610466957, R2 0.47154587507247925\n",
      "epoch 2078, loss 0.01361564639955759, R2 0.4340512156486511\n",
      "Eval loss 0.013718407601118088, R2 0.4715811014175415\n",
      "epoch 2079, loss 0.013614797964692116, R2 0.43408703804016113\n",
      "Eval loss 0.013717484660446644, R2 0.47161728143692017\n",
      "epoch 2080, loss 0.013613947667181492, R2 0.4341220259666443\n",
      "Eval loss 0.013716564513742924, R2 0.47165238857269287\n",
      "epoch 2081, loss 0.013613099232316017, R2 0.434157133102417\n",
      "Eval loss 0.013715642504394054, R2 0.4716874361038208\n",
      "epoch 2082, loss 0.013612252660095692, R2 0.4341922998428345\n",
      "Eval loss 0.01371472142636776, R2 0.47172337770462036\n",
      "epoch 2083, loss 0.013611404225230217, R2 0.43422794342041016\n",
      "Eval loss 0.013713804073631763, R2 0.47175920009613037\n",
      "epoch 2084, loss 0.013610557653009892, R2 0.4342629313468933\n",
      "Eval loss 0.013712884858250618, R2 0.4717943072319031\n",
      "epoch 2085, loss 0.013609712943434715, R2 0.43429839611053467\n",
      "Eval loss 0.013711967505514622, R2 0.4718295931816101\n",
      "epoch 2086, loss 0.01360886637121439, R2 0.43433302640914917\n",
      "Eval loss 0.013711050152778625, R2 0.4718645215034485\n",
      "epoch 2087, loss 0.013608021661639214, R2 0.43436819314956665\n",
      "Eval loss 0.01371013280004263, R2 0.47190016508102417\n",
      "epoch 2088, loss 0.013607176952064037, R2 0.43440401554107666\n",
      "Eval loss 0.013709216378629208, R2 0.4719354510307312\n",
      "epoch 2089, loss 0.013606333173811436, R2 0.4344383478164673\n",
      "Eval loss 0.013708301819860935, R2 0.4719708561897278\n",
      "epoch 2090, loss 0.013605491258203983, R2 0.4344731569290161\n",
      "Eval loss 0.013707388192415237, R2 0.4720062017440796\n",
      "epoch 2091, loss 0.013604649342596531, R2 0.4345085024833679\n",
      "Eval loss 0.013706473633646965, R2 0.47204142808914185\n",
      "epoch 2092, loss 0.013603807426989079, R2 0.4345434904098511\n",
      "Eval loss 0.013705560937523842, R2 0.4720762372016907\n",
      "epoch 2093, loss 0.0136029664427042, R2 0.4345782995223999\n",
      "Eval loss 0.013704647310078144, R2 0.47211146354675293\n",
      "epoch 2094, loss 0.013602127321064472, R2 0.4346131682395935\n",
      "Eval loss 0.01370373647660017, R2 0.4721466898918152\n",
      "epoch 2095, loss 0.013601288199424744, R2 0.43464863300323486\n",
      "Eval loss 0.013702822849154472, R2 0.4721815586090088\n",
      "epoch 2096, loss 0.013600447215139866, R2 0.4346829652786255\n",
      "Eval loss 0.013701913878321648, R2 0.4722169041633606\n",
      "epoch 2097, loss 0.013599609956145287, R2 0.4347177743911743\n",
      "Eval loss 0.013701003976166248, R2 0.4722517728805542\n",
      "epoch 2098, loss 0.013598771765828133, R2 0.4347531795501709\n",
      "Eval loss 0.013700093142688274, R2 0.47228652238845825\n",
      "epoch 2099, loss 0.013597934506833553, R2 0.4347876310348511\n",
      "Eval loss 0.013699186034500599, R2 0.4723218083381653\n",
      "epoch 2100, loss 0.013597098179161549, R2 0.4348222017288208\n",
      "Eval loss 0.0136982761323452, R2 0.4723571538925171\n",
      "epoch 2101, loss 0.013596261851489544, R2 0.43485695123672485\n",
      "Eval loss 0.013697370886802673, R2 0.4723917245864868\n",
      "epoch 2102, loss 0.013595426455140114, R2 0.4348917007446289\n",
      "Eval loss 0.013696463778614998, R2 0.4724263548851013\n",
      "epoch 2103, loss 0.013594591990113258, R2 0.43492692708969116\n",
      "Eval loss 0.013695557601749897, R2 0.4724617004394531\n",
      "epoch 2104, loss 0.013593759387731552, R2 0.4349616765975952\n",
      "Eval loss 0.01369465235620737, R2 0.4724963903427124\n",
      "epoch 2105, loss 0.013592924922704697, R2 0.43499618768692017\n",
      "Eval loss 0.013693748041987419, R2 0.472531259059906\n",
      "epoch 2106, loss 0.013592090457677841, R2 0.4350305199623108\n",
      "Eval loss 0.013692844659090042, R2 0.47256577014923096\n",
      "epoch 2107, loss 0.01359125878661871, R2 0.4350649118423462\n",
      "Eval loss 0.013691941276192665, R2 0.47260087728500366\n",
      "epoch 2108, loss 0.013590427115559578, R2 0.4350993037223816\n",
      "Eval loss 0.013691037893295288, R2 0.47263532876968384\n",
      "epoch 2109, loss 0.013589595444500446, R2 0.4351338744163513\n",
      "Eval loss 0.01369013637304306, R2 0.4726705551147461\n",
      "epoch 2110, loss 0.013588767498731613, R2 0.4351683259010315\n",
      "Eval loss 0.013689233921468258, R2 0.47270500659942627\n",
      "epoch 2111, loss 0.013587936758995056, R2 0.4352031946182251\n",
      "Eval loss 0.013688335195183754, R2 0.472739577293396\n",
      "epoch 2112, loss 0.013587107881903648, R2 0.4352380037307739\n",
      "Eval loss 0.0136874346062541, R2 0.4727744460105896\n",
      "epoch 2113, loss 0.01358627900481224, R2 0.4352719187736511\n",
      "Eval loss 0.013686534948647022, R2 0.4728090763092041\n",
      "epoch 2114, loss 0.013585451059043407, R2 0.4353063106536865\n",
      "Eval loss 0.013685636222362518, R2 0.47284388542175293\n",
      "epoch 2115, loss 0.013584623113274574, R2 0.435341477394104\n",
      "Eval loss 0.013684739358723164, R2 0.47287845611572266\n",
      "epoch 2116, loss 0.01358379703015089, R2 0.43537527322769165\n",
      "Eval loss 0.013683841563761234, R2 0.47291284799575806\n",
      "epoch 2117, loss 0.013582971878349781, R2 0.4354097843170166\n",
      "Eval loss 0.01368294470012188, R2 0.4729469418525696\n",
      "epoch 2118, loss 0.013582145795226097, R2 0.43544429540634155\n",
      "Eval loss 0.013682049699127674, R2 0.47298187017440796\n",
      "epoch 2119, loss 0.013581320643424988, R2 0.43547821044921875\n",
      "Eval loss 0.01368115283548832, R2 0.47301626205444336\n",
      "epoch 2120, loss 0.013580496422946453, R2 0.4355122447013855\n",
      "Eval loss 0.013680257834494114, R2 0.47305089235305786\n",
      "epoch 2121, loss 0.013579673133790493, R2 0.43554651737213135\n",
      "Eval loss 0.013679363764822483, R2 0.47308528423309326\n",
      "epoch 2122, loss 0.013578848913311958, R2 0.4355813264846802\n",
      "Eval loss 0.013678469695150852, R2 0.47311991453170776\n",
      "epoch 2123, loss 0.013578026555478573, R2 0.4356149435043335\n",
      "Eval loss 0.013677578419446945, R2 0.4731542468070984\n",
      "epoch 2124, loss 0.013577206060290337, R2 0.43564921617507935\n",
      "Eval loss 0.013676686212420464, R2 0.47318828105926514\n",
      "epoch 2125, loss 0.0135763855651021, R2 0.43568313121795654\n",
      "Eval loss 0.013675793074071407, R2 0.4732227325439453\n",
      "epoch 2126, loss 0.01357556413859129, R2 0.43571728467941284\n",
      "Eval loss 0.01367490366101265, R2 0.47325682640075684\n",
      "epoch 2127, loss 0.013574743643403053, R2 0.43575143814086914\n",
      "Eval loss 0.013674013316631317, R2 0.4732917547225952\n",
      "epoch 2128, loss 0.013573923148214817, R2 0.43578553199768066\n",
      "Eval loss 0.01367312390357256, R2 0.4733254909515381\n",
      "epoch 2129, loss 0.01357310451567173, R2 0.43581950664520264\n",
      "Eval loss 0.013672234490513802, R2 0.4733596444129944\n",
      "epoch 2130, loss 0.013572287745773792, R2 0.4358534812927246\n",
      "Eval loss 0.013671346940100193, R2 0.4733937978744507\n",
      "epoch 2131, loss 0.013571469113230705, R2 0.43588805198669434\n",
      "Eval loss 0.013670457527041435, R2 0.4734281897544861\n",
      "epoch 2132, loss 0.013570652343332767, R2 0.43592166900634766\n",
      "Eval loss 0.013669573701918125, R2 0.47346240282058716\n",
      "epoch 2133, loss 0.013569836504757404, R2 0.43595534563064575\n",
      "Eval loss 0.013668686151504517, R2 0.47349631786346436\n",
      "epoch 2134, loss 0.013569020666182041, R2 0.4359893202781677\n",
      "Eval loss 0.013667801395058632, R2 0.4735311269760132\n",
      "epoch 2135, loss 0.013568205758929253, R2 0.43602317571640015\n",
      "Eval loss 0.013666917569935322, R2 0.47356486320495605\n",
      "epoch 2136, loss 0.013567390851676464, R2 0.4360572099685669\n",
      "Eval loss 0.013666031882166862, R2 0.47359901666641235\n",
      "epoch 2137, loss 0.01356657687574625, R2 0.4360910654067993\n",
      "Eval loss 0.013665148057043552, R2 0.47363293170928955\n",
      "epoch 2138, loss 0.013565764762461185, R2 0.43612462282180786\n",
      "Eval loss 0.013664266094565392, R2 0.4736672043800354\n",
      "epoch 2139, loss 0.013564949855208397, R2 0.4361591935157776\n",
      "Eval loss 0.013663383200764656, R2 0.47370100021362305\n",
      "epoch 2140, loss 0.013564139604568481, R2 0.4361921548843384\n",
      "Eval loss 0.01366250030696392, R2 0.47373485565185547\n",
      "epoch 2141, loss 0.013563327491283417, R2 0.43622589111328125\n",
      "Eval loss 0.01366161834448576, R2 0.4737687110900879\n",
      "epoch 2142, loss 0.013562515377998352, R2 0.4362596869468689\n",
      "Eval loss 0.013660740107297897, R2 0.473802387714386\n",
      "epoch 2143, loss 0.013561706990003586, R2 0.4362933039665222\n",
      "Eval loss 0.01365986093878746, R2 0.47383642196655273\n",
      "epoch 2144, loss 0.01356089673936367, R2 0.4363269805908203\n",
      "Eval loss 0.013658979907631874, R2 0.4738701581954956\n",
      "epoch 2145, loss 0.01356008555740118, R2 0.4363607168197632\n",
      "Eval loss 0.013658101670444012, R2 0.47390443086624146\n",
      "epoch 2146, loss 0.013559277169406414, R2 0.43639427423477173\n",
      "Eval loss 0.013657224364578724, R2 0.4739379286766052\n",
      "epoch 2147, loss 0.013558469712734222, R2 0.43642836809158325\n",
      "Eval loss 0.013656347058713436, R2 0.4739716053009033\n",
      "epoch 2148, loss 0.013557663187384605, R2 0.4364619255065918\n",
      "Eval loss 0.013655469752848148, R2 0.4740058183670044\n",
      "epoch 2149, loss 0.01355685479938984, R2 0.43649476766586304\n",
      "Eval loss 0.01365459244698286, R2 0.4740391969680786\n",
      "epoch 2150, loss 0.013556049205362797, R2 0.43652862310409546\n",
      "Eval loss 0.013653717935085297, R2 0.4740731716156006\n",
      "epoch 2151, loss 0.013555244542658329, R2 0.4365624189376831\n",
      "Eval loss 0.013652843423187733, R2 0.4741068482398987\n",
      "epoch 2152, loss 0.013554439879953861, R2 0.4365953803062439\n",
      "Eval loss 0.013651970773935318, R2 0.4741401672363281\n",
      "epoch 2153, loss 0.013553634285926819, R2 0.4366295337677002\n",
      "Eval loss 0.013651096262037754, R2 0.4741743206977844\n",
      "epoch 2154, loss 0.013552829623222351, R2 0.4366624355316162\n",
      "Eval loss 0.013650222681462765, R2 0.47420763969421387\n",
      "epoch 2155, loss 0.013552026823163033, R2 0.4366956353187561\n",
      "Eval loss 0.013649350963532925, R2 0.47424155473709106\n",
      "epoch 2156, loss 0.013551224023103714, R2 0.436729371547699\n",
      "Eval loss 0.01364848017692566, R2 0.47427505254745483\n",
      "epoch 2157, loss 0.01355042215436697, R2 0.43676310777664185\n",
      "Eval loss 0.013647609390318394, R2 0.4743087887763977\n",
      "epoch 2158, loss 0.0135496212169528, R2 0.43679583072662354\n",
      "Eval loss 0.013646737672388554, R2 0.47434186935424805\n",
      "epoch 2159, loss 0.013548819348216057, R2 0.436829149723053\n",
      "Eval loss 0.013645869679749012, R2 0.47437548637390137\n",
      "epoch 2160, loss 0.013548018410801888, R2 0.4368622899055481\n",
      "Eval loss 0.013644999824464321, R2 0.47440868616104126\n",
      "epoch 2161, loss 0.013547218404710293, R2 0.43689554929733276\n",
      "Eval loss 0.013644132763147354, R2 0.4744422435760498\n",
      "epoch 2162, loss 0.013546421192586422, R2 0.4369294047355652\n",
      "Eval loss 0.013643264770507812, R2 0.4744756817817688\n",
      "epoch 2163, loss 0.013545622117817402, R2 0.4369618892669678\n",
      "Eval loss 0.013642395846545696, R2 0.4745092988014221\n",
      "epoch 2164, loss 0.013544824905693531, R2 0.4369950294494629\n",
      "Eval loss 0.013641531579196453, R2 0.4745425581932068\n",
      "epoch 2165, loss 0.013544025830924511, R2 0.4370289444923401\n",
      "Eval loss 0.013640664517879486, R2 0.4745759963989258\n",
      "epoch 2166, loss 0.013543229550123215, R2 0.43706202507019043\n",
      "Eval loss 0.013639799319207668, R2 0.47460949420928955\n",
      "epoch 2167, loss 0.013542432337999344, R2 0.43709444999694824\n",
      "Eval loss 0.013638935051858425, R2 0.4746427536010742\n",
      "epoch 2168, loss 0.013541636988520622, R2 0.43712764978408813\n",
      "Eval loss 0.013638070784509182, R2 0.47467559576034546\n",
      "epoch 2169, loss 0.0135408416390419, R2 0.4371607303619385\n",
      "Eval loss 0.013637207448482513, R2 0.4747094511985779\n",
      "epoch 2170, loss 0.013540047220885754, R2 0.4371935725212097\n",
      "Eval loss 0.013636344112455845, R2 0.4747425317764282\n",
      "epoch 2171, loss 0.013539253734052181, R2 0.4372265338897705\n",
      "Eval loss 0.013635481707751751, R2 0.47477561235427856\n",
      "epoch 2172, loss 0.013538459315896034, R2 0.4372595548629761\n",
      "Eval loss 0.013634620234370232, R2 0.4748087525367737\n",
      "epoch 2173, loss 0.013537667691707611, R2 0.43729251623153687\n",
      "Eval loss 0.013633759692311287, R2 0.47484225034713745\n",
      "epoch 2174, loss 0.013536875136196613, R2 0.43732601404190063\n",
      "Eval loss 0.013632900081574917, R2 0.47487473487854004\n",
      "epoch 2175, loss 0.01353608351200819, R2 0.43735891580581665\n",
      "Eval loss 0.013632040470838547, R2 0.47490835189819336\n",
      "epoch 2176, loss 0.013535291887819767, R2 0.437391996383667\n",
      "Eval loss 0.013631182722747326, R2 0.4749414920806885\n",
      "epoch 2177, loss 0.013534501194953918, R2 0.43742483854293823\n",
      "Eval loss 0.013630323112010956, R2 0.4749743342399597\n",
      "epoch 2178, loss 0.013533711433410645, R2 0.43745678663253784\n",
      "Eval loss 0.013629465363919735, R2 0.4750070571899414\n",
      "epoch 2179, loss 0.013532922603189945, R2 0.43748974800109863\n",
      "Eval loss 0.01362860668450594, R2 0.4750405550003052\n",
      "epoch 2180, loss 0.01353213470429182, R2 0.4375224709510803\n",
      "Eval loss 0.013627750799059868, R2 0.47507327795028687\n",
      "epoch 2181, loss 0.013531345874071121, R2 0.4375552535057068\n",
      "Eval loss 0.013626894913613796, R2 0.47510653734207153\n",
      "epoch 2182, loss 0.013530557975172997, R2 0.43758803606033325\n",
      "Eval loss 0.013626040890812874, R2 0.47513914108276367\n",
      "epoch 2183, loss 0.013529771938920021, R2 0.43762069940567017\n",
      "Eval loss 0.013625185936689377, R2 0.47517234086990356\n",
      "epoch 2184, loss 0.013528983108699322, R2 0.437653124332428\n",
      "Eval loss 0.013624331913888454, R2 0.47520512342453003\n",
      "epoch 2185, loss 0.013528198003768921, R2 0.43768608570098877\n",
      "Eval loss 0.013623478822410107, R2 0.47523796558380127\n",
      "epoch 2186, loss 0.013527411967515945, R2 0.43771892786026\n",
      "Eval loss 0.013622625730931759, R2 0.47527098655700684\n",
      "epoch 2187, loss 0.01352662779390812, R2 0.43775177001953125\n",
      "Eval loss 0.013621772639453411, R2 0.47530364990234375\n",
      "epoch 2188, loss 0.013525842688977718, R2 0.43778419494628906\n",
      "Eval loss 0.013620921410620213, R2 0.475336492061615\n",
      "epoch 2189, loss 0.013525059446692467, R2 0.4378165602684021\n",
      "Eval loss 0.013620070181787014, R2 0.4753692150115967\n",
      "epoch 2190, loss 0.01352427713572979, R2 0.43784910440444946\n",
      "Eval loss 0.013619220815598965, R2 0.4754018187522888\n",
      "epoch 2191, loss 0.013523494824767113, R2 0.4378816485404968\n",
      "Eval loss 0.01361837051808834, R2 0.4754346013069153\n",
      "epoch 2192, loss 0.013522711582481861, R2 0.4379141926765442\n",
      "Eval loss 0.013617521151900291, R2 0.47546714544296265\n",
      "epoch 2193, loss 0.013521932065486908, R2 0.437946617603302\n",
      "Eval loss 0.013616673648357391, R2 0.47550010681152344\n",
      "epoch 2194, loss 0.013521150685846806, R2 0.4379795789718628\n",
      "Eval loss 0.013615823350846767, R2 0.4755328297615051\n",
      "epoch 2195, loss 0.013520370237529278, R2 0.4380114674568176\n",
      "Eval loss 0.013614975847303867, R2 0.4755656123161316\n",
      "epoch 2196, loss 0.013519590720534325, R2 0.43804389238357544\n",
      "Eval loss 0.013614130206406116, R2 0.47559839487075806\n",
      "epoch 2197, loss 0.013518811203539371, R2 0.4380764961242676\n",
      "Eval loss 0.01361328549683094, R2 0.4756302833557129\n",
      "epoch 2198, loss 0.013518032617866993, R2 0.43810880184173584\n",
      "Eval loss 0.01361243799328804, R2 0.47566336393356323\n",
      "epoch 2199, loss 0.013517254963517189, R2 0.43814152479171753\n",
      "Eval loss 0.013611593283712864, R2 0.47569549083709717\n",
      "epoch 2200, loss 0.013516477309167385, R2 0.43817347288131714\n",
      "Eval loss 0.013610750436782837, R2 0.475727915763855\n",
      "epoch 2201, loss 0.01351570151746273, R2 0.4382055401802063\n",
      "Eval loss 0.013609906658530235, R2 0.47576045989990234\n",
      "epoch 2202, loss 0.013514923863112926, R2 0.43823784589767456\n",
      "Eval loss 0.013609061948955059, R2 0.47579342126846313\n",
      "epoch 2203, loss 0.013514149002730846, R2 0.4382697343826294\n",
      "Eval loss 0.013608220033347607, R2 0.47582554817199707\n",
      "epoch 2204, loss 0.013513373211026192, R2 0.43830233812332153\n",
      "Eval loss 0.013607379049062729, R2 0.47585779428482056\n",
      "epoch 2205, loss 0.01351260021328926, R2 0.43833446502685547\n",
      "Eval loss 0.013606538064777851, R2 0.4758908152580261\n",
      "epoch 2206, loss 0.013511823490262032, R2 0.43836724758148193\n",
      "Eval loss 0.013605697080492973, R2 0.47592276334762573\n",
      "epoch 2207, loss 0.013511051423847675, R2 0.4383995532989502\n",
      "Eval loss 0.01360485702753067, R2 0.47595494985580444\n",
      "epoch 2208, loss 0.013510276563465595, R2 0.4384315609931946\n",
      "Eval loss 0.013604016974568367, R2 0.47598743438720703\n",
      "epoch 2209, loss 0.013509506359696388, R2 0.4384632706642151\n",
      "Eval loss 0.013603177852928638, R2 0.4760197401046753\n",
      "epoch 2210, loss 0.013508732430636883, R2 0.4384952187538147\n",
      "Eval loss 0.01360234059393406, R2 0.4760521650314331\n",
      "epoch 2211, loss 0.013507962226867676, R2 0.4385272264480591\n",
      "Eval loss 0.01360150147229433, R2 0.4760841727256775\n",
      "epoch 2212, loss 0.013507191091775894, R2 0.43855929374694824\n",
      "Eval loss 0.0136006660759449, R2 0.4761166572570801\n",
      "epoch 2213, loss 0.013506420888006687, R2 0.43859148025512695\n",
      "Eval loss 0.013599829748272896, R2 0.47614890336990356\n",
      "epoch 2214, loss 0.01350565254688263, R2 0.43862342834472656\n",
      "Eval loss 0.013598993420600891, R2 0.4761807918548584\n",
      "epoch 2215, loss 0.013504883274435997, R2 0.4386557936668396\n",
      "Eval loss 0.013598158024251461, R2 0.4762134552001953\n",
      "epoch 2216, loss 0.013504114001989365, R2 0.43868738412857056\n",
      "Eval loss 0.013597322627902031, R2 0.4762454628944397\n",
      "epoch 2217, loss 0.013503346592187881, R2 0.4387192726135254\n",
      "Eval loss 0.013596490025520325, R2 0.4762777090072632\n",
      "epoch 2218, loss 0.013502581045031548, R2 0.4387509226799011\n",
      "Eval loss 0.01359565556049347, R2 0.4763096570968628\n",
      "epoch 2219, loss 0.013501811772584915, R2 0.43878358602523804\n",
      "Eval loss 0.013594823889434338, R2 0.4763414263725281\n",
      "epoch 2220, loss 0.013501045294106007, R2 0.43881475925445557\n",
      "Eval loss 0.013593992218375206, R2 0.47637373208999634\n",
      "epoch 2221, loss 0.013500280678272247, R2 0.43884652853012085\n",
      "Eval loss 0.013593160547316074, R2 0.4764057993888855\n",
      "epoch 2222, loss 0.013499513268470764, R2 0.43887895345687866\n",
      "Eval loss 0.013592328876256943, R2 0.476437509059906\n",
      "epoch 2223, loss 0.013498750515282154, R2 0.4389103055000305\n",
      "Eval loss 0.013591498136520386, R2 0.47646963596343994\n",
      "epoch 2224, loss 0.013497985899448395, R2 0.4389426112174988\n",
      "Eval loss 0.013590667396783829, R2 0.4765019416809082\n",
      "epoch 2225, loss 0.01349722407758236, R2 0.4389737844467163\n",
      "Eval loss 0.013589838519692421, R2 0.4765337109565735\n",
      "epoch 2226, loss 0.01349645759910345, R2 0.439005970954895\n",
      "Eval loss 0.013589009642601013, R2 0.4765658378601074\n",
      "epoch 2227, loss 0.01349569670855999, R2 0.4390370845794678\n",
      "Eval loss 0.01358818169683218, R2 0.4765974283218384\n",
      "epoch 2228, loss 0.013494935818016529, R2 0.43906867504119873\n",
      "Eval loss 0.013587354682385921, R2 0.47662943601608276\n",
      "epoch 2229, loss 0.013494173064827919, R2 0.43910038471221924\n",
      "Eval loss 0.013586526736617088, R2 0.4766613245010376\n",
      "epoch 2230, loss 0.013493414036929607, R2 0.4391319751739502\n",
      "Eval loss 0.013585700653493404, R2 0.4766930937767029\n",
      "epoch 2231, loss 0.013492652215063572, R2 0.43916380405426025\n",
      "Eval loss 0.013584875501692295, R2 0.47672492265701294\n",
      "epoch 2232, loss 0.013491892255842686, R2 0.43919575214385986\n",
      "Eval loss 0.013584049418568611, R2 0.4767564535140991\n",
      "epoch 2233, loss 0.0134911322966218, R2 0.43922680616378784\n",
      "Eval loss 0.013583226129412651, R2 0.4767884612083435\n",
      "epoch 2234, loss 0.013490376062691212, R2 0.4392582178115845\n",
      "Eval loss 0.013582402840256691, R2 0.47682029008865356\n",
      "epoch 2235, loss 0.0134896170347929, R2 0.43928974866867065\n",
      "Eval loss 0.013581579551100731, R2 0.47685158252716064\n",
      "epoch 2236, loss 0.013488858938217163, R2 0.43932199478149414\n",
      "Eval loss 0.013580757193267345, R2 0.4768834114074707\n",
      "epoch 2237, loss 0.013488102704286575, R2 0.4393528699874878\n",
      "Eval loss 0.013579933904111385, R2 0.4769149422645569\n",
      "epoch 2238, loss 0.013487345539033413, R2 0.4393841624259949\n",
      "Eval loss 0.013579112477600574, R2 0.4769470691680908\n",
      "epoch 2239, loss 0.013486591167747974, R2 0.4394155740737915\n",
      "Eval loss 0.013578291051089764, R2 0.4769783616065979\n",
      "epoch 2240, loss 0.013485834002494812, R2 0.4394470453262329\n",
      "Eval loss 0.013577470555901527, R2 0.4770103096961975\n",
      "epoch 2241, loss 0.013485079631209373, R2 0.43947839736938477\n",
      "Eval loss 0.013576650992035866, R2 0.4770416021347046\n",
      "epoch 2242, loss 0.01348432619124651, R2 0.4395102262496948\n",
      "Eval loss 0.01357583049684763, R2 0.4770730137825012\n",
      "epoch 2243, loss 0.013483571819961071, R2 0.4395408630371094\n",
      "Eval loss 0.013575012795627117, R2 0.4771047830581665\n",
      "epoch 2244, loss 0.013482818379998207, R2 0.43957287073135376\n",
      "Eval loss 0.013574195094406605, R2 0.4771360158920288\n",
      "epoch 2245, loss 0.013482065871357918, R2 0.4396038055419922\n",
      "Eval loss 0.013573377393186092, R2 0.4771677851676941\n",
      "epoch 2246, loss 0.013481314294040203, R2 0.43963485956192017\n",
      "Eval loss 0.013572560623288155, R2 0.47719913721084595\n",
      "epoch 2247, loss 0.013480561785399914, R2 0.4396666884422302\n",
      "Eval loss 0.013571744784712791, R2 0.4772305488586426\n",
      "epoch 2248, loss 0.013479812070727348, R2 0.4396973252296448\n",
      "Eval loss 0.013570928946137428, R2 0.4772619605064392\n",
      "epoch 2249, loss 0.013479061424732208, R2 0.4397285580635071\n",
      "Eval loss 0.013570113107562065, R2 0.47729355096817017\n",
      "epoch 2250, loss 0.013478311710059643, R2 0.43975991010665894\n",
      "Eval loss 0.013569300062954426, R2 0.477324903011322\n",
      "epoch 2251, loss 0.013477561995387077, R2 0.43979084491729736\n",
      "Eval loss 0.013568483293056488, R2 0.47735679149627686\n",
      "epoch 2252, loss 0.013476812280714512, R2 0.4398220181465149\n",
      "Eval loss 0.013567671179771423, R2 0.4773874878883362\n",
      "epoch 2253, loss 0.013476064428687096, R2 0.4398532509803772\n",
      "Eval loss 0.01356685720384121, R2 0.47741878032684326\n",
      "epoch 2254, loss 0.01347531471401453, R2 0.4398842453956604\n",
      "Eval loss 0.01356604602187872, R2 0.4774501919746399\n",
      "epoch 2255, loss 0.013474569655954838, R2 0.43991541862487793\n",
      "Eval loss 0.013565233908593655, R2 0.477481484413147\n",
      "epoch 2256, loss 0.01347382366657257, R2 0.4399462342262268\n",
      "Eval loss 0.013564424589276314, R2 0.4775123596191406\n",
      "epoch 2257, loss 0.01347307674586773, R2 0.4399774670600891\n",
      "Eval loss 0.013563612475991249, R2 0.47754406929016113\n",
      "epoch 2258, loss 0.013472329825162888, R2 0.44000834226608276\n",
      "Eval loss 0.013562803156673908, R2 0.47757482528686523\n",
      "epoch 2259, loss 0.013471586629748344, R2 0.4400397539138794\n",
      "Eval loss 0.013561993837356567, R2 0.47760629653930664\n",
      "epoch 2260, loss 0.013470839709043503, R2 0.4400702714920044\n",
      "Eval loss 0.013561184518039227, R2 0.4776371717453003\n",
      "epoch 2261, loss 0.01347009651362896, R2 0.44010114669799805\n",
      "Eval loss 0.013560377061367035, R2 0.47766828536987305\n",
      "epoch 2262, loss 0.013469352386891842, R2 0.4401320815086365\n",
      "Eval loss 0.013559568673372269, R2 0.4776993989944458\n",
      "epoch 2263, loss 0.013468611054122448, R2 0.4401629567146301\n",
      "Eval loss 0.013558761216700077, R2 0.47773051261901855\n",
      "epoch 2264, loss 0.013467865996062756, R2 0.44019389152526855\n",
      "Eval loss 0.01355795469135046, R2 0.4777621626853943\n",
      "epoch 2265, loss 0.013467125594615936, R2 0.44022446870803833\n",
      "Eval loss 0.013557150028645992, R2 0.4777928590774536\n",
      "epoch 2266, loss 0.013466384261846542, R2 0.44025546312332153\n",
      "Eval loss 0.01355634443461895, R2 0.4778240919113159\n",
      "epoch 2267, loss 0.013465641997754574, R2 0.44028669595718384\n",
      "Eval loss 0.013555538840591908, R2 0.4778546094894409\n",
      "epoch 2268, loss 0.013464901596307755, R2 0.4403170943260193\n",
      "Eval loss 0.013554735109210014, R2 0.47788602113723755\n",
      "epoch 2269, loss 0.013464163057506084, R2 0.4403477907180786\n",
      "Eval loss 0.013553931377828121, R2 0.47791701555252075\n",
      "epoch 2270, loss 0.013463422656059265, R2 0.4403785467147827\n",
      "Eval loss 0.013553126715123653, R2 0.47794800996780396\n",
      "epoch 2271, loss 0.01346268318593502, R2 0.4404093623161316\n",
      "Eval loss 0.01355232484638691, R2 0.4779787063598633\n",
      "epoch 2272, loss 0.0134619465097785, R2 0.44043993949890137\n",
      "Eval loss 0.01355152390897274, R2 0.47800928354263306\n",
      "epoch 2273, loss 0.01346120797097683, R2 0.4404706358909607\n",
      "Eval loss 0.01355072297155857, R2 0.478040874004364\n",
      "epoch 2274, loss 0.013460470363497734, R2 0.44050127267837524\n",
      "Eval loss 0.013549922034144402, R2 0.4780709743499756\n",
      "epoch 2275, loss 0.013459733687341213, R2 0.44053173065185547\n",
      "Eval loss 0.013549122028052807, R2 0.47810208797454834\n",
      "epoch 2276, loss 0.013458997942507267, R2 0.44056248664855957\n",
      "Eval loss 0.013548321090638638, R2 0.4781326651573181\n",
      "epoch 2277, loss 0.013458261266350746, R2 0.4405938386917114\n",
      "Eval loss 0.013547522015869617, R2 0.4781637191772461\n",
      "epoch 2278, loss 0.01345752738416195, R2 0.44062381982803345\n",
      "Eval loss 0.013546722009778023, R2 0.4781942367553711\n",
      "epoch 2279, loss 0.013456791639328003, R2 0.4406547546386719\n",
      "Eval loss 0.013545925728976727, R2 0.47822487354278564\n",
      "epoch 2280, loss 0.013456057757139206, R2 0.4406847357749939\n",
      "Eval loss 0.013545125722885132, R2 0.47825586795806885\n",
      "epoch 2281, loss 0.013455322943627834, R2 0.4407154321670532\n",
      "Eval loss 0.01354433037340641, R2 0.4782866835594177\n",
      "epoch 2282, loss 0.013454591855406761, R2 0.4407461881637573\n",
      "Eval loss 0.013543534092605114, R2 0.4783173203468323\n",
      "epoch 2283, loss 0.013453857973217964, R2 0.44077610969543457\n",
      "Eval loss 0.013542739674448967, R2 0.4783480763435364\n",
      "epoch 2284, loss 0.013453126884996891, R2 0.44080650806427\n",
      "Eval loss 0.013541944324970245, R2 0.4783785939216614\n",
      "epoch 2285, loss 0.013452394865453243, R2 0.44083696603775024\n",
      "Eval loss 0.013541148975491524, R2 0.4784092307090759\n",
      "epoch 2286, loss 0.013451662845909595, R2 0.4408673644065857\n",
      "Eval loss 0.013540355488657951, R2 0.47843992710113525\n",
      "epoch 2287, loss 0.013450933620333672, R2 0.44089770317077637\n",
      "Eval loss 0.013539562001824379, R2 0.4784700274467468\n",
      "epoch 2288, loss 0.013450203463435173, R2 0.44092857837677\n",
      "Eval loss 0.013538767583668232, R2 0.47850096225738525\n",
      "epoch 2289, loss 0.013449474237859249, R2 0.44095855951309204\n",
      "Eval loss 0.013537976890802383, R2 0.47853124141693115\n",
      "epoch 2290, loss 0.0134487459436059, R2 0.4409886598587036\n",
      "Eval loss 0.013537184335291386, R2 0.47856223583221436\n",
      "epoch 2291, loss 0.013448014855384827, R2 0.4410197138786316\n",
      "Eval loss 0.013536394573748112, R2 0.47859203815460205\n",
      "epoch 2292, loss 0.013447288423776627, R2 0.4410492181777954\n",
      "Eval loss 0.013535602949559689, R2 0.4786228537559509\n",
      "epoch 2293, loss 0.013446561060845852, R2 0.44108015298843384\n",
      "Eval loss 0.013534813188016415, R2 0.4786529541015625\n",
      "epoch 2294, loss 0.013445833697915077, R2 0.44111037254333496\n",
      "Eval loss 0.013534024357795715, R2 0.4786836504936218\n",
      "epoch 2295, loss 0.013445108197629452, R2 0.441139817237854\n",
      "Eval loss 0.013533234596252441, R2 0.4787137508392334\n",
      "epoch 2296, loss 0.013444381766021252, R2 0.4411700367927551\n",
      "Eval loss 0.013532445766031742, R2 0.4787442684173584\n",
      "epoch 2297, loss 0.013443658128380775, R2 0.44120103120803833\n",
      "Eval loss 0.013531657867133617, R2 0.4787749648094177\n",
      "epoch 2298, loss 0.01344293076545, R2 0.44123101234436035\n",
      "Eval loss 0.013530871830880642, R2 0.47880494594573975\n",
      "epoch 2299, loss 0.013442207127809525, R2 0.44126075506210327\n",
      "Eval loss 0.013530083931982517, R2 0.4788357615470886\n",
      "epoch 2300, loss 0.013441484421491623, R2 0.44129008054733276\n",
      "Eval loss 0.013529297895729542, R2 0.4788660407066345\n",
      "epoch 2301, loss 0.013440761715173721, R2 0.44132035970687866\n",
      "Eval loss 0.013528512790799141, R2 0.4788956046104431\n",
      "epoch 2302, loss 0.013440038077533245, R2 0.44135111570358276\n",
      "Eval loss 0.013527726754546165, R2 0.4789261817932129\n",
      "epoch 2303, loss 0.013439315371215343, R2 0.4413807988166809\n",
      "Eval loss 0.013526943512260914, R2 0.47895610332489014\n",
      "epoch 2304, loss 0.013438593596220016, R2 0.4414111375808716\n",
      "Eval loss 0.013526158407330513, R2 0.47898632287979126\n",
      "epoch 2305, loss 0.013437872752547264, R2 0.4414411187171936\n",
      "Eval loss 0.013525374233722687, R2 0.47901731729507446\n",
      "epoch 2306, loss 0.013437150977551937, R2 0.44147056341171265\n",
      "Eval loss 0.013524592854082584, R2 0.47904664278030396\n",
      "epoch 2307, loss 0.01343643106520176, R2 0.441500723361969\n",
      "Eval loss 0.013523809611797333, R2 0.4790771007537842\n",
      "epoch 2308, loss 0.013435711152851582, R2 0.4415304660797119\n",
      "Eval loss 0.01352302823215723, R2 0.47910720109939575\n",
      "epoch 2309, loss 0.013434993103146553, R2 0.4415602684020996\n",
      "Eval loss 0.013522246852517128, R2 0.47913700342178345\n",
      "epoch 2310, loss 0.01343427412211895, R2 0.44159090518951416\n",
      "Eval loss 0.013521467335522175, R2 0.4791673421859741\n",
      "epoch 2311, loss 0.013433557003736496, R2 0.44161999225616455\n",
      "Eval loss 0.013520685024559498, R2 0.4791974425315857\n",
      "epoch 2312, loss 0.013432838022708893, R2 0.4416505694389343\n",
      "Eval loss 0.013519905507564545, R2 0.4792276620864868\n",
      "epoch 2313, loss 0.013432120904326439, R2 0.44167983531951904\n",
      "Eval loss 0.013519126921892166, R2 0.47925716638565063\n",
      "epoch 2314, loss 0.013431403785943985, R2 0.4417094588279724\n",
      "Eval loss 0.013518348336219788, R2 0.47928792238235474\n",
      "epoch 2315, loss 0.013430687598884106, R2 0.44173938035964966\n",
      "Eval loss 0.013517571613192558, R2 0.4793173670768738\n",
      "epoch 2316, loss 0.013429972343146801, R2 0.44176894426345825\n",
      "Eval loss 0.01351679302752018, R2 0.47934722900390625\n",
      "epoch 2317, loss 0.013429256156086922, R2 0.4417992830276489\n",
      "Eval loss 0.013516017235815525, R2 0.4793774485588074\n",
      "epoch 2318, loss 0.013428542762994766, R2 0.4418283700942993\n",
      "Eval loss 0.013515240512788296, R2 0.4794071912765503\n",
      "epoch 2319, loss 0.013427828438580036, R2 0.4418582320213318\n",
      "Eval loss 0.013514464721083641, R2 0.4794367551803589\n",
      "epoch 2320, loss 0.013427114114165306, R2 0.44188791513442993\n",
      "Eval loss 0.013513689860701561, R2 0.47946691513061523\n",
      "epoch 2321, loss 0.01342640072107315, R2 0.4419172406196594\n",
      "Eval loss 0.013512914068996906, R2 0.47949695587158203\n",
      "epoch 2322, loss 0.013425689190626144, R2 0.44194722175598145\n",
      "Eval loss 0.0135121401399374, R2 0.4795262813568115\n",
      "epoch 2323, loss 0.013424975797533989, R2 0.4419766664505005\n",
      "Eval loss 0.013511366210877895, R2 0.479556143283844\n",
      "epoch 2324, loss 0.013424266129732132, R2 0.4420061707496643\n",
      "Eval loss 0.013510593213140965, R2 0.4795861840248108\n",
      "epoch 2325, loss 0.013423553667962551, R2 0.4420357942581177\n",
      "Eval loss 0.013509822078049183, R2 0.47961556911468506\n",
      "epoch 2326, loss 0.013422844000160694, R2 0.44206613302230835\n",
      "Eval loss 0.013509048148989677, R2 0.47964566946029663\n",
      "epoch 2327, loss 0.013422132469713688, R2 0.44209539890289307\n",
      "Eval loss 0.013508277013897896, R2 0.47967541217803955\n",
      "epoch 2328, loss 0.013421422801911831, R2 0.4421243667602539\n",
      "Eval loss 0.013507506810128689, R2 0.47970491647720337\n",
      "epoch 2329, loss 0.013420714996755123, R2 0.44215428829193115\n",
      "Eval loss 0.013506736606359482, R2 0.47973471879959106\n",
      "epoch 2330, loss 0.013420004397630692, R2 0.44218361377716064\n",
      "Eval loss 0.01350596733391285, R2 0.47976410388946533\n",
      "epoch 2331, loss 0.013419298455119133, R2 0.4422128200531006\n",
      "Eval loss 0.013505197130143642, R2 0.4797942042350769\n",
      "epoch 2332, loss 0.01341858971863985, R2 0.4422420859336853\n",
      "Eval loss 0.013504428789019585, R2 0.4798234701156616\n",
      "epoch 2333, loss 0.013417882844805717, R2 0.4422721862792969\n",
      "Eval loss 0.013503660447895527, R2 0.47985321283340454\n",
      "epoch 2334, loss 0.013417174108326435, R2 0.44230127334594727\n",
      "Eval loss 0.013502892106771469, R2 0.4798826575279236\n",
      "epoch 2335, loss 0.013416470028460026, R2 0.4423302412033081\n",
      "Eval loss 0.013502126559615135, R2 0.4799121618270874\n",
      "epoch 2336, loss 0.013415764085948467, R2 0.44235992431640625\n",
      "Eval loss 0.013501360081136227, R2 0.47994184494018555\n",
      "epoch 2337, loss 0.013415058143436909, R2 0.4423888921737671\n",
      "Eval loss 0.013500593602657318, R2 0.4799710512161255\n",
      "epoch 2338, loss 0.013414353132247925, R2 0.4424182176589966\n",
      "Eval loss 0.01349982712417841, R2 0.48000073432922363\n",
      "epoch 2339, loss 0.01341365184634924, R2 0.44244736433029175\n",
      "Eval loss 0.013499063439667225, R2 0.48003047704696655\n",
      "epoch 2340, loss 0.013412946835160255, R2 0.44247668981552124\n",
      "Eval loss 0.013498296961188316, R2 0.4800598621368408\n",
      "epoch 2341, loss 0.013412242755293846, R2 0.4425058960914612\n",
      "Eval loss 0.013497535139322281, R2 0.48008888959884644\n",
      "epoch 2342, loss 0.013411539606750011, R2 0.4425351619720459\n",
      "Eval loss 0.013496771454811096, R2 0.4801182746887207\n",
      "epoch 2343, loss 0.013410838320851326, R2 0.4425644874572754\n",
      "Eval loss 0.013496008701622486, R2 0.4801482558250427\n",
      "epoch 2344, loss 0.013410136103630066, R2 0.4425938129425049\n",
      "Eval loss 0.013495247811079025, R2 0.480177104473114\n",
      "epoch 2345, loss 0.01340943481773138, R2 0.4426226019859314\n",
      "Eval loss 0.013494485057890415, R2 0.48020678758621216\n",
      "epoch 2346, loss 0.013408735394477844, R2 0.4426516890525818\n",
      "Eval loss 0.01349372323602438, R2 0.4802364706993103\n",
      "epoch 2347, loss 0.013408034108579159, R2 0.4426806569099426\n",
      "Eval loss 0.013492964208126068, R2 0.48026537895202637\n",
      "epoch 2348, loss 0.013407335616648197, R2 0.44271010160446167\n",
      "Eval loss 0.013492202386260033, R2 0.4802948832511902\n",
      "epoch 2349, loss 0.013406634330749512, R2 0.4427390694618225\n",
      "Eval loss 0.013491444289684296, R2 0.4803234934806824\n",
      "epoch 2350, loss 0.013405936770141125, R2 0.44276857376098633\n",
      "Eval loss 0.013490683399140835, R2 0.48035311698913574\n",
      "epoch 2351, loss 0.013405239209532738, R2 0.4427977204322815\n",
      "Eval loss 0.013489927165210247, R2 0.4803817868232727\n",
      "epoch 2352, loss 0.013404539786279202, R2 0.4428260922431946\n",
      "Eval loss 0.01348916720598936, R2 0.4804113507270813\n",
      "epoch 2353, loss 0.013403844088315964, R2 0.44285523891448975\n",
      "Eval loss 0.013488410972058773, R2 0.48044031858444214\n",
      "epoch 2354, loss 0.013403146527707577, R2 0.44288402795791626\n",
      "Eval loss 0.01348765380680561, R2 0.4804702401161194\n",
      "epoch 2355, loss 0.013402450829744339, R2 0.4429134726524353\n",
      "Eval loss 0.013486896641552448, R2 0.48049867153167725\n",
      "epoch 2356, loss 0.013401754200458527, R2 0.4429420828819275\n",
      "Eval loss 0.013486141338944435, R2 0.48052775859832764\n",
      "epoch 2357, loss 0.013401060365140438, R2 0.4429716467857361\n",
      "Eval loss 0.013485384173691273, R2 0.4805569052696228\n",
      "epoch 2358, loss 0.013400363735854626, R2 0.4429996609687805\n",
      "Eval loss 0.013484630733728409, R2 0.4805862307548523\n",
      "epoch 2359, loss 0.013399669900536537, R2 0.4430285096168518\n",
      "Eval loss 0.013483877293765545, R2 0.4806152582168579\n",
      "epoch 2360, loss 0.013398975133895874, R2 0.4430575966835022\n",
      "Eval loss 0.013483121991157532, R2 0.4806445240974426\n",
      "epoch 2361, loss 0.013398281298577785, R2 0.44308626651763916\n",
      "Eval loss 0.013482370413839817, R2 0.48067331314086914\n",
      "epoch 2362, loss 0.013397591188549995, R2 0.4431149363517761\n",
      "Eval loss 0.013481616042554379, R2 0.4807020425796509\n",
      "epoch 2363, loss 0.013396897353231907, R2 0.4431437849998474\n",
      "Eval loss 0.013480864465236664, R2 0.4807310104370117\n",
      "epoch 2364, loss 0.013396206311881542, R2 0.4431734085083008\n",
      "Eval loss 0.013480112887918949, R2 0.4807605743408203\n",
      "epoch 2365, loss 0.013395513407886028, R2 0.44320130348205566\n",
      "Eval loss 0.013479362241923809, R2 0.4807890057563782\n",
      "epoch 2366, loss 0.013394825160503387, R2 0.44323062896728516\n",
      "Eval loss 0.013478611595928669, R2 0.4808182120323181\n",
      "epoch 2367, loss 0.013394132256507874, R2 0.44325870275497437\n",
      "Eval loss 0.013477860949933529, R2 0.48084670305252075\n",
      "epoch 2368, loss 0.013393441215157509, R2 0.44328707456588745\n",
      "Eval loss 0.013477111235260963, R2 0.4808759093284607\n",
      "epoch 2369, loss 0.013392752967774868, R2 0.4433162212371826\n",
      "Eval loss 0.013476362451910973, R2 0.480904757976532\n",
      "epoch 2370, loss 0.013392063789069653, R2 0.44334447383880615\n",
      "Eval loss 0.013475614599883556, R2 0.48093366622924805\n",
      "epoch 2371, loss 0.013391374610364437, R2 0.44337350130081177\n",
      "Eval loss 0.013474865816533566, R2 0.48096269369125366\n",
      "epoch 2372, loss 0.013390686362981796, R2 0.44340193271636963\n",
      "Eval loss 0.01347411796450615, R2 0.480991005897522\n",
      "epoch 2373, loss 0.01338999904692173, R2 0.44343048334121704\n",
      "Eval loss 0.013473371043801308, R2 0.4810194969177246\n",
      "epoch 2374, loss 0.013389311730861664, R2 0.4434587359428406\n",
      "Eval loss 0.01347262505441904, R2 0.4810490012168884\n",
      "epoch 2375, loss 0.013388624414801598, R2 0.44348764419555664\n",
      "Eval loss 0.013471878133714199, R2 0.4810771346092224\n",
      "epoch 2376, loss 0.01338793896138668, R2 0.4435157775878906\n",
      "Eval loss 0.013471133075654507, R2 0.4811067581176758\n",
      "epoch 2377, loss 0.013387253507971764, R2 0.4435446262359619\n",
      "Eval loss 0.013470388017594814, R2 0.4811350107192993\n",
      "epoch 2378, loss 0.013386568054556847, R2 0.4435729384422302\n",
      "Eval loss 0.013469644822180271, R2 0.4811633229255676\n",
      "epoch 2379, loss 0.01338588260114193, R2 0.4436017870903015\n",
      "Eval loss 0.013468901626765728, R2 0.48119211196899414\n",
      "epoch 2380, loss 0.013385199010372162, R2 0.44363003969192505\n",
      "Eval loss 0.013468155637383461, R2 0.4812208414077759\n",
      "epoch 2381, loss 0.013384515419602394, R2 0.4436582326889038\n",
      "Eval loss 0.013467414304614067, R2 0.48124969005584717\n",
      "epoch 2382, loss 0.013383830897510052, R2 0.44368690252304077\n",
      "Eval loss 0.013466672971844673, R2 0.48127812147140503\n",
      "epoch 2383, loss 0.013383149169385433, R2 0.44371575117111206\n",
      "Eval loss 0.01346592977643013, R2 0.4813065528869629\n",
      "epoch 2384, loss 0.01338246650993824, R2 0.44374340772628784\n",
      "Eval loss 0.01346518937498331, R2 0.4813354015350342\n",
      "epoch 2385, loss 0.013381785713136196, R2 0.4437718987464905\n",
      "Eval loss 0.013464448973536491, R2 0.4813634753227234\n",
      "epoch 2386, loss 0.013381102122366428, R2 0.44380027055740356\n",
      "Eval loss 0.013463708572089672, R2 0.4813922643661499\n",
      "epoch 2387, loss 0.013380421325564384, R2 0.4438292980194092\n",
      "Eval loss 0.013462968170642853, R2 0.48142051696777344\n",
      "epoch 2388, loss 0.01337974052876234, R2 0.4438576102256775\n",
      "Eval loss 0.013462229631841183, R2 0.4814492464065552\n",
      "epoch 2389, loss 0.013379061594605446, R2 0.4438856840133667\n",
      "Eval loss 0.013461491093039513, R2 0.4814772605895996\n",
      "epoch 2390, loss 0.013378382660448551, R2 0.4439135193824768\n",
      "Eval loss 0.013460753485560417, R2 0.481505811214447\n",
      "epoch 2391, loss 0.013377703726291656, R2 0.443942129611969\n",
      "Eval loss 0.013460014015436172, R2 0.4815344214439392\n",
      "epoch 2392, loss 0.013377023860812187, R2 0.44397056102752686\n",
      "Eval loss 0.013459278270602226, R2 0.4815624952316284\n",
      "epoch 2393, loss 0.013376346789300442, R2 0.4439976215362549\n",
      "Eval loss 0.013458541594445705, R2 0.48159098625183105\n",
      "epoch 2394, loss 0.013375669717788696, R2 0.4440259337425232\n",
      "Eval loss 0.013457804918289185, R2 0.4816197156906128\n",
      "epoch 2395, loss 0.013374991714954376, R2 0.44405484199523926\n",
      "Eval loss 0.013457068242132664, R2 0.48164790868759155\n",
      "epoch 2396, loss 0.013374315574765205, R2 0.44408243894577026\n",
      "Eval loss 0.013456334359943867, R2 0.48167622089385986\n",
      "epoch 2397, loss 0.013373639434576035, R2 0.4441105127334595\n",
      "Eval loss 0.01345560047775507, R2 0.4817044138908386\n",
      "epoch 2398, loss 0.013372964225709438, R2 0.44413912296295166\n",
      "Eval loss 0.013454866595566273, R2 0.48173272609710693\n",
      "epoch 2399, loss 0.013372288085520267, R2 0.44416719675064087\n",
      "Eval loss 0.013454132713377476, R2 0.4817606806755066\n",
      "epoch 2400, loss 0.013371611945331097, R2 0.4441947937011719\n",
      "Eval loss 0.013453398831188679, R2 0.48178911209106445\n",
      "epoch 2401, loss 0.01337093859910965, R2 0.44422298669815063\n",
      "Eval loss 0.013452665880322456, R2 0.48181718587875366\n",
      "epoch 2402, loss 0.013370264321565628, R2 0.44425082206726074\n",
      "Eval loss 0.013451935723423958, R2 0.4818456172943115\n",
      "epoch 2403, loss 0.013369590975344181, R2 0.4442789554595947\n",
      "Eval loss 0.013451202772557735, R2 0.4818739891052246\n",
      "epoch 2404, loss 0.013368918560445309, R2 0.4443070888519287\n",
      "Eval loss 0.013450471684336662, R2 0.4819021224975586\n",
      "epoch 2405, loss 0.013368245214223862, R2 0.4443347454071045\n",
      "Eval loss 0.013449742458760738, R2 0.4819297790527344\n",
      "epoch 2406, loss 0.01336757279932499, R2 0.4443630576133728\n",
      "Eval loss 0.01344901230186224, R2 0.4819585084915161\n",
      "epoch 2407, loss 0.013366902247071266, R2 0.44439053535461426\n",
      "Eval loss 0.013448280282318592, R2 0.4819865822792053\n",
      "epoch 2408, loss 0.013366229832172394, R2 0.4444192051887512\n",
      "Eval loss 0.013447552919387817, R2 0.48201411962509155\n",
      "epoch 2409, loss 0.013365560211241245, R2 0.444446325302124\n",
      "Eval loss 0.013446824625134468, R2 0.4820423126220703\n",
      "epoch 2410, loss 0.013364889658987522, R2 0.44447416067123413\n",
      "Eval loss 0.013446097262203693, R2 0.4820706248283386\n",
      "epoch 2411, loss 0.013364220038056374, R2 0.44450223445892334\n",
      "Eval loss 0.013445369899272919, R2 0.4820985198020935\n",
      "epoch 2412, loss 0.013363550417125225, R2 0.4445304274559021\n",
      "Eval loss 0.013444643467664719, R2 0.4821266531944275\n",
      "epoch 2413, loss 0.013362882658839226, R2 0.444557785987854\n",
      "Eval loss 0.01344391517341137, R2 0.48215436935424805\n",
      "epoch 2414, loss 0.013362213037908077, R2 0.4445854425430298\n",
      "Eval loss 0.013443191535770893, R2 0.48218244314193726\n",
      "epoch 2415, loss 0.013361547142267227, R2 0.4446132779121399\n",
      "Eval loss 0.013442465104162693, R2 0.4822103977203369\n",
      "epoch 2416, loss 0.013360876590013504, R2 0.4446411728858948\n",
      "Eval loss 0.013441739603877068, R2 0.4822384715080261\n",
      "epoch 2417, loss 0.013360212557017803, R2 0.44466882944107056\n",
      "Eval loss 0.013441015034914017, R2 0.4822661280632019\n",
      "epoch 2418, loss 0.013359544798731804, R2 0.4446963667869568\n",
      "Eval loss 0.013440292328596115, R2 0.482293963432312\n",
      "epoch 2419, loss 0.013358878903090954, R2 0.44472402334213257\n",
      "Eval loss 0.013439567759633064, R2 0.48232173919677734\n",
      "epoch 2420, loss 0.013358213938772678, R2 0.4447518587112427\n",
      "Eval loss 0.013438845984637737, R2 0.482349693775177\n",
      "epoch 2421, loss 0.013357548974454403, R2 0.44477951526641846\n",
      "Eval loss 0.013438121415674686, R2 0.48237818479537964\n",
      "epoch 2422, loss 0.013356884010136127, R2 0.4448069930076599\n",
      "Eval loss 0.013437400572001934, R2 0.4824056625366211\n",
      "epoch 2423, loss 0.013356219045817852, R2 0.4448345899581909\n",
      "Eval loss 0.013436680659651756, R2 0.4824334979057312\n",
      "epoch 2424, loss 0.013355557806789875, R2 0.4448623061180115\n",
      "Eval loss 0.01343595888465643, R2 0.48246103525161743\n",
      "epoch 2425, loss 0.0133548928424716, R2 0.4448900818824768\n",
      "Eval loss 0.013435238972306252, R2 0.48248863220214844\n",
      "epoch 2426, loss 0.013354228809475899, R2 0.44491785764694214\n",
      "Eval loss 0.01343451626598835, R2 0.4825162887573242\n",
      "epoch 2427, loss 0.013353568501770496, R2 0.44494444131851196\n",
      "Eval loss 0.013433799147605896, R2 0.4825439453125\n",
      "epoch 2428, loss 0.01335290540009737, R2 0.4449726939201355\n",
      "Eval loss 0.013433080166578293, R2 0.48257189989089966\n",
      "epoch 2429, loss 0.013352244161069393, R2 0.4450003504753113\n",
      "Eval loss 0.01343236118555069, R2 0.4825994372367859\n",
      "epoch 2430, loss 0.013351584784686565, R2 0.44502758979797363\n",
      "Eval loss 0.013431644067168236, R2 0.4826272130012512\n",
      "epoch 2431, loss 0.013350923545658588, R2 0.4450545310974121\n",
      "Eval loss 0.013430925086140633, R2 0.48265475034713745\n",
      "epoch 2432, loss 0.013350261375308037, R2 0.44508224725723267\n",
      "Eval loss 0.013430207967758179, R2 0.4826825261116028\n",
      "epoch 2433, loss 0.013349603861570358, R2 0.44511014223098755\n",
      "Eval loss 0.0134294917806983, R2 0.4827101230621338\n",
      "epoch 2434, loss 0.01334894448518753, R2 0.44513750076293945\n",
      "Eval loss 0.01342877559363842, R2 0.4827377200126648\n",
      "epoch 2435, loss 0.013348285108804703, R2 0.4451645612716675\n",
      "Eval loss 0.013428060337901115, R2 0.48276495933532715\n",
      "epoch 2436, loss 0.013347627595067024, R2 0.4451913833618164\n",
      "Eval loss 0.01342734694480896, R2 0.4827927350997925\n",
      "epoch 2437, loss 0.013346969150006771, R2 0.44521909952163696\n",
      "Eval loss 0.013426629826426506, R2 0.4828205108642578\n",
      "epoch 2438, loss 0.013346311636269093, R2 0.4452463984489441\n",
      "Eval loss 0.01342591643333435, R2 0.4828478693962097\n",
      "epoch 2439, loss 0.013345655985176563, R2 0.4452739953994751\n",
      "Eval loss 0.013425203040242195, R2 0.4828757643699646\n",
      "epoch 2440, loss 0.01334499940276146, R2 0.44530099630355835\n",
      "Eval loss 0.013424490578472614, R2 0.482902467250824\n",
      "epoch 2441, loss 0.013344342820346355, R2 0.44532841444015503\n",
      "Eval loss 0.013423777185380459, R2 0.482930064201355\n",
      "epoch 2442, loss 0.013343687169253826, R2 0.44535571336746216\n",
      "Eval loss 0.013423066586256027, R2 0.48295748233795166\n",
      "epoch 2443, loss 0.013343031518161297, R2 0.4453829526901245\n",
      "Eval loss 0.013422355987131596, R2 0.482984721660614\n",
      "epoch 2444, loss 0.013342378661036491, R2 0.44540971517562866\n",
      "Eval loss 0.01342164445668459, R2 0.4830125570297241\n",
      "epoch 2445, loss 0.013341723009943962, R2 0.44543731212615967\n",
      "Eval loss 0.013420933857560158, R2 0.4830397963523865\n",
      "epoch 2446, loss 0.013341070152819157, R2 0.44546395540237427\n",
      "Eval loss 0.013420223258435726, R2 0.48306745290756226\n",
      "epoch 2447, loss 0.013340417295694351, R2 0.4454919695854187\n",
      "Eval loss 0.013419512659311295, R2 0.4830945134162903\n",
      "epoch 2448, loss 0.013339763507246971, R2 0.445518434047699\n",
      "Eval loss 0.013418803922832012, R2 0.48312151432037354\n",
      "epoch 2449, loss 0.01333911158144474, R2 0.4455457329750061\n",
      "Eval loss 0.013418096117675304, R2 0.4831492304801941\n",
      "epoch 2450, loss 0.013338458724319935, R2 0.4455731511116028\n",
      "Eval loss 0.013417387381196022, R2 0.48317641019821167\n",
      "epoch 2451, loss 0.013337807729840279, R2 0.4456002712249756\n",
      "Eval loss 0.013416679576039314, R2 0.48320382833480835\n",
      "epoch 2452, loss 0.013337156735360622, R2 0.44562697410583496\n",
      "Eval loss 0.013415973633527756, R2 0.4832311272621155\n",
      "epoch 2453, loss 0.01333650667220354, R2 0.44565415382385254\n",
      "Eval loss 0.013415265828371048, R2 0.4832577705383301\n",
      "epoch 2454, loss 0.013335857540369034, R2 0.44568026065826416\n",
      "Eval loss 0.013414560817182064, R2 0.48328495025634766\n",
      "epoch 2455, loss 0.013335205614566803, R2 0.4457080364227295\n",
      "Eval loss 0.01341385580599308, R2 0.4833122491836548\n",
      "epoch 2456, loss 0.013334558345377445, R2 0.4457356333732605\n",
      "Eval loss 0.013413148932158947, R2 0.48333966732025146\n",
      "epoch 2457, loss 0.013333909213542938, R2 0.4457617998123169\n",
      "Eval loss 0.013412442989647388, R2 0.4833669662475586\n",
      "epoch 2458, loss 0.01333326194435358, R2 0.44578903913497925\n",
      "Eval loss 0.013411741703748703, R2 0.4833938479423523\n",
      "epoch 2459, loss 0.013332612812519073, R2 0.4458158016204834\n",
      "Eval loss 0.013411038555204868, R2 0.48342108726501465\n",
      "epoch 2460, loss 0.013331964612007141, R2 0.44584280252456665\n",
      "Eval loss 0.013410333544015884, R2 0.4834479093551636\n",
      "epoch 2461, loss 0.013331318274140358, R2 0.44587016105651855\n",
      "Eval loss 0.01340963039547205, R2 0.4834751486778259\n",
      "epoch 2462, loss 0.013330671004951, R2 0.4458965063095093\n",
      "Eval loss 0.013408930040895939, R2 0.48350244760513306\n",
      "epoch 2463, loss 0.013330024667084217, R2 0.44592320919036865\n",
      "Eval loss 0.01340822596102953, R2 0.48352938890457153\n",
      "epoch 2464, loss 0.013329379260540009, R2 0.4459502696990967\n",
      "Eval loss 0.013407525606453419, R2 0.4835560917854309\n",
      "epoch 2465, loss 0.013328734785318375, R2 0.4459775686264038\n",
      "Eval loss 0.013406827114522457, R2 0.48358315229415894\n",
      "epoch 2466, loss 0.013328089378774166, R2 0.44600367546081543\n",
      "Eval loss 0.013406124897301197, R2 0.48361021280288696\n",
      "epoch 2467, loss 0.013327444903552532, R2 0.44603079557418823\n",
      "Eval loss 0.013405426405370235, R2 0.48363709449768066\n",
      "epoch 2468, loss 0.013326801359653473, R2 0.44605720043182373\n",
      "Eval loss 0.013404726050794125, R2 0.4836640954017639\n",
      "epoch 2469, loss 0.013326157815754414, R2 0.4460841417312622\n",
      "Eval loss 0.013404026627540588, R2 0.48369067907333374\n",
      "epoch 2470, loss 0.013325515203177929, R2 0.4461108446121216\n",
      "Eval loss 0.013403328135609627, R2 0.4837176203727722\n",
      "epoch 2471, loss 0.013324872590601444, R2 0.44613754749298096\n",
      "Eval loss 0.01340263057500124, R2 0.48374462127685547\n",
      "epoch 2472, loss 0.01332422997802496, R2 0.44616425037384033\n",
      "Eval loss 0.013401933945715427, R2 0.4837719202041626\n",
      "epoch 2473, loss 0.01332358829677105, R2 0.44619113206863403\n",
      "Eval loss 0.01340123638510704, R2 0.4837985038757324\n",
      "epoch 2474, loss 0.01332294661551714, R2 0.44621777534484863\n",
      "Eval loss 0.013400539755821228, R2 0.48382532596588135\n",
      "epoch 2475, loss 0.013322306796908379, R2 0.44624489545822144\n",
      "Eval loss 0.01339984405785799, R2 0.4838521480560303\n",
      "epoch 2476, loss 0.013321666046977043, R2 0.44627153873443604\n",
      "Eval loss 0.013399148359894753, R2 0.48387861251831055\n",
      "epoch 2477, loss 0.013321027159690857, R2 0.4462977647781372\n",
      "Eval loss 0.01339845359325409, R2 0.4839056730270386\n",
      "epoch 2478, loss 0.013320387341082096, R2 0.44632434844970703\n",
      "Eval loss 0.013397758826613426, R2 0.4839323163032532\n",
      "epoch 2479, loss 0.01331974659115076, R2 0.4463508129119873\n",
      "Eval loss 0.013397064059972763, R2 0.483958899974823\n",
      "epoch 2480, loss 0.013319107703864574, R2 0.446377694606781\n",
      "Eval loss 0.013396372087299824, R2 0.4839860200881958\n",
      "epoch 2481, loss 0.013318469747900963, R2 0.4464036822319031\n",
      "Eval loss 0.013395678251981735, R2 0.4840126037597656\n",
      "epoch 2482, loss 0.013317832723259926, R2 0.44643014669418335\n",
      "Eval loss 0.013394985347986221, R2 0.4840396046638489\n",
      "epoch 2483, loss 0.013317195698618889, R2 0.4464566707611084\n",
      "Eval loss 0.013394291512668133, R2 0.48406630754470825\n",
      "epoch 2484, loss 0.013316558673977852, R2 0.446483314037323\n",
      "Eval loss 0.013393601402640343, R2 0.4840925931930542\n",
      "epoch 2485, loss 0.013315921649336815, R2 0.44650977849960327\n",
      "Eval loss 0.013392910361289978, R2 0.4841192364692688\n",
      "epoch 2486, loss 0.013315286487340927, R2 0.4465358257293701\n",
      "Eval loss 0.013392219319939613, R2 0.48414599895477295\n",
      "epoch 2487, loss 0.01331465132534504, R2 0.44656258821487427\n",
      "Eval loss 0.013391529209911823, R2 0.4841727018356323\n",
      "epoch 2488, loss 0.013314015232026577, R2 0.44658905267715454\n",
      "Eval loss 0.013390838168561459, R2 0.4841988682746887\n",
      "epoch 2489, loss 0.013313381001353264, R2 0.44661593437194824\n",
      "Eval loss 0.013390149921178818, R2 0.48422539234161377\n",
      "epoch 2490, loss 0.01331274677067995, R2 0.44664227962493896\n",
      "Eval loss 0.013389461673796177, R2 0.48425203561782837\n",
      "epoch 2491, loss 0.013312113471329212, R2 0.4466679096221924\n",
      "Eval loss 0.013388771563768387, R2 0.48427844047546387\n",
      "epoch 2492, loss 0.013311478309333324, R2 0.4466943144798279\n",
      "Eval loss 0.013388085179030895, R2 0.48430508375167847\n",
      "epoch 2493, loss 0.013310846872627735, R2 0.44672054052352905\n",
      "Eval loss 0.013387397862970829, R2 0.48433172702789307\n",
      "epoch 2494, loss 0.013310215435922146, R2 0.446747362613678\n",
      "Eval loss 0.013386711478233337, R2 0.48435813188552856\n",
      "epoch 2495, loss 0.013309582136571407, R2 0.44677311182022095\n",
      "Eval loss 0.013386025093495846, R2 0.48438411951065063\n",
      "epoch 2496, loss 0.013308950699865818, R2 0.4467993974685669\n",
      "Eval loss 0.01338533777743578, R2 0.4844105839729309\n",
      "epoch 2497, loss 0.013308318331837654, R2 0.44682568311691284\n",
      "Eval loss 0.013384653255343437, R2 0.48443710803985596\n",
      "epoch 2498, loss 0.01330768782645464, R2 0.44685184955596924\n",
      "Eval loss 0.01338396780192852, R2 0.48446398973464966\n",
      "epoch 2499, loss 0.013307057321071625, R2 0.4468780755996704\n",
      "Eval loss 0.013383283279836178, R2 0.48449015617370605\n",
      "epoch 2500, loss 0.013306427747011185, R2 0.4469042420387268\n",
      "Eval loss 0.013382598757743835, R2 0.48451656103134155\n",
      "epoch 2501, loss 0.01330579910427332, R2 0.4469303488731384\n",
      "Eval loss 0.013381915166974068, R2 0.4845426082611084\n",
      "epoch 2502, loss 0.013305168598890305, R2 0.4469565749168396\n",
      "Eval loss 0.0133812315762043, R2 0.484569251537323\n",
      "epoch 2503, loss 0.013304540887475014, R2 0.4469826817512512\n",
      "Eval loss 0.013380549848079681, R2 0.48459547758102417\n",
      "epoch 2504, loss 0.013303913176059723, R2 0.44700878858566284\n",
      "Eval loss 0.013379868119955063, R2 0.48462170362472534\n",
      "epoch 2505, loss 0.013303285464644432, R2 0.44703537225723267\n",
      "Eval loss 0.013379184529185295, R2 0.4846479296684265\n",
      "epoch 2506, loss 0.013302655890583992, R2 0.4470610022544861\n",
      "Eval loss 0.013378503732383251, R2 0.4846745729446411\n",
      "epoch 2507, loss 0.013302031904459, R2 0.44708698987960815\n",
      "Eval loss 0.013377822004258633, R2 0.48470038175582886\n",
      "epoch 2508, loss 0.013301403261721134, R2 0.44711363315582275\n",
      "Eval loss 0.013377144001424313, R2 0.4847266674041748\n",
      "epoch 2509, loss 0.013300778344273567, R2 0.44713956117630005\n",
      "Eval loss 0.013376462273299694, R2 0.48475295305252075\n",
      "epoch 2510, loss 0.013300152495503426, R2 0.4471650719642639\n",
      "Eval loss 0.013375782407820225, R2 0.48477911949157715\n",
      "epoch 2511, loss 0.013299526646733284, R2 0.44719111919403076\n",
      "Eval loss 0.013375104404985905, R2 0.4848049283027649\n",
      "epoch 2512, loss 0.013298902660608292, R2 0.4472172260284424\n",
      "Eval loss 0.01337442547082901, R2 0.4848310351371765\n",
      "epoch 2513, loss 0.013298277743160725, R2 0.4472431540489197\n",
      "Eval loss 0.013373746536672115, R2 0.48485708236694336\n",
      "epoch 2514, loss 0.013297652825713158, R2 0.4472689628601074\n",
      "Eval loss 0.013373071327805519, R2 0.4848833680152893\n",
      "epoch 2515, loss 0.01329702977091074, R2 0.4472954273223877\n",
      "Eval loss 0.013372392393648624, R2 0.4849093556404114\n",
      "epoch 2516, loss 0.013296406716108322, R2 0.44732075929641724\n",
      "Eval loss 0.013371717184782028, R2 0.48493510484695435\n",
      "epoch 2517, loss 0.013295785523951054, R2 0.44734621047973633\n",
      "Eval loss 0.013371040113270283, R2 0.4849616289138794\n",
      "epoch 2518, loss 0.013295162469148636, R2 0.4473726749420166\n",
      "Eval loss 0.013370363973081112, R2 0.48498767614364624\n",
      "epoch 2519, loss 0.013294540345668793, R2 0.4473983645439148\n",
      "Eval loss 0.01336968969553709, R2 0.4850136637687683\n",
      "epoch 2520, loss 0.01329391822218895, R2 0.4474239945411682\n",
      "Eval loss 0.013369014486670494, R2 0.4850393533706665\n",
      "epoch 2521, loss 0.013293297961354256, R2 0.44744980335235596\n",
      "Eval loss 0.013368338346481323, R2 0.48506611585617065\n",
      "epoch 2522, loss 0.013292678631842136, R2 0.44747573137283325\n",
      "Eval loss 0.01336766593158245, R2 0.48509126901626587\n",
      "epoch 2523, loss 0.013292057439684868, R2 0.447502076625824\n",
      "Eval loss 0.013366992585361004, R2 0.48511749505996704\n",
      "epoch 2524, loss 0.013291437178850174, R2 0.4475277066230774\n",
      "Eval loss 0.013366318307816982, R2 0.48514318466186523\n",
      "epoch 2525, loss 0.01329081691801548, R2 0.4475530982017517\n",
      "Eval loss 0.013365646824240685, R2 0.48516911268234253\n",
      "epoch 2526, loss 0.01329019945114851, R2 0.4475793242454529\n",
      "Eval loss 0.013364976271986961, R2 0.48519521951675415\n",
      "epoch 2527, loss 0.01328958012163639, R2 0.4476047158241272\n",
      "Eval loss 0.013364303857088089, R2 0.48522108793258667\n",
      "epoch 2528, loss 0.013288963586091995, R2 0.4476308822631836\n",
      "Eval loss 0.013363632373511791, R2 0.4852471351623535\n",
      "epoch 2529, loss 0.01328834518790245, R2 0.44765639305114746\n",
      "Eval loss 0.013362961821258068, R2 0.48527246713638306\n",
      "epoch 2530, loss 0.013287728652358055, R2 0.4476814866065979\n",
      "Eval loss 0.013362291269004345, R2 0.485298752784729\n",
      "epoch 2531, loss 0.013287111185491085, R2 0.4477071762084961\n",
      "Eval loss 0.013361621648073196, R2 0.48532456159591675\n",
      "epoch 2532, loss 0.013286495581269264, R2 0.44773274660110474\n",
      "Eval loss 0.013360952027142048, R2 0.4853498935699463\n",
      "epoch 2533, loss 0.013285879045724869, R2 0.4477585554122925\n",
      "Eval loss 0.013360283337533474, R2 0.48537611961364746\n",
      "epoch 2534, loss 0.013285264372825623, R2 0.4477841258049011\n",
      "Eval loss 0.013359615579247475, R2 0.48540198802948\n",
      "epoch 2535, loss 0.013284646905958652, R2 0.4478095769882202\n",
      "Eval loss 0.0133589468896389, R2 0.4854274392127991\n",
      "epoch 2536, loss 0.013284035958349705, R2 0.44783496856689453\n",
      "Eval loss 0.013358279131352901, R2 0.4854530096054077\n",
      "epoch 2537, loss 0.013283418491482735, R2 0.4478606581687927\n",
      "Eval loss 0.013357612304389477, R2 0.4854789972305298\n",
      "epoch 2538, loss 0.013282805681228638, R2 0.4478861093521118\n",
      "Eval loss 0.013356945477426052, R2 0.48550498485565186\n",
      "epoch 2539, loss 0.013282191939651966, R2 0.4479116201400757\n",
      "Eval loss 0.013356279581785202, R2 0.48552989959716797\n",
      "epoch 2540, loss 0.013281578198075294, R2 0.44793713092803955\n",
      "Eval loss 0.013355614617466927, R2 0.4855554699897766\n",
      "epoch 2541, loss 0.013280966319143772, R2 0.44796252250671387\n",
      "Eval loss 0.013354947790503502, R2 0.4855814576148987\n",
      "epoch 2542, loss 0.013280355371534824, R2 0.44798851013183594\n",
      "Eval loss 0.013354284688830376, R2 0.4856070280075073\n",
      "epoch 2543, loss 0.013279743492603302, R2 0.4480132460594177\n",
      "Eval loss 0.0133536197245121, R2 0.48563265800476074\n",
      "epoch 2544, loss 0.013279130682349205, R2 0.44803887605667114\n",
      "Eval loss 0.013352954760193825, R2 0.4856584072113037\n",
      "epoch 2545, loss 0.013278519734740257, R2 0.44806426763534546\n",
      "Eval loss 0.013352291658520699, R2 0.485683798789978\n",
      "epoch 2546, loss 0.013277910649776459, R2 0.4480897784233093\n",
      "Eval loss 0.013351629488170147, R2 0.4857093095779419\n",
      "epoch 2547, loss 0.013277299702167511, R2 0.4481149911880493\n",
      "Eval loss 0.013350967317819595, R2 0.48573464155197144\n",
      "epoch 2548, loss 0.013276689685881138, R2 0.44814085960388184\n",
      "Eval loss 0.013350304216146469, R2 0.48576033115386963\n",
      "epoch 2549, loss 0.013276081532239914, R2 0.4481661319732666\n",
      "Eval loss 0.013349642977118492, R2 0.4857855439186096\n",
      "epoch 2550, loss 0.013275470584630966, R2 0.4481913447380066\n",
      "Eval loss 0.013348983600735664, R2 0.48581135272979736\n",
      "epoch 2551, loss 0.013274864293634892, R2 0.44821619987487793\n",
      "Eval loss 0.013348323293030262, R2 0.4858365058898926\n",
      "epoch 2552, loss 0.013274255208671093, R2 0.44824153184890747\n",
      "Eval loss 0.013347660191357136, R2 0.4858620762825012\n",
      "epoch 2553, loss 0.013273647055029869, R2 0.44826680421829224\n",
      "Eval loss 0.013347002677619457, R2 0.4858875274658203\n",
      "epoch 2554, loss 0.01327303983271122, R2 0.44829219579696655\n",
      "Eval loss 0.013346342369914055, R2 0.4859127998352051\n",
      "epoch 2555, loss 0.013272433541715145, R2 0.4483172297477722\n",
      "Eval loss 0.013345683924853802, R2 0.48593801259994507\n",
      "epoch 2556, loss 0.013271826319396496, R2 0.44834214448928833\n",
      "Eval loss 0.013345024548470974, R2 0.48596376180648804\n",
      "epoch 2557, loss 0.013271220959722996, R2 0.4483676552772522\n",
      "Eval loss 0.013344367034733295, R2 0.4859887361526489\n",
      "epoch 2558, loss 0.01327061653137207, R2 0.44839274883270264\n",
      "Eval loss 0.013343708589673042, R2 0.486014723777771\n",
      "epoch 2559, loss 0.01327001117169857, R2 0.44841861724853516\n",
      "Eval loss 0.013343052938580513, R2 0.48603981733322144\n",
      "epoch 2560, loss 0.01326940581202507, R2 0.44844311475753784\n",
      "Eval loss 0.013342395424842834, R2 0.4860645532608032\n",
      "epoch 2561, loss 0.01326880231499672, R2 0.4484681487083435\n",
      "Eval loss 0.013341739773750305, R2 0.4860902428627014\n",
      "epoch 2562, loss 0.013268197886645794, R2 0.4484933018684387\n",
      "Eval loss 0.013341084122657776, R2 0.48611581325531006\n",
      "epoch 2563, loss 0.013267594389617443, R2 0.4485185742378235\n",
      "Eval loss 0.013340427540242672, R2 0.48614078760147095\n",
      "epoch 2564, loss 0.013266990892589092, R2 0.4485434889793396\n",
      "Eval loss 0.013339772820472717, R2 0.4861661195755005\n",
      "epoch 2565, loss 0.013266388326883316, R2 0.44856905937194824\n",
      "Eval loss 0.013339119032025337, R2 0.4861912131309509\n",
      "epoch 2566, loss 0.013265786692500114, R2 0.4485936760902405\n",
      "Eval loss 0.013338465243577957, R2 0.48621606826782227\n",
      "epoch 2567, loss 0.013265184126794338, R2 0.4486190676689148\n",
      "Eval loss 0.013337811455130577, R2 0.48624128103256226\n",
      "epoch 2568, loss 0.013264583423733711, R2 0.44864422082901\n",
      "Eval loss 0.013337158598005772, R2 0.4862670302391052\n",
      "epoch 2569, loss 0.01326398178935051, R2 0.4486681818962097\n",
      "Eval loss 0.013336505740880966, R2 0.4862915277481079\n",
      "epoch 2570, loss 0.013263381086289883, R2 0.44869351387023926\n",
      "Eval loss 0.013335853815078735, R2 0.48631709814071655\n",
      "epoch 2571, loss 0.013262780383229256, R2 0.44871848821640015\n",
      "Eval loss 0.013335200026631355, R2 0.48634201288223267\n",
      "epoch 2572, loss 0.013262179680168629, R2 0.44874346256256104\n",
      "Eval loss 0.013334551826119423, R2 0.48636728525161743\n",
      "epoch 2573, loss 0.013261581771075726, R2 0.44876885414123535\n",
      "Eval loss 0.013333900831639767, R2 0.4863924980163574\n",
      "epoch 2574, loss 0.013260981999337673, R2 0.4487934112548828\n",
      "Eval loss 0.013333247974514961, R2 0.4864174723625183\n",
      "epoch 2575, loss 0.01326038222759962, R2 0.4488179683685303\n",
      "Eval loss 0.013332598842680454, R2 0.48644232749938965\n",
      "epoch 2576, loss 0.013259784318506718, R2 0.44884371757507324\n",
      "Eval loss 0.013331948779523373, R2 0.4864673614501953\n",
      "epoch 2577, loss 0.01325918734073639, R2 0.4488679766654968\n",
      "Eval loss 0.01333130057901144, R2 0.4864926338195801\n",
      "epoch 2578, loss 0.01325859036296606, R2 0.44889265298843384\n",
      "Eval loss 0.013330651447176933, R2 0.48651736974716187\n",
      "epoch 2579, loss 0.013257992453873158, R2 0.4489176869392395\n",
      "Eval loss 0.013330006040632725, R2 0.4865419268608093\n",
      "epoch 2580, loss 0.013257396407425404, R2 0.44894224405288696\n",
      "Eval loss 0.013329355046153069, R2 0.48656725883483887\n",
      "epoch 2581, loss 0.013256799429655075, R2 0.4489672780036926\n",
      "Eval loss 0.01332870777696371, R2 0.4865923523902893\n",
      "epoch 2582, loss 0.013256204314529896, R2 0.44899147748947144\n",
      "Eval loss 0.013328062370419502, R2 0.48661768436431885\n",
      "epoch 2583, loss 0.013255609199404716, R2 0.4490165710449219\n",
      "Eval loss 0.01332741416990757, R2 0.4866417646408081\n",
      "epoch 2584, loss 0.013255014084279537, R2 0.44904130697250366\n",
      "Eval loss 0.013326768763363361, R2 0.486666738986969\n",
      "epoch 2585, loss 0.013254418969154358, R2 0.4490662217140198\n",
      "Eval loss 0.013326124288141727, R2 0.4866917133331299\n",
      "epoch 2586, loss 0.013253825716674328, R2 0.44909119606018066\n",
      "Eval loss 0.013325478881597519, R2 0.4867163300514221\n",
      "epoch 2587, loss 0.013253232464194298, R2 0.4491153955459595\n",
      "Eval loss 0.013324834406375885, R2 0.48674124479293823\n",
      "epoch 2588, loss 0.013252638280391693, R2 0.44913989305496216\n",
      "Eval loss 0.013324189931154251, R2 0.4867662191390991\n",
      "epoch 2589, loss 0.013252045027911663, R2 0.4491649270057678\n",
      "Eval loss 0.013323546387255192, R2 0.4867910146713257\n",
      "epoch 2590, loss 0.013251451775431633, R2 0.4491904377937317\n",
      "Eval loss 0.013322902843356133, R2 0.48681551218032837\n",
      "epoch 2591, loss 0.013250861316919327, R2 0.4492141008377075\n",
      "Eval loss 0.013322260230779648, R2 0.4868406057357788\n",
      "epoch 2592, loss 0.013250268064439297, R2 0.4492390751838684\n",
      "Eval loss 0.013321619480848312, R2 0.4868652820587158\n",
      "epoch 2593, loss 0.013249676674604416, R2 0.44926315546035767\n",
      "Eval loss 0.013320974074304104, R2 0.4868897795677185\n",
      "epoch 2594, loss 0.01324908621609211, R2 0.44928818941116333\n",
      "Eval loss 0.013320332393050194, R2 0.4869149923324585\n",
      "epoch 2595, loss 0.013248494826257229, R2 0.4493122696876526\n",
      "Eval loss 0.013319691643118858, R2 0.4869391918182373\n",
      "epoch 2596, loss 0.013247904367744923, R2 0.44933682680130005\n",
      "Eval loss 0.013319051824510098, R2 0.4869643449783325\n",
      "epoch 2597, loss 0.013247315771877766, R2 0.44936197996139526\n",
      "Eval loss 0.013318410143256187, R2 0.4869886040687561\n",
      "epoch 2598, loss 0.01324672531336546, R2 0.4493856430053711\n",
      "Eval loss 0.013317772187292576, R2 0.48701363801956177\n",
      "epoch 2599, loss 0.013246136717498302, R2 0.4494103193283081\n",
      "Eval loss 0.01331713143736124, R2 0.4870378375053406\n",
      "epoch 2600, loss 0.013245548121631145, R2 0.44943493604660034\n",
      "Eval loss 0.01331649161875248, R2 0.487062931060791\n",
      "epoch 2601, loss 0.013244959525763988, R2 0.4494597315788269\n",
      "Eval loss 0.013315854594111443, R2 0.48708683252334595\n",
      "epoch 2602, loss 0.01324437279254198, R2 0.44948363304138184\n",
      "Eval loss 0.013315215706825256, R2 0.4871116280555725\n",
      "epoch 2603, loss 0.013243784196674824, R2 0.44950807094573975\n",
      "Eval loss 0.013314577750861645, R2 0.48713618516921997\n",
      "epoch 2604, loss 0.013243197463452816, R2 0.4495326280593872\n",
      "Eval loss 0.013313941657543182, R2 0.48716098070144653\n",
      "epoch 2605, loss 0.013242611661553383, R2 0.449556827545166\n",
      "Eval loss 0.01331330370157957, R2 0.48718541860580444\n",
      "epoch 2606, loss 0.0132420239970088, R2 0.44958120584487915\n",
      "Eval loss 0.013312667608261108, R2 0.48721009492874146\n",
      "epoch 2607, loss 0.013241439126431942, R2 0.4496055841445923\n",
      "Eval loss 0.013312031514942646, R2 0.4872344136238098\n",
      "epoch 2608, loss 0.013240853324532509, R2 0.4496304392814636\n",
      "Eval loss 0.013311396352946758, R2 0.4872587323188782\n",
      "epoch 2609, loss 0.01324026845395565, R2 0.449654221534729\n",
      "Eval loss 0.013310760259628296, R2 0.48728352785110474\n",
      "epoch 2610, loss 0.013239684514701366, R2 0.4496791958808899\n",
      "Eval loss 0.013310126960277557, R2 0.48730790615081787\n",
      "epoch 2611, loss 0.013239099644124508, R2 0.4497029781341553\n",
      "Eval loss 0.01330949179828167, R2 0.48733222484588623\n",
      "epoch 2612, loss 0.01323851477354765, R2 0.4497271180152893\n",
      "Eval loss 0.013308857567608356, R2 0.4873565435409546\n",
      "epoch 2613, loss 0.013237932696938515, R2 0.44975125789642334\n",
      "Eval loss 0.013308223336935043, R2 0.4873813986778259\n",
      "epoch 2614, loss 0.01323734875768423, R2 0.449775755405426\n",
      "Eval loss 0.013307590037584305, R2 0.4874056577682495\n",
      "epoch 2615, loss 0.013236764818429947, R2 0.4498005509376526\n",
      "Eval loss 0.013306958600878716, R2 0.4874304533004761\n",
      "epoch 2616, loss 0.013236183673143387, R2 0.4498239755630493\n",
      "Eval loss 0.013306326232850552, R2 0.48745405673980713\n",
      "epoch 2617, loss 0.013235600665211678, R2 0.4498480558395386\n",
      "Eval loss 0.013305694796144962, R2 0.4874783754348755\n",
      "epoch 2618, loss 0.013235020451247692, R2 0.44987285137176514\n",
      "Eval loss 0.013305063359439373, R2 0.4875029921531677\n",
      "epoch 2619, loss 0.013234440237283707, R2 0.4498966932296753\n",
      "Eval loss 0.013304432854056358, R2 0.4875272512435913\n",
      "epoch 2620, loss 0.013233859091997147, R2 0.44992130994796753\n",
      "Eval loss 0.013303801417350769, R2 0.4875512719154358\n",
      "epoch 2621, loss 0.013233277946710587, R2 0.44994497299194336\n",
      "Eval loss 0.013303170911967754, R2 0.487575888633728\n",
      "epoch 2622, loss 0.013232697732746601, R2 0.4499695897102356\n",
      "Eval loss 0.013302541337907314, R2 0.48760008811950684\n",
      "epoch 2623, loss 0.01323211845010519, R2 0.44999295473098755\n",
      "Eval loss 0.013301914557814598, R2 0.4876244068145752\n",
      "epoch 2624, loss 0.01323153916746378, R2 0.4500170946121216\n",
      "Eval loss 0.013301284052431583, R2 0.48764824867248535\n",
      "epoch 2625, loss 0.013230958953499794, R2 0.45004135370254517\n",
      "Eval loss 0.013300657272338867, R2 0.48767268657684326\n",
      "epoch 2626, loss 0.013230381533503532, R2 0.45006513595581055\n",
      "Eval loss 0.013300028629601002, R2 0.48769688606262207\n",
      "epoch 2627, loss 0.013229805044829845, R2 0.45008933544158936\n",
      "Eval loss 0.013299400918185711, R2 0.48772090673446655\n",
      "epoch 2628, loss 0.013229225762188435, R2 0.4501137137413025\n",
      "Eval loss 0.01329877320677042, R2 0.4877449870109558\n",
      "epoch 2629, loss 0.013228649273514748, R2 0.450137197971344\n",
      "Eval loss 0.013298146426677704, R2 0.4877692461013794\n",
      "epoch 2630, loss 0.013228070922195911, R2 0.4501612186431885\n",
      "Eval loss 0.013297521509230137, R2 0.48779362440109253\n",
      "epoch 2631, loss 0.013227495364844799, R2 0.45018529891967773\n",
      "Eval loss 0.013296893797814846, R2 0.48781734704971313\n",
      "epoch 2632, loss 0.013226918876171112, R2 0.4502090811729431\n",
      "Eval loss 0.013296270743012428, R2 0.4878413677215576\n",
      "epoch 2633, loss 0.013226344250142574, R2 0.4502331614494324\n",
      "Eval loss 0.013295644894242287, R2 0.4878656268119812\n",
      "epoch 2634, loss 0.013225768692791462, R2 0.450256884098053\n",
      "Eval loss 0.013295019045472145, R2 0.4878898859024048\n",
      "epoch 2635, loss 0.013225194066762924, R2 0.45028096437454224\n",
      "Eval loss 0.013294395990669727, R2 0.48791372776031494\n",
      "epoch 2636, loss 0.013224618509411812, R2 0.45030468702316284\n",
      "Eval loss 0.01329377293586731, R2 0.487937867641449\n",
      "epoch 2637, loss 0.013224044814705849, R2 0.450329065322876\n",
      "Eval loss 0.013293149881064892, R2 0.4879615902900696\n",
      "epoch 2638, loss 0.013223471119999886, R2 0.45035332441329956\n",
      "Eval loss 0.013292526826262474, R2 0.4879860281944275\n",
      "epoch 2639, loss 0.013222897425293922, R2 0.45037662982940674\n",
      "Eval loss 0.013291904702782631, R2 0.4880101680755615\n",
      "epoch 2640, loss 0.013222325593233109, R2 0.45040005445480347\n",
      "Eval loss 0.013291281647980213, R2 0.48803383111953735\n",
      "epoch 2641, loss 0.01322175096720457, R2 0.45042407512664795\n",
      "Eval loss 0.01329065952450037, R2 0.4880579113960266\n",
      "epoch 2642, loss 0.013221180066466331, R2 0.450447678565979\n",
      "Eval loss 0.013290039263665676, R2 0.488081693649292\n",
      "epoch 2643, loss 0.013220607303082943, R2 0.4504716396331787\n",
      "Eval loss 0.013289418071508408, R2 0.4881058931350708\n",
      "epoch 2644, loss 0.013220036402344704, R2 0.45049482583999634\n",
      "Eval loss 0.013288797810673714, R2 0.48812925815582275\n",
      "epoch 2645, loss 0.013219466432929039, R2 0.45051926374435425\n",
      "Eval loss 0.013288178481161594, R2 0.4881533980369568\n",
      "epoch 2646, loss 0.01321889366954565, R2 0.45054352283477783\n",
      "Eval loss 0.013287557289004326, R2 0.4881772994995117\n",
      "epoch 2647, loss 0.01321832463145256, R2 0.45056718587875366\n",
      "Eval loss 0.01328694075345993, R2 0.48820072412490845\n",
      "epoch 2648, loss 0.01321775559335947, R2 0.450589656829834\n",
      "Eval loss 0.013286321423947811, R2 0.4882245659828186\n",
      "epoch 2649, loss 0.013217185623943806, R2 0.4506140351295471\n",
      "Eval loss 0.013285703957080841, R2 0.48824840784072876\n",
      "epoch 2650, loss 0.013216615654528141, R2 0.4506374001502991\n",
      "Eval loss 0.013285085558891296, R2 0.488272488117218\n",
      "epoch 2651, loss 0.013216046616435051, R2 0.45066118240356445\n",
      "Eval loss 0.013284467160701752, R2 0.4882963299751282\n",
      "epoch 2652, loss 0.013215476647019386, R2 0.45068472623825073\n",
      "Eval loss 0.013283850625157356, R2 0.4883202314376831\n",
      "epoch 2653, loss 0.013214911334216595, R2 0.4507080316543579\n",
      "Eval loss 0.01328323408961296, R2 0.48834383487701416\n",
      "epoch 2654, loss 0.01321434322744608, R2 0.45073163509368896\n",
      "Eval loss 0.013282617554068565, R2 0.4883672595024109\n",
      "epoch 2655, loss 0.013213776983320713, R2 0.45075535774230957\n",
      "Eval loss 0.01328200288116932, R2 0.4883913993835449\n",
      "epoch 2656, loss 0.013213208876550198, R2 0.4507794976234436\n",
      "Eval loss 0.013281388208270073, R2 0.4884149432182312\n",
      "epoch 2657, loss 0.013212641701102257, R2 0.4508025646209717\n",
      "Eval loss 0.013280771672725677, R2 0.4884386658668518\n",
      "epoch 2658, loss 0.013212074525654316, R2 0.45082664489746094\n",
      "Eval loss 0.013280157931149006, R2 0.48846203088760376\n",
      "epoch 2659, loss 0.013211512006819248, R2 0.4508495330810547\n",
      "Eval loss 0.01327954325824976, R2 0.48848599195480347\n",
      "epoch 2660, loss 0.013210944831371307, R2 0.45087307691574097\n",
      "Eval loss 0.013278931379318237, R2 0.4885094165802002\n",
      "epoch 2661, loss 0.013210379518568516, R2 0.45089656114578247\n",
      "Eval loss 0.013278317637741566, R2 0.4885333776473999\n",
      "epoch 2662, loss 0.013209814205765724, R2 0.4509204626083374\n",
      "Eval loss 0.013277703896164894, R2 0.48855680227279663\n",
      "epoch 2663, loss 0.013209250755608082, R2 0.45094406604766846\n",
      "Eval loss 0.013277091085910797, R2 0.4885801672935486\n",
      "epoch 2664, loss 0.013208686374127865, R2 0.4509667754173279\n",
      "Eval loss 0.013276479206979275, R2 0.48860418796539307\n",
      "epoch 2665, loss 0.013208122923970222, R2 0.45099037885665894\n",
      "Eval loss 0.013275868259370327, R2 0.48862767219543457\n",
      "epoch 2666, loss 0.01320755947381258, R2 0.45101451873779297\n",
      "Eval loss 0.01327525544911623, R2 0.4886508584022522\n",
      "epoch 2667, loss 0.013206997886300087, R2 0.45103782415390015\n",
      "Eval loss 0.013274644501507282, R2 0.488674521446228\n",
      "epoch 2668, loss 0.013206436298787594, R2 0.4510604739189148\n",
      "Eval loss 0.013274034485220909, R2 0.48869818449020386\n",
      "epoch 2669, loss 0.013205872848629951, R2 0.45108407735824585\n",
      "Eval loss 0.013273426331579685, R2 0.4887216091156006\n",
      "epoch 2670, loss 0.013205312192440033, R2 0.4511072039604187\n",
      "Eval loss 0.013272816315293312, R2 0.4887450933456421\n",
      "epoch 2671, loss 0.013204749673604965, R2 0.45113110542297363\n",
      "Eval loss 0.013272205367684364, R2 0.48876863718032837\n",
      "epoch 2672, loss 0.013204189948737621, R2 0.45115387439727783\n",
      "Eval loss 0.01327159721404314, R2 0.4887917637825012\n",
      "epoch 2673, loss 0.013203630223870277, R2 0.4511776566505432\n",
      "Eval loss 0.013270989991724491, R2 0.4888151288032532\n",
      "epoch 2674, loss 0.013203069567680359, R2 0.4512002468109131\n",
      "Eval loss 0.013270381838083267, R2 0.4888388514518738\n",
      "epoch 2675, loss 0.01320251077413559, R2 0.451224148273468\n",
      "Eval loss 0.013269774615764618, R2 0.4888620972633362\n",
      "epoch 2676, loss 0.013201949186623096, R2 0.451246976852417\n",
      "Eval loss 0.013269167393445969, R2 0.48888570070266724\n",
      "epoch 2677, loss 0.013201391324400902, R2 0.4512706995010376\n",
      "Eval loss 0.01326856017112732, R2 0.4889090657234192\n",
      "epoch 2678, loss 0.013200832530856133, R2 0.45129358768463135\n",
      "Eval loss 0.01326795481145382, R2 0.48893237113952637\n",
      "epoch 2679, loss 0.013200273737311363, R2 0.4513166546821594\n",
      "Eval loss 0.01326734758913517, R2 0.48895591497421265\n",
      "epoch 2680, loss 0.013199716806411743, R2 0.4513399600982666\n",
      "Eval loss 0.01326674409210682, R2 0.4889790415763855\n",
      "epoch 2681, loss 0.013199158944189548, R2 0.45136314630508423\n",
      "Eval loss 0.013266138732433319, R2 0.4890024662017822\n",
      "epoch 2682, loss 0.013198601081967354, R2 0.45138633251190186\n",
      "Eval loss 0.013265532441437244, R2 0.4890258312225342\n",
      "epoch 2683, loss 0.01319804321974516, R2 0.4514092206954956\n",
      "Eval loss 0.013264929875731468, R2 0.4890490174293518\n",
      "epoch 2684, loss 0.013197488151490688, R2 0.4514327645301819\n",
      "Eval loss 0.013264325447380543, R2 0.4890718460083008\n",
      "epoch 2685, loss 0.013196933083236217, R2 0.4514561891555786\n",
      "Eval loss 0.013263721019029617, R2 0.48909544944763184\n",
      "epoch 2686, loss 0.013196377083659172, R2 0.45147842168807983\n",
      "Eval loss 0.013263119384646416, R2 0.48911893367767334\n",
      "epoch 2687, loss 0.013195822015404701, R2 0.45150166749954224\n",
      "Eval loss 0.013262515887618065, R2 0.489141583442688\n",
      "epoch 2688, loss 0.01319526694715023, R2 0.4515247344970703\n",
      "Eval loss 0.013261913321912289, R2 0.48916494846343994\n",
      "epoch 2689, loss 0.01319471187889576, R2 0.4515479803085327\n",
      "Eval loss 0.013261312618851662, R2 0.4891884922981262\n",
      "epoch 2690, loss 0.013194157741963863, R2 0.45157086849212646\n",
      "Eval loss 0.013260711915791035, R2 0.48921167850494385\n",
      "epoch 2691, loss 0.013193602673709393, R2 0.4515937566757202\n",
      "Eval loss 0.013260110281407833, R2 0.4892348647117615\n",
      "epoch 2692, loss 0.01319305133074522, R2 0.4516168236732483\n",
      "Eval loss 0.013259507715702057, R2 0.4892578721046448\n",
      "epoch 2693, loss 0.01319249626249075, R2 0.45163989067077637\n",
      "Eval loss 0.013258908875286579, R2 0.48928046226501465\n",
      "epoch 2694, loss 0.013191945850849152, R2 0.4516633152961731\n",
      "Eval loss 0.013258307240903378, R2 0.4893036484718323\n",
      "epoch 2695, loss 0.013191391713917255, R2 0.4516858458518982\n",
      "Eval loss 0.013257709331810474, R2 0.48932701349258423\n",
      "epoch 2696, loss 0.013190839439630508, R2 0.4517087936401367\n",
      "Eval loss 0.013257110491394997, R2 0.48935049772262573\n",
      "epoch 2697, loss 0.013190287165343761, R2 0.45173174142837524\n",
      "Eval loss 0.013256511650979519, R2 0.48937326669692993\n",
      "epoch 2698, loss 0.013189738616347313, R2 0.4517545700073242\n",
      "Eval loss 0.013255913741886616, R2 0.489396333694458\n",
      "epoch 2699, loss 0.013189186342060566, R2 0.45177751779556274\n",
      "Eval loss 0.013255315832793713, R2 0.48941922187805176\n",
      "epoch 2700, loss 0.013188636861741543, R2 0.4518001675605774\n",
      "Eval loss 0.013254718855023384, R2 0.48944222927093506\n",
      "epoch 2701, loss 0.01318808551877737, R2 0.45182377099990845\n",
      "Eval loss 0.013254121877253056, R2 0.4894648790359497\n",
      "epoch 2702, loss 0.013187536038458347, R2 0.45184606313705444\n",
      "Eval loss 0.013253524899482727, R2 0.4894883632659912\n",
      "epoch 2703, loss 0.013186986558139324, R2 0.4518696069717407\n",
      "Eval loss 0.013252929784357548, R2 0.4895111322402954\n",
      "epoch 2704, loss 0.01318643894046545, R2 0.4518917202949524\n",
      "Eval loss 0.01325233280658722, R2 0.48953384160995483\n",
      "epoch 2705, loss 0.013185889460146427, R2 0.45191508531570435\n",
      "Eval loss 0.01325173769146204, R2 0.48955702781677246\n",
      "epoch 2706, loss 0.013185340911149979, R2 0.45193731784820557\n",
      "Eval loss 0.01325114257633686, R2 0.4895796775817871\n",
      "epoch 2707, loss 0.013184793293476105, R2 0.45195990800857544\n",
      "Eval loss 0.013250547461211681, R2 0.4896027445793152\n",
      "epoch 2708, loss 0.013184243813157082, R2 0.4519834518432617\n",
      "Eval loss 0.013249953277409077, R2 0.4896254539489746\n",
      "epoch 2709, loss 0.013183698058128357, R2 0.45200562477111816\n",
      "Eval loss 0.013249360024929047, R2 0.48964864015579224\n",
      "epoch 2710, loss 0.013183151371777058, R2 0.4520283341407776\n",
      "Eval loss 0.013248766772449017, R2 0.4896714687347412\n",
      "epoch 2711, loss 0.013182604685425758, R2 0.45205157995224\n",
      "Eval loss 0.013248172588646412, R2 0.48969435691833496\n",
      "epoch 2712, loss 0.013182059861719608, R2 0.45207369327545166\n",
      "Eval loss 0.013247580267488956, R2 0.48971718549728394\n",
      "epoch 2713, loss 0.013181513175368309, R2 0.4520966410636902\n",
      "Eval loss 0.013246988877654076, R2 0.4897400736808777\n",
      "epoch 2714, loss 0.013180967420339584, R2 0.4521191120147705\n",
      "Eval loss 0.01324639655649662, R2 0.4897626042366028\n",
      "epoch 2715, loss 0.013180422596633434, R2 0.4521419405937195\n",
      "Eval loss 0.013245806097984314, R2 0.48978567123413086\n",
      "epoch 2716, loss 0.013179877772927284, R2 0.4521644115447998\n",
      "Eval loss 0.013245215639472008, R2 0.48980826139450073\n",
      "epoch 2717, loss 0.013179332949221134, R2 0.45218706130981445\n",
      "Eval loss 0.013244624249637127, R2 0.48983120918273926\n",
      "epoch 2718, loss 0.013178789988160133, R2 0.452210009098053\n",
      "Eval loss 0.013244034722447395, R2 0.48985421657562256\n",
      "epoch 2719, loss 0.013178245164453983, R2 0.4522322416305542\n",
      "Eval loss 0.013243444263935089, R2 0.48987632989883423\n",
      "epoch 2720, loss 0.013177703134715557, R2 0.45225536823272705\n",
      "Eval loss 0.013242854736745358, R2 0.4898988604545593\n",
      "epoch 2721, loss 0.013177160173654556, R2 0.45227789878845215\n",
      "Eval loss 0.013242267072200775, R2 0.4899218678474426\n",
      "epoch 2722, loss 0.01317661814391613, R2 0.4523000717163086\n",
      "Eval loss 0.013241678476333618, R2 0.48994433879852295\n",
      "epoch 2723, loss 0.01317607518285513, R2 0.4523230195045471\n",
      "Eval loss 0.013241088949143887, R2 0.4899672269821167\n",
      "epoch 2724, loss 0.013175535015761852, R2 0.452345073223114\n",
      "Eval loss 0.01324050035327673, R2 0.48998987674713135\n",
      "epoch 2725, loss 0.013174992986023426, R2 0.45236819982528687\n",
      "Eval loss 0.013239912688732147, R2 0.4900128245353699\n",
      "epoch 2726, loss 0.013174451887607574, R2 0.4523899555206299\n",
      "Eval loss 0.013239326886832714, R2 0.49003511667251587\n",
      "epoch 2727, loss 0.013173910789191723, R2 0.45241260528564453\n",
      "Eval loss 0.013238742016255856, R2 0.4900575876235962\n",
      "epoch 2728, loss 0.013173370622098446, R2 0.4524348974227905\n",
      "Eval loss 0.013238154351711273, R2 0.4900801181793213\n",
      "epoch 2729, loss 0.013172829523682594, R2 0.4524570107460022\n",
      "Eval loss 0.01323756668716669, R2 0.4901025891304016\n",
      "epoch 2730, loss 0.013172291219234467, R2 0.4524797797203064\n",
      "Eval loss 0.013236982747912407, R2 0.4901254177093506\n",
      "epoch 2731, loss 0.01317175105214119, R2 0.4525027275085449\n",
      "Eval loss 0.013236396946012974, R2 0.4901476502418518\n",
      "epoch 2732, loss 0.013171211816370487, R2 0.4525246024131775\n",
      "Eval loss 0.01323581300675869, R2 0.490170419216156\n",
      "epoch 2733, loss 0.013170672580599785, R2 0.452547550201416\n",
      "Eval loss 0.013235229067504406, R2 0.49019289016723633\n",
      "epoch 2734, loss 0.013170134276151657, R2 0.4525696039199829\n",
      "Eval loss 0.013234644196927547, R2 0.4902154803276062\n",
      "epoch 2735, loss 0.013169598765671253, R2 0.45259183645248413\n",
      "Eval loss 0.013234060257673264, R2 0.4902377724647522\n",
      "epoch 2736, loss 0.01316905952990055, R2 0.45261406898498535\n",
      "Eval loss 0.013233477249741554, R2 0.49026066064834595\n",
      "epoch 2737, loss 0.013168523088097572, R2 0.45263671875\n",
      "Eval loss 0.013232894241809845, R2 0.49028319120407104\n",
      "epoch 2738, loss 0.01316798571497202, R2 0.4526592493057251\n",
      "Eval loss 0.01323231216520071, R2 0.49030542373657227\n",
      "epoch 2739, loss 0.013167450204491615, R2 0.4526810050010681\n",
      "Eval loss 0.01323173101991415, R2 0.4903273582458496\n",
      "epoch 2740, loss 0.013166913762688637, R2 0.45270395278930664\n",
      "Eval loss 0.013231148943305016, R2 0.49035006761550903\n",
      "epoch 2741, loss 0.013166378252208233, R2 0.45272570848464966\n",
      "Eval loss 0.013230567798018456, R2 0.4903721809387207\n",
      "epoch 2742, loss 0.013165842741727829, R2 0.45274776220321655\n",
      "Eval loss 0.013229988515377045, R2 0.49039483070373535\n",
      "epoch 2743, loss 0.01316530816257, R2 0.45277053117752075\n",
      "Eval loss 0.013229407370090485, R2 0.4904173016548157\n",
      "epoch 2744, loss 0.013164771720767021, R2 0.45279228687286377\n",
      "Eval loss 0.013228826224803925, R2 0.4904397130012512\n",
      "epoch 2745, loss 0.013164239004254341, R2 0.45281463861465454\n",
      "Eval loss 0.013228246942162514, R2 0.4904620051383972\n",
      "epoch 2746, loss 0.013163706287741661, R2 0.45283716917037964\n",
      "Eval loss 0.013227667659521103, R2 0.4904842972755432\n",
      "epoch 2747, loss 0.013163171708583832, R2 0.4528588056564331\n",
      "Eval loss 0.013227089308202267, R2 0.49050647020339966\n",
      "epoch 2748, loss 0.013162638992071152, R2 0.45288097858428955\n",
      "Eval loss 0.013226510025560856, R2 0.49052894115448\n",
      "epoch 2749, loss 0.013162105344235897, R2 0.452903151512146\n",
      "Eval loss 0.013225930742919445, R2 0.49055105447769165\n",
      "epoch 2750, loss 0.013161573559045792, R2 0.45292526483535767\n",
      "Eval loss 0.013225354254245758, R2 0.4905734658241272\n",
      "epoch 2751, loss 0.013161040842533112, R2 0.45294827222824097\n",
      "Eval loss 0.013224775902926922, R2 0.4905956983566284\n",
      "epoch 2752, loss 0.013160508126020432, R2 0.4529697299003601\n",
      "Eval loss 0.013224199414253235, R2 0.49061793088912964\n",
      "epoch 2753, loss 0.013159978203475475, R2 0.45299142599105835\n",
      "Eval loss 0.013223622925579548, R2 0.4906401038169861\n",
      "epoch 2754, loss 0.01315944641828537, R2 0.45301347970962524\n",
      "Eval loss 0.013223047368228436, R2 0.49066227674484253\n",
      "epoch 2755, loss 0.013158914633095264, R2 0.45303577184677124\n",
      "Eval loss 0.013222470879554749, R2 0.49068403244018555\n",
      "epoch 2756, loss 0.013158385641872883, R2 0.45305776596069336\n",
      "Eval loss 0.013221897184848785, R2 0.4907061457633972\n",
      "epoch 2757, loss 0.013157855719327927, R2 0.4530797600746155\n",
      "Eval loss 0.013221321627497673, R2 0.4907287359237671\n",
      "epoch 2758, loss 0.01315732579678297, R2 0.4531018137931824\n",
      "Eval loss 0.013220747001469135, R2 0.49075090885162354\n",
      "epoch 2759, loss 0.013156796805560589, R2 0.4531239867210388\n",
      "Eval loss 0.013220173306763172, R2 0.49077290296554565\n",
      "epoch 2760, loss 0.013156266883015633, R2 0.4531456232070923\n",
      "Eval loss 0.013219596818089485, R2 0.490795373916626\n",
      "epoch 2761, loss 0.013155737891793251, R2 0.45316779613494873\n",
      "Eval loss 0.013219024986028671, R2 0.49081742763519287\n",
      "epoch 2762, loss 0.013155210763216019, R2 0.4531893730163574\n",
      "Eval loss 0.013218451291322708, R2 0.4908391833305359\n",
      "epoch 2763, loss 0.013154680840671062, R2 0.4532126188278198\n",
      "Eval loss 0.013217879459261894, R2 0.49086135625839233\n",
      "epoch 2764, loss 0.01315415557473898, R2 0.4532337784767151\n",
      "Eval loss 0.013217305764555931, R2 0.4908829927444458\n",
      "epoch 2765, loss 0.013153627514839172, R2 0.45325571298599243\n",
      "Eval loss 0.013216735795140266, R2 0.49090510606765747\n",
      "epoch 2766, loss 0.013153101317584515, R2 0.45327723026275635\n",
      "Eval loss 0.013216162100434303, R2 0.490927517414093\n",
      "epoch 2767, loss 0.013152574189007282, R2 0.45329928398132324\n",
      "Eval loss 0.013215591199696064, R2 0.4909493327140808\n",
      "epoch 2768, loss 0.01315204706043005, R2 0.45332103967666626\n",
      "Eval loss 0.0132150212302804, R2 0.49097132682800293\n",
      "epoch 2769, loss 0.013151522725820541, R2 0.4533430337905884\n",
      "Eval loss 0.013214451260864735, R2 0.4909931421279907\n",
      "epoch 2770, loss 0.013150996528565884, R2 0.4533650875091553\n",
      "Eval loss 0.013213880360126495, R2 0.49101555347442627\n",
      "epoch 2771, loss 0.013150470331311226, R2 0.45338690280914307\n",
      "Eval loss 0.01321331039071083, R2 0.49103736877441406\n",
      "epoch 2772, loss 0.013149945065379143, R2 0.45340877771377563\n",
      "Eval loss 0.01321274135261774, R2 0.4910591244697571\n",
      "epoch 2773, loss 0.013149421662092209, R2 0.4534303545951843\n",
      "Eval loss 0.013212171383202076, R2 0.4910809397697449\n",
      "epoch 2774, loss 0.0131488973274827, R2 0.4534521698951721\n",
      "Eval loss 0.01321160327643156, R2 0.49110299348831177\n",
      "epoch 2775, loss 0.013148373924195766, R2 0.45347392559051514\n",
      "Eval loss 0.013211037032306194, R2 0.49112480878829956\n",
      "epoch 2776, loss 0.013147849589586258, R2 0.45349568128585815\n",
      "Eval loss 0.013210466131567955, R2 0.49114692211151123\n",
      "epoch 2777, loss 0.013147327117621899, R2 0.45351743698120117\n",
      "Eval loss 0.013209899887442589, R2 0.4911690354347229\n",
      "epoch 2778, loss 0.01314680464565754, R2 0.45353931188583374\n",
      "Eval loss 0.013209332711994648, R2 0.4911903142929077\n",
      "epoch 2779, loss 0.013146283105015755, R2 0.4535614848136902\n",
      "Eval loss 0.013208767399191856, R2 0.4912121891975403\n",
      "epoch 2780, loss 0.013145760633051395, R2 0.4535825252532959\n",
      "Eval loss 0.013208199292421341, R2 0.4912339448928833\n",
      "epoch 2781, loss 0.01314523909240961, R2 0.45360422134399414\n",
      "Eval loss 0.013207634910941124, R2 0.49125581979751587\n",
      "epoch 2782, loss 0.013144717551767826, R2 0.4536259174346924\n",
      "Eval loss 0.013207068666815758, R2 0.491277813911438\n",
      "epoch 2783, loss 0.013144196942448616, R2 0.45364755392074585\n",
      "Eval loss 0.01320650428533554, R2 0.4912993907928467\n",
      "epoch 2784, loss 0.013143674470484257, R2 0.4536692500114441\n",
      "Eval loss 0.013205938041210175, R2 0.4913214445114136\n",
      "epoch 2785, loss 0.013143154792487621, R2 0.4536908268928528\n",
      "Eval loss 0.013205374591052532, R2 0.49134302139282227\n",
      "epoch 2786, loss 0.013142635114490986, R2 0.45371294021606445\n",
      "Eval loss 0.013204810209572315, R2 0.49136465787887573\n",
      "epoch 2787, loss 0.01314211543649435, R2 0.45373421907424927\n",
      "Eval loss 0.013204246759414673, R2 0.49138617515563965\n",
      "epoch 2788, loss 0.013141595758497715, R2 0.4537559747695923\n",
      "Eval loss 0.013203684240579605, R2 0.49140816926956177\n",
      "epoch 2789, loss 0.013141077011823654, R2 0.4537771940231323\n",
      "Eval loss 0.013203120790421963, R2 0.49142956733703613\n",
      "epoch 2790, loss 0.013140558265149593, R2 0.4537994861602783\n",
      "Eval loss 0.01320255734026432, R2 0.4914513826370239\n",
      "epoch 2791, loss 0.013140039518475533, R2 0.453821063041687\n",
      "Eval loss 0.013201996684074402, R2 0.4914727210998535\n",
      "epoch 2792, loss 0.013139522634446621, R2 0.4538419842720032\n",
      "Eval loss 0.01320143323391676, R2 0.49149441719055176\n",
      "epoch 2793, loss 0.01313900575041771, R2 0.4538641571998596\n",
      "Eval loss 0.013200872577726841, R2 0.4915162920951843\n",
      "epoch 2794, loss 0.013138487935066223, R2 0.4538848400115967\n",
      "Eval loss 0.013200311921536922, R2 0.4915379285812378\n",
      "epoch 2795, loss 0.013137971051037312, R2 0.45390617847442627\n",
      "Eval loss 0.013199752196669579, R2 0.4915594458580017\n",
      "epoch 2796, loss 0.013137453235685825, R2 0.4539278745651245\n",
      "Eval loss 0.013199192471802235, R2 0.4915813207626343\n",
      "epoch 2797, loss 0.013136938214302063, R2 0.4539492726325989\n",
      "Eval loss 0.013198629021644592, R2 0.4916030168533325\n",
      "epoch 2798, loss 0.013136422261595726, R2 0.4539705514907837\n",
      "Eval loss 0.013198072090744972, R2 0.4916238784790039\n",
      "epoch 2799, loss 0.013135906308889389, R2 0.4539921283721924\n",
      "Eval loss 0.013197512365877628, R2 0.49164557456970215\n",
      "epoch 2800, loss 0.013135391287505627, R2 0.45401352643966675\n",
      "Eval loss 0.013196953572332859, R2 0.4916672706604004\n",
      "epoch 2801, loss 0.01313487533479929, R2 0.45403534173965454\n",
      "Eval loss 0.01319639477878809, R2 0.49168890714645386\n",
      "epoch 2802, loss 0.013134361244738102, R2 0.4540565609931946\n",
      "Eval loss 0.013195836916565895, R2 0.4917104244232178\n",
      "epoch 2803, loss 0.01313384622335434, R2 0.4540777802467346\n",
      "Eval loss 0.013195278123021126, R2 0.4917319416999817\n",
      "epoch 2804, loss 0.013133333064615726, R2 0.4540991187095642\n",
      "Eval loss 0.013194721192121506, R2 0.49175339937210083\n",
      "epoch 2805, loss 0.013132819905877113, R2 0.454120397567749\n",
      "Eval loss 0.01319416519254446, R2 0.4917744994163513\n",
      "epoch 2806, loss 0.0131323067471385, R2 0.45414191484451294\n",
      "Eval loss 0.01319360826164484, R2 0.4917961359024048\n",
      "epoch 2807, loss 0.013131794519722462, R2 0.4541635513305664\n",
      "Eval loss 0.013193050399422646, R2 0.49181729555130005\n",
      "epoch 2808, loss 0.013131282292306423, R2 0.45418453216552734\n",
      "Eval loss 0.013192495331168175, R2 0.4918386936187744\n",
      "epoch 2809, loss 0.013130770064890385, R2 0.45420563220977783\n",
      "Eval loss 0.013191940262913704, R2 0.49186068773269653\n",
      "epoch 2810, loss 0.01313025876879692, R2 0.454227089881897\n",
      "Eval loss 0.013191385194659233, R2 0.4918816089630127\n",
      "epoch 2811, loss 0.013129746541380882, R2 0.45424872636795044\n",
      "Eval loss 0.013190830126404762, R2 0.49190282821655273\n",
      "epoch 2812, loss 0.013129235245287418, R2 0.4542691111564636\n",
      "Eval loss 0.013190275989472866, R2 0.4919247627258301\n",
      "epoch 2813, loss 0.013128723949193954, R2 0.4542912244796753\n",
      "Eval loss 0.01318972185254097, R2 0.4919455051422119\n",
      "epoch 2814, loss 0.013128213584423065, R2 0.45431190729141235\n",
      "Eval loss 0.013189168646931648, R2 0.4919671416282654\n",
      "epoch 2815, loss 0.013127703219652176, R2 0.45433342456817627\n",
      "Eval loss 0.013188615441322327, R2 0.49198853969573975\n",
      "epoch 2816, loss 0.013127193786203861, R2 0.45435482263565063\n",
      "Eval loss 0.013188062235713005, R2 0.492009699344635\n",
      "epoch 2817, loss 0.013126684352755547, R2 0.4543759822845459\n",
      "Eval loss 0.013187510892748833, R2 0.4920308589935303\n",
      "epoch 2818, loss 0.013126175850629807, R2 0.45439743995666504\n",
      "Eval loss 0.013186958618462086, R2 0.49205225706100464\n",
      "epoch 2819, loss 0.013125667348504066, R2 0.45441770553588867\n",
      "Eval loss 0.013186406344175339, R2 0.49207353591918945\n",
      "epoch 2820, loss 0.013125157915055752, R2 0.45443886518478394\n",
      "Eval loss 0.013185854069888592, R2 0.4920949339866638\n",
      "epoch 2821, loss 0.013124649412930012, R2 0.4544602632522583\n",
      "Eval loss 0.01318530272692442, R2 0.49211591482162476\n",
      "epoch 2822, loss 0.013124141842126846, R2 0.4544811248779297\n",
      "Eval loss 0.013184753246605396, R2 0.4921373724937439\n",
      "epoch 2823, loss 0.013123634271323681, R2 0.45450276136398315\n",
      "Eval loss 0.013184202834963799, R2 0.49215853214263916\n",
      "epoch 2824, loss 0.013123126700520515, R2 0.45452314615249634\n",
      "Eval loss 0.013183651491999626, R2 0.49217933416366577\n",
      "epoch 2825, loss 0.013122620061039925, R2 0.45454442501068115\n",
      "Eval loss 0.013183102943003178, R2 0.49220049381256104\n",
      "epoch 2826, loss 0.013122114352881908, R2 0.45456594228744507\n",
      "Eval loss 0.013182554394006729, R2 0.4922215938568115\n",
      "epoch 2827, loss 0.013121607713401318, R2 0.4545864462852478\n",
      "Eval loss 0.013182003982365131, R2 0.49224281311035156\n",
      "epoch 2828, loss 0.013121102005243301, R2 0.45460832118988037\n",
      "Eval loss 0.013181456364691257, R2 0.4922642111778259\n",
      "epoch 2829, loss 0.01312059722840786, R2 0.45462846755981445\n",
      "Eval loss 0.013180907815694809, R2 0.4922851324081421\n",
      "epoch 2830, loss 0.013120091520249844, R2 0.45464950799942017\n",
      "Eval loss 0.01318036112934351, R2 0.49230653047561646\n",
      "epoch 2831, loss 0.013119586743414402, R2 0.4546704888343811\n",
      "Eval loss 0.013179811649024487, R2 0.49232739210128784\n",
      "epoch 2832, loss 0.01311908382922411, R2 0.4546915888786316\n",
      "Eval loss 0.013179265893995762, R2 0.4923485517501831\n",
      "epoch 2833, loss 0.013118578121066093, R2 0.4547129273414612\n",
      "Eval loss 0.013178718276321888, R2 0.4923696517944336\n",
      "epoch 2834, loss 0.013118073344230652, R2 0.4547339081764221\n",
      "Eval loss 0.013178173452615738, R2 0.49239033460617065\n",
      "epoch 2835, loss 0.013117569498717785, R2 0.4547542929649353\n",
      "Eval loss 0.013177626766264439, R2 0.49241214990615845\n",
      "epoch 2836, loss 0.013117068447172642, R2 0.45477569103240967\n",
      "Eval loss 0.013177081011235714, R2 0.4924325942993164\n",
      "epoch 2837, loss 0.01311656553298235, R2 0.4547958970069885\n",
      "Eval loss 0.01317653525620699, R2 0.49245375394821167\n",
      "epoch 2838, loss 0.013116062618792057, R2 0.454816997051239\n",
      "Eval loss 0.013175991363823414, R2 0.49247485399246216\n",
      "epoch 2839, loss 0.013115559704601765, R2 0.4548383951187134\n",
      "Eval loss 0.013175446540117264, R2 0.4924958348274231\n",
      "epoch 2840, loss 0.013115057721734047, R2 0.4548594355583191\n",
      "Eval loss 0.013174902647733688, R2 0.49251633882522583\n",
      "epoch 2841, loss 0.013114559464156628, R2 0.45487964153289795\n",
      "Eval loss 0.013174358755350113, R2 0.4925379157066345\n",
      "epoch 2842, loss 0.01311405561864376, R2 0.4549003839492798\n",
      "Eval loss 0.013173815794289112, R2 0.4925583600997925\n",
      "epoch 2843, loss 0.013113556429743767, R2 0.4549211263656616\n",
      "Eval loss 0.013173270970582962, R2 0.4925796389579773\n",
      "epoch 2844, loss 0.013113054446876049, R2 0.4549418091773987\n",
      "Eval loss 0.013172728940844536, R2 0.49260038137435913\n",
      "epoch 2845, loss 0.01311255432665348, R2 0.4549627900123596\n",
      "Eval loss 0.01317218691110611, R2 0.4926212430000305\n",
      "epoch 2846, loss 0.013112054206430912, R2 0.4549841284751892\n",
      "Eval loss 0.013171644881367683, R2 0.4926421046257019\n",
      "epoch 2847, loss 0.013111555948853493, R2 0.4550044536590576\n",
      "Eval loss 0.013171103782951832, R2 0.4926631450653076\n",
      "epoch 2848, loss 0.013111053965985775, R2 0.4550251364707947\n",
      "Eval loss 0.013170560821890831, R2 0.492684006690979\n",
      "epoch 2849, loss 0.013110557571053505, R2 0.45504581928253174\n",
      "Eval loss 0.013170020654797554, R2 0.49270451068878174\n",
      "epoch 2850, loss 0.013110057450830936, R2 0.4550667405128479\n",
      "Eval loss 0.013169479556381702, R2 0.49272555112838745\n",
      "epoch 2851, loss 0.013109560124576092, R2 0.4550876021385193\n",
      "Eval loss 0.013168939389288425, R2 0.49274665117263794\n",
      "epoch 2852, loss 0.013109060935676098, R2 0.45510798692703247\n",
      "Eval loss 0.013168400153517723, R2 0.492766797542572\n",
      "epoch 2853, loss 0.013108564540743828, R2 0.45512861013412476\n",
      "Eval loss 0.01316786091774702, R2 0.4927878975868225\n",
      "epoch 2854, loss 0.013108067214488983, R2 0.4551492929458618\n",
      "Eval loss 0.013167319819331169, R2 0.49280840158462524\n",
      "epoch 2855, loss 0.013107570819556713, R2 0.4551700949668884\n",
      "Eval loss 0.01316678337752819, R2 0.49282926321029663\n",
      "epoch 2856, loss 0.013107073493301868, R2 0.45519059896469116\n",
      "Eval loss 0.013166243210434914, R2 0.4928501844406128\n",
      "epoch 2857, loss 0.013106578961014748, R2 0.45521116256713867\n",
      "Eval loss 0.013165706768631935, R2 0.49287116527557373\n",
      "epoch 2858, loss 0.013106080703437328, R2 0.45523184537887573\n",
      "Eval loss 0.013165167532861233, R2 0.4928913116455078\n",
      "epoch 2859, loss 0.013105586171150208, R2 0.45525312423706055\n",
      "Eval loss 0.01316463015973568, R2 0.4929123520851135\n",
      "epoch 2860, loss 0.013105090707540512, R2 0.4552728533744812\n",
      "Eval loss 0.013164094649255276, R2 0.49293291568756104\n",
      "epoch 2861, loss 0.013104596175253391, R2 0.45529359579086304\n",
      "Eval loss 0.013163557276129723, R2 0.49295347929000854\n",
      "epoch 2862, loss 0.01310410164296627, R2 0.4553146958351135\n",
      "Eval loss 0.01316301990300417, R2 0.49297404289245605\n",
      "epoch 2863, loss 0.01310360711067915, R2 0.4553346633911133\n",
      "Eval loss 0.013162486255168915, R2 0.4929947853088379\n",
      "epoch 2864, loss 0.013103113509714603, R2 0.45535504817962646\n",
      "Eval loss 0.013161948882043362, R2 0.49301576614379883\n",
      "epoch 2865, loss 0.013102618977427483, R2 0.45537644624710083\n",
      "Eval loss 0.013161414302885532, R2 0.49303680658340454\n",
      "epoch 2866, loss 0.013102125376462936, R2 0.45539629459381104\n",
      "Eval loss 0.013160878792405128, R2 0.4930565357208252\n",
      "epoch 2867, loss 0.013101632706820965, R2 0.455417275428772\n",
      "Eval loss 0.0131603442132473, R2 0.4930773973464966\n",
      "epoch 2868, loss 0.013101140968501568, R2 0.45543718338012695\n",
      "Eval loss 0.01315980963408947, R2 0.49309754371643066\n",
      "epoch 2869, loss 0.013100647367537022, R2 0.45545822381973267\n",
      "Eval loss 0.013159275986254215, R2 0.4931185841560364\n",
      "epoch 2870, loss 0.013100155629217625, R2 0.4554790258407593\n",
      "Eval loss 0.013158743269741535, R2 0.49313926696777344\n",
      "epoch 2871, loss 0.013099662959575653, R2 0.4554986357688904\n",
      "Eval loss 0.01315820962190628, R2 0.49315935373306274\n",
      "epoch 2872, loss 0.01309917215257883, R2 0.45551902055740356\n",
      "Eval loss 0.013157677836716175, R2 0.4931802749633789\n",
      "epoch 2873, loss 0.013098682276904583, R2 0.45553940534591675\n",
      "Eval loss 0.013157145120203495, R2 0.4932006001472473\n",
      "epoch 2874, loss 0.013098190538585186, R2 0.4555598497390747\n",
      "Eval loss 0.013156612403690815, R2 0.4932211637496948\n",
      "epoch 2875, loss 0.013097699731588364, R2 0.45558005571365356\n",
      "Eval loss 0.013156082481145859, R2 0.49324172735214233\n",
      "epoch 2876, loss 0.013097209855914116, R2 0.45560044050216675\n",
      "Eval loss 0.013155548833310604, R2 0.49326199293136597\n",
      "epoch 2877, loss 0.013096719980239868, R2 0.4556209444999695\n",
      "Eval loss 0.013155017979443073, R2 0.4932825565338135\n",
      "epoch 2878, loss 0.013096229173243046, R2 0.455641508102417\n",
      "Eval loss 0.013154488988220692, R2 0.49330294132232666\n",
      "epoch 2879, loss 0.013095742091536522, R2 0.4556621313095093\n",
      "Eval loss 0.013153956271708012, R2 0.4933234453201294\n",
      "epoch 2880, loss 0.013095253147184849, R2 0.4556824564933777\n",
      "Eval loss 0.01315342728048563, R2 0.4933440089225769\n",
      "epoch 2881, loss 0.013094764202833176, R2 0.45570260286331177\n",
      "Eval loss 0.013152899220585823, R2 0.4933639168739319\n",
      "epoch 2882, loss 0.013094276189804077, R2 0.4557226896286011\n",
      "Eval loss 0.013152368366718292, R2 0.49338459968566895\n",
      "epoch 2883, loss 0.013093789108097553, R2 0.4557427763938904\n",
      "Eval loss 0.01315183937549591, R2 0.49340498447418213\n",
      "epoch 2884, loss 0.01309330016374588, R2 0.4557632803916931\n",
      "Eval loss 0.013151312246918678, R2 0.493425190448761\n",
      "epoch 2885, loss 0.01309281401336193, R2 0.4557838439941406\n",
      "Eval loss 0.013150783255696297, R2 0.4934456944465637\n",
      "epoch 2886, loss 0.013092326000332832, R2 0.45580410957336426\n",
      "Eval loss 0.013150254264473915, R2 0.493465781211853\n",
      "epoch 2887, loss 0.013091838918626308, R2 0.4558238387107849\n",
      "Eval loss 0.013149727135896683, R2 0.4934864044189453\n",
      "epoch 2888, loss 0.013091354630887508, R2 0.4558447003364563\n",
      "Eval loss 0.013149199075996876, R2 0.49350684881210327\n",
      "epoch 2889, loss 0.013090865686535835, R2 0.4558646082878113\n",
      "Eval loss 0.013148671947419643, R2 0.49352699518203735\n",
      "epoch 2890, loss 0.013090381398797035, R2 0.4558843970298767\n",
      "Eval loss 0.013148145750164986, R2 0.49354714155197144\n",
      "epoch 2891, loss 0.01308989617973566, R2 0.45590460300445557\n",
      "Eval loss 0.013147621415555477, R2 0.4935673475265503\n",
      "epoch 2892, loss 0.013089410960674286, R2 0.45592474937438965\n",
      "Eval loss 0.013147094286978245, R2 0.4935874938964844\n",
      "epoch 2893, loss 0.013088925741612911, R2 0.45594513416290283\n",
      "Eval loss 0.013146568089723587, R2 0.4936080574989319\n",
      "epoch 2894, loss 0.013088441453874111, R2 0.45596522092819214\n",
      "Eval loss 0.013146043755114079, R2 0.4936280846595764\n",
      "epoch 2895, loss 0.013087958097457886, R2 0.4559851288795471\n",
      "Eval loss 0.013145520351827145, R2 0.49364858865737915\n",
      "epoch 2896, loss 0.01308747474104166, R2 0.4560052752494812\n",
      "Eval loss 0.013144995085895061, R2 0.493668794631958\n",
      "epoch 2897, loss 0.013086991384625435, R2 0.4560258388519287\n",
      "Eval loss 0.013144470751285553, R2 0.4936891198158264\n",
      "epoch 2898, loss 0.01308650802820921, R2 0.4560454487800598\n",
      "Eval loss 0.013143946416676044, R2 0.4937092065811157\n",
      "epoch 2899, loss 0.013086025603115559, R2 0.4560653567314148\n",
      "Eval loss 0.01314342301338911, R2 0.4937290549278259\n",
      "epoch 2900, loss 0.013085542246699333, R2 0.456085741519928\n",
      "Eval loss 0.013142900541424751, R2 0.4937494993209839\n",
      "epoch 2901, loss 0.013085059821605682, R2 0.4561057686805725\n",
      "Eval loss 0.013142379000782967, R2 0.4937695860862732\n",
      "epoch 2902, loss 0.01308457925915718, R2 0.4561256170272827\n",
      "Eval loss 0.013141855597496033, R2 0.4937896132469177\n",
      "epoch 2903, loss 0.01308409683406353, R2 0.45614564418792725\n",
      "Eval loss 0.013141332194209099, R2 0.49380970001220703\n",
      "epoch 2904, loss 0.013083615340292454, R2 0.45616620779037476\n",
      "Eval loss 0.013140811584889889, R2 0.49382948875427246\n",
      "epoch 2905, loss 0.013083134777843952, R2 0.4561861753463745\n",
      "Eval loss 0.013140290975570679, R2 0.4938499927520752\n",
      "epoch 2906, loss 0.01308265421539545, R2 0.45620614290237427\n",
      "Eval loss 0.013139769434928894, R2 0.4938696026802063\n",
      "epoch 2907, loss 0.013082172721624374, R2 0.4562256336212158\n",
      "Eval loss 0.013139248825609684, R2 0.49389010667800903\n",
      "epoch 2908, loss 0.013081694021821022, R2 0.45624589920043945\n",
      "Eval loss 0.013138728216290474, R2 0.493910014629364\n",
      "epoch 2909, loss 0.01308121532201767, R2 0.4562654495239258\n",
      "Eval loss 0.013138208538293839, R2 0.49393051862716675\n",
      "epoch 2910, loss 0.013080733828246593, R2 0.45628541707992554\n",
      "Eval loss 0.013137689791619778, R2 0.4939500689506531\n",
      "epoch 2911, loss 0.01308025699108839, R2 0.45630526542663574\n",
      "Eval loss 0.013137171044945717, R2 0.4939698576927185\n",
      "epoch 2912, loss 0.013079776428639889, R2 0.4563252329826355\n",
      "Eval loss 0.013136652298271656, R2 0.4939900040626526\n",
      "epoch 2913, loss 0.013079298660159111, R2 0.45634472370147705\n",
      "Eval loss 0.01313613262027502, R2 0.4940100312232971\n",
      "epoch 2914, loss 0.013078820891678333, R2 0.4563654661178589\n",
      "Eval loss 0.013135614804923534, R2 0.49402981996536255\n",
      "epoch 2915, loss 0.013078342191874981, R2 0.45638519525527954\n",
      "Eval loss 0.013135096058249474, R2 0.4940497875213623\n",
      "epoch 2916, loss 0.013077865354716778, R2 0.4564048647880554\n",
      "Eval loss 0.013134580105543137, R2 0.49406999349594116\n",
      "epoch 2917, loss 0.013077387586236, R2 0.4564245343208313\n",
      "Eval loss 0.013134061358869076, R2 0.49408960342407227\n",
      "epoch 2918, loss 0.013076911680400372, R2 0.45644479990005493\n",
      "Eval loss 0.013133544474840164, R2 0.494109570980072\n",
      "epoch 2919, loss 0.013076434843242168, R2 0.4564642906188965\n",
      "Eval loss 0.013133030384778976, R2 0.4941295385360718\n",
      "epoch 2920, loss 0.01307595893740654, R2 0.4564839005470276\n",
      "Eval loss 0.013132511638104916, R2 0.4941491484642029\n",
      "epoch 2921, loss 0.013075483031570911, R2 0.456503689289093\n",
      "Eval loss 0.013131994754076004, R2 0.4941697120666504\n",
      "epoch 2922, loss 0.013075007125735283, R2 0.4565238356590271\n",
      "Eval loss 0.013131481595337391, R2 0.4941890239715576\n",
      "epoch 2923, loss 0.013074531219899654, R2 0.4565434455871582\n",
      "Eval loss 0.013130965642631054, R2 0.494209349155426\n",
      "epoch 2924, loss 0.0130740562453866, R2 0.4565638303756714\n",
      "Eval loss 0.013130451552569866, R2 0.4942290186882019\n",
      "epoch 2925, loss 0.013073580339550972, R2 0.45658278465270996\n",
      "Eval loss 0.013129936531186104, R2 0.4942483901977539\n",
      "epoch 2926, loss 0.013073109090328217, R2 0.45660239458084106\n",
      "Eval loss 0.013129421509802341, R2 0.494268536567688\n",
      "epoch 2927, loss 0.013072635047137737, R2 0.45662206411361694\n",
      "Eval loss 0.013128908351063728, R2 0.49428826570510864\n",
      "epoch 2928, loss 0.013072160072624683, R2 0.4566423296928406\n",
      "Eval loss 0.013128395192325115, R2 0.4943079352378845\n",
      "epoch 2929, loss 0.013071686960756779, R2 0.4566616415977478\n",
      "Eval loss 0.013127880170941353, R2 0.4943279027938843\n",
      "epoch 2930, loss 0.0130712129175663, R2 0.45668119192123413\n",
      "Eval loss 0.013127368874847889, R2 0.49434757232666016\n",
      "epoch 2931, loss 0.013070741668343544, R2 0.4567006230354309\n",
      "Eval loss 0.013126855716109276, R2 0.4943673610687256\n",
      "epoch 2932, loss 0.01307026855647564, R2 0.45672041177749634\n",
      "Eval loss 0.013126342557370663, R2 0.49438726902008057\n",
      "epoch 2933, loss 0.013069795444607735, R2 0.45674026012420654\n",
      "Eval loss 0.013125831261277199, R2 0.49440664052963257\n",
      "epoch 2934, loss 0.013069323264062405, R2 0.45675987005233765\n",
      "Eval loss 0.01312532089650631, R2 0.4944264888763428\n",
      "epoch 2935, loss 0.013068851083517075, R2 0.4567793607711792\n",
      "Eval loss 0.013124809600412846, R2 0.49444615840911865\n",
      "epoch 2936, loss 0.013068380765616894, R2 0.4567989110946655\n",
      "Eval loss 0.013124299235641956, R2 0.49446600675582886\n",
      "epoch 2937, loss 0.013067910447716713, R2 0.4568186402320862\n",
      "Eval loss 0.013123787939548492, R2 0.49448567628860474\n",
      "epoch 2938, loss 0.013067439198493958, R2 0.45683807134628296\n",
      "Eval loss 0.013123277574777603, R2 0.4945048689842224\n",
      "epoch 2939, loss 0.013066968880593777, R2 0.4568576216697693\n",
      "Eval loss 0.013122768141329288, R2 0.4945247769355774\n",
      "epoch 2940, loss 0.01306649949401617, R2 0.45687776803970337\n",
      "Eval loss 0.013122258707880974, R2 0.4945445656776428\n",
      "epoch 2941, loss 0.013066028244793415, R2 0.4568968415260315\n",
      "Eval loss 0.01312174927443266, R2 0.4945645332336426\n",
      "epoch 2942, loss 0.013065559789538383, R2 0.4569162130355835\n",
      "Eval loss 0.013121240772306919, R2 0.4945836663246155\n",
      "epoch 2943, loss 0.013065090402960777, R2 0.4569358825683594\n",
      "Eval loss 0.013120732270181179, R2 0.49460339546203613\n",
      "epoch 2944, loss 0.013064621016383171, R2 0.4569551944732666\n",
      "Eval loss 0.013120223768055439, R2 0.4946228265762329\n",
      "epoch 2945, loss 0.01306415256112814, R2 0.45697516202926636\n",
      "Eval loss 0.013119716197252274, R2 0.4946421980857849\n",
      "epoch 2946, loss 0.013063685037195683, R2 0.45699477195739746\n",
      "Eval loss 0.013119208626449108, R2 0.4946616291999817\n",
      "epoch 2947, loss 0.013063215650618076, R2 0.4570136070251465\n",
      "Eval loss 0.013118701055645943, R2 0.4946814775466919\n",
      "epoch 2948, loss 0.013062749058008194, R2 0.4570331573486328\n",
      "Eval loss 0.013118195347487926, R2 0.49470090866088867\n",
      "epoch 2949, loss 0.013062281534075737, R2 0.45705240964889526\n",
      "Eval loss 0.013117688708007336, R2 0.494720458984375\n",
      "epoch 2950, loss 0.013061814941465855, R2 0.45707231760025024\n",
      "Eval loss 0.013117182068526745, R2 0.4947403073310852\n",
      "epoch 2951, loss 0.013061348348855972, R2 0.45709139108657837\n",
      "Eval loss 0.013116676360368729, R2 0.494759738445282\n",
      "epoch 2952, loss 0.013060882687568665, R2 0.45711058378219604\n",
      "Eval loss 0.013116171583533287, R2 0.49477875232696533\n",
      "epoch 2953, loss 0.013060414232313633, R2 0.4571305513381958\n",
      "Eval loss 0.01311566587537527, R2 0.4947982430458069\n",
      "epoch 2954, loss 0.0130599495023489, R2 0.45714932680130005\n",
      "Eval loss 0.01311516109853983, R2 0.4948180913925171\n",
      "epoch 2955, loss 0.013059484772384167, R2 0.4571690559387207\n",
      "Eval loss 0.013114658184349537, R2 0.49483704566955566\n",
      "epoch 2956, loss 0.013059019111096859, R2 0.45718806982040405\n",
      "Eval loss 0.01311415247619152, R2 0.4948563575744629\n",
      "epoch 2957, loss 0.0130585553124547, R2 0.45720797777175903\n",
      "Eval loss 0.013113650493323803, R2 0.4948761463165283\n",
      "epoch 2958, loss 0.013058089651167393, R2 0.45722687244415283\n",
      "Eval loss 0.013113145716488361, R2 0.4948952794075012\n",
      "epoch 2959, loss 0.01305762492120266, R2 0.45724648237228394\n",
      "Eval loss 0.013112641870975494, R2 0.49491482973098755\n",
      "epoch 2960, loss 0.013057162053883076, R2 0.4572651982307434\n",
      "Eval loss 0.013112140819430351, R2 0.4949338436126709\n",
      "epoch 2961, loss 0.013056698255240917, R2 0.45728451013565063\n",
      "Eval loss 0.013111636973917484, R2 0.4949532747268677\n",
      "epoch 2962, loss 0.013056233525276184, R2 0.4573040008544922\n",
      "Eval loss 0.013111135922372341, R2 0.4949728846549988\n",
      "epoch 2963, loss 0.013055769726634026, R2 0.4573230743408203\n",
      "Eval loss 0.013110632076859474, R2 0.4949922561645508\n",
      "epoch 2964, loss 0.013055307790637016, R2 0.4573425054550171\n",
      "Eval loss 0.013110131025314331, R2 0.4950118660926819\n",
      "epoch 2965, loss 0.013054845854640007, R2 0.4573613405227661\n",
      "Eval loss 0.013109629973769188, R2 0.4950311779975891\n",
      "epoch 2966, loss 0.013054382987320423, R2 0.4573807120323181\n",
      "Eval loss 0.01310912799090147, R2 0.4950503706932068\n",
      "epoch 2967, loss 0.013053921051323414, R2 0.4574001431465149\n",
      "Eval loss 0.013108628802001476, R2 0.4950692653656006\n",
      "epoch 2968, loss 0.01305346004664898, R2 0.4574192762374878\n",
      "Eval loss 0.013108129613101482, R2 0.4950888156890869\n",
      "epoch 2969, loss 0.013052999041974545, R2 0.4574384093284607\n",
      "Eval loss 0.013107629492878914, R2 0.49510806798934937\n",
      "epoch 2970, loss 0.01305253803730011, R2 0.45745742321014404\n",
      "Eval loss 0.013107129372656345, R2 0.49512720108032227\n",
      "epoch 2971, loss 0.0130520761013031, R2 0.45747697353363037\n",
      "Eval loss 0.013106628321111202, R2 0.4951462149620056\n",
      "epoch 2972, loss 0.013051616959273815, R2 0.4574957489967346\n",
      "Eval loss 0.013106130994856358, R2 0.4951656460762024\n",
      "epoch 2973, loss 0.01305115595459938, R2 0.4575154185295105\n",
      "Eval loss 0.013105632737278938, R2 0.4951845407485962\n",
      "epoch 2974, loss 0.01305069774389267, R2 0.45753395557403564\n",
      "Eval loss 0.013105133548378944, R2 0.49520403146743774\n",
      "epoch 2975, loss 0.01305023767054081, R2 0.4575532078742981\n",
      "Eval loss 0.013104635290801525, R2 0.49522310495376587\n",
      "epoch 2976, loss 0.01304977759718895, R2 0.45757216215133667\n",
      "Eval loss 0.013104138895869255, R2 0.4952423572540283\n",
      "epoch 2977, loss 0.013049320317804813, R2 0.45759117603302\n",
      "Eval loss 0.01310364156961441, R2 0.4952612519264221\n",
      "epoch 2978, loss 0.013048860244452953, R2 0.45761096477508545\n",
      "Eval loss 0.013103144243359566, R2 0.4952806830406189\n",
      "epoch 2979, loss 0.013048402965068817, R2 0.4576296806335449\n",
      "Eval loss 0.01310264877974987, R2 0.49529945850372314\n",
      "epoch 2980, loss 0.013047943823039532, R2 0.4576483964920044\n",
      "Eval loss 0.013102151453495026, R2 0.4953188896179199\n",
      "epoch 2981, loss 0.01304748747497797, R2 0.45766735076904297\n",
      "Eval loss 0.013101656921207905, R2 0.49533796310424805\n",
      "epoch 2982, loss 0.01304702926427126, R2 0.45768606662750244\n",
      "Eval loss 0.01310115959495306, R2 0.49535709619522095\n",
      "epoch 2983, loss 0.013046573847532272, R2 0.45770537853240967\n",
      "Eval loss 0.01310066506266594, R2 0.4953758716583252\n",
      "epoch 2984, loss 0.013046115636825562, R2 0.457724392414093\n",
      "Eval loss 0.013100172393023968, R2 0.495395302772522\n",
      "epoch 2985, loss 0.013045658357441425, R2 0.4577435851097107\n",
      "Eval loss 0.013099675998091698, R2 0.495414137840271\n",
      "epoch 2986, loss 0.013045202009379864, R2 0.4577628970146179\n",
      "Eval loss 0.013099181465804577, R2 0.4954330325126648\n",
      "epoch 2987, loss 0.013044746592640877, R2 0.45778167247772217\n",
      "Eval loss 0.013098686933517456, R2 0.4954519271850586\n",
      "epoch 2988, loss 0.013044290244579315, R2 0.45779991149902344\n",
      "Eval loss 0.01309819333255291, R2 0.495471715927124\n",
      "epoch 2989, loss 0.013043836690485477, R2 0.45781946182250977\n",
      "Eval loss 0.013097699731588364, R2 0.49549007415771484\n",
      "epoch 2990, loss 0.01304338127374649, R2 0.4578379988670349\n",
      "Eval loss 0.013097207061946392, R2 0.49550968408584595\n",
      "epoch 2991, loss 0.013042926788330078, R2 0.4578569531440735\n",
      "Eval loss 0.013096715323626995, R2 0.49552834033966064\n",
      "epoch 2992, loss 0.013042472302913666, R2 0.4578758478164673\n",
      "Eval loss 0.013096220791339874, R2 0.4955478310585022\n",
      "epoch 2993, loss 0.013042016886174679, R2 0.4578947424888611\n",
      "Eval loss 0.013095729984343052, R2 0.49556630849838257\n",
      "epoch 2994, loss 0.013041564263403416, R2 0.45791375637054443\n",
      "Eval loss 0.01309523731470108, R2 0.495585560798645\n",
      "epoch 2995, loss 0.013041109777987003, R2 0.45793312788009644\n",
      "Eval loss 0.013094747439026833, R2 0.49560409784317017\n",
      "epoch 2996, loss 0.01304065715521574, R2 0.45795130729675293\n",
      "Eval loss 0.013094254769384861, R2 0.49562329053878784\n",
      "epoch 2997, loss 0.013040203601121902, R2 0.45797061920166016\n",
      "Eval loss 0.01309376209974289, R2 0.4956420660018921\n",
      "epoch 2998, loss 0.01303975097835064, R2 0.4579890966415405\n",
      "Eval loss 0.013093275018036366, R2 0.4956611394882202\n",
      "epoch 2999, loss 0.01303929928690195, R2 0.4580078721046448\n",
      "Eval loss 0.013092785142362118, R2 0.4956798553466797\n",
      "epoch 3000, loss 0.013038846664130688, R2 0.4580265283584595\n",
      "Eval loss 0.01309229526668787, R2 0.49569857120513916\n",
      "epoch 3001, loss 0.013038394972682, R2 0.4580453038215637\n",
      "Eval loss 0.013091804459691048, R2 0.4957176446914673\n",
      "epoch 3002, loss 0.01303794328123331, R2 0.4580642580986023\n",
      "Eval loss 0.013091315515339375, R2 0.49573636054992676\n",
      "epoch 3003, loss 0.013037491589784622, R2 0.4580833315849304\n",
      "Eval loss 0.013090825639665127, R2 0.49575531482696533\n",
      "epoch 3004, loss 0.013037040829658508, R2 0.458102285861969\n",
      "Eval loss 0.013090339489281178, R2 0.49577420949935913\n",
      "epoch 3005, loss 0.013036591932177544, R2 0.45812028646469116\n",
      "Eval loss 0.013089850544929504, R2 0.4957927465438843\n",
      "epoch 3006, loss 0.013036140240728855, R2 0.45813918113708496\n",
      "Eval loss 0.013089362531900406, R2 0.495811402797699\n",
      "epoch 3007, loss 0.01303569134324789, R2 0.4581581950187683\n",
      "Eval loss 0.013088874518871307, R2 0.49583035707473755\n",
      "epoch 3008, loss 0.013035241514444351, R2 0.4581763744354248\n",
      "Eval loss 0.013088387437164783, R2 0.4958491325378418\n",
      "epoch 3009, loss 0.013034790754318237, R2 0.4581952691078186\n",
      "Eval loss 0.01308790035545826, R2 0.4958680272102356\n",
      "epoch 3010, loss 0.013034340925514698, R2 0.4582136273384094\n",
      "Eval loss 0.01308741420507431, R2 0.4958866238594055\n",
      "epoch 3011, loss 0.013033892028033733, R2 0.45823246240615845\n",
      "Eval loss 0.013086926192045212, R2 0.49590539932250977\n",
      "epoch 3012, loss 0.013033444061875343, R2 0.4582517743110657\n",
      "Eval loss 0.013086440972983837, R2 0.49592363834381104\n",
      "epoch 3013, loss 0.013032995164394379, R2 0.45827025175094604\n",
      "Eval loss 0.013085955753922462, R2 0.49594247341156006\n",
      "epoch 3014, loss 0.013032547198235989, R2 0.4582885503768921\n",
      "Eval loss 0.013085470534861088, R2 0.49596118927001953\n",
      "epoch 3015, loss 0.013032101094722748, R2 0.45830708742141724\n",
      "Eval loss 0.013084984384477139, R2 0.49598050117492676\n",
      "epoch 3016, loss 0.013031653128564358, R2 0.45832550525665283\n",
      "Eval loss 0.013084500096738338, R2 0.4959985613822937\n",
      "epoch 3017, loss 0.013031206093728542, R2 0.45834410190582275\n",
      "Eval loss 0.013084016740322113, R2 0.4960171580314636\n",
      "epoch 3018, loss 0.013030758127570152, R2 0.45836275815963745\n",
      "Eval loss 0.013083532452583313, R2 0.4960361123085022\n",
      "epoch 3019, loss 0.013030312024056911, R2 0.4583812952041626\n",
      "Eval loss 0.013083048164844513, R2 0.4960547685623169\n",
      "epoch 3020, loss 0.01302986592054367, R2 0.45839983224868774\n",
      "Eval loss 0.013082565739750862, R2 0.4960733652114868\n",
      "epoch 3021, loss 0.013029418885707855, R2 0.45841890573501587\n",
      "Eval loss 0.013082079589366913, R2 0.4960917830467224\n",
      "epoch 3022, loss 0.013028972782194614, R2 0.4584369659423828\n",
      "Eval loss 0.013081599958240986, R2 0.49611037969589233\n",
      "epoch 3023, loss 0.013028529472649097, R2 0.4584553837776184\n",
      "Eval loss 0.013081115670502186, R2 0.4961293339729309\n",
      "epoch 3024, loss 0.013028083369135857, R2 0.4584740996360779\n",
      "Eval loss 0.01308063417673111, R2 0.49614763259887695\n",
      "epoch 3025, loss 0.013027637265622616, R2 0.458492636680603\n",
      "Eval loss 0.013080152682960033, R2 0.496166467666626\n",
      "epoch 3026, loss 0.013027193024754524, R2 0.45851075649261475\n",
      "Eval loss 0.013079670257866383, R2 0.4961848855018616\n",
      "epoch 3027, loss 0.013026749715209007, R2 0.45852935314178467\n",
      "Eval loss 0.013079189695417881, R2 0.49620354175567627\n",
      "epoch 3028, loss 0.013026305474340916, R2 0.45854848623275757\n",
      "Eval loss 0.013078710064291954, R2 0.4962216019630432\n",
      "epoch 3029, loss 0.013025862164795399, R2 0.45856624841690063\n",
      "Eval loss 0.013078227639198303, R2 0.49624013900756836\n",
      "epoch 3030, loss 0.013025418855249882, R2 0.45858484506607056\n",
      "Eval loss 0.013077747076749802, R2 0.4962589740753174\n",
      "epoch 3031, loss 0.01302497461438179, R2 0.4586033225059509\n",
      "Eval loss 0.013077265582978725, R2 0.4962773323059082\n",
      "epoch 3032, loss 0.013024532236158848, R2 0.4586215019226074\n",
      "Eval loss 0.013076787814497948, R2 0.4962960481643677\n",
      "epoch 3033, loss 0.013024089857935905, R2 0.45864027738571167\n",
      "Eval loss 0.013076309114694595, R2 0.4963146448135376\n",
      "epoch 3034, loss 0.013023648411035538, R2 0.45865893363952637\n",
      "Eval loss 0.013075829483568668, R2 0.49633312225341797\n",
      "epoch 3035, loss 0.01302320696413517, R2 0.45867645740509033\n",
      "Eval loss 0.013075349852442741, R2 0.49635130167007446\n",
      "epoch 3036, loss 0.013022764585912228, R2 0.4586957097053528\n",
      "Eval loss 0.013074872083961964, R2 0.49636971950531006\n",
      "epoch 3037, loss 0.013022322207689285, R2 0.4587133526802063\n",
      "Eval loss 0.013074393384158611, R2 0.49638813734054565\n",
      "epoch 3038, loss 0.013021881692111492, R2 0.45873236656188965\n",
      "Eval loss 0.013073915615677834, R2 0.4964066743850708\n",
      "epoch 3039, loss 0.01302143931388855, R2 0.4587506055831909\n",
      "Eval loss 0.01307343877851963, R2 0.49642491340637207\n",
      "epoch 3040, loss 0.013021000660955906, R2 0.4587683081626892\n",
      "Eval loss 0.013072961941361427, R2 0.49644315242767334\n",
      "epoch 3041, loss 0.013020559214055538, R2 0.45878666639328003\n",
      "Eval loss 0.013072485104203224, R2 0.4964616298675537\n",
      "epoch 3042, loss 0.01302011962980032, R2 0.4588050842285156\n",
      "Eval loss 0.013072009198367596, R2 0.49647998809814453\n",
      "epoch 3043, loss 0.013019680045545101, R2 0.45882338285446167\n",
      "Eval loss 0.013071531429886818, R2 0.4964985251426697\n",
      "epoch 3044, loss 0.013019241392612457, R2 0.4588419795036316\n",
      "Eval loss 0.01307105552405119, R2 0.49651700258255005\n",
      "epoch 3045, loss 0.013018802739679813, R2 0.45886021852493286\n",
      "Eval loss 0.013070580549538136, R2 0.49653488397598267\n",
      "epoch 3046, loss 0.01301836408674717, R2 0.45887792110443115\n",
      "Eval loss 0.013070104643702507, R2 0.49655336141586304\n",
      "epoch 3047, loss 0.013017925433814526, R2 0.4588966369628906\n",
      "Eval loss 0.013069629669189453, R2 0.49657148122787476\n",
      "epoch 3048, loss 0.013017484918236732, R2 0.45891445875167847\n",
      "Eval loss 0.013069155625998974, R2 0.49658989906311035\n",
      "epoch 3049, loss 0.013017048127949238, R2 0.45893263816833496\n",
      "Eval loss 0.01306868065148592, R2 0.4966081976890564\n",
      "epoch 3050, loss 0.013016610406339169, R2 0.4589509963989258\n",
      "Eval loss 0.013068205676972866, R2 0.49662649631500244\n",
      "epoch 3051, loss 0.0130161726847291, R2 0.4589693546295166\n",
      "Eval loss 0.013067733496427536, R2 0.4966447949409485\n",
      "epoch 3052, loss 0.01301573682576418, R2 0.45898765325546265\n",
      "Eval loss 0.013067260384559631, R2 0.4966629147529602\n",
      "epoch 3053, loss 0.01301530096679926, R2 0.4590052366256714\n",
      "Eval loss 0.013066785410046577, R2 0.4966811537742615\n",
      "epoch 3054, loss 0.013014864176511765, R2 0.4590234160423279\n",
      "Eval loss 0.013066313229501247, R2 0.49669957160949707\n",
      "epoch 3055, loss 0.01301442738622427, R2 0.4590415358543396\n",
      "Eval loss 0.013065841048955917, R2 0.49671727418899536\n",
      "epoch 3056, loss 0.0130139896646142, R2 0.45905977487564087\n",
      "Eval loss 0.013065368868410587, R2 0.49673545360565186\n",
      "epoch 3057, loss 0.01301355380564928, R2 0.45907801389694214\n",
      "Eval loss 0.013064896687865257, R2 0.49675410985946655\n",
      "epoch 3058, loss 0.01301311794668436, R2 0.4590957760810852\n",
      "Eval loss 0.013064424507319927, R2 0.4967724084854126\n",
      "epoch 3059, loss 0.01301268395036459, R2 0.4591140151023865\n",
      "Eval loss 0.013063953258097172, R2 0.49678999185562134\n",
      "epoch 3060, loss 0.013012249022722244, R2 0.4591326117515564\n",
      "Eval loss 0.013063481077551842, R2 0.49680817127227783\n",
      "epoch 3061, loss 0.013011813163757324, R2 0.45915019512176514\n",
      "Eval loss 0.013063010759651661, R2 0.4968264102935791\n",
      "epoch 3062, loss 0.013011381030082703, R2 0.45916831493377686\n",
      "Eval loss 0.013062541373074055, R2 0.49684470891952515\n",
      "epoch 3063, loss 0.013010946102440357, R2 0.45918625593185425\n",
      "Eval loss 0.013062071986496449, R2 0.49686259031295776\n",
      "epoch 3064, loss 0.013010511174798012, R2 0.4592044949531555\n",
      "Eval loss 0.013061601668596268, R2 0.4968808889389038\n",
      "epoch 3065, loss 0.01301007904112339, R2 0.4592222571372986\n",
      "Eval loss 0.013061132282018661, R2 0.49689924716949463\n",
      "epoch 3066, loss 0.01300964504480362, R2 0.45924049615859985\n",
      "Eval loss 0.013060662895441055, R2 0.4969167113304138\n",
      "epoch 3067, loss 0.013009212911128998, R2 0.459258496761322\n",
      "Eval loss 0.013060194440186024, R2 0.4969353675842285\n",
      "epoch 3068, loss 0.013008779846131802, R2 0.45927631855010986\n",
      "Eval loss 0.013059725984930992, R2 0.49695295095443726\n",
      "epoch 3069, loss 0.01300834771245718, R2 0.45929425954818726\n",
      "Eval loss 0.013059255667030811, R2 0.4969712495803833\n",
      "epoch 3070, loss 0.013007914647459984, R2 0.4593122601509094\n",
      "Eval loss 0.013058789074420929, R2 0.496989369392395\n",
      "epoch 3071, loss 0.013007484376430511, R2 0.45933014154434204\n",
      "Eval loss 0.013058319687843323, R2 0.49700742959976196\n",
      "epoch 3072, loss 0.013007051311433315, R2 0.4593486189842224\n",
      "Eval loss 0.013057854026556015, R2 0.4970249533653259\n",
      "epoch 3073, loss 0.013006621040403843, R2 0.4593660235404968\n",
      "Eval loss 0.013057386502623558, R2 0.4970429539680481\n",
      "epoch 3074, loss 0.013006188906729221, R2 0.4593845009803772\n",
      "Eval loss 0.013056918978691101, R2 0.49706125259399414\n",
      "epoch 3075, loss 0.013005758635699749, R2 0.4594019055366516\n",
      "Eval loss 0.013056453317403793, R2 0.49707919359207153\n",
      "epoch 3076, loss 0.013005328364670277, R2 0.4594194293022156\n",
      "Eval loss 0.013055987656116486, R2 0.4970971345901489\n",
      "epoch 3077, loss 0.013004899024963379, R2 0.4594377875328064\n",
      "Eval loss 0.013055521063506603, R2 0.4971150755882263\n",
      "epoch 3078, loss 0.013004467822611332, R2 0.4594551920890808\n",
      "Eval loss 0.013055055402219296, R2 0.49713313579559326\n",
      "epoch 3079, loss 0.013004038482904434, R2 0.4594733715057373\n",
      "Eval loss 0.013054591603577137, R2 0.49715089797973633\n",
      "epoch 3080, loss 0.013003609143197536, R2 0.4594917297363281\n",
      "Eval loss 0.01305412594228983, R2 0.49716854095458984\n",
      "epoch 3081, loss 0.013003180734813213, R2 0.459509015083313\n",
      "Eval loss 0.013053660281002522, R2 0.49718689918518066\n",
      "epoch 3082, loss 0.01300275232642889, R2 0.4595266580581665\n",
      "Eval loss 0.013053196482360363, R2 0.49720460176467896\n",
      "epoch 3083, loss 0.013002322055399418, R2 0.4595448970794678\n",
      "Eval loss 0.013052732683718204, R2 0.4972224831581116\n",
      "epoch 3084, loss 0.013001893647015095, R2 0.4595624804496765\n",
      "Eval loss 0.01305226981639862, R2 0.49724000692367554\n",
      "epoch 3085, loss 0.013001466169953346, R2 0.45958083868026733\n",
      "Eval loss 0.013051805086433887, R2 0.49725818634033203\n",
      "epoch 3086, loss 0.013001038692891598, R2 0.45959824323654175\n",
      "Eval loss 0.013051343150436878, R2 0.497275710105896\n",
      "epoch 3087, loss 0.01300061121582985, R2 0.459616482257843\n",
      "Eval loss 0.01305087935179472, R2 0.49729418754577637\n",
      "epoch 3088, loss 0.013000184670090675, R2 0.45963406562805176\n",
      "Eval loss 0.013050416484475136, R2 0.497311532497406\n",
      "epoch 3089, loss 0.012999758124351501, R2 0.45965147018432617\n",
      "Eval loss 0.013049954548478127, R2 0.4973292350769043\n",
      "epoch 3090, loss 0.012999331578612328, R2 0.45966899394989014\n",
      "Eval loss 0.013049491681158543, R2 0.49734705686569214\n",
      "epoch 3091, loss 0.012998905032873154, R2 0.4596872329711914\n",
      "Eval loss 0.013049031607806683, R2 0.4973650574684143\n",
      "epoch 3092, loss 0.01299847848713398, R2 0.4597044587135315\n",
      "Eval loss 0.013048570603132248, R2 0.4973828196525574\n",
      "epoch 3093, loss 0.012998053804039955, R2 0.45972180366516113\n",
      "Eval loss 0.013048108667135239, R2 0.49740058183670044\n",
      "epoch 3094, loss 0.012997627258300781, R2 0.4597405195236206\n",
      "Eval loss 0.013047648593783379, R2 0.4974183440208435\n",
      "epoch 3095, loss 0.012997201643884182, R2 0.4597575068473816\n",
      "Eval loss 0.013047187589108944, R2 0.49743592739105225\n",
      "epoch 3096, loss 0.012996777892112732, R2 0.4597751498222351\n",
      "Eval loss 0.013046728447079659, R2 0.49745362997055054\n",
      "epoch 3097, loss 0.012996353209018707, R2 0.4597935080528259\n",
      "Eval loss 0.013046268373727798, R2 0.4974721074104309\n",
      "epoch 3098, loss 0.012995928525924683, R2 0.45981115102767944\n",
      "Eval loss 0.013045808300375938, R2 0.4974895119667053\n",
      "epoch 3099, loss 0.012995504774153233, R2 0.45982825756073\n",
      "Eval loss 0.013045348227024078, R2 0.4975072145462036\n",
      "epoch 3100, loss 0.012995080091059208, R2 0.4598459005355835\n",
      "Eval loss 0.013044889084994793, R2 0.49752503633499146\n",
      "epoch 3101, loss 0.012994657270610332, R2 0.4598633050918579\n",
      "Eval loss 0.013044431805610657, R2 0.49754226207733154\n",
      "epoch 3102, loss 0.012994235381484032, R2 0.4598815441131592\n",
      "Eval loss 0.013043971732258797, R2 0.4975602626800537\n",
      "epoch 3103, loss 0.012993809767067432, R2 0.45989906787872314\n",
      "Eval loss 0.013043515384197235, R2 0.4975775480270386\n",
      "epoch 3104, loss 0.012993388809263706, R2 0.4599170684814453\n",
      "Eval loss 0.013043058104813099, R2 0.4975954294204712\n",
      "epoch 3105, loss 0.01299296598881483, R2 0.45993363857269287\n",
      "Eval loss 0.013042599894106388, R2 0.4976128339767456\n",
      "epoch 3106, loss 0.01299254223704338, R2 0.4599517583847046\n",
      "Eval loss 0.013042142614722252, R2 0.49763011932373047\n",
      "epoch 3107, loss 0.012992121279239655, R2 0.45996904373168945\n",
      "Eval loss 0.01304168626666069, R2 0.49764811992645264\n",
      "epoch 3108, loss 0.012991699390113354, R2 0.45998626947402954\n",
      "Eval loss 0.013041229918599129, R2 0.4976655840873718\n",
      "epoch 3109, loss 0.012991278432309628, R2 0.46000391244888306\n",
      "Eval loss 0.013040773570537567, R2 0.49768316745758057\n",
      "epoch 3110, loss 0.012990856543183327, R2 0.46002113819122314\n",
      "Eval loss 0.013040316291153431, R2 0.4977007508277893\n",
      "epoch 3111, loss 0.012990436516702175, R2 0.4600387215614319\n",
      "Eval loss 0.013039860874414444, R2 0.49771803617477417\n",
      "epoch 3112, loss 0.012990015558898449, R2 0.4600560665130615\n",
      "Eval loss 0.013039405457675457, R2 0.4977356791496277\n",
      "epoch 3113, loss 0.012989595532417297, R2 0.460074245929718\n",
      "Eval loss 0.013038950972259045, R2 0.4977536201477051\n",
      "epoch 3114, loss 0.012989174574613571, R2 0.46009135246276855\n",
      "Eval loss 0.013038495555520058, R2 0.4977710247039795\n",
      "epoch 3115, loss 0.012988757342100143, R2 0.46010905504226685\n",
      "Eval loss 0.01303804200142622, R2 0.49778836965560913\n",
      "epoch 3116, loss 0.012988336384296417, R2 0.46012604236602783\n",
      "Eval loss 0.013037588447332382, R2 0.49780601263046265\n",
      "epoch 3117, loss 0.01298791728913784, R2 0.46014344692230225\n",
      "Eval loss 0.013037133030593395, R2 0.4978235363960266\n",
      "epoch 3118, loss 0.012987498193979263, R2 0.46016085147857666\n",
      "Eval loss 0.013036679476499557, R2 0.4978410005569458\n",
      "epoch 3119, loss 0.012987080961465836, R2 0.4601782560348511\n",
      "Eval loss 0.01303622592240572, R2 0.49785828590393066\n",
      "epoch 3120, loss 0.01298666000366211, R2 0.46019625663757324\n",
      "Eval loss 0.013035773299634457, R2 0.497875452041626\n",
      "epoch 3121, loss 0.012986244633793831, R2 0.4602130055427551\n",
      "Eval loss 0.013035321608185768, R2 0.49789315462112427\n",
      "epoch 3122, loss 0.012985825538635254, R2 0.46023058891296387\n",
      "Eval loss 0.013034867122769356, R2 0.49791038036346436\n",
      "epoch 3123, loss 0.012985406443476677, R2 0.46024781465530396\n",
      "Eval loss 0.013034416362643242, R2 0.497927725315094\n",
      "epoch 3124, loss 0.01298498921096325, R2 0.4602653384208679\n",
      "Eval loss 0.013033963739871979, R2 0.49794572591781616\n",
      "epoch 3125, loss 0.012984571978449821, R2 0.46028250455856323\n",
      "Eval loss 0.01303351204842329, R2 0.497963011264801\n",
      "epoch 3126, loss 0.012984156608581543, R2 0.4603002667427063\n",
      "Eval loss 0.013033062219619751, R2 0.49798035621643066\n",
      "epoch 3127, loss 0.01298374030739069, R2 0.46031707525253296\n",
      "Eval loss 0.013032610528171062, R2 0.4979977011680603\n",
      "epoch 3128, loss 0.012983323074877262, R2 0.4603344202041626\n",
      "Eval loss 0.013032159768044949, R2 0.49801480770111084\n",
      "epoch 3129, loss 0.012982907705008984, R2 0.46035224199295044\n",
      "Eval loss 0.013031710870563984, R2 0.49803221225738525\n",
      "epoch 3130, loss 0.01298249140381813, R2 0.4603695273399353\n",
      "Eval loss 0.01303126011043787, R2 0.4980497360229492\n",
      "epoch 3131, loss 0.012982076033949852, R2 0.46038663387298584\n",
      "Eval loss 0.013030809350311756, R2 0.49806708097457886\n",
      "epoch 3132, loss 0.012981659732758999, R2 0.46040356159210205\n",
      "Eval loss 0.013030361384153366, R2 0.49808424711227417\n",
      "epoch 3133, loss 0.012981245294213295, R2 0.46042078733444214\n",
      "Eval loss 0.013029910624027252, R2 0.49810171127319336\n",
      "epoch 3134, loss 0.012980829924345016, R2 0.460438072681427\n",
      "Eval loss 0.013029460795223713, R2 0.49811887741088867\n",
      "epoch 3135, loss 0.012980417348444462, R2 0.4604552388191223\n",
      "Eval loss 0.013029012829065323, R2 0.49813586473464966\n",
      "epoch 3136, loss 0.012980002909898758, R2 0.4604722261428833\n",
      "Eval loss 0.013028564862906933, R2 0.4981531500816345\n",
      "epoch 3137, loss 0.012979588471353054, R2 0.4604896306991577\n",
      "Eval loss 0.013028116896748543, R2 0.4981706738471985\n",
      "epoch 3138, loss 0.0129791758954525, R2 0.46050751209259033\n",
      "Eval loss 0.013027668930590153, R2 0.4981876015663147\n",
      "epoch 3139, loss 0.012978761456906796, R2 0.4605245590209961\n",
      "Eval loss 0.013027220964431763, R2 0.49820536375045776\n",
      "epoch 3140, loss 0.012978347949683666, R2 0.4605412483215332\n",
      "Eval loss 0.013026774860918522, R2 0.4982225298881531\n",
      "epoch 3141, loss 0.01297793723642826, R2 0.46055829524993896\n",
      "Eval loss 0.013026327826082706, R2 0.49823927879333496\n",
      "epoch 3142, loss 0.012977523729205132, R2 0.4605754613876343\n",
      "Eval loss 0.013025879859924316, R2 0.49825698137283325\n",
      "epoch 3143, loss 0.012977110221982002, R2 0.46059268712997437\n",
      "Eval loss 0.013025435619056225, R2 0.49827396869659424\n",
      "epoch 3144, loss 0.012976699508726597, R2 0.4606100916862488\n",
      "Eval loss 0.01302498858422041, R2 0.498291015625\n",
      "epoch 3145, loss 0.012976287864148617, R2 0.46062684059143066\n",
      "Eval loss 0.013024542480707169, R2 0.49830836057662964\n",
      "epoch 3146, loss 0.012975875288248062, R2 0.4606447219848633\n",
      "Eval loss 0.013024097308516502, R2 0.4983254671096802\n",
      "epoch 3147, loss 0.012975463643670082, R2 0.4606611132621765\n",
      "Eval loss 0.013023652136325836, R2 0.49834251403808594\n",
      "epoch 3148, loss 0.012975053861737251, R2 0.4606781601905823\n",
      "Eval loss 0.013023206032812595, R2 0.49835968017578125\n",
      "epoch 3149, loss 0.012974643148481846, R2 0.46069538593292236\n",
      "Eval loss 0.013022762723267078, R2 0.49837708473205566\n",
      "epoch 3150, loss 0.01297423243522644, R2 0.4607124328613281\n",
      "Eval loss 0.013022317551076412, R2 0.4983937740325928\n",
      "epoch 3151, loss 0.012973821721971035, R2 0.46072936058044434\n",
      "Eval loss 0.01302187331020832, R2 0.49841099977493286\n",
      "epoch 3152, loss 0.012973413802683353, R2 0.46074599027633667\n",
      "Eval loss 0.013021430931985378, R2 0.4984279274940491\n",
      "epoch 3153, loss 0.012973003089427948, R2 0.4607635736465454\n",
      "Eval loss 0.013020986691117287, R2 0.49844515323638916\n",
      "epoch 3154, loss 0.012972594238817692, R2 0.46078038215637207\n",
      "Eval loss 0.01302054338157177, R2 0.49846237897872925\n",
      "epoch 3155, loss 0.012972185388207436, R2 0.46079736948013306\n",
      "Eval loss 0.013020101934671402, R2 0.49847954511642456\n",
      "epoch 3156, loss 0.01297177467495203, R2 0.4608144164085388\n",
      "Eval loss 0.013019658625125885, R2 0.49849677085876465\n",
      "epoch 3157, loss 0.012971365824341774, R2 0.4608319401741028\n",
      "Eval loss 0.013019215315580368, R2 0.49851322174072266\n",
      "epoch 3158, loss 0.012970957905054092, R2 0.4608483910560608\n",
      "Eval loss 0.013018774800002575, R2 0.4985305070877075\n",
      "epoch 3159, loss 0.012970550917088985, R2 0.460865318775177\n",
      "Eval loss 0.013018331490457058, R2 0.49854761362075806\n",
      "epoch 3160, loss 0.01297014206647873, R2 0.46088212728500366\n",
      "Eval loss 0.013017890974879265, R2 0.49856454133987427\n",
      "epoch 3161, loss 0.012969736009836197, R2 0.4608997106552124\n",
      "Eval loss 0.013017449527978897, R2 0.4985818862915039\n",
      "epoch 3162, loss 0.012969328090548515, R2 0.4609161615371704\n",
      "Eval loss 0.01301700808107853, R2 0.4985988736152649\n",
      "epoch 3163, loss 0.012968921102583408, R2 0.4609330892562866\n",
      "Eval loss 0.01301656849682331, R2 0.49861568212509155\n",
      "epoch 3164, loss 0.012968513183295727, R2 0.46095019578933716\n",
      "Eval loss 0.013016127981245518, R2 0.4986327886581421\n",
      "epoch 3165, loss 0.012968108057975769, R2 0.4609668254852295\n",
      "Eval loss 0.0130156883969903, R2 0.4986494183540344\n",
      "epoch 3166, loss 0.012967701070010662, R2 0.46098411083221436\n",
      "Eval loss 0.013015247881412506, R2 0.4986662268638611\n",
      "epoch 3167, loss 0.012967295944690704, R2 0.46100062131881714\n",
      "Eval loss 0.013014808297157288, R2 0.49868345260620117\n",
      "epoch 3168, loss 0.012966888025403023, R2 0.46101808547973633\n",
      "Eval loss 0.013014370575547218, R2 0.4987003207206726\n",
      "epoch 3169, loss 0.012966485694050789, R2 0.46103477478027344\n",
      "Eval loss 0.013013930991292, R2 0.4987172484397888\n",
      "epoch 3170, loss 0.012966077774763107, R2 0.46105122566223145\n",
      "Eval loss 0.013013492338359356, R2 0.4987339973449707\n",
      "epoch 3171, loss 0.012965673580765724, R2 0.4610680341720581\n",
      "Eval loss 0.013013053685426712, R2 0.49875086545944214\n",
      "epoch 3172, loss 0.012965269386768341, R2 0.4610850214958191\n",
      "Eval loss 0.013012615032494068, R2 0.498767614364624\n",
      "epoch 3173, loss 0.012964863330125809, R2 0.4611017107963562\n",
      "Eval loss 0.013012178242206573, R2 0.49878430366516113\n",
      "epoch 3174, loss 0.012964460067451, R2 0.4611186385154724\n",
      "Eval loss 0.01301173958927393, R2 0.49880164861679077\n",
      "epoch 3175, loss 0.012964055873453617, R2 0.4611354470252991\n",
      "Eval loss 0.013011302798986435, R2 0.498818039894104\n",
      "epoch 3176, loss 0.012963652610778809, R2 0.46115219593048096\n",
      "Eval loss 0.013010865077376366, R2 0.49883490800857544\n",
      "epoch 3177, loss 0.012963250279426575, R2 0.4611687660217285\n",
      "Eval loss 0.01301043014973402, R2 0.4988522529602051\n",
      "epoch 3178, loss 0.012962847016751766, R2 0.4611855745315552\n",
      "Eval loss 0.013009993359446526, R2 0.4988688826560974\n",
      "epoch 3179, loss 0.012962444685399532, R2 0.46120262145996094\n",
      "Eval loss 0.013009555637836456, R2 0.49888545274734497\n",
      "epoch 3180, loss 0.01296204049140215, R2 0.46121901273727417\n",
      "Eval loss 0.013009120710194111, R2 0.49890267848968506\n",
      "epoch 3181, loss 0.01296163909137249, R2 0.4612357020378113\n",
      "Eval loss 0.01300868671387434, R2 0.4989187717437744\n",
      "epoch 3182, loss 0.012961235828697681, R2 0.46125251054763794\n",
      "Eval loss 0.013008249923586845, R2 0.49893563985824585\n",
      "epoch 3183, loss 0.012960835359990597, R2 0.4612691402435303\n",
      "Eval loss 0.0130078149959445, R2 0.4989525079727173\n",
      "epoch 3184, loss 0.012960434891283512, R2 0.4612857699394226\n",
      "Eval loss 0.01300738099962473, R2 0.49896925687789917\n",
      "epoch 3185, loss 0.012960032559931278, R2 0.4613025188446045\n",
      "Eval loss 0.013006947003304958, R2 0.4989864230155945\n",
      "epoch 3186, loss 0.012959630228579044, R2 0.4613192677497864\n",
      "Eval loss 0.013006512075662613, R2 0.4990025758743286\n",
      "epoch 3187, loss 0.012959230691194534, R2 0.46133601665496826\n",
      "Eval loss 0.013006079010665417, R2 0.499019980430603\n",
      "epoch 3188, loss 0.01295883022248745, R2 0.46135252714157104\n",
      "Eval loss 0.013005645014345646, R2 0.49903595447540283\n",
      "epoch 3189, loss 0.012958427891135216, R2 0.4613693952560425\n",
      "Eval loss 0.01300521194934845, R2 0.4990529417991638\n",
      "epoch 3190, loss 0.01295802928507328, R2 0.4613857865333557\n",
      "Eval loss 0.013004777953028679, R2 0.4990693926811218\n",
      "epoch 3191, loss 0.012957628816366196, R2 0.4614025950431824\n",
      "Eval loss 0.013004345819354057, R2 0.4990861415863037\n",
      "epoch 3192, loss 0.01295723021030426, R2 0.4614189863204956\n",
      "Eval loss 0.013003913685679436, R2 0.4991030693054199\n",
      "epoch 3193, loss 0.012956828810274601, R2 0.4614362120628357\n",
      "Eval loss 0.013003481552004814, R2 0.49911946058273315\n",
      "epoch 3194, loss 0.01295643113553524, R2 0.4614527225494385\n",
      "Eval loss 0.013003048487007618, R2 0.4991360902786255\n",
      "epoch 3195, loss 0.012956032529473305, R2 0.4614689350128174\n",
      "Eval loss 0.013002617284655571, R2 0.4991527795791626\n",
      "epoch 3196, loss 0.01295563392341137, R2 0.4614853262901306\n",
      "Eval loss 0.013002187013626099, R2 0.49916934967041016\n",
      "epoch 3197, loss 0.012955235317349434, R2 0.4615018963813782\n",
      "Eval loss 0.013001755811274052, R2 0.4991859197616577\n",
      "epoch 3198, loss 0.012954838573932648, R2 0.4615190625190735\n",
      "Eval loss 0.013001324608922005, R2 0.4992026686668396\n",
      "epoch 3199, loss 0.012954439036548138, R2 0.4615350365638733\n",
      "Eval loss 0.013000895269215107, R2 0.4992189407348633\n",
      "epoch 3200, loss 0.012954042293131351, R2 0.4615520238876343\n",
      "Eval loss 0.01300046406686306, R2 0.49923598766326904\n",
      "epoch 3201, loss 0.012953645549714565, R2 0.4615686535835266\n",
      "Eval loss 0.013000034727156162, R2 0.4992523789405823\n",
      "epoch 3202, loss 0.012953247874975204, R2 0.461584210395813\n",
      "Eval loss 0.01299960445612669, R2 0.4992687702178955\n",
      "epoch 3203, loss 0.012952852062880993, R2 0.4616013169288635\n",
      "Eval loss 0.012999174185097218, R2 0.4992855191230774\n",
      "epoch 3204, loss 0.012952454388141632, R2 0.46161752939224243\n",
      "Eval loss 0.012998746708035469, R2 0.49930208921432495\n",
      "epoch 3205, loss 0.01295205857604742, R2 0.46163398027420044\n",
      "Eval loss 0.012998317368328571, R2 0.4993184804916382\n",
      "epoch 3206, loss 0.012951661832630634, R2 0.4616506099700928\n",
      "Eval loss 0.012997888959944248, R2 0.49933475255966187\n",
      "epoch 3207, loss 0.012951266951858997, R2 0.46166735887527466\n",
      "Eval loss 0.0129974614828825, R2 0.4993517994880676\n",
      "epoch 3208, loss 0.01295087207108736, R2 0.4616832733154297\n",
      "Eval loss 0.012997033074498177, R2 0.4993678331375122\n",
      "epoch 3209, loss 0.012950476258993149, R2 0.4617002606391907\n",
      "Eval loss 0.012996603734791279, R2 0.4993845224380493\n",
      "epoch 3210, loss 0.012950081378221512, R2 0.4617161154747009\n",
      "Eval loss 0.01299617812037468, R2 0.49940061569213867\n",
      "epoch 3211, loss 0.01294968742877245, R2 0.46173256635665894\n",
      "Eval loss 0.012995750643312931, R2 0.4994170665740967\n",
      "epoch 3212, loss 0.012949292548000813, R2 0.46174895763397217\n",
      "Eval loss 0.012995323166251183, R2 0.49943411350250244\n",
      "epoch 3213, loss 0.012948897667229176, R2 0.4617658853530884\n",
      "Eval loss 0.012994896620512009, R2 0.499450147151947\n",
      "epoch 3214, loss 0.012948504649102688, R2 0.4617818593978882\n",
      "Eval loss 0.01299447100609541, R2 0.4994664192199707\n",
      "epoch 3215, loss 0.012948109768331051, R2 0.46179813146591187\n",
      "Eval loss 0.01299404539167881, R2 0.49948304891586304\n",
      "epoch 3216, loss 0.012947716750204563, R2 0.461814284324646\n",
      "Eval loss 0.012993617914617062, R2 0.49949949979782104\n",
      "epoch 3217, loss 0.0129473228007555, R2 0.4618307948112488\n",
      "Eval loss 0.012993192300200462, R2 0.49951618909835815\n",
      "epoch 3218, loss 0.012946930713951588, R2 0.46184712648391724\n",
      "Eval loss 0.012992767617106438, R2 0.49953246116638184\n",
      "epoch 3219, loss 0.0129465376958251, R2 0.46186363697052\n",
      "Eval loss 0.012992342002689838, R2 0.49954867362976074\n",
      "epoch 3220, loss 0.012946143746376038, R2 0.46188080310821533\n",
      "Eval loss 0.012991918250918388, R2 0.4995647072792053\n",
      "epoch 3221, loss 0.012945753522217274, R2 0.46189624071121216\n",
      "Eval loss 0.012991493567824364, R2 0.4995812177658081\n",
      "epoch 3222, loss 0.012945362366735935, R2 0.4619123339653015\n",
      "Eval loss 0.012991067953407764, R2 0.4995977282524109\n",
      "epoch 3223, loss 0.012944970279932022, R2 0.4619286060333252\n",
      "Eval loss 0.012990646064281464, R2 0.49961429834365845\n",
      "epoch 3224, loss 0.012944578193128109, R2 0.4619450569152832\n",
      "Eval loss 0.012990223243832588, R2 0.4996304512023926\n",
      "epoch 3225, loss 0.012944187968969345, R2 0.46196144819259644\n",
      "Eval loss 0.012989798560738564, R2 0.49964678287506104\n",
      "epoch 3226, loss 0.012943795882165432, R2 0.46197742223739624\n",
      "Eval loss 0.012989375740289688, R2 0.49966275691986084\n",
      "epoch 3227, loss 0.012943404726684093, R2 0.46199363470077515\n",
      "Eval loss 0.012988952919840813, R2 0.49967968463897705\n",
      "epoch 3228, loss 0.012943015433847904, R2 0.4620100259780884\n",
      "Eval loss 0.012988529168069363, R2 0.49969542026519775\n",
      "epoch 3229, loss 0.012942624278366566, R2 0.46202629804611206\n",
      "Eval loss 0.012988109141588211, R2 0.49971169233322144\n",
      "epoch 3230, loss 0.012942235916852951, R2 0.46204274892807007\n",
      "Eval loss 0.012987686321139336, R2 0.49972832202911377\n",
      "epoch 3231, loss 0.012941844761371613, R2 0.46205848455429077\n",
      "Eval loss 0.012987262569367886, R2 0.4997440576553345\n",
      "epoch 3232, loss 0.012941455468535423, R2 0.4620746970176697\n",
      "Eval loss 0.012986843474209309, R2 0.49976080656051636\n",
      "epoch 3233, loss 0.012941066175699234, R2 0.4620909094810486\n",
      "Eval loss 0.012986422516405582, R2 0.49977684020996094\n",
      "epoch 3234, loss 0.012940676882863045, R2 0.46210724115371704\n",
      "Eval loss 0.012986000627279282, R2 0.49979305267333984\n",
      "epoch 3235, loss 0.01294028852134943, R2 0.46212321519851685\n",
      "Eval loss 0.01298558060079813, R2 0.4998091459274292\n",
      "epoch 3236, loss 0.01293990109115839, R2 0.4621394872665405\n",
      "Eval loss 0.012985160574316978, R2 0.4998254179954529\n",
      "epoch 3237, loss 0.0129395117983222, R2 0.46215546131134033\n",
      "Eval loss 0.012984739616513252, R2 0.49984151124954224\n",
      "epoch 3238, loss 0.01293912436813116, R2 0.4621717929840088\n",
      "Eval loss 0.012984320521354675, R2 0.4998573660850525\n",
      "epoch 3239, loss 0.012938736006617546, R2 0.4621877074241638\n",
      "Eval loss 0.012983901426196098, R2 0.49987369775772095\n",
      "epoch 3240, loss 0.012938348576426506, R2 0.4622039794921875\n",
      "Eval loss 0.012983482331037521, R2 0.49988996982574463\n",
      "epoch 3241, loss 0.01293796207755804, R2 0.46222007274627686\n",
      "Eval loss 0.01298306230455637, R2 0.4999058246612549\n",
      "epoch 3242, loss 0.012937573716044426, R2 0.46223604679107666\n",
      "Eval loss 0.012982643209397793, R2 0.49992185831069946\n",
      "epoch 3243, loss 0.012937188148498535, R2 0.46225208044052124\n",
      "Eval loss 0.012982224114239216, R2 0.49993813037872314\n",
      "epoch 3244, loss 0.01293680164963007, R2 0.4622679352760315\n",
      "Eval loss 0.012981806881725788, R2 0.4999544620513916\n",
      "epoch 3245, loss 0.01293641421943903, R2 0.46228474378585815\n",
      "Eval loss 0.012981387786567211, R2 0.49997031688690186\n",
      "epoch 3246, loss 0.012936028651893139, R2 0.4623004198074341\n",
      "Eval loss 0.012980971485376358, R2 0.49998635053634644\n",
      "epoch 3247, loss 0.012935643084347248, R2 0.46231698989868164\n",
      "Eval loss 0.012980552390217781, R2 0.5000028014183044\n",
      "epoch 3248, loss 0.012935257516801357, R2 0.46233177185058594\n",
      "Eval loss 0.012980137951672077, R2 0.5000187754631042\n",
      "epoch 3249, loss 0.012934873811900616, R2 0.4623486399650574\n",
      "Eval loss 0.0129797188565135, R2 0.5000349283218384\n",
      "epoch 3250, loss 0.012934488244354725, R2 0.4623647928237915\n",
      "Eval loss 0.012979304417967796, R2 0.5000507235527039\n",
      "epoch 3251, loss 0.012934102676808834, R2 0.4623802900314331\n",
      "Eval loss 0.012978887185454369, R2 0.5000669360160828\n",
      "epoch 3252, loss 0.012933718040585518, R2 0.4623963236808777\n",
      "Eval loss 0.012978470884263515, R2 0.5000827312469482\n",
      "epoch 3253, loss 0.012933334335684776, R2 0.46241259574890137\n",
      "Eval loss 0.012978055514395237, R2 0.5000991225242615\n",
      "epoch 3254, loss 0.01293294969946146, R2 0.4624282121658325\n",
      "Eval loss 0.012977640144526958, R2 0.5001149773597717\n",
      "epoch 3255, loss 0.012932565994560719, R2 0.46244436502456665\n",
      "Eval loss 0.012977225705981255, R2 0.5001309514045715\n",
      "epoch 3256, loss 0.012932182289659977, R2 0.46246063709259033\n",
      "Eval loss 0.012976810336112976, R2 0.5001469254493713\n",
      "epoch 3257, loss 0.01293179951608181, R2 0.46247589588165283\n",
      "Eval loss 0.012976394966244698, R2 0.5001628398895264\n",
      "epoch 3258, loss 0.012931415811181068, R2 0.4624916911125183\n",
      "Eval loss 0.012975979596376419, R2 0.5001789331436157\n",
      "epoch 3259, loss 0.012931033968925476, R2 0.46250802278518677\n",
      "Eval loss 0.012975567020475864, R2 0.5001951456069946\n",
      "epoch 3260, loss 0.012930652126669884, R2 0.46252375841140747\n",
      "Eval loss 0.01297515258193016, R2 0.5002107620239258\n",
      "epoch 3261, loss 0.012930269353091717, R2 0.4625398516654968\n",
      "Eval loss 0.012974739074707031, R2 0.5002262592315674\n",
      "epoch 3262, loss 0.012929885648190975, R2 0.4625559449195862\n",
      "Eval loss 0.012974325567483902, R2 0.5002428293228149\n",
      "epoch 3263, loss 0.012929505668580532, R2 0.46257156133651733\n",
      "Eval loss 0.012973912060260773, R2 0.5002584457397461\n",
      "epoch 3264, loss 0.012929122895002365, R2 0.46258729696273804\n",
      "Eval loss 0.012973499484360218, R2 0.5002741813659668\n",
      "epoch 3265, loss 0.012928741984069347, R2 0.4626031517982483\n",
      "Eval loss 0.012973086908459663, R2 0.5002905130386353\n",
      "epoch 3266, loss 0.01292836107313633, R2 0.46261900663375854\n",
      "Eval loss 0.012972675263881683, R2 0.5003063678741455\n",
      "epoch 3267, loss 0.012927979230880737, R2 0.462635338306427\n",
      "Eval loss 0.012972262687981129, R2 0.5003218650817871\n",
      "epoch 3268, loss 0.01292759831994772, R2 0.46265071630477905\n",
      "Eval loss 0.012971851043403149, R2 0.5003379583358765\n",
      "epoch 3269, loss 0.012927217409014702, R2 0.4626673460006714\n",
      "Eval loss 0.012971438467502594, R2 0.5003540515899658\n",
      "epoch 3270, loss 0.012926839292049408, R2 0.46268224716186523\n",
      "Eval loss 0.012971028685569763, R2 0.500369668006897\n",
      "epoch 3271, loss 0.012926459312438965, R2 0.46269768476486206\n",
      "Eval loss 0.012970617040991783, R2 0.5003851652145386\n",
      "epoch 3272, loss 0.012926078401505947, R2 0.4627145528793335\n",
      "Eval loss 0.012970205396413803, R2 0.5004010796546936\n",
      "epoch 3273, loss 0.012925699353218079, R2 0.46272963285446167\n",
      "Eval loss 0.012969796545803547, R2 0.5004168152809143\n",
      "epoch 3274, loss 0.012925319373607635, R2 0.4627455472946167\n",
      "Eval loss 0.012969384901225567, R2 0.5004328489303589\n",
      "epoch 3275, loss 0.012924942187964916, R2 0.4627615809440613\n",
      "Eval loss 0.012968975119292736, R2 0.50044846534729\n",
      "epoch 3276, loss 0.012924563139677048, R2 0.4627768397331238\n",
      "Eval loss 0.012968567200005054, R2 0.5004644393920898\n",
      "epoch 3277, loss 0.01292418409138918, R2 0.4627932906150818\n",
      "Eval loss 0.012968156486749649, R2 0.5004804134368896\n",
      "epoch 3278, loss 0.01292380504310131, R2 0.46280837059020996\n",
      "Eval loss 0.012967748567461967, R2 0.5004956722259521\n",
      "epoch 3279, loss 0.012923428788781166, R2 0.4628239870071411\n",
      "Eval loss 0.012967336922883987, R2 0.500511884689331\n",
      "epoch 3280, loss 0.012923049740493298, R2 0.46283990144729614\n",
      "Eval loss 0.012966929003596306, R2 0.5005272626876831\n",
      "epoch 3281, loss 0.012922673486173153, R2 0.46285539865493774\n",
      "Eval loss 0.012966521084308624, R2 0.5005434155464172\n",
      "epoch 3282, loss 0.012922294437885284, R2 0.4628714919090271\n",
      "Eval loss 0.012966113165020943, R2 0.5005590915679932\n",
      "epoch 3283, loss 0.012921919114887714, R2 0.4628867506980896\n",
      "Eval loss 0.01296570710837841, R2 0.5005743503570557\n",
      "epoch 3284, loss 0.012921541929244995, R2 0.46290260553359985\n",
      "Eval loss 0.01296529732644558, R2 0.5005900859832764\n",
      "epoch 3285, loss 0.012921166606247425, R2 0.4629180431365967\n",
      "Eval loss 0.012964889407157898, R2 0.5006062984466553\n",
      "epoch 3286, loss 0.012920789420604706, R2 0.4629337191581726\n",
      "Eval loss 0.01296448428183794, R2 0.5006217360496521\n",
      "epoch 3287, loss 0.012920413166284561, R2 0.46294987201690674\n",
      "Eval loss 0.012964077293872833, R2 0.5006372928619385\n",
      "epoch 3288, loss 0.012920036911964417, R2 0.4629649519920349\n",
      "Eval loss 0.012963669374585152, R2 0.5006536841392517\n",
      "epoch 3289, loss 0.012919662520289421, R2 0.4629806876182556\n",
      "Eval loss 0.012963265180587769, R2 0.5006684064865112\n",
      "epoch 3290, loss 0.012919286265969276, R2 0.4629966616630554\n",
      "Eval loss 0.012962857261300087, R2 0.500684380531311\n",
      "epoch 3291, loss 0.012918911874294281, R2 0.4630117416381836\n",
      "Eval loss 0.01296245213598013, R2 0.5006997585296631\n",
      "epoch 3292, loss 0.012918537482619286, R2 0.46302729845046997\n",
      "Eval loss 0.012962047010660172, R2 0.5007154941558838\n",
      "epoch 3293, loss 0.012918162159621716, R2 0.46304357051849365\n",
      "Eval loss 0.01296164095401764, R2 0.500731348991394\n",
      "epoch 3294, loss 0.01291778776794672, R2 0.4630590081214905\n",
      "Eval loss 0.012961235828697681, R2 0.5007468461990356\n",
      "epoch 3295, loss 0.012917413376271725, R2 0.4630740284919739\n",
      "Eval loss 0.012960831634700298, R2 0.5007621049880981\n",
      "epoch 3296, loss 0.012917039915919304, R2 0.46309006214141846\n",
      "Eval loss 0.012960427440702915, R2 0.5007779598236084\n",
      "epoch 3297, loss 0.012916666455566883, R2 0.4631052613258362\n",
      "Eval loss 0.012960024178028107, R2 0.5007935762405396\n",
      "epoch 3298, loss 0.012916292063891888, R2 0.46312081813812256\n",
      "Eval loss 0.012959619984030724, R2 0.5008089542388916\n",
      "epoch 3299, loss 0.012915919534862041, R2 0.46313613653182983\n",
      "Eval loss 0.012959214858710766, R2 0.5008245706558228\n",
      "epoch 3300, loss 0.012915547005832195, R2 0.463151752948761\n",
      "Eval loss 0.012958811596035957, R2 0.5008401870727539\n",
      "epoch 3301, loss 0.012915173545479774, R2 0.46316778659820557\n",
      "Eval loss 0.012958408333361149, R2 0.5008559226989746\n",
      "epoch 3302, loss 0.012914801947772503, R2 0.46318256855010986\n",
      "Eval loss 0.01295800507068634, R2 0.5008710026741028\n",
      "epoch 3303, loss 0.012914427556097507, R2 0.46319830417633057\n",
      "Eval loss 0.012957603670656681, R2 0.5008867979049683\n",
      "epoch 3304, loss 0.012914057821035385, R2 0.46321403980255127\n",
      "Eval loss 0.012957201339304447, R2 0.5009024143218994\n",
      "epoch 3305, loss 0.012913686223328114, R2 0.46322911977767944\n",
      "Eval loss 0.012956799007952213, R2 0.5009176731109619\n",
      "epoch 3306, loss 0.012913314625620842, R2 0.4632449150085449\n",
      "Eval loss 0.012956395745277405, R2 0.5009332895278931\n",
      "epoch 3307, loss 0.012912943959236145, R2 0.46325981616973877\n",
      "Eval loss 0.01295599527657032, R2 0.5009490251541138\n",
      "epoch 3308, loss 0.012912570498883724, R2 0.4632754921913147\n",
      "Eval loss 0.01295559387654066, R2 0.5009641647338867\n",
      "epoch 3309, loss 0.012912200763821602, R2 0.4632906913757324\n",
      "Eval loss 0.012955193407833576, R2 0.5009794235229492\n",
      "epoch 3310, loss 0.012911830097436905, R2 0.46330612897872925\n",
      "Eval loss 0.012954791076481342, R2 0.5009950995445251\n",
      "epoch 3311, loss 0.012911460362374783, R2 0.4633218050003052\n",
      "Eval loss 0.012954391539096832, R2 0.5010101795196533\n",
      "epoch 3312, loss 0.012911089695990086, R2 0.463336706161499\n",
      "Eval loss 0.012953991070389748, R2 0.5010260343551636\n",
      "epoch 3313, loss 0.012910719029605389, R2 0.4633524417877197\n",
      "Eval loss 0.012953591533005238, R2 0.5010411739349365\n",
      "epoch 3314, loss 0.01291035208851099, R2 0.4633677005767822\n",
      "Eval loss 0.012953191064298153, R2 0.5010565519332886\n",
      "epoch 3315, loss 0.012909980490803719, R2 0.46338313817977905\n",
      "Eval loss 0.012952790595591068, R2 0.5010726451873779\n",
      "epoch 3316, loss 0.012909612618386745, R2 0.463398277759552\n",
      "Eval loss 0.012952392920851707, R2 0.501087486743927\n",
      "epoch 3317, loss 0.012909241952002048, R2 0.46341371536254883\n",
      "Eval loss 0.012951993383467197, R2 0.5011025667190552\n",
      "epoch 3318, loss 0.0129088731482625, R2 0.46342897415161133\n",
      "Eval loss 0.012951595708727837, R2 0.5011184811592102\n",
      "epoch 3319, loss 0.012908505275845528, R2 0.4634448289871216\n",
      "Eval loss 0.012951196171343327, R2 0.5011334419250488\n",
      "epoch 3320, loss 0.012908137403428555, R2 0.4634596109390259\n",
      "Eval loss 0.012950796633958817, R2 0.50114905834198\n",
      "epoch 3321, loss 0.012907769531011581, R2 0.46347475051879883\n",
      "Eval loss 0.012950398959219456, R2 0.501164436340332\n",
      "epoch 3322, loss 0.012907399795949459, R2 0.46349090337753296\n",
      "Eval loss 0.012950001284480095, R2 0.5011794567108154\n",
      "epoch 3323, loss 0.012907033786177635, R2 0.46350598335266113\n",
      "Eval loss 0.012949603609740734, R2 0.5011950731277466\n",
      "epoch 3324, loss 0.012906666845083237, R2 0.463520884513855\n",
      "Eval loss 0.012949206866323948, R2 0.5012102127075195\n",
      "epoch 3325, loss 0.012906299903988838, R2 0.4635361433029175\n",
      "Eval loss 0.012948808260262012, R2 0.5012257099151611\n",
      "epoch 3326, loss 0.01290593296289444, R2 0.46355152130126953\n",
      "Eval loss 0.01294841431081295, R2 0.5012408494949341\n",
      "epoch 3327, loss 0.012905566021800041, R2 0.4635666608810425\n",
      "Eval loss 0.012948017567396164, R2 0.501255989074707\n",
      "epoch 3328, loss 0.012905200012028217, R2 0.4635816812515259\n",
      "Eval loss 0.012947620823979378, R2 0.5012713074684143\n",
      "epoch 3329, loss 0.012904834002256393, R2 0.4635974168777466\n",
      "Eval loss 0.012947222217917442, R2 0.5012863874435425\n",
      "epoch 3330, loss 0.01290446799248457, R2 0.4636121392250061\n",
      "Eval loss 0.012946829199790955, R2 0.5013015270233154\n",
      "epoch 3331, loss 0.01290410291403532, R2 0.4636274576187134\n",
      "Eval loss 0.012946432456374168, R2 0.5013170838356018\n",
      "epoch 3332, loss 0.012903735972940922, R2 0.4636423587799072\n",
      "Eval loss 0.012946037575602531, R2 0.5013319849967957\n",
      "epoch 3333, loss 0.012903371825814247, R2 0.4636576771736145\n",
      "Eval loss 0.01294564176350832, R2 0.501347541809082\n",
      "epoch 3334, loss 0.012903005816042423, R2 0.463672935962677\n",
      "Eval loss 0.012945249676704407, R2 0.5013623237609863\n",
      "epoch 3335, loss 0.012902642600238323, R2 0.46368783712387085\n",
      "Eval loss 0.012944853864610195, R2 0.5013782978057861\n",
      "epoch 3336, loss 0.0129022765904665, R2 0.4637032151222229\n",
      "Eval loss 0.012944458983838558, R2 0.5013933777809143\n",
      "epoch 3337, loss 0.012901914305984974, R2 0.4637182354927063\n",
      "Eval loss 0.012944065034389496, R2 0.5014082193374634\n",
      "epoch 3338, loss 0.01290154829621315, R2 0.4637336730957031\n",
      "Eval loss 0.012943670153617859, R2 0.501423716545105\n",
      "epoch 3339, loss 0.01290118508040905, R2 0.463748574256897\n",
      "Eval loss 0.012943277135491371, R2 0.5014386177062988\n",
      "epoch 3340, loss 0.012900820933282375, R2 0.4637642502784729\n",
      "Eval loss 0.012942884117364883, R2 0.5014538764953613\n",
      "epoch 3341, loss 0.01290045864880085, R2 0.4637794494628906\n",
      "Eval loss 0.012942491099238396, R2 0.5014687180519104\n",
      "epoch 3342, loss 0.012900096364319324, R2 0.4637945294380188\n",
      "Eval loss 0.012942098081111908, R2 0.5014840364456177\n",
      "epoch 3343, loss 0.01289973221719265, R2 0.46380895376205444\n",
      "Eval loss 0.01294170692563057, R2 0.5014989376068115\n",
      "epoch 3344, loss 0.01289936900138855, R2 0.46382391452789307\n",
      "Eval loss 0.012941313907504082, R2 0.5015143752098083\n",
      "epoch 3345, loss 0.012899007648229599, R2 0.463839590549469\n",
      "Eval loss 0.012940921820700169, R2 0.5015288591384888\n",
      "epoch 3346, loss 0.012898645363748074, R2 0.46385395526885986\n",
      "Eval loss 0.01294053066521883, R2 0.5015445351600647\n",
      "epoch 3347, loss 0.012898284010589123, R2 0.4638693332672119\n",
      "Eval loss 0.012940139509737492, R2 0.5015596151351929\n",
      "epoch 3348, loss 0.012897921726107597, R2 0.46388423442840576\n",
      "Eval loss 0.012939747422933578, R2 0.5015743970870972\n",
      "epoch 3349, loss 0.012897559441626072, R2 0.46389931440353394\n",
      "Eval loss 0.01293935626745224, R2 0.5015895366668701\n",
      "epoch 3350, loss 0.012897198088467121, R2 0.463914155960083\n",
      "Eval loss 0.012938966043293476, R2 0.5016047954559326\n",
      "epoch 3351, loss 0.012896837666630745, R2 0.46392983198165894\n",
      "Eval loss 0.012938576750457287, R2 0.5016195178031921\n",
      "epoch 3352, loss 0.01289647538214922, R2 0.46394431591033936\n",
      "Eval loss 0.012938184663653374, R2 0.5016345977783203\n",
      "epoch 3353, loss 0.012896114028990269, R2 0.46395987272262573\n",
      "Eval loss 0.012937796302139759, R2 0.5016493797302246\n",
      "epoch 3354, loss 0.012895755469799042, R2 0.4639747738838196\n",
      "Eval loss 0.01293740514665842, R2 0.5016648769378662\n",
      "epoch 3355, loss 0.012895395047962666, R2 0.4639892578125\n",
      "Eval loss 0.012937015853822231, R2 0.501679539680481\n",
      "epoch 3356, loss 0.012895036488771439, R2 0.46400415897369385\n",
      "Eval loss 0.012936627492308617, R2 0.5016947388648987\n",
      "epoch 3357, loss 0.012894676066935062, R2 0.46401911973953247\n",
      "Eval loss 0.012936238199472427, R2 0.5017094612121582\n",
      "epoch 3358, loss 0.012894315645098686, R2 0.4640347957611084\n",
      "Eval loss 0.012935848906636238, R2 0.5017246007919312\n",
      "epoch 3359, loss 0.01289395708590746, R2 0.4640490412712097\n",
      "Eval loss 0.012935462407767773, R2 0.5017393827438354\n",
      "epoch 3360, loss 0.012893599458038807, R2 0.4640638828277588\n",
      "Eval loss 0.012935073114931583, R2 0.501754641532898\n",
      "epoch 3361, loss 0.012893239967525005, R2 0.46407949924468994\n",
      "Eval loss 0.012934684753417969, R2 0.5017697215080261\n",
      "epoch 3362, loss 0.012892880477011204, R2 0.4640936255455017\n",
      "Eval loss 0.012934296391904354, R2 0.5017845630645752\n",
      "epoch 3363, loss 0.012892522849142551, R2 0.4641091227531433\n",
      "Eval loss 0.01293390803039074, R2 0.501799464225769\n",
      "epoch 3364, loss 0.012892165221273899, R2 0.4641234874725342\n",
      "Eval loss 0.012933521531522274, R2 0.5018139481544495\n",
      "epoch 3365, loss 0.012891805730760098, R2 0.46413862705230713\n",
      "Eval loss 0.012933135032653809, R2 0.5018292665481567\n",
      "epoch 3366, loss 0.01289144903421402, R2 0.4641532897949219\n",
      "Eval loss 0.012932748533785343, R2 0.5018438100814819\n",
      "epoch 3367, loss 0.012891092337667942, R2 0.4641679525375366\n",
      "Eval loss 0.012932362034916878, R2 0.5018590688705444\n",
      "epoch 3368, loss 0.01289073470979929, R2 0.46418315172195435\n",
      "Eval loss 0.012931976467370987, R2 0.5018739104270935\n",
      "epoch 3369, loss 0.012890378013253212, R2 0.46419811248779297\n",
      "Eval loss 0.012931589968502522, R2 0.5018889307975769\n",
      "epoch 3370, loss 0.01289002038538456, R2 0.4642128348350525\n",
      "Eval loss 0.012931203469634056, R2 0.5019035339355469\n",
      "epoch 3371, loss 0.012889664620161057, R2 0.46422725915908813\n",
      "Eval loss 0.012930817902088165, R2 0.501918375492096\n",
      "epoch 3372, loss 0.012889306992292404, R2 0.464242160320282\n",
      "Eval loss 0.012930433265864849, R2 0.5019330382347107\n",
      "epoch 3373, loss 0.012888951227068901, R2 0.46425706148147583\n",
      "Eval loss 0.012930047698318958, R2 0.5019481778144836\n",
      "epoch 3374, loss 0.012888595461845398, R2 0.46427202224731445\n",
      "Eval loss 0.012929663062095642, R2 0.5019627213478088\n",
      "epoch 3375, loss 0.012888239696621895, R2 0.46428680419921875\n",
      "Eval loss 0.0129292793571949, R2 0.501977801322937\n",
      "epoch 3376, loss 0.012887884862720966, R2 0.46430104970932007\n",
      "Eval loss 0.01292889378964901, R2 0.5019924640655518\n",
      "epoch 3377, loss 0.012887530028820038, R2 0.46431612968444824\n",
      "Eval loss 0.012928511016070843, R2 0.5020075440406799\n",
      "epoch 3378, loss 0.012887174263596535, R2 0.4643314480781555\n",
      "Eval loss 0.012928125448524952, R2 0.5020222663879395\n",
      "epoch 3379, loss 0.012886819429695606, R2 0.46434617042541504\n",
      "Eval loss 0.01292774360626936, R2 0.5020370483398438\n",
      "epoch 3380, loss 0.012886465527117252, R2 0.46436095237731934\n",
      "Eval loss 0.012927359901368618, R2 0.5020517110824585\n",
      "epoch 3381, loss 0.012886111624538898, R2 0.46437567472457886\n",
      "Eval loss 0.012926977127790451, R2 0.5020663142204285\n",
      "epoch 3382, loss 0.012885757721960545, R2 0.4643896818161011\n",
      "Eval loss 0.012926594354212284, R2 0.502081036567688\n",
      "epoch 3383, loss 0.012885404750704765, R2 0.46440452337265015\n",
      "Eval loss 0.012926211580634117, R2 0.5020958185195923\n",
      "epoch 3384, loss 0.012885050848126411, R2 0.4644193649291992\n",
      "Eval loss 0.012925829738378525, R2 0.5021103620529175\n",
      "epoch 3385, loss 0.012884696945548058, R2 0.4644337296485901\n",
      "Eval loss 0.012925446964800358, R2 0.5021252632141113\n",
      "epoch 3386, loss 0.012884343974292278, R2 0.4644487500190735\n",
      "Eval loss 0.01292506605386734, R2 0.5021400451660156\n",
      "epoch 3387, loss 0.012883991003036499, R2 0.4644632339477539\n",
      "Eval loss 0.012924683280289173, R2 0.5021549463272095\n",
      "epoch 3388, loss 0.01288363803178072, R2 0.46447789669036865\n",
      "Eval loss 0.012924302369356155, R2 0.5021688938140869\n",
      "epoch 3389, loss 0.01288328692317009, R2 0.4644925594329834\n",
      "Eval loss 0.012923920527100563, R2 0.5021839141845703\n",
      "epoch 3390, loss 0.012882933020591736, R2 0.4645075798034668\n",
      "Eval loss 0.012923541478812695, R2 0.5021989345550537\n",
      "epoch 3391, loss 0.012882581911981106, R2 0.4645225405693054\n",
      "Eval loss 0.012923159636557102, R2 0.5022136569023132\n",
      "epoch 3392, loss 0.012882230803370476, R2 0.4645366072654724\n",
      "Eval loss 0.012922779656946659, R2 0.5022281408309937\n",
      "epoch 3393, loss 0.012881877832114697, R2 0.46455109119415283\n",
      "Eval loss 0.012922399677336216, R2 0.5022425055503845\n",
      "epoch 3394, loss 0.012881526723504066, R2 0.46456605195999146\n",
      "Eval loss 0.012922019697725773, R2 0.502257227897644\n",
      "epoch 3395, loss 0.012881175614893436, R2 0.4645804762840271\n",
      "Eval loss 0.01292163971811533, R2 0.5022720694541931\n",
      "epoch 3396, loss 0.012880824506282806, R2 0.46459490060806274\n",
      "Eval loss 0.012921259738504887, R2 0.5022871494293213\n",
      "epoch 3397, loss 0.012880474328994751, R2 0.4646099805831909\n",
      "Eval loss 0.012920882552862167, R2 0.5023010969161987\n",
      "epoch 3398, loss 0.01288012508302927, R2 0.46462398767471313\n",
      "Eval loss 0.01292050164192915, R2 0.502315878868103\n",
      "epoch 3399, loss 0.01287977397441864, R2 0.46463853120803833\n",
      "Eval loss 0.012920123524963856, R2 0.5023301839828491\n",
      "epoch 3400, loss 0.012879423797130585, R2 0.4646534323692322\n",
      "Eval loss 0.012919744476675987, R2 0.5023452043533325\n",
      "epoch 3401, loss 0.01287907175719738, R2 0.46466773748397827\n",
      "Eval loss 0.012919367291033268, R2 0.5023598670959473\n",
      "epoch 3402, loss 0.012878724373877048, R2 0.4646822214126587\n",
      "Eval loss 0.0129189882427454, R2 0.5023739337921143\n",
      "epoch 3403, loss 0.012878373265266418, R2 0.46469658613204956\n",
      "Eval loss 0.012918611988425255, R2 0.502389132976532\n",
      "epoch 3404, loss 0.012878026813268661, R2 0.46471136808395386\n",
      "Eval loss 0.012918233871459961, R2 0.5024031400680542\n",
      "epoch 3405, loss 0.012877676635980606, R2 0.4647257328033447\n",
      "Eval loss 0.012917854823172092, R2 0.5024183988571167\n",
      "epoch 3406, loss 0.0128773283213377, R2 0.4647403955459595\n",
      "Eval loss 0.012917479500174522, R2 0.5024321675300598\n",
      "epoch 3407, loss 0.012876980006694794, R2 0.4647548794746399\n",
      "Eval loss 0.012917102314531803, R2 0.5024470090866089\n",
      "epoch 3408, loss 0.012876630760729313, R2 0.464769184589386\n",
      "Eval loss 0.012916725128889084, R2 0.5024613738059998\n",
      "epoch 3409, loss 0.012876282446086407, R2 0.46478384733200073\n",
      "Eval loss 0.012916350737214088, R2 0.5024759769439697\n",
      "epoch 3410, loss 0.01287593599408865, R2 0.46479809284210205\n",
      "Eval loss 0.01291597355157137, R2 0.5024900436401367\n",
      "epoch 3411, loss 0.012875587679445744, R2 0.4648130536079407\n",
      "Eval loss 0.0129155982285738, R2 0.5025046467781067\n",
      "epoch 3412, loss 0.012875240296125412, R2 0.4648270010948181\n",
      "Eval loss 0.012915221974253654, R2 0.5025192499160767\n",
      "epoch 3413, loss 0.012874891981482506, R2 0.46484148502349854\n",
      "Eval loss 0.012914846651256084, R2 0.5025337338447571\n",
      "epoch 3414, loss 0.012874547392129898, R2 0.46485579013824463\n",
      "Eval loss 0.01291447039693594, R2 0.5025482177734375\n",
      "epoch 3415, loss 0.012874199077486992, R2 0.46487027406692505\n",
      "Eval loss 0.012914097867906094, R2 0.502562403678894\n",
      "epoch 3416, loss 0.01287385169416666, R2 0.464885413646698\n",
      "Eval loss 0.012913722544908524, R2 0.5025767087936401\n",
      "epoch 3417, loss 0.012873507104814053, R2 0.4648990035057068\n",
      "Eval loss 0.012913348153233528, R2 0.5025914311408997\n",
      "epoch 3418, loss 0.012873160652816296, R2 0.46491360664367676\n",
      "Eval loss 0.012912973761558533, R2 0.5026060342788696\n",
      "epoch 3419, loss 0.012872814200818539, R2 0.4649280309677124\n",
      "Eval loss 0.012912600301206112, R2 0.5026199817657471\n",
      "epoch 3420, loss 0.012872469611465931, R2 0.46494215726852417\n",
      "Eval loss 0.012912225909531116, R2 0.5026348233222961\n",
      "epoch 3421, loss 0.012872124090790749, R2 0.46495652198791504\n",
      "Eval loss 0.012911851517856121, R2 0.5026490688323975\n",
      "epoch 3422, loss 0.012871777638792992, R2 0.4649709463119507\n",
      "Eval loss 0.01291147992014885, R2 0.5026631355285645\n",
      "epoch 3423, loss 0.012871434912085533, R2 0.464985191822052\n",
      "Eval loss 0.012911106459796429, R2 0.5026777982711792\n",
      "epoch 3424, loss 0.012871090322732925, R2 0.4649996757507324\n",
      "Eval loss 0.012910732999444008, R2 0.5026918649673462\n",
      "epoch 3425, loss 0.012870744802057743, R2 0.46501368284225464\n",
      "Eval loss 0.012910361401736736, R2 0.5027063488960266\n",
      "epoch 3426, loss 0.012870398350059986, R2 0.46502822637557983\n",
      "Eval loss 0.012909987941384315, R2 0.5027210116386414\n",
      "epoch 3427, loss 0.012870056554675102, R2 0.46504247188568115\n",
      "Eval loss 0.012909616343677044, R2 0.5027350187301636\n",
      "epoch 3428, loss 0.012869712896645069, R2 0.4650574326515198\n",
      "Eval loss 0.012909245677292347, R2 0.5027499198913574\n",
      "epoch 3429, loss 0.012869367375969887, R2 0.46507126092910767\n",
      "Eval loss 0.0129088731482625, R2 0.5027635097503662\n",
      "epoch 3430, loss 0.012869025580585003, R2 0.46508586406707764\n",
      "Eval loss 0.012908502481877804, R2 0.5027782320976257\n",
      "epoch 3431, loss 0.01286868192255497, R2 0.46509963274002075\n",
      "Eval loss 0.012908131815493107, R2 0.502792477607727\n",
      "epoch 3432, loss 0.012868338264524937, R2 0.4651145339012146\n",
      "Eval loss 0.01290776114910841, R2 0.5028067827224731\n",
      "epoch 3433, loss 0.012867995537817478, R2 0.4651283025741577\n",
      "Eval loss 0.012907389551401138, R2 0.5028206706047058\n",
      "epoch 3434, loss 0.012867653742432594, R2 0.4651421904563904\n",
      "Eval loss 0.01290702074766159, R2 0.5028350353240967\n",
      "epoch 3435, loss 0.012867311015725136, R2 0.4651567339897156\n",
      "Eval loss 0.012906649149954319, R2 0.5028495788574219\n",
      "epoch 3436, loss 0.012866969220340252, R2 0.46517080068588257\n",
      "Eval loss 0.012906278483569622, R2 0.5028637647628784\n",
      "epoch 3437, loss 0.012866626493632793, R2 0.4651857018470764\n",
      "Eval loss 0.012905909679830074, R2 0.5028778314590454\n",
      "epoch 3438, loss 0.012866285629570484, R2 0.46519923210144043\n",
      "Eval loss 0.012905541807413101, R2 0.5028921365737915\n",
      "epoch 3439, loss 0.01286594569683075, R2 0.4652133584022522\n",
      "Eval loss 0.012905171141028404, R2 0.5029065608978271\n",
      "epoch 3440, loss 0.01286560483276844, R2 0.4652271866798401\n",
      "Eval loss 0.012904802337288857, R2 0.5029206275939941\n",
      "epoch 3441, loss 0.012865261174738407, R2 0.46524178981781006\n",
      "Eval loss 0.012904433533549309, R2 0.5029346942901611\n",
      "epoch 3442, loss 0.012864919379353523, R2 0.4652565121650696\n",
      "Eval loss 0.012904064729809761, R2 0.5029488801956177\n",
      "epoch 3443, loss 0.012864580377936363, R2 0.46527063846588135\n",
      "Eval loss 0.012903696857392788, R2 0.5029631853103638\n",
      "epoch 3444, loss 0.01286423858255148, R2 0.4652842879295349\n",
      "Eval loss 0.012903328984975815, R2 0.5029770731925964\n",
      "epoch 3445, loss 0.01286389958113432, R2 0.46529823541641235\n",
      "Eval loss 0.012902960181236267, R2 0.5029913187026978\n",
      "epoch 3446, loss 0.012863559648394585, R2 0.46531254053115845\n",
      "Eval loss 0.012902594171464443, R2 0.5030055046081543\n",
      "epoch 3447, loss 0.012863220646977425, R2 0.46532660722732544\n",
      "Eval loss 0.01290222629904747, R2 0.5030195713043213\n",
      "epoch 3448, loss 0.01286288071423769, R2 0.4653407335281372\n",
      "Eval loss 0.012901859357953072, R2 0.5030336976051331\n",
      "epoch 3449, loss 0.012862540781497955, R2 0.46535468101501465\n",
      "Eval loss 0.012901492416858673, R2 0.5030479431152344\n",
      "epoch 3450, loss 0.012862201780080795, R2 0.4653691053390503\n",
      "Eval loss 0.012901123613119125, R2 0.5030620098114014\n",
      "epoch 3451, loss 0.01286186184734106, R2 0.46538305282592773\n",
      "Eval loss 0.01290075946599245, R2 0.5030760765075684\n",
      "epoch 3452, loss 0.012861523777246475, R2 0.4653976559638977\n",
      "Eval loss 0.012900393456220627, R2 0.5030906200408936\n",
      "epoch 3453, loss 0.01286118570715189, R2 0.4654116630554199\n",
      "Eval loss 0.012900028377771378, R2 0.5031045079231262\n",
      "epoch 3454, loss 0.012860847637057304, R2 0.46542543172836304\n",
      "Eval loss 0.012899661436676979, R2 0.5031189322471619\n",
      "epoch 3455, loss 0.012860508635640144, R2 0.46543949842453003\n",
      "Eval loss 0.01289929635822773, R2 0.5031324028968811\n",
      "epoch 3456, loss 0.012860171496868134, R2 0.4654533863067627\n",
      "Eval loss 0.01289893127977848, R2 0.5031462907791138\n",
      "epoch 3457, loss 0.012859833426773548, R2 0.4654673933982849\n",
      "Eval loss 0.012898566201329231, R2 0.5031609535217285\n",
      "epoch 3458, loss 0.012859496288001537, R2 0.46548140048980713\n",
      "Eval loss 0.012898202985525131, R2 0.5031745433807373\n",
      "epoch 3459, loss 0.012859158217906952, R2 0.4654954671859741\n",
      "Eval loss 0.012897836975753307, R2 0.5031887888908386\n",
      "epoch 3460, loss 0.012858822010457516, R2 0.46550947427749634\n",
      "Eval loss 0.012897473759949207, R2 0.5032029151916504\n",
      "epoch 3461, loss 0.01285848580300808, R2 0.465523898601532\n",
      "Eval loss 0.012897107750177383, R2 0.5032166838645935\n",
      "epoch 3462, loss 0.012858148664236069, R2 0.4655380845069885\n",
      "Eval loss 0.012896744534373283, R2 0.5032306909561157\n",
      "epoch 3463, loss 0.012857811525464058, R2 0.46555179357528687\n",
      "Eval loss 0.012896380387246609, R2 0.5032453536987305\n",
      "epoch 3464, loss 0.012857475318014622, R2 0.46556556224823\n",
      "Eval loss 0.012896018102765083, R2 0.5032587051391602\n",
      "epoch 3465, loss 0.01285714004188776, R2 0.4655798673629761\n",
      "Eval loss 0.012895653955638409, R2 0.5032727122306824\n",
      "epoch 3466, loss 0.012856805697083473, R2 0.46559327840805054\n",
      "Eval loss 0.012895292602479458, R2 0.5032867789268494\n",
      "epoch 3467, loss 0.012856470420956612, R2 0.4656077027320862\n",
      "Eval loss 0.012894927524030209, R2 0.503300666809082\n",
      "epoch 3468, loss 0.0128561332821846, R2 0.4656211733818054\n",
      "Eval loss 0.012894566170871258, R2 0.5033148527145386\n",
      "epoch 3469, loss 0.012855799868702888, R2 0.46563541889190674\n",
      "Eval loss 0.012894202955067158, R2 0.5033289194107056\n",
      "epoch 3470, loss 0.012855464592576027, R2 0.4656486511230469\n",
      "Eval loss 0.012893841601908207, R2 0.5033425092697144\n",
      "epoch 3471, loss 0.012855129316449165, R2 0.4656631350517273\n",
      "Eval loss 0.012893480248749256, R2 0.5033568739891052\n",
      "epoch 3472, loss 0.012854794040322304, R2 0.46567749977111816\n",
      "Eval loss 0.01289311982691288, R2 0.503370463848114\n",
      "epoch 3473, loss 0.012854460626840591, R2 0.4656912088394165\n",
      "Eval loss 0.012892755679786205, R2 0.5033849477767944\n",
      "epoch 3474, loss 0.012854126282036304, R2 0.46570515632629395\n",
      "Eval loss 0.012892397120594978, R2 0.5033984184265137\n",
      "epoch 3475, loss 0.012853793799877167, R2 0.46571844816207886\n",
      "Eval loss 0.012892034836113453, R2 0.503412127494812\n",
      "epoch 3476, loss 0.01285345945507288, R2 0.4657323360443115\n",
      "Eval loss 0.012891676276922226, R2 0.5034265518188477\n",
      "epoch 3477, loss 0.012853125110268593, R2 0.4657462239265442\n",
      "Eval loss 0.012891314923763275, R2 0.5034401416778564\n",
      "epoch 3478, loss 0.01285279169678688, R2 0.4657597541809082\n",
      "Eval loss 0.012890955433249474, R2 0.5034538507461548\n",
      "epoch 3479, loss 0.012852459214627743, R2 0.46577394008636475\n",
      "Eval loss 0.012890595011413097, R2 0.5034682750701904\n",
      "epoch 3480, loss 0.012852126732468605, R2 0.46578770875930786\n",
      "Eval loss 0.012890235520899296, R2 0.503481388092041\n",
      "epoch 3481, loss 0.012851794250309467, R2 0.46580225229263306\n",
      "Eval loss 0.012889876030385494, R2 0.5034955739974976\n",
      "epoch 3482, loss 0.012851462699472904, R2 0.4658157229423523\n",
      "Eval loss 0.012889516539871693, R2 0.5035094022750854\n",
      "epoch 3483, loss 0.012851130217313766, R2 0.46582895517349243\n",
      "Eval loss 0.012889157980680466, R2 0.5035228729248047\n",
      "epoch 3484, loss 0.012850797735154629, R2 0.46584296226501465\n",
      "Eval loss 0.012888799421489239, R2 0.5035372972488403\n",
      "epoch 3485, loss 0.01285046711564064, R2 0.46585673093795776\n",
      "Eval loss 0.012888438999652863, R2 0.50355064868927\n",
      "epoch 3486, loss 0.012850133702158928, R2 0.46587055921554565\n",
      "Eval loss 0.012888080440461636, R2 0.5035648345947266\n",
      "epoch 3487, loss 0.012849804013967514, R2 0.465884268283844\n",
      "Eval loss 0.012887722812592983, R2 0.5035783052444458\n",
      "epoch 3488, loss 0.012849473394453526, R2 0.4658985733985901\n",
      "Eval loss 0.012887365184724331, R2 0.5035921335220337\n",
      "epoch 3489, loss 0.012849141843616962, R2 0.4659118056297302\n",
      "Eval loss 0.012887007556855679, R2 0.5036057233810425\n",
      "epoch 3490, loss 0.012848812155425549, R2 0.4659256935119629\n",
      "Eval loss 0.012886651791632175, R2 0.5036196112632751\n",
      "epoch 3491, loss 0.012848480604588985, R2 0.4659392833709717\n",
      "Eval loss 0.012886293232440948, R2 0.5036335587501526\n",
      "epoch 3492, loss 0.012848149985074997, R2 0.4659530520439148\n",
      "Eval loss 0.01288593653589487, R2 0.5036474466323853\n",
      "epoch 3493, loss 0.012847819365561008, R2 0.46596693992614746\n",
      "Eval loss 0.012885579839348793, R2 0.5036611557006836\n",
      "epoch 3494, loss 0.012847491540014744, R2 0.465980589389801\n",
      "Eval loss 0.012885223142802715, R2 0.5036747455596924\n",
      "epoch 3495, loss 0.012847160920500755, R2 0.46599429845809937\n",
      "Eval loss 0.012884866446256638, R2 0.5036883354187012\n",
      "epoch 3496, loss 0.012846832163631916, R2 0.4660078287124634\n",
      "Eval loss 0.012884510681033134, R2 0.5037022233009338\n",
      "epoch 3497, loss 0.012846502475440502, R2 0.4660215377807617\n",
      "Eval loss 0.012884153984487057, R2 0.503715991973877\n",
      "epoch 3498, loss 0.012846173718571663, R2 0.4660351872444153\n",
      "Eval loss 0.012883799150586128, R2 0.5037293434143066\n",
      "epoch 3499, loss 0.012845844030380249, R2 0.46604907512664795\n",
      "Eval loss 0.012883445248007774, R2 0.5037434101104736\n",
      "epoch 3500, loss 0.012845516204833984, R2 0.4660624861717224\n",
      "Eval loss 0.012883089482784271, R2 0.5037567019462585\n",
      "epoch 3501, loss 0.01284518651664257, R2 0.46607673168182373\n",
      "Eval loss 0.012882734648883343, R2 0.5037709474563599\n",
      "epoch 3502, loss 0.012844857759773731, R2 0.46609002351760864\n",
      "Eval loss 0.012882380746304989, R2 0.5037842988967896\n",
      "epoch 3503, loss 0.012844531796872616, R2 0.4661034345626831\n",
      "Eval loss 0.012882024981081486, R2 0.5037976503372192\n",
      "epoch 3504, loss 0.012844203971326351, R2 0.46611708402633667\n",
      "Eval loss 0.012881671078503132, R2 0.5038113594055176\n",
      "epoch 3505, loss 0.012843876145780087, R2 0.46613067388534546\n",
      "Eval loss 0.012881316244602203, R2 0.503825306892395\n",
      "epoch 3506, loss 0.012843548320233822, R2 0.4661444425582886\n",
      "Eval loss 0.012880963273346424, R2 0.5038390159606934\n",
      "epoch 3507, loss 0.012843221426010132, R2 0.4661579132080078\n",
      "Eval loss 0.012880608439445496, R2 0.5038525462150574\n",
      "epoch 3508, loss 0.012842893600463867, R2 0.4661715030670166\n",
      "Eval loss 0.012880255468189716, R2 0.5038659572601318\n",
      "epoch 3509, loss 0.012842567637562752, R2 0.46618545055389404\n",
      "Eval loss 0.012879903428256512, R2 0.5038797855377197\n",
      "epoch 3510, loss 0.012842241674661636, R2 0.4661993384361267\n",
      "Eval loss 0.012879548594355583, R2 0.5038934946060181\n",
      "epoch 3511, loss 0.012841914780437946, R2 0.4662126898765564\n",
      "Eval loss 0.012879199348390102, R2 0.5039066672325134\n",
      "epoch 3512, loss 0.012841588817536831, R2 0.46622592210769653\n",
      "Eval loss 0.012878844514489174, R2 0.5039206743240356\n",
      "epoch 3513, loss 0.012841262854635715, R2 0.46623945236206055\n",
      "Eval loss 0.01287849247455597, R2 0.5039340257644653\n",
      "epoch 3514, loss 0.0128409368917346, R2 0.46625351905822754\n",
      "Eval loss 0.01287814136594534, R2 0.5039477348327637\n",
      "epoch 3515, loss 0.012840612791478634, R2 0.4662665128707886\n",
      "Eval loss 0.01287778839468956, R2 0.5039608478546143\n",
      "epoch 3516, loss 0.012840285897254944, R2 0.46628040075302124\n",
      "Eval loss 0.012877438217401505, R2 0.5039744973182678\n",
      "epoch 3517, loss 0.012839960865676403, R2 0.4662937521934509\n",
      "Eval loss 0.012877085246145725, R2 0.5039880871772766\n",
      "epoch 3518, loss 0.012839636765420437, R2 0.46630704402923584\n",
      "Eval loss 0.012876736000180244, R2 0.5040017366409302\n",
      "epoch 3519, loss 0.012839311733841896, R2 0.4663203954696655\n",
      "Eval loss 0.012876384891569614, R2 0.5040152072906494\n",
      "epoch 3520, loss 0.012838988564908504, R2 0.4663340449333191\n",
      "Eval loss 0.012876033782958984, R2 0.5040284395217896\n",
      "epoch 3521, loss 0.012838663533329964, R2 0.46634751558303833\n",
      "Eval loss 0.012875682674348354, R2 0.5040421485900879\n",
      "epoch 3522, loss 0.012838338501751423, R2 0.46636104583740234\n",
      "Eval loss 0.012875332497060299, R2 0.5040558576583862\n",
      "epoch 3523, loss 0.012838015332818031, R2 0.4663742780685425\n",
      "Eval loss 0.012874983251094818, R2 0.5040695667266846\n",
      "epoch 3524, loss 0.012837693095207214, R2 0.46638768911361694\n",
      "Eval loss 0.012874632142484188, R2 0.5040827393531799\n",
      "epoch 3525, loss 0.012837368063628674, R2 0.46640121936798096\n",
      "Eval loss 0.012874283827841282, R2 0.5040958523750305\n",
      "epoch 3526, loss 0.012837044894695282, R2 0.46641480922698975\n",
      "Eval loss 0.012873933650553226, R2 0.5041096210479736\n",
      "epoch 3527, loss 0.012836722657084465, R2 0.4664280414581299\n",
      "Eval loss 0.012873584404587746, R2 0.504122793674469\n",
      "epoch 3528, loss 0.012836398556828499, R2 0.4664418697357178\n",
      "Eval loss 0.012873237021267414, R2 0.5041365027427673\n",
      "epoch 3529, loss 0.012836076319217682, R2 0.4664546847343445\n",
      "Eval loss 0.012872886843979359, R2 0.5041496753692627\n",
      "epoch 3530, loss 0.01283575315028429, R2 0.4664684534072876\n",
      "Eval loss 0.012872539460659027, R2 0.5041632056236267\n",
      "epoch 3531, loss 0.012835431843996048, R2 0.4664819836616516\n",
      "Eval loss 0.012872190214693546, R2 0.5041767954826355\n",
      "epoch 3532, loss 0.012835108675062656, R2 0.4664950966835022\n",
      "Eval loss 0.012871842831373215, R2 0.5041903257369995\n",
      "epoch 3533, loss 0.012834787368774414, R2 0.46650832891464233\n",
      "Eval loss 0.012871495448052883, R2 0.5042035579681396\n",
      "epoch 3534, loss 0.012834466993808746, R2 0.4665226340293884\n",
      "Eval loss 0.012871147133409977, R2 0.5042167901992798\n",
      "epoch 3535, loss 0.012834143824875355, R2 0.466535747051239\n",
      "Eval loss 0.012870799750089645, R2 0.5042300224304199\n",
      "epoch 3536, loss 0.012833823449909687, R2 0.4665485620498657\n",
      "Eval loss 0.012870452366769314, R2 0.5042436122894287\n",
      "epoch 3537, loss 0.01283350121229887, R2 0.4665619730949402\n",
      "Eval loss 0.012870105914771557, R2 0.5042569637298584\n",
      "epoch 3538, loss 0.012833181768655777, R2 0.4665752053260803\n",
      "Eval loss 0.0128697594627738, R2 0.5042703151702881\n",
      "epoch 3539, loss 0.01283286139369011, R2 0.46658873558044434\n",
      "Eval loss 0.012869413010776043, R2 0.5042837858200073\n",
      "epoch 3540, loss 0.012832540087401867, R2 0.4666023850440979\n",
      "Eval loss 0.01286906749010086, R2 0.5042967796325684\n",
      "epoch 3541, loss 0.0128322197124362, R2 0.4666152000427246\n",
      "Eval loss 0.012868721038103104, R2 0.5043102502822876\n",
      "epoch 3542, loss 0.012831900268793106, R2 0.46662843227386475\n",
      "Eval loss 0.012868374586105347, R2 0.5043236017227173\n",
      "epoch 3543, loss 0.012831580825150013, R2 0.46664172410964966\n",
      "Eval loss 0.012868029996752739, R2 0.5043367743492126\n",
      "epoch 3544, loss 0.012831260450184345, R2 0.466654896736145\n",
      "Eval loss 0.012867682613432407, R2 0.5043505430221558\n",
      "epoch 3545, loss 0.012830941006541252, R2 0.46666836738586426\n",
      "Eval loss 0.0128673380240798, R2 0.5043636560440063\n",
      "epoch 3546, loss 0.012830621562898159, R2 0.4666817784309387\n",
      "Eval loss 0.012866994366049767, R2 0.5043772459030151\n",
      "epoch 3547, loss 0.012830303981900215, R2 0.46669501066207886\n",
      "Eval loss 0.012866648845374584, R2 0.5043902397155762\n",
      "epoch 3548, loss 0.012829984538257122, R2 0.4667091369628906\n",
      "Eval loss 0.012866304256021976, R2 0.5044033527374268\n",
      "epoch 3549, loss 0.012829665094614029, R2 0.4667215347290039\n",
      "Eval loss 0.012865961529314518, R2 0.5044164657592773\n",
      "epoch 3550, loss 0.012829347513616085, R2 0.4667345881462097\n",
      "Eval loss 0.012865616008639336, R2 0.5044304728507996\n",
      "epoch 3551, loss 0.012829029001295567, R2 0.46674829721450806\n",
      "Eval loss 0.012865273281931877, R2 0.5044434070587158\n",
      "epoch 3552, loss 0.012828711420297623, R2 0.4667617082595825\n",
      "Eval loss 0.012864929623901844, R2 0.5044565200805664\n",
      "epoch 3553, loss 0.012828393839299679, R2 0.46677422523498535\n",
      "Eval loss 0.012864585034549236, R2 0.5044697523117065\n",
      "epoch 3554, loss 0.01282807532697916, R2 0.4667876362800598\n",
      "Eval loss 0.012864241376519203, R2 0.5044829845428467\n",
      "epoch 3555, loss 0.012827758677303791, R2 0.46680063009262085\n",
      "Eval loss 0.01286389958113432, R2 0.5044964551925659\n",
      "epoch 3556, loss 0.012827441096305847, R2 0.46681398153305054\n",
      "Eval loss 0.012863554991781712, R2 0.504509449005127\n",
      "epoch 3557, loss 0.012827123515307903, R2 0.4668272137641907\n",
      "Eval loss 0.012863213196396828, R2 0.5045223236083984\n",
      "epoch 3558, loss 0.012826807796955109, R2 0.46683961153030396\n",
      "Eval loss 0.012862871401011944, R2 0.5045357942581177\n",
      "epoch 3559, loss 0.01282649114727974, R2 0.4668533205986023\n",
      "Eval loss 0.01286252774298191, R2 0.5045490264892578\n",
      "epoch 3560, loss 0.01282617449760437, R2 0.4668666124343872\n",
      "Eval loss 0.012862187810242176, R2 0.5045621395111084\n",
      "epoch 3561, loss 0.012825858779251575, R2 0.4668797254562378\n",
      "Eval loss 0.012861845083534718, R2 0.504575252532959\n",
      "epoch 3562, loss 0.012825542129576206, R2 0.46689343452453613\n",
      "Eval loss 0.012861505150794983, R2 0.5045878887176514\n",
      "epoch 3563, loss 0.012825225479900837, R2 0.4669058918952942\n",
      "Eval loss 0.012861164286732674, R2 0.5046013593673706\n",
      "epoch 3564, loss 0.012824910692870617, R2 0.4669190049171448\n",
      "Eval loss 0.012860821560025215, R2 0.5046147108078003\n",
      "epoch 3565, loss 0.012824595905840397, R2 0.4669322967529297\n",
      "Eval loss 0.01286048162728548, R2 0.504627525806427\n",
      "epoch 3566, loss 0.012824280187487602, R2 0.4669454097747803\n",
      "Eval loss 0.012860139831900597, R2 0.5046409368515015\n",
      "epoch 3567, loss 0.012823965400457382, R2 0.4669581651687622\n",
      "Eval loss 0.012859799899160862, R2 0.5046537518501282\n",
      "epoch 3568, loss 0.012823649682104588, R2 0.4669712781906128\n",
      "Eval loss 0.012859460897743702, R2 0.5046671628952026\n",
      "epoch 3569, loss 0.012823334895074368, R2 0.4669848680496216\n",
      "Eval loss 0.012859118171036243, R2 0.5046802759170532\n",
      "epoch 3570, loss 0.012823021039366722, R2 0.4669979214668274\n",
      "Eval loss 0.012858781032264233, R2 0.5046936273574829\n",
      "epoch 3571, loss 0.012822707183659077, R2 0.46701061725616455\n",
      "Eval loss 0.012858440168201923, R2 0.5047064423561096\n",
      "epoch 3572, loss 0.012822392396628857, R2 0.4670238494873047\n",
      "Eval loss 0.012858101166784763, R2 0.5047193765640259\n",
      "epoch 3573, loss 0.012822078540921211, R2 0.4670372009277344\n",
      "Eval loss 0.012857761234045029, R2 0.5047327280044556\n",
      "epoch 3574, loss 0.012821764685213566, R2 0.4670499563217163\n",
      "Eval loss 0.012857422232627869, R2 0.504745364189148\n",
      "epoch 3575, loss 0.01282145082950592, R2 0.46706247329711914\n",
      "Eval loss 0.012857085093855858, R2 0.5047584772109985\n",
      "epoch 3576, loss 0.012821139767765999, R2 0.46707576513290405\n",
      "Eval loss 0.012856747023761272, R2 0.5047719478607178\n",
      "epoch 3577, loss 0.012820825912058353, R2 0.46708881855010986\n",
      "Eval loss 0.012856407091021538, R2 0.5047847032546997\n",
      "epoch 3578, loss 0.012820513918995857, R2 0.46710193157196045\n",
      "Eval loss 0.012856069952249527, R2 0.5047974586486816\n",
      "epoch 3579, loss 0.012820200063288212, R2 0.46711498498916626\n",
      "Eval loss 0.012855731882154942, R2 0.5048105716705322\n",
      "epoch 3580, loss 0.012819888070225716, R2 0.46712779998779297\n",
      "Eval loss 0.01285539474338293, R2 0.5048237442970276\n",
      "epoch 3581, loss 0.012819577008485794, R2 0.4671408534049988\n",
      "Eval loss 0.012855056673288345, R2 0.504836916923523\n",
      "epoch 3582, loss 0.012819264084100723, R2 0.4671536684036255\n",
      "Eval loss 0.012854719534516335, R2 0.5048494338989258\n",
      "epoch 3583, loss 0.012818951159715652, R2 0.4671667218208313\n",
      "Eval loss 0.012854381464421749, R2 0.5048624873161316\n",
      "epoch 3584, loss 0.012818640097975731, R2 0.46718066930770874\n",
      "Eval loss 0.012854046188294888, R2 0.5048760175704956\n",
      "epoch 3585, loss 0.012818328104913235, R2 0.46719276905059814\n",
      "Eval loss 0.012853708118200302, R2 0.5048887133598328\n",
      "epoch 3586, loss 0.012818016111850739, R2 0.4672057628631592\n",
      "Eval loss 0.012853373773396015, R2 0.5049017667770386\n",
      "epoch 3587, loss 0.012817706912755966, R2 0.46721893548965454\n",
      "Eval loss 0.012853037565946579, R2 0.5049148201942444\n",
      "epoch 3588, loss 0.01281739491969347, R2 0.46723121404647827\n",
      "Eval loss 0.012852701358497143, R2 0.5049272179603577\n",
      "epoch 3589, loss 0.012817083857953548, R2 0.46724432706832886\n",
      "Eval loss 0.012852364219725132, R2 0.5049404501914978\n",
      "epoch 3590, loss 0.012816773727536201, R2 0.46725738048553467\n",
      "Eval loss 0.012852029874920845, R2 0.5049533843994141\n",
      "epoch 3591, loss 0.012816463597118855, R2 0.4672701358795166\n",
      "Eval loss 0.012851694598793983, R2 0.5049662590026855\n",
      "epoch 3592, loss 0.012816153466701508, R2 0.4672830104827881\n",
      "Eval loss 0.012851358391344547, R2 0.5049792528152466\n",
      "epoch 3593, loss 0.012815842404961586, R2 0.46729594469070435\n",
      "Eval loss 0.012851024977862835, R2 0.5049917697906494\n",
      "epoch 3594, loss 0.012815535999834538, R2 0.4673088788986206\n",
      "Eval loss 0.012850688770413399, R2 0.5050047039985657\n",
      "epoch 3595, loss 0.012815224006772041, R2 0.46732234954833984\n",
      "Eval loss 0.012850356288254261, R2 0.5050178170204163\n",
      "epoch 3596, loss 0.012814914807677269, R2 0.467334508895874\n",
      "Eval loss 0.0128500210121274, R2 0.5050303936004639\n",
      "epoch 3597, loss 0.012814606539905071, R2 0.46734797954559326\n",
      "Eval loss 0.012849687598645687, R2 0.5050438642501831\n",
      "epoch 3598, loss 0.012814296409487724, R2 0.4673600196838379\n",
      "Eval loss 0.012849354185163975, R2 0.5050561428070068\n",
      "epoch 3599, loss 0.012813989073038101, R2 0.4673730134963989\n",
      "Eval loss 0.012849019840359688, R2 0.5050691366195679\n",
      "epoch 3600, loss 0.012813678942620754, R2 0.46738654375076294\n",
      "Eval loss 0.01284868735820055, R2 0.505082368850708\n",
      "epoch 3601, loss 0.012813371606171131, R2 0.46739882230758667\n",
      "Eval loss 0.012848352082073689, R2 0.5050951242446899\n",
      "epoch 3602, loss 0.012813062407076359, R2 0.4674123525619507\n",
      "Eval loss 0.012848020531237125, R2 0.5051076412200928\n",
      "epoch 3603, loss 0.012812755070626736, R2 0.46742409467697144\n",
      "Eval loss 0.012847686186432838, R2 0.5051202774047852\n",
      "epoch 3604, loss 0.012812446802854538, R2 0.46743708848953247\n",
      "Eval loss 0.012847354635596275, R2 0.5051333904266357\n",
      "epoch 3605, loss 0.012812139466404915, R2 0.4674503803253174\n",
      "Eval loss 0.012847022153437138, R2 0.5051463842391968\n",
      "epoch 3606, loss 0.012811832129955292, R2 0.46746259927749634\n",
      "Eval loss 0.012846690602600574, R2 0.5051590204238892\n",
      "epoch 3607, loss 0.012811526656150818, R2 0.4674755334854126\n",
      "Eval loss 0.012846359051764011, R2 0.5051716566085815\n",
      "epoch 3608, loss 0.01281121838837862, R2 0.467488169670105\n",
      "Eval loss 0.012846027500927448, R2 0.505184531211853\n",
      "epoch 3609, loss 0.012810911051928997, R2 0.4675009250640869\n",
      "Eval loss 0.012845695950090885, R2 0.5051973462104797\n",
      "epoch 3610, loss 0.012810604646801949, R2 0.46751344203948975\n",
      "Eval loss 0.012845363467931747, R2 0.5052098035812378\n",
      "epoch 3611, loss 0.0128102982416749, R2 0.46752655506134033\n",
      "Eval loss 0.012845031917095184, R2 0.5052232146263123\n",
      "epoch 3612, loss 0.012809991836547852, R2 0.46753960847854614\n",
      "Eval loss 0.012844703160226345, R2 0.5052355527877808\n",
      "epoch 3613, loss 0.012809685431420803, R2 0.4675520062446594\n",
      "Eval loss 0.012844372540712357, R2 0.5052483081817627\n",
      "epoch 3614, loss 0.01280937995761633, R2 0.46756458282470703\n",
      "Eval loss 0.012844040058553219, R2 0.5052613019943237\n",
      "epoch 3615, loss 0.012809074483811855, R2 0.4675777554512024\n",
      "Eval loss 0.01284371130168438, R2 0.5052734613418579\n",
      "epoch 3616, loss 0.012808768078684807, R2 0.46758997440338135\n",
      "Eval loss 0.012843380682170391, R2 0.5052865147590637\n",
      "epoch 3617, loss 0.012808464467525482, R2 0.46760278940200806\n",
      "Eval loss 0.012843050993978977, R2 0.5052993297576904\n",
      "epoch 3618, loss 0.012808158993721008, R2 0.46761584281921387\n",
      "Eval loss 0.012842722237110138, R2 0.5053117275238037\n",
      "epoch 3619, loss 0.012807853519916534, R2 0.4676281809806824\n",
      "Eval loss 0.01284239161759615, R2 0.5053247213363647\n",
      "epoch 3620, loss 0.01280754990875721, R2 0.4676414728164673\n",
      "Eval loss 0.01284206286072731, R2 0.5053372383117676\n",
      "epoch 3621, loss 0.01280724536627531, R2 0.4676539897918701\n",
      "Eval loss 0.012841734103858471, R2 0.5053501129150391\n",
      "epoch 3622, loss 0.012806939892470837, R2 0.4676661491394043\n",
      "Eval loss 0.012841404415667057, R2 0.5053625106811523\n",
      "epoch 3623, loss 0.012806636281311512, R2 0.46767860651016235\n",
      "Eval loss 0.012841075658798218, R2 0.5053750276565552\n",
      "epoch 3624, loss 0.012806331738829613, R2 0.46769124269485474\n",
      "Eval loss 0.012840747833251953, R2 0.5053879022598267\n",
      "epoch 3625, loss 0.012806029058992863, R2 0.4677043557167053\n",
      "Eval loss 0.012840420007705688, R2 0.5054004192352295\n",
      "epoch 3626, loss 0.012805725447833538, R2 0.4677162766456604\n",
      "Eval loss 0.01284009125083685, R2 0.5054133534431458\n",
      "epoch 3627, loss 0.012805421836674213, R2 0.46772927045822144\n",
      "Eval loss 0.012839763425290585, R2 0.5054256916046143\n",
      "epoch 3628, loss 0.012805118225514889, R2 0.4677422046661377\n",
      "Eval loss 0.01283943559974432, R2 0.5054384469985962\n",
      "epoch 3629, loss 0.012804816477000713, R2 0.4677542448043823\n",
      "Eval loss 0.01283910870552063, R2 0.5054512023925781\n",
      "epoch 3630, loss 0.012804512865841389, R2 0.4677668809890747\n",
      "Eval loss 0.012838782742619514, R2 0.5054636001586914\n",
      "epoch 3631, loss 0.012804209254682064, R2 0.4677794575691223\n",
      "Eval loss 0.012838453985750675, R2 0.5054761171340942\n",
      "epoch 3632, loss 0.012803907506167889, R2 0.46779268980026245\n",
      "Eval loss 0.012838128954172134, R2 0.5054885149002075\n",
      "epoch 3633, loss 0.012803604826331139, R2 0.46780461072921753\n",
      "Eval loss 0.012837802059948444, R2 0.5055012702941895\n",
      "epoch 3634, loss 0.012803304009139538, R2 0.46781712770462036\n",
      "Eval loss 0.012837475165724754, R2 0.5055140256881714\n",
      "epoch 3635, loss 0.012803000397980213, R2 0.46782970428466797\n",
      "Eval loss 0.012837150134146214, R2 0.5055264830589294\n",
      "epoch 3636, loss 0.012802699580788612, R2 0.4678422212600708\n",
      "Eval loss 0.012836823239922523, R2 0.505539059638977\n",
      "epoch 3637, loss 0.012802397832274437, R2 0.4678547978401184\n",
      "Eval loss 0.012836498208343983, R2 0.5055517554283142\n",
      "epoch 3638, loss 0.012802097015082836, R2 0.46786725521087646\n",
      "Eval loss 0.012836173176765442, R2 0.5055644512176514\n",
      "epoch 3639, loss 0.01280179712921381, R2 0.4678799510002136\n",
      "Eval loss 0.012835847213864326, R2 0.5055763721466064\n",
      "epoch 3640, loss 0.01280149444937706, R2 0.4678926467895508\n",
      "Eval loss 0.01283552311360836, R2 0.5055893659591675\n",
      "epoch 3641, loss 0.012801195494830608, R2 0.4679047465324402\n",
      "Eval loss 0.012835197150707245, R2 0.5056020021438599\n",
      "epoch 3642, loss 0.012800891883671284, R2 0.4679180383682251\n",
      "Eval loss 0.012834872119128704, R2 0.5056143999099731\n",
      "epoch 3643, loss 0.012800594791769981, R2 0.4679298996925354\n",
      "Eval loss 0.012834548018872738, R2 0.505626916885376\n",
      "epoch 3644, loss 0.012800293043255806, R2 0.46794241666793823\n",
      "Eval loss 0.012834223918616772, R2 0.5056391954421997\n",
      "epoch 3645, loss 0.012799992226064205, R2 0.4679551124572754\n",
      "Eval loss 0.01283390074968338, R2 0.5056518316268921\n",
      "epoch 3646, loss 0.012799694202840328, R2 0.46796751022338867\n",
      "Eval loss 0.012833576649427414, R2 0.5056641697883606\n",
      "epoch 3647, loss 0.012799392454326153, R2 0.46797966957092285\n",
      "Eval loss 0.012833251617848873, R2 0.5056763887405396\n",
      "epoch 3648, loss 0.01279909536242485, R2 0.46799206733703613\n",
      "Eval loss 0.012832928448915482, R2 0.5056891441345215\n",
      "epoch 3649, loss 0.012798795476555824, R2 0.46800416707992554\n",
      "Eval loss 0.01283260527998209, R2 0.5057014226913452\n",
      "epoch 3650, loss 0.012798495590686798, R2 0.46801698207855225\n",
      "Eval loss 0.012832283042371273, R2 0.505713939666748\n",
      "epoch 3651, loss 0.012798195704817772, R2 0.4680299162864685\n",
      "Eval loss 0.012831958942115307, R2 0.5057264566421509\n",
      "epoch 3652, loss 0.012797899544239044, R2 0.46804195642471313\n",
      "Eval loss 0.012831637635827065, R2 0.5057389736175537\n",
      "epoch 3653, loss 0.012797598727047443, R2 0.46805423498153687\n",
      "Eval loss 0.012831314466893673, R2 0.5057511329650879\n",
      "epoch 3654, loss 0.01279730349779129, R2 0.4680665135383606\n",
      "Eval loss 0.01283099316060543, R2 0.5057635307312012\n",
      "epoch 3655, loss 0.012797004543244839, R2 0.46807944774627686\n",
      "Eval loss 0.012830669060349464, R2 0.5057761669158936\n",
      "epoch 3656, loss 0.012796706520020962, R2 0.46809184551239014\n",
      "Eval loss 0.012830347754061222, R2 0.5057886838912964\n",
      "epoch 3657, loss 0.01279640756547451, R2 0.46810394525527954\n",
      "Eval loss 0.01283002644777298, R2 0.5058009028434753\n",
      "epoch 3658, loss 0.012796111404895782, R2 0.4681161046028137\n",
      "Eval loss 0.012829706072807312, R2 0.50581294298172\n",
      "epoch 3659, loss 0.012795812450349331, R2 0.46812915802001953\n",
      "Eval loss 0.012829385697841644, R2 0.5058255791664124\n",
      "epoch 3660, loss 0.012795515358448029, R2 0.4681413769721985\n",
      "Eval loss 0.012829063460230827, R2 0.5058378577232361\n",
      "epoch 3661, loss 0.0127952191978693, R2 0.468153178691864\n",
      "Eval loss 0.01282874308526516, R2 0.5058503150939941\n",
      "epoch 3662, loss 0.012794923037290573, R2 0.4681658148765564\n",
      "Eval loss 0.012828422710299492, R2 0.5058627128601074\n",
      "epoch 3663, loss 0.012794625014066696, R2 0.4681782126426697\n",
      "Eval loss 0.012828102335333824, R2 0.5058751702308655\n",
      "epoch 3664, loss 0.012794328853487968, R2 0.46819037199020386\n",
      "Eval loss 0.012827781960368156, R2 0.5058873891830444\n",
      "epoch 3665, loss 0.01279403269290924, R2 0.46820229291915894\n",
      "Eval loss 0.012827461585402489, R2 0.5058998465538025\n",
      "epoch 3666, loss 0.012793737463653088, R2 0.4682154059410095\n",
      "Eval loss 0.012827142141759396, R2 0.5059118866920471\n",
      "epoch 3667, loss 0.012793440371751785, R2 0.46822673082351685\n",
      "Eval loss 0.012826822698116302, R2 0.5059243440628052\n",
      "epoch 3668, loss 0.012793145142495632, R2 0.46823936700820923\n",
      "Eval loss 0.01282650325447321, R2 0.5059366226196289\n",
      "epoch 3669, loss 0.012792850844562054, R2 0.46825212240219116\n",
      "Eval loss 0.012826183810830116, R2 0.5059486031532288\n",
      "epoch 3670, loss 0.012792554683983326, R2 0.4682644009590149\n",
      "Eval loss 0.012825864367187023, R2 0.5059610605239868\n",
      "epoch 3671, loss 0.012792260386049747, R2 0.46827632188796997\n",
      "Eval loss 0.01282554492354393, R2 0.505973219871521\n",
      "epoch 3672, loss 0.01279196422547102, R2 0.4682886004447937\n",
      "Eval loss 0.012825227342545986, R2 0.5059857368469238\n",
      "epoch 3673, loss 0.012791669927537441, R2 0.4683011770248413\n",
      "Eval loss 0.012824908830225468, R2 0.5059981346130371\n",
      "epoch 3674, loss 0.012791374698281288, R2 0.46831345558166504\n",
      "Eval loss 0.01282459031790495, R2 0.5060102939605713\n",
      "epoch 3675, loss 0.012791081331670284, R2 0.468325138092041\n",
      "Eval loss 0.01282427180558443, R2 0.5060223937034607\n",
      "epoch 3676, loss 0.012790787033736706, R2 0.46833741664886475\n",
      "Eval loss 0.012823954224586487, R2 0.506034791469574\n",
      "epoch 3677, loss 0.012790492735803127, R2 0.4683494567871094\n",
      "Eval loss 0.012823635712265968, R2 0.5060470104217529\n",
      "epoch 3678, loss 0.012790199369192123, R2 0.46836167573928833\n",
      "Eval loss 0.012823318131268024, R2 0.5060591697692871\n",
      "epoch 3679, loss 0.01278990600258112, R2 0.4683745503425598\n",
      "Eval loss 0.01282300241291523, R2 0.5060713291168213\n",
      "epoch 3680, loss 0.012789612635970116, R2 0.4683868885040283\n",
      "Eval loss 0.012822684831917286, R2 0.5060833692550659\n",
      "epoch 3681, loss 0.012789318338036537, R2 0.4683992862701416\n",
      "Eval loss 0.012822368182241917, R2 0.5060961246490479\n",
      "epoch 3682, loss 0.012789024971425533, R2 0.4684111475944519\n",
      "Eval loss 0.012822053395211697, R2 0.5061080455780029\n",
      "epoch 3683, loss 0.01278873160481453, R2 0.4684230089187622\n",
      "Eval loss 0.012821735814213753, R2 0.5061204433441162\n",
      "epoch 3684, loss 0.012788440100848675, R2 0.46843546628952026\n",
      "Eval loss 0.012821420095860958, R2 0.5061321258544922\n",
      "epoch 3685, loss 0.01278814859688282, R2 0.4684470295906067\n",
      "Eval loss 0.012821104377508163, R2 0.5061444044113159\n",
      "epoch 3686, loss 0.012787855230271816, R2 0.46845924854278564\n",
      "Eval loss 0.012820787727832794, R2 0.5061569213867188\n",
      "epoch 3687, loss 0.012787562794983387, R2 0.4684717655181885\n",
      "Eval loss 0.012820472940802574, R2 0.5061686038970947\n",
      "epoch 3688, loss 0.012787271291017532, R2 0.46848368644714355\n",
      "Eval loss 0.012820156291127205, R2 0.5061808228492737\n",
      "epoch 3689, loss 0.012786977924406528, R2 0.4684959053993225\n",
      "Eval loss 0.012819841504096985, R2 0.5061935186386108\n",
      "epoch 3690, loss 0.0127866854891181, R2 0.4685078263282776\n",
      "Eval loss 0.01281952578574419, R2 0.5062050819396973\n",
      "epoch 3691, loss 0.012786395847797394, R2 0.4685204029083252\n",
      "Eval loss 0.012819210067391396, R2 0.5062179565429688\n",
      "epoch 3692, loss 0.012786105275154114, R2 0.46853214502334595\n",
      "Eval loss 0.012818895280361176, R2 0.5062299966812134\n",
      "epoch 3693, loss 0.012785813771188259, R2 0.46854478120803833\n",
      "Eval loss 0.01281858328729868, R2 0.5062416791915894\n",
      "epoch 3694, loss 0.012785522267222404, R2 0.4685562252998352\n",
      "Eval loss 0.01281826663762331, R2 0.5062540173530579\n",
      "epoch 3695, loss 0.012785231694579124, R2 0.4685683250427246\n",
      "Eval loss 0.012817952781915665, R2 0.5062661170959473\n",
      "epoch 3696, loss 0.012784942053258419, R2 0.46858054399490356\n",
      "Eval loss 0.012817639857530594, R2 0.5062781572341919\n",
      "epoch 3697, loss 0.012784651480615139, R2 0.46859294176101685\n",
      "Eval loss 0.012817326933145523, R2 0.5062898397445679\n",
      "epoch 3698, loss 0.012784359976649284, R2 0.46860456466674805\n",
      "Eval loss 0.012817013077437878, R2 0.5063021779060364\n",
      "epoch 3699, loss 0.012784072197973728, R2 0.4686164855957031\n",
      "Eval loss 0.012816701084375381, R2 0.5063140392303467\n",
      "epoch 3700, loss 0.012783780694007874, R2 0.46862858533859253\n",
      "Eval loss 0.012816387228667736, R2 0.5063263177871704\n",
      "epoch 3701, loss 0.012783491052687168, R2 0.46864062547683716\n",
      "Eval loss 0.012816074304282665, R2 0.5063380002975464\n",
      "epoch 3702, loss 0.012783202342689037, R2 0.4686528444290161\n",
      "Eval loss 0.012815761379897594, R2 0.5063505172729492\n",
      "epoch 3703, loss 0.012782913632690907, R2 0.4686645269393921\n",
      "Eval loss 0.012815449386835098, R2 0.5063620805740356\n",
      "epoch 3704, loss 0.012782623991370201, R2 0.4686768651008606\n",
      "Eval loss 0.012815136462450027, R2 0.5063744187355042\n",
      "epoch 3705, loss 0.01278233528137207, R2 0.4686886668205261\n",
      "Eval loss 0.012814824469387531, R2 0.5063865184783936\n",
      "epoch 3706, loss 0.012782045640051365, R2 0.4687005877494812\n",
      "Eval loss 0.012814512476325035, R2 0.5063986778259277\n",
      "epoch 3707, loss 0.012781756930053234, R2 0.4687127470970154\n",
      "Eval loss 0.012814201414585114, R2 0.5064101815223694\n",
      "epoch 3708, loss 0.012781469151377678, R2 0.4687248468399048\n",
      "Eval loss 0.012813889421522617, R2 0.506422758102417\n",
      "epoch 3709, loss 0.012781179510056973, R2 0.46873676776885986\n",
      "Eval loss 0.012813578359782696, R2 0.5064342021942139\n",
      "epoch 3710, loss 0.012780892662703991, R2 0.46874916553497314\n",
      "Eval loss 0.012813268229365349, R2 0.5064467191696167\n",
      "epoch 3711, loss 0.01278060395270586, R2 0.468761146068573\n",
      "Eval loss 0.012812957167625427, R2 0.5064584016799927\n",
      "epoch 3712, loss 0.012780317105352879, R2 0.4687725901603699\n",
      "Eval loss 0.012812646105885506, R2 0.5064703822135925\n",
      "epoch 3713, loss 0.012780030257999897, R2 0.46878451108932495\n",
      "Eval loss 0.012812335975468159, R2 0.5064823627471924\n",
      "epoch 3714, loss 0.01277974247932434, R2 0.4687963128089905\n",
      "Eval loss 0.012812024913728237, R2 0.5064942836761475\n",
      "epoch 3715, loss 0.012779456563293934, R2 0.4688083529472351\n",
      "Eval loss 0.012811715714633465, R2 0.5065065026283264\n",
      "epoch 3716, loss 0.012779167853295803, R2 0.46882033348083496\n",
      "Eval loss 0.012811405584216118, R2 0.5065183639526367\n",
      "epoch 3717, loss 0.012778881005942822, R2 0.46883296966552734\n",
      "Eval loss 0.012811094522476196, R2 0.5065300464630127\n",
      "epoch 3718, loss 0.01277859602123499, R2 0.46884429454803467\n",
      "Eval loss 0.012810786254703999, R2 0.5065418481826782\n",
      "epoch 3719, loss 0.012778307311236858, R2 0.46885645389556885\n",
      "Eval loss 0.012810476124286652, R2 0.5065539479255676\n",
      "epoch 3720, loss 0.012778022326529026, R2 0.4688681364059448\n",
      "Eval loss 0.012810167856514454, R2 0.5065661072731018\n",
      "epoch 3721, loss 0.012777736410498619, R2 0.4688803553581238\n",
      "Eval loss 0.012809858657419682, R2 0.5065779089927673\n",
      "epoch 3722, loss 0.012777450494468212, R2 0.46889156103134155\n",
      "Eval loss 0.01280954945832491, R2 0.5065896511077881\n",
      "epoch 3723, loss 0.012777164578437805, R2 0.46890395879745483\n",
      "Eval loss 0.012809240259230137, R2 0.5066016912460327\n",
      "epoch 3724, loss 0.012776880525052547, R2 0.4689154624938965\n",
      "Eval loss 0.012808932922780514, R2 0.5066133737564087\n",
      "epoch 3725, loss 0.01277659460902214, R2 0.4689273238182068\n",
      "Eval loss 0.01280862558633089, R2 0.5066251158714294\n",
      "epoch 3726, loss 0.012776307761669159, R2 0.4689393639564514\n",
      "Eval loss 0.012808317318558693, R2 0.5066370964050293\n",
      "epoch 3727, loss 0.012776024639606476, R2 0.46895116567611694\n",
      "Eval loss 0.012808009050786495, R2 0.5066486597061157\n",
      "epoch 3728, loss 0.012775739654898643, R2 0.4689633250236511\n",
      "Eval loss 0.012807702645659447, R2 0.5066604614257812\n",
      "epoch 3729, loss 0.012775454670190811, R2 0.4689747095108032\n",
      "Eval loss 0.012807394377887249, R2 0.5066729784011841\n",
      "epoch 3730, loss 0.012775170616805553, R2 0.4689863324165344\n",
      "Eval loss 0.012807086110115051, R2 0.5066845417022705\n",
      "epoch 3731, loss 0.01277488749474287, R2 0.4689987897872925\n",
      "Eval loss 0.012806778773665428, R2 0.506696343421936\n",
      "epoch 3732, loss 0.012774601578712463, R2 0.4690108299255371\n",
      "Eval loss 0.012806473299860954, R2 0.5067081451416016\n",
      "epoch 3733, loss 0.012774317525327206, R2 0.4690224528312683\n",
      "Eval loss 0.012806166894733906, R2 0.5067198276519775\n",
      "epoch 3734, loss 0.012774034403264523, R2 0.4690338969230652\n",
      "Eval loss 0.012805858626961708, R2 0.5067319869995117\n",
      "epoch 3735, loss 0.012773750349879265, R2 0.4690455198287964\n",
      "Eval loss 0.012805553153157234, R2 0.5067434310913086\n",
      "epoch 3736, loss 0.012773468159139156, R2 0.46905726194381714\n",
      "Eval loss 0.012805246748030186, R2 0.5067552328109741\n",
      "epoch 3737, loss 0.012773185037076473, R2 0.4690697193145752\n",
      "Eval loss 0.012804942205548286, R2 0.506766676902771\n",
      "epoch 3738, loss 0.01277290191501379, R2 0.4690808057785034\n",
      "Eval loss 0.012804635800421238, R2 0.5067785978317261\n",
      "epoch 3739, loss 0.012772618792951107, R2 0.46909254789352417\n",
      "Eval loss 0.012804330326616764, R2 0.5067903995513916\n",
      "epoch 3740, loss 0.012772337533533573, R2 0.46910423040390015\n",
      "Eval loss 0.01280402485281229, R2 0.5068022012710571\n",
      "epoch 3741, loss 0.01277205254882574, R2 0.4691162705421448\n",
      "Eval loss 0.012803720310330391, R2 0.5068144798278809\n",
      "epoch 3742, loss 0.012771772220730782, R2 0.4691277742385864\n",
      "Eval loss 0.012803414836525917, R2 0.5068256855010986\n",
      "epoch 3743, loss 0.012771489098668098, R2 0.4691395163536072\n",
      "Eval loss 0.012803110294044018, R2 0.5068378448486328\n",
      "epoch 3744, loss 0.01277120690792799, R2 0.4691509008407593\n",
      "Eval loss 0.012802805751562119, R2 0.5068494081497192\n",
      "epoch 3745, loss 0.012770925648510456, R2 0.4691629409790039\n",
      "Eval loss 0.012802503071725368, R2 0.5068612098693848\n",
      "epoch 3746, loss 0.012770643457770348, R2 0.469174861907959\n",
      "Eval loss 0.01280219666659832, R2 0.5068728923797607\n",
      "epoch 3747, loss 0.012770362198352814, R2 0.46918636560440063\n",
      "Eval loss 0.012801893055438995, R2 0.5068842768669128\n",
      "epoch 3748, loss 0.012770081870257854, R2 0.4691980481147766\n",
      "Eval loss 0.012801590375602245, R2 0.5068962574005127\n",
      "epoch 3749, loss 0.012769797816872597, R2 0.4692096710205078\n",
      "Eval loss 0.012801285833120346, R2 0.5069079399108887\n",
      "epoch 3750, loss 0.012769519351422787, R2 0.46922141313552856\n",
      "Eval loss 0.012800982221961021, R2 0.5069196224212646\n",
      "epoch 3751, loss 0.012769238092005253, R2 0.46923327445983887\n",
      "Eval loss 0.012800678610801697, R2 0.5069315433502197\n",
      "epoch 3752, loss 0.012768957763910294, R2 0.46924471855163574\n",
      "Eval loss 0.012800375930964947, R2 0.506942868232727\n",
      "epoch 3753, loss 0.01276867650449276, R2 0.4692564010620117\n",
      "Eval loss 0.012800073251128197, R2 0.5069546699523926\n",
      "epoch 3754, loss 0.012768397107720375, R2 0.4692680239677429\n",
      "Eval loss 0.012799772433936596, R2 0.506966233253479\n",
      "epoch 3755, loss 0.012768115848302841, R2 0.4692804217338562\n",
      "Eval loss 0.012799468822777271, R2 0.5069780945777893\n",
      "epoch 3756, loss 0.012767837382853031, R2 0.4692918062210083\n",
      "Eval loss 0.012799167074263096, R2 0.5069897174835205\n",
      "epoch 3757, loss 0.012767557054758072, R2 0.4693034887313843\n",
      "Eval loss 0.01279886532574892, R2 0.5070013403892517\n",
      "epoch 3758, loss 0.012767278589308262, R2 0.4693145155906677\n",
      "Eval loss 0.01279856264591217, R2 0.5070131421089172\n",
      "epoch 3759, loss 0.012766999192535877, R2 0.4693261384963989\n",
      "Eval loss 0.01279825996607542, R2 0.5070244073867798\n",
      "epoch 3760, loss 0.012766719795763493, R2 0.4693375825881958\n",
      "Eval loss 0.012797960080206394, R2 0.5070360898971558\n",
      "epoch 3761, loss 0.012766440398991108, R2 0.46934956312179565\n",
      "Eval loss 0.012797659263014793, R2 0.5070474147796631\n",
      "epoch 3762, loss 0.012766162864863873, R2 0.46936094760894775\n",
      "Eval loss 0.012797357514500618, R2 0.5070590972900391\n",
      "epoch 3763, loss 0.012765884399414062, R2 0.4693725109100342\n",
      "Eval loss 0.012797055765986443, R2 0.5070706009864807\n",
      "epoch 3764, loss 0.012765605933964252, R2 0.4693840742111206\n",
      "Eval loss 0.012796755880117416, R2 0.5070825815200806\n",
      "epoch 3765, loss 0.012765327468514442, R2 0.46939563751220703\n",
      "Eval loss 0.012796454131603241, R2 0.5070940256118774\n",
      "epoch 3766, loss 0.012765047140419483, R2 0.46940743923187256\n",
      "Eval loss 0.012796156108379364, R2 0.507105827331543\n",
      "epoch 3767, loss 0.012764770537614822, R2 0.46941930055618286\n",
      "Eval loss 0.012795855291187763, R2 0.5071168541908264\n",
      "epoch 3768, loss 0.012764493003487587, R2 0.46943068504333496\n",
      "Eval loss 0.012795556336641312, R2 0.5071286559104919\n",
      "epoch 3769, loss 0.012764214538037777, R2 0.4694417119026184\n",
      "Eval loss 0.012795253656804562, R2 0.507140040397644\n",
      "epoch 3770, loss 0.012763937935233116, R2 0.4694533944129944\n",
      "Eval loss 0.012794955633580685, R2 0.5071514844894409\n",
      "epoch 3771, loss 0.01276366040110588, R2 0.4694649577140808\n",
      "Eval loss 0.012794656679034233, R2 0.5071634650230408\n",
      "epoch 3772, loss 0.01276338379830122, R2 0.46947646141052246\n",
      "Eval loss 0.012794357724487782, R2 0.5071749687194824\n",
      "epoch 3773, loss 0.01276310719549656, R2 0.46948808431625366\n",
      "Eval loss 0.012794057838618755, R2 0.5071865320205688\n",
      "epoch 3774, loss 0.012762829661369324, R2 0.46949928998947144\n",
      "Eval loss 0.01279375795274973, R2 0.5071976184844971\n",
      "epoch 3775, loss 0.012762553989887238, R2 0.46951091289520264\n",
      "Eval loss 0.012793459929525852, R2 0.507209837436676\n",
      "epoch 3776, loss 0.012762277387082577, R2 0.46952223777770996\n",
      "Eval loss 0.0127931609749794, R2 0.5072208642959595\n",
      "epoch 3777, loss 0.012762000784277916, R2 0.4695337414741516\n",
      "Eval loss 0.012792863883078098, R2 0.5072325468063354\n",
      "epoch 3778, loss 0.012761724181473255, R2 0.4695456027984619\n",
      "Eval loss 0.012792564928531647, R2 0.5072438716888428\n",
      "epoch 3779, loss 0.012761450372636318, R2 0.4695568084716797\n",
      "Eval loss 0.012792267836630344, R2 0.5072554349899292\n",
      "epoch 3780, loss 0.012761174701154232, R2 0.4695689082145691\n",
      "Eval loss 0.012791968882083893, R2 0.5072665214538574\n",
      "epoch 3781, loss 0.012760899029672146, R2 0.4695795774459839\n",
      "Eval loss 0.01279167365282774, R2 0.5072785019874573\n",
      "epoch 3782, loss 0.01276062335819006, R2 0.4695912003517151\n",
      "Eval loss 0.012791374698281288, R2 0.507289707660675\n",
      "epoch 3783, loss 0.012760347686707973, R2 0.46960264444351196\n",
      "Eval loss 0.012791077606379986, R2 0.507300853729248\n",
      "epoch 3784, loss 0.012760072946548462, R2 0.46961408853530884\n",
      "Eval loss 0.012790781445801258, R2 0.5073127746582031\n",
      "epoch 3785, loss 0.012759797275066376, R2 0.4696255326271057\n",
      "Eval loss 0.01279048528522253, R2 0.507323682308197\n",
      "epoch 3786, loss 0.012759523466229439, R2 0.4696369171142578\n",
      "Eval loss 0.012790188193321228, R2 0.5073356032371521\n",
      "epoch 3787, loss 0.012759248726069927, R2 0.46964848041534424\n",
      "Eval loss 0.012789891101419926, R2 0.5073467493057251\n",
      "epoch 3788, loss 0.012758973985910416, R2 0.4696599245071411\n",
      "Eval loss 0.012789595872163773, R2 0.5073584318161011\n",
      "epoch 3789, loss 0.012758700177073479, R2 0.4696717858314514\n",
      "Eval loss 0.01278929878026247, R2 0.5073696374893188\n",
      "epoch 3790, loss 0.012758426368236542, R2 0.4696824550628662\n",
      "Eval loss 0.012789002619683743, R2 0.5073810815811157\n",
      "epoch 3791, loss 0.01275815162807703, R2 0.46969354152679443\n",
      "Eval loss 0.012788706459105015, R2 0.5073923468589783\n",
      "epoch 3792, loss 0.012757879681885242, R2 0.46970558166503906\n",
      "Eval loss 0.012788411229848862, R2 0.5074036121368408\n",
      "epoch 3793, loss 0.012757605873048306, R2 0.4697166085243225\n",
      "Eval loss 0.012788116000592709, R2 0.5074148178100586\n",
      "epoch 3794, loss 0.012757332995533943, R2 0.46972793340682983\n",
      "Eval loss 0.01278782170265913, R2 0.5074264407157898\n",
      "epoch 3795, loss 0.012757059186697006, R2 0.46973931789398193\n",
      "Eval loss 0.012787526473402977, R2 0.5074383020401001\n",
      "epoch 3796, loss 0.012756786309182644, R2 0.4697503447532654\n",
      "Eval loss 0.012787231244146824, R2 0.5074493288993835\n",
      "epoch 3797, loss 0.012756513431668282, R2 0.4697621464729309\n",
      "Eval loss 0.01278693787753582, R2 0.5074605941772461\n",
      "epoch 3798, loss 0.012756241485476494, R2 0.4697732925415039\n",
      "Eval loss 0.012786641716957092, R2 0.5074717402458191\n",
      "epoch 3799, loss 0.012755967676639557, R2 0.46978533267974854\n",
      "Eval loss 0.012786349281668663, R2 0.5074833035469055\n",
      "epoch 3800, loss 0.01275569573044777, R2 0.4697965383529663\n",
      "Eval loss 0.01278605405241251, R2 0.5074945688247681\n",
      "epoch 3801, loss 0.012755423784255981, R2 0.4698081612586975\n",
      "Eval loss 0.012785759754478931, R2 0.5075060129165649\n",
      "epoch 3802, loss 0.012755150906741619, R2 0.4698188304901123\n",
      "Eval loss 0.012785467319190502, R2 0.5075172781944275\n",
      "epoch 3803, loss 0.012754878960549831, R2 0.46983009576797485\n",
      "Eval loss 0.012785173021256924, R2 0.5075286030769348\n",
      "epoch 3804, loss 0.012754609808325768, R2 0.469840943813324\n",
      "Eval loss 0.01278487965464592, R2 0.5075403451919556\n",
      "epoch 3805, loss 0.01275433599948883, R2 0.4698526859283447\n",
      "Eval loss 0.01278458721935749, R2 0.5075510740280151\n",
      "epoch 3806, loss 0.012754065915942192, R2 0.46986424922943115\n",
      "Eval loss 0.012784292921423912, R2 0.507562518119812\n",
      "epoch 3807, loss 0.012753794901072979, R2 0.46987515687942505\n",
      "Eval loss 0.012784002348780632, R2 0.5075737237930298\n",
      "epoch 3808, loss 0.012753523886203766, R2 0.4698869585990906\n",
      "Eval loss 0.012783709913492203, R2 0.5075846910476685\n",
      "epoch 3809, loss 0.012753252871334553, R2 0.4698975086212158\n",
      "Eval loss 0.012783415615558624, R2 0.5075960159301758\n",
      "epoch 3810, loss 0.012752983719110489, R2 0.4699093699455261\n",
      "Eval loss 0.012783125042915344, R2 0.5076074600219727\n",
      "epoch 3811, loss 0.012752712704241276, R2 0.4699205160140991\n",
      "Eval loss 0.01278283353894949, R2 0.507619321346283\n",
      "epoch 3812, loss 0.012752443552017212, R2 0.4699311852455139\n",
      "Eval loss 0.012782540172338486, R2 0.5076298713684082\n",
      "epoch 3813, loss 0.012752172537147999, R2 0.4699426293373108\n",
      "Eval loss 0.012782249599695206, R2 0.5076415538787842\n",
      "epoch 3814, loss 0.01275190245360136, R2 0.46995365619659424\n",
      "Eval loss 0.012781957164406776, R2 0.5076524615287781\n",
      "epoch 3815, loss 0.012751631438732147, R2 0.4699649214744568\n",
      "Eval loss 0.012781666591763496, R2 0.5076636672019958\n",
      "epoch 3816, loss 0.012751362286508083, R2 0.46997612714767456\n",
      "Eval loss 0.012781375087797642, R2 0.5076749324798584\n",
      "epoch 3817, loss 0.01275109313428402, R2 0.46998733282089233\n",
      "Eval loss 0.012781084515154362, R2 0.5076855421066284\n",
      "epoch 3818, loss 0.012750823982059956, R2 0.46999847888946533\n",
      "Eval loss 0.012780793942511082, R2 0.507697582244873\n",
      "epoch 3819, loss 0.012750555761158466, R2 0.47000980377197266\n",
      "Eval loss 0.012780502438545227, R2 0.5077085494995117\n",
      "epoch 3820, loss 0.012750285677611828, R2 0.4700215458869934\n",
      "Eval loss 0.012780212797224522, R2 0.507719874382019\n",
      "epoch 3821, loss 0.012750017456710339, R2 0.4700325131416321\n",
      "Eval loss 0.012779920361936092, R2 0.5077310800552368\n",
      "epoch 3822, loss 0.01274974923580885, R2 0.4700431823730469\n",
      "Eval loss 0.012779631651937962, R2 0.5077419281005859\n",
      "epoch 3823, loss 0.012749481946229935, R2 0.4700547456741333\n",
      "Eval loss 0.01277934294193983, R2 0.5077528953552246\n",
      "epoch 3824, loss 0.012749211862683296, R2 0.47006547451019287\n",
      "Eval loss 0.012779051437973976, R2 0.5077641010284424\n",
      "epoch 3825, loss 0.012748944573104382, R2 0.47007715702056885\n",
      "Eval loss 0.01277876365929842, R2 0.5077756643295288\n",
      "epoch 3826, loss 0.012748676352202892, R2 0.4700884222984314\n",
      "Eval loss 0.01277847494930029, R2 0.507786750793457\n",
      "epoch 3827, loss 0.012748409993946552, R2 0.47009897232055664\n",
      "Eval loss 0.012778185307979584, R2 0.5077977776527405\n",
      "epoch 3828, loss 0.012748142704367638, R2 0.4701099395751953\n",
      "Eval loss 0.012777896597981453, R2 0.5078085660934448\n",
      "epoch 3829, loss 0.012747874483466148, R2 0.47012096643447876\n",
      "Eval loss 0.012777606956660748, R2 0.5078200101852417\n",
      "epoch 3830, loss 0.012747607193887234, R2 0.47013306617736816\n",
      "Eval loss 0.012777318246662617, R2 0.5078312158584595\n",
      "epoch 3831, loss 0.012747338972985744, R2 0.4701438546180725\n",
      "Eval loss 0.012777028605341911, R2 0.5078428983688354\n",
      "epoch 3832, loss 0.012747073546051979, R2 0.47015440464019775\n",
      "Eval loss 0.012776742689311504, R2 0.5078533291816711\n",
      "epoch 3833, loss 0.012746808119118214, R2 0.4701654314994812\n",
      "Eval loss 0.012776453979313374, R2 0.5078641772270203\n",
      "epoch 3834, loss 0.012746539898216724, R2 0.4701767563819885\n",
      "Eval loss 0.012776166200637817, R2 0.5078755617141724\n",
      "epoch 3835, loss 0.012746273539960384, R2 0.47018760442733765\n",
      "Eval loss 0.012775879353284836, R2 0.5078867673873901\n",
      "epoch 3836, loss 0.012746008113026619, R2 0.4701988697052002\n",
      "Eval loss 0.012775590643286705, R2 0.5078977346420288\n",
      "epoch 3837, loss 0.012745740823447704, R2 0.4702097773551941\n",
      "Eval loss 0.012775303795933723, R2 0.5079089403152466\n",
      "epoch 3838, loss 0.012745474465191364, R2 0.4702208638191223\n",
      "Eval loss 0.012775016948580742, R2 0.5079195499420166\n",
      "epoch 3839, loss 0.012745209038257599, R2 0.4702320694923401\n",
      "Eval loss 0.012774731032550335, R2 0.5079308748245239\n",
      "epoch 3840, loss 0.012744943611323833, R2 0.4702429175376892\n",
      "Eval loss 0.012774442322552204, R2 0.5079419612884521\n",
      "epoch 3841, loss 0.012744678184390068, R2 0.47025376558303833\n",
      "Eval loss 0.012774156406521797, R2 0.5079528093338013\n",
      "epoch 3842, loss 0.012744413688778877, R2 0.4702649712562561\n",
      "Eval loss 0.01277387049049139, R2 0.507964015007019\n",
      "epoch 3843, loss 0.012744148261845112, R2 0.47027599811553955\n",
      "Eval loss 0.012773584574460983, R2 0.5079751014709473\n",
      "epoch 3844, loss 0.012743882834911346, R2 0.4702875018119812\n",
      "Eval loss 0.012773298658430576, R2 0.5079861879348755\n",
      "epoch 3845, loss 0.01274361927062273, R2 0.47029799222946167\n",
      "Eval loss 0.01277301274240017, R2 0.507996678352356\n",
      "epoch 3846, loss 0.012743353843688965, R2 0.4703094959259033\n",
      "Eval loss 0.012772725895047188, R2 0.5080077648162842\n",
      "epoch 3847, loss 0.012743090279400349, R2 0.470319926738739\n",
      "Eval loss 0.01277244184166193, R2 0.5080191493034363\n",
      "epoch 3848, loss 0.012742824852466583, R2 0.47033101320266724\n",
      "Eval loss 0.012772155925631523, R2 0.5080300569534302\n",
      "epoch 3849, loss 0.012742562219500542, R2 0.47034192085266113\n",
      "Eval loss 0.01277187094092369, R2 0.5080411434173584\n",
      "epoch 3850, loss 0.012742296792566776, R2 0.4703529477119446\n",
      "Eval loss 0.012771585024893284, R2 0.5080522894859314\n",
      "epoch 3851, loss 0.01274203322827816, R2 0.47036391496658325\n",
      "Eval loss 0.012771300040185452, R2 0.5080629587173462\n",
      "epoch 3852, loss 0.012741769663989544, R2 0.47037482261657715\n",
      "Eval loss 0.01277101505547762, R2 0.5080739259719849\n",
      "epoch 3853, loss 0.012741507031023502, R2 0.4703857898712158\n",
      "Eval loss 0.012770731933414936, R2 0.5080850124359131\n",
      "epoch 3854, loss 0.012741242535412312, R2 0.4703969359397888\n",
      "Eval loss 0.012770446017384529, R2 0.5080958604812622\n",
      "epoch 3855, loss 0.01274097990244627, R2 0.4704076647758484\n",
      "Eval loss 0.01277016382664442, R2 0.5081069469451904\n",
      "epoch 3856, loss 0.012740718200802803, R2 0.4704185724258423\n",
      "Eval loss 0.012769880704581738, R2 0.50811767578125\n",
      "epoch 3857, loss 0.012740455567836761, R2 0.4704294800758362\n",
      "Eval loss 0.012769595719873905, R2 0.5081287622451782\n",
      "epoch 3858, loss 0.012740192003548145, R2 0.47044074535369873\n",
      "Eval loss 0.012769311666488647, R2 0.508139431476593\n",
      "epoch 3859, loss 0.012739930301904678, R2 0.4704514741897583\n",
      "Eval loss 0.012769029475748539, R2 0.5081504583358765\n",
      "epoch 3860, loss 0.012739666737616062, R2 0.470462441444397\n",
      "Eval loss 0.012768745422363281, R2 0.5081612467765808\n",
      "epoch 3861, loss 0.01273940596729517, R2 0.47047311067581177\n",
      "Eval loss 0.012768462300300598, R2 0.5081720352172852\n",
      "epoch 3862, loss 0.012739143334329128, R2 0.47048401832580566\n",
      "Eval loss 0.01276818010956049, R2 0.5081831812858582\n",
      "epoch 3863, loss 0.012738881632685661, R2 0.4704948663711548\n",
      "Eval loss 0.012767897918820381, R2 0.5081940293312073\n",
      "epoch 3864, loss 0.01273861899971962, R2 0.47050565481185913\n",
      "Eval loss 0.012767615728080273, R2 0.5082050561904907\n",
      "epoch 3865, loss 0.012738359160721302, R2 0.4705166220664978\n",
      "Eval loss 0.01276733260601759, R2 0.5082160830497742\n",
      "epoch 3866, loss 0.01273809839040041, R2 0.47052812576293945\n",
      "Eval loss 0.012767050415277481, R2 0.5082268118858337\n",
      "epoch 3867, loss 0.012737836688756943, R2 0.47053831815719604\n",
      "Eval loss 0.012766768224537373, R2 0.5082375407218933\n",
      "epoch 3868, loss 0.01273757591843605, R2 0.47054916620254517\n",
      "Eval loss 0.012766486033797264, R2 0.5082484483718872\n",
      "epoch 3869, loss 0.012737315148115158, R2 0.4705600142478943\n",
      "Eval loss 0.01276620477437973, R2 0.5082594156265259\n",
      "epoch 3870, loss 0.012737054377794266, R2 0.47057080268859863\n",
      "Eval loss 0.012765923514962196, R2 0.508270263671875\n",
      "epoch 3871, loss 0.012736793607473373, R2 0.47058171033859253\n",
      "Eval loss 0.012765642255544662, R2 0.5082809329032898\n",
      "epoch 3872, loss 0.012736532837152481, R2 0.4705924987792969\n",
      "Eval loss 0.012765361927449703, R2 0.5082914233207703\n",
      "epoch 3873, loss 0.012736272998154163, R2 0.4706040024757385\n",
      "Eval loss 0.012765079736709595, R2 0.5083023309707642\n",
      "epoch 3874, loss 0.012736011296510696, R2 0.4706141948699951\n",
      "Eval loss 0.01276480033993721, R2 0.5083135366439819\n",
      "epoch 3875, loss 0.012735753320157528, R2 0.47062546014785767\n",
      "Eval loss 0.01276452001184225, R2 0.5083242654800415\n",
      "epoch 3876, loss 0.012735494412481785, R2 0.47063565254211426\n",
      "Eval loss 0.012764238752424717, R2 0.508335292339325\n",
      "epoch 3877, loss 0.012735233642160892, R2 0.47064638137817383\n",
      "Eval loss 0.012763958424329758, R2 0.5083459615707397\n",
      "epoch 3878, loss 0.01273497473448515, R2 0.4706571102142334\n",
      "Eval loss 0.012763677164912224, R2 0.5083563327789307\n",
      "epoch 3879, loss 0.012734716758131981, R2 0.4706680178642273\n",
      "Eval loss 0.012763398699462414, R2 0.5083674192428589\n",
      "epoch 3880, loss 0.012734456919133663, R2 0.47067928314208984\n",
      "Eval loss 0.012763118371367455, R2 0.5083781480789185\n",
      "epoch 3881, loss 0.01273419801145792, R2 0.47068941593170166\n",
      "Eval loss 0.012762838043272495, R2 0.5083886384963989\n",
      "epoch 3882, loss 0.012733939103782177, R2 0.4707005023956299\n",
      "Eval loss 0.012762557715177536, R2 0.5083999633789062\n",
      "epoch 3883, loss 0.012733680196106434, R2 0.47071129083633423\n",
      "Eval loss 0.0127622801810503, R2 0.5084103941917419\n",
      "epoch 3884, loss 0.01273342315107584, R2 0.47072160243988037\n",
      "Eval loss 0.01276200171560049, R2 0.5084208250045776\n",
      "epoch 3885, loss 0.012733166106045246, R2 0.47073233127593994\n",
      "Eval loss 0.012761722318828106, R2 0.5084317922592163\n",
      "epoch 3886, loss 0.012732905335724354, R2 0.47074395418167114\n",
      "Eval loss 0.01276144478470087, R2 0.5084422826766968\n",
      "epoch 3887, loss 0.012732649222016335, R2 0.47075414657592773\n",
      "Eval loss 0.012761165387928486, R2 0.5084534883499146\n",
      "epoch 3888, loss 0.012732390314340591, R2 0.470764696598053\n",
      "Eval loss 0.01276088785380125, R2 0.5084642171859741\n",
      "epoch 3889, loss 0.012732132337987423, R2 0.4707756042480469\n",
      "Eval loss 0.01276060938835144, R2 0.5084747672080994\n",
      "epoch 3890, loss 0.012731876224279404, R2 0.47078609466552734\n",
      "Eval loss 0.012760331854224205, R2 0.5084854364395142\n",
      "epoch 3891, loss 0.012731618247926235, R2 0.4707968235015869\n",
      "Eval loss 0.012760053388774395, R2 0.5084961652755737\n",
      "epoch 3892, loss 0.012731361202895641, R2 0.4708079695701599\n",
      "Eval loss 0.01275977585464716, R2 0.508506715297699\n",
      "epoch 3893, loss 0.012731104157865047, R2 0.47081804275512695\n",
      "Eval loss 0.012759499251842499, R2 0.5085176825523376\n",
      "epoch 3894, loss 0.012730848044157028, R2 0.4708288311958313\n",
      "Eval loss 0.012759220786392689, R2 0.5085282325744629\n",
      "epoch 3895, loss 0.012730590999126434, R2 0.4708395004272461\n",
      "Eval loss 0.012758944183588028, R2 0.5085386037826538\n",
      "epoch 3896, loss 0.012730334885418415, R2 0.4708501100540161\n",
      "Eval loss 0.012758667580783367, R2 0.5085494518280029\n",
      "epoch 3897, loss 0.012730078771710396, R2 0.4708607792854309\n",
      "Eval loss 0.012758391909301281, R2 0.5085601806640625\n",
      "epoch 3898, loss 0.012729821726679802, R2 0.4708712697029114\n",
      "Eval loss 0.012758113443851471, R2 0.5085707902908325\n",
      "epoch 3899, loss 0.012729565612971783, R2 0.4708826541900635\n",
      "Eval loss 0.01275783684104681, R2 0.5085819959640503\n",
      "epoch 3900, loss 0.012729310430586338, R2 0.4708927273750305\n",
      "Eval loss 0.012757561169564724, R2 0.5085922479629517\n",
      "epoch 3901, loss 0.012729054316878319, R2 0.47090303897857666\n",
      "Eval loss 0.012757286429405212, R2 0.5086026191711426\n",
      "epoch 3902, loss 0.012728799134492874, R2 0.470913827419281\n",
      "Eval loss 0.012757009826600552, R2 0.5086134672164917\n",
      "epoch 3903, loss 0.01272854395210743, R2 0.4709245562553406\n",
      "Eval loss 0.012756734155118465, R2 0.5086238980293274\n",
      "epoch 3904, loss 0.012728288769721985, R2 0.47093504667282104\n",
      "Eval loss 0.01275645848363638, R2 0.5086346864700317\n",
      "epoch 3905, loss 0.012728032656013966, R2 0.4709460139274597\n",
      "Eval loss 0.012756182812154293, R2 0.5086452960968018\n",
      "epoch 3906, loss 0.012727778404951096, R2 0.4709567427635193\n",
      "Eval loss 0.012755909003317356, R2 0.5086556673049927\n",
      "epoch 3907, loss 0.012727524153888226, R2 0.47096747159957886\n",
      "Eval loss 0.01275563333183527, R2 0.508666455745697\n",
      "epoch 3908, loss 0.012727268040180206, R2 0.4709779620170593\n",
      "Eval loss 0.012755356729030609, R2 0.5086767673492432\n",
      "epoch 3909, loss 0.012727014720439911, R2 0.4709886908531189\n",
      "Eval loss 0.012755082920193672, R2 0.5086876153945923\n",
      "epoch 3910, loss 0.012726759538054466, R2 0.47099876403808594\n",
      "Eval loss 0.01275480818003416, R2 0.5086981058120728\n",
      "epoch 3911, loss 0.01272650621831417, R2 0.4710099697113037\n",
      "Eval loss 0.012754535302519798, R2 0.5087087154388428\n",
      "epoch 3912, loss 0.01272625382989645, R2 0.471019983291626\n",
      "Eval loss 0.012754260562360287, R2 0.5087190270423889\n",
      "epoch 3913, loss 0.012725998647511005, R2 0.4710308909416199\n",
      "Eval loss 0.012753985822200775, R2 0.5087299346923828\n",
      "epoch 3914, loss 0.01272574532777071, R2 0.4710407257080078\n",
      "Eval loss 0.012753712013363838, R2 0.5087401866912842\n",
      "epoch 3915, loss 0.01272549293935299, R2 0.47105157375335693\n",
      "Eval loss 0.012753439135849476, R2 0.5087506771087646\n",
      "epoch 3916, loss 0.012725237756967545, R2 0.4710625410079956\n",
      "Eval loss 0.012753165327012539, R2 0.5087615251541138\n",
      "epoch 3917, loss 0.012724986299872398, R2 0.4710731506347656\n",
      "Eval loss 0.012752891518175602, R2 0.5087723731994629\n",
      "epoch 3918, loss 0.012724732980132103, R2 0.4710833430290222\n",
      "Eval loss 0.01275261677801609, R2 0.5087826251983643\n",
      "epoch 3919, loss 0.012724479660391808, R2 0.4710935354232788\n",
      "Eval loss 0.012752344831824303, R2 0.5087932348251343\n",
      "epoch 3920, loss 0.012724227271974087, R2 0.4711040258407593\n",
      "Eval loss 0.012752072885632515, R2 0.5088036060333252\n",
      "epoch 3921, loss 0.012723973952233791, R2 0.4711146950721741\n",
      "Eval loss 0.012751800939440727, R2 0.5088143348693848\n",
      "epoch 3922, loss 0.012723722495138645, R2 0.4711254835128784\n",
      "Eval loss 0.012751528061926365, R2 0.5088245868682861\n",
      "epoch 3923, loss 0.012723470106720924, R2 0.471135675907135\n",
      "Eval loss 0.012751256115734577, R2 0.508834958076477\n",
      "epoch 3924, loss 0.012723216786980629, R2 0.4711461663246155\n",
      "Eval loss 0.01275098416954279, R2 0.5088456869125366\n",
      "epoch 3925, loss 0.012722966261208057, R2 0.47115641832351685\n",
      "Eval loss 0.012750712223351002, R2 0.5088561773300171\n",
      "epoch 3926, loss 0.012722714804112911, R2 0.4711669087409973\n",
      "Eval loss 0.012750441208481789, R2 0.5088664293289185\n",
      "epoch 3927, loss 0.012722463347017765, R2 0.4711778163909912\n",
      "Eval loss 0.012750170193612576, R2 0.5088770389556885\n",
      "epoch 3928, loss 0.012722211889922619, R2 0.4711877703666687\n",
      "Eval loss 0.012749897316098213, R2 0.508887529373169\n",
      "epoch 3929, loss 0.012721960432827473, R2 0.4711984395980835\n",
      "Eval loss 0.012749625369906425, R2 0.5088980197906494\n",
      "epoch 3930, loss 0.012721708975732327, R2 0.47120869159698486\n",
      "Eval loss 0.012749354355037212, R2 0.5089088678359985\n",
      "epoch 3931, loss 0.012721458449959755, R2 0.4712193012237549\n",
      "Eval loss 0.012749083340168, R2 0.5089190006256104\n",
      "epoch 3932, loss 0.012721207924187183, R2 0.47122955322265625\n",
      "Eval loss 0.012748812325298786, R2 0.5089288949966431\n",
      "epoch 3933, loss 0.012720956467092037, R2 0.4712402820587158\n",
      "Eval loss 0.012748543173074722, R2 0.5089393854141235\n",
      "epoch 3934, loss 0.012720707803964615, R2 0.4712509512901306\n",
      "Eval loss 0.01274827215820551, R2 0.5089497566223145\n",
      "epoch 3935, loss 0.012720457278192043, R2 0.471261203289032\n",
      "Eval loss 0.012748001143336296, R2 0.5089606046676636\n",
      "epoch 3936, loss 0.012720205821096897, R2 0.4712711572647095\n",
      "Eval loss 0.012747731991112232, R2 0.5089707970619202\n",
      "epoch 3937, loss 0.012719957157969475, R2 0.4712815284729004\n",
      "Eval loss 0.012747461907565594, R2 0.5089813470840454\n",
      "epoch 3938, loss 0.012719706632196903, R2 0.47129207849502563\n",
      "Eval loss 0.012747191824018955, R2 0.5089914798736572\n",
      "epoch 3939, loss 0.012719457969069481, R2 0.4713022708892822\n",
      "Eval loss 0.012746921740472317, R2 0.509002149105072\n",
      "epoch 3940, loss 0.012719206511974335, R2 0.47131335735321045\n",
      "Eval loss 0.012746652588248253, R2 0.5090123414993286\n",
      "epoch 3941, loss 0.012718957848846912, R2 0.47132354974746704\n",
      "Eval loss 0.012746382504701614, R2 0.5090224742889404\n",
      "epoch 3942, loss 0.012718710117042065, R2 0.4713340401649475\n",
      "Eval loss 0.012746114283800125, R2 0.5090333819389343\n",
      "epoch 3943, loss 0.012718460522592068, R2 0.47134339809417725\n",
      "Eval loss 0.012745845131576061, R2 0.5090436339378357\n",
      "epoch 3944, loss 0.012718209996819496, R2 0.4713541269302368\n",
      "Eval loss 0.012745575979351997, R2 0.5090535879135132\n",
      "epoch 3945, loss 0.012717962265014648, R2 0.47136443853378296\n",
      "Eval loss 0.012745307758450508, R2 0.5090638399124146\n",
      "epoch 3946, loss 0.012717712670564651, R2 0.4713748097419739\n",
      "Eval loss 0.012745039537549019, R2 0.5090748071670532\n",
      "epoch 3947, loss 0.01271746400743723, R2 0.4713855981826782\n",
      "Eval loss 0.01274477131664753, R2 0.5090848207473755\n",
      "epoch 3948, loss 0.012717216275632381, R2 0.47139590978622437\n",
      "Eval loss 0.012744504027068615, R2 0.5090948343276978\n",
      "epoch 3949, loss 0.012716967612504959, R2 0.4714057445526123\n",
      "Eval loss 0.0127442367374897, R2 0.5091055631637573\n",
      "epoch 3950, loss 0.012716720812022686, R2 0.47141605615615845\n",
      "Eval loss 0.012743966653943062, R2 0.5091159343719482\n",
      "epoch 3951, loss 0.012716474011540413, R2 0.47142648696899414\n",
      "Eval loss 0.012743700295686722, R2 0.5091259479522705\n",
      "epoch 3952, loss 0.012716224417090416, R2 0.4714365005493164\n",
      "Eval loss 0.012743433006107807, R2 0.5091365575790405\n",
      "epoch 3953, loss 0.012715976685285568, R2 0.4714469313621521\n",
      "Eval loss 0.012743164785206318, R2 0.5091464519500732\n",
      "epoch 3954, loss 0.01271573081612587, R2 0.47145718336105347\n",
      "Eval loss 0.012742899358272552, R2 0.5091569423675537\n",
      "epoch 3955, loss 0.012715483084321022, R2 0.4714685082435608\n",
      "Eval loss 0.012742631137371063, R2 0.509166955947876\n",
      "epoch 3956, loss 0.012715235352516174, R2 0.4714779257774353\n",
      "Eval loss 0.012742363847792149, R2 0.5091773271560669\n",
      "epoch 3957, loss 0.012714989483356476, R2 0.47148799896240234\n",
      "Eval loss 0.012742097489535809, R2 0.5091879367828369\n",
      "epoch 3958, loss 0.012714742682874203, R2 0.4714982509613037\n",
      "Eval loss 0.012741832993924618, R2 0.5091980695724487\n",
      "epoch 3959, loss 0.01271449588239193, R2 0.47150886058807373\n",
      "Eval loss 0.012741564773023129, R2 0.5092086791992188\n",
      "epoch 3960, loss 0.012714250013232231, R2 0.4715195894241333\n",
      "Eval loss 0.012741300277411938, R2 0.509218692779541\n",
      "epoch 3961, loss 0.012714003212749958, R2 0.47152894735336304\n",
      "Eval loss 0.012741033919155598, R2 0.5092288255691528\n",
      "epoch 3962, loss 0.01271375548094511, R2 0.4715392589569092\n",
      "Eval loss 0.012740766629576683, R2 0.5092393755912781\n",
      "epoch 3963, loss 0.012713508680462837, R2 0.4715496897697449\n",
      "Eval loss 0.012740501202642918, R2 0.5092494487762451\n",
      "epoch 3964, loss 0.012713265605270863, R2 0.47155964374542236\n",
      "Eval loss 0.012740237638354301, R2 0.5092596411705017\n",
      "epoch 3965, loss 0.012713020667433739, R2 0.4715700149536133\n",
      "Eval loss 0.012739971280097961, R2 0.5092698931694031\n",
      "epoch 3966, loss 0.012712773866951466, R2 0.4715800881385803\n",
      "Eval loss 0.01273970678448677, R2 0.5092799067497253\n",
      "epoch 3967, loss 0.012712527997791767, R2 0.4715909957885742\n",
      "Eval loss 0.01273944228887558, R2 0.5092902779579163\n",
      "epoch 3968, loss 0.012712284922599792, R2 0.47160059213638306\n",
      "Eval loss 0.012739176861941814, R2 0.5093005895614624\n",
      "epoch 3969, loss 0.01271203812211752, R2 0.4716106653213501\n",
      "Eval loss 0.012738911435008049, R2 0.5093107223510742\n",
      "epoch 3970, loss 0.01271179411560297, R2 0.4716207981109619\n",
      "Eval loss 0.012738647870719433, R2 0.5093204379081726\n",
      "epoch 3971, loss 0.012711548246443272, R2 0.4716310501098633\n",
      "Eval loss 0.012738384306430817, R2 0.5093307495117188\n",
      "epoch 3972, loss 0.012711303308606148, R2 0.4716408848762512\n",
      "Eval loss 0.012738118879497051, R2 0.5093411207199097\n",
      "epoch 3973, loss 0.012711059302091599, R2 0.47165149450302124\n",
      "Eval loss 0.01273785624653101, R2 0.5093517899513245\n",
      "epoch 3974, loss 0.01271081529557705, R2 0.4716615080833435\n",
      "Eval loss 0.012737592682242393, R2 0.5093615055084229\n",
      "epoch 3975, loss 0.0127105712890625, R2 0.4716716408729553\n",
      "Eval loss 0.012737327255308628, R2 0.509371280670166\n",
      "epoch 3976, loss 0.012710326351225376, R2 0.47168248891830444\n",
      "Eval loss 0.012737065553665161, R2 0.5093816518783569\n",
      "epoch 3977, loss 0.012710082344710827, R2 0.47169196605682373\n",
      "Eval loss 0.012736801989376545, R2 0.5093915462493896\n",
      "epoch 3978, loss 0.012709839269518852, R2 0.4717023968696594\n",
      "Eval loss 0.012736537493765354, R2 0.5094019770622253\n",
      "epoch 3979, loss 0.012709596194326878, R2 0.47171247005462646\n",
      "Eval loss 0.012736274860799313, R2 0.5094118118286133\n",
      "epoch 3980, loss 0.012709351256489754, R2 0.4717223644256592\n",
      "Eval loss 0.012736011296510696, R2 0.5094223022460938\n",
      "epoch 3981, loss 0.012709110043942928, R2 0.47173255681991577\n",
      "Eval loss 0.012735750526189804, R2 0.509432315826416\n",
      "epoch 3982, loss 0.012708866037428379, R2 0.4717426896095276\n",
      "Eval loss 0.012735488824546337, R2 0.5094425678253174\n",
      "epoch 3983, loss 0.012708622962236404, R2 0.4717526435852051\n",
      "Eval loss 0.012735223397612572, R2 0.5094528794288635\n",
      "epoch 3984, loss 0.01270837988704443, R2 0.4717627167701721\n",
      "Eval loss 0.01273496262729168, R2 0.5094626545906067\n",
      "epoch 3985, loss 0.01270813774317503, R2 0.47177261114120483\n",
      "Eval loss 0.012734700925648212, R2 0.5094727277755737\n",
      "epoch 3986, loss 0.012707894667983055, R2 0.471782922744751\n",
      "Eval loss 0.012734439224004745, R2 0.5094828605651855\n",
      "epoch 3987, loss 0.01270765345543623, R2 0.4717927575111389\n",
      "Eval loss 0.012734177522361279, R2 0.5094932317733765\n",
      "epoch 3988, loss 0.012707410380244255, R2 0.4718030095100403\n",
      "Eval loss 0.012733914889395237, R2 0.5095030069351196\n",
      "epoch 3989, loss 0.012707168236374855, R2 0.4718135595321655\n",
      "Eval loss 0.012733654119074345, R2 0.5095130801200867\n",
      "epoch 3990, loss 0.012706926092505455, R2 0.47182315587997437\n",
      "Eval loss 0.012733392417430878, R2 0.5095228552818298\n",
      "epoch 3991, loss 0.01270668487995863, R2 0.47183316946029663\n",
      "Eval loss 0.01273313257843256, R2 0.5095335841178894\n",
      "epoch 3992, loss 0.012706443667411804, R2 0.4718431830406189\n",
      "Eval loss 0.012732870876789093, R2 0.5095433592796326\n",
      "epoch 3993, loss 0.012706201523542404, R2 0.47185325622558594\n",
      "Eval loss 0.012732611037790775, R2 0.5095532536506653\n",
      "epoch 3994, loss 0.012705960310995579, R2 0.4718638062477112\n",
      "Eval loss 0.012732350267469883, R2 0.5095629692077637\n",
      "epoch 3995, loss 0.012705720029771328, R2 0.4718736410140991\n",
      "Eval loss 0.012732090428471565, R2 0.5095729827880859\n",
      "epoch 3996, loss 0.012705478817224503, R2 0.4718838334083557\n",
      "Eval loss 0.012731828726828098, R2 0.5095831155776978\n",
      "epoch 3997, loss 0.012705238536000252, R2 0.47189295291900635\n",
      "Eval loss 0.01273156888782978, R2 0.5095930695533752\n",
      "epoch 3998, loss 0.012704997323453426, R2 0.47190314531326294\n",
      "Eval loss 0.012731309980154037, R2 0.5096033811569214\n",
      "epoch 3999, loss 0.012704757042229176, R2 0.47191333770751953\n",
      "Eval loss 0.01273105014115572, R2 0.5096136331558228\n",
      "epoch 4000, loss 0.012704516761004925, R2 0.47192418575286865\n",
      "Eval loss 0.012730790302157402, R2 0.5096235275268555\n",
      "epoch 4001, loss 0.012704276479780674, R2 0.4719333052635193\n",
      "Eval loss 0.012730531394481659, R2 0.5096331834793091\n",
      "epoch 4002, loss 0.012704036198556423, R2 0.4719431400299072\n",
      "Eval loss 0.012730272486805916, R2 0.5096430778503418\n",
      "epoch 4003, loss 0.012703794986009598, R2 0.4719539284706116\n",
      "Eval loss 0.012730012647807598, R2 0.5096533298492432\n",
      "epoch 4004, loss 0.012703555636107922, R2 0.47196322679519653\n",
      "Eval loss 0.01272975280880928, R2 0.5096632242202759\n",
      "epoch 4005, loss 0.01270331535488367, R2 0.4719734191894531\n",
      "Eval loss 0.012729495763778687, R2 0.5096733570098877\n",
      "epoch 4006, loss 0.01270307693630457, R2 0.47198331356048584\n",
      "Eval loss 0.012729236856102943, R2 0.5096832513809204\n",
      "epoch 4007, loss 0.012702837586402893, R2 0.47199326753616333\n",
      "Eval loss 0.012728977017104626, R2 0.5096930265426636\n",
      "epoch 4008, loss 0.012702598236501217, R2 0.4720035195350647\n",
      "Eval loss 0.012728719040751457, R2 0.5097031593322754\n",
      "epoch 4009, loss 0.01270235888659954, R2 0.47201329469680786\n",
      "Eval loss 0.012728461064398289, R2 0.5097130537033081\n",
      "epoch 4010, loss 0.01270212046802044, R2 0.4720227122306824\n",
      "Eval loss 0.01272820495069027, R2 0.5097227096557617\n",
      "epoch 4011, loss 0.012701881118118763, R2 0.47203266620635986\n",
      "Eval loss 0.012727946974337101, R2 0.5097330212593079\n",
      "epoch 4012, loss 0.012701641768217087, R2 0.4720427989959717\n",
      "Eval loss 0.012727688997983932, R2 0.5097429752349854\n",
      "epoch 4013, loss 0.01270140428096056, R2 0.47205281257629395\n",
      "Eval loss 0.01272743009030819, R2 0.5097529292106628\n",
      "epoch 4014, loss 0.012701165862381458, R2 0.4720624089241028\n",
      "Eval loss 0.012727174907922745, R2 0.5097626447677612\n",
      "epoch 4015, loss 0.012700927443802357, R2 0.4720724821090698\n",
      "Eval loss 0.012726916000247002, R2 0.5097730159759521\n",
      "epoch 4016, loss 0.01270068995654583, R2 0.47208237648010254\n",
      "Eval loss 0.012726659886538982, R2 0.5097824931144714\n",
      "epoch 4017, loss 0.012700452469289303, R2 0.4720923900604248\n",
      "Eval loss 0.012726401910185814, R2 0.5097923874855042\n",
      "epoch 4018, loss 0.012700214050710201, R2 0.47210216522216797\n",
      "Eval loss 0.012726145796477795, R2 0.5098021030426025\n",
      "epoch 4019, loss 0.012699976563453674, R2 0.4721119999885559\n",
      "Eval loss 0.0127258887514472, R2 0.5098121166229248\n",
      "epoch 4020, loss 0.012699738144874573, R2 0.47212207317352295\n",
      "Eval loss 0.012725633569061756, R2 0.5098217129707336\n",
      "epoch 4021, loss 0.012699502520263195, R2 0.4721318483352661\n",
      "Eval loss 0.012725377455353737, R2 0.5098319053649902\n",
      "epoch 4022, loss 0.012699264101684093, R2 0.4721421003341675\n",
      "Eval loss 0.012725119479000568, R2 0.5098420977592468\n",
      "epoch 4023, loss 0.012699028477072716, R2 0.4721514582633972\n",
      "Eval loss 0.012724865227937698, R2 0.5098515748977661\n",
      "epoch 4024, loss 0.012698791921138763, R2 0.47216129302978516\n",
      "Eval loss 0.012724610045552254, R2 0.5098611116409302\n",
      "epoch 4025, loss 0.012698555365204811, R2 0.4721711277961731\n",
      "Eval loss 0.012724355794489384, R2 0.509871244430542\n",
      "epoch 4026, loss 0.012698318809270859, R2 0.47218096256256104\n",
      "Eval loss 0.01272409688681364, R2 0.5098814964294434\n",
      "epoch 4027, loss 0.012698081322014332, R2 0.4721909761428833\n",
      "Eval loss 0.012723843567073345, R2 0.5098906755447388\n",
      "epoch 4028, loss 0.01269784476608038, R2 0.47220081090927124\n",
      "Eval loss 0.012723587453365326, R2 0.5099009871482849\n",
      "epoch 4029, loss 0.012697609141469002, R2 0.4722104072570801\n",
      "Eval loss 0.012723332270979881, R2 0.5099103450775146\n",
      "epoch 4030, loss 0.012697374448180199, R2 0.47222018241882324\n",
      "Eval loss 0.012723078951239586, R2 0.5099204182624817\n",
      "epoch 4031, loss 0.012697136960923672, R2 0.47223055362701416\n",
      "Eval loss 0.012722821906208992, R2 0.509930431842804\n",
      "epoch 4032, loss 0.012696902267634869, R2 0.47223979234695435\n",
      "Eval loss 0.012722568586468697, R2 0.5099396705627441\n",
      "epoch 4033, loss 0.012696666643023491, R2 0.47224974632263184\n",
      "Eval loss 0.012722316198050976, R2 0.509949803352356\n",
      "epoch 4034, loss 0.012696431018412113, R2 0.47225940227508545\n",
      "Eval loss 0.012722061015665531, R2 0.5099597573280334\n",
      "epoch 4035, loss 0.012696195393800735, R2 0.4722691774368286\n",
      "Eval loss 0.012721807695925236, R2 0.5099691152572632\n",
      "epoch 4036, loss 0.012695959769189358, R2 0.4722789525985718\n",
      "Eval loss 0.012721553444862366, R2 0.5099791288375854\n",
      "epoch 4037, loss 0.012695726938545704, R2 0.47228866815567017\n",
      "Eval loss 0.01272130012512207, R2 0.5099889039993286\n",
      "epoch 4038, loss 0.0126954922452569, R2 0.47229862213134766\n",
      "Eval loss 0.01272104773670435, R2 0.5099983811378479\n",
      "epoch 4039, loss 0.012695257551968098, R2 0.4723082184791565\n",
      "Eval loss 0.012720792554318905, R2 0.5100082159042358\n",
      "epoch 4040, loss 0.012695022858679295, R2 0.4723181128501892\n",
      "Eval loss 0.012720540165901184, R2 0.5100183486938477\n",
      "epoch 4041, loss 0.012694788165390491, R2 0.47232770919799805\n",
      "Eval loss 0.012720287777483463, R2 0.5100282430648804\n",
      "epoch 4042, loss 0.012694553472101688, R2 0.47233760356903076\n",
      "Eval loss 0.012720032595098019, R2 0.5100380182266235\n",
      "epoch 4043, loss 0.01269431971013546, R2 0.4723471403121948\n",
      "Eval loss 0.012719781138002872, R2 0.510047435760498\n",
      "epoch 4044, loss 0.012694085948169231, R2 0.47235751152038574\n",
      "Eval loss 0.012719528749585152, R2 0.5100573301315308\n",
      "epoch 4045, loss 0.012693851254880428, R2 0.4723671078681946\n",
      "Eval loss 0.012719275429844856, R2 0.5100669264793396\n",
      "epoch 4046, loss 0.012693618424236774, R2 0.4723764657974243\n",
      "Eval loss 0.01271902397274971, R2 0.5100762844085693\n",
      "epoch 4047, loss 0.012693383730947971, R2 0.47238606214523315\n",
      "Eval loss 0.01271877158433199, R2 0.5100860595703125\n",
      "epoch 4048, loss 0.012693151831626892, R2 0.47239571809768677\n",
      "Eval loss 0.012718519195914268, R2 0.5100960731506348\n",
      "epoch 4049, loss 0.012692917138338089, R2 0.47240549325942993\n",
      "Eval loss 0.012718267738819122, R2 0.5101057291030884\n",
      "epoch 4050, loss 0.01269268523901701, R2 0.4724152684211731\n",
      "Eval loss 0.012718015350401402, R2 0.5101152658462524\n",
      "epoch 4051, loss 0.012692452408373356, R2 0.4724249243736267\n",
      "Eval loss 0.012717763893306255, R2 0.5101249814033508\n",
      "epoch 4052, loss 0.012692219577729702, R2 0.47243446111679077\n",
      "Eval loss 0.012717514298856258, R2 0.510134756565094\n",
      "epoch 4053, loss 0.012691986747086048, R2 0.4724447727203369\n",
      "Eval loss 0.012717261910438538, R2 0.5101444721221924\n",
      "epoch 4054, loss 0.012691753916442394, R2 0.4724539518356323\n",
      "Eval loss 0.012717010453343391, R2 0.5101538896560669\n",
      "epoch 4055, loss 0.012691522017121315, R2 0.47246360778808594\n",
      "Eval loss 0.012716760858893394, R2 0.5101634860038757\n",
      "epoch 4056, loss 0.012691290117800236, R2 0.4724739193916321\n",
      "Eval loss 0.012716510333120823, R2 0.5101734399795532\n",
      "epoch 4057, loss 0.012691057287156582, R2 0.47248274087905884\n",
      "Eval loss 0.012716258876025677, R2 0.5101833939552307\n",
      "epoch 4058, loss 0.012690825387835503, R2 0.4724922180175781\n",
      "Eval loss 0.01271600928157568, R2 0.5101927518844604\n",
      "epoch 4059, loss 0.012690593488514423, R2 0.47250205278396606\n",
      "Eval loss 0.012715759687125683, R2 0.5102023482322693\n",
      "epoch 4060, loss 0.012690362520515919, R2 0.4725121259689331\n",
      "Eval loss 0.012715510092675686, R2 0.5102119445800781\n",
      "epoch 4061, loss 0.012690131552517414, R2 0.47252142429351807\n",
      "Eval loss 0.012715259566903114, R2 0.5102216005325317\n",
      "epoch 4062, loss 0.012689899653196335, R2 0.47253090143203735\n",
      "Eval loss 0.012715009041130543, R2 0.5102313756942749\n",
      "epoch 4063, loss 0.01268966868519783, R2 0.4725406765937805\n",
      "Eval loss 0.012714759446680546, R2 0.5102411508560181\n",
      "epoch 4064, loss 0.012689437717199326, R2 0.47255009412765503\n",
      "Eval loss 0.012714509852230549, R2 0.5102506279945374\n",
      "epoch 4065, loss 0.012689205817878246, R2 0.47255969047546387\n",
      "Eval loss 0.012714260257780552, R2 0.5102598071098328\n",
      "epoch 4066, loss 0.012688975781202316, R2 0.4725698232650757\n",
      "Eval loss 0.012714012525975704, R2 0.5102694034576416\n",
      "epoch 4067, loss 0.012688745744526386, R2 0.4725790023803711\n",
      "Eval loss 0.012713762000203133, R2 0.5102794170379639\n",
      "epoch 4068, loss 0.012688514776527882, R2 0.47258859872817993\n",
      "Eval loss 0.012713514268398285, R2 0.5102885365486145\n",
      "epoch 4069, loss 0.012688283808529377, R2 0.4725983738899231\n",
      "Eval loss 0.012713265605270863, R2 0.5102983713150024\n",
      "epoch 4070, loss 0.012688053771853447, R2 0.47260743379592896\n",
      "Eval loss 0.01271301694214344, R2 0.510308027267456\n",
      "epoch 4071, loss 0.012687823735177517, R2 0.47261786460876465\n",
      "Eval loss 0.012712768279016018, R2 0.5103175640106201\n",
      "epoch 4072, loss 0.012687594629824162, R2 0.4726272225379944\n",
      "Eval loss 0.012712518684566021, R2 0.5103268623352051\n",
      "epoch 4073, loss 0.012687364593148232, R2 0.4726364016532898\n",
      "Eval loss 0.012712271884083748, R2 0.5103368759155273\n",
      "epoch 4074, loss 0.012687135487794876, R2 0.47264575958251953\n",
      "Eval loss 0.012712025083601475, R2 0.5103460550308228\n",
      "epoch 4075, loss 0.012686904519796371, R2 0.4726558327674866\n",
      "Eval loss 0.012711775489151478, R2 0.5103554725646973\n",
      "epoch 4076, loss 0.012686677277088165, R2 0.47266483306884766\n",
      "Eval loss 0.012711528688669205, R2 0.5103650093078613\n",
      "epoch 4077, loss 0.012686445377767086, R2 0.47267448902130127\n",
      "Eval loss 0.012711282819509506, R2 0.510374903678894\n",
      "epoch 4078, loss 0.01268621813505888, R2 0.4726839065551758\n",
      "Eval loss 0.012711034156382084, R2 0.5103843808174133\n",
      "epoch 4079, loss 0.012685989961028099, R2 0.4726935625076294\n",
      "Eval loss 0.012710786424577236, R2 0.5103936195373535\n",
      "epoch 4080, loss 0.012685760855674744, R2 0.4727034568786621\n",
      "Eval loss 0.012710539624094963, R2 0.5104035139083862\n",
      "epoch 4081, loss 0.012685531750321388, R2 0.4727124571800232\n",
      "Eval loss 0.01271029282361269, R2 0.5104130506515503\n",
      "epoch 4082, loss 0.012685302644968033, R2 0.47272181510925293\n",
      "Eval loss 0.012710046023130417, R2 0.5104227066040039\n",
      "epoch 4083, loss 0.012685074470937252, R2 0.47273141145706177\n",
      "Eval loss 0.012709801085293293, R2 0.5104317665100098\n",
      "epoch 4084, loss 0.012684847228229046, R2 0.47274142503738403\n",
      "Eval loss 0.01270955428481102, R2 0.5104413628578186\n",
      "epoch 4085, loss 0.01268461998552084, R2 0.47275036573410034\n",
      "Eval loss 0.012709307484328747, R2 0.5104506015777588\n",
      "epoch 4086, loss 0.012684390880167484, R2 0.4727596640586853\n",
      "Eval loss 0.012709061615169048, R2 0.5104600191116333\n",
      "epoch 4087, loss 0.012684161774814129, R2 0.4727698564529419\n",
      "Eval loss 0.012708814814686775, R2 0.5104695558547974\n",
      "epoch 4088, loss 0.012683934532105923, R2 0.4727793335914612\n",
      "Eval loss 0.012708570808172226, R2 0.5104790925979614\n",
      "epoch 4089, loss 0.012683708220720291, R2 0.4727882146835327\n",
      "Eval loss 0.012708324007689953, R2 0.5104886293411255\n",
      "epoch 4090, loss 0.01268348004668951, R2 0.472798228263855\n",
      "Eval loss 0.012708080001175404, R2 0.5104981660842896\n",
      "epoch 4091, loss 0.012683254666626453, R2 0.47280776500701904\n",
      "Eval loss 0.012707834132015705, R2 0.510507345199585\n",
      "epoch 4092, loss 0.012683026492595673, R2 0.47281622886657715\n",
      "Eval loss 0.012707589194178581, R2 0.510516881942749\n",
      "epoch 4093, loss 0.012682800181210041, R2 0.47282665967941284\n",
      "Eval loss 0.012707342393696308, R2 0.510526716709137\n",
      "epoch 4094, loss 0.012682572938501835, R2 0.4728352427482605\n",
      "Eval loss 0.012707098387181759, R2 0.510536253452301\n",
      "epoch 4095, loss 0.012682346627116203, R2 0.472845196723938\n",
      "Eval loss 0.01270685251802206, R2 0.510545551776886\n",
      "epoch 4096, loss 0.012682119384407997, R2 0.47285425662994385\n",
      "Eval loss 0.012706609442830086, R2 0.5105547904968262\n",
      "epoch 4097, loss 0.012681892141699791, R2 0.4728638529777527\n",
      "Eval loss 0.012706364504992962, R2 0.5105645060539246\n",
      "epoch 4098, loss 0.012681667692959309, R2 0.4728737473487854\n",
      "Eval loss 0.012706119567155838, R2 0.5105733871459961\n",
      "epoch 4099, loss 0.012681441381573677, R2 0.47288262844085693\n",
      "Eval loss 0.012705877423286438, R2 0.5105831623077393\n",
      "epoch 4100, loss 0.012681215070188046, R2 0.4728918671607971\n",
      "Eval loss 0.012705632485449314, R2 0.5105924606323242\n",
      "epoch 4101, loss 0.012680989690124989, R2 0.4729014039039612\n",
      "Eval loss 0.01270538941025734, R2 0.5106019377708435\n",
      "epoch 4102, loss 0.012680763378739357, R2 0.4729114770889282\n",
      "Eval loss 0.012705144472420216, R2 0.5106111764907837\n",
      "epoch 4103, loss 0.0126805379986763, R2 0.47291964292526245\n",
      "Eval loss 0.012704900465905666, R2 0.5106207132339478\n",
      "epoch 4104, loss 0.012680313549935818, R2 0.47292935848236084\n",
      "Eval loss 0.012704658322036266, R2 0.5106299519538879\n",
      "epoch 4105, loss 0.01268008816987276, R2 0.4729387164115906\n",
      "Eval loss 0.012704415246844292, R2 0.5106393098831177\n",
      "epoch 4106, loss 0.01267986185848713, R2 0.47294825315475464\n",
      "Eval loss 0.012704173102974892, R2 0.510648787021637\n",
      "epoch 4107, loss 0.012679638341069221, R2 0.47295743227005005\n",
      "Eval loss 0.012703930027782917, R2 0.5106580257415771\n",
      "epoch 4108, loss 0.012679412961006165, R2 0.4729667901992798\n",
      "Eval loss 0.012703686021268368, R2 0.5106676816940308\n",
      "epoch 4109, loss 0.012679187580943108, R2 0.47297632694244385\n",
      "Eval loss 0.012703443877398968, R2 0.5106767416000366\n",
      "epoch 4110, loss 0.0126789640635252, R2 0.4729854464530945\n",
      "Eval loss 0.012703201733529568, R2 0.5106856822967529\n",
      "epoch 4111, loss 0.012678738683462143, R2 0.4729952812194824\n",
      "Eval loss 0.012702958658337593, R2 0.5106953978538513\n",
      "epoch 4112, loss 0.01267851423472166, R2 0.4730041027069092\n",
      "Eval loss 0.012702715583145618, R2 0.510704517364502\n",
      "epoch 4113, loss 0.012678289785981178, R2 0.47301363945007324\n",
      "Eval loss 0.012702475301921368, R2 0.510714054107666\n",
      "epoch 4114, loss 0.01267806626856327, R2 0.4730232357978821\n",
      "Eval loss 0.012702234089374542, R2 0.510723352432251\n",
      "epoch 4115, loss 0.012677843682467937, R2 0.4730326533317566\n",
      "Eval loss 0.012701991945505142, R2 0.5107323527336121\n",
      "epoch 4116, loss 0.01267762016505003, R2 0.473041296005249\n",
      "Eval loss 0.012701748870313168, R2 0.5107418894767761\n",
      "epoch 4117, loss 0.012677395716309547, R2 0.4730507731437683\n",
      "Eval loss 0.012701508589088917, R2 0.5107512474060059\n",
      "epoch 4118, loss 0.01267717033624649, R2 0.4730599522590637\n",
      "Eval loss 0.012701267376542091, R2 0.5107604265213013\n",
      "epoch 4119, loss 0.012676949612796307, R2 0.47307002544403076\n",
      "Eval loss 0.01270102709531784, R2 0.5107697248458862\n",
      "epoch 4120, loss 0.01267672423273325, R2 0.47307854890823364\n",
      "Eval loss 0.012700785882771015, R2 0.5107792615890503\n",
      "epoch 4121, loss 0.012676502577960491, R2 0.47308844327926636\n",
      "Eval loss 0.01270054467022419, R2 0.5107886791229248\n",
      "epoch 4122, loss 0.012676279991865158, R2 0.4730975031852722\n",
      "Eval loss 0.012700303457677364, R2 0.5107977390289307\n",
      "epoch 4123, loss 0.01267605647444725, R2 0.4731068015098572\n",
      "Eval loss 0.012700064107775688, R2 0.5108069181442261\n",
      "epoch 4124, loss 0.012675835750997066, R2 0.47311562299728394\n",
      "Eval loss 0.012699823826551437, R2 0.5108157396316528\n",
      "epoch 4125, loss 0.012675612233579159, R2 0.4731252193450928\n",
      "Eval loss 0.012699582614004612, R2 0.5108255743980408\n",
      "epoch 4126, loss 0.012675389647483826, R2 0.47313469648361206\n",
      "Eval loss 0.012699342332780361, R2 0.5108344554901123\n",
      "epoch 4127, loss 0.012675167061388493, R2 0.4731437563896179\n",
      "Eval loss 0.01269910205155611, R2 0.5108439922332764\n",
      "epoch 4128, loss 0.012674945406615734, R2 0.47315263748168945\n",
      "Eval loss 0.012698863632977009, R2 0.5108529925346375\n",
      "epoch 4129, loss 0.012674723751842976, R2 0.4731616973876953\n",
      "Eval loss 0.012698623351752758, R2 0.5108621120452881\n",
      "epoch 4130, loss 0.012674502097070217, R2 0.4731708765029907\n",
      "Eval loss 0.012698384001851082, R2 0.5108712911605835\n",
      "epoch 4131, loss 0.012674280442297459, R2 0.4731806516647339\n",
      "Eval loss 0.012698144651949406, R2 0.5108808279037476\n",
      "epoch 4132, loss 0.0126740587875247, R2 0.4731898307800293\n",
      "Eval loss 0.012697904370725155, R2 0.5108899474143982\n",
      "epoch 4133, loss 0.012673838064074516, R2 0.47319865226745605\n",
      "Eval loss 0.012697666883468628, R2 0.5108990669250488\n",
      "epoch 4134, loss 0.012673615477979183, R2 0.47320806980133057\n",
      "Eval loss 0.012697427533566952, R2 0.5109084844589233\n",
      "epoch 4135, loss 0.012673394754529, R2 0.4732174277305603\n",
      "Eval loss 0.01269718911498785, R2 0.5109176635742188\n",
      "epoch 4136, loss 0.01267317309975624, R2 0.47322630882263184\n",
      "Eval loss 0.012696950696408749, R2 0.5109270811080933\n",
      "epoch 4137, loss 0.012672952376306057, R2 0.47323548793792725\n",
      "Eval loss 0.012696711346507072, R2 0.5109357833862305\n",
      "epoch 4138, loss 0.012672732584178448, R2 0.47324442863464355\n",
      "Eval loss 0.012696473859250546, R2 0.5109454989433289\n",
      "epoch 4139, loss 0.012672511860728264, R2 0.4732539653778076\n",
      "Eval loss 0.012696235440671444, R2 0.5109543800354004\n",
      "epoch 4140, loss 0.01267229113727808, R2 0.4732627868652344\n",
      "Eval loss 0.012695997953414917, R2 0.5109635591506958\n",
      "epoch 4141, loss 0.012672072276473045, R2 0.47327208518981934\n",
      "Eval loss 0.01269576046615839, R2 0.5109729766845703\n",
      "epoch 4142, loss 0.012671851553022861, R2 0.4732810854911804\n",
      "Eval loss 0.012695522978901863, R2 0.5109821557998657\n",
      "epoch 4143, loss 0.012671630829572678, R2 0.47329026460647583\n",
      "Eval loss 0.012695284560322762, R2 0.5109908580780029\n",
      "epoch 4144, loss 0.012671411968767643, R2 0.47329938411712646\n",
      "Eval loss 0.012695047073066235, R2 0.5110000371932983\n",
      "epoch 4145, loss 0.012671191245317459, R2 0.4733085036277771\n",
      "Eval loss 0.012694809585809708, R2 0.511009156703949\n",
      "epoch 4146, loss 0.012670972384512424, R2 0.4733174443244934\n",
      "Eval loss 0.012694573029875755, R2 0.5110179781913757\n",
      "epoch 4147, loss 0.01267075352370739, R2 0.47332704067230225\n",
      "Eval loss 0.012694334611296654, R2 0.511027455329895\n",
      "epoch 4148, loss 0.012670531868934631, R2 0.4733359217643738\n",
      "Eval loss 0.012694098986685276, R2 0.5110363960266113\n",
      "epoch 4149, loss 0.012670313008129597, R2 0.47334522008895874\n",
      "Eval loss 0.012693861499428749, R2 0.5110458135604858\n",
      "epoch 4150, loss 0.012670095078647137, R2 0.4733540415763855\n",
      "Eval loss 0.012693625874817371, R2 0.5110547542572021\n",
      "epoch 4151, loss 0.012669877149164677, R2 0.47336363792419434\n",
      "Eval loss 0.012693390250205994, R2 0.5110636949539185\n",
      "epoch 4152, loss 0.012669656425714493, R2 0.47337281703948975\n",
      "Eval loss 0.012693153694272041, R2 0.5110734105110168\n",
      "epoch 4153, loss 0.012669439427554607, R2 0.47338151931762695\n",
      "Eval loss 0.012692918069660664, R2 0.5110822916030884\n",
      "epoch 4154, loss 0.012669219635426998, R2 0.47339046001434326\n",
      "Eval loss 0.012692681513726711, R2 0.5110914707183838\n",
      "epoch 4155, loss 0.012669001705944538, R2 0.47339969873428345\n",
      "Eval loss 0.012692445889115334, R2 0.5111000537872314\n",
      "epoch 4156, loss 0.012668782845139503, R2 0.4734087586402893\n",
      "Eval loss 0.01269221119582653, R2 0.5111094117164612\n",
      "epoch 4157, loss 0.012668565846979618, R2 0.4734181761741638\n",
      "Eval loss 0.012691973708570004, R2 0.5111182928085327\n",
      "epoch 4158, loss 0.012668347917497158, R2 0.47342735528945923\n",
      "Eval loss 0.012691739946603775, R2 0.5111275911331177\n",
      "epoch 4159, loss 0.012668129988014698, R2 0.4734359383583069\n",
      "Eval loss 0.012691504321992397, R2 0.5111366510391235\n",
      "epoch 4160, loss 0.012667911127209663, R2 0.4734448194503784\n",
      "Eval loss 0.01269126869738102, R2 0.5111455917358398\n",
      "epoch 4161, loss 0.012667695060372353, R2 0.4734543561935425\n",
      "Eval loss 0.012691034935414791, R2 0.5111550092697144\n",
      "epoch 4162, loss 0.012667476199567318, R2 0.4734627604484558\n",
      "Eval loss 0.012690799310803413, R2 0.5111639499664307\n",
      "epoch 4163, loss 0.012667259201407433, R2 0.4734719395637512\n",
      "Eval loss 0.012690562754869461, R2 0.5111725926399231\n",
      "epoch 4164, loss 0.012667042203247547, R2 0.4734809994697571\n",
      "Eval loss 0.012690330855548382, R2 0.5111815929412842\n",
      "epoch 4165, loss 0.012666825205087662, R2 0.47348999977111816\n",
      "Eval loss 0.012690095230937004, R2 0.5111910700798035\n",
      "epoch 4166, loss 0.012666609138250351, R2 0.4734991788864136\n",
      "Eval loss 0.012689861468970776, R2 0.5111995935440063\n",
      "epoch 4167, loss 0.012666391208767891, R2 0.473508358001709\n",
      "Eval loss 0.012689628638327122, R2 0.5112087726593018\n",
      "epoch 4168, loss 0.01266617514193058, R2 0.47351735830307007\n",
      "Eval loss 0.012689394876360893, R2 0.5112180113792419\n",
      "epoch 4169, loss 0.012665958143770695, R2 0.4735260009765625\n",
      "Eval loss 0.012689161114394665, R2 0.511227011680603\n",
      "epoch 4170, loss 0.012665742076933384, R2 0.47353553771972656\n",
      "Eval loss 0.012688925489783287, R2 0.5112358331680298\n",
      "epoch 4171, loss 0.012665525078773499, R2 0.473544180393219\n",
      "Eval loss 0.012688691727817059, R2 0.5112449526786804\n",
      "epoch 4172, loss 0.012665309943258762, R2 0.47355347871780396\n",
      "Eval loss 0.01268845982849598, R2 0.5112535953521729\n",
      "epoch 4173, loss 0.012665093876421452, R2 0.47356247901916504\n",
      "Eval loss 0.01268822606652975, R2 0.5112627744674683\n",
      "epoch 4174, loss 0.012664878740906715, R2 0.47357141971588135\n",
      "Eval loss 0.012687992304563522, R2 0.5112719535827637\n",
      "epoch 4175, loss 0.01266466174274683, R2 0.4735795855522156\n",
      "Eval loss 0.012687761336565018, R2 0.5112809538841248\n",
      "epoch 4176, loss 0.012664446607232094, R2 0.47358936071395874\n",
      "Eval loss 0.01268752757459879, R2 0.5112894773483276\n",
      "epoch 4177, loss 0.012664231471717358, R2 0.47359782457351685\n",
      "Eval loss 0.012687294743955135, R2 0.5112987756729126\n",
      "epoch 4178, loss 0.012664015404880047, R2 0.47360676527023315\n",
      "Eval loss 0.01268706377595663, R2 0.5113076567649841\n",
      "epoch 4179, loss 0.01266380026936531, R2 0.4736158847808838\n",
      "Eval loss 0.012686830945312977, R2 0.5113167762756348\n",
      "epoch 4180, loss 0.012663586996495724, R2 0.473624587059021\n",
      "Eval loss 0.012686599045991898, R2 0.5113255977630615\n",
      "epoch 4181, loss 0.012663371860980988, R2 0.47363370656967163\n",
      "Eval loss 0.012686366215348244, R2 0.5113346576690674\n",
      "epoch 4182, loss 0.012663156725466251, R2 0.47364264726638794\n",
      "Eval loss 0.012686135247349739, R2 0.5113433003425598\n",
      "epoch 4183, loss 0.012662941589951515, R2 0.4736512303352356\n",
      "Eval loss 0.01268590148538351, R2 0.5113524198532104\n",
      "epoch 4184, loss 0.012662728317081928, R2 0.47366029024124146\n",
      "Eval loss 0.012685670517385006, R2 0.5113610029220581\n",
      "epoch 4185, loss 0.012662512250244617, R2 0.4736696481704712\n",
      "Eval loss 0.012685439549386501, R2 0.5113702416419983\n",
      "epoch 4186, loss 0.01266229897737503, R2 0.4736787676811218\n",
      "Eval loss 0.012685208581387997, R2 0.5113791227340698\n",
      "epoch 4187, loss 0.012662085704505444, R2 0.4736875295639038\n",
      "Eval loss 0.012684977613389492, R2 0.5113880634307861\n",
      "epoch 4188, loss 0.012661871500313282, R2 0.47369587421417236\n",
      "Eval loss 0.012684746645390987, R2 0.5113972425460815\n",
      "epoch 4189, loss 0.01266165729612112, R2 0.47370481491088867\n",
      "Eval loss 0.012684514746069908, R2 0.5114058256149292\n",
      "epoch 4190, loss 0.012661444954574108, R2 0.47371381521224976\n",
      "Eval loss 0.012684284709393978, R2 0.5114147067070007\n",
      "epoch 4191, loss 0.012661228887736797, R2 0.47372329235076904\n",
      "Eval loss 0.012684054672718048, R2 0.5114235877990723\n",
      "epoch 4192, loss 0.012661016546189785, R2 0.4737319350242615\n",
      "Eval loss 0.012683822773396969, R2 0.511432409286499\n",
      "epoch 4193, loss 0.012660804204642773, R2 0.47374075651168823\n",
      "Eval loss 0.012683592736721039, R2 0.5114413499832153\n",
      "epoch 4194, loss 0.012660590000450611, R2 0.47374916076660156\n",
      "Eval loss 0.012683362700045109, R2 0.511449933052063\n",
      "epoch 4195, loss 0.012660376727581024, R2 0.4737586975097656\n",
      "Eval loss 0.012683134526014328, R2 0.5114593505859375\n",
      "epoch 4196, loss 0.012660163454711437, R2 0.47376739978790283\n",
      "Eval loss 0.012682903558015823, R2 0.5114680528640747\n",
      "epoch 4197, loss 0.012659951113164425, R2 0.47377586364746094\n",
      "Eval loss 0.012682672590017319, R2 0.5114765167236328\n",
      "epoch 4198, loss 0.012659738771617413, R2 0.47378504276275635\n",
      "Eval loss 0.012682443484663963, R2 0.5114854574203491\n",
      "epoch 4199, loss 0.0126595264300704, R2 0.4737938642501831\n",
      "Eval loss 0.012682213447988033, R2 0.5114941596984863\n",
      "epoch 4200, loss 0.012659314088523388, R2 0.47380268573760986\n",
      "Eval loss 0.012681986205279827, R2 0.5115029811859131\n",
      "epoch 4201, loss 0.012659100815653801, R2 0.4738110899925232\n",
      "Eval loss 0.012681756168603897, R2 0.5115119814872742\n",
      "epoch 4202, loss 0.012658888474106789, R2 0.4738200306892395\n",
      "Eval loss 0.012681527063250542, R2 0.5115206241607666\n",
      "epoch 4203, loss 0.012658677063882351, R2 0.47382885217666626\n",
      "Eval loss 0.012681297026574612, R2 0.5115299224853516\n",
      "epoch 4204, loss 0.012658466584980488, R2 0.4738374352455139\n",
      "Eval loss 0.012681069783866405, R2 0.5115388631820679\n",
      "epoch 4205, loss 0.012658254243433475, R2 0.473846435546875\n",
      "Eval loss 0.01268084067851305, R2 0.5115473866462708\n",
      "epoch 4206, loss 0.012658041901886463, R2 0.47385525703430176\n",
      "Eval loss 0.01268061250448227, R2 0.5115562677383423\n",
      "epoch 4207, loss 0.0126578314229846, R2 0.4738638401031494\n",
      "Eval loss 0.012680384330451488, R2 0.5115646719932556\n",
      "epoch 4208, loss 0.012657619081437588, R2 0.47387295961380005\n",
      "Eval loss 0.012680154293775558, R2 0.5115739703178406\n",
      "epoch 4209, loss 0.01265740767121315, R2 0.4738816022872925\n",
      "Eval loss 0.012679927051067352, R2 0.5115824341773987\n",
      "epoch 4210, loss 0.012657198123633862, R2 0.47389066219329834\n",
      "Eval loss 0.012679699808359146, R2 0.5115913152694702\n",
      "epoch 4211, loss 0.012656984850764275, R2 0.4738994836807251\n",
      "Eval loss 0.012679471634328365, R2 0.511600136756897\n",
      "epoch 4212, loss 0.01265677623450756, R2 0.4739081859588623\n",
      "Eval loss 0.01267924439162016, R2 0.5116087198257446\n",
      "epoch 4213, loss 0.012656564824283123, R2 0.4739166498184204\n",
      "Eval loss 0.012679017148911953, R2 0.5116177797317505\n",
      "epoch 4214, loss 0.01265635434538126, R2 0.4739258885383606\n",
      "Eval loss 0.012678788043558598, R2 0.5116260051727295\n",
      "epoch 4215, loss 0.012656143866479397, R2 0.4739341139793396\n",
      "Eval loss 0.012678561732172966, R2 0.5116353034973145\n",
      "epoch 4216, loss 0.012655933387577534, R2 0.47394269704818726\n",
      "Eval loss 0.012678335420787334, R2 0.511644184589386\n",
      "epoch 4217, loss 0.012655723839998245, R2 0.47395145893096924\n",
      "Eval loss 0.012678108178079128, R2 0.5116526484489441\n",
      "epoch 4218, loss 0.012655514292418957, R2 0.47396010160446167\n",
      "Eval loss 0.012677880935370922, R2 0.5116614103317261\n",
      "epoch 4219, loss 0.012655303813517094, R2 0.47396957874298096\n",
      "Eval loss 0.01267765462398529, R2 0.5116699934005737\n",
      "epoch 4220, loss 0.01265509333461523, R2 0.47397780418395996\n",
      "Eval loss 0.012677428312599659, R2 0.5116786956787109\n",
      "epoch 4221, loss 0.012654884718358517, R2 0.47398632764816284\n",
      "Eval loss 0.012677202932536602, R2 0.5116873383522034\n",
      "epoch 4222, loss 0.012654675170779228, R2 0.4739956855773926\n",
      "Eval loss 0.01267697662115097, R2 0.5116962194442749\n",
      "epoch 4223, loss 0.01265446562319994, R2 0.4740040898323059\n",
      "Eval loss 0.012676749378442764, R2 0.5117049813270569\n",
      "epoch 4224, loss 0.012654256075620651, R2 0.4740126132965088\n",
      "Eval loss 0.012676523067057133, R2 0.5117133855819702\n",
      "epoch 4225, loss 0.012654048390686512, R2 0.4740210771560669\n",
      "Eval loss 0.012676296755671501, R2 0.5117223858833313\n",
      "epoch 4226, loss 0.012653837911784649, R2 0.4740298390388489\n",
      "Eval loss 0.012676071375608444, R2 0.5117307901382446\n",
      "epoch 4227, loss 0.01265363022685051, R2 0.47403860092163086\n",
      "Eval loss 0.012675845995545387, R2 0.5117394924163818\n",
      "epoch 4228, loss 0.012653420679271221, R2 0.47404736280441284\n",
      "Eval loss 0.01267562061548233, R2 0.5117484331130981\n",
      "epoch 4229, loss 0.012653211131691933, R2 0.4740560054779053\n",
      "Eval loss 0.012675396166741848, R2 0.5117567777633667\n",
      "epoch 4230, loss 0.012653004378080368, R2 0.4740641117095947\n",
      "Eval loss 0.012675170786678791, R2 0.5117654800415039\n",
      "epoch 4231, loss 0.012652793899178505, R2 0.47407370805740356\n",
      "Eval loss 0.012674945406615734, R2 0.5117744207382202\n",
      "epoch 4232, loss 0.012652586214244366, R2 0.4740825295448303\n",
      "Eval loss 0.012674720957875252, R2 0.5117830038070679\n",
      "epoch 4233, loss 0.012652378529310226, R2 0.4740905165672302\n",
      "Eval loss 0.012674497440457344, R2 0.5117915868759155\n",
      "epoch 4234, loss 0.012652170844376087, R2 0.4740992784500122\n",
      "Eval loss 0.012674270197749138, R2 0.5118004679679871\n",
      "epoch 4235, loss 0.012651964090764523, R2 0.4741085171699524\n",
      "Eval loss 0.01267404854297638, R2 0.5118092894554138\n",
      "epoch 4236, loss 0.012651755474507809, R2 0.47411638498306274\n",
      "Eval loss 0.012673824094235897, R2 0.5118173360824585\n",
      "epoch 4237, loss 0.012651548720896244, R2 0.4741249680519104\n",
      "Eval loss 0.012673599645495415, R2 0.5118262767791748\n",
      "epoch 4238, loss 0.01265134010463953, R2 0.47413432598114014\n",
      "Eval loss 0.012673375196754932, R2 0.5118352174758911\n",
      "epoch 4239, loss 0.012651133351027966, R2 0.4741429090499878\n",
      "Eval loss 0.01267315074801445, R2 0.5118435621261597\n",
      "epoch 4240, loss 0.012650925666093826, R2 0.4741508960723877\n",
      "Eval loss 0.012672927230596542, R2 0.5118520259857178\n",
      "epoch 4241, loss 0.012650719843804836, R2 0.4741594195365906\n",
      "Eval loss 0.01267270464450121, R2 0.511860728263855\n",
      "epoch 4242, loss 0.012650512158870697, R2 0.47416824102401733\n",
      "Eval loss 0.012672480195760727, R2 0.5118690729141235\n",
      "epoch 4243, loss 0.012650304473936558, R2 0.4741763472557068\n",
      "Eval loss 0.012672257609665394, R2 0.511877715587616\n",
      "epoch 4244, loss 0.012650098651647568, R2 0.4741852283477783\n",
      "Eval loss 0.012672033160924911, R2 0.511886477470398\n",
      "epoch 4245, loss 0.012649891898036003, R2 0.4741939902305603\n",
      "Eval loss 0.012671811506152153, R2 0.5118950605392456\n",
      "epoch 4246, loss 0.012649686075747013, R2 0.47420239448547363\n",
      "Eval loss 0.012671587988734245, R2 0.5119039416313171\n",
      "epoch 4247, loss 0.012649479322135448, R2 0.47421085834503174\n",
      "Eval loss 0.012671365402638912, R2 0.51191246509552\n",
      "epoch 4248, loss 0.012649272568523884, R2 0.47421956062316895\n",
      "Eval loss 0.012671143747866154, R2 0.5119209289550781\n",
      "epoch 4249, loss 0.01264906581491232, R2 0.4742282032966614\n",
      "Eval loss 0.012670919299125671, R2 0.5119295120239258\n",
      "epoch 4250, loss 0.012648860923945904, R2 0.4742368459701538\n",
      "Eval loss 0.012670698575675488, R2 0.5119377374649048\n",
      "epoch 4251, loss 0.012648655101656914, R2 0.47424525022506714\n",
      "Eval loss 0.012670475989580154, R2 0.5119463205337524\n",
      "epoch 4252, loss 0.012648450210690498, R2 0.47425442934036255\n",
      "Eval loss 0.012670254334807396, R2 0.5119551420211792\n",
      "epoch 4253, loss 0.012648243457078934, R2 0.4742630124092102\n",
      "Eval loss 0.012670032680034637, R2 0.5119637250900269\n",
      "epoch 4254, loss 0.012648038566112518, R2 0.4742710590362549\n",
      "Eval loss 0.012669810093939304, R2 0.5119721293449402\n",
      "epoch 4255, loss 0.012647833675146103, R2 0.47427940368652344\n",
      "Eval loss 0.01266958937048912, R2 0.5119807720184326\n",
      "epoch 4256, loss 0.012647628784179688, R2 0.4742887616157532\n",
      "Eval loss 0.012669366784393787, R2 0.5119894742965698\n",
      "epoch 4257, loss 0.012647423893213272, R2 0.4742964506149292\n",
      "Eval loss 0.012669145129621029, R2 0.5119976997375488\n",
      "epoch 4258, loss 0.012647219002246857, R2 0.4743049740791321\n",
      "Eval loss 0.012668924406170845, R2 0.512006402015686\n",
      "epoch 4259, loss 0.012647014111280441, R2 0.47431349754333496\n",
      "Eval loss 0.012668702751398087, R2 0.5120148658752441\n",
      "epoch 4260, loss 0.012646809220314026, R2 0.47432196140289307\n",
      "Eval loss 0.012668482027947903, R2 0.5120231509208679\n",
      "epoch 4261, loss 0.01264660432934761, R2 0.4743306636810303\n",
      "Eval loss 0.012668260373175144, R2 0.5120317935943604\n",
      "epoch 4262, loss 0.012646399438381195, R2 0.47433900833129883\n",
      "Eval loss 0.012668040581047535, R2 0.5120401382446289\n",
      "epoch 4263, loss 0.012646195478737354, R2 0.47434747219085693\n",
      "Eval loss 0.012667819857597351, R2 0.512048602104187\n",
      "epoch 4264, loss 0.012645991519093513, R2 0.474356472492218\n",
      "Eval loss 0.012667600065469742, R2 0.5120574235916138\n",
      "epoch 4265, loss 0.012645788490772247, R2 0.47436439990997314\n",
      "Eval loss 0.012667378410696983, R2 0.5120657682418823\n",
      "epoch 4266, loss 0.012645584531128407, R2 0.4743730425834656\n",
      "Eval loss 0.012667158618569374, R2 0.5120741128921509\n",
      "epoch 4267, loss 0.012645380571484566, R2 0.47438186407089233\n",
      "Eval loss 0.01266693789511919, R2 0.5120829343795776\n",
      "epoch 4268, loss 0.012645176611840725, R2 0.4743896722793579\n",
      "Eval loss 0.012666719034314156, R2 0.5120912790298462\n",
      "epoch 4269, loss 0.01264497172087431, R2 0.47439849376678467\n",
      "Eval loss 0.012666500173509121, R2 0.5120997428894043\n",
      "epoch 4270, loss 0.012644769623875618, R2 0.47440677881240845\n",
      "Eval loss 0.012666279450058937, R2 0.5121083855628967\n",
      "epoch 4271, loss 0.012644566595554352, R2 0.47441571950912476\n",
      "Eval loss 0.012666060589253902, R2 0.5121168494224548\n",
      "epoch 4272, loss 0.012644363567233086, R2 0.4744236469268799\n",
      "Eval loss 0.012665839865803719, R2 0.5121248960494995\n",
      "epoch 4273, loss 0.01264416053891182, R2 0.47443193197250366\n",
      "Eval loss 0.012665622867643833, R2 0.5121332406997681\n",
      "epoch 4274, loss 0.012643957510590553, R2 0.4744405150413513\n",
      "Eval loss 0.012665401212871075, R2 0.5121422410011292\n",
      "epoch 4275, loss 0.012643753550946712, R2 0.4744489789009094\n",
      "Eval loss 0.012665183283388615, R2 0.5121506452560425\n",
      "epoch 4276, loss 0.01264355331659317, R2 0.47445744276046753\n",
      "Eval loss 0.01266496442258358, R2 0.5121590495109558\n",
      "epoch 4277, loss 0.012643348425626755, R2 0.47446584701538086\n",
      "Eval loss 0.01266474649310112, R2 0.5121679306030273\n",
      "epoch 4278, loss 0.012643146328628063, R2 0.4744742512702942\n",
      "Eval loss 0.012664527632296085, R2 0.5121760368347168\n",
      "epoch 4279, loss 0.012642944231629372, R2 0.47448331117630005\n",
      "Eval loss 0.012664309702813625, R2 0.5121840238571167\n",
      "epoch 4280, loss 0.012642743065953255, R2 0.4744914770126343\n",
      "Eval loss 0.01266409084200859, R2 0.5121927261352539\n",
      "epoch 4281, loss 0.012642540968954563, R2 0.4744994044303894\n",
      "Eval loss 0.01266387291252613, R2 0.5122010707855225\n",
      "epoch 4282, loss 0.012642339803278446, R2 0.47450774908065796\n",
      "Eval loss 0.012663654051721096, R2 0.512209415435791\n",
      "epoch 4283, loss 0.012642137706279755, R2 0.4745163321495056\n",
      "Eval loss 0.01266343705356121, R2 0.5122177600860596\n",
      "epoch 4284, loss 0.012641936540603638, R2 0.47452467679977417\n",
      "Eval loss 0.01266321912407875, R2 0.5122259855270386\n",
      "epoch 4285, loss 0.012641734443604946, R2 0.4745336174964905\n",
      "Eval loss 0.01266300119459629, R2 0.5122343897819519\n",
      "epoch 4286, loss 0.012641532346606255, R2 0.4745413064956665\n",
      "Eval loss 0.01266278326511383, R2 0.5122429132461548\n",
      "epoch 4287, loss 0.012641331180930138, R2 0.47454988956451416\n",
      "Eval loss 0.012662566266953945, R2 0.5122512578964233\n",
      "epoch 4288, loss 0.012641129083931446, R2 0.4745582342147827\n",
      "Eval loss 0.012662348337471485, R2 0.5122596621513367\n",
      "epoch 4289, loss 0.012640928849577904, R2 0.47456640005111694\n",
      "Eval loss 0.012662133201956749, R2 0.5122677087783813\n",
      "epoch 4290, loss 0.012640727683901787, R2 0.47457557916641235\n",
      "Eval loss 0.012661915272474289, R2 0.5122762322425842\n",
      "epoch 4291, loss 0.012640527449548244, R2 0.47458308935165405\n",
      "Eval loss 0.012661698274314404, R2 0.5122843980789185\n",
      "epoch 4292, loss 0.012640327215194702, R2 0.47459137439727783\n",
      "Eval loss 0.012661482207477093, R2 0.5122931003570557\n",
      "epoch 4293, loss 0.01264012698084116, R2 0.4745998978614807\n",
      "Eval loss 0.012661266140639782, R2 0.5123016834259033\n",
      "epoch 4294, loss 0.012639924883842468, R2 0.47460848093032837\n",
      "Eval loss 0.012661049142479897, R2 0.5123099088668823\n",
      "epoch 4295, loss 0.012639724649488926, R2 0.47461646795272827\n",
      "Eval loss 0.012660832144320011, R2 0.5123180747032166\n",
      "epoch 4296, loss 0.012639525346457958, R2 0.4746249318122864\n",
      "Eval loss 0.0126606160774827, R2 0.5123262405395508\n",
      "epoch 4297, loss 0.01263932604342699, R2 0.47463321685791016\n",
      "Eval loss 0.012660399079322815, R2 0.5123347640037537\n",
      "epoch 4298, loss 0.012639125809073448, R2 0.4746418595314026\n",
      "Eval loss 0.012660184875130653, R2 0.5123430490493774\n",
      "epoch 4299, loss 0.012638925574719906, R2 0.47464966773986816\n",
      "Eval loss 0.012659968808293343, R2 0.5123516321182251\n",
      "epoch 4300, loss 0.012638727203011513, R2 0.47465842962265015\n",
      "Eval loss 0.012659752741456032, R2 0.5123600959777832\n",
      "epoch 4301, loss 0.012638527899980545, R2 0.47466617822647095\n",
      "Eval loss 0.01265953853726387, R2 0.5123679637908936\n",
      "epoch 4302, loss 0.012638326734304428, R2 0.4746750593185425\n",
      "Eval loss 0.012659321539103985, R2 0.5123763084411621\n",
      "epoch 4303, loss 0.012638128362596035, R2 0.4746828079223633\n",
      "Eval loss 0.012659108266234398, R2 0.5123840570449829\n",
      "epoch 4304, loss 0.012637928128242493, R2 0.47469133138656616\n",
      "Eval loss 0.012658892199397087, R2 0.5123927593231201\n",
      "epoch 4305, loss 0.0126377297565341, R2 0.4746990203857422\n",
      "Eval loss 0.012658677995204926, R2 0.5124012231826782\n",
      "epoch 4306, loss 0.012637530453503132, R2 0.47470784187316895\n",
      "Eval loss 0.01265846285969019, R2 0.5124093294143677\n",
      "epoch 4307, loss 0.012637332081794739, R2 0.47471606731414795\n",
      "Eval loss 0.012658246792852879, R2 0.5124179124832153\n",
      "epoch 4308, loss 0.012637133710086346, R2 0.4747241735458374\n",
      "Eval loss 0.012658033519983292, R2 0.5124258995056152\n",
      "epoch 4309, loss 0.012636934407055378, R2 0.4747324585914612\n",
      "Eval loss 0.01265781931579113, R2 0.5124341249465942\n",
      "epoch 4310, loss 0.01263673696666956, R2 0.4747406244277954\n",
      "Eval loss 0.012657604180276394, R2 0.5124425888061523\n",
      "epoch 4311, loss 0.012636539526283741, R2 0.47474849224090576\n",
      "Eval loss 0.012657391838729382, R2 0.5124505758285522\n",
      "epoch 4312, loss 0.012636341154575348, R2 0.4747570753097534\n",
      "Eval loss 0.01265717577189207, R2 0.5124589204788208\n",
      "epoch 4313, loss 0.012636140920221806, R2 0.47476524114608765\n",
      "Eval loss 0.01265696156769991, R2 0.5124671459197998\n",
      "epoch 4314, loss 0.012635945342481136, R2 0.4747735261917114\n",
      "Eval loss 0.012656749226152897, R2 0.5124753713607788\n",
      "epoch 4315, loss 0.012635746039450169, R2 0.47478169202804565\n",
      "Eval loss 0.01265653595328331, R2 0.5124835968017578\n",
      "epoch 4316, loss 0.012635549530386925, R2 0.47479069232940674\n",
      "Eval loss 0.012656322680413723, R2 0.5124919414520264\n",
      "epoch 4317, loss 0.012635351158678532, R2 0.47479844093322754\n",
      "Eval loss 0.012656108476221561, R2 0.5124998688697815\n",
      "epoch 4318, loss 0.012635153718292713, R2 0.47480642795562744\n",
      "Eval loss 0.012655895203351974, R2 0.5125085115432739\n",
      "epoch 4319, loss 0.012634956277906895, R2 0.47481465339660645\n",
      "Eval loss 0.012655682861804962, R2 0.5125166177749634\n",
      "epoch 4320, loss 0.012634758837521076, R2 0.47482287883758545\n",
      "Eval loss 0.012655467726290226, R2 0.5125247240066528\n",
      "epoch 4321, loss 0.012634562328457832, R2 0.4748310446739197\n",
      "Eval loss 0.012655255384743214, R2 0.5125325918197632\n",
      "epoch 4322, loss 0.012634364888072014, R2 0.4748397469520569\n",
      "Eval loss 0.012655043043196201, R2 0.5125410556793213\n",
      "epoch 4323, loss 0.012634167447686195, R2 0.4748474359512329\n",
      "Eval loss 0.012654831632971764, R2 0.5125490427017212\n",
      "epoch 4324, loss 0.012633971869945526, R2 0.474855899810791\n",
      "Eval loss 0.012654618360102177, R2 0.5125576257705688\n",
      "epoch 4325, loss 0.012633775360882282, R2 0.4748638868331909\n",
      "Eval loss 0.012654406018555164, R2 0.5125656127929688\n",
      "epoch 4326, loss 0.012633577920496464, R2 0.4748719334602356\n",
      "Eval loss 0.012654193677008152, R2 0.5125739574432373\n",
      "epoch 4327, loss 0.012633382342755795, R2 0.47488075494766235\n",
      "Eval loss 0.01265398133546114, R2 0.5125819444656372\n",
      "epoch 4328, loss 0.012633183971047401, R2 0.47488880157470703\n",
      "Eval loss 0.012653770856559277, R2 0.5125901103019714\n",
      "epoch 4329, loss 0.012632990255951881, R2 0.4748963713645935\n",
      "Eval loss 0.01265355758368969, R2 0.5125982761383057\n",
      "epoch 4330, loss 0.012632792815566063, R2 0.4749050736427307\n",
      "Eval loss 0.012653347104787827, R2 0.5126063823699951\n",
      "epoch 4331, loss 0.012632597237825394, R2 0.47491270303726196\n",
      "Eval loss 0.012653135694563389, R2 0.5126142501831055\n",
      "epoch 4332, loss 0.012632402591407299, R2 0.4749208092689514\n",
      "Eval loss 0.012652925215661526, R2 0.5126226544380188\n",
      "epoch 4333, loss 0.012632207944989204, R2 0.47492891550064087\n",
      "Eval loss 0.012652712874114513, R2 0.512630820274353\n",
      "epoch 4334, loss 0.01263201143592596, R2 0.4749370813369751\n",
      "Eval loss 0.012652501463890076, R2 0.5126386880874634\n",
      "epoch 4335, loss 0.012631816789507866, R2 0.4749454855918884\n",
      "Eval loss 0.012652289122343063, R2 0.5126471519470215\n",
      "epoch 4336, loss 0.012631620280444622, R2 0.474953293800354\n",
      "Eval loss 0.012652079574763775, R2 0.5126550793647766\n",
      "epoch 4337, loss 0.012631424702703953, R2 0.47496145963668823\n",
      "Eval loss 0.012651869095861912, R2 0.5126634836196899\n",
      "epoch 4338, loss 0.012631230056285858, R2 0.47496968507766724\n",
      "Eval loss 0.012651657685637474, R2 0.5126715898513794\n",
      "epoch 4339, loss 0.012631036341190338, R2 0.47497743368148804\n",
      "Eval loss 0.012651447206735611, R2 0.5126798748970032\n",
      "epoch 4340, loss 0.012630841694772243, R2 0.47498589754104614\n",
      "Eval loss 0.012651237659156322, R2 0.5126878023147583\n",
      "epoch 4341, loss 0.012630646117031574, R2 0.4749940037727356\n",
      "Eval loss 0.012651028111577034, R2 0.5126959085464478\n",
      "epoch 4342, loss 0.012630452401936054, R2 0.4750016927719116\n",
      "Eval loss 0.012650816701352596, R2 0.512703537940979\n",
      "epoch 4343, loss 0.01263025775551796, R2 0.4750099778175354\n",
      "Eval loss 0.012650607153773308, R2 0.5127119421958923\n",
      "epoch 4344, loss 0.01263006217777729, R2 0.47501808404922485\n",
      "Eval loss 0.012650398537516594, R2 0.5127202868461609\n",
      "epoch 4345, loss 0.012629869394004345, R2 0.4750262498855591\n",
      "Eval loss 0.012650186195969582, R2 0.5127281546592712\n",
      "epoch 4346, loss 0.012629673816263676, R2 0.47503405809402466\n",
      "Eval loss 0.012649978511035442, R2 0.5127361416816711\n",
      "epoch 4347, loss 0.012629480101168156, R2 0.47504210472106934\n",
      "Eval loss 0.012649767100811005, R2 0.5127444267272949\n",
      "epoch 4348, loss 0.01262928731739521, R2 0.4750509262084961\n",
      "Eval loss 0.012649559415876865, R2 0.5127522945404053\n",
      "epoch 4349, loss 0.01262909360229969, R2 0.4750588536262512\n",
      "Eval loss 0.012649349868297577, R2 0.5127600431442261\n",
      "epoch 4350, loss 0.012628900818526745, R2 0.4750663638114929\n",
      "Eval loss 0.012649141252040863, R2 0.5127683877944946\n",
      "epoch 4351, loss 0.012628707103431225, R2 0.4750744104385376\n",
      "Eval loss 0.012648931704461575, R2 0.5127764940261841\n",
      "epoch 4352, loss 0.012628513388335705, R2 0.4750824570655823\n",
      "Eval loss 0.01264872308820486, R2 0.5127848386764526\n",
      "epoch 4353, loss 0.012628319673240185, R2 0.47509050369262695\n",
      "Eval loss 0.012648514471948147, R2 0.5127925872802734\n",
      "epoch 4354, loss 0.01262812688946724, R2 0.47509902715682983\n",
      "Eval loss 0.012648304924368858, R2 0.5128003358840942\n",
      "epoch 4355, loss 0.012627934105694294, R2 0.47510653734207153\n",
      "Eval loss 0.012648098170757294, R2 0.5128083229064941\n",
      "epoch 4356, loss 0.012627742253243923, R2 0.47511470317840576\n",
      "Eval loss 0.01264788955450058, R2 0.512816309928894\n",
      "epoch 4357, loss 0.012627548538148403, R2 0.4751223921775818\n",
      "Eval loss 0.012647680938243866, R2 0.5128246545791626\n",
      "epoch 4358, loss 0.012627355754375458, R2 0.47513091564178467\n",
      "Eval loss 0.012647474184632301, R2 0.5128323435783386\n",
      "epoch 4359, loss 0.012627162039279938, R2 0.475139319896698\n",
      "Eval loss 0.012647264637053013, R2 0.5128403902053833\n",
      "epoch 4360, loss 0.012626970186829567, R2 0.4751465916633606\n",
      "Eval loss 0.012647056952118874, R2 0.512848973274231\n",
      "epoch 4361, loss 0.012626777403056622, R2 0.4751546382904053\n",
      "Eval loss 0.01264684833586216, R2 0.5128568410873413\n",
      "epoch 4362, loss 0.01262658555060625, R2 0.4751631021499634\n",
      "Eval loss 0.012646641582250595, R2 0.5128649473190308\n",
      "epoch 4363, loss 0.012626394629478455, R2 0.4751710295677185\n",
      "Eval loss 0.01264643482863903, R2 0.5128726959228516\n",
      "epoch 4364, loss 0.012626202777028084, R2 0.4751785397529602\n",
      "Eval loss 0.012646228075027466, R2 0.512880802154541\n",
      "epoch 4365, loss 0.012626010924577713, R2 0.47518646717071533\n",
      "Eval loss 0.012646021321415901, R2 0.5128887891769409\n",
      "epoch 4366, loss 0.012625818140804768, R2 0.47519451379776\n",
      "Eval loss 0.012645811773836613, R2 0.5128965377807617\n",
      "epoch 4367, loss 0.012625626288354397, R2 0.47520262002944946\n",
      "Eval loss 0.012645606882870197, R2 0.5129044055938721\n",
      "epoch 4368, loss 0.01262543722987175, R2 0.47521036863327026\n",
      "Eval loss 0.012645399197936058, R2 0.5129122734069824\n",
      "epoch 4369, loss 0.012625244446098804, R2 0.4752185344696045\n",
      "Eval loss 0.012645193375647068, R2 0.512920618057251\n",
      "epoch 4370, loss 0.012625051662325859, R2 0.4752271771430969\n",
      "Eval loss 0.012644987553358078, R2 0.5129284262657166\n",
      "epoch 4371, loss 0.012624862603843212, R2 0.47523438930511475\n",
      "Eval loss 0.012644779868423939, R2 0.5129362344741821\n",
      "epoch 4372, loss 0.012624670751392841, R2 0.4752423167228699\n",
      "Eval loss 0.012644573114812374, R2 0.5129443407058716\n",
      "epoch 4373, loss 0.01262448076158762, R2 0.475250244140625\n",
      "Eval loss 0.012644367292523384, R2 0.5129523277282715\n",
      "epoch 4374, loss 0.012624289840459824, R2 0.4752587080001831\n",
      "Eval loss 0.012644162401556969, R2 0.5129599571228027\n",
      "epoch 4375, loss 0.012624097988009453, R2 0.4752659797668457\n",
      "Eval loss 0.012643956579267979, R2 0.5129683017730713\n",
      "epoch 4376, loss 0.012623907998204231, R2 0.47527438402175903\n",
      "Eval loss 0.012643749825656414, R2 0.5129760503768921\n",
      "epoch 4377, loss 0.01262371800839901, R2 0.4752819538116455\n",
      "Eval loss 0.01264354307204485, R2 0.5129837393760681\n",
      "epoch 4378, loss 0.012623527087271214, R2 0.4752897024154663\n",
      "Eval loss 0.012643338181078434, R2 0.5129916667938232\n",
      "epoch 4379, loss 0.012623338028788567, R2 0.47529757022857666\n",
      "Eval loss 0.012643133290112019, R2 0.5130000114440918\n",
      "epoch 4380, loss 0.012623148038983345, R2 0.4753054976463318\n",
      "Eval loss 0.012642927467823029, R2 0.513007640838623\n",
      "epoch 4381, loss 0.012622958049178123, R2 0.47531336545944214\n",
      "Eval loss 0.012642723508179188, R2 0.5130155086517334\n",
      "epoch 4382, loss 0.012622768059372902, R2 0.4753219485282898\n",
      "Eval loss 0.012642518617212772, R2 0.5130239129066467\n",
      "epoch 4383, loss 0.012622579000890255, R2 0.4753291606903076\n",
      "Eval loss 0.012642312794923782, R2 0.5130311250686646\n",
      "epoch 4384, loss 0.012622389011085033, R2 0.4753367304801941\n",
      "Eval loss 0.012642106972634792, R2 0.5130391120910645\n",
      "epoch 4385, loss 0.012622199952602386, R2 0.4753448963165283\n",
      "Eval loss 0.012641903944313526, R2 0.5130468606948853\n",
      "epoch 4386, loss 0.012622009962797165, R2 0.47535258531570435\n",
      "Eval loss 0.01264169905334711, R2 0.5130550861358643\n",
      "epoch 4387, loss 0.012621820904314518, R2 0.47536081075668335\n",
      "Eval loss 0.01264149509370327, R2 0.5130628347396851\n",
      "epoch 4388, loss 0.012621632777154446, R2 0.47536832094192505\n",
      "Eval loss 0.012641290202736855, R2 0.5130709409713745\n",
      "epoch 4389, loss 0.012621442787349224, R2 0.4753763675689697\n",
      "Eval loss 0.012641086243093014, R2 0.5130782127380371\n",
      "epoch 4390, loss 0.012621254660189152, R2 0.47538483142852783\n",
      "Eval loss 0.012640882283449173, R2 0.5130867958068848\n",
      "epoch 4391, loss 0.012621065601706505, R2 0.47539204359054565\n",
      "Eval loss 0.012640678323805332, R2 0.5130942463874817\n",
      "epoch 4392, loss 0.012620876543223858, R2 0.475399911403656\n",
      "Eval loss 0.012640475295484066, R2 0.5131023526191711\n",
      "epoch 4393, loss 0.012620687484741211, R2 0.4754078984260559\n",
      "Eval loss 0.012640271335840225, R2 0.5131099224090576\n",
      "epoch 4394, loss 0.012620498426258564, R2 0.4754154682159424\n",
      "Eval loss 0.01264006644487381, R2 0.5131179690361023\n",
      "epoch 4395, loss 0.012620311230421066, R2 0.4754233956336975\n",
      "Eval loss 0.012639864347875118, R2 0.5131253004074097\n",
      "epoch 4396, loss 0.012620123103260994, R2 0.4754309058189392\n",
      "Eval loss 0.012639659456908703, R2 0.5131334662437439\n",
      "epoch 4397, loss 0.012619935907423496, R2 0.47543931007385254\n",
      "Eval loss 0.012639457359910011, R2 0.5131412744522095\n",
      "epoch 4398, loss 0.01261974684894085, R2 0.47544699907302856\n",
      "Eval loss 0.012639254331588745, R2 0.5131489634513855\n",
      "epoch 4399, loss 0.012619560584425926, R2 0.475455105304718\n",
      "Eval loss 0.012639051303267479, R2 0.5131571888923645\n",
      "epoch 4400, loss 0.012619370594620705, R2 0.47546249628067017\n",
      "Eval loss 0.012638848274946213, R2 0.5131646394729614\n",
      "epoch 4401, loss 0.012619184330105782, R2 0.47547024488449097\n",
      "Eval loss 0.012638647109270096, R2 0.5131726264953613\n",
      "epoch 4402, loss 0.01261899620294571, R2 0.47547823190689087\n",
      "Eval loss 0.012638445012271404, R2 0.5131802558898926\n",
      "epoch 4403, loss 0.012618809938430786, R2 0.4754863381385803\n",
      "Eval loss 0.012638241983950138, R2 0.5131880044937134\n",
      "epoch 4404, loss 0.012618622742593288, R2 0.47549355030059814\n",
      "Eval loss 0.012638039886951447, R2 0.5131958723068237\n",
      "epoch 4405, loss 0.01261843554675579, R2 0.4755013585090637\n",
      "Eval loss 0.01263783872127533, R2 0.513203501701355\n",
      "epoch 4406, loss 0.012618248350918293, R2 0.4755093455314636\n",
      "Eval loss 0.012637635692954063, R2 0.5132116079330444\n",
      "epoch 4407, loss 0.01261806208640337, R2 0.4755169153213501\n",
      "Eval loss 0.012637432664632797, R2 0.5132197141647339\n",
      "epoch 4408, loss 0.012617873959243298, R2 0.4755247235298157\n",
      "Eval loss 0.01263723149895668, R2 0.513227105140686\n",
      "epoch 4409, loss 0.0126176867634058, R2 0.4755324721336365\n",
      "Eval loss 0.012637030333280563, R2 0.5132350325584412\n",
      "epoch 4410, loss 0.012617500498890877, R2 0.4755402207374573\n",
      "Eval loss 0.012636828236281872, R2 0.5132423639297485\n",
      "epoch 4411, loss 0.012617314234375954, R2 0.4755481481552124\n",
      "Eval loss 0.01263662800192833, R2 0.5132502317428589\n",
      "epoch 4412, loss 0.012617127038538456, R2 0.47555625438690186\n",
      "Eval loss 0.012636426836252213, R2 0.513258159160614\n",
      "epoch 4413, loss 0.012616941705346107, R2 0.4755634665489197\n",
      "Eval loss 0.012636225670576096, R2 0.5132657289505005\n",
      "epoch 4414, loss 0.012616756372153759, R2 0.4755709767341614\n",
      "Eval loss 0.012636023573577404, R2 0.5132741332054138\n",
      "epoch 4415, loss 0.012616569176316261, R2 0.47557878494262695\n",
      "Eval loss 0.012635822407901287, R2 0.5132812857627869\n",
      "epoch 4416, loss 0.012616385705769062, R2 0.47558677196502686\n",
      "Eval loss 0.012635622173547745, R2 0.5132893323898315\n",
      "epoch 4417, loss 0.012616199441254139, R2 0.47559428215026855\n",
      "Eval loss 0.012635421939194202, R2 0.5132966041564941\n",
      "epoch 4418, loss 0.012616012245416641, R2 0.47560209035873413\n",
      "Eval loss 0.012635220773518085, R2 0.5133043527603149\n",
      "epoch 4419, loss 0.012615827843546867, R2 0.47560977935791016\n",
      "Eval loss 0.012635020539164543, R2 0.5133124589920044\n",
      "epoch 4420, loss 0.012615642510354519, R2 0.4756174683570862\n",
      "Eval loss 0.012634820304811, R2 0.5133200883865356\n",
      "epoch 4421, loss 0.012615456245839596, R2 0.47562533617019653\n",
      "Eval loss 0.012634621001780033, R2 0.5133275985717773\n",
      "epoch 4422, loss 0.012615271843969822, R2 0.47563302516937256\n",
      "Eval loss 0.012634419836103916, R2 0.5133354663848877\n",
      "epoch 4423, loss 0.012615085579454899, R2 0.47564059495925903\n",
      "Eval loss 0.012634221464395523, R2 0.5133428573608398\n",
      "epoch 4424, loss 0.0126149021089077, R2 0.4756482243537903\n",
      "Eval loss 0.01263402123004198, R2 0.5133508443832397\n",
      "epoch 4425, loss 0.012614716775715351, R2 0.4756559729576111\n",
      "Eval loss 0.012633820995688438, R2 0.5133585929870605\n",
      "epoch 4426, loss 0.012614531442523003, R2 0.47566378116607666\n",
      "Eval loss 0.012633620761334896, R2 0.5133663415908813\n",
      "epoch 4427, loss 0.012614347040653229, R2 0.47567129135131836\n",
      "Eval loss 0.012633422389626503, R2 0.513373851776123\n",
      "epoch 4428, loss 0.01261416357010603, R2 0.47567909955978394\n",
      "Eval loss 0.01263322215527296, R2 0.5133817195892334\n",
      "epoch 4429, loss 0.012613979168236256, R2 0.4756867289543152\n",
      "Eval loss 0.012633022852241993, R2 0.5133892297744751\n",
      "epoch 4430, loss 0.012613793835043907, R2 0.47569429874420166\n",
      "Eval loss 0.012632823549211025, R2 0.5133970975875854\n",
      "epoch 4431, loss 0.012613611295819283, R2 0.47570186853408813\n",
      "Eval loss 0.012632626108825207, R2 0.5134042501449585\n",
      "epoch 4432, loss 0.012613426893949509, R2 0.4757091999053955\n",
      "Eval loss 0.012632426805794239, R2 0.5134121179580688\n",
      "epoch 4433, loss 0.012613242492079735, R2 0.4757174253463745\n",
      "Eval loss 0.012632228434085846, R2 0.5134198665618896\n",
      "epoch 4434, loss 0.012613058090209961, R2 0.47572457790374756\n",
      "Eval loss 0.012632028199732304, R2 0.5134272575378418\n",
      "epoch 4435, loss 0.012612874619662762, R2 0.4757329821586609\n",
      "Eval loss 0.012631830759346485, R2 0.513434886932373\n",
      "epoch 4436, loss 0.012612692080438137, R2 0.47574007511138916\n",
      "Eval loss 0.012631632387638092, R2 0.5134429931640625\n",
      "epoch 4437, loss 0.012612507678568363, R2 0.47574740648269653\n",
      "Eval loss 0.012631434947252274, R2 0.5134501457214355\n",
      "epoch 4438, loss 0.012612324208021164, R2 0.4757559299468994\n",
      "Eval loss 0.01263123657554388, R2 0.5134581923484802\n",
      "epoch 4439, loss 0.012612140737473965, R2 0.4757630228996277\n",
      "Eval loss 0.012631038203835487, R2 0.5134657025337219\n",
      "epoch 4440, loss 0.012611957266926765, R2 0.47577112913131714\n",
      "Eval loss 0.012630841694772243, R2 0.5134732723236084\n",
      "epoch 4441, loss 0.01261177659034729, R2 0.47577816247940063\n",
      "Eval loss 0.012630644254386425, R2 0.5134806632995605\n",
      "epoch 4442, loss 0.012611592188477516, R2 0.47578632831573486\n",
      "Eval loss 0.012630445882678032, R2 0.5134888291358948\n",
      "epoch 4443, loss 0.012611410580575466, R2 0.47579389810562134\n",
      "Eval loss 0.012630247510969639, R2 0.5134965181350708\n",
      "epoch 4444, loss 0.012611227110028267, R2 0.47580116987228394\n",
      "Eval loss 0.012630051001906395, R2 0.513504147529602\n",
      "epoch 4445, loss 0.012611045502126217, R2 0.4758085012435913\n",
      "Eval loss 0.012629854492843151, R2 0.513511061668396\n",
      "epoch 4446, loss 0.012610862031579018, R2 0.47581636905670166\n",
      "Eval loss 0.012629657052457333, R2 0.5135188102722168\n",
      "epoch 4447, loss 0.012610679492354393, R2 0.4758237600326538\n",
      "Eval loss 0.012629459612071514, R2 0.5135263800621033\n",
      "epoch 4448, loss 0.012610497884452343, R2 0.4758313298225403\n",
      "Eval loss 0.01262926310300827, R2 0.5135340690612793\n",
      "epoch 4449, loss 0.012610314413905144, R2 0.4758387804031372\n",
      "Eval loss 0.012629065662622452, R2 0.5135414600372314\n",
      "epoch 4450, loss 0.012610131874680519, R2 0.4758470058441162\n",
      "Eval loss 0.012628870084881783, R2 0.5135496854782104\n",
      "epoch 4451, loss 0.012609952129423618, R2 0.47585397958755493\n",
      "Eval loss 0.012628672644495964, R2 0.5135571360588074\n",
      "epoch 4452, loss 0.012609769590198994, R2 0.47586190700531006\n",
      "Eval loss 0.012628475204110146, R2 0.5135647058486938\n",
      "epoch 4453, loss 0.012609587982296944, R2 0.4758692979812622\n",
      "Eval loss 0.012628279626369476, R2 0.513572096824646\n",
      "epoch 4454, loss 0.012609406374394894, R2 0.47587668895721436\n",
      "Eval loss 0.012628084048628807, R2 0.5135794878005981\n",
      "epoch 4455, loss 0.012609225697815418, R2 0.4758843779563904\n",
      "Eval loss 0.012627887539565563, R2 0.5135871171951294\n",
      "epoch 4456, loss 0.012609042227268219, R2 0.4758918285369873\n",
      "Eval loss 0.01262769103050232, R2 0.5135947465896606\n",
      "epoch 4457, loss 0.012608861550688744, R2 0.47589927911758423\n",
      "Eval loss 0.01262749545276165, R2 0.5136020183563232\n",
      "epoch 4458, loss 0.012608680874109268, R2 0.47590696811676025\n",
      "Eval loss 0.01262729987502098, R2 0.5136100053787231\n",
      "epoch 4459, loss 0.012608501128852367, R2 0.4759142994880676\n",
      "Eval loss 0.012627104297280312, R2 0.5136175155639648\n",
      "epoch 4460, loss 0.012608318589627743, R2 0.4759220480918884\n",
      "Eval loss 0.012626911513507366, R2 0.5136248469352722\n",
      "epoch 4461, loss 0.012608136981725693, R2 0.4759296178817749\n",
      "Eval loss 0.012626714073121548, R2 0.5136319994926453\n",
      "epoch 4462, loss 0.012607958167791367, R2 0.4759368896484375\n",
      "Eval loss 0.012626518495380878, R2 0.5136398077011108\n",
      "epoch 4463, loss 0.012607777491211891, R2 0.47594505548477173\n",
      "Eval loss 0.012626323848962784, R2 0.5136470198631287\n",
      "epoch 4464, loss 0.01260759774595499, R2 0.4759518504142761\n",
      "Eval loss 0.012626130133867264, R2 0.5136547684669495\n",
      "epoch 4465, loss 0.01260741613805294, R2 0.4759594202041626\n",
      "Eval loss 0.012625934556126595, R2 0.5136623382568359\n",
      "epoch 4466, loss 0.012607235461473465, R2 0.4759674072265625\n",
      "Eval loss 0.012625738978385925, R2 0.5136695504188538\n",
      "epoch 4467, loss 0.012607055716216564, R2 0.47597455978393555\n",
      "Eval loss 0.01262554433196783, R2 0.5136770009994507\n",
      "epoch 4468, loss 0.012606875970959663, R2 0.4759814739227295\n",
      "Eval loss 0.012625349685549736, R2 0.513684868812561\n",
      "epoch 4469, loss 0.012606696225702763, R2 0.4759899973869324\n",
      "Eval loss 0.012625155970454216, R2 0.5136920213699341\n",
      "epoch 4470, loss 0.012606517411768436, R2 0.475996732711792\n",
      "Eval loss 0.012624962255358696, R2 0.5136995315551758\n",
      "epoch 4471, loss 0.01260633859783411, R2 0.4760041832923889\n",
      "Eval loss 0.01262476947158575, R2 0.5137068033218384\n",
      "epoch 4472, loss 0.012606157921254635, R2 0.4760116934776306\n",
      "Eval loss 0.012624574825167656, R2 0.5137144327163696\n",
      "epoch 4473, loss 0.012605978175997734, R2 0.47601884603500366\n",
      "Eval loss 0.012624380178749561, R2 0.5137221813201904\n",
      "epoch 4474, loss 0.012605797499418259, R2 0.47602683305740356\n",
      "Eval loss 0.012624185532331467, R2 0.5137298107147217\n",
      "epoch 4475, loss 0.012605620548129082, R2 0.4760340452194214\n",
      "Eval loss 0.012623991817235947, R2 0.5137369632720947\n",
      "epoch 4476, loss 0.012605439871549606, R2 0.4760420322418213\n",
      "Eval loss 0.012623799033463001, R2 0.5137447118759155\n",
      "epoch 4477, loss 0.01260526105761528, R2 0.47604900598526\n",
      "Eval loss 0.012623606249690056, R2 0.5137522220611572\n",
      "epoch 4478, loss 0.012605083175003529, R2 0.4760563373565674\n",
      "Eval loss 0.01262341346591711, R2 0.513759434223175\n",
      "epoch 4479, loss 0.012604905292391777, R2 0.4760642647743225\n",
      "Eval loss 0.01262321975082159, R2 0.513766884803772\n",
      "epoch 4480, loss 0.012604725547134876, R2 0.47607189416885376\n",
      "Eval loss 0.012623026967048645, R2 0.513774037361145\n",
      "epoch 4481, loss 0.01260454673320055, R2 0.47607868909835815\n",
      "Eval loss 0.012622833251953125, R2 0.5137814879417419\n",
      "epoch 4482, loss 0.01260436698794365, R2 0.4760863184928894\n",
      "Eval loss 0.01262264046818018, R2 0.5137890577316284\n",
      "epoch 4483, loss 0.012604189105331898, R2 0.4760935306549072\n",
      "Eval loss 0.012622447684407234, R2 0.5137964487075806\n",
      "epoch 4484, loss 0.012604010291397572, R2 0.476101279258728\n",
      "Eval loss 0.012622254900634289, R2 0.513804018497467\n",
      "epoch 4485, loss 0.01260383427143097, R2 0.4761084318161011\n",
      "Eval loss 0.012622063048183918, R2 0.5138117074966431\n",
      "epoch 4486, loss 0.012603654526174068, R2 0.476115882396698\n",
      "Eval loss 0.012621869333088398, R2 0.5138187408447266\n",
      "epoch 4487, loss 0.012603477574884892, R2 0.4761236310005188\n",
      "Eval loss 0.012621678411960602, R2 0.5138259530067444\n",
      "epoch 4488, loss 0.01260329969227314, R2 0.47613048553466797\n",
      "Eval loss 0.012621485628187656, R2 0.5138336420059204\n",
      "epoch 4489, loss 0.012603121809661388, R2 0.4761378765106201\n",
      "Eval loss 0.012621293775737286, R2 0.513840913772583\n",
      "epoch 4490, loss 0.012602945789694786, R2 0.47614574432373047\n",
      "Eval loss 0.01262110099196434, R2 0.5138484835624695\n",
      "epoch 4491, loss 0.012602767907083035, R2 0.47615259885787964\n",
      "Eval loss 0.012620911002159119, R2 0.5138555765151978\n",
      "epoch 4492, loss 0.012602590024471283, R2 0.4761599898338318\n",
      "Eval loss 0.012620719149708748, R2 0.5138633251190186\n",
      "epoch 4493, loss 0.012602412141859531, R2 0.47616738080978394\n",
      "Eval loss 0.012620527297258377, R2 0.5138705968856812\n",
      "epoch 4494, loss 0.012602235190570354, R2 0.4761747717857361\n",
      "Eval loss 0.012620336376130581, R2 0.5138781070709229\n",
      "epoch 4495, loss 0.012602058239281178, R2 0.4761822819709778\n",
      "Eval loss 0.01262014452368021, R2 0.5138851404190063\n",
      "epoch 4496, loss 0.012601881287992, R2 0.4761894941329956\n",
      "Eval loss 0.012619955465197563, R2 0.513892650604248\n",
      "epoch 4497, loss 0.012601706199347973, R2 0.4761967062950134\n",
      "Eval loss 0.012619763612747192, R2 0.5139001607894897\n",
      "epoch 4498, loss 0.012601527385413647, R2 0.4762040376663208\n",
      "Eval loss 0.012619571760296822, R2 0.513907253742218\n",
      "epoch 4499, loss 0.012601351365447044, R2 0.4762113094329834\n",
      "Eval loss 0.0126193817704916, R2 0.5139144659042358\n",
      "epoch 4500, loss 0.012601176276803017, R2 0.47621941566467285\n",
      "Eval loss 0.01261918991804123, R2 0.5139220952987671\n",
      "epoch 4501, loss 0.01260099932551384, R2 0.4762263298034668\n",
      "Eval loss 0.012619001790881157, R2 0.5139290690422058\n",
      "epoch 4502, loss 0.012600821442902088, R2 0.47623348236083984\n",
      "Eval loss 0.012618809007108212, R2 0.5139367580413818\n",
      "epoch 4503, loss 0.01260064635425806, R2 0.4762412905693054\n",
      "Eval loss 0.012618618085980415, R2 0.5139442682266235\n",
      "epoch 4504, loss 0.012600469402968884, R2 0.4762486219406128\n",
      "Eval loss 0.012618428096175194, R2 0.5139514207839966\n",
      "epoch 4505, loss 0.012600294314324856, R2 0.4762560725212097\n",
      "Eval loss 0.012618239969015121, R2 0.5139588117599487\n",
      "epoch 4506, loss 0.012600118294358253, R2 0.4762628674507141\n",
      "Eval loss 0.012618049047887325, R2 0.5139659643173218\n",
      "epoch 4507, loss 0.012599943205714226, R2 0.47627002000808716\n",
      "Eval loss 0.012617859058082104, R2 0.5139731168746948\n",
      "epoch 4508, loss 0.012599768117070198, R2 0.4762774705886841\n",
      "Eval loss 0.012617669068276882, R2 0.513980507850647\n",
      "epoch 4509, loss 0.012599592097103596, R2 0.47628462314605713\n",
      "Eval loss 0.01261747907847166, R2 0.5139881372451782\n",
      "epoch 4510, loss 0.012599417008459568, R2 0.47629237174987793\n",
      "Eval loss 0.012617289088666439, R2 0.5139954090118408\n",
      "epoch 4511, loss 0.012599240057170391, R2 0.4762992262840271\n",
      "Eval loss 0.012617100961506367, R2 0.5140023231506348\n",
      "epoch 4512, loss 0.012599064968526363, R2 0.47630733251571655\n",
      "Eval loss 0.012616910971701145, R2 0.5140095949172974\n",
      "epoch 4513, loss 0.012598888948559761, R2 0.47631382942199707\n",
      "Eval loss 0.012616721913218498, R2 0.5140173435211182\n",
      "epoch 4514, loss 0.012598714791238308, R2 0.47632110118865967\n",
      "Eval loss 0.012616532854735851, R2 0.5140241384506226\n",
      "epoch 4515, loss 0.012598540633916855, R2 0.4763284921646118\n",
      "Eval loss 0.012616344727575779, R2 0.5140318870544434\n",
      "epoch 4516, loss 0.012598365545272827, R2 0.4763357639312744\n",
      "Eval loss 0.012616154737770557, R2 0.5140390396118164\n",
      "epoch 4517, loss 0.012598191387951374, R2 0.4763433337211609\n",
      "Eval loss 0.012615966610610485, R2 0.5140464305877686\n",
      "epoch 4518, loss 0.012598017230629921, R2 0.47635018825531006\n",
      "Eval loss 0.012615777552127838, R2 0.5140537023544312\n",
      "epoch 4519, loss 0.012597842141985893, R2 0.47635751962661743\n",
      "Eval loss 0.01261559035629034, R2 0.5140607357025146\n",
      "epoch 4520, loss 0.012597668915987015, R2 0.4763643741607666\n",
      "Eval loss 0.012615401297807693, R2 0.5140680074691772\n",
      "epoch 4521, loss 0.012597493827342987, R2 0.4763716459274292\n",
      "Eval loss 0.012615213170647621, R2 0.5140752792358398\n",
      "epoch 4522, loss 0.012597320601344109, R2 0.476378858089447\n",
      "Eval loss 0.012615024112164974, R2 0.5140824317932129\n",
      "epoch 4523, loss 0.012597146444022655, R2 0.47638630867004395\n",
      "Eval loss 0.012614836916327477, R2 0.5140896439552307\n",
      "epoch 4524, loss 0.012596971355378628, R2 0.4763941764831543\n",
      "Eval loss 0.012614648789167404, R2 0.5140973329544067\n",
      "epoch 4525, loss 0.01259679812937975, R2 0.47640126943588257\n",
      "Eval loss 0.012614461593329906, R2 0.5141043663024902\n",
      "epoch 4526, loss 0.012596623040735722, R2 0.4764080047607422\n",
      "Eval loss 0.012614273466169834, R2 0.5141116380691528\n",
      "epoch 4527, loss 0.012596450746059418, R2 0.47641515731811523\n",
      "Eval loss 0.012614085339009762, R2 0.5141189098358154\n",
      "epoch 4528, loss 0.01259627752006054, R2 0.47642308473587036\n",
      "Eval loss 0.012613898143172264, R2 0.5141260623931885\n",
      "epoch 4529, loss 0.012596103362739086, R2 0.4764297604560852\n",
      "Eval loss 0.012613710016012192, R2 0.5141334533691406\n",
      "epoch 4530, loss 0.012595931068062782, R2 0.4764367938041687\n",
      "Eval loss 0.012613524682819843, R2 0.5141406059265137\n",
      "epoch 4531, loss 0.012595757842063904, R2 0.47644418478012085\n",
      "Eval loss 0.012613337486982346, R2 0.5141474008560181\n",
      "epoch 4532, loss 0.012595584616065025, R2 0.4764515161514282\n",
      "Eval loss 0.012613149359822273, R2 0.5141547918319702\n",
      "epoch 4533, loss 0.012595411390066147, R2 0.4764583706855774\n",
      "Eval loss 0.01261296309530735, R2 0.5141622424125671\n",
      "epoch 4534, loss 0.012595240026712418, R2 0.47646552324295044\n",
      "Eval loss 0.012612776830792427, R2 0.5141689777374268\n",
      "epoch 4535, loss 0.01259506493806839, R2 0.47647297382354736\n",
      "Eval loss 0.01261258963495493, R2 0.5141764879226685\n",
      "epoch 4536, loss 0.01259489357471466, R2 0.4764798879623413\n",
      "Eval loss 0.012612403370440006, R2 0.514183521270752\n",
      "epoch 4537, loss 0.012594721280038357, R2 0.47648710012435913\n",
      "Eval loss 0.012612215243279934, R2 0.514191210269928\n",
      "epoch 4538, loss 0.012594548054039478, R2 0.4764944314956665\n",
      "Eval loss 0.01261203084141016, R2 0.514197826385498\n",
      "epoch 4539, loss 0.012594376690685749, R2 0.47650158405303955\n",
      "Eval loss 0.012611843645572662, R2 0.514204740524292\n",
      "epoch 4540, loss 0.012594204396009445, R2 0.47650855779647827\n",
      "Eval loss 0.012611659243702888, R2 0.514211893081665\n",
      "epoch 4541, loss 0.012594031170010567, R2 0.47651606798171997\n",
      "Eval loss 0.012611471116542816, R2 0.5142191648483276\n",
      "epoch 4542, loss 0.012593860737979412, R2 0.47652286291122437\n",
      "Eval loss 0.012611285783350468, R2 0.5142265558242798\n",
      "epoch 4543, loss 0.012593688443303108, R2 0.47653019428253174\n",
      "Eval loss 0.01261110045015812, R2 0.5142334699630737\n",
      "epoch 4544, loss 0.012593517079949379, R2 0.4765377640724182\n",
      "Eval loss 0.012610914185643196, R2 0.5142407417297363\n",
      "epoch 4545, loss 0.012593344785273075, R2 0.4765447974205017\n",
      "Eval loss 0.012610729783773422, R2 0.5142479538917542\n",
      "epoch 4546, loss 0.01259317435324192, R2 0.4765514135360718\n",
      "Eval loss 0.012610544450581074, R2 0.5142552852630615\n",
      "epoch 4547, loss 0.012593001127243042, R2 0.4765594005584717\n",
      "Eval loss 0.01261035818606615, R2 0.5142621397972107\n",
      "epoch 4548, loss 0.012592830695211887, R2 0.4765656590461731\n",
      "Eval loss 0.012610173784196377, R2 0.5142691135406494\n",
      "epoch 4549, loss 0.012592659331858158, R2 0.47657281160354614\n",
      "Eval loss 0.012609989382326603, R2 0.5142768025398254\n",
      "epoch 4550, loss 0.012592488899827003, R2 0.47658002376556396\n",
      "Eval loss 0.01260980311781168, R2 0.5142837762832642\n",
      "epoch 4551, loss 0.012592318467795849, R2 0.4765869379043579\n",
      "Eval loss 0.012609618715941906, R2 0.514290452003479\n",
      "epoch 4552, loss 0.012592146173119545, R2 0.4765942692756653\n",
      "Eval loss 0.012609434314072132, R2 0.5142977237701416\n",
      "epoch 4553, loss 0.012591973878443241, R2 0.476601243019104\n",
      "Eval loss 0.012609248980879784, R2 0.5143047571182251\n",
      "epoch 4554, loss 0.012591804377734661, R2 0.4766083359718323\n",
      "Eval loss 0.012609066441655159, R2 0.5143117904663086\n",
      "epoch 4555, loss 0.012591633945703506, R2 0.4766160845756531\n",
      "Eval loss 0.012608880177140236, R2 0.5143192410469055\n",
      "epoch 4556, loss 0.012591465376317501, R2 0.47662240266799927\n",
      "Eval loss 0.012608695775270462, R2 0.5143263339996338\n",
      "epoch 4557, loss 0.012591293081641197, R2 0.476629376411438\n",
      "Eval loss 0.012608512304723263, R2 0.5143337249755859\n",
      "epoch 4558, loss 0.012591123580932617, R2 0.4766365885734558\n",
      "Eval loss 0.012608327902853489, R2 0.5143406391143799\n",
      "epoch 4559, loss 0.012590953148901463, R2 0.4766436815261841\n",
      "Eval loss 0.012608143500983715, R2 0.5143475532531738\n",
      "epoch 4560, loss 0.012590782716870308, R2 0.47665077447891235\n",
      "Eval loss 0.012607960030436516, R2 0.514354407787323\n",
      "epoch 4561, loss 0.012590612284839153, R2 0.4766578674316406\n",
      "Eval loss 0.012607776559889317, R2 0.5143617391586304\n",
      "epoch 4562, loss 0.012590442784130573, R2 0.4766647219657898\n",
      "Eval loss 0.012607594951987267, R2 0.5143688917160034\n",
      "epoch 4563, loss 0.012590274214744568, R2 0.4766724109649658\n",
      "Eval loss 0.012607409618794918, R2 0.5143755674362183\n",
      "epoch 4564, loss 0.012590103782713413, R2 0.4766789674758911\n",
      "Eval loss 0.012607226148247719, R2 0.5143829584121704\n",
      "epoch 4565, loss 0.012589935213327408, R2 0.4766858220100403\n",
      "Eval loss 0.012607043609023094, R2 0.5143901109695435\n",
      "epoch 4566, loss 0.012589764781296253, R2 0.4766935706138611\n",
      "Eval loss 0.012606860138475895, R2 0.5143975019454956\n",
      "epoch 4567, loss 0.012589595280587673, R2 0.4767003059387207\n",
      "Eval loss 0.012606676667928696, R2 0.5144040584564209\n",
      "epoch 4568, loss 0.012589425779879093, R2 0.47670799493789673\n",
      "Eval loss 0.012606495060026646, R2 0.5144110918045044\n",
      "epoch 4569, loss 0.012589257210493088, R2 0.4767146706581116\n",
      "Eval loss 0.012606311589479446, R2 0.5144181251525879\n",
      "epoch 4570, loss 0.012589086778461933, R2 0.47672128677368164\n",
      "Eval loss 0.012606129981577396, R2 0.5144251585006714\n",
      "epoch 4571, loss 0.012588919140398502, R2 0.47672826051712036\n",
      "Eval loss 0.012605947442352772, R2 0.514431893825531\n",
      "epoch 4572, loss 0.012588751502335072, R2 0.4767358899116516\n",
      "Eval loss 0.012605763971805573, R2 0.5144392251968384\n",
      "epoch 4573, loss 0.012588581070303917, R2 0.4767424464225769\n",
      "Eval loss 0.012605581432580948, R2 0.5144460201263428\n",
      "epoch 4574, loss 0.012588412500917912, R2 0.47674912214279175\n",
      "Eval loss 0.012605398893356323, R2 0.5144532918930054\n",
      "epoch 4575, loss 0.012588245794177055, R2 0.4767562747001648\n",
      "Eval loss 0.012605217285454273, R2 0.514460027217865\n",
      "epoch 4576, loss 0.012588076293468475, R2 0.47676342725753784\n",
      "Eval loss 0.012605033814907074, R2 0.5144672393798828\n",
      "epoch 4577, loss 0.012587906792759895, R2 0.47677046060562134\n",
      "Eval loss 0.012604855000972748, R2 0.5144745707511902\n",
      "epoch 4578, loss 0.01258774008601904, R2 0.47677725553512573\n",
      "Eval loss 0.012604672461748123, R2 0.5144811868667603\n",
      "epoch 4579, loss 0.012587571516633034, R2 0.47678446769714355\n",
      "Eval loss 0.012604489922523499, R2 0.5144883394241333\n",
      "epoch 4580, loss 0.012587403878569603, R2 0.4767912030220032\n",
      "Eval loss 0.012604308314621449, R2 0.5144954919815063\n",
      "epoch 4581, loss 0.012587235309183598, R2 0.476797878742218\n",
      "Eval loss 0.012604125775396824, R2 0.5145025253295898\n",
      "epoch 4582, loss 0.012587067671120167, R2 0.4768056869506836\n",
      "Eval loss 0.012603945098817348, R2 0.5145094394683838\n",
      "epoch 4583, loss 0.01258690096437931, R2 0.47681277990341187\n",
      "Eval loss 0.012603764422237873, R2 0.5145162343978882\n",
      "epoch 4584, loss 0.01258673146367073, R2 0.47681933641433716\n",
      "Eval loss 0.012603582814335823, R2 0.5145230293273926\n",
      "epoch 4585, loss 0.012586564756929874, R2 0.4768260717391968\n",
      "Eval loss 0.012603401206433773, R2 0.5145299434661865\n",
      "epoch 4586, loss 0.012586397118866444, R2 0.4768330454826355\n",
      "Eval loss 0.012603220529854298, R2 0.5145373344421387\n",
      "epoch 4587, loss 0.012586229480803013, R2 0.4768400192260742\n",
      "Eval loss 0.012603039853274822, R2 0.514543890953064\n",
      "epoch 4588, loss 0.012586063705384731, R2 0.4768471121788025\n",
      "Eval loss 0.012602859176695347, R2 0.5145508050918579\n",
      "epoch 4589, loss 0.012585895135998726, R2 0.4768539071083069\n",
      "Eval loss 0.012602678500115871, R2 0.5145580768585205\n",
      "epoch 4590, loss 0.012585729360580444, R2 0.47686082124710083\n",
      "Eval loss 0.012602497823536396, R2 0.5145650506019592\n",
      "epoch 4591, loss 0.012585561722517014, R2 0.47686779499053955\n",
      "Eval loss 0.012602318078279495, R2 0.514572024345398\n",
      "epoch 4592, loss 0.012585395947098732, R2 0.4768747091293335\n",
      "Eval loss 0.01260213740170002, R2 0.5145790576934814\n",
      "epoch 4593, loss 0.012585228309035301, R2 0.47688114643096924\n",
      "Eval loss 0.012601957656443119, R2 0.5145857334136963\n",
      "epoch 4594, loss 0.012585061602294445, R2 0.47688859701156616\n",
      "Eval loss 0.012601777911186218, R2 0.5145925283432007\n",
      "epoch 4595, loss 0.012584894895553589, R2 0.47689568996429443\n",
      "Eval loss 0.012601598165929317, R2 0.5145995616912842\n",
      "epoch 4596, loss 0.012584730051457882, R2 0.47690272331237793\n",
      "Eval loss 0.012601417489349842, R2 0.5146065354347229\n",
      "epoch 4597, loss 0.012584562413394451, R2 0.476909339427948\n",
      "Eval loss 0.012601238675415516, R2 0.5146132707595825\n",
      "epoch 4598, loss 0.012584397569298744, R2 0.47691619396209717\n",
      "Eval loss 0.01260105799883604, R2 0.5146206617355347\n",
      "epoch 4599, loss 0.012584231793880463, R2 0.4769231081008911\n",
      "Eval loss 0.012600877322256565, R2 0.51462721824646\n",
      "epoch 4600, loss 0.012584065087139606, R2 0.4769299626350403\n",
      "Eval loss 0.012600699439644814, R2 0.514634370803833\n",
      "epoch 4601, loss 0.01258389838039875, R2 0.47693711519241333\n",
      "Eval loss 0.012600520625710487, R2 0.514641523361206\n",
      "epoch 4602, loss 0.012583733536303043, R2 0.4769434332847595\n",
      "Eval loss 0.012600340880453587, R2 0.5146480202674866\n",
      "epoch 4603, loss 0.012583567760884762, R2 0.4769505262374878\n",
      "Eval loss 0.01260016206651926, R2 0.5146547555923462\n",
      "epoch 4604, loss 0.01258340198546648, R2 0.47695809602737427\n",
      "Eval loss 0.012599983252584934, R2 0.5146619081497192\n",
      "epoch 4605, loss 0.012583238072693348, R2 0.47696441411972046\n",
      "Eval loss 0.012599804438650608, R2 0.5146685242652893\n",
      "epoch 4606, loss 0.012583071365952492, R2 0.47697198390960693\n",
      "Eval loss 0.012599625624716282, R2 0.5146753787994385\n",
      "epoch 4607, loss 0.01258290559053421, R2 0.4769783616065979\n",
      "Eval loss 0.01259944774210453, R2 0.5146822929382324\n",
      "epoch 4608, loss 0.012582741677761078, R2 0.4769851565361023\n",
      "Eval loss 0.012599269859492779, R2 0.5146890878677368\n",
      "epoch 4609, loss 0.012582574971020222, R2 0.4769919514656067\n",
      "Eval loss 0.012599090114235878, R2 0.5146960020065308\n",
      "epoch 4610, loss 0.01258241105824709, R2 0.47699862718582153\n",
      "Eval loss 0.012598911300301552, R2 0.5147033929824829\n",
      "epoch 4611, loss 0.012582246214151382, R2 0.47700560092926025\n",
      "Eval loss 0.012598734349012375, R2 0.5147100687026978\n",
      "epoch 4612, loss 0.012582081370055676, R2 0.4770129323005676\n",
      "Eval loss 0.012598556466400623, R2 0.5147168636322021\n",
      "epoch 4613, loss 0.012581917457282543, R2 0.4770197868347168\n",
      "Eval loss 0.012598379515111446, R2 0.5147238969802856\n",
      "epoch 4614, loss 0.012581752613186836, R2 0.47702664136886597\n",
      "Eval loss 0.012598199769854546, R2 0.5147304534912109\n",
      "epoch 4615, loss 0.012581588700413704, R2 0.47703278064727783\n",
      "Eval loss 0.012598021887242794, R2 0.5147372484207153\n",
      "epoch 4616, loss 0.012581423856317997, R2 0.47703981399536133\n",
      "Eval loss 0.012597843073308468, R2 0.5147444009780884\n",
      "epoch 4617, loss 0.01258125901222229, R2 0.4770466685295105\n",
      "Eval loss 0.01259766798466444, R2 0.5147514343261719\n",
      "epoch 4618, loss 0.012581096030771732, R2 0.4770534038543701\n",
      "Eval loss 0.012597490102052689, R2 0.5147581100463867\n",
      "epoch 4619, loss 0.01258093025535345, R2 0.4770604372024536\n",
      "Eval loss 0.012597311288118362, R2 0.5147651433944702\n",
      "epoch 4620, loss 0.012580767273902893, R2 0.4770670533180237\n",
      "Eval loss 0.01259713526815176, R2 0.5147719383239746\n",
      "epoch 4621, loss 0.012580604292452335, R2 0.4770738482475281\n",
      "Eval loss 0.012596958316862583, R2 0.5147784352302551\n",
      "epoch 4622, loss 0.012580439448356628, R2 0.4770808815956116\n",
      "Eval loss 0.012596781365573406, R2 0.5147854089736938\n",
      "epoch 4623, loss 0.01258027646690607, R2 0.47708696126937866\n",
      "Eval loss 0.012596605345606804, R2 0.5147923231124878\n",
      "epoch 4624, loss 0.012580113485455513, R2 0.47709423303604126\n",
      "Eval loss 0.012596427462995052, R2 0.5147987604141235\n",
      "epoch 4625, loss 0.012579948641359806, R2 0.4771016240119934\n",
      "Eval loss 0.01259625144302845, R2 0.5148056745529175\n",
      "epoch 4626, loss 0.012579786591231823, R2 0.477108359336853\n",
      "Eval loss 0.012596075423061848, R2 0.5148124694824219\n",
      "epoch 4627, loss 0.01257962454110384, R2 0.47711461782455444\n",
      "Eval loss 0.012595897540450096, R2 0.5148192644119263\n",
      "epoch 4628, loss 0.012579458765685558, R2 0.4771219491958618\n",
      "Eval loss 0.012595721520483494, R2 0.5148260593414307\n",
      "epoch 4629, loss 0.012579296715557575, R2 0.47712820768356323\n",
      "Eval loss 0.012595546431839466, R2 0.5148324966430664\n",
      "epoch 4630, loss 0.012579134665429592, R2 0.47713524103164673\n",
      "Eval loss 0.012595370411872864, R2 0.5148396492004395\n",
      "epoch 4631, loss 0.012578971683979034, R2 0.47714173793792725\n",
      "Eval loss 0.012595192529261112, R2 0.5148464441299438\n",
      "epoch 4632, loss 0.012578808702528477, R2 0.47714847326278687\n",
      "Eval loss 0.012595018371939659, R2 0.5148528814315796\n",
      "epoch 4633, loss 0.012578646652400494, R2 0.47715508937835693\n",
      "Eval loss 0.012594842351973057, R2 0.5148597955703735\n",
      "epoch 4634, loss 0.012578482739627361, R2 0.47716188430786133\n",
      "Eval loss 0.012594667263329029, R2 0.5148664116859436\n",
      "epoch 4635, loss 0.012578321620821953, R2 0.4771687388420105\n",
      "Eval loss 0.012594490312039852, R2 0.5148730874061584\n",
      "epoch 4636, loss 0.012578158639371395, R2 0.4771760106086731\n",
      "Eval loss 0.01259431429207325, R2 0.5148805379867554\n",
      "epoch 4637, loss 0.012577995657920837, R2 0.4771823287010193\n",
      "Eval loss 0.012594139203429222, R2 0.514886736869812\n",
      "epoch 4638, loss 0.012577834539115429, R2 0.47718900442123413\n",
      "Eval loss 0.012593965977430344, R2 0.5148938894271851\n",
      "epoch 4639, loss 0.012577671557664871, R2 0.47719621658325195\n",
      "Eval loss 0.012593788094818592, R2 0.5149005651473999\n",
      "epoch 4640, loss 0.012577510438859463, R2 0.4772024154663086\n",
      "Eval loss 0.012593614868819714, R2 0.5149073600769043\n",
      "epoch 4641, loss 0.01257734838873148, R2 0.47720950841903687\n",
      "Eval loss 0.012593439780175686, R2 0.5149142742156982\n",
      "epoch 4642, loss 0.012577187269926071, R2 0.47721588611602783\n",
      "Eval loss 0.012593264691531658, R2 0.5149204730987549\n",
      "epoch 4643, loss 0.012577025219798088, R2 0.47722262144088745\n",
      "Eval loss 0.01259308960288763, R2 0.5149272680282593\n",
      "epoch 4644, loss 0.012576863169670105, R2 0.47722935676574707\n",
      "Eval loss 0.012592915445566177, R2 0.5149343013763428\n",
      "epoch 4645, loss 0.012576702982187271, R2 0.47723621129989624\n",
      "Eval loss 0.012592741288244724, R2 0.5149407386779785\n",
      "epoch 4646, loss 0.012576541863381863, R2 0.47724270820617676\n",
      "Eval loss 0.012592566199600697, R2 0.5149472951889038\n",
      "epoch 4647, loss 0.01257637981325388, R2 0.47724997997283936\n",
      "Eval loss 0.012592392973601818, R2 0.5149543285369873\n",
      "epoch 4648, loss 0.012576218694448471, R2 0.477256178855896\n",
      "Eval loss 0.01259221788495779, R2 0.5149612426757812\n",
      "epoch 4649, loss 0.012576057575643063, R2 0.47726333141326904\n",
      "Eval loss 0.012592042796313763, R2 0.5149677991867065\n",
      "epoch 4650, loss 0.012575896456837654, R2 0.47726970911026\n",
      "Eval loss 0.012591870501637459, R2 0.5149747133255005\n",
      "epoch 4651, loss 0.012575737200677395, R2 0.47727614641189575\n",
      "Eval loss 0.012591695412993431, R2 0.5149812698364258\n",
      "epoch 4652, loss 0.012575576081871986, R2 0.47728288173675537\n",
      "Eval loss 0.012591522186994553, R2 0.514987587928772\n",
      "epoch 4653, loss 0.012575414963066578, R2 0.4772900938987732\n",
      "Eval loss 0.012591348960995674, R2 0.5149942636489868\n",
      "epoch 4654, loss 0.012575252912938595, R2 0.4772968292236328\n",
      "Eval loss 0.012591175734996796, R2 0.5150014162063599\n",
      "epoch 4655, loss 0.012575093656778336, R2 0.4773029088973999\n",
      "Eval loss 0.012591001577675343, R2 0.5150080323219299\n",
      "epoch 4656, loss 0.012574934400618076, R2 0.4773094058036804\n",
      "Eval loss 0.012590829282999039, R2 0.5150142908096313\n",
      "epoch 4657, loss 0.012574774213135242, R2 0.47731637954711914\n",
      "Eval loss 0.01259065605700016, R2 0.5150210857391357\n",
      "epoch 4658, loss 0.012574614025652409, R2 0.47732287645339966\n",
      "Eval loss 0.012590481899678707, R2 0.5150278806686401\n",
      "epoch 4659, loss 0.012574452906847, R2 0.4773295521736145\n",
      "Eval loss 0.012590307742357254, R2 0.5150347352027893\n",
      "epoch 4660, loss 0.012574292719364166, R2 0.4773363471031189\n",
      "Eval loss 0.012590136379003525, R2 0.5150410532951355\n",
      "epoch 4661, loss 0.012574133463203907, R2 0.47734278440475464\n",
      "Eval loss 0.012589963153004646, R2 0.5150480270385742\n",
      "epoch 4662, loss 0.012573973275721073, R2 0.4773494601249695\n",
      "Eval loss 0.012589788995683193, R2 0.5150545835494995\n",
      "epoch 4663, loss 0.012573814019560814, R2 0.4773564338684082\n",
      "Eval loss 0.01258961670100689, R2 0.51506108045578\n",
      "epoch 4664, loss 0.01257365383207798, R2 0.4773627519607544\n",
      "Eval loss 0.012589446268975735, R2 0.5150678157806396\n",
      "epoch 4665, loss 0.01257349457591772, R2 0.47736984491348267\n",
      "Eval loss 0.012589272111654282, R2 0.5150744915008545\n",
      "epoch 4666, loss 0.012573334388434887, R2 0.4773760437965393\n",
      "Eval loss 0.012589101679623127, R2 0.5150808095932007\n",
      "epoch 4667, loss 0.012573175132274628, R2 0.4773826599121094\n",
      "Eval loss 0.012588928453624249, R2 0.5150877237319946\n",
      "epoch 4668, loss 0.012573016807436943, R2 0.47738921642303467\n",
      "Eval loss 0.012588756158947945, R2 0.5150942206382751\n",
      "epoch 4669, loss 0.012572858482599258, R2 0.47739583253860474\n",
      "Eval loss 0.012588584795594215, R2 0.5151005387306213\n",
      "epoch 4670, loss 0.012572698295116425, R2 0.4774024486541748\n",
      "Eval loss 0.012588412500917912, R2 0.5151075124740601\n",
      "epoch 4671, loss 0.01257253997027874, R2 0.4774090647697449\n",
      "Eval loss 0.012588241137564182, R2 0.5151141881942749\n",
      "epoch 4672, loss 0.012572381645441055, R2 0.4774153232574463\n",
      "Eval loss 0.012588068842887878, R2 0.5151205658912659\n",
      "epoch 4673, loss 0.01257222332060337, R2 0.4774223566055298\n",
      "Eval loss 0.01258789747953415, R2 0.5151275992393494\n",
      "epoch 4674, loss 0.012572063133120537, R2 0.47742903232574463\n",
      "Eval loss 0.012587725184857845, R2 0.515133798122406\n",
      "epoch 4675, loss 0.012571904808282852, R2 0.47743546962738037\n",
      "Eval loss 0.01258755475282669, R2 0.5151403546333313\n",
      "epoch 4676, loss 0.012571747414767742, R2 0.4774419665336609\n",
      "Eval loss 0.012587384320795536, R2 0.5151472091674805\n",
      "epoch 4677, loss 0.012571588158607483, R2 0.47744858264923096\n",
      "Eval loss 0.012587213888764381, R2 0.5151536464691162\n",
      "epoch 4678, loss 0.012571431696414948, R2 0.4774557948112488\n",
      "Eval loss 0.012587039731442928, R2 0.515160322189331\n",
      "epoch 4679, loss 0.012571273371577263, R2 0.4774615168571472\n",
      "Eval loss 0.012586870230734348, R2 0.5151673555374146\n",
      "epoch 4680, loss 0.012571115046739578, R2 0.47746825218200684\n",
      "Eval loss 0.012586700730025768, R2 0.5151733160018921\n",
      "epoch 4681, loss 0.012570956721901894, R2 0.4774748682975769\n",
      "Eval loss 0.012586529366672039, R2 0.5151801109313965\n",
      "epoch 4682, loss 0.012570801191031933, R2 0.4774816632270813\n",
      "Eval loss 0.01258635800331831, R2 0.5151864290237427\n",
      "epoch 4683, loss 0.012570642866194248, R2 0.47748810052871704\n",
      "Eval loss 0.012586187571287155, R2 0.5151934623718262\n",
      "epoch 4684, loss 0.012570484541356564, R2 0.477495014667511\n",
      "Eval loss 0.012586018070578575, R2 0.5151995420455933\n",
      "epoch 4685, loss 0.012570328079164028, R2 0.47750115394592285\n",
      "Eval loss 0.01258584763854742, R2 0.5152063965797424\n",
      "epoch 4686, loss 0.012570169754326344, R2 0.4775077700614929\n",
      "Eval loss 0.01258567813783884, R2 0.5152127742767334\n",
      "epoch 4687, loss 0.012570013292133808, R2 0.4775140881538391\n",
      "Eval loss 0.012585507705807686, R2 0.515219509601593\n",
      "epoch 4688, loss 0.012569854967296124, R2 0.47752082347869873\n",
      "Eval loss 0.012585338205099106, R2 0.5152260065078735\n",
      "epoch 4689, loss 0.012569697573781013, R2 0.477527379989624\n",
      "Eval loss 0.012585166841745377, R2 0.5152323246002197\n",
      "epoch 4690, loss 0.012569542042911053, R2 0.4775336980819702\n",
      "Eval loss 0.012584997341036797, R2 0.5152391791343689\n",
      "epoch 4691, loss 0.012569383718073368, R2 0.4775407314300537\n",
      "Eval loss 0.012584828771650791, R2 0.5152456760406494\n",
      "epoch 4692, loss 0.012569226324558258, R2 0.4775471091270447\n",
      "Eval loss 0.012584659270942211, R2 0.5152521729469299\n",
      "epoch 4693, loss 0.012569070793688297, R2 0.47755324840545654\n",
      "Eval loss 0.012584488838911057, R2 0.5152584314346313\n",
      "epoch 4694, loss 0.012568912468850613, R2 0.47755998373031616\n",
      "Eval loss 0.012584320269525051, R2 0.5152651071548462\n",
      "epoch 4695, loss 0.012568756937980652, R2 0.47756630182266235\n",
      "Eval loss 0.012584148906171322, R2 0.5152719616889954\n",
      "epoch 4696, loss 0.012568601407110691, R2 0.4775727391242981\n",
      "Eval loss 0.012583980336785316, R2 0.5152780413627625\n",
      "epoch 4697, loss 0.012568444944918156, R2 0.4775799512863159\n",
      "Eval loss 0.012583812698721886, R2 0.5152847766876221\n",
      "epoch 4698, loss 0.01256828848272562, R2 0.47758591175079346\n",
      "Eval loss 0.012583643198013306, R2 0.5152913331985474\n",
      "epoch 4699, loss 0.012568132020533085, R2 0.4775922894477844\n",
      "Eval loss 0.012583475559949875, R2 0.5152977705001831\n",
      "epoch 4700, loss 0.012567978352308273, R2 0.4775986671447754\n",
      "Eval loss 0.01258330512791872, R2 0.5153042078018188\n",
      "epoch 4701, loss 0.012567820958793163, R2 0.4776052236557007\n",
      "Eval loss 0.012583136558532715, R2 0.5153111219406128\n",
      "epoch 4702, loss 0.012567665427923203, R2 0.47761183977127075\n",
      "Eval loss 0.012582967057824135, R2 0.5153175592422485\n",
      "epoch 4703, loss 0.012567508965730667, R2 0.4776180386543274\n",
      "Eval loss 0.012582800351083279, R2 0.5153235197067261\n",
      "epoch 4704, loss 0.012567353434860706, R2 0.47762465476989746\n",
      "Eval loss 0.012582631781697273, R2 0.5153301358222961\n",
      "epoch 4705, loss 0.012567196972668171, R2 0.477631151676178\n",
      "Eval loss 0.012582463212311268, R2 0.5153365135192871\n",
      "epoch 4706, loss 0.012567042373120785, R2 0.4776380658149719\n",
      "Eval loss 0.012582295574247837, R2 0.5153432488441467\n",
      "epoch 4707, loss 0.012566887773573399, R2 0.4776439666748047\n",
      "Eval loss 0.012582127936184406, R2 0.5153498649597168\n",
      "epoch 4708, loss 0.012566731311380863, R2 0.47765064239501953\n",
      "Eval loss 0.012581958435475826, R2 0.515356183052063\n",
      "epoch 4709, loss 0.012566575780510902, R2 0.4776569604873657\n",
      "Eval loss 0.01258179172873497, R2 0.5153623819351196\n",
      "epoch 4710, loss 0.01256642211228609, R2 0.47766315937042236\n",
      "Eval loss 0.01258162222802639, R2 0.5153688788414001\n",
      "epoch 4711, loss 0.01256626658141613, R2 0.47766977548599243\n",
      "Eval loss 0.012581455521285534, R2 0.5153756141662598\n",
      "epoch 4712, loss 0.012566111981868744, R2 0.47767627239227295\n",
      "Eval loss 0.012581289745867252, R2 0.5153824090957642\n",
      "epoch 4713, loss 0.012565957382321358, R2 0.4776827096939087\n",
      "Eval loss 0.012581120245158672, R2 0.5153884887695312\n",
      "epoch 4714, loss 0.012565801851451397, R2 0.47768914699554443\n",
      "Eval loss 0.012580953538417816, R2 0.515394926071167\n",
      "epoch 4715, loss 0.012565646320581436, R2 0.47769594192504883\n",
      "Eval loss 0.01258078683167696, R2 0.5154012441635132\n",
      "epoch 4716, loss 0.012565492652356625, R2 0.4777024984359741\n",
      "Eval loss 0.012580620124936104, R2 0.515407919883728\n",
      "epoch 4717, loss 0.012565337121486664, R2 0.47770893573760986\n",
      "Eval loss 0.012580451555550098, R2 0.5154139995574951\n",
      "epoch 4718, loss 0.012565183453261852, R2 0.47771501541137695\n",
      "Eval loss 0.012580283917486668, R2 0.51542067527771\n",
      "epoch 4719, loss 0.012565028853714466, R2 0.4777219295501709\n",
      "Eval loss 0.012580120004713535, R2 0.5154273509979248\n",
      "epoch 4720, loss 0.012564875185489655, R2 0.47772765159606934\n",
      "Eval loss 0.012579952366650105, R2 0.515433669090271\n",
      "epoch 4721, loss 0.012564722448587418, R2 0.477733850479126\n",
      "Eval loss 0.012579785659909248, R2 0.5154397487640381\n",
      "epoch 4722, loss 0.012564567849040031, R2 0.47774046659469604\n",
      "Eval loss 0.012579618021845818, R2 0.5154461860656738\n",
      "epoch 4723, loss 0.01256441418081522, R2 0.47774749994277954\n",
      "Eval loss 0.01257945317775011, R2 0.5154529809951782\n",
      "epoch 4724, loss 0.012564260512590408, R2 0.47775304317474365\n",
      "Eval loss 0.012579286471009254, R2 0.5154592990875244\n",
      "epoch 4725, loss 0.012564105913043022, R2 0.47775977849960327\n",
      "Eval loss 0.012579120695590973, R2 0.515465259552002\n",
      "epoch 4726, loss 0.012563951313495636, R2 0.47776585817337036\n",
      "Eval loss 0.012578953988850117, R2 0.5154716968536377\n",
      "epoch 4727, loss 0.012563799507915974, R2 0.4777725338935852\n",
      "Eval loss 0.01257878728210926, R2 0.5154781341552734\n",
      "epoch 4728, loss 0.012563646771013737, R2 0.47777873277664185\n",
      "Eval loss 0.012578621506690979, R2 0.5154849290847778\n",
      "epoch 4729, loss 0.012563493102788925, R2 0.477785587310791\n",
      "Eval loss 0.012578455731272697, R2 0.5154911279678345\n",
      "epoch 4730, loss 0.012563338503241539, R2 0.4777917265892029\n",
      "Eval loss 0.012578289955854416, R2 0.5154975652694702\n",
      "epoch 4731, loss 0.012563185766339302, R2 0.4777987003326416\n",
      "Eval loss 0.012578124180436134, R2 0.5155036449432373\n",
      "epoch 4732, loss 0.01256303209811449, R2 0.47780489921569824\n",
      "Eval loss 0.012577960267663002, R2 0.515510082244873\n",
      "epoch 4733, loss 0.012562879361212254, R2 0.4778106212615967\n",
      "Eval loss 0.01257779449224472, R2 0.5155165195465088\n",
      "epoch 4734, loss 0.012562726624310017, R2 0.47781693935394287\n",
      "Eval loss 0.012577628716826439, R2 0.5155227184295654\n",
      "epoch 4735, loss 0.012562575750052929, R2 0.47782307863235474\n",
      "Eval loss 0.012577462941408157, R2 0.5155290961265564\n",
      "epoch 4736, loss 0.012562422081828117, R2 0.47782963514328003\n",
      "Eval loss 0.01257729809731245, R2 0.515535831451416\n",
      "epoch 4737, loss 0.01256226934492588, R2 0.47783660888671875\n",
      "Eval loss 0.012577134184539318, R2 0.5155420303344727\n",
      "epoch 4738, loss 0.012562117539346218, R2 0.4778422713279724\n",
      "Eval loss 0.012576969340443611, R2 0.5155484080314636\n",
      "epoch 4739, loss 0.012561963871121407, R2 0.47784847021102905\n",
      "Eval loss 0.01257680356502533, R2 0.515554666519165\n",
      "epoch 4740, loss 0.012561812065541744, R2 0.47785496711730957\n",
      "Eval loss 0.012576637789607048, R2 0.5155614614486694\n",
      "epoch 4741, loss 0.012561658397316933, R2 0.47786134481430054\n",
      "Eval loss 0.01257647480815649, R2 0.5155671834945679\n",
      "epoch 4742, loss 0.012561507523059845, R2 0.47786766290664673\n",
      "Eval loss 0.012576309964060783, R2 0.5155736207962036\n",
      "epoch 4743, loss 0.012561355717480183, R2 0.47787392139434814\n",
      "Eval loss 0.012576144188642502, R2 0.5155801773071289\n",
      "epoch 4744, loss 0.01256120391190052, R2 0.47788023948669434\n",
      "Eval loss 0.012575981207191944, R2 0.5155864953994751\n",
      "epoch 4745, loss 0.012561053037643433, R2 0.47788649797439575\n",
      "Eval loss 0.012575817294418812, R2 0.5155924558639526\n",
      "epoch 4746, loss 0.012560900300741196, R2 0.47789305448532104\n",
      "Eval loss 0.01257565338164568, R2 0.5155989527702332\n",
      "epoch 4747, loss 0.012560750357806683, R2 0.47789931297302246\n",
      "Eval loss 0.012575489468872547, R2 0.515605092048645\n",
      "epoch 4748, loss 0.012560596689581871, R2 0.47790515422821045\n",
      "Eval loss 0.01257532462477684, R2 0.5156115889549255\n",
      "epoch 4749, loss 0.012560445815324783, R2 0.4779117703437805\n",
      "Eval loss 0.012575161643326283, R2 0.5156181454658508\n",
      "epoch 4750, loss 0.012560294941067696, R2 0.47791802883148193\n",
      "Eval loss 0.012574996799230576, R2 0.5156244039535522\n",
      "epoch 4751, loss 0.012560143135488033, R2 0.47792452573776245\n",
      "Eval loss 0.012574833817780018, R2 0.5156304836273193\n",
      "epoch 4752, loss 0.01255999319255352, R2 0.47793108224868774\n",
      "Eval loss 0.012574669905006886, R2 0.5156372785568237\n",
      "epoch 4753, loss 0.012559841386973858, R2 0.47793692350387573\n",
      "Eval loss 0.012574506923556328, R2 0.5156432390213013\n",
      "epoch 4754, loss 0.01255969051271677, R2 0.477943480014801\n",
      "Eval loss 0.012574343010783195, R2 0.5156495571136475\n",
      "epoch 4755, loss 0.012559539638459682, R2 0.47794944047927856\n",
      "Eval loss 0.012574180029332638, R2 0.5156558752059937\n",
      "epoch 4756, loss 0.012559388764202595, R2 0.47795569896698\n",
      "Eval loss 0.012574017979204655, R2 0.5156618356704712\n",
      "epoch 4757, loss 0.012559237889945507, R2 0.4779621362686157\n",
      "Eval loss 0.012573853135108948, R2 0.5156681537628174\n",
      "epoch 4758, loss 0.012559087947010994, R2 0.47796785831451416\n",
      "Eval loss 0.01257369201630354, R2 0.5156745910644531\n",
      "epoch 4759, loss 0.012558936141431332, R2 0.477974534034729\n",
      "Eval loss 0.012573527172207832, R2 0.5156808495521545\n",
      "epoch 4760, loss 0.012558786198496819, R2 0.47798120975494385\n",
      "Eval loss 0.012573366053402424, R2 0.5156872272491455\n",
      "epoch 4761, loss 0.012558636255562305, R2 0.47798681259155273\n",
      "Eval loss 0.01257320400327444, R2 0.5156934261322021\n",
      "epoch 4762, loss 0.012558486312627792, R2 0.4779932498931885\n",
      "Eval loss 0.012573041953146458, R2 0.5156998634338379\n",
      "epoch 4763, loss 0.01255833636969328, R2 0.4779994487762451\n",
      "Eval loss 0.012572878040373325, R2 0.5157060027122498\n",
      "epoch 4764, loss 0.012558184564113617, R2 0.4780057668685913\n",
      "Eval loss 0.012572715990245342, R2 0.5157122611999512\n",
      "epoch 4765, loss 0.012558034621179104, R2 0.4780121445655823\n",
      "Eval loss 0.01257255394011736, R2 0.5157186388969421\n",
      "epoch 4766, loss 0.01255788654088974, R2 0.4780183434486389\n",
      "Eval loss 0.012572391889989376, R2 0.5157244205474854\n",
      "epoch 4767, loss 0.012557736597955227, R2 0.47802436351776123\n",
      "Eval loss 0.012572230771183968, R2 0.515730619430542\n",
      "epoch 4768, loss 0.01255758572369814, R2 0.47803062200546265\n",
      "Eval loss 0.01257206778973341, R2 0.5157369375228882\n",
      "epoch 4769, loss 0.0125574367120862, R2 0.4780368208885193\n",
      "Eval loss 0.012571904808282852, R2 0.515743613243103\n",
      "epoch 4770, loss 0.012557288631796837, R2 0.4780430197715759\n",
      "Eval loss 0.012571743689477444, R2 0.5157493948936462\n",
      "epoch 4771, loss 0.012557136826217175, R2 0.4780498147010803\n",
      "Eval loss 0.012571582570672035, R2 0.5157560110092163\n",
      "epoch 4772, loss 0.012556987814605236, R2 0.47805601358413696\n",
      "Eval loss 0.012571420520544052, R2 0.515762448310852\n",
      "epoch 4773, loss 0.012556839734315872, R2 0.4780616760253906\n",
      "Eval loss 0.012571259401738644, R2 0.5157681703567505\n",
      "epoch 4774, loss 0.012556689791381359, R2 0.47806787490844727\n",
      "Eval loss 0.01257109735161066, R2 0.515774130821228\n",
      "epoch 4775, loss 0.012556541711091995, R2 0.4780738949775696\n",
      "Eval loss 0.012570937164127827, R2 0.5157809257507324\n",
      "epoch 4776, loss 0.012556393630802631, R2 0.4780803918838501\n",
      "Eval loss 0.012570776045322418, R2 0.5157869458198547\n",
      "epoch 4777, loss 0.012556242756545544, R2 0.4780871272087097\n",
      "Eval loss 0.012570616789162159, R2 0.5157932043075562\n",
      "epoch 4778, loss 0.01255609467625618, R2 0.47809261083602905\n",
      "Eval loss 0.012570454739034176, R2 0.5157995223999023\n",
      "epoch 4779, loss 0.01255594752728939, R2 0.4780992269515991\n",
      "Eval loss 0.012570293620228767, R2 0.5158055424690247\n",
      "epoch 4780, loss 0.012555797584354877, R2 0.47810494899749756\n",
      "Eval loss 0.012570131570100784, R2 0.5158116817474365\n",
      "epoch 4781, loss 0.012555648572742939, R2 0.47811150550842285\n",
      "Eval loss 0.012569972313940525, R2 0.5158176422119141\n",
      "epoch 4782, loss 0.012555500492453575, R2 0.47811752557754517\n",
      "Eval loss 0.012569813057780266, R2 0.5158241987228394\n",
      "epoch 4783, loss 0.012555352412164211, R2 0.4781239628791809\n",
      "Eval loss 0.012569652870297432, R2 0.5158298015594482\n",
      "epoch 4784, loss 0.012555205263197422, R2 0.4781295657157898\n",
      "Eval loss 0.012569491751492023, R2 0.5158362984657288\n",
      "epoch 4785, loss 0.012555055320262909, R2 0.4781358242034912\n",
      "Eval loss 0.01256932970136404, R2 0.5158426761627197\n",
      "epoch 4786, loss 0.012554907239973545, R2 0.47814202308654785\n",
      "Eval loss 0.012569171376526356, R2 0.5158487558364868\n",
      "epoch 4787, loss 0.012554760091006756, R2 0.47814810276031494\n",
      "Eval loss 0.012569011189043522, R2 0.5158549547195435\n",
      "epoch 4788, loss 0.012554612942039967, R2 0.47815388441085815\n",
      "Eval loss 0.012568849138915539, R2 0.5158610343933105\n",
      "epoch 4789, loss 0.012554463930428028, R2 0.4781600832939148\n",
      "Eval loss 0.01256868988275528, R2 0.5158669948577881\n",
      "epoch 4790, loss 0.012554316781461239, R2 0.47816717624664307\n",
      "Eval loss 0.012568528763949871, R2 0.5158736705780029\n",
      "epoch 4791, loss 0.01255416963249445, R2 0.4781731367111206\n",
      "Eval loss 0.012568370439112186, R2 0.5158796310424805\n",
      "epoch 4792, loss 0.01255402248352766, R2 0.4781787395477295\n",
      "Eval loss 0.012568211182951927, R2 0.515885591506958\n",
      "epoch 4793, loss 0.012553874403238297, R2 0.47818493843078613\n",
      "Eval loss 0.012568051926791668, R2 0.5158915519714355\n",
      "epoch 4794, loss 0.012553727254271507, R2 0.4781910181045532\n",
      "Eval loss 0.012567892670631409, R2 0.5158981084823608\n",
      "epoch 4795, loss 0.012553579173982143, R2 0.4781971573829651\n",
      "Eval loss 0.012567734345793724, R2 0.5159042477607727\n",
      "epoch 4796, loss 0.012553432025015354, R2 0.47820329666137695\n",
      "Eval loss 0.012567575089633465, R2 0.515910267829895\n",
      "epoch 4797, loss 0.01255328580737114, R2 0.47820955514907837\n",
      "Eval loss 0.012567415833473206, R2 0.515916109085083\n",
      "epoch 4798, loss 0.012553139589726925, R2 0.47821545600891113\n",
      "Eval loss 0.012567255645990372, R2 0.5159222483634949\n",
      "epoch 4799, loss 0.012552991509437561, R2 0.478221595287323\n",
      "Eval loss 0.012567097321152687, R2 0.5159289240837097\n",
      "epoch 4800, loss 0.012552845291793346, R2 0.4782276749610901\n",
      "Eval loss 0.012566938064992428, R2 0.5159344673156738\n",
      "epoch 4801, loss 0.012552700005471706, R2 0.4782337546348572\n",
      "Eval loss 0.012566779740154743, R2 0.5159405469894409\n",
      "epoch 4802, loss 0.012552552856504917, R2 0.4782400131225586\n",
      "Eval loss 0.012566621415317059, R2 0.5159475803375244\n",
      "epoch 4803, loss 0.012552405707538128, R2 0.47824662923812866\n",
      "Eval loss 0.012566463090479374, R2 0.5159530639648438\n",
      "epoch 4804, loss 0.012552259489893913, R2 0.47825223207473755\n",
      "Eval loss 0.01256630476564169, R2 0.5159591436386108\n",
      "epoch 4805, loss 0.012552112340927124, R2 0.47825849056243896\n",
      "Eval loss 0.012566146440804005, R2 0.5159655809402466\n",
      "epoch 4806, loss 0.012551967054605484, R2 0.4782642126083374\n",
      "Eval loss 0.01256598811596632, R2 0.5159710645675659\n",
      "epoch 4807, loss 0.01255182083696127, R2 0.4782704710960388\n",
      "Eval loss 0.012565829791128635, R2 0.515977144241333\n",
      "epoch 4808, loss 0.012551674619317055, R2 0.47827649116516113\n",
      "Eval loss 0.01256567146629095, R2 0.5159835815429688\n",
      "epoch 4809, loss 0.012551529332995415, R2 0.4782825708389282\n",
      "Eval loss 0.012565515004098415, R2 0.5159894824028015\n",
      "epoch 4810, loss 0.012551384046673775, R2 0.4782884120941162\n",
      "Eval loss 0.01256535667926073, R2 0.515995442867279\n",
      "epoch 4811, loss 0.01255123596638441, R2 0.4782945513725281\n",
      "Eval loss 0.01256519928574562, R2 0.5160016417503357\n",
      "epoch 4812, loss 0.01255109254270792, R2 0.4783007502555847\n",
      "Eval loss 0.012565040960907936, R2 0.5160076022148132\n",
      "epoch 4813, loss 0.012550946325063705, R2 0.47830677032470703\n",
      "Eval loss 0.0125648844987154, R2 0.516014039516449\n",
      "epoch 4814, loss 0.012550801038742065, R2 0.478313148021698\n",
      "Eval loss 0.012564726173877716, R2 0.5160199999809265\n",
      "epoch 4815, loss 0.012550655752420425, R2 0.4783186912536621\n",
      "Eval loss 0.012564568780362606, R2 0.5160260200500488\n",
      "epoch 4816, loss 0.01255050953477621, R2 0.4783249497413635\n",
      "Eval loss 0.012564413249492645, R2 0.5160322189331055\n",
      "epoch 4817, loss 0.01255036424845457, R2 0.4783307909965515\n",
      "Eval loss 0.01256425492465496, R2 0.5160378217697144\n",
      "epoch 4818, loss 0.01255022082477808, R2 0.47833746671676636\n",
      "Eval loss 0.012564098462462425, R2 0.516044020652771\n",
      "epoch 4819, loss 0.012550074607133865, R2 0.47834300994873047\n",
      "Eval loss 0.012563941068947315, R2 0.5160503387451172\n",
      "epoch 4820, loss 0.0125499302521348, R2 0.4783490300178528\n",
      "Eval loss 0.012563783675432205, R2 0.5160564184188843\n",
      "epoch 4821, loss 0.01254978496581316, R2 0.47835540771484375\n",
      "Eval loss 0.012563626281917095, R2 0.5160624980926514\n",
      "epoch 4822, loss 0.01254963967949152, R2 0.47836095094680786\n",
      "Eval loss 0.012563470751047134, R2 0.5160682201385498\n",
      "epoch 4823, loss 0.012549495324492455, R2 0.4783669114112854\n",
      "Eval loss 0.012563313357532024, R2 0.5160744190216064\n",
      "epoch 4824, loss 0.012549350969493389, R2 0.4783734679222107\n",
      "Eval loss 0.012563157826662064, R2 0.516080379486084\n",
      "epoch 4825, loss 0.012549205683171749, R2 0.4783793091773987\n",
      "Eval loss 0.012563001364469528, R2 0.5160864591598511\n",
      "epoch 4826, loss 0.012549063190817833, R2 0.4783852696418762\n",
      "Eval loss 0.012562844902276993, R2 0.5160923004150391\n",
      "epoch 4827, loss 0.012548917904496193, R2 0.4783911108970642\n",
      "Eval loss 0.012562688440084457, R2 0.516098141670227\n",
      "epoch 4828, loss 0.012548772618174553, R2 0.4783971309661865\n",
      "Eval loss 0.012562531977891922, R2 0.5161046385765076\n",
      "epoch 4829, loss 0.012548629194498062, R2 0.4784034490585327\n",
      "Eval loss 0.012562376447021961, R2 0.5161106586456299\n",
      "epoch 4830, loss 0.012548484839498997, R2 0.4784095883369446\n",
      "Eval loss 0.012562219984829426, R2 0.5161168575286865\n",
      "epoch 4831, loss 0.012548339553177357, R2 0.47841495275497437\n",
      "Eval loss 0.012562064453959465, R2 0.5161226987838745\n",
      "epoch 4832, loss 0.01254819706082344, R2 0.4784209132194519\n",
      "Eval loss 0.012561908923089504, R2 0.5161287784576416\n",
      "epoch 4833, loss 0.01254805363714695, R2 0.478426992893219\n",
      "Eval loss 0.012561754323542118, R2 0.5161346197128296\n",
      "epoch 4834, loss 0.012547909282147884, R2 0.4784325361251831\n",
      "Eval loss 0.012561597861349583, R2 0.5161402225494385\n",
      "epoch 4835, loss 0.012547765858471394, R2 0.4784388542175293\n",
      "Eval loss 0.012561442330479622, R2 0.5161464810371399\n",
      "epoch 4836, loss 0.012547622434794903, R2 0.4784449338912964\n",
      "Eval loss 0.012561287730932236, R2 0.5161521434783936\n",
      "epoch 4837, loss 0.012547479011118412, R2 0.4784507751464844\n",
      "Eval loss 0.012561132200062275, R2 0.5161585807800293\n",
      "epoch 4838, loss 0.012547335587441921, R2 0.4784572124481201\n",
      "Eval loss 0.012560976669192314, R2 0.5161641240119934\n",
      "epoch 4839, loss 0.012547191232442856, R2 0.4784628748893738\n",
      "Eval loss 0.012560820206999779, R2 0.5161704421043396\n",
      "epoch 4840, loss 0.012547049671411514, R2 0.47846925258636475\n",
      "Eval loss 0.012560667470097542, R2 0.5161763429641724\n",
      "epoch 4841, loss 0.012546906247735023, R2 0.47847485542297363\n",
      "Eval loss 0.012560511939227581, R2 0.5161824822425842\n",
      "epoch 4842, loss 0.012546763755381107, R2 0.4784805178642273\n",
      "Eval loss 0.012560357339680195, R2 0.5161881446838379\n",
      "epoch 4843, loss 0.012546619400382042, R2 0.47848665714263916\n",
      "Eval loss 0.012560202740132809, R2 0.516194224357605\n",
      "epoch 4844, loss 0.012546476908028126, R2 0.47849202156066895\n",
      "Eval loss 0.012560048140585423, R2 0.5161998867988586\n",
      "epoch 4845, loss 0.012546335346996784, R2 0.47849828004837036\n",
      "Eval loss 0.012559892609715462, R2 0.5162063241004944\n",
      "epoch 4846, loss 0.012546191923320293, R2 0.4785042405128479\n",
      "Eval loss 0.012559738010168076, R2 0.5162118673324585\n",
      "epoch 4847, loss 0.012546049430966377, R2 0.478510320186615\n",
      "Eval loss 0.012559584341943264, R2 0.5162184238433838\n",
      "epoch 4848, loss 0.012545906938612461, R2 0.4785161018371582\n",
      "Eval loss 0.012559430673718452, R2 0.5162240862846375\n",
      "epoch 4849, loss 0.01254576537758112, R2 0.47852200269699097\n",
      "Eval loss 0.012559277005493641, R2 0.5162298679351807\n",
      "epoch 4850, loss 0.012545621953904629, R2 0.4785279631614685\n",
      "Eval loss 0.012559122405946255, R2 0.5162358283996582\n",
      "epoch 4851, loss 0.012545480392873287, R2 0.47853362560272217\n",
      "Eval loss 0.012558968737721443, R2 0.5162417888641357\n",
      "epoch 4852, loss 0.012545338831841946, R2 0.47853970527648926\n",
      "Eval loss 0.012558815069496632, R2 0.5162478685379028\n",
      "epoch 4853, loss 0.01254519633948803, R2 0.478545606136322\n",
      "Eval loss 0.01255866140127182, R2 0.5162537097930908\n",
      "epoch 4854, loss 0.012545053847134113, R2 0.4785517454147339\n",
      "Eval loss 0.012558506801724434, R2 0.5162595510482788\n",
      "epoch 4855, loss 0.012544912286102772, R2 0.4785577654838562\n",
      "Eval loss 0.012558353133499622, R2 0.5162654519081116\n",
      "epoch 4856, loss 0.012544769793748856, R2 0.4785640239715576\n",
      "Eval loss 0.01255819946527481, R2 0.5162713527679443\n",
      "epoch 4857, loss 0.012544628232717514, R2 0.4785690903663635\n",
      "Eval loss 0.012558046728372574, R2 0.5162773132324219\n",
      "epoch 4858, loss 0.012544487603008747, R2 0.47857505083084106\n",
      "Eval loss 0.012557893991470337, R2 0.5162829160690308\n",
      "epoch 4859, loss 0.012544346041977406, R2 0.4785808324813843\n",
      "Eval loss 0.012557740323245525, R2 0.5162887573242188\n",
      "epoch 4860, loss 0.01254420354962349, R2 0.47858673334121704\n",
      "Eval loss 0.012557587586343288, R2 0.5162948369979858\n",
      "epoch 4861, loss 0.012544062919914722, R2 0.478593111038208\n",
      "Eval loss 0.012557433918118477, R2 0.5163010358810425\n",
      "epoch 4862, loss 0.012543920427560806, R2 0.4785991311073303\n",
      "Eval loss 0.01255728118121624, R2 0.5163066387176514\n",
      "epoch 4863, loss 0.01254377979785204, R2 0.4786050319671631\n",
      "Eval loss 0.012557130306959152, R2 0.5163127183914185\n",
      "epoch 4864, loss 0.012543638236820698, R2 0.4786105751991272\n",
      "Eval loss 0.01255697663873434, R2 0.516318678855896\n",
      "epoch 4865, loss 0.01254349760711193, R2 0.47861671447753906\n",
      "Eval loss 0.012556824833154678, R2 0.5163243412971497\n",
      "epoch 4866, loss 0.012543356977403164, R2 0.47862207889556885\n",
      "Eval loss 0.012556671164929867, R2 0.5163302421569824\n",
      "epoch 4867, loss 0.012543215416371822, R2 0.4786279797554016\n",
      "Eval loss 0.01255651842802763, R2 0.5163359642028809\n",
      "epoch 4868, loss 0.012543076649308205, R2 0.4786337614059448\n",
      "Eval loss 0.012556366622447968, R2 0.5163416862487793\n",
      "epoch 4869, loss 0.012542934156954288, R2 0.4786398410797119\n",
      "Eval loss 0.012556212954223156, R2 0.5163478851318359\n",
      "epoch 4870, loss 0.012542794458568096, R2 0.4786454439163208\n",
      "Eval loss 0.012556063011288643, R2 0.5163537263870239\n",
      "epoch 4871, loss 0.012542652897536755, R2 0.47865134477615356\n",
      "Eval loss 0.012555909343063831, R2 0.5163594484329224\n",
      "epoch 4872, loss 0.012542514130473137, R2 0.4786571264266968\n",
      "Eval loss 0.012555757537484169, R2 0.5163654088973999\n",
      "epoch 4873, loss 0.01254237350076437, R2 0.47866344451904297\n",
      "Eval loss 0.012555604800581932, R2 0.5163711905479431\n",
      "epoch 4874, loss 0.012542234733700752, R2 0.47866857051849365\n",
      "Eval loss 0.012555453926324844, R2 0.5163769721984863\n",
      "epoch 4875, loss 0.01254209317266941, R2 0.4786747694015503\n",
      "Eval loss 0.012555303052067757, R2 0.5163828134536743\n",
      "epoch 4876, loss 0.012541952542960644, R2 0.47868067026138306\n",
      "Eval loss 0.012555151246488094, R2 0.5163887739181519\n",
      "epoch 4877, loss 0.012541813775897026, R2 0.47868621349334717\n",
      "Eval loss 0.012554999440908432, R2 0.5163947939872742\n",
      "epoch 4878, loss 0.012541673146188259, R2 0.47869259119033813\n",
      "Eval loss 0.01255484763532877, R2 0.5164004564285278\n",
      "epoch 4879, loss 0.012541533447802067, R2 0.47869837284088135\n",
      "Eval loss 0.012554696761071682, R2 0.5164060592651367\n",
      "epoch 4880, loss 0.0125413928180933, R2 0.4787038564682007\n",
      "Eval loss 0.012554546818137169, R2 0.5164122581481934\n",
      "epoch 4881, loss 0.012541254051029682, R2 0.47870951890945435\n",
      "Eval loss 0.012554394081234932, R2 0.5164179801940918\n",
      "epoch 4882, loss 0.012541113421320915, R2 0.4787154793739319\n",
      "Eval loss 0.012554243206977844, R2 0.5164236426353455\n",
      "epoch 4883, loss 0.012540976516902447, R2 0.478721022605896\n",
      "Eval loss 0.012554092332720757, R2 0.5164294242858887\n",
      "epoch 4884, loss 0.01254083402454853, R2 0.4787275791168213\n",
      "Eval loss 0.012553941458463669, R2 0.5164353847503662\n",
      "epoch 4885, loss 0.012540696188807487, R2 0.4787328243255615\n",
      "Eval loss 0.012553788721561432, R2 0.5164414048194885\n",
      "epoch 4886, loss 0.01254055742174387, R2 0.4787384271621704\n",
      "Eval loss 0.012553640641272068, R2 0.5164468288421631\n",
      "epoch 4887, loss 0.012540417723357677, R2 0.4787445664405823\n",
      "Eval loss 0.01255348976701498, R2 0.5164525508880615\n",
      "epoch 4888, loss 0.01254027895629406, R2 0.47874999046325684\n",
      "Eval loss 0.012553339824080467, R2 0.51645827293396\n",
      "epoch 4889, loss 0.012540140189230442, R2 0.47875577211380005\n",
      "Eval loss 0.012553188018500805, R2 0.5164644122123718\n",
      "epoch 4890, loss 0.01254000049084425, R2 0.47876155376434326\n",
      "Eval loss 0.012553039006888866, R2 0.5164703130722046\n",
      "epoch 4891, loss 0.012539861723780632, R2 0.47876739501953125\n",
      "Eval loss 0.012552889063954353, R2 0.5164759159088135\n",
      "epoch 4892, loss 0.012539722956717014, R2 0.4787731170654297\n",
      "Eval loss 0.012552737258374691, R2 0.5164817571640015\n",
      "epoch 4893, loss 0.012539584189653397, R2 0.47877955436706543\n",
      "Eval loss 0.012552588246762753, R2 0.5164874792098999\n",
      "epoch 4894, loss 0.012539445422589779, R2 0.47878479957580566\n",
      "Eval loss 0.01255243644118309, R2 0.5164936780929565\n",
      "epoch 4895, loss 0.012539307586848736, R2 0.47879040241241455\n",
      "Eval loss 0.012552288360893726, R2 0.5164991021156311\n",
      "epoch 4896, loss 0.012539170682430267, R2 0.47879642248153687\n",
      "Eval loss 0.012552138417959213, R2 0.5165050029754639\n",
      "epoch 4897, loss 0.01253903191536665, R2 0.47880232334136963\n",
      "Eval loss 0.012551989406347275, R2 0.5165104866027832\n",
      "epoch 4898, loss 0.012538894079625607, R2 0.47880762815475464\n",
      "Eval loss 0.012551838532090187, R2 0.5165162682533264\n",
      "epoch 4899, loss 0.012538755312561989, R2 0.4788135290145874\n",
      "Eval loss 0.012551689520478249, R2 0.5165222883224487\n",
      "epoch 4900, loss 0.012538619339466095, R2 0.47881901264190674\n",
      "Eval loss 0.01255154050886631, R2 0.5165280103683472\n",
      "epoch 4901, loss 0.012538478709757328, R2 0.47882503271102905\n",
      "Eval loss 0.012551390565931797, R2 0.5165334939956665\n",
      "epoch 4902, loss 0.01253834180533886, R2 0.4788312315940857\n",
      "Eval loss 0.012551240622997284, R2 0.516539454460144\n",
      "epoch 4903, loss 0.012538203969597816, R2 0.4788365960121155\n",
      "Eval loss 0.012551093474030495, R2 0.5165451169013977\n",
      "epoch 4904, loss 0.012538067065179348, R2 0.4788421392440796\n",
      "Eval loss 0.012550943531095982, R2 0.5165507793426514\n",
      "epoch 4905, loss 0.01253792829811573, R2 0.4788477420806885\n",
      "Eval loss 0.012550795450806618, R2 0.516556441783905\n",
      "epoch 4906, loss 0.012537791393697262, R2 0.47885358333587646\n",
      "Eval loss 0.01255064457654953, R2 0.5165622234344482\n",
      "epoch 4907, loss 0.012537653557956219, R2 0.4788591265678406\n",
      "Eval loss 0.01255049742758274, R2 0.5165680646896362\n",
      "epoch 4908, loss 0.01253751665353775, R2 0.47886568307876587\n",
      "Eval loss 0.012550348415970802, R2 0.5165737867355347\n",
      "epoch 4909, loss 0.012537378817796707, R2 0.47887122631073\n",
      "Eval loss 0.01255019847303629, R2 0.5165795683860779\n",
      "epoch 4910, loss 0.012537241913378239, R2 0.47887706756591797\n",
      "Eval loss 0.0125500513240695, R2 0.5165852308273315\n",
      "epoch 4911, loss 0.01253710500895977, R2 0.47888195514678955\n",
      "Eval loss 0.012549903243780136, R2 0.5165910720825195\n",
      "epoch 4912, loss 0.012536968104541302, R2 0.4788876175880432\n",
      "Eval loss 0.012549755163490772, R2 0.5165961980819702\n",
      "epoch 4913, loss 0.012536832131445408, R2 0.478892982006073\n",
      "Eval loss 0.012549606151878834, R2 0.516602098941803\n",
      "epoch 4914, loss 0.01253669336438179, R2 0.4788992404937744\n",
      "Eval loss 0.01254945807158947, R2 0.5166082382202148\n",
      "epoch 4915, loss 0.012536557391285896, R2 0.4789049029350281\n",
      "Eval loss 0.012549309991300106, R2 0.5166139602661133\n",
      "epoch 4916, loss 0.012536420486867428, R2 0.4789108633995056\n",
      "Eval loss 0.012549161911010742, R2 0.5166195034980774\n",
      "epoch 4917, loss 0.012536284513771534, R2 0.4789160490036011\n",
      "Eval loss 0.012549012899398804, R2 0.5166254043579102\n",
      "epoch 4918, loss 0.012536147609353065, R2 0.4789220690727234\n",
      "Eval loss 0.012548867613077164, R2 0.5166308879852295\n",
      "epoch 4919, loss 0.012536010704934597, R2 0.47892725467681885\n",
      "Eval loss 0.012548718601465225, R2 0.516636848449707\n",
      "epoch 4920, loss 0.012535875663161278, R2 0.47893303632736206\n",
      "Eval loss 0.01254857238382101, R2 0.5166422128677368\n",
      "epoch 4921, loss 0.012535739690065384, R2 0.4789386987686157\n",
      "Eval loss 0.012548423372209072, R2 0.5166479349136353\n",
      "epoch 4922, loss 0.012535602785646915, R2 0.4789445400238037\n",
      "Eval loss 0.012548276223242283, R2 0.5166535377502441\n",
      "epoch 4923, loss 0.012535466812551022, R2 0.47895002365112305\n",
      "Eval loss 0.012548129074275494, R2 0.5166589021682739\n",
      "epoch 4924, loss 0.012535329908132553, R2 0.4789557456970215\n",
      "Eval loss 0.012547981925308704, R2 0.516664981842041\n",
      "epoch 4925, loss 0.012535195797681808, R2 0.4789620041847229\n",
      "Eval loss 0.012547834776341915, R2 0.516670823097229\n",
      "epoch 4926, loss 0.012535057961940765, R2 0.47896701097488403\n",
      "Eval loss 0.012547687627375126, R2 0.5166760087013245\n",
      "epoch 4927, loss 0.01253492385149002, R2 0.4789731502532959\n",
      "Eval loss 0.012547540478408337, R2 0.5166820883750916\n",
      "epoch 4928, loss 0.012534787878394127, R2 0.47897839546203613\n",
      "Eval loss 0.012547392398118973, R2 0.5166876316070557\n",
      "epoch 4929, loss 0.012534652836620808, R2 0.47898387908935547\n",
      "Eval loss 0.012547247111797333, R2 0.5166932344436646\n",
      "epoch 4930, loss 0.01253451593220234, R2 0.47898972034454346\n",
      "Eval loss 0.012547099962830544, R2 0.5166987776756287\n",
      "epoch 4931, loss 0.01253438089042902, R2 0.478995680809021\n",
      "Eval loss 0.012546953745186329, R2 0.5167046785354614\n",
      "epoch 4932, loss 0.0125342458486557, R2 0.479000985622406\n",
      "Eval loss 0.012546807527542114, R2 0.5167103409767151\n",
      "epoch 4933, loss 0.012534110806882381, R2 0.47900640964508057\n",
      "Eval loss 0.012546662241220474, R2 0.516715943813324\n",
      "epoch 4934, loss 0.012533975765109062, R2 0.479012668132782\n",
      "Eval loss 0.012546515092253685, R2 0.5167213082313538\n",
      "epoch 4935, loss 0.012533840723335743, R2 0.47901761531829834\n",
      "Eval loss 0.012546367943286896, R2 0.5167269706726074\n",
      "epoch 4936, loss 0.012533704750239849, R2 0.4790237545967102\n",
      "Eval loss 0.012546222656965256, R2 0.5167323350906372\n",
      "epoch 4937, loss 0.012533570639789104, R2 0.4790290594100952\n",
      "Eval loss 0.012546074576675892, R2 0.5167384147644043\n",
      "epoch 4938, loss 0.01253343652933836, R2 0.4790349006652832\n",
      "Eval loss 0.012545929290354252, R2 0.5167437791824341\n",
      "epoch 4939, loss 0.012533302418887615, R2 0.4790400266647339\n",
      "Eval loss 0.012545784004032612, R2 0.516749918460846\n",
      "epoch 4940, loss 0.012533165514469147, R2 0.47904568910598755\n",
      "Eval loss 0.012545638717710972, R2 0.5167549252510071\n",
      "epoch 4941, loss 0.012533031404018402, R2 0.4790511131286621\n",
      "Eval loss 0.012545491568744183, R2 0.5167607665061951\n",
      "epoch 4942, loss 0.012532895430922508, R2 0.47905677556991577\n",
      "Eval loss 0.012545346282422543, R2 0.5167661905288696\n",
      "epoch 4943, loss 0.012532762251794338, R2 0.4790622591972351\n",
      "Eval loss 0.012545200064778328, R2 0.5167720913887024\n",
      "epoch 4944, loss 0.012532627210021019, R2 0.4790681004524231\n",
      "Eval loss 0.012545055709779263, R2 0.5167779922485352\n",
      "epoch 4945, loss 0.012532494030892849, R2 0.4790736436843872\n",
      "Eval loss 0.012544910423457623, R2 0.5167831182479858\n",
      "epoch 4946, loss 0.012532359920442104, R2 0.479079008102417\n",
      "Eval loss 0.012544764205813408, R2 0.5167890787124634\n",
      "epoch 4947, loss 0.012532226741313934, R2 0.47908490896224976\n",
      "Eval loss 0.012544620782136917, R2 0.5167944431304932\n",
      "epoch 4948, loss 0.01253209076821804, R2 0.47909003496170044\n",
      "Eval loss 0.012544474564492702, R2 0.516800045967102\n",
      "epoch 4949, loss 0.01253195758908987, R2 0.4790964126586914\n",
      "Eval loss 0.012544329278171062, R2 0.5168056488037109\n",
      "epoch 4950, loss 0.0125318244099617, R2 0.4791014790534973\n",
      "Eval loss 0.012544184923171997, R2 0.5168112516403198\n",
      "epoch 4951, loss 0.012531689368188381, R2 0.47910720109939575\n",
      "Eval loss 0.012544039636850357, R2 0.5168169736862183\n",
      "epoch 4952, loss 0.012531556189060211, R2 0.4791126251220703\n",
      "Eval loss 0.012543894350528717, R2 0.5168222188949585\n",
      "epoch 4953, loss 0.012531423941254616, R2 0.4791184663772583\n",
      "Eval loss 0.012543749995529652, R2 0.5168277025222778\n",
      "epoch 4954, loss 0.012531289830803871, R2 0.47912365198135376\n",
      "Eval loss 0.012543605640530586, R2 0.5168336629867554\n",
      "epoch 4955, loss 0.012531156651675701, R2 0.4791293740272522\n",
      "Eval loss 0.01254346128553152, R2 0.5168391466140747\n",
      "epoch 4956, loss 0.012531021609902382, R2 0.47913479804992676\n",
      "Eval loss 0.01254331599920988, R2 0.5168448686599731\n",
      "epoch 4957, loss 0.012530889362096786, R2 0.4791402816772461\n",
      "Eval loss 0.01254317257553339, R2 0.5168502330780029\n",
      "epoch 4958, loss 0.012530757114291191, R2 0.47914600372314453\n",
      "Eval loss 0.0125430291518569, R2 0.5168557167053223\n",
      "epoch 4959, loss 0.012530623003840446, R2 0.4791516661643982\n",
      "Eval loss 0.01254288386553526, R2 0.5168614387512207\n",
      "epoch 4960, loss 0.012530490756034851, R2 0.47915685176849365\n",
      "Eval loss 0.012542740441858768, R2 0.5168668031692505\n",
      "epoch 4961, loss 0.012530357576906681, R2 0.47916245460510254\n",
      "Eval loss 0.012542597018182278, R2 0.5168724060058594\n",
      "epoch 4962, loss 0.012530224397778511, R2 0.4791679382324219\n",
      "Eval loss 0.012542450800538063, R2 0.5168777704238892\n",
      "epoch 4963, loss 0.012530091218650341, R2 0.4791739583015442\n",
      "Eval loss 0.012542309239506721, R2 0.5168834924697876\n",
      "epoch 4964, loss 0.01252995990216732, R2 0.4791789650917053\n",
      "Eval loss 0.012542163953185081, R2 0.5168887972831726\n",
      "epoch 4965, loss 0.01252982672303915, R2 0.4791852831840515\n",
      "Eval loss 0.01254202052950859, R2 0.5168942809104919\n",
      "epoch 4966, loss 0.012529692612588406, R2 0.47919005155563354\n",
      "Eval loss 0.0125418771058321, R2 0.516899824142456\n",
      "epoch 4967, loss 0.01252956222742796, R2 0.4791954755783081\n",
      "Eval loss 0.012541733682155609, R2 0.5169057846069336\n",
      "epoch 4968, loss 0.012529428116977215, R2 0.47920119762420654\n",
      "Eval loss 0.012541591189801693, R2 0.5169111490249634\n",
      "epoch 4969, loss 0.012529296800494194, R2 0.4792068600654602\n",
      "Eval loss 0.012541446834802628, R2 0.5169163942337036\n",
      "epoch 4970, loss 0.012529165484011173, R2 0.4792121648788452\n",
      "Eval loss 0.012541303411126137, R2 0.5169220566749573\n",
      "epoch 4971, loss 0.012529031373560429, R2 0.479217529296875\n",
      "Eval loss 0.012541161850094795, R2 0.5169273614883423\n",
      "epoch 4972, loss 0.012528900988399982, R2 0.4792231321334839\n",
      "Eval loss 0.01254101749509573, R2 0.5169332027435303\n",
      "epoch 4973, loss 0.012528768740594387, R2 0.4792284369468689\n",
      "Eval loss 0.012540875934064388, R2 0.5169388055801392\n",
      "epoch 4974, loss 0.012528635561466217, R2 0.47923415899276733\n",
      "Eval loss 0.012540733441710472, R2 0.5169442892074585\n",
      "epoch 4975, loss 0.012528504244983196, R2 0.4792401194572449\n",
      "Eval loss 0.012540590018033981, R2 0.5169498324394226\n",
      "epoch 4976, loss 0.012528372928500175, R2 0.4792448878288269\n",
      "Eval loss 0.01254044659435749, R2 0.5169552564620972\n",
      "epoch 4977, loss 0.012528239749372005, R2 0.47925060987472534\n",
      "Eval loss 0.012540304102003574, R2 0.5169609785079956\n",
      "epoch 4978, loss 0.01252810936421156, R2 0.4792560338973999\n",
      "Eval loss 0.012540160678327084, R2 0.5169663429260254\n",
      "epoch 4979, loss 0.012527978047728539, R2 0.4792618155479431\n",
      "Eval loss 0.012540019117295742, R2 0.5169717073440552\n",
      "epoch 4980, loss 0.012527847662568092, R2 0.4792667627334595\n",
      "Eval loss 0.012539875693619251, R2 0.516977071762085\n",
      "epoch 4981, loss 0.012527715414762497, R2 0.4792722463607788\n",
      "Eval loss 0.01253973413258791, R2 0.5169825553894043\n",
      "epoch 4982, loss 0.012527584098279476, R2 0.4792773723602295\n",
      "Eval loss 0.012539594434201717, R2 0.5169880390167236\n",
      "epoch 4983, loss 0.012527452781796455, R2 0.4792831540107727\n",
      "Eval loss 0.012539451941847801, R2 0.516993522644043\n",
      "epoch 4984, loss 0.012527321465313435, R2 0.47928857803344727\n",
      "Eval loss 0.012539307586848736, R2 0.5169987678527832\n",
      "epoch 4985, loss 0.012527191080152988, R2 0.47929418087005615\n",
      "Eval loss 0.01253916509449482, R2 0.5170042514801025\n",
      "epoch 4986, loss 0.012527059763669968, R2 0.4793001413345337\n",
      "Eval loss 0.012539024464786053, R2 0.5170098543167114\n",
      "epoch 4987, loss 0.01252693124115467, R2 0.47930532693862915\n",
      "Eval loss 0.012538882903754711, R2 0.5170155763626099\n",
      "epoch 4988, loss 0.0125267980620265, R2 0.4793105125427246\n",
      "Eval loss 0.012538742274045944, R2 0.5170210003852844\n",
      "epoch 4989, loss 0.01252666860818863, R2 0.4793164134025574\n",
      "Eval loss 0.012538598850369453, R2 0.5170265436172485\n",
      "epoch 4990, loss 0.012526538223028183, R2 0.4793211817741394\n",
      "Eval loss 0.012538459151983261, R2 0.5170317888259888\n",
      "epoch 4991, loss 0.012526406906545162, R2 0.4793267846107483\n",
      "Eval loss 0.01253831759095192, R2 0.517037034034729\n",
      "epoch 4992, loss 0.012526275590062141, R2 0.4793320894241333\n",
      "Eval loss 0.012538176961243153, R2 0.5170426368713379\n",
      "epoch 4993, loss 0.01252614613622427, R2 0.4793371558189392\n",
      "Eval loss 0.012538035400211811, R2 0.5170482397079468\n",
      "epoch 4994, loss 0.012526017613708973, R2 0.47934281826019287\n",
      "Eval loss 0.012537892907857895, R2 0.5170532464981079\n",
      "epoch 4995, loss 0.012525884434580803, R2 0.479348361492157\n",
      "Eval loss 0.012537752278149128, R2 0.5170591473579407\n",
      "epoch 4996, loss 0.012525755912065506, R2 0.47935450077056885\n",
      "Eval loss 0.012537611648440361, R2 0.5170642733573914\n",
      "epoch 4997, loss 0.012525627389550209, R2 0.4793592095375061\n",
      "Eval loss 0.01253747008740902, R2 0.5170698761940002\n",
      "epoch 4998, loss 0.012525495141744614, R2 0.47936415672302246\n",
      "Eval loss 0.012537330389022827, R2 0.51707524061203\n",
      "epoch 4999, loss 0.012525365687906742, R2 0.4793700575828552\n",
      "Eval loss 0.01253718975931406, R2 0.5170806646347046\n",
      "epoch 5000, loss 0.012525235302746296, R2 0.4793761372566223\n",
      "Eval loss 0.012537048198282719, R2 0.5170857906341553\n",
      "epoch 5001, loss 0.012525106780230999, R2 0.47938066720962524\n",
      "Eval loss 0.012536909431219101, R2 0.5170911550521851\n",
      "epoch 5002, loss 0.012524976395070553, R2 0.479386568069458\n",
      "Eval loss 0.01253676787018776, R2 0.5170966386795044\n",
      "epoch 5003, loss 0.012524846941232681, R2 0.4793914556503296\n",
      "Eval loss 0.012536627240478992, R2 0.517102062702179\n",
      "epoch 5004, loss 0.01252471748739481, R2 0.47939687967300415\n",
      "Eval loss 0.0125364875420928, R2 0.5171074271202087\n",
      "epoch 5005, loss 0.012524588033556938, R2 0.47940224409103394\n",
      "Eval loss 0.012536345981061459, R2 0.5171129703521729\n",
      "epoch 5006, loss 0.012524458579719067, R2 0.4794076085090637\n",
      "Eval loss 0.012536206282675266, R2 0.5171189308166504\n",
      "epoch 5007, loss 0.01252433005720377, R2 0.4794134497642517\n",
      "Eval loss 0.0125360656529665, R2 0.5171242356300354\n",
      "epoch 5008, loss 0.012524200603365898, R2 0.4794183373451233\n",
      "Eval loss 0.012535926885902882, R2 0.5171293020248413\n",
      "epoch 5009, loss 0.012524071149528027, R2 0.47942423820495605\n",
      "Eval loss 0.01253578718751669, R2 0.5171345472335815\n",
      "epoch 5010, loss 0.012523941695690155, R2 0.47942906618118286\n",
      "Eval loss 0.012535647489130497, R2 0.5171400308609009\n",
      "epoch 5011, loss 0.012523813173174858, R2 0.479434609413147\n",
      "Eval loss 0.01253550685942173, R2 0.5171451568603516\n",
      "epoch 5012, loss 0.012523684650659561, R2 0.47943997383117676\n",
      "Eval loss 0.012535368092358112, R2 0.5171511173248291\n",
      "epoch 5013, loss 0.012523556128144264, R2 0.4794451594352722\n",
      "Eval loss 0.01253522839397192, R2 0.5171562433242798\n",
      "epoch 5014, loss 0.012523427605628967, R2 0.47945064306259155\n",
      "Eval loss 0.012535090558230877, R2 0.5171616673469543\n",
      "epoch 5015, loss 0.01252329908311367, R2 0.47945547103881836\n",
      "Eval loss 0.01253494992852211, R2 0.5171666145324707\n",
      "epoch 5016, loss 0.012523170560598373, R2 0.479461133480072\n",
      "Eval loss 0.012534810230135918, R2 0.5171723365783691\n",
      "epoch 5017, loss 0.012523042038083076, R2 0.4794664978981018\n",
      "Eval loss 0.0125346714630723, R2 0.5171778202056885\n",
      "epoch 5018, loss 0.01252291351556778, R2 0.4794718027114868\n",
      "Eval loss 0.012534532696008682, R2 0.5171830058097839\n",
      "epoch 5019, loss 0.012522784061729908, R2 0.4794778823852539\n",
      "Eval loss 0.012534393928945065, R2 0.5171881914138794\n",
      "epoch 5020, loss 0.012522655539214611, R2 0.4794832468032837\n",
      "Eval loss 0.012534254230558872, R2 0.5171940326690674\n",
      "epoch 5021, loss 0.012522527948021889, R2 0.4794883728027344\n",
      "Eval loss 0.01253411453217268, R2 0.5171992778778076\n",
      "epoch 5022, loss 0.012522401288151741, R2 0.47949349880218506\n",
      "Eval loss 0.012533977627754211, R2 0.5172044038772583\n",
      "epoch 5023, loss 0.012522272765636444, R2 0.4794984459877014\n",
      "Eval loss 0.012533837929368019, R2 0.5172096490859985\n",
      "epoch 5024, loss 0.012522144243121147, R2 0.4795038104057312\n",
      "Eval loss 0.012533700093626976, R2 0.5172152519226074\n",
      "epoch 5025, loss 0.012522016651928425, R2 0.4795095920562744\n",
      "Eval loss 0.012533561326563358, R2 0.5172202587127686\n",
      "epoch 5026, loss 0.012521889060735703, R2 0.4795144200325012\n",
      "Eval loss 0.01253342255949974, R2 0.5172257423400879\n",
      "epoch 5027, loss 0.01252176146954298, R2 0.47952020168304443\n",
      "Eval loss 0.012533284723758698, R2 0.5172311067581177\n",
      "epoch 5028, loss 0.012521633878350258, R2 0.47952502965927124\n",
      "Eval loss 0.01253314595669508, R2 0.517236590385437\n",
      "epoch 5029, loss 0.012521506287157536, R2 0.4795301556587219\n",
      "Eval loss 0.012533008120954037, R2 0.5172415971755981\n",
      "epoch 5030, loss 0.012521378695964813, R2 0.47953563928604126\n",
      "Eval loss 0.012532870285212994, R2 0.5172467231750488\n",
      "epoch 5031, loss 0.012521252036094666, R2 0.4795414209365845\n",
      "Eval loss 0.012532731518149376, R2 0.5172524452209473\n",
      "epoch 5032, loss 0.012521124444901943, R2 0.4795461893081665\n",
      "Eval loss 0.012532593682408333, R2 0.5172576904296875\n",
      "epoch 5033, loss 0.012520996853709221, R2 0.4795514941215515\n",
      "Eval loss 0.01253245584666729, R2 0.5172630548477173\n",
      "epoch 5034, loss 0.012520870193839073, R2 0.4795575737953186\n",
      "Eval loss 0.012532318942248821, R2 0.5172683000564575\n",
      "epoch 5035, loss 0.01252074260264635, R2 0.47956210374832153\n",
      "Eval loss 0.012532181106507778, R2 0.5172736048698425\n",
      "epoch 5036, loss 0.012520616874098778, R2 0.4795674681663513\n",
      "Eval loss 0.012532043270766735, R2 0.5172790288925171\n",
      "epoch 5037, loss 0.012520487420260906, R2 0.4795726537704468\n",
      "Eval loss 0.012531906366348267, R2 0.5172841548919678\n",
      "epoch 5038, loss 0.012520362623035908, R2 0.47957801818847656\n",
      "Eval loss 0.012531768530607224, R2 0.5172893404960632\n",
      "epoch 5039, loss 0.01252023596316576, R2 0.47958362102508545\n",
      "Eval loss 0.012531631626188755, R2 0.5172947645187378\n",
      "epoch 5040, loss 0.012520110234618187, R2 0.47958850860595703\n",
      "Eval loss 0.012531493790447712, R2 0.517300009727478\n",
      "epoch 5041, loss 0.012519982643425465, R2 0.4795936942100525\n",
      "Eval loss 0.012531356886029243, R2 0.5173051953315735\n",
      "epoch 5042, loss 0.012519855052232742, R2 0.47959911823272705\n",
      "Eval loss 0.0125312190502882, R2 0.5173102021217346\n",
      "epoch 5043, loss 0.01251972932368517, R2 0.4796043634414673\n",
      "Eval loss 0.012531082145869732, R2 0.5173157453536987\n",
      "epoch 5044, loss 0.012519603595137596, R2 0.4796099066734314\n",
      "Eval loss 0.012530946172773838, R2 0.5173208713531494\n",
      "epoch 5045, loss 0.012519476935267448, R2 0.4796146750450134\n",
      "Eval loss 0.012530808337032795, R2 0.5173263549804688\n",
      "epoch 5046, loss 0.0125193502753973, R2 0.47961997985839844\n",
      "Eval loss 0.012530672363936901, R2 0.5173317193984985\n",
      "epoch 5047, loss 0.012519226409494877, R2 0.4796251058578491\n",
      "Eval loss 0.012530535459518433, R2 0.5173368453979492\n",
      "epoch 5048, loss 0.01251909788697958, R2 0.47963041067123413\n",
      "Eval loss 0.012530400417745113, R2 0.5173417329788208\n",
      "epoch 5049, loss 0.012518972158432007, R2 0.47963565587997437\n",
      "Eval loss 0.01253026258200407, R2 0.5173477530479431\n",
      "epoch 5050, loss 0.012518846429884434, R2 0.4796410799026489\n",
      "Eval loss 0.012530126608908176, R2 0.5173527002334595\n",
      "epoch 5051, loss 0.012518721632659435, R2 0.47964608669281006\n",
      "Eval loss 0.012529989704489708, R2 0.5173580646514893\n",
      "epoch 5052, loss 0.012518594972789288, R2 0.4796513319015503\n",
      "Eval loss 0.012529854662716389, R2 0.5173636674880981\n",
      "epoch 5053, loss 0.012518469244241714, R2 0.4796565771102905\n",
      "Eval loss 0.01252971775829792, R2 0.517368495464325\n",
      "epoch 5054, loss 0.012518344447016716, R2 0.47966188192367554\n",
      "Eval loss 0.012529581785202026, R2 0.5173734426498413\n",
      "epoch 5055, loss 0.012518217787146568, R2 0.4796674847602844\n",
      "Eval loss 0.012529445812106133, R2 0.517379105091095\n",
      "epoch 5056, loss 0.01251809298992157, R2 0.4796721935272217\n",
      "Eval loss 0.01252930797636509, R2 0.5173839330673218\n",
      "epoch 5057, loss 0.012517968192696571, R2 0.4796779155731201\n",
      "Eval loss 0.012529173865914345, R2 0.517389178276062\n",
      "epoch 5058, loss 0.012517843395471573, R2 0.4796827435493469\n",
      "Eval loss 0.012529038824141026, R2 0.5173945426940918\n",
      "epoch 5059, loss 0.012517717666924, R2 0.47968780994415283\n",
      "Eval loss 0.012528902851045132, R2 0.5174000263214111\n",
      "epoch 5060, loss 0.012517591938376427, R2 0.4796932339668274\n",
      "Eval loss 0.012528766877949238, R2 0.5174046754837036\n",
      "epoch 5061, loss 0.012517467141151428, R2 0.4796983599662781\n",
      "Eval loss 0.012528630904853344, R2 0.5174103379249573\n",
      "epoch 5062, loss 0.012517341412603855, R2 0.47970378398895264\n",
      "Eval loss 0.01252849493175745, R2 0.5174152851104736\n",
      "epoch 5063, loss 0.012517217546701431, R2 0.47970861196517944\n",
      "Eval loss 0.012528360821306705, R2 0.5174206495285034\n",
      "epoch 5064, loss 0.012517091818153858, R2 0.4797137975692749\n",
      "Eval loss 0.012528224848210812, R2 0.5174256563186646\n",
      "epoch 5065, loss 0.012516969814896584, R2 0.47971904277801514\n",
      "Eval loss 0.012528090737760067, R2 0.5174316167831421\n",
      "epoch 5066, loss 0.01251684408634901, R2 0.47972381114959717\n",
      "Eval loss 0.012527954764664173, R2 0.5174363851547241\n",
      "epoch 5067, loss 0.012516719289124012, R2 0.47972917556762695\n",
      "Eval loss 0.01252781879156828, R2 0.5174416303634644\n",
      "epoch 5068, loss 0.012516594491899014, R2 0.47973477840423584\n",
      "Eval loss 0.01252768374979496, R2 0.5174465179443359\n",
      "epoch 5069, loss 0.012516469694674015, R2 0.4797401428222656\n",
      "Eval loss 0.012527549639344215, R2 0.5174522399902344\n",
      "epoch 5070, loss 0.012516345828771591, R2 0.4797448515892029\n",
      "Eval loss 0.012527414597570896, R2 0.5174570679664612\n",
      "epoch 5071, loss 0.012516221962869167, R2 0.47974997758865356\n",
      "Eval loss 0.012527279555797577, R2 0.5174621343612671\n",
      "epoch 5072, loss 0.012516097165644169, R2 0.47975581884384155\n",
      "Eval loss 0.012527146376669407, R2 0.5174677968025208\n",
      "epoch 5073, loss 0.01251597423106432, R2 0.4797602891921997\n",
      "Eval loss 0.012527011334896088, R2 0.5174727439880371\n",
      "epoch 5074, loss 0.012515849433839321, R2 0.47976547479629517\n",
      "Eval loss 0.012526878155767918, R2 0.5174779891967773\n",
      "epoch 5075, loss 0.012515725567936897, R2 0.4797709584236145\n",
      "Eval loss 0.012526742182672024, R2 0.5174828171730042\n",
      "epoch 5076, loss 0.012515600770711899, R2 0.47977596521377563\n",
      "Eval loss 0.012526607140898705, R2 0.5174882411956787\n",
      "epoch 5077, loss 0.0125154759734869, R2 0.47978097200393677\n",
      "Eval loss 0.012526473961770535, R2 0.5174934267997742\n",
      "epoch 5078, loss 0.012515353038907051, R2 0.47978675365448\n",
      "Eval loss 0.012526340782642365, R2 0.517498254776001\n",
      "epoch 5079, loss 0.012515230104327202, R2 0.4797910451889038\n",
      "Eval loss 0.01252620667219162, R2 0.5175037384033203\n",
      "epoch 5080, loss 0.012515107169747353, R2 0.47979629039764404\n",
      "Eval loss 0.0125260716304183, R2 0.5175089240074158\n",
      "epoch 5081, loss 0.012514982372522354, R2 0.47980165481567383\n",
      "Eval loss 0.01252593845129013, R2 0.5175141096115112\n",
      "epoch 5082, loss 0.01251486036926508, R2 0.47980672121047974\n",
      "Eval loss 0.01252580527216196, R2 0.5175191760063171\n",
      "epoch 5083, loss 0.012514736503362656, R2 0.4798123836517334\n",
      "Eval loss 0.012525671161711216, R2 0.5175244808197021\n",
      "epoch 5084, loss 0.012514612637460232, R2 0.479816734790802\n",
      "Eval loss 0.012525537051260471, R2 0.5175292491912842\n",
      "epoch 5085, loss 0.012514489702880383, R2 0.47982245683670044\n",
      "Eval loss 0.012525403872132301, R2 0.517534613609314\n",
      "epoch 5086, loss 0.012514365836977959, R2 0.4798269271850586\n",
      "Eval loss 0.012525269761681557, R2 0.5175395011901855\n",
      "epoch 5087, loss 0.01251424290239811, R2 0.47983241081237793\n",
      "Eval loss 0.012525136582553387, R2 0.517545223236084\n",
      "epoch 5088, loss 0.01251411996781826, R2 0.4798378348350525\n",
      "Eval loss 0.012525004334747791, R2 0.5175497531890869\n",
      "epoch 5089, loss 0.01251399889588356, R2 0.4798423647880554\n",
      "Eval loss 0.012524872086942196, R2 0.5175552368164062\n",
      "epoch 5090, loss 0.012513875029981136, R2 0.4798475503921509\n",
      "Eval loss 0.012524737045168877, R2 0.5175604820251465\n",
      "epoch 5091, loss 0.012513752095401287, R2 0.4798526167869568\n",
      "Eval loss 0.012524602934718132, R2 0.5175653696060181\n",
      "epoch 5092, loss 0.012513629160821438, R2 0.4798583984375\n",
      "Eval loss 0.012524470686912537, R2 0.5175703167915344\n",
      "epoch 5093, loss 0.012513507157564163, R2 0.4798628091812134\n",
      "Eval loss 0.012524338439106941, R2 0.5175752639770508\n",
      "epoch 5094, loss 0.012513384222984314, R2 0.47986793518066406\n",
      "Eval loss 0.012524206191301346, R2 0.5175807476043701\n",
      "epoch 5095, loss 0.01251326221972704, R2 0.47987353801727295\n",
      "Eval loss 0.01252407394349575, R2 0.5175857543945312\n",
      "epoch 5096, loss 0.01251313928514719, R2 0.4798780679702759\n",
      "Eval loss 0.01252394076436758, R2 0.5175909996032715\n",
      "epoch 5097, loss 0.012513017281889915, R2 0.47988319396972656\n",
      "Eval loss 0.01252380758523941, R2 0.5175960063934326\n",
      "epoch 5098, loss 0.01251289527863264, R2 0.47988826036453247\n",
      "Eval loss 0.012523675337433815, R2 0.5176013708114624\n",
      "epoch 5099, loss 0.012512772344052792, R2 0.4798933267593384\n",
      "Eval loss 0.01252354308962822, R2 0.5176061391830444\n",
      "epoch 5100, loss 0.012512650340795517, R2 0.47989827394485474\n",
      "Eval loss 0.01252340991050005, R2 0.5176114439964294\n",
      "epoch 5101, loss 0.012512528337538242, R2 0.47990351915359497\n",
      "Eval loss 0.012523278594017029, R2 0.5176165103912354\n",
      "epoch 5102, loss 0.012512406334280968, R2 0.4799085855484009\n",
      "Eval loss 0.012523145414888859, R2 0.5176213383674622\n",
      "epoch 5103, loss 0.012512284331023693, R2 0.479914128780365\n",
      "Eval loss 0.012523013167083263, R2 0.5176265835762024\n",
      "epoch 5104, loss 0.012512162327766418, R2 0.47991907596588135\n",
      "Eval loss 0.012522882781922817, R2 0.5176317691802979\n",
      "epoch 5105, loss 0.012512042187154293, R2 0.47992372512817383\n",
      "Eval loss 0.012522748671472073, R2 0.5176368951797485\n",
      "epoch 5106, loss 0.012511921115219593, R2 0.4799286127090454\n",
      "Eval loss 0.012522617354989052, R2 0.5176420211791992\n",
      "epoch 5107, loss 0.012511799111962318, R2 0.4799339771270752\n",
      "Eval loss 0.012522486038506031, R2 0.5176466107368469\n",
      "epoch 5108, loss 0.012511677108705044, R2 0.47993922233581543\n",
      "Eval loss 0.012522353790700436, R2 0.517652153968811\n",
      "epoch 5109, loss 0.012511556036770344, R2 0.479944109916687\n",
      "Eval loss 0.012522222474217415, R2 0.517657458782196\n",
      "epoch 5110, loss 0.012511434964835644, R2 0.47994911670684814\n",
      "Eval loss 0.012522092089056969, R2 0.5176623463630676\n",
      "epoch 5111, loss 0.012511312030255795, R2 0.4799538850784302\n",
      "Eval loss 0.012521959841251373, R2 0.5176675319671631\n",
      "epoch 5112, loss 0.012511190958321095, R2 0.47995924949645996\n",
      "Eval loss 0.012521828524768353, R2 0.5176723599433899\n",
      "epoch 5113, loss 0.012511070817708969, R2 0.47996407747268677\n",
      "Eval loss 0.012521696276962757, R2 0.5176773071289062\n",
      "epoch 5114, loss 0.012510949745774269, R2 0.4799692630767822\n",
      "Eval loss 0.012521566823124886, R2 0.5176824331283569\n",
      "epoch 5115, loss 0.012510827742516994, R2 0.47997403144836426\n",
      "Eval loss 0.01252143457531929, R2 0.5176875591278076\n",
      "epoch 5116, loss 0.012510706670582294, R2 0.4799796938896179\n",
      "Eval loss 0.01252130325883627, R2 0.517693042755127\n",
      "epoch 5117, loss 0.012510586529970169, R2 0.47998422384262085\n",
      "Eval loss 0.012521172873675823, R2 0.5176973342895508\n",
      "epoch 5118, loss 0.012510465458035469, R2 0.4799894094467163\n",
      "Eval loss 0.012521041557192802, R2 0.5177023410797119\n",
      "epoch 5119, loss 0.012510346248745918, R2 0.47999435663223267\n",
      "Eval loss 0.012520910240709782, R2 0.5177077054977417\n",
      "epoch 5120, loss 0.012510225176811218, R2 0.47999924421310425\n",
      "Eval loss 0.01252078078687191, R2 0.5177125930786133\n",
      "epoch 5121, loss 0.012510104104876518, R2 0.4800042510032654\n",
      "Eval loss 0.012520648539066315, R2 0.5177174806594849\n",
      "epoch 5122, loss 0.012509983032941818, R2 0.4800094962120056\n",
      "Eval loss 0.012520517222583294, R2 0.5177228450775146\n",
      "epoch 5123, loss 0.012509863823652267, R2 0.48001474142074585\n",
      "Eval loss 0.012520388700067997, R2 0.5177280902862549\n",
      "epoch 5124, loss 0.012509742751717567, R2 0.48001962900161743\n",
      "Eval loss 0.01252025831490755, R2 0.5177331566810608\n",
      "epoch 5125, loss 0.012509622611105442, R2 0.48002445697784424\n",
      "Eval loss 0.01252012699842453, R2 0.5177375078201294\n",
      "epoch 5126, loss 0.012509503401815891, R2 0.48002976179122925\n",
      "Eval loss 0.012519998475909233, R2 0.5177425742149353\n",
      "epoch 5127, loss 0.012509383261203766, R2 0.4800342321395874\n",
      "Eval loss 0.012519867159426212, R2 0.5177478790283203\n",
      "epoch 5128, loss 0.012509262189269066, R2 0.4800392985343933\n",
      "Eval loss 0.012519735842943192, R2 0.5177529454231262\n",
      "epoch 5129, loss 0.012509142979979515, R2 0.48004424571990967\n",
      "Eval loss 0.01251960638910532, R2 0.5177579522132874\n",
      "epoch 5130, loss 0.01250902283936739, R2 0.480049192905426\n",
      "Eval loss 0.012519477866590023, R2 0.5177628993988037\n",
      "epoch 5131, loss 0.012508904561400414, R2 0.48005396127700806\n",
      "Eval loss 0.012519347481429577, R2 0.5177676677703857\n",
      "epoch 5132, loss 0.012508782558143139, R2 0.4800596833229065\n",
      "Eval loss 0.01251921709626913, R2 0.5177727937698364\n",
      "epoch 5133, loss 0.012508664280176163, R2 0.48006463050842285\n",
      "Eval loss 0.01251908764243126, R2 0.517777681350708\n",
      "epoch 5134, loss 0.012508544139564037, R2 0.48006880283355713\n",
      "Eval loss 0.012518957257270813, R2 0.5177826881408691\n",
      "epoch 5135, loss 0.012508424930274487, R2 0.4800739288330078\n",
      "Eval loss 0.012518828734755516, R2 0.5177880525588989\n",
      "epoch 5136, loss 0.01250830665230751, R2 0.4800789952278137\n",
      "Eval loss 0.01251869834959507, R2 0.5177926421165466\n",
      "epoch 5137, loss 0.012508186511695385, R2 0.48008447885513306\n",
      "Eval loss 0.012518569827079773, R2 0.517797589302063\n",
      "epoch 5138, loss 0.012508067302405834, R2 0.4800889492034912\n",
      "Eval loss 0.012518440373241901, R2 0.5178028345108032\n",
      "epoch 5139, loss 0.012507948093116283, R2 0.4800940752029419\n",
      "Eval loss 0.01251831091940403, R2 0.5178080201148987\n",
      "epoch 5140, loss 0.012507829815149307, R2 0.48009932041168213\n",
      "Eval loss 0.012518180534243584, R2 0.5178131461143494\n",
      "epoch 5141, loss 0.012507709674537182, R2 0.4801044464111328\n",
      "Eval loss 0.012518052943050861, R2 0.5178180932998657\n",
      "epoch 5142, loss 0.01250759232789278, R2 0.4801088571548462\n",
      "Eval loss 0.01251792348921299, R2 0.5178232192993164\n",
      "epoch 5143, loss 0.01250747125595808, R2 0.480114221572876\n",
      "Eval loss 0.012517794035375118, R2 0.5178277492523193\n",
      "epoch 5144, loss 0.012507352977991104, R2 0.4801188111305237\n",
      "Eval loss 0.012517665512859821, R2 0.5178323984146118\n",
      "epoch 5145, loss 0.012507233768701553, R2 0.4801234006881714\n",
      "Eval loss 0.012517536990344524, R2 0.5178377628326416\n",
      "epoch 5146, loss 0.012507115490734577, R2 0.48012852668762207\n",
      "Eval loss 0.012517408467829227, R2 0.5178428888320923\n",
      "epoch 5147, loss 0.012506997212767601, R2 0.48013389110565186\n",
      "Eval loss 0.012517280876636505, R2 0.5178472995758057\n",
      "epoch 5148, loss 0.012506878934800625, R2 0.4801381826400757\n",
      "Eval loss 0.012517150491476059, R2 0.517852246761322\n",
      "epoch 5149, loss 0.012506759725511074, R2 0.4801441431045532\n",
      "Eval loss 0.012517022900283337, R2 0.5178576111793518\n",
      "epoch 5150, loss 0.012506642378866673, R2 0.4801481366157532\n",
      "Eval loss 0.012516895309090614, R2 0.51786208152771\n",
      "epoch 5151, loss 0.012506524100899696, R2 0.48015326261520386\n",
      "Eval loss 0.012516765855252743, R2 0.517867386341095\n",
      "epoch 5152, loss 0.012506404891610146, R2 0.4801582098007202\n",
      "Eval loss 0.012516636401414871, R2 0.5178723335266113\n",
      "epoch 5153, loss 0.01250628661364317, R2 0.48016297817230225\n",
      "Eval loss 0.012516508810222149, R2 0.5178773999214172\n",
      "epoch 5154, loss 0.012506168335676193, R2 0.48016852140426636\n",
      "Eval loss 0.012516381219029427, R2 0.517882227897644\n",
      "epoch 5155, loss 0.012506050057709217, R2 0.48017263412475586\n",
      "Eval loss 0.012516253627836704, R2 0.5178871154785156\n",
      "epoch 5156, loss 0.012505931779742241, R2 0.4801778793334961\n",
      "Eval loss 0.012516126036643982, R2 0.5178917646408081\n",
      "epoch 5157, loss 0.012505815364420414, R2 0.4801827073097229\n",
      "Eval loss 0.01251599658280611, R2 0.5178970098495483\n",
      "epoch 5158, loss 0.012505696155130863, R2 0.48018747568130493\n",
      "Eval loss 0.012515868991613388, R2 0.5179017782211304\n",
      "epoch 5159, loss 0.012505579739809036, R2 0.48019230365753174\n",
      "Eval loss 0.012515741400420666, R2 0.5179065465927124\n",
      "epoch 5160, loss 0.01250546146184206, R2 0.48019689321517944\n",
      "Eval loss 0.012515613809227943, R2 0.5179117918014526\n",
      "epoch 5161, loss 0.012505344115197659, R2 0.48020225763320923\n",
      "Eval loss 0.01251548808068037, R2 0.5179166197776794\n",
      "epoch 5162, loss 0.012505225837230682, R2 0.48020702600479126\n",
      "Eval loss 0.012515359558165073, R2 0.5179218053817749\n",
      "epoch 5163, loss 0.01250510849058628, R2 0.48021256923675537\n",
      "Eval loss 0.012515231966972351, R2 0.517926037311554\n",
      "epoch 5164, loss 0.012504992075264454, R2 0.48021721839904785\n",
      "Eval loss 0.012515104375779629, R2 0.5179311037063599\n",
      "epoch 5165, loss 0.012504873797297478, R2 0.4802221655845642\n",
      "Eval loss 0.012514978647232056, R2 0.5179363489151001\n",
      "epoch 5166, loss 0.012504758313298225, R2 0.4802265167236328\n",
      "Eval loss 0.012514850124716759, R2 0.5179409980773926\n",
      "epoch 5167, loss 0.01250464003533125, R2 0.4802314043045044\n",
      "Eval loss 0.012514721602201462, R2 0.5179458260536194\n",
      "epoch 5168, loss 0.012504522688686848, R2 0.480236291885376\n",
      "Eval loss 0.012514596804976463, R2 0.5179506540298462\n",
      "epoch 5169, loss 0.01250440627336502, R2 0.4802411198616028\n",
      "Eval loss 0.012514469213783741, R2 0.5179555416107178\n",
      "epoch 5170, loss 0.01250428892672062, R2 0.48024600744247437\n",
      "Eval loss 0.012514343485236168, R2 0.5179606676101685\n",
      "epoch 5171, loss 0.012504173442721367, R2 0.48025113344192505\n",
      "Eval loss 0.012514215894043446, R2 0.5179656147956848\n",
      "epoch 5172, loss 0.012504056096076965, R2 0.480255663394928\n",
      "Eval loss 0.012514090165495872, R2 0.5179705619812012\n",
      "epoch 5173, loss 0.012503938749432564, R2 0.48026037216186523\n",
      "Eval loss 0.01251396257430315, R2 0.5179753303527832\n",
      "epoch 5174, loss 0.012503822334110737, R2 0.48026537895202637\n",
      "Eval loss 0.012513835914433002, R2 0.5179802179336548\n",
      "epoch 5175, loss 0.01250370591878891, R2 0.4802708625793457\n",
      "Eval loss 0.01251371018588543, R2 0.5179848074913025\n",
      "epoch 5176, loss 0.012503588572144508, R2 0.4802757501602173\n",
      "Eval loss 0.012513582594692707, R2 0.5179897546768188\n",
      "epoch 5177, loss 0.012503472156822681, R2 0.4802802801132202\n",
      "Eval loss 0.01251345593482256, R2 0.5179949998855591\n",
      "epoch 5178, loss 0.012503355741500854, R2 0.4802854061126709\n",
      "Eval loss 0.012513331137597561, R2 0.5179994106292725\n",
      "epoch 5179, loss 0.012503239326179028, R2 0.4802895784378052\n",
      "Eval loss 0.012513204477727413, R2 0.518004834651947\n",
      "epoch 5180, loss 0.012503123842179775, R2 0.4802948832511902\n",
      "Eval loss 0.012513077817857265, R2 0.5180095434188843\n",
      "epoch 5181, loss 0.012503007426857948, R2 0.4802992343902588\n",
      "Eval loss 0.012512952089309692, R2 0.518014132976532\n",
      "epoch 5182, loss 0.012502891942858696, R2 0.4803047180175781\n",
      "Eval loss 0.01251282636076212, R2 0.5180191397666931\n",
      "epoch 5183, loss 0.012502774596214294, R2 0.4803089499473572\n",
      "Eval loss 0.012512699700891972, R2 0.5180238485336304\n",
      "epoch 5184, loss 0.012502659112215042, R2 0.4803142547607422\n",
      "Eval loss 0.012512574903666973, R2 0.5180290937423706\n",
      "epoch 5185, loss 0.01250254362821579, R2 0.48031818866729736\n",
      "Eval loss 0.0125124491751194, R2 0.518033504486084\n",
      "epoch 5186, loss 0.012502429075539112, R2 0.48032331466674805\n",
      "Eval loss 0.012512324377894402, R2 0.5180381536483765\n",
      "epoch 5187, loss 0.01250231172889471, R2 0.48032820224761963\n",
      "Eval loss 0.012512197718024254, R2 0.5180436372756958\n",
      "epoch 5188, loss 0.012502196244895458, R2 0.48033297061920166\n",
      "Eval loss 0.01251207385212183, R2 0.5180479884147644\n",
      "epoch 5189, loss 0.012502080760896206, R2 0.48033809661865234\n",
      "Eval loss 0.012511945329606533, R2 0.5180527567863464\n",
      "epoch 5190, loss 0.012501964345574379, R2 0.4803427457809448\n",
      "Eval loss 0.01251182146370411, R2 0.5180578231811523\n",
      "epoch 5191, loss 0.012501849792897701, R2 0.48034703731536865\n",
      "Eval loss 0.012511695735156536, R2 0.5180628299713135\n",
      "epoch 5192, loss 0.012501734308898449, R2 0.48035264015197754\n",
      "Eval loss 0.012511570006608963, R2 0.518067479133606\n",
      "epoch 5193, loss 0.012501620687544346, R2 0.48035693168640137\n",
      "Eval loss 0.012511445209383965, R2 0.5180723667144775\n",
      "epoch 5194, loss 0.012501504272222519, R2 0.4803622364997864\n",
      "Eval loss 0.012511320412158966, R2 0.5180772542953491\n",
      "epoch 5195, loss 0.012501388788223267, R2 0.4803665280342102\n",
      "Eval loss 0.012511196546256542, R2 0.5180816650390625\n",
      "epoch 5196, loss 0.012501274235546589, R2 0.48037129640579224\n",
      "Eval loss 0.012511070817708969, R2 0.5180867314338684\n",
      "epoch 5197, loss 0.012501158751547337, R2 0.4803764224052429\n",
      "Eval loss 0.01251094602048397, R2 0.5180915594100952\n",
      "epoch 5198, loss 0.01250104233622551, R2 0.48038095235824585\n",
      "Eval loss 0.012510822154581547, R2 0.5180963277816772\n",
      "epoch 5199, loss 0.012500928714871407, R2 0.4803861379623413\n",
      "Eval loss 0.012510696426033974, R2 0.5181008577346802\n",
      "epoch 5200, loss 0.012500813230872154, R2 0.48039060831069946\n",
      "Eval loss 0.01251057256013155, R2 0.5181056261062622\n",
      "epoch 5201, loss 0.012500697746872902, R2 0.48039543628692627\n",
      "Eval loss 0.012510447762906551, R2 0.518110454082489\n",
      "epoch 5202, loss 0.012500583194196224, R2 0.4804001450538635\n",
      "Eval loss 0.012510322965681553, R2 0.5181156992912292\n",
      "epoch 5203, loss 0.012500470504164696, R2 0.48040419816970825\n",
      "Eval loss 0.012510198168456554, R2 0.5181204676628113\n",
      "epoch 5204, loss 0.012500355020165443, R2 0.48040950298309326\n",
      "Eval loss 0.01251007430255413, R2 0.5181252956390381\n",
      "epoch 5205, loss 0.01250024139881134, R2 0.48041439056396484\n",
      "Eval loss 0.012509950436651707, R2 0.5181300640106201\n",
      "epoch 5206, loss 0.012500126846134663, R2 0.4804191589355469\n",
      "Eval loss 0.012509825639426708, R2 0.5181348323822021\n",
      "epoch 5207, loss 0.012500012293457985, R2 0.48042356967926025\n",
      "Eval loss 0.012509701773524284, R2 0.5181394815444946\n",
      "epoch 5208, loss 0.012499896809458733, R2 0.4804285168647766\n",
      "Eval loss 0.012509578838944435, R2 0.5181443691253662\n",
      "epoch 5209, loss 0.01249978318810463, R2 0.4804334044456482\n",
      "Eval loss 0.012509454973042011, R2 0.5181491374969482\n",
      "epoch 5210, loss 0.012499669566750526, R2 0.4804382920265198\n",
      "Eval loss 0.012509331107139587, R2 0.5181536078453064\n",
      "epoch 5211, loss 0.012499555945396423, R2 0.48044323921203613\n",
      "Eval loss 0.012509206309914589, R2 0.5181586742401123\n",
      "epoch 5212, loss 0.01249944232404232, R2 0.4804479479789734\n",
      "Eval loss 0.012509082444012165, R2 0.5181630253791809\n",
      "epoch 5213, loss 0.012499326840043068, R2 0.4804522395133972\n",
      "Eval loss 0.012508958578109741, R2 0.5181680917739868\n",
      "epoch 5214, loss 0.01249921228736639, R2 0.48045700788497925\n",
      "Eval loss 0.012508835643529892, R2 0.5181729793548584\n",
      "epoch 5215, loss 0.012499099597334862, R2 0.48046183586120605\n",
      "Eval loss 0.012508713640272617, R2 0.5181773900985718\n",
      "epoch 5216, loss 0.012498986907303333, R2 0.4804668426513672\n",
      "Eval loss 0.012508588843047619, R2 0.5181823372840881\n",
      "epoch 5217, loss 0.012498872354626656, R2 0.480471134185791\n",
      "Eval loss 0.01250846590846777, R2 0.5181872248649597\n",
      "epoch 5218, loss 0.012498759664595127, R2 0.4804760217666626\n",
      "Eval loss 0.012508343905210495, R2 0.5181916356086731\n",
      "epoch 5219, loss 0.01249864511191845, R2 0.4804806113243103\n",
      "Eval loss 0.012508219107985497, R2 0.5181964635848999\n",
      "epoch 5220, loss 0.012498531490564346, R2 0.48048532009124756\n",
      "Eval loss 0.012508096173405647, R2 0.5182012319564819\n",
      "epoch 5221, loss 0.012498418800532818, R2 0.48049014806747437\n",
      "Eval loss 0.012507974170148373, R2 0.5182061195373535\n",
      "epoch 5222, loss 0.01249830611050129, R2 0.4804948568344116\n",
      "Eval loss 0.012507850304245949, R2 0.5182108879089355\n",
      "epoch 5223, loss 0.012498192489147186, R2 0.4804995656013489\n",
      "Eval loss 0.012507726438343525, R2 0.5182156562805176\n",
      "epoch 5224, loss 0.012498078867793083, R2 0.4805040955543518\n",
      "Eval loss 0.01250760443508625, R2 0.5182199478149414\n",
      "epoch 5225, loss 0.012497966177761555, R2 0.48050880432128906\n",
      "Eval loss 0.012507482431828976, R2 0.5182247161865234\n",
      "epoch 5226, loss 0.012497853487730026, R2 0.4805135130882263\n",
      "Eval loss 0.012507359497249126, R2 0.5182297229766846\n",
      "epoch 5227, loss 0.012497740797698498, R2 0.4805181622505188\n",
      "Eval loss 0.012507237493991852, R2 0.518234372138977\n",
      "epoch 5228, loss 0.012497627176344395, R2 0.4805232286453247\n",
      "Eval loss 0.012507114559412003, R2 0.5182388424873352\n",
      "epoch 5229, loss 0.012497513554990292, R2 0.48052746057510376\n",
      "Eval loss 0.012506991624832153, R2 0.5182439088821411\n",
      "epoch 5230, loss 0.012497400864958763, R2 0.48053228855133057\n",
      "Eval loss 0.012506870552897453, R2 0.5182486772537231\n",
      "epoch 5231, loss 0.01249728910624981, R2 0.48053693771362305\n",
      "Eval loss 0.012506747618317604, R2 0.5182531476020813\n",
      "epoch 5232, loss 0.01249717641621828, R2 0.48054230213165283\n",
      "Eval loss 0.012506626546382904, R2 0.5182576179504395\n",
      "epoch 5233, loss 0.012497063726186752, R2 0.4805464744567871\n",
      "Eval loss 0.012506501749157906, R2 0.5182629823684692\n",
      "epoch 5234, loss 0.012496951967477798, R2 0.4805513024330139\n",
      "Eval loss 0.012506380677223206, R2 0.5182675123214722\n",
      "epoch 5235, loss 0.01249683927744627, R2 0.4805559515953064\n",
      "Eval loss 0.012506257742643356, R2 0.5182719826698303\n",
      "epoch 5236, loss 0.012496726587414742, R2 0.480560302734375\n",
      "Eval loss 0.012506136670708656, R2 0.5182765126228333\n",
      "epoch 5237, loss 0.012496614828705788, R2 0.48056501150131226\n",
      "Eval loss 0.012506015598773956, R2 0.5182814598083496\n",
      "epoch 5238, loss 0.01249650213867426, R2 0.48056983947753906\n",
      "Eval loss 0.012505892664194107, R2 0.5182863473892212\n",
      "epoch 5239, loss 0.012496390379965305, R2 0.48057448863983154\n",
      "Eval loss 0.012505771592259407, R2 0.5182909965515137\n",
      "epoch 5240, loss 0.012496278621256351, R2 0.48057907819747925\n",
      "Eval loss 0.012505649589002132, R2 0.5182955265045166\n",
      "epoch 5241, loss 0.012496166862547398, R2 0.48058372735977173\n",
      "Eval loss 0.012505527585744858, R2 0.5183002948760986\n",
      "epoch 5242, loss 0.012496055103838444, R2 0.480588436126709\n",
      "Eval loss 0.012505407445132732, R2 0.5183048844337463\n",
      "epoch 5243, loss 0.012495942413806915, R2 0.48059356212615967\n",
      "Eval loss 0.012505285441875458, R2 0.5183098316192627\n",
      "epoch 5244, loss 0.012495830655097961, R2 0.48059773445129395\n",
      "Eval loss 0.012505164369940758, R2 0.5183141231536865\n",
      "epoch 5245, loss 0.012495717965066433, R2 0.4806022644042969\n",
      "Eval loss 0.012505043298006058, R2 0.5183188915252686\n",
      "epoch 5246, loss 0.012495607137680054, R2 0.4806073307991028\n",
      "Eval loss 0.012504921294748783, R2 0.5183233022689819\n",
      "epoch 5247, loss 0.012495494447648525, R2 0.48061203956604004\n",
      "Eval loss 0.012504801154136658, R2 0.5183279514312744\n",
      "epoch 5248, loss 0.012495385482907295, R2 0.4806162714958191\n",
      "Eval loss 0.012504681013524532, R2 0.5183331966400146\n",
      "epoch 5249, loss 0.012495273724198341, R2 0.4806208610534668\n",
      "Eval loss 0.012504559010267258, R2 0.5183378458023071\n",
      "epoch 5250, loss 0.012495161965489388, R2 0.4806255102157593\n",
      "Eval loss 0.012504437938332558, R2 0.5183421969413757\n",
      "epoch 5251, loss 0.012495050206780434, R2 0.48063015937805176\n",
      "Eval loss 0.012504317797720432, R2 0.5183471441268921\n",
      "epoch 5252, loss 0.012494940310716629, R2 0.48063457012176514\n",
      "Eval loss 0.012504196725785732, R2 0.5183514952659607\n",
      "epoch 5253, loss 0.0124948276206851, R2 0.4806394577026367\n",
      "Eval loss 0.012504075653851032, R2 0.5183560252189636\n",
      "epoch 5254, loss 0.012494716793298721, R2 0.4806440472602844\n",
      "Eval loss 0.012503954581916332, R2 0.5183608531951904\n",
      "epoch 5255, loss 0.012494605965912342, R2 0.48064911365509033\n",
      "Eval loss 0.012503834441304207, R2 0.5183655023574829\n",
      "epoch 5256, loss 0.012494495138525963, R2 0.4806530475616455\n",
      "Eval loss 0.012503714300692081, R2 0.5183702707290649\n",
      "epoch 5257, loss 0.012494383379817009, R2 0.480657696723938\n",
      "Eval loss 0.012503593228757381, R2 0.5183744430541992\n",
      "epoch 5258, loss 0.012494273483753204, R2 0.4806622862815857\n",
      "Eval loss 0.01250347401946783, R2 0.5183794498443604\n",
      "epoch 5259, loss 0.0124941635876894, R2 0.4806668758392334\n",
      "Eval loss 0.01250335294753313, R2 0.5183843374252319\n",
      "epoch 5260, loss 0.012494051828980446, R2 0.4806720018386841\n",
      "Eval loss 0.01250323373824358, R2 0.5183886289596558\n",
      "epoch 5261, loss 0.012493941001594067, R2 0.48067623376846313\n",
      "Eval loss 0.012503113597631454, R2 0.5183932781219482\n",
      "epoch 5262, loss 0.012493831105530262, R2 0.4806811809539795\n",
      "Eval loss 0.012502993457019329, R2 0.518397867679596\n",
      "epoch 5263, loss 0.012493719346821308, R2 0.48068535327911377\n",
      "Eval loss 0.012502872385084629, R2 0.5184022188186646\n",
      "epoch 5264, loss 0.012493610382080078, R2 0.48069000244140625\n",
      "Eval loss 0.012502754107117653, R2 0.5184068083763123\n",
      "epoch 5265, loss 0.012493499554693699, R2 0.4806944727897644\n",
      "Eval loss 0.012502633966505527, R2 0.5184115767478943\n",
      "epoch 5266, loss 0.01249338872730732, R2 0.48069870471954346\n",
      "Eval loss 0.012502515688538551, R2 0.5184162855148315\n",
      "epoch 5267, loss 0.012493278831243515, R2 0.480704128742218\n",
      "Eval loss 0.012502394616603851, R2 0.5184206366539001\n",
      "epoch 5268, loss 0.012493169866502285, R2 0.48070818185806274\n",
      "Eval loss 0.012502274475991726, R2 0.5184248685836792\n",
      "epoch 5269, loss 0.012493059039115906, R2 0.4807129502296448\n",
      "Eval loss 0.012502155266702175, R2 0.5184304714202881\n",
      "epoch 5270, loss 0.012492948211729527, R2 0.4807175397872925\n",
      "Eval loss 0.01250203512609005, R2 0.5184347629547119\n",
      "epoch 5271, loss 0.012492839246988297, R2 0.48072242736816406\n",
      "Eval loss 0.012501916848123074, R2 0.5184393525123596\n",
      "epoch 5272, loss 0.012492728419601917, R2 0.4807266592979431\n",
      "Eval loss 0.012501798570156097, R2 0.5184438824653625\n",
      "epoch 5273, loss 0.012492620386183262, R2 0.48073118925094604\n",
      "Eval loss 0.012501677498221397, R2 0.5184484124183655\n",
      "epoch 5274, loss 0.012492509558796883, R2 0.4807360768318176\n",
      "Eval loss 0.012501558288931847, R2 0.5184534192085266\n",
      "epoch 5275, loss 0.012492400594055653, R2 0.48074012994766235\n",
      "Eval loss 0.01250144001096487, R2 0.5184578895568848\n",
      "epoch 5276, loss 0.012492288835346699, R2 0.48074495792388916\n",
      "Eval loss 0.01250132080167532, R2 0.5184623003005981\n",
      "epoch 5277, loss 0.012492179870605469, R2 0.4807494878768921\n",
      "Eval loss 0.012501202523708344, R2 0.518467128276825\n",
      "epoch 5278, loss 0.012492070905864239, R2 0.480754017829895\n",
      "Eval loss 0.012501084245741367, R2 0.5184715390205383\n",
      "epoch 5279, loss 0.012491961941123009, R2 0.4807583689689636\n",
      "Eval loss 0.012500964105129242, R2 0.5184761881828308\n",
      "epoch 5280, loss 0.012491853907704353, R2 0.48076289892196655\n",
      "Eval loss 0.01250084675848484, R2 0.5184808373451233\n",
      "epoch 5281, loss 0.012491744011640549, R2 0.4807679057121277\n",
      "Eval loss 0.012500726617872715, R2 0.5184851884841919\n",
      "epoch 5282, loss 0.012491634115576744, R2 0.4807724952697754\n",
      "Eval loss 0.012500609271228313, R2 0.5184898376464844\n",
      "epoch 5283, loss 0.012491525150835514, R2 0.48077672719955444\n",
      "Eval loss 0.012500489130616188, R2 0.5184943675994873\n",
      "epoch 5284, loss 0.01249141525477171, R2 0.4807812571525574\n",
      "Eval loss 0.012500371783971786, R2 0.5184988379478455\n",
      "epoch 5285, loss 0.012491307221353054, R2 0.480785608291626\n",
      "Eval loss 0.012500252574682236, R2 0.5185031890869141\n",
      "epoch 5286, loss 0.01249119732528925, R2 0.48079031705856323\n",
      "Eval loss 0.012500136159360409, R2 0.5185079574584961\n",
      "epoch 5287, loss 0.012491090223193169, R2 0.48079514503479004\n",
      "Eval loss 0.012500016950070858, R2 0.5185126662254333\n",
      "epoch 5288, loss 0.012490980327129364, R2 0.48079919815063477\n",
      "Eval loss 0.012499897740781307, R2 0.518517255783081\n",
      "epoch 5289, loss 0.012490871362388134, R2 0.4808042049407959\n",
      "Eval loss 0.012499780394136906, R2 0.518521785736084\n",
      "epoch 5290, loss 0.012490762397646904, R2 0.4808083772659302\n",
      "Eval loss 0.012499663047492504, R2 0.5185263156890869\n",
      "epoch 5291, loss 0.012490654364228249, R2 0.4808129072189331\n",
      "Eval loss 0.012499544769525528, R2 0.5185304284095764\n",
      "epoch 5292, loss 0.012490545399487019, R2 0.48081743717193604\n",
      "Eval loss 0.012499425560235977, R2 0.5185352563858032\n",
      "epoch 5293, loss 0.012490436434745789, R2 0.48082244396209717\n",
      "Eval loss 0.01249930914491415, R2 0.5185394883155823\n",
      "epoch 5294, loss 0.012490328401327133, R2 0.4808262586593628\n",
      "Eval loss 0.012499191798269749, R2 0.5185443162918091\n",
      "epoch 5295, loss 0.012490221299231052, R2 0.48083120584487915\n",
      "Eval loss 0.012499075382947922, R2 0.5185484886169434\n",
      "epoch 5296, loss 0.012490112334489822, R2 0.4808359146118164\n",
      "Eval loss 0.012498957104980946, R2 0.5185534954071045\n",
      "epoch 5297, loss 0.012490003369748592, R2 0.4808396100997925\n",
      "Eval loss 0.012498839758336544, R2 0.518557608127594\n",
      "epoch 5298, loss 0.012489896267652512, R2 0.4808444380760193\n",
      "Eval loss 0.012498720549046993, R2 0.5185623168945312\n",
      "epoch 5299, loss 0.012489788234233856, R2 0.4808487296104431\n",
      "Eval loss 0.012498604133725166, R2 0.5185669660568237\n",
      "epoch 5300, loss 0.0124896802008152, R2 0.48085319995880127\n",
      "Eval loss 0.012498486787080765, R2 0.5185711979866028\n",
      "epoch 5301, loss 0.01248957123607397, R2 0.4808579087257385\n",
      "Eval loss 0.012498369440436363, R2 0.5185763835906982\n",
      "epoch 5302, loss 0.01248946413397789, R2 0.48086220026016235\n",
      "Eval loss 0.012498253025114536, R2 0.5185806155204773\n",
      "epoch 5303, loss 0.012489356100559235, R2 0.4808672070503235\n",
      "Eval loss 0.012498135678470135, R2 0.5185849666595459\n",
      "epoch 5304, loss 0.012489249929785728, R2 0.4808710813522339\n",
      "Eval loss 0.012498018331825733, R2 0.5185893774032593\n",
      "epoch 5305, loss 0.012489141896367073, R2 0.480876088142395\n",
      "Eval loss 0.012497900053858757, R2 0.5185939073562622\n",
      "epoch 5306, loss 0.012489033862948418, R2 0.4808797836303711\n",
      "Eval loss 0.012497784569859505, R2 0.5185984969139099\n",
      "epoch 5307, loss 0.012488925829529762, R2 0.4808845520019531\n",
      "Eval loss 0.012497669085860252, R2 0.5186029672622681\n",
      "epoch 5308, loss 0.012488818727433681, R2 0.48088914155960083\n",
      "Eval loss 0.01249755173921585, R2 0.5186071991920471\n",
      "epoch 5309, loss 0.012488712556660175, R2 0.48089343309402466\n",
      "Eval loss 0.012497435323894024, R2 0.5186119675636292\n",
      "epoch 5310, loss 0.012488603591918945, R2 0.4808979630470276\n",
      "Eval loss 0.012497317045927048, R2 0.5186166763305664\n",
      "epoch 5311, loss 0.01248849742114544, R2 0.48090237379074097\n",
      "Eval loss 0.012497201561927795, R2 0.5186209678649902\n",
      "epoch 5312, loss 0.01248838845640421, R2 0.4809069037437439\n",
      "Eval loss 0.012497085146605968, R2 0.5186251401901245\n",
      "epoch 5313, loss 0.012488282285630703, R2 0.4809113144874573\n",
      "Eval loss 0.012496968731284142, R2 0.5186299681663513\n",
      "epoch 5314, loss 0.012488175183534622, R2 0.48091644048690796\n",
      "Eval loss 0.012496852315962315, R2 0.5186344385147095\n",
      "epoch 5315, loss 0.01248806994408369, R2 0.4809200167655945\n",
      "Eval loss 0.012496736831963062, R2 0.5186386108398438\n",
      "epoch 5316, loss 0.012487961910665035, R2 0.48092448711395264\n",
      "Eval loss 0.01249661948531866, R2 0.5186433792114258\n",
      "epoch 5317, loss 0.01248785387724638, R2 0.4809291362762451\n",
      "Eval loss 0.012496503069996834, R2 0.5186477303504944\n",
      "epoch 5318, loss 0.012487746775150299, R2 0.4809340834617615\n",
      "Eval loss 0.012496388517320156, R2 0.5186519622802734\n",
      "epoch 5319, loss 0.012487639673054218, R2 0.48093855381011963\n",
      "Eval loss 0.012496271170675755, R2 0.5186566710472107\n",
      "epoch 5320, loss 0.012487534433603287, R2 0.48094308376312256\n",
      "Eval loss 0.012496155686676502, R2 0.5186611413955688\n",
      "epoch 5321, loss 0.012487426400184631, R2 0.48094719648361206\n",
      "Eval loss 0.01249604020267725, R2 0.5186662673950195\n",
      "epoch 5322, loss 0.012487322092056274, R2 0.4809512495994568\n",
      "Eval loss 0.012495924718677998, R2 0.5186701416969299\n",
      "epoch 5323, loss 0.012487214989960194, R2 0.48095566034317017\n",
      "Eval loss 0.012495809234678745, R2 0.5186746120452881\n",
      "epoch 5324, loss 0.012487106956541538, R2 0.4809606671333313\n",
      "Eval loss 0.012495693750679493, R2 0.518679141998291\n",
      "epoch 5325, loss 0.012487000785768032, R2 0.4809646010398865\n",
      "Eval loss 0.012495579198002815, R2 0.5186833143234253\n",
      "epoch 5326, loss 0.012486894614994526, R2 0.48096901178359985\n",
      "Eval loss 0.012495462782680988, R2 0.5186876654624939\n",
      "epoch 5327, loss 0.01248678844422102, R2 0.48097342252731323\n",
      "Eval loss 0.012495346367359161, R2 0.5186922550201416\n",
      "epoch 5328, loss 0.012486684136092663, R2 0.4809780716896057\n",
      "Eval loss 0.012495231814682484, R2 0.5186971426010132\n",
      "epoch 5329, loss 0.012486577033996582, R2 0.4809821844100952\n",
      "Eval loss 0.012495115399360657, R2 0.5187011957168579\n",
      "epoch 5330, loss 0.012486469931900501, R2 0.4809873104095459\n",
      "Eval loss 0.012495001778006554, R2 0.5187062621116638\n",
      "epoch 5331, loss 0.012486365623772144, R2 0.480991005897522\n",
      "Eval loss 0.012494886294007301, R2 0.5187098979949951\n",
      "epoch 5332, loss 0.012486259452998638, R2 0.4809960722923279\n",
      "Eval loss 0.012494769878685474, R2 0.5187149047851562\n",
      "epoch 5333, loss 0.012486153282225132, R2 0.48099982738494873\n",
      "Eval loss 0.012494654394686222, R2 0.518718957901001\n",
      "epoch 5334, loss 0.0124860480427742, R2 0.48100417852401733\n",
      "Eval loss 0.012494540773332119, R2 0.5187233090400696\n",
      "epoch 5335, loss 0.01248594094067812, R2 0.4810086488723755\n",
      "Eval loss 0.012494427151978016, R2 0.5187278389930725\n",
      "epoch 5336, loss 0.012485836632549763, R2 0.4810129404067993\n",
      "Eval loss 0.012494311667978764, R2 0.5187321901321411\n",
      "epoch 5337, loss 0.012485731393098831, R2 0.481017529964447\n",
      "Eval loss 0.01249419804662466, R2 0.5187363624572754\n",
      "epoch 5338, loss 0.0124856261536479, R2 0.4810222387313843\n",
      "Eval loss 0.012494082562625408, R2 0.5187410116195679\n",
      "epoch 5339, loss 0.012485519051551819, R2 0.481026828289032\n",
      "Eval loss 0.01249396800994873, R2 0.5187452435493469\n",
      "epoch 5340, loss 0.012485414743423462, R2 0.4810307025909424\n",
      "Eval loss 0.012493854388594627, R2 0.5187497138977051\n",
      "epoch 5341, loss 0.01248530950397253, R2 0.48103487491607666\n",
      "Eval loss 0.0124937379732728, R2 0.5187541246414185\n",
      "epoch 5342, loss 0.01248520240187645, R2 0.4810391664505005\n",
      "Eval loss 0.012493624351918697, R2 0.5187584757804871\n",
      "epoch 5343, loss 0.012485099025070667, R2 0.48104363679885864\n",
      "Eval loss 0.01249350979924202, R2 0.5187633037567139\n",
      "epoch 5344, loss 0.012484993785619736, R2 0.48104798793792725\n",
      "Eval loss 0.012493397109210491, R2 0.5187678337097168\n",
      "epoch 5345, loss 0.012484886683523655, R2 0.4810524582862854\n",
      "Eval loss 0.012493282556533813, R2 0.5187720656394958\n",
      "epoch 5346, loss 0.012484785169363022, R2 0.48105669021606445\n",
      "Eval loss 0.012493168003857136, R2 0.5187761783599854\n",
      "epoch 5347, loss 0.012484678067266941, R2 0.4810616374015808\n",
      "Eval loss 0.012493054382503033, R2 0.5187807083129883\n",
      "epoch 5348, loss 0.012484573759138584, R2 0.48106616735458374\n",
      "Eval loss 0.01249293889850378, R2 0.5187851190567017\n",
      "epoch 5349, loss 0.012484468519687653, R2 0.481070339679718\n",
      "Eval loss 0.012492825277149677, R2 0.5187894105911255\n",
      "epoch 5350, loss 0.012484362348914146, R2 0.4810749292373657\n",
      "Eval loss 0.012492711655795574, R2 0.5187937617301941\n",
      "epoch 5351, loss 0.012484259903430939, R2 0.4810786843299866\n",
      "Eval loss 0.012492598034441471, R2 0.5187985897064209\n",
      "epoch 5352, loss 0.012484154663980007, R2 0.4810829162597656\n",
      "Eval loss 0.012492484413087368, R2 0.5188026428222656\n",
      "epoch 5353, loss 0.012484049424529076, R2 0.4810875654220581\n",
      "Eval loss 0.01249237172305584, R2 0.5188071727752686\n",
      "epoch 5354, loss 0.012483946047723293, R2 0.48109155893325806\n",
      "Eval loss 0.012492259033024311, R2 0.5188111066818237\n",
      "epoch 5355, loss 0.012483841739594936, R2 0.4810960292816162\n",
      "Eval loss 0.012492144480347633, R2 0.5188158750534058\n",
      "epoch 5356, loss 0.012483738362789154, R2 0.4811002016067505\n",
      "Eval loss 0.012492029927670956, R2 0.5188201665878296\n",
      "epoch 5357, loss 0.012483632192015648, R2 0.4811047911643982\n",
      "Eval loss 0.012491917237639427, R2 0.5188241004943848\n",
      "epoch 5358, loss 0.012483528815209866, R2 0.4811089038848877\n",
      "Eval loss 0.012491804547607899, R2 0.5188291668891907\n",
      "epoch 5359, loss 0.012483424507081509, R2 0.4811134338378906\n",
      "Eval loss 0.01249169185757637, R2 0.5188335180282593\n",
      "epoch 5360, loss 0.012483321130275726, R2 0.4811173677444458\n",
      "Eval loss 0.012491577304899693, R2 0.518837571144104\n",
      "epoch 5361, loss 0.012483215890824795, R2 0.48112189769744873\n",
      "Eval loss 0.012491464614868164, R2 0.5188419818878174\n",
      "epoch 5362, loss 0.012483112514019012, R2 0.48112618923187256\n",
      "Eval loss 0.012491351924836636, R2 0.5188463926315308\n",
      "epoch 5363, loss 0.012483007274568081, R2 0.48113107681274414\n",
      "Eval loss 0.012491239234805107, R2 0.5188503265380859\n",
      "epoch 5364, loss 0.012482903897762299, R2 0.48113536834716797\n",
      "Eval loss 0.012491127476096153, R2 0.5188547968864441\n",
      "epoch 5365, loss 0.012482801452279091, R2 0.4811391234397888\n",
      "Eval loss 0.01249101385474205, R2 0.5188595056533813\n",
      "epoch 5366, loss 0.01248269621282816, R2 0.4811438322067261\n",
      "Eval loss 0.012490900233387947, R2 0.5188635587692261\n",
      "epoch 5367, loss 0.012482593767344952, R2 0.48114776611328125\n",
      "Eval loss 0.012490788474678993, R2 0.5188679695129395\n",
      "epoch 5368, loss 0.012482489459216595, R2 0.48115211725234985\n",
      "Eval loss 0.012490675784647465, R2 0.5188724994659424\n",
      "epoch 5369, loss 0.012482386082410812, R2 0.4811564087867737\n",
      "Eval loss 0.012490563094615936, R2 0.5188763737678528\n",
      "epoch 5370, loss 0.01248228270560503, R2 0.4811611771583557\n",
      "Eval loss 0.012490450404584408, R2 0.5188807249069214\n",
      "epoch 5371, loss 0.012482179328799248, R2 0.48116499185562134\n",
      "Eval loss 0.012490339577198029, R2 0.5188850164413452\n",
      "epoch 5372, loss 0.01248207502067089, R2 0.4811697006225586\n",
      "Eval loss 0.012490225955843925, R2 0.5188896656036377\n",
      "epoch 5373, loss 0.012481973506510258, R2 0.481173574924469\n",
      "Eval loss 0.012490114197134972, R2 0.5188940763473511\n",
      "epoch 5374, loss 0.012481870129704475, R2 0.4811776876449585\n",
      "Eval loss 0.012490000575780869, R2 0.5188987255096436\n",
      "epoch 5375, loss 0.012481765821576118, R2 0.481182336807251\n",
      "Eval loss 0.01248988974839449, R2 0.5189024209976196\n",
      "epoch 5376, loss 0.012481662444770336, R2 0.481187105178833\n",
      "Eval loss 0.012489777989685535, R2 0.518906831741333\n",
      "epoch 5377, loss 0.012481559999287128, R2 0.4811907410621643\n",
      "Eval loss 0.012489665299654007, R2 0.5189111232757568\n",
      "epoch 5378, loss 0.01248145755380392, R2 0.48119497299194336\n",
      "Eval loss 0.012489554472267628, R2 0.5189155340194702\n",
      "epoch 5379, loss 0.012481355108320713, R2 0.4811992645263672\n",
      "Eval loss 0.0124894417822361, R2 0.5189197063446045\n",
      "epoch 5380, loss 0.01248125173151493, R2 0.481203556060791\n",
      "Eval loss 0.012489330023527145, R2 0.5189241170883179\n",
      "epoch 5381, loss 0.012481147423386574, R2 0.48120784759521484\n",
      "Eval loss 0.012489217333495617, R2 0.5189285278320312\n",
      "epoch 5382, loss 0.01248104590922594, R2 0.48121213912963867\n",
      "Eval loss 0.012489106506109238, R2 0.5189326405525208\n",
      "epoch 5383, loss 0.012480943463742733, R2 0.4812163710594177\n",
      "Eval loss 0.012488995678722858, R2 0.5189367532730103\n",
      "epoch 5384, loss 0.012480841018259525, R2 0.4812213182449341\n",
      "Eval loss 0.01248888298869133, R2 0.5189411044120789\n",
      "epoch 5385, loss 0.012480737641453743, R2 0.48122507333755493\n",
      "Eval loss 0.012488771229982376, R2 0.5189456939697266\n",
      "epoch 5386, loss 0.01248063426464796, R2 0.48122936487197876\n",
      "Eval loss 0.012488661333918571, R2 0.5189499258995056\n",
      "epoch 5387, loss 0.012480534613132477, R2 0.48123353719711304\n",
      "Eval loss 0.012488550506532192, R2 0.5189539194107056\n",
      "epoch 5388, loss 0.012480429373681545, R2 0.4812377095222473\n",
      "Eval loss 0.012488437816500664, R2 0.5189582705497742\n",
      "epoch 5389, loss 0.012480328790843487, R2 0.4812420606613159\n",
      "Eval loss 0.012488326989114285, R2 0.5189626216888428\n",
      "epoch 5390, loss 0.012480226345360279, R2 0.4812466502189636\n",
      "Eval loss 0.012488216161727905, R2 0.5189672112464905\n",
      "epoch 5391, loss 0.012480124831199646, R2 0.481251060962677\n",
      "Eval loss 0.012488104403018951, R2 0.5189715027809143\n",
      "epoch 5392, loss 0.012480022385716438, R2 0.48125481605529785\n",
      "Eval loss 0.012487993575632572, R2 0.5189756751060486\n",
      "epoch 5393, loss 0.012479919008910656, R2 0.48125892877578735\n",
      "Eval loss 0.012487882748246193, R2 0.5189799070358276\n",
      "epoch 5394, loss 0.012479818426072598, R2 0.48126381635665894\n",
      "Eval loss 0.012487771920859814, R2 0.518984317779541\n",
      "epoch 5395, loss 0.01247971598058939, R2 0.4812670350074768\n",
      "Eval loss 0.012487661093473434, R2 0.5189886093139648\n",
      "epoch 5396, loss 0.012479614466428757, R2 0.48127228021621704\n",
      "Eval loss 0.012487553060054779, R2 0.5189926624298096\n",
      "epoch 5397, loss 0.012479512020945549, R2 0.48127585649490356\n",
      "Eval loss 0.01248744037002325, R2 0.5189967155456543\n",
      "epoch 5398, loss 0.01247941143810749, R2 0.48128020763397217\n",
      "Eval loss 0.012487330473959446, R2 0.5190010070800781\n",
      "epoch 5399, loss 0.012479308992624283, R2 0.48128432035446167\n",
      "Eval loss 0.012487218715250492, R2 0.5190054774284363\n",
      "epoch 5400, loss 0.012479208409786224, R2 0.4812886714935303\n",
      "Eval loss 0.012487108819186687, R2 0.5190098285675049\n",
      "epoch 5401, loss 0.012479105964303017, R2 0.4812929034233093\n",
      "Eval loss 0.012486999854445457, R2 0.5190142393112183\n",
      "epoch 5402, loss 0.012479004450142384, R2 0.48129695653915405\n",
      "Eval loss 0.012486889027059078, R2 0.5190179347991943\n",
      "epoch 5403, loss 0.012478903867304325, R2 0.48130112886428833\n",
      "Eval loss 0.012486778199672699, R2 0.5190227031707764\n",
      "epoch 5404, loss 0.012478801421821117, R2 0.4813055396080017\n",
      "Eval loss 0.01248666737228632, R2 0.5190265774726868\n",
      "epoch 5405, loss 0.012478701770305634, R2 0.48131006956100464\n",
      "Eval loss 0.01248655840754509, R2 0.5190312266349792\n",
      "epoch 5406, loss 0.012478599324822426, R2 0.4813137650489807\n",
      "Eval loss 0.012486448511481285, R2 0.5190350413322449\n",
      "epoch 5407, loss 0.012478498741984367, R2 0.48131799697875977\n",
      "Eval loss 0.012486337684094906, R2 0.5190393924713135\n",
      "epoch 5408, loss 0.012478397227823734, R2 0.48132216930389404\n",
      "Eval loss 0.012486226856708527, R2 0.5190434455871582\n",
      "epoch 5409, loss 0.012478296644985676, R2 0.4813264012336731\n",
      "Eval loss 0.012486117891967297, R2 0.5190480351448059\n",
      "epoch 5410, loss 0.012478195130825043, R2 0.48133111000061035\n",
      "Eval loss 0.012486007995903492, R2 0.5190520286560059\n",
      "epoch 5411, loss 0.012478094547986984, R2 0.4813344478607178\n",
      "Eval loss 0.012485899031162262, R2 0.5190562009811401\n",
      "epoch 5412, loss 0.012477993033826351, R2 0.4813395142555237\n",
      "Eval loss 0.012485788203775883, R2 0.519060492515564\n",
      "epoch 5413, loss 0.012477892450988293, R2 0.4813433289527893\n",
      "Eval loss 0.012485679239034653, R2 0.5190644860267639\n",
      "epoch 5414, loss 0.012477792799472809, R2 0.48134732246398926\n",
      "Eval loss 0.012485571205615997, R2 0.5190690755844116\n",
      "epoch 5415, loss 0.012477690353989601, R2 0.4813520908355713\n",
      "Eval loss 0.012485460378229618, R2 0.5190731287002563\n",
      "epoch 5416, loss 0.012477590702474117, R2 0.4813557267189026\n",
      "Eval loss 0.012485349550843239, R2 0.5190773606300354\n",
      "epoch 5417, loss 0.012477490119636059, R2 0.48135989904403687\n",
      "Eval loss 0.012485241517424583, R2 0.5190816521644592\n",
      "epoch 5418, loss 0.012477390468120575, R2 0.48136407136917114\n",
      "Eval loss 0.012485133484005928, R2 0.5190858244895935\n",
      "epoch 5419, loss 0.012477288953959942, R2 0.4813687801361084\n",
      "Eval loss 0.012485022656619549, R2 0.519089937210083\n",
      "epoch 5420, loss 0.012477189302444458, R2 0.4813724160194397\n",
      "Eval loss 0.012484913691878319, R2 0.5190941095352173\n",
      "epoch 5421, loss 0.0124770887196064, R2 0.4813770651817322\n",
      "Eval loss 0.012484805658459663, R2 0.519098162651062\n",
      "epoch 5422, loss 0.012476989068090916, R2 0.4813805818557739\n",
      "Eval loss 0.012484696693718433, R2 0.5191026926040649\n",
      "epoch 5423, loss 0.012476888485252857, R2 0.4813850522041321\n",
      "Eval loss 0.012484587728977203, R2 0.5191066861152649\n",
      "epoch 5424, loss 0.012476788833737373, R2 0.48138904571533203\n",
      "Eval loss 0.012484477832913399, R2 0.5191110372543335\n",
      "epoch 5425, loss 0.012476688250899315, R2 0.481393039226532\n",
      "Eval loss 0.012484368868172169, R2 0.5191153883934021\n",
      "epoch 5426, loss 0.012476587668061256, R2 0.48139744997024536\n",
      "Eval loss 0.012484261766076088, R2 0.519119143486023\n",
      "epoch 5427, loss 0.012476488947868347, R2 0.48140132427215576\n",
      "Eval loss 0.012484152801334858, R2 0.5191235542297363\n",
      "epoch 5428, loss 0.012476388365030289, R2 0.48140591382980347\n",
      "Eval loss 0.012484044767916203, R2 0.5191277265548706\n",
      "epoch 5429, loss 0.012476288713514805, R2 0.48141002655029297\n",
      "Eval loss 0.012483935803174973, R2 0.519132137298584\n",
      "epoch 5430, loss 0.012476189993321896, R2 0.48141396045684814\n",
      "Eval loss 0.012483827769756317, R2 0.5191362500190735\n",
      "epoch 5431, loss 0.012476087547838688, R2 0.4814187288284302\n",
      "Eval loss 0.012483718805015087, R2 0.5191400051116943\n",
      "epoch 5432, loss 0.012475988827645779, R2 0.4814223051071167\n",
      "Eval loss 0.012483607977628708, R2 0.5191445350646973\n",
      "epoch 5433, loss 0.012475891038775444, R2 0.4814263582229614\n",
      "Eval loss 0.012483501806855202, R2 0.5191487073898315\n",
      "epoch 5434, loss 0.012475792318582535, R2 0.4814304709434509\n",
      "Eval loss 0.012483393773436546, R2 0.519152820110321\n",
      "epoch 5435, loss 0.012475690804421902, R2 0.48143470287323\n",
      "Eval loss 0.012483285740017891, R2 0.519156813621521\n",
      "epoch 5436, loss 0.012475593015551567, R2 0.48143893480300903\n",
      "Eval loss 0.012483176775276661, R2 0.5191609859466553\n",
      "epoch 5437, loss 0.012475492432713509, R2 0.481442928314209\n",
      "Eval loss 0.01248306967318058, R2 0.5191651582717896\n",
      "epoch 5438, loss 0.012475394643843174, R2 0.48144716024398804\n",
      "Eval loss 0.01248296070843935, R2 0.5191695094108582\n",
      "epoch 5439, loss 0.012475294061005116, R2 0.48145121335983276\n",
      "Eval loss 0.01248285360634327, R2 0.5191737413406372\n",
      "epoch 5440, loss 0.012475195340812206, R2 0.48145580291748047\n",
      "Eval loss 0.012482746504247189, R2 0.5191777944564819\n",
      "epoch 5441, loss 0.012475096620619297, R2 0.4814595580101013\n",
      "Eval loss 0.012482637539505959, R2 0.5191816687583923\n",
      "epoch 5442, loss 0.012474997900426388, R2 0.4814636707305908\n",
      "Eval loss 0.012482532300055027, R2 0.5191857218742371\n",
      "epoch 5443, loss 0.012474899180233479, R2 0.48146742582321167\n",
      "Eval loss 0.012482422403991222, R2 0.5191901922225952\n",
      "epoch 5444, loss 0.01247480046004057, R2 0.4814718961715698\n",
      "Eval loss 0.012482316233217716, R2 0.5191942453384399\n",
      "epoch 5445, loss 0.01247469987720251, R2 0.48147672414779663\n",
      "Eval loss 0.01248220819979906, R2 0.5191985368728638\n",
      "epoch 5446, loss 0.012474602088332176, R2 0.48148059844970703\n",
      "Eval loss 0.012482100166380405, R2 0.5192023515701294\n",
      "epoch 5447, loss 0.012474502436816692, R2 0.4814841151237488\n",
      "Eval loss 0.012481993064284325, R2 0.5192067623138428\n",
      "epoch 5448, loss 0.012474404647946358, R2 0.48148882389068604\n",
      "Eval loss 0.012481885962188244, R2 0.519210934638977\n",
      "epoch 5449, loss 0.012474305927753448, R2 0.481492280960083\n",
      "Eval loss 0.012481778860092163, R2 0.5192148685455322\n",
      "epoch 5450, loss 0.012474208138883114, R2 0.48149681091308594\n",
      "Eval loss 0.012481670826673508, R2 0.5192195177078247\n",
      "epoch 5451, loss 0.012474109418690205, R2 0.4815000891685486\n",
      "Eval loss 0.012481563724577427, R2 0.5192234516143799\n",
      "epoch 5452, loss 0.012474010698497295, R2 0.48150455951690674\n",
      "Eval loss 0.01248145755380392, R2 0.5192273855209351\n",
      "epoch 5453, loss 0.012473911978304386, R2 0.4815087914466858\n",
      "Eval loss 0.01248135045170784, R2 0.5192312002182007\n",
      "epoch 5454, loss 0.012473814189434052, R2 0.48151320219039917\n",
      "Eval loss 0.01248124334961176, R2 0.5192356705665588\n",
      "epoch 5455, loss 0.012473714537918568, R2 0.48151683807373047\n",
      "Eval loss 0.012481137178838253, R2 0.519239604473114\n",
      "epoch 5456, loss 0.012473617680370808, R2 0.4815213680267334\n",
      "Eval loss 0.012481029145419598, R2 0.5192440748214722\n",
      "epoch 5457, loss 0.012473518960177898, R2 0.4815259575843811\n",
      "Eval loss 0.012480923905968666, R2 0.519247829914093\n",
      "epoch 5458, loss 0.012473421171307564, R2 0.4815290570259094\n",
      "Eval loss 0.01248081587255001, R2 0.5192519426345825\n",
      "epoch 5459, loss 0.01247332151979208, R2 0.4815328121185303\n",
      "Eval loss 0.01248070877045393, R2 0.5192559957504272\n",
      "epoch 5460, loss 0.01247322466224432, R2 0.48153722286224365\n",
      "Eval loss 0.012480603531002998, R2 0.5192601680755615\n",
      "epoch 5461, loss 0.012473126873373985, R2 0.4815414547920227\n",
      "Eval loss 0.012480498291552067, R2 0.5192643404006958\n",
      "epoch 5462, loss 0.01247302908450365, R2 0.4815453290939331\n",
      "Eval loss 0.012480390258133411, R2 0.5192686319351196\n",
      "epoch 5463, loss 0.012472933158278465, R2 0.4815495014190674\n",
      "Eval loss 0.012480284087359905, R2 0.5192726254463196\n",
      "epoch 5464, loss 0.012472834438085556, R2 0.48155343532562256\n",
      "Eval loss 0.012480178847908974, R2 0.5192767977714539\n",
      "epoch 5465, loss 0.012472737580537796, R2 0.48155760765075684\n",
      "Eval loss 0.012480070814490318, R2 0.5192805528640747\n",
      "epoch 5466, loss 0.012472638860344887, R2 0.48156219720840454\n",
      "Eval loss 0.012479965575039387, R2 0.5192846059799194\n",
      "epoch 5467, loss 0.012472541071474552, R2 0.4815656542778015\n",
      "Eval loss 0.01247985940426588, R2 0.519288957118988\n",
      "epoch 5468, loss 0.012472444213926792, R2 0.4815698266029358\n",
      "Eval loss 0.012479753233492374, R2 0.5192927718162537\n",
      "epoch 5469, loss 0.012472346425056458, R2 0.4815737009048462\n",
      "Eval loss 0.012479648925364017, R2 0.5192967653274536\n",
      "epoch 5470, loss 0.012472248636186123, R2 0.4815777540206909\n",
      "Eval loss 0.012479541823267937, R2 0.5193013548851013\n",
      "epoch 5471, loss 0.012472150847315788, R2 0.48158198595046997\n",
      "Eval loss 0.01247943565249443, R2 0.5193052887916565\n",
      "epoch 5472, loss 0.012472055852413177, R2 0.4815864562988281\n",
      "Eval loss 0.012479330413043499, R2 0.5193096399307251\n",
      "epoch 5473, loss 0.012471957132220268, R2 0.48159003257751465\n",
      "Eval loss 0.012479224242269993, R2 0.5193135738372803\n",
      "epoch 5474, loss 0.012471861205995083, R2 0.4815935492515564\n",
      "Eval loss 0.012479119002819061, R2 0.5193171501159668\n",
      "epoch 5475, loss 0.012471763417124748, R2 0.4815980792045593\n",
      "Eval loss 0.012479014694690704, R2 0.5193216800689697\n",
      "epoch 5476, loss 0.012471666559576988, R2 0.4816019535064697\n",
      "Eval loss 0.012478908523917198, R2 0.5193254947662354\n",
      "epoch 5477, loss 0.012471569702029228, R2 0.481606125831604\n",
      "Eval loss 0.012478802353143692, R2 0.5193297863006592\n",
      "epoch 5478, loss 0.012471471913158894, R2 0.4816102385520935\n",
      "Eval loss 0.012478698045015335, R2 0.5193338394165039\n",
      "epoch 5479, loss 0.012471375055611134, R2 0.48161405324935913\n",
      "Eval loss 0.012478591874241829, R2 0.5193377733230591\n",
      "epoch 5480, loss 0.012471279129385948, R2 0.48161858320236206\n",
      "Eval loss 0.012478486634790897, R2 0.5193419456481934\n",
      "epoch 5481, loss 0.012471182271838188, R2 0.4816220998764038\n",
      "Eval loss 0.01247838232666254, R2 0.5193458795547485\n",
      "epoch 5482, loss 0.012471085414290428, R2 0.48162662982940674\n",
      "Eval loss 0.012478276155889034, R2 0.5193502306938171\n",
      "epoch 5483, loss 0.012470989488065243, R2 0.48162996768951416\n",
      "Eval loss 0.012478171847760677, R2 0.519353985786438\n",
      "epoch 5484, loss 0.012470893561840057, R2 0.48163408041000366\n",
      "Eval loss 0.012478065676987171, R2 0.5193580389022827\n",
      "epoch 5485, loss 0.012470795772969723, R2 0.48163819313049316\n",
      "Eval loss 0.01247796043753624, R2 0.519362211227417\n",
      "epoch 5486, loss 0.012470699846744537, R2 0.481641948223114\n",
      "Eval loss 0.012477857992053032, R2 0.5193657875061035\n",
      "epoch 5487, loss 0.012470603920519352, R2 0.4816462993621826\n",
      "Eval loss 0.012477751821279526, R2 0.5193700194358826\n",
      "epoch 5488, loss 0.012470507062971592, R2 0.481650173664093\n",
      "Eval loss 0.012477647513151169, R2 0.5193743109703064\n",
      "epoch 5489, loss 0.012470412068068981, R2 0.4816541075706482\n",
      "Eval loss 0.012477542273700237, R2 0.5193782448768616\n",
      "epoch 5490, loss 0.012470316141843796, R2 0.48165857791900635\n",
      "Eval loss 0.012477437034249306, R2 0.519382119178772\n",
      "epoch 5491, loss 0.012470218352973461, R2 0.4816617965698242\n",
      "Eval loss 0.012477333657443523, R2 0.5193866491317749\n",
      "epoch 5492, loss 0.01247012335807085, R2 0.4816661477088928\n",
      "Eval loss 0.012477229349315166, R2 0.5193902850151062\n",
      "epoch 5493, loss 0.01247002836316824, R2 0.48167043924331665\n",
      "Eval loss 0.01247712317854166, R2 0.519394040107727\n",
      "epoch 5494, loss 0.01246993150562048, R2 0.48167407512664795\n",
      "Eval loss 0.012477019801735878, R2 0.5193982124328613\n",
      "epoch 5495, loss 0.012469836510717869, R2 0.4816780686378479\n",
      "Eval loss 0.012476915493607521, R2 0.519402801990509\n",
      "epoch 5496, loss 0.012469738721847534, R2 0.4816821217536926\n",
      "Eval loss 0.012476812116801739, R2 0.5194063186645508\n",
      "epoch 5497, loss 0.012469643726944923, R2 0.481686532497406\n",
      "Eval loss 0.012476707808673382, R2 0.5194103717803955\n",
      "epoch 5498, loss 0.012469547800719738, R2 0.48169004917144775\n",
      "Eval loss 0.012476603500545025, R2 0.5194140672683716\n",
      "epoch 5499, loss 0.012469453737139702, R2 0.48169392347335815\n",
      "Eval loss 0.012476499192416668, R2 0.5194182395935059\n",
      "epoch 5500, loss 0.012469358742237091, R2 0.48169809579849243\n",
      "Eval loss 0.012476395815610886, R2 0.5194226503372192\n",
      "epoch 5501, loss 0.012469262816011906, R2 0.4817025065422058\n",
      "Eval loss 0.012476291507482529, R2 0.5194264054298401\n",
      "epoch 5502, loss 0.01246916688978672, R2 0.481705904006958\n",
      "Eval loss 0.012476188130676746, R2 0.5194302797317505\n",
      "epoch 5503, loss 0.01246907003223896, R2 0.48171019554138184\n",
      "Eval loss 0.01247608382254839, R2 0.5194340944290161\n",
      "epoch 5504, loss 0.01246897503733635, R2 0.48171383142471313\n",
      "Eval loss 0.012475979514420033, R2 0.5194382667541504\n",
      "epoch 5505, loss 0.012468880973756313, R2 0.48171794414520264\n",
      "Eval loss 0.012475877068936825, R2 0.5194425582885742\n",
      "epoch 5506, loss 0.012468785047531128, R2 0.48172205686569214\n",
      "Eval loss 0.012475772760808468, R2 0.5194462537765503\n",
      "epoch 5507, loss 0.012468690052628517, R2 0.48172587156295776\n",
      "Eval loss 0.01247567031532526, R2 0.519450306892395\n",
      "epoch 5508, loss 0.012468595057725906, R2 0.4817294478416443\n",
      "Eval loss 0.012475566007196903, R2 0.5194544792175293\n",
      "epoch 5509, loss 0.012468500062823296, R2 0.4817335605621338\n",
      "Eval loss 0.012475462630391121, R2 0.5194582939147949\n",
      "epoch 5510, loss 0.012468405067920685, R2 0.48173820972442627\n",
      "Eval loss 0.012475359253585339, R2 0.5194624662399292\n",
      "epoch 5511, loss 0.0124683091416955, R2 0.48174166679382324\n",
      "Eval loss 0.012475255876779556, R2 0.5194664001464844\n",
      "epoch 5512, loss 0.012468215078115463, R2 0.4817456007003784\n",
      "Eval loss 0.012475153431296349, R2 0.5194700956344604\n",
      "epoch 5513, loss 0.012468121014535427, R2 0.4817495346069336\n",
      "Eval loss 0.012475050054490566, R2 0.5194739103317261\n",
      "epoch 5514, loss 0.012468026019632816, R2 0.48175346851348877\n",
      "Eval loss 0.012474944815039635, R2 0.5194782614707947\n",
      "epoch 5515, loss 0.012467931024730206, R2 0.48175740242004395\n",
      "Eval loss 0.012474844232201576, R2 0.5194826126098633\n",
      "epoch 5516, loss 0.012467836029827595, R2 0.4817611575126648\n",
      "Eval loss 0.012474740855395794, R2 0.5194863080978394\n",
      "epoch 5517, loss 0.012467741966247559, R2 0.48176509141921997\n",
      "Eval loss 0.012474637478590012, R2 0.5194902420043945\n",
      "epoch 5518, loss 0.012467647902667522, R2 0.48176902532577515\n",
      "Eval loss 0.012474535033106804, R2 0.5194941759109497\n",
      "epoch 5519, loss 0.012467552907764912, R2 0.4817732572555542\n",
      "Eval loss 0.012474431656301022, R2 0.5194979906082153\n",
      "epoch 5520, loss 0.012467458844184875, R2 0.48177671432495117\n",
      "Eval loss 0.012474329210817814, R2 0.5195021629333496\n",
      "epoch 5521, loss 0.012467363849282265, R2 0.4817809462547302\n",
      "Eval loss 0.012474225834012032, R2 0.5195056796073914\n",
      "epoch 5522, loss 0.012467269785702229, R2 0.4817847013473511\n",
      "Eval loss 0.012474124319851398, R2 0.5195099115371704\n",
      "epoch 5523, loss 0.012467175722122192, R2 0.48178863525390625\n",
      "Eval loss 0.01247402187436819, R2 0.519513726234436\n",
      "epoch 5524, loss 0.012467080727219582, R2 0.4817925691604614\n",
      "Eval loss 0.012473918497562408, R2 0.5195177793502808\n",
      "epoch 5525, loss 0.01246698759496212, R2 0.48179763555526733\n",
      "Eval loss 0.0124738160520792, R2 0.5195214748382568\n",
      "epoch 5526, loss 0.012466893531382084, R2 0.4818001985549927\n",
      "Eval loss 0.012473714537918568, R2 0.5195257663726807\n",
      "epoch 5527, loss 0.012466798536479473, R2 0.4818043112754822\n",
      "Eval loss 0.01247361209243536, R2 0.5195299386978149\n",
      "epoch 5528, loss 0.012466705404222012, R2 0.4818081855773926\n",
      "Eval loss 0.012473508715629578, R2 0.5195338726043701\n",
      "epoch 5529, loss 0.012466611340641975, R2 0.4818119406700134\n",
      "Eval loss 0.012473409064114094, R2 0.5195373296737671\n",
      "epoch 5530, loss 0.012466518208384514, R2 0.48181647062301636\n",
      "Eval loss 0.012473304755985737, R2 0.5195414423942566\n",
      "epoch 5531, loss 0.012466425076127052, R2 0.4818199872970581\n",
      "Eval loss 0.012473203241825104, R2 0.5195451974868774\n",
      "epoch 5532, loss 0.012466330081224442, R2 0.4818236231803894\n",
      "Eval loss 0.01247310172766447, R2 0.5195491313934326\n",
      "epoch 5533, loss 0.012466236017644405, R2 0.48182785511016846\n",
      "Eval loss 0.012472999282181263, R2 0.5195533037185669\n",
      "epoch 5534, loss 0.012466142885386944, R2 0.48183155059814453\n",
      "Eval loss 0.012472896836698055, R2 0.5195568799972534\n",
      "epoch 5535, loss 0.012466048821806908, R2 0.48183560371398926\n",
      "Eval loss 0.012472795322537422, R2 0.5195608139038086\n",
      "epoch 5536, loss 0.01246595662087202, R2 0.48183947801589966\n",
      "Eval loss 0.012472694739699364, R2 0.5195648074150085\n",
      "epoch 5537, loss 0.012465862557291985, R2 0.4818432331085205\n",
      "Eval loss 0.012472592294216156, R2 0.5195684432983398\n",
      "epoch 5538, loss 0.012465769425034523, R2 0.48184704780578613\n",
      "Eval loss 0.012472489848732948, R2 0.5195724964141846\n",
      "epoch 5539, loss 0.012465675361454487, R2 0.48185133934020996\n",
      "Eval loss 0.01247238926589489, R2 0.5195768475532532\n",
      "epoch 5540, loss 0.0124655831605196, R2 0.48185503482818604\n",
      "Eval loss 0.012472287751734257, R2 0.5195804834365845\n",
      "epoch 5541, loss 0.012465490028262138, R2 0.48185884952545166\n",
      "Eval loss 0.012472186237573624, R2 0.5195845365524292\n",
      "epoch 5542, loss 0.012465396896004677, R2 0.4818629026412964\n",
      "Eval loss 0.01247208472341299, R2 0.5195887088775635\n",
      "epoch 5543, loss 0.01246530469506979, R2 0.48186689615249634\n",
      "Eval loss 0.012471983209252357, R2 0.5195924043655396\n",
      "epoch 5544, loss 0.012465209700167179, R2 0.4818706512451172\n",
      "Eval loss 0.012471881695091724, R2 0.5195963978767395\n",
      "epoch 5545, loss 0.012465117499232292, R2 0.48187482357025146\n",
      "Eval loss 0.01247178204357624, R2 0.5195997953414917\n",
      "epoch 5546, loss 0.012465025298297405, R2 0.48187804222106934\n",
      "Eval loss 0.012471678666770458, R2 0.5196040868759155\n",
      "epoch 5547, loss 0.012464931234717369, R2 0.48188209533691406\n",
      "Eval loss 0.0124715780839324, R2 0.5196078419685364\n",
      "epoch 5548, loss 0.012464839033782482, R2 0.48188579082489014\n",
      "Eval loss 0.012471477501094341, R2 0.5196122527122498\n",
      "epoch 5549, loss 0.01246474590152502, R2 0.48189014196395874\n",
      "Eval loss 0.012471375986933708, R2 0.5196157097816467\n",
      "epoch 5550, loss 0.012464654631912708, R2 0.4818934202194214\n",
      "Eval loss 0.01247127540409565, R2 0.5196195840835571\n",
      "epoch 5551, loss 0.012464559637010098, R2 0.48189735412597656\n",
      "Eval loss 0.012471174821257591, R2 0.5196231603622437\n",
      "epoch 5552, loss 0.01246446929872036, R2 0.48190128803253174\n",
      "Eval loss 0.012471075169742107, R2 0.5196272134780884\n",
      "epoch 5553, loss 0.012464375235140324, R2 0.4819052219390869\n",
      "Eval loss 0.012470973655581474, R2 0.5196312665939331\n",
      "epoch 5554, loss 0.012464282102882862, R2 0.481908917427063\n",
      "Eval loss 0.012470872141420841, R2 0.5196352005004883\n",
      "epoch 5555, loss 0.01246419083327055, R2 0.4819125533103943\n",
      "Eval loss 0.012470772489905357, R2 0.5196391344070435\n",
      "epoch 5556, loss 0.012464097701013088, R2 0.4819169044494629\n",
      "Eval loss 0.012470672838389874, R2 0.5196425914764404\n",
      "epoch 5557, loss 0.012464006431400776, R2 0.48192036151885986\n",
      "Eval loss 0.012470570392906666, R2 0.5196468830108643\n",
      "epoch 5558, loss 0.012463914230465889, R2 0.48192471265792847\n",
      "Eval loss 0.012470470741391182, R2 0.5196504592895508\n",
      "epoch 5559, loss 0.012463822029531002, R2 0.48192787170410156\n",
      "Eval loss 0.012470371089875698, R2 0.5196544528007507\n",
      "epoch 5560, loss 0.012463729828596115, R2 0.4819318652153015\n",
      "Eval loss 0.012470269575715065, R2 0.519658088684082\n",
      "epoch 5561, loss 0.012463637627661228, R2 0.4819357395172119\n",
      "Eval loss 0.012470169924199581, R2 0.5196622014045715\n",
      "epoch 5562, loss 0.012463544495403767, R2 0.4819393754005432\n",
      "Eval loss 0.012470070272684097, R2 0.5196657776832581\n",
      "epoch 5563, loss 0.012463455088436604, R2 0.4819433093070984\n",
      "Eval loss 0.012469969689846039, R2 0.5196700096130371\n",
      "epoch 5564, loss 0.012463361956179142, R2 0.4819471836090088\n",
      "Eval loss 0.012469870038330555, R2 0.5196739435195923\n",
      "epoch 5565, loss 0.012463269755244255, R2 0.4819509983062744\n",
      "Eval loss 0.012469770386815071, R2 0.5196775794029236\n",
      "epoch 5566, loss 0.012463178485631943, R2 0.48195528984069824\n",
      "Eval loss 0.012469667941331863, R2 0.5196813344955444\n",
      "epoch 5567, loss 0.01246308721601963, R2 0.48195910453796387\n",
      "Eval loss 0.012469570152461529, R2 0.5196852684020996\n",
      "epoch 5568, loss 0.012462995946407318, R2 0.48196256160736084\n",
      "Eval loss 0.01246946956962347, R2 0.5196893215179443\n",
      "epoch 5569, loss 0.012462903745472431, R2 0.48196637630462646\n",
      "Eval loss 0.012469370849430561, R2 0.51969313621521\n",
      "epoch 5570, loss 0.012462811544537544, R2 0.4819701910018921\n",
      "Eval loss 0.012469270266592503, R2 0.5196965932846069\n",
      "epoch 5571, loss 0.012462720274925232, R2 0.4819740056991577\n",
      "Eval loss 0.012469171546399593, R2 0.5197009444236755\n",
      "epoch 5572, loss 0.01246262900531292, R2 0.481977641582489\n",
      "Eval loss 0.01246907189488411, R2 0.5197043418884277\n",
      "epoch 5573, loss 0.012462537735700607, R2 0.48198139667510986\n",
      "Eval loss 0.012468972243368626, R2 0.5197083353996277\n",
      "epoch 5574, loss 0.01246244553476572, R2 0.48198509216308594\n",
      "Eval loss 0.012468871660530567, R2 0.5197124481201172\n",
      "epoch 5575, loss 0.012462354265153408, R2 0.4819890260696411\n",
      "Eval loss 0.012468772940337658, R2 0.5197159051895142\n",
      "epoch 5576, loss 0.012462264858186245, R2 0.48199278116226196\n",
      "Eval loss 0.012468673288822174, R2 0.5197199583053589\n",
      "epoch 5577, loss 0.012462172657251358, R2 0.4819967746734619\n",
      "Eval loss 0.012468574568629265, R2 0.519723653793335\n",
      "epoch 5578, loss 0.01246208231896162, R2 0.48200035095214844\n",
      "Eval loss 0.012468475848436356, R2 0.5197271108627319\n",
      "epoch 5579, loss 0.012461991049349308, R2 0.4820038080215454\n",
      "Eval loss 0.012468377128243446, R2 0.5197309851646423\n",
      "epoch 5580, loss 0.012461898848414421, R2 0.4820084571838379\n",
      "Eval loss 0.012468277476727962, R2 0.519734799861908\n",
      "epoch 5581, loss 0.012461809441447258, R2 0.48201167583465576\n",
      "Eval loss 0.012468177825212479, R2 0.5197389125823975\n",
      "epoch 5582, loss 0.012461717240512371, R2 0.4820154905319214\n",
      "Eval loss 0.012468080967664719, R2 0.5197428464889526\n",
      "epoch 5583, loss 0.012461627833545208, R2 0.48201990127563477\n",
      "Eval loss 0.012467979453504086, R2 0.5197468996047974\n",
      "epoch 5584, loss 0.012461536563932896, R2 0.48202306032180786\n",
      "Eval loss 0.012467881664633751, R2 0.5197501182556152\n",
      "epoch 5585, loss 0.012461445294320583, R2 0.482027530670166\n",
      "Eval loss 0.012467782013118267, R2 0.51975417137146\n",
      "epoch 5586, loss 0.012461354956030846, R2 0.48203057050704956\n",
      "Eval loss 0.012467684224247932, R2 0.5197579860687256\n",
      "epoch 5587, loss 0.012461264617741108, R2 0.4820343255996704\n",
      "Eval loss 0.012467584572732449, R2 0.5197615027427673\n",
      "epoch 5588, loss 0.01246117427945137, R2 0.48203808069229126\n",
      "Eval loss 0.012467486783862114, R2 0.5197655558586121\n",
      "epoch 5589, loss 0.012461083009839058, R2 0.48204201459884644\n",
      "Eval loss 0.01246738899499178, R2 0.5197693109512329\n",
      "epoch 5590, loss 0.012460993602871895, R2 0.4820457696914673\n",
      "Eval loss 0.012467289343476295, R2 0.5197731256484985\n",
      "epoch 5591, loss 0.012460903264582157, R2 0.48204952478408813\n",
      "Eval loss 0.012467190623283386, R2 0.5197770595550537\n",
      "epoch 5592, loss 0.012460811994969845, R2 0.48205333948135376\n",
      "Eval loss 0.012467092834413052, R2 0.5197808742523193\n",
      "epoch 5593, loss 0.012460721656680107, R2 0.48205721378326416\n",
      "Eval loss 0.012466994114220142, R2 0.5197843909263611\n",
      "epoch 5594, loss 0.012460630387067795, R2 0.48206138610839844\n",
      "Eval loss 0.012466896325349808, R2 0.5197887420654297\n",
      "epoch 5595, loss 0.012460541911423206, R2 0.4820649027824402\n",
      "Eval loss 0.012466797605156898, R2 0.519792377948761\n",
      "epoch 5596, loss 0.012460451573133469, R2 0.48206818103790283\n",
      "Eval loss 0.01246669888496399, R2 0.5197961926460266\n",
      "epoch 5597, loss 0.012460361234843731, R2 0.4820718765258789\n",
      "Eval loss 0.012466601096093655, R2 0.5197996497154236\n",
      "epoch 5598, loss 0.012460271827876568, R2 0.48207545280456543\n",
      "Eval loss 0.01246650330722332, R2 0.5198034048080444\n",
      "epoch 5599, loss 0.01246018148958683, R2 0.4820798635482788\n",
      "Eval loss 0.01246640458703041, R2 0.5198071002960205\n",
      "epoch 5600, loss 0.012460091151297092, R2 0.48208343982696533\n",
      "Eval loss 0.01246630772948265, R2 0.5198107957839966\n",
      "epoch 5601, loss 0.012460000813007355, R2 0.48208749294281006\n",
      "Eval loss 0.012466209009289742, R2 0.5198144912719727\n",
      "epoch 5602, loss 0.012459912337362766, R2 0.48209089040756226\n",
      "Eval loss 0.012466112151741982, R2 0.5198186635971069\n",
      "epoch 5603, loss 0.012459821999073029, R2 0.482094943523407\n",
      "Eval loss 0.012466015294194221, R2 0.5198222398757935\n",
      "epoch 5604, loss 0.01245973166078329, R2 0.48209822177886963\n",
      "Eval loss 0.012465916574001312, R2 0.5198257565498352\n",
      "epoch 5605, loss 0.012459642253816128, R2 0.4821017384529114\n",
      "Eval loss 0.012465818785130978, R2 0.519829511642456\n",
      "epoch 5606, loss 0.012459552846848965, R2 0.48210567235946655\n",
      "Eval loss 0.012465721927583218, R2 0.5198332071304321\n",
      "epoch 5607, loss 0.01245946530252695, R2 0.48210930824279785\n",
      "Eval loss 0.012465625070035458, R2 0.5198370218276978\n",
      "epoch 5608, loss 0.012459374964237213, R2 0.4821128845214844\n",
      "Eval loss 0.012465527281165123, R2 0.5198407173156738\n",
      "epoch 5609, loss 0.012459284625947475, R2 0.4821164608001709\n",
      "Eval loss 0.012465429492294788, R2 0.519844651222229\n",
      "epoch 5610, loss 0.012459197081625462, R2 0.48212045431137085\n",
      "Eval loss 0.012465331703424454, R2 0.5198485851287842\n",
      "epoch 5611, loss 0.012459106743335724, R2 0.4821240305900574\n",
      "Eval loss 0.012465233914554119, R2 0.5198521614074707\n",
      "epoch 5612, loss 0.012459016405045986, R2 0.48212742805480957\n",
      "Eval loss 0.012465137057006359, R2 0.5198558568954468\n",
      "epoch 5613, loss 0.012458926998078823, R2 0.4821316599845886\n",
      "Eval loss 0.012465041130781174, R2 0.5198594331741333\n",
      "epoch 5614, loss 0.01245883945375681, R2 0.4821352958679199\n",
      "Eval loss 0.012464942410588264, R2 0.5198636054992676\n",
      "epoch 5615, loss 0.012458750046789646, R2 0.48213887214660645\n",
      "Eval loss 0.012464845553040504, R2 0.5198670029640198\n",
      "epoch 5616, loss 0.012458659708499908, R2 0.4821425676345825\n",
      "Eval loss 0.012464750558137894, R2 0.5198711156845093\n",
      "epoch 5617, loss 0.012458572164177895, R2 0.4821462631225586\n",
      "Eval loss 0.01246465090662241, R2 0.5198744535446167\n",
      "epoch 5618, loss 0.012458483688533306, R2 0.48214995861053467\n",
      "Eval loss 0.012464554980397224, R2 0.5198787450790405\n",
      "epoch 5619, loss 0.012458394281566143, R2 0.48215383291244507\n",
      "Eval loss 0.012464458122849464, R2 0.5198826193809509\n",
      "epoch 5620, loss 0.012458305805921555, R2 0.482157826423645\n",
      "Eval loss 0.012464361265301704, R2 0.5198856592178345\n",
      "epoch 5621, loss 0.012458216398954391, R2 0.4821610450744629\n",
      "Eval loss 0.012464266270399094, R2 0.5198898911476135\n",
      "epoch 5622, loss 0.012458127923309803, R2 0.48216474056243896\n",
      "Eval loss 0.012464167550206184, R2 0.5198938250541687\n",
      "epoch 5623, loss 0.012458039447665215, R2 0.4821682572364807\n",
      "Eval loss 0.012464071623980999, R2 0.5198969841003418\n",
      "epoch 5624, loss 0.0124579519033432, R2 0.482171893119812\n",
      "Eval loss 0.012463975697755814, R2 0.519900918006897\n",
      "epoch 5625, loss 0.012457862496376038, R2 0.48217540979385376\n",
      "Eval loss 0.012463877908885479, R2 0.5199049711227417\n",
      "epoch 5626, loss 0.012457773089408875, R2 0.4821794629096985\n",
      "Eval loss 0.012463781982660294, R2 0.5199082493782043\n",
      "epoch 5627, loss 0.01245768554508686, R2 0.4821830987930298\n",
      "Eval loss 0.012463686987757683, R2 0.5199118852615356\n",
      "epoch 5628, loss 0.012457597069442272, R2 0.48218727111816406\n",
      "Eval loss 0.012463588267564774, R2 0.5199155807495117\n",
      "epoch 5629, loss 0.012457508593797684, R2 0.48219043016433716\n",
      "Eval loss 0.012463492341339588, R2 0.5199191570281982\n",
      "epoch 5630, loss 0.012457420118153095, R2 0.48219478130340576\n",
      "Eval loss 0.012463398277759552, R2 0.5199229121208191\n",
      "epoch 5631, loss 0.012457332573831081, R2 0.48219776153564453\n",
      "Eval loss 0.012463302351534367, R2 0.5199265480041504\n",
      "epoch 5632, loss 0.012457244098186493, R2 0.48220163583755493\n",
      "Eval loss 0.012463205493986607, R2 0.519930362701416\n",
      "epoch 5633, loss 0.012457157485187054, R2 0.4822050929069519\n",
      "Eval loss 0.012463109567761421, R2 0.5199339985847473\n",
      "epoch 5634, loss 0.012457067146897316, R2 0.482208788394928\n",
      "Eval loss 0.012463012710213661, R2 0.5199378728866577\n",
      "epoch 5635, loss 0.012456980533897877, R2 0.4822124242782593\n",
      "Eval loss 0.012462918646633625, R2 0.519941508769989\n",
      "epoch 5636, loss 0.012456892058253288, R2 0.48221611976623535\n",
      "Eval loss 0.012462821789085865, R2 0.5199453830718994\n",
      "epoch 5637, loss 0.012456804513931274, R2 0.4822198748588562\n",
      "Eval loss 0.012462726794183254, R2 0.5199488997459412\n",
      "epoch 5638, loss 0.01245671696960926, R2 0.48222339153289795\n",
      "Eval loss 0.01246262900531292, R2 0.519952654838562\n",
      "epoch 5639, loss 0.012456628493964672, R2 0.48222702741622925\n",
      "Eval loss 0.012462534941732883, R2 0.519956111907959\n",
      "epoch 5640, loss 0.012456540949642658, R2 0.4822308421134949\n",
      "Eval loss 0.012462439946830273, R2 0.5199602842330933\n",
      "epoch 5641, loss 0.012456453405320644, R2 0.4822341799736023\n",
      "Eval loss 0.012462344020605087, R2 0.5199636220932007\n",
      "epoch 5642, loss 0.01245636586099863, R2 0.4822379946708679\n",
      "Eval loss 0.012462248094379902, R2 0.5199671983718872\n",
      "epoch 5643, loss 0.012456279247999191, R2 0.48224174976348877\n",
      "Eval loss 0.012462154030799866, R2 0.5199707746505737\n",
      "epoch 5644, loss 0.012456189841032028, R2 0.48224592208862305\n",
      "Eval loss 0.01246205810457468, R2 0.5199743509292603\n",
      "epoch 5645, loss 0.012456103228032589, R2 0.4822489023208618\n",
      "Eval loss 0.01246196310967207, R2 0.5199784636497498\n",
      "epoch 5646, loss 0.012456015683710575, R2 0.4822525382041931\n",
      "Eval loss 0.012461868114769459, R2 0.5199816823005676\n",
      "epoch 5647, loss 0.012455929070711136, R2 0.48225629329681396\n",
      "Eval loss 0.012461771257221699, R2 0.5199858546257019\n",
      "epoch 5648, loss 0.012455841526389122, R2 0.48225945234298706\n",
      "Eval loss 0.012461676262319088, R2 0.5199893712997437\n",
      "epoch 5649, loss 0.012455753050744534, R2 0.482263445854187\n",
      "Eval loss 0.012461580336093903, R2 0.5199933052062988\n",
      "epoch 5650, loss 0.012455666437745094, R2 0.48226702213287354\n",
      "Eval loss 0.012461488135159016, R2 0.5199965238571167\n",
      "epoch 5651, loss 0.012455579824745655, R2 0.4822707772254944\n",
      "Eval loss 0.01246139220893383, R2 0.5200005769729614\n",
      "epoch 5652, loss 0.01245549414306879, R2 0.48227405548095703\n",
      "Eval loss 0.012461298145353794, R2 0.5200037956237793\n",
      "epoch 5653, loss 0.012455406598746777, R2 0.48227834701538086\n",
      "Eval loss 0.012461201287806034, R2 0.5200077891349792\n",
      "epoch 5654, loss 0.012455319054424763, R2 0.4822814464569092\n",
      "Eval loss 0.012461106292903423, R2 0.5200116038322449\n",
      "epoch 5655, loss 0.012455231510102749, R2 0.4822852611541748\n",
      "Eval loss 0.012461013160645962, R2 0.5200146436691284\n",
      "epoch 5656, loss 0.012455145828425884, R2 0.48228931427001953\n",
      "Eval loss 0.012460918165743351, R2 0.520018458366394\n",
      "epoch 5657, loss 0.012455059215426445, R2 0.4822922945022583\n",
      "Eval loss 0.01246082317084074, R2 0.5200227499008179\n",
      "epoch 5658, loss 0.012454971671104431, R2 0.4822959303855896\n",
      "Eval loss 0.012460727244615555, R2 0.5200262069702148\n",
      "epoch 5659, loss 0.012454884126782417, R2 0.48230016231536865\n",
      "Eval loss 0.012460633181035519, R2 0.5200292468070984\n",
      "epoch 5660, loss 0.012454798445105553, R2 0.482303261756897\n",
      "Eval loss 0.012460540048778057, R2 0.5200331211090088\n",
      "epoch 5661, loss 0.012454711832106113, R2 0.48230671882629395\n",
      "Eval loss 0.012460445053875446, R2 0.5200366377830505\n",
      "epoch 5662, loss 0.012454625219106674, R2 0.4823099970817566\n",
      "Eval loss 0.01246035099029541, R2 0.5200406908988953\n",
      "epoch 5663, loss 0.012454538606107235, R2 0.48231393098831177\n",
      "Eval loss 0.012460255064070225, R2 0.5200442671775818\n",
      "epoch 5664, loss 0.01245445292443037, R2 0.4823179841041565\n",
      "Eval loss 0.012460162863135338, R2 0.5200477838516235\n",
      "epoch 5665, loss 0.012454365380108356, R2 0.48232126235961914\n",
      "Eval loss 0.012460068799555302, R2 0.5200514197349548\n",
      "epoch 5666, loss 0.012454280629754066, R2 0.48232513666152954\n",
      "Eval loss 0.012459974735975266, R2 0.520054817199707\n",
      "epoch 5667, loss 0.012454193085432053, R2 0.48232829570770264\n",
      "Eval loss 0.01245988067239523, R2 0.520058274269104\n",
      "epoch 5668, loss 0.012454107403755188, R2 0.4823318123817444\n",
      "Eval loss 0.012459786608815193, R2 0.5200618505477905\n",
      "epoch 5669, loss 0.012454020790755749, R2 0.48233526945114136\n",
      "Eval loss 0.012459691613912582, R2 0.5200655460357666\n",
      "epoch 5670, loss 0.012453935109078884, R2 0.4823390245437622\n",
      "Eval loss 0.012459597550332546, R2 0.5200693011283875\n",
      "epoch 5671, loss 0.012453848496079445, R2 0.48234260082244873\n",
      "Eval loss 0.012459504418075085, R2 0.5200730562210083\n",
      "epoch 5672, loss 0.012453761883080006, R2 0.48234665393829346\n",
      "Eval loss 0.012459410354495049, R2 0.5200766324996948\n",
      "epoch 5673, loss 0.012453676201403141, R2 0.4823499321937561\n",
      "Eval loss 0.012459317222237587, R2 0.5200799703598022\n",
      "epoch 5674, loss 0.012453590519726276, R2 0.4823535084724426\n",
      "Eval loss 0.01245922315865755, R2 0.5200839042663574\n",
      "epoch 5675, loss 0.012453505769371986, R2 0.48235684633255005\n",
      "Eval loss 0.01245913002640009, R2 0.5200870037078857\n",
      "epoch 5676, loss 0.012453420087695122, R2 0.4823604226112366\n",
      "Eval loss 0.012459035962820053, R2 0.52009117603302\n",
      "epoch 5677, loss 0.012453333474695683, R2 0.4823639988899231\n",
      "Eval loss 0.012458942830562592, R2 0.5200945138931274\n",
      "epoch 5678, loss 0.012453247793018818, R2 0.4823675751686096\n",
      "Eval loss 0.012458848766982555, R2 0.5200979709625244\n",
      "epoch 5679, loss 0.012453163042664528, R2 0.48237109184265137\n",
      "Eval loss 0.012458756566047668, R2 0.5201015472412109\n",
      "epoch 5680, loss 0.012453077360987663, R2 0.4823746681213379\n",
      "Eval loss 0.012458661571145058, R2 0.5201056003570557\n",
      "epoch 5681, loss 0.012452991679310799, R2 0.4823782444000244\n",
      "Eval loss 0.01245856937021017, R2 0.5201090574264526\n",
      "epoch 5682, loss 0.012452905997633934, R2 0.4823821187019348\n",
      "Eval loss 0.01245847623795271, R2 0.5201128721237183\n",
      "epoch 5683, loss 0.012452819384634495, R2 0.4823850393295288\n",
      "Eval loss 0.012458383105695248, R2 0.5201164484024048\n",
      "epoch 5684, loss 0.01245273556560278, R2 0.48238903284072876\n",
      "Eval loss 0.012458289042115211, R2 0.5201196670532227\n",
      "epoch 5685, loss 0.012452649883925915, R2 0.48239243030548096\n",
      "Eval loss 0.012458196841180325, R2 0.5201232433319092\n",
      "epoch 5686, loss 0.01245256420224905, R2 0.48239630460739136\n",
      "Eval loss 0.012458103708922863, R2 0.5201270580291748\n",
      "epoch 5687, loss 0.01245247945189476, R2 0.4823998212814331\n",
      "Eval loss 0.012458010576665401, R2 0.5201305150985718\n",
      "epoch 5688, loss 0.012452392838895321, R2 0.48240309953689575\n",
      "Eval loss 0.012457918375730515, R2 0.5201340913772583\n",
      "epoch 5689, loss 0.012452309019863605, R2 0.48240745067596436\n",
      "Eval loss 0.012457825243473053, R2 0.5201374292373657\n",
      "epoch 5690, loss 0.012452223338186741, R2 0.48241013288497925\n",
      "Eval loss 0.012457733042538166, R2 0.5201412439346313\n",
      "epoch 5691, loss 0.012452137656509876, R2 0.4824138879776001\n",
      "Eval loss 0.01245763897895813, R2 0.5201448798179626\n",
      "epoch 5692, loss 0.01245205383747816, R2 0.48241716623306274\n",
      "Eval loss 0.012457545846700668, R2 0.5201481580734253\n",
      "epoch 5693, loss 0.01245196908712387, R2 0.4824203848838806\n",
      "Eval loss 0.012457453645765781, R2 0.5201518535614014\n",
      "epoch 5694, loss 0.01245188433676958, R2 0.4824247360229492\n",
      "Eval loss 0.012457361444830894, R2 0.5201557278633118\n",
      "epoch 5695, loss 0.01245179958641529, R2 0.48242777585983276\n",
      "Eval loss 0.012457269243896008, R2 0.5201592445373535\n",
      "epoch 5696, loss 0.012451713904738426, R2 0.4824320077896118\n",
      "Eval loss 0.01245717704296112, R2 0.5201623439788818\n",
      "epoch 5697, loss 0.01245163008570671, R2 0.48243480920791626\n",
      "Eval loss 0.012457084842026234, R2 0.5201663970947266\n",
      "epoch 5698, loss 0.01245154533535242, R2 0.4824388027191162\n",
      "Eval loss 0.012456991709768772, R2 0.5201694965362549\n",
      "epoch 5699, loss 0.01245146058499813, R2 0.48244231939315796\n",
      "Eval loss 0.01245690044015646, R2 0.5201736092567444\n",
      "epoch 5700, loss 0.01245137583464384, R2 0.4824453592300415\n",
      "Eval loss 0.012456807307898998, R2 0.5201767683029175\n",
      "epoch 5701, loss 0.01245129108428955, R2 0.4824490547180176\n",
      "Eval loss 0.01245671696960926, R2 0.5201801061630249\n",
      "epoch 5702, loss 0.01245120633393526, R2 0.4824525713920593\n",
      "Eval loss 0.012456623837351799, R2 0.5201839804649353\n",
      "epoch 5703, loss 0.012451122514903545, R2 0.48245590925216675\n",
      "Eval loss 0.012456531636416912, R2 0.5201876759529114\n",
      "epoch 5704, loss 0.01245103869587183, R2 0.48245954513549805\n",
      "Eval loss 0.0124564403668046, R2 0.5201913118362427\n",
      "epoch 5705, loss 0.01245095394551754, R2 0.48246294260025024\n",
      "Eval loss 0.012456347234547138, R2 0.5201946496963501\n",
      "epoch 5706, loss 0.01245086919516325, R2 0.4824663996696472\n",
      "Eval loss 0.012456255033612251, R2 0.5201982259750366\n",
      "epoch 5707, loss 0.01245078444480896, R2 0.48247045278549194\n",
      "Eval loss 0.012456164695322514, R2 0.5202019214630127\n",
      "epoch 5708, loss 0.012450700625777245, R2 0.4824734330177307\n",
      "Eval loss 0.012456071563065052, R2 0.5202053785324097\n",
      "epoch 5709, loss 0.012450617738068104, R2 0.4824768900871277\n",
      "Eval loss 0.01245598029345274, R2 0.5202085971832275\n",
      "epoch 5710, loss 0.012450532987713814, R2 0.4824802279472351\n",
      "Eval loss 0.012455889023840427, R2 0.5202124118804932\n",
      "epoch 5711, loss 0.012450450100004673, R2 0.48248404264450073\n",
      "Eval loss 0.01245579682290554, R2 0.5202158093452454\n",
      "epoch 5712, loss 0.012450365349650383, R2 0.48248738050460815\n",
      "Eval loss 0.012455705553293228, R2 0.5202198028564453\n",
      "epoch 5713, loss 0.012450281530618668, R2 0.4824908971786499\n",
      "Eval loss 0.012455613352358341, R2 0.5202227830886841\n",
      "epoch 5714, loss 0.012450197711586952, R2 0.4824939966201782\n",
      "Eval loss 0.012455521151423454, R2 0.5202265977859497\n",
      "epoch 5715, loss 0.012450112961232662, R2 0.4824979901313782\n",
      "Eval loss 0.012455430813133717, R2 0.5202299356460571\n",
      "epoch 5716, loss 0.012450030073523521, R2 0.48250144720077515\n",
      "Eval loss 0.012455339543521404, R2 0.520233154296875\n",
      "epoch 5717, loss 0.012449946254491806, R2 0.48250478506088257\n",
      "Eval loss 0.012455248273909092, R2 0.5202374458312988\n",
      "epoch 5718, loss 0.01244986243546009, R2 0.4825083017349243\n",
      "Eval loss 0.01245515700429678, R2 0.520240306854248\n",
      "epoch 5719, loss 0.01244977954775095, R2 0.48251140117645264\n",
      "Eval loss 0.012455066666007042, R2 0.5202441215515137\n",
      "epoch 5720, loss 0.012449695728719234, R2 0.48251521587371826\n",
      "Eval loss 0.01245497539639473, R2 0.5202474594116211\n",
      "epoch 5721, loss 0.012449612841010094, R2 0.4825184941291809\n",
      "Eval loss 0.012454883195459843, R2 0.5202511548995972\n",
      "epoch 5722, loss 0.012449529021978378, R2 0.48252183198928833\n",
      "Eval loss 0.01245479192584753, R2 0.5202546715736389\n",
      "epoch 5723, loss 0.012449446134269238, R2 0.4825255870819092\n",
      "Eval loss 0.012454701587557793, R2 0.5202578902244568\n",
      "epoch 5724, loss 0.012449362315237522, R2 0.48252904415130615\n",
      "Eval loss 0.012454611249268055, R2 0.5202614068984985\n",
      "epoch 5725, loss 0.012449279427528381, R2 0.48253268003463745\n",
      "Eval loss 0.012454520910978317, R2 0.5202651023864746\n",
      "epoch 5726, loss 0.012449195608496666, R2 0.4825361371040344\n",
      "Eval loss 0.012454429641366005, R2 0.5202684998512268\n",
      "epoch 5727, loss 0.012449112720787525, R2 0.4825392961502075\n",
      "Eval loss 0.012454338371753693, R2 0.5202717781066895\n",
      "epoch 5728, loss 0.01244902890175581, R2 0.4825427532196045\n",
      "Eval loss 0.01245424710214138, R2 0.520275354385376\n",
      "epoch 5729, loss 0.012448946945369244, R2 0.4825468063354492\n",
      "Eval loss 0.012454155832529068, R2 0.5202791690826416\n",
      "epoch 5730, loss 0.012448862195014954, R2 0.48254984617233276\n",
      "Eval loss 0.012454066425561905, R2 0.520282506942749\n",
      "epoch 5731, loss 0.012448779307305813, R2 0.48255348205566406\n",
      "Eval loss 0.012453977018594742, R2 0.5202862024307251\n",
      "epoch 5732, loss 0.012448697350919247, R2 0.4825572371482849\n",
      "Eval loss 0.01245388574898243, R2 0.520289421081543\n",
      "epoch 5733, loss 0.012448614463210106, R2 0.4825601577758789\n",
      "Eval loss 0.012453794479370117, R2 0.5202933549880981\n",
      "epoch 5734, loss 0.01244853250682354, R2 0.4825635552406311\n",
      "Eval loss 0.012453706003725529, R2 0.5202965140342712\n",
      "epoch 5735, loss 0.012448448687791824, R2 0.4825671911239624\n",
      "Eval loss 0.012453616596758366, R2 0.5202999711036682\n",
      "epoch 5736, loss 0.012448365800082684, R2 0.48257046937942505\n",
      "Eval loss 0.012453524395823479, R2 0.5203033685684204\n",
      "epoch 5737, loss 0.012448281981050968, R2 0.48257410526275635\n",
      "Eval loss 0.012453434988856316, R2 0.5203068256378174\n",
      "epoch 5738, loss 0.012448200955986977, R2 0.48257750272750854\n",
      "Eval loss 0.012453344650566578, R2 0.5203100442886353\n",
      "epoch 5739, loss 0.012448118068277836, R2 0.4825812578201294\n",
      "Eval loss 0.01245325431227684, R2 0.5203136205673218\n",
      "epoch 5740, loss 0.01244803611189127, R2 0.4825841784477234\n",
      "Eval loss 0.012453164905309677, R2 0.5203174948692322\n",
      "epoch 5741, loss 0.012447954155504704, R2 0.4825875759124756\n",
      "Eval loss 0.012453073635697365, R2 0.5203205943107605\n",
      "epoch 5742, loss 0.012447871267795563, R2 0.48259103298187256\n",
      "Eval loss 0.012452985160052776, R2 0.5203241109848022\n",
      "epoch 5743, loss 0.012447788380086422, R2 0.48259466886520386\n",
      "Eval loss 0.012452893890440464, R2 0.5203275084495544\n",
      "epoch 5744, loss 0.012447706423699856, R2 0.4825977683067322\n",
      "Eval loss 0.012452803552150726, R2 0.5203311443328857\n",
      "epoch 5745, loss 0.01244762446731329, R2 0.4826018214225769\n",
      "Eval loss 0.012452716007828712, R2 0.5203346610069275\n",
      "epoch 5746, loss 0.012447542510926723, R2 0.4826047420501709\n",
      "Eval loss 0.0124526247382164, R2 0.5203377604484558\n",
      "epoch 5747, loss 0.012447460554540157, R2 0.4826081395149231\n",
      "Eval loss 0.012452534399926662, R2 0.5203418731689453\n",
      "epoch 5748, loss 0.012447377666831017, R2 0.4826122522354126\n",
      "Eval loss 0.012452445924282074, R2 0.5203450322151184\n",
      "epoch 5749, loss 0.012447294779121876, R2 0.4826151728630066\n",
      "Eval loss 0.012452356517314911, R2 0.5203484892845154\n",
      "epoch 5750, loss 0.012447213754057884, R2 0.48261839151382446\n",
      "Eval loss 0.012452268041670322, R2 0.520351767539978\n",
      "epoch 5751, loss 0.012447131797671318, R2 0.48262178897857666\n",
      "Eval loss 0.012452177703380585, R2 0.520355224609375\n",
      "epoch 5752, loss 0.012447050772607327, R2 0.4826256036758423\n",
      "Eval loss 0.012452087365090847, R2 0.5203591585159302\n",
      "epoch 5753, loss 0.012446967884898186, R2 0.4826287627220154\n",
      "Eval loss 0.012451997958123684, R2 0.5203623175621033\n",
      "epoch 5754, loss 0.01244688592851162, R2 0.48263198137283325\n",
      "Eval loss 0.012451909482479095, R2 0.5203657150268555\n",
      "epoch 5755, loss 0.012446803972125053, R2 0.48263537883758545\n",
      "Eval loss 0.012451820075511932, R2 0.5203689932823181\n",
      "epoch 5756, loss 0.012446722015738487, R2 0.482638955116272\n",
      "Eval loss 0.012451731599867344, R2 0.5203723907470703\n",
      "epoch 5757, loss 0.012446640990674496, R2 0.48264235258102417\n",
      "Eval loss 0.012451641261577606, R2 0.5203760862350464\n",
      "epoch 5758, loss 0.012446559965610504, R2 0.48264604806900024\n",
      "Eval loss 0.012451553717255592, R2 0.5203793048858643\n",
      "epoch 5759, loss 0.012446478009223938, R2 0.4826487898826599\n",
      "Eval loss 0.012451463378965855, R2 0.5203828811645508\n",
      "epoch 5760, loss 0.012446396984159946, R2 0.4826529622077942\n",
      "Eval loss 0.01245137583464384, R2 0.5203860998153687\n",
      "epoch 5761, loss 0.012446315959095955, R2 0.48265570402145386\n",
      "Eval loss 0.012451286427676678, R2 0.5203893184661865\n",
      "epoch 5762, loss 0.012446234934031963, R2 0.4826592206954956\n",
      "Eval loss 0.012451197020709515, R2 0.5203931331634521\n",
      "epoch 5763, loss 0.012446152977645397, R2 0.48266249895095825\n",
      "Eval loss 0.012451107613742352, R2 0.5203961730003357\n",
      "epoch 5764, loss 0.012446071021258831, R2 0.482666552066803\n",
      "Eval loss 0.012451020069420338, R2 0.5203999280929565\n",
      "epoch 5765, loss 0.01244598999619484, R2 0.48266923427581787\n",
      "Eval loss 0.01245093159377575, R2 0.5204033851623535\n",
      "epoch 5766, loss 0.012445908971130848, R2 0.4826732873916626\n",
      "Eval loss 0.012450842186808586, R2 0.5204064249992371\n",
      "epoch 5767, loss 0.012445827946066856, R2 0.4826759696006775\n",
      "Eval loss 0.012450753711163998, R2 0.5204101800918579\n",
      "epoch 5768, loss 0.01244574598968029, R2 0.4826793670654297\n",
      "Eval loss 0.01245066523551941, R2 0.5204132199287415\n",
      "epoch 5769, loss 0.012445665895938873, R2 0.48268288373947144\n",
      "Eval loss 0.012450575828552246, R2 0.5204166173934937\n",
      "epoch 5770, loss 0.012445583008229733, R2 0.48268628120422363\n",
      "Eval loss 0.012450489215552807, R2 0.5204200148582458\n",
      "epoch 5771, loss 0.012445502914488316, R2 0.48268961906433105\n",
      "Eval loss 0.012450399808585644, R2 0.5204237699508667\n",
      "epoch 5772, loss 0.012445421889424324, R2 0.4826928377151489\n",
      "Eval loss 0.01245031040161848, R2 0.5204271674156189\n",
      "epoch 5773, loss 0.012445341795682907, R2 0.48269617557525635\n",
      "Eval loss 0.012450223788619041, R2 0.5204305052757263\n",
      "epoch 5774, loss 0.01244526170194149, R2 0.4827001690864563\n",
      "Eval loss 0.012450136244297028, R2 0.5204336047172546\n",
      "epoch 5775, loss 0.012445180676877499, R2 0.4827035069465637\n",
      "Eval loss 0.012450047768652439, R2 0.5204374194145203\n",
      "epoch 5776, loss 0.012445098720490932, R2 0.4827069640159607\n",
      "Eval loss 0.012449958361685276, R2 0.5204408168792725\n",
      "epoch 5777, loss 0.012445017695426941, R2 0.4827096462249756\n",
      "Eval loss 0.012449869886040688, R2 0.5204441547393799\n",
      "epoch 5778, loss 0.012444937601685524, R2 0.482712984085083\n",
      "Eval loss 0.012449783273041248, R2 0.5204474925994873\n",
      "epoch 5779, loss 0.012444856576621532, R2 0.48271650075912476\n",
      "Eval loss 0.012449695728719234, R2 0.5204505920410156\n",
      "epoch 5780, loss 0.01244477741420269, R2 0.48271965980529785\n",
      "Eval loss 0.012449607253074646, R2 0.5204539895057678\n",
      "epoch 5781, loss 0.012444696389138699, R2 0.4827231764793396\n",
      "Eval loss 0.012449518777430058, R2 0.5204575061798096\n",
      "epoch 5782, loss 0.012444616295397282, R2 0.4827263355255127\n",
      "Eval loss 0.012449432164430618, R2 0.5204610824584961\n",
      "epoch 5783, loss 0.01244453527033329, R2 0.4827297329902649\n",
      "Eval loss 0.01244934368878603, R2 0.5204644203186035\n",
      "epoch 5784, loss 0.012444455176591873, R2 0.48273319005966187\n",
      "Eval loss 0.01244925707578659, R2 0.5204677581787109\n",
      "epoch 5785, loss 0.01244437601417303, R2 0.48273634910583496\n",
      "Eval loss 0.012449168600142002, R2 0.5204711556434631\n",
      "epoch 5786, loss 0.01244429498910904, R2 0.48273950815200806\n",
      "Eval loss 0.012449081055819988, R2 0.520474374294281\n",
      "epoch 5787, loss 0.012444214895367622, R2 0.482743501663208\n",
      "Eval loss 0.0124489925801754, R2 0.5204780697822571\n",
      "epoch 5788, loss 0.01244413573294878, R2 0.4827464818954468\n",
      "Eval loss 0.01244890596717596, R2 0.5204809904098511\n",
      "epoch 5789, loss 0.012444054707884789, R2 0.48274970054626465\n",
      "Eval loss 0.012448818422853947, R2 0.5204849243164062\n",
      "epoch 5790, loss 0.012443973682820797, R2 0.48275357484817505\n",
      "Eval loss 0.012448729947209358, R2 0.5204877853393555\n",
      "epoch 5791, loss 0.012443894520401955, R2 0.48275649547576904\n",
      "Eval loss 0.012448642402887344, R2 0.520491361618042\n",
      "epoch 5792, loss 0.012443815357983112, R2 0.48276013135910034\n",
      "Eval loss 0.01244855672121048, R2 0.5204945802688599\n",
      "epoch 5793, loss 0.012443735264241695, R2 0.48276299238204956\n",
      "Eval loss 0.012448469176888466, R2 0.5204982161521912\n",
      "epoch 5794, loss 0.012443654239177704, R2 0.482766330242157\n",
      "Eval loss 0.012448381632566452, R2 0.5205017328262329\n",
      "epoch 5795, loss 0.012443575076758862, R2 0.48276960849761963\n",
      "Eval loss 0.012448296882212162, R2 0.520504891872406\n",
      "epoch 5796, loss 0.012443494983017445, R2 0.48277294635772705\n",
      "Eval loss 0.012448207475244999, R2 0.5205082893371582\n",
      "epoch 5797, loss 0.012443416751921177, R2 0.48277604579925537\n",
      "Eval loss 0.012448119930922985, R2 0.5205115079879761\n",
      "epoch 5798, loss 0.01244333665817976, R2 0.4827795624732971\n",
      "Eval loss 0.01244803424924612, R2 0.5205150842666626\n",
      "epoch 5799, loss 0.012443258427083492, R2 0.4827834367752075\n",
      "Eval loss 0.012447946704924107, R2 0.5205185413360596\n",
      "epoch 5800, loss 0.0124431774020195, R2 0.48278629779815674\n",
      "Eval loss 0.012447861023247242, R2 0.5205215215682983\n",
      "epoch 5801, loss 0.012443097308278084, R2 0.48278963565826416\n",
      "Eval loss 0.012447772547602654, R2 0.5205248594284058\n",
      "epoch 5802, loss 0.012443019077181816, R2 0.4827929139137268\n",
      "Eval loss 0.012447686865925789, R2 0.5205281972885132\n",
      "epoch 5803, loss 0.012442939914762974, R2 0.48279666900634766\n",
      "Eval loss 0.012447601184248924, R2 0.5205316543579102\n",
      "epoch 5804, loss 0.012442859821021557, R2 0.48279935121536255\n",
      "Eval loss 0.012447512708604336, R2 0.5205349326133728\n",
      "epoch 5805, loss 0.01244277972728014, R2 0.4828031659126282\n",
      "Eval loss 0.012447426095604897, R2 0.5205379724502563\n",
      "epoch 5806, loss 0.012442699633538723, R2 0.4828062057495117\n",
      "Eval loss 0.012447340413928032, R2 0.5205414295196533\n",
      "epoch 5807, loss 0.01244262233376503, R2 0.4828094244003296\n",
      "Eval loss 0.012447252869606018, R2 0.5205450654029846\n",
      "epoch 5808, loss 0.012442543171346188, R2 0.4828125238418579\n",
      "Eval loss 0.012447167187929153, R2 0.5205482244491577\n",
      "epoch 5809, loss 0.01244246307760477, R2 0.48281586170196533\n",
      "Eval loss 0.012447080574929714, R2 0.5205515623092651\n",
      "epoch 5810, loss 0.012442384846508503, R2 0.4828190803527832\n",
      "Eval loss 0.01244699489325285, R2 0.5205546021461487\n",
      "epoch 5811, loss 0.01244230568408966, R2 0.4828225374221802\n",
      "Eval loss 0.01244690828025341, R2 0.5205586552619934\n",
      "epoch 5812, loss 0.012442226521670818, R2 0.48282569646835327\n",
      "Eval loss 0.012446822598576546, R2 0.5205615162849426\n",
      "epoch 5813, loss 0.012442147359251976, R2 0.48282915353775024\n",
      "Eval loss 0.012446735985577106, R2 0.5205649733543396\n",
      "epoch 5814, loss 0.012442068196833134, R2 0.48283225297927856\n",
      "Eval loss 0.012446650303900242, R2 0.5205681324005127\n",
      "epoch 5815, loss 0.012441989034414291, R2 0.48283618688583374\n",
      "Eval loss 0.012446563690900803, R2 0.5205714702606201\n",
      "epoch 5816, loss 0.012441911734640598, R2 0.4828387498855591\n",
      "Eval loss 0.012446478009223938, R2 0.5205748081207275\n",
      "epoch 5817, loss 0.012441831640899181, R2 0.48284274339675903\n",
      "Eval loss 0.012446392327547073, R2 0.5205780863761902\n",
      "epoch 5818, loss 0.012441753409802914, R2 0.4828454852104187\n",
      "Eval loss 0.012446306645870209, R2 0.5205812454223633\n",
      "epoch 5819, loss 0.012441675178706646, R2 0.482848584651947\n",
      "Eval loss 0.01244622003287077, R2 0.5205845832824707\n",
      "epoch 5820, loss 0.012441596016287804, R2 0.482852041721344\n",
      "Eval loss 0.012446134351193905, R2 0.5205880403518677\n",
      "epoch 5821, loss 0.012441517785191536, R2 0.48285579681396484\n",
      "Eval loss 0.01244604866951704, R2 0.5205916166305542\n",
      "epoch 5822, loss 0.012441439554095268, R2 0.4828585386276245\n",
      "Eval loss 0.012445962987840176, R2 0.520594596862793\n",
      "epoch 5823, loss 0.012441362254321575, R2 0.48286157846450806\n",
      "Eval loss 0.012445877306163311, R2 0.5205976366996765\n",
      "epoch 5824, loss 0.012441282160580158, R2 0.48286473751068115\n",
      "Eval loss 0.012445791624486446, R2 0.5206009149551392\n",
      "epoch 5825, loss 0.012441204860806465, R2 0.48286813497543335\n",
      "Eval loss 0.012445705011487007, R2 0.5206042528152466\n",
      "epoch 5826, loss 0.012441125698387623, R2 0.482871413230896\n",
      "Eval loss 0.012445619329810143, R2 0.5206078290939331\n",
      "epoch 5827, loss 0.01244104839861393, R2 0.4828748106956482\n",
      "Eval loss 0.012445535510778427, R2 0.520611047744751\n",
      "epoch 5828, loss 0.012440969236195087, R2 0.4828784465789795\n",
      "Eval loss 0.012445449829101562, R2 0.520614504814148\n",
      "epoch 5829, loss 0.012440891936421394, R2 0.4828813076019287\n",
      "Eval loss 0.012445363216102123, R2 0.5206177234649658\n",
      "epoch 5830, loss 0.012440813705325127, R2 0.48288506269454956\n",
      "Eval loss 0.012445278465747833, R2 0.5206212997436523\n",
      "epoch 5831, loss 0.012440734542906284, R2 0.482887864112854\n",
      "Eval loss 0.012445193715393543, R2 0.5206242799758911\n",
      "epoch 5832, loss 0.012440657243132591, R2 0.48289138078689575\n",
      "Eval loss 0.012445107102394104, R2 0.5206276178359985\n",
      "epoch 5833, loss 0.012440579943358898, R2 0.4828939437866211\n",
      "Eval loss 0.012445022352039814, R2 0.5206305980682373\n",
      "epoch 5834, loss 0.01244050171226263, R2 0.4828973412513733\n",
      "Eval loss 0.012444938533008099, R2 0.5206338167190552\n",
      "epoch 5835, loss 0.012440424412488937, R2 0.48290061950683594\n",
      "Eval loss 0.01244485192000866, R2 0.520637571811676\n",
      "epoch 5836, loss 0.01244034618139267, R2 0.4829038381576538\n",
      "Eval loss 0.012444768100976944, R2 0.5206406116485596\n",
      "epoch 5837, loss 0.012440267950296402, R2 0.48290759325027466\n",
      "Eval loss 0.012444684281945229, R2 0.5206438899040222\n",
      "epoch 5838, loss 0.012440191581845284, R2 0.4829109311103821\n",
      "Eval loss 0.012444598600268364, R2 0.5206469297409058\n",
      "epoch 5839, loss 0.012440113350749016, R2 0.4829135537147522\n",
      "Eval loss 0.0124445129185915, R2 0.520650327205658\n",
      "epoch 5840, loss 0.012440035119652748, R2 0.48291677236557007\n",
      "Eval loss 0.01244442816823721, R2 0.5206538438796997\n",
      "epoch 5841, loss 0.012439957819879055, R2 0.48291999101638794\n",
      "Eval loss 0.012444342486560345, R2 0.5206567645072937\n",
      "epoch 5842, loss 0.012439880520105362, R2 0.4829230308532715\n",
      "Eval loss 0.01244425866752863, R2 0.5206602811813354\n",
      "epoch 5843, loss 0.012439803220331669, R2 0.4829269051551819\n",
      "Eval loss 0.012444175779819489, R2 0.5206636190414429\n",
      "epoch 5844, loss 0.012439725920557976, R2 0.4829297661781311\n",
      "Eval loss 0.012444090098142624, R2 0.520666778087616\n",
      "epoch 5845, loss 0.012439647689461708, R2 0.4829333424568176\n",
      "Eval loss 0.012444005347788334, R2 0.5206700563430786\n",
      "epoch 5846, loss 0.01243957132101059, R2 0.4829365611076355\n",
      "Eval loss 0.012443921528756618, R2 0.5206732749938965\n",
      "epoch 5847, loss 0.012439493089914322, R2 0.48293960094451904\n",
      "Eval loss 0.012443836778402328, R2 0.5206763744354248\n",
      "epoch 5848, loss 0.012439416721463203, R2 0.48294246196746826\n",
      "Eval loss 0.012443752028048038, R2 0.5206794738769531\n",
      "epoch 5849, loss 0.012439338490366936, R2 0.4829457402229309\n",
      "Eval loss 0.012443666346371174, R2 0.5206828117370605\n",
      "epoch 5850, loss 0.012439262121915817, R2 0.482948899269104\n",
      "Eval loss 0.012443583458662033, R2 0.5206862688064575\n",
      "epoch 5851, loss 0.01243918389081955, R2 0.4829521179199219\n",
      "Eval loss 0.012443498708307743, R2 0.5206892490386963\n",
      "epoch 5852, loss 0.012439107522368431, R2 0.4829554557800293\n",
      "Eval loss 0.012443415820598602, R2 0.5206924676895142\n",
      "epoch 5853, loss 0.012439031153917313, R2 0.48295849561691284\n",
      "Eval loss 0.012443331070244312, R2 0.5206957459449768\n",
      "epoch 5854, loss 0.012438954785466194, R2 0.48296183347702026\n",
      "Eval loss 0.012443247251212597, R2 0.5206989645957947\n",
      "epoch 5855, loss 0.012438877485692501, R2 0.4829648733139038\n",
      "Eval loss 0.012443161569535732, R2 0.5207022428512573\n",
      "epoch 5856, loss 0.012438800185918808, R2 0.4829680919647217\n",
      "Eval loss 0.012443078681826591, R2 0.5207056999206543\n",
      "epoch 5857, loss 0.012438722886145115, R2 0.4829714894294739\n",
      "Eval loss 0.012442994862794876, R2 0.5207086801528931\n",
      "epoch 5858, loss 0.012438647449016571, R2 0.48297494649887085\n",
      "Eval loss 0.01244291104376316, R2 0.52071213722229\n",
      "epoch 5859, loss 0.012438571080565453, R2 0.4829779267311096\n",
      "Eval loss 0.012442827224731445, R2 0.5207154154777527\n",
      "epoch 5860, loss 0.012438492849469185, R2 0.4829808473587036\n",
      "Eval loss 0.01244274340569973, R2 0.5207183361053467\n",
      "epoch 5861, loss 0.012438416481018066, R2 0.48298418521881104\n",
      "Eval loss 0.012442659586668015, R2 0.5207217335700989\n",
      "epoch 5862, loss 0.012438339181244373, R2 0.4829874038696289\n",
      "Eval loss 0.0124425757676363, R2 0.5207252502441406\n",
      "epoch 5863, loss 0.01243826374411583, R2 0.482990562915802\n",
      "Eval loss 0.012442493811249733, R2 0.5207282304763794\n",
      "epoch 5864, loss 0.012438186444342136, R2 0.48299360275268555\n",
      "Eval loss 0.012442408129572868, R2 0.5207313895225525\n",
      "epoch 5865, loss 0.012438111007213593, R2 0.4829968810081482\n",
      "Eval loss 0.012442323379218578, R2 0.5207345485687256\n",
      "epoch 5866, loss 0.0124380337074399, R2 0.48299992084503174\n",
      "Eval loss 0.012442242354154587, R2 0.520737886428833\n",
      "epoch 5867, loss 0.012437957338988781, R2 0.4830031394958496\n",
      "Eval loss 0.012442157603800297, R2 0.52074134349823\n",
      "epoch 5868, loss 0.012437881901860237, R2 0.48300641775131226\n",
      "Eval loss 0.012442074716091156, R2 0.5207443833351135\n",
      "epoch 5869, loss 0.012437806464731693, R2 0.483009397983551\n",
      "Eval loss 0.012441991828382015, R2 0.5207476019859314\n",
      "epoch 5870, loss 0.012437729164958, R2 0.4830126166343689\n",
      "Eval loss 0.012441907078027725, R2 0.520750880241394\n",
      "epoch 5871, loss 0.012437652796506882, R2 0.4830164313316345\n",
      "Eval loss 0.012441825121641159, R2 0.5207537412643433\n",
      "epoch 5872, loss 0.012437577359378338, R2 0.4830195903778076\n",
      "Eval loss 0.012441742233932018, R2 0.5207570791244507\n",
      "epoch 5873, loss 0.01243750099092722, R2 0.4830220937728882\n",
      "Eval loss 0.012441658414900303, R2 0.5207604169845581\n",
      "epoch 5874, loss 0.012437423691153526, R2 0.48302561044692993\n",
      "Eval loss 0.012441576458513737, R2 0.520763635635376\n",
      "epoch 5875, loss 0.012437348254024982, R2 0.483028769493103\n",
      "Eval loss 0.012441491708159447, R2 0.52076655626297\n",
      "epoch 5876, loss 0.012437272816896439, R2 0.4830324053764343\n",
      "Eval loss 0.01244140975177288, R2 0.5207701325416565\n",
      "epoch 5877, loss 0.012437195517122746, R2 0.4830350875854492\n",
      "Eval loss 0.012441327795386314, R2 0.5207732915878296\n",
      "epoch 5878, loss 0.012437121011316776, R2 0.48303836584091187\n",
      "Eval loss 0.012441243045032024, R2 0.5207761526107788\n",
      "epoch 5879, loss 0.012437044642865658, R2 0.4830412268638611\n",
      "Eval loss 0.012441160157322884, R2 0.520780086517334\n",
      "epoch 5880, loss 0.012436969205737114, R2 0.4830448627471924\n",
      "Eval loss 0.012441078200936317, R2 0.5207831859588623\n",
      "epoch 5881, loss 0.01243689563125372, R2 0.4830472469329834\n",
      "Eval loss 0.012440996244549751, R2 0.5207858085632324\n",
      "epoch 5882, loss 0.012436818331480026, R2 0.48305046558380127\n",
      "Eval loss 0.01244091335684061, R2 0.5207890272140503\n",
      "epoch 5883, loss 0.012436741963028908, R2 0.4830538034439087\n",
      "Eval loss 0.01244083046913147, R2 0.5207920074462891\n",
      "epoch 5884, loss 0.012436666525900364, R2 0.4830571413040161\n",
      "Eval loss 0.012440747581422329, R2 0.5207958221435547\n",
      "epoch 5885, loss 0.012436592020094395, R2 0.4830598831176758\n",
      "Eval loss 0.012440665625035763, R2 0.5207988023757935\n",
      "epoch 5886, loss 0.012436515651643276, R2 0.48306286334991455\n",
      "Eval loss 0.012440582737326622, R2 0.5208019018173218\n",
      "epoch 5887, loss 0.012436440214514732, R2 0.4830666780471802\n",
      "Eval loss 0.012440499849617481, R2 0.5208051800727844\n",
      "epoch 5888, loss 0.012436365708708763, R2 0.4830693006515503\n",
      "Eval loss 0.01244041696190834, R2 0.5208079814910889\n",
      "epoch 5889, loss 0.012436289340257645, R2 0.48307257890701294\n",
      "Eval loss 0.012440335005521774, R2 0.5208115577697754\n",
      "epoch 5890, loss 0.012436214834451675, R2 0.48307543992996216\n",
      "Eval loss 0.012440252117812634, R2 0.5208145976066589\n",
      "epoch 5891, loss 0.012436138466000557, R2 0.48307859897613525\n",
      "Eval loss 0.012440171092748642, R2 0.5208175778388977\n",
      "epoch 5892, loss 0.012436064891517162, R2 0.4830818176269531\n",
      "Eval loss 0.012440088205039501, R2 0.5208207368850708\n",
      "epoch 5893, loss 0.012435988523066044, R2 0.48308515548706055\n",
      "Eval loss 0.012440006248652935, R2 0.5208237767219543\n",
      "epoch 5894, loss 0.012435914017260075, R2 0.48308807611465454\n",
      "Eval loss 0.012439924292266369, R2 0.5208269357681274\n",
      "epoch 5895, loss 0.012435839511454105, R2 0.4830912947654724\n",
      "Eval loss 0.012439842335879803, R2 0.520830512046814\n",
      "epoch 5896, loss 0.012435764074325562, R2 0.4830947518348694\n",
      "Eval loss 0.012439762242138386, R2 0.5208337306976318\n",
      "epoch 5897, loss 0.012435688637197018, R2 0.48309755325317383\n",
      "Eval loss 0.01243967842310667, R2 0.5208369493484497\n",
      "epoch 5898, loss 0.012435614131391048, R2 0.48310017585754395\n",
      "Eval loss 0.012439596466720104, R2 0.5208395719528198\n",
      "epoch 5899, loss 0.01243553962558508, R2 0.48310428857803345\n",
      "Eval loss 0.012439515441656113, R2 0.5208430290222168\n",
      "epoch 5900, loss 0.012435464188456535, R2 0.4831072688102722\n",
      "Eval loss 0.012439432553946972, R2 0.5208463668823242\n",
      "epoch 5901, loss 0.012435389682650566, R2 0.48310989141464233\n",
      "Eval loss 0.012439352460205555, R2 0.5208491086959839\n",
      "epoch 5902, loss 0.012435316108167171, R2 0.4831131100654602\n",
      "Eval loss 0.01243926864117384, R2 0.5208525657653809\n",
      "epoch 5903, loss 0.012435240671038628, R2 0.4831165671348572\n",
      "Eval loss 0.012439187616109848, R2 0.520855724811554\n",
      "epoch 5904, loss 0.012435166165232658, R2 0.4831196069717407\n",
      "Eval loss 0.012439105659723282, R2 0.5208590030670166\n",
      "epoch 5905, loss 0.01243509165942669, R2 0.48312222957611084\n",
      "Eval loss 0.012439023703336716, R2 0.5208618640899658\n",
      "epoch 5906, loss 0.012435016222298145, R2 0.48312538862228394\n",
      "Eval loss 0.012438942678272724, R2 0.5208650231361389\n",
      "epoch 5907, loss 0.012434943579137325, R2 0.48312872648239136\n",
      "Eval loss 0.012438861653208733, R2 0.5208685398101807\n",
      "epoch 5908, loss 0.012434868142008781, R2 0.4831315279006958\n",
      "Eval loss 0.012438779696822166, R2 0.5208717584609985\n",
      "epoch 5909, loss 0.012434793636202812, R2 0.4831351041793823\n",
      "Eval loss 0.01243869960308075, R2 0.5208743810653687\n",
      "epoch 5910, loss 0.012434719130396843, R2 0.48313772678375244\n",
      "Eval loss 0.012438617646694183, R2 0.5208775997161865\n",
      "epoch 5911, loss 0.012434644624590874, R2 0.4831410050392151\n",
      "Eval loss 0.012438534758985043, R2 0.5208808779716492\n",
      "epoch 5912, loss 0.012434571050107479, R2 0.4831443428993225\n",
      "Eval loss 0.012438453733921051, R2 0.5208835601806641\n",
      "epoch 5913, loss 0.01243449468165636, R2 0.48314720392227173\n",
      "Eval loss 0.01243837270885706, R2 0.5208868980407715\n",
      "epoch 5914, loss 0.01243442203849554, R2 0.48315006494522095\n",
      "Eval loss 0.012438291683793068, R2 0.5208901166915894\n",
      "epoch 5915, loss 0.012434348464012146, R2 0.48315316438674927\n",
      "Eval loss 0.012438211590051651, R2 0.5208929181098938\n",
      "epoch 5916, loss 0.012434273958206177, R2 0.4831562638282776\n",
      "Eval loss 0.012438131496310234, R2 0.520896315574646\n",
      "epoch 5917, loss 0.012434200383722782, R2 0.48315930366516113\n",
      "Eval loss 0.012438049539923668, R2 0.5208995938301086\n",
      "epoch 5918, loss 0.012434124946594238, R2 0.48316240310668945\n",
      "Eval loss 0.012437969446182251, R2 0.5209023952484131\n",
      "epoch 5919, loss 0.012434052303433418, R2 0.4831659197807312\n",
      "Eval loss 0.012437887489795685, R2 0.5209053754806519\n",
      "epoch 5920, loss 0.012433978728950024, R2 0.48316866159439087\n",
      "Eval loss 0.012437807396054268, R2 0.5209084749221802\n",
      "epoch 5921, loss 0.012433904223144054, R2 0.4831717610359192\n",
      "Eval loss 0.012437726370990276, R2 0.5209120512008667\n",
      "epoch 5922, loss 0.01243383064866066, R2 0.4831753373146057\n",
      "Eval loss 0.01243764441460371, R2 0.5209147930145264\n",
      "epoch 5923, loss 0.01243375614285469, R2 0.4831782579421997\n",
      "Eval loss 0.012437564320862293, R2 0.5209181308746338\n",
      "epoch 5924, loss 0.012433682568371296, R2 0.4831809997558594\n",
      "Eval loss 0.012437483295798302, R2 0.520920991897583\n",
      "epoch 5925, loss 0.012433608993887901, R2 0.4831838607788086\n",
      "Eval loss 0.012437405064702034, R2 0.5209241509437561\n",
      "epoch 5926, loss 0.012433535419404507, R2 0.48318690061569214\n",
      "Eval loss 0.012437323108315468, R2 0.5209274291992188\n",
      "epoch 5927, loss 0.012433461844921112, R2 0.48319000005722046\n",
      "Eval loss 0.012437241151928902, R2 0.5209305286407471\n",
      "epoch 5928, loss 0.012433389201760292, R2 0.48319268226623535\n",
      "Eval loss 0.01243716012686491, R2 0.5209336876869202\n",
      "epoch 5929, loss 0.012433315627276897, R2 0.48319607973098755\n",
      "Eval loss 0.012437081895768642, R2 0.5209369659423828\n",
      "epoch 5930, loss 0.012433242052793503, R2 0.4831997752189636\n",
      "Eval loss 0.012437000870704651, R2 0.5209397077560425\n",
      "epoch 5931, loss 0.012433166615664959, R2 0.4832022786140442\n",
      "Eval loss 0.012436921708285809, R2 0.5209428668022156\n",
      "epoch 5932, loss 0.012433094903826714, R2 0.4832049012184143\n",
      "Eval loss 0.012436839751899242, R2 0.5209456086158752\n",
      "epoch 5933, loss 0.012433021329343319, R2 0.4832087755203247\n",
      "Eval loss 0.012436759658157825, R2 0.520949125289917\n",
      "epoch 5934, loss 0.012432948686182499, R2 0.4832112789154053\n",
      "Eval loss 0.012436679564416409, R2 0.5209522247314453\n",
      "epoch 5935, loss 0.012432875111699104, R2 0.48321425914764404\n",
      "Eval loss 0.012436599470674992, R2 0.5209554433822632\n",
      "epoch 5936, loss 0.012432802468538284, R2 0.4832172393798828\n",
      "Eval loss 0.012436519376933575, R2 0.520958662033081\n",
      "epoch 5937, loss 0.01243272889405489, R2 0.4832209348678589\n",
      "Eval loss 0.012436440214514732, R2 0.5209614038467407\n",
      "epoch 5938, loss 0.012432655319571495, R2 0.48322349786758423\n",
      "Eval loss 0.012436360120773315, R2 0.5209646224975586\n",
      "epoch 5939, loss 0.0124325817450881, R2 0.4832265377044678\n",
      "Eval loss 0.012436280027031898, R2 0.5209676027297974\n",
      "epoch 5940, loss 0.012432510033249855, R2 0.48322951793670654\n",
      "Eval loss 0.012436200864613056, R2 0.5209706425666809\n",
      "epoch 5941, loss 0.012432437390089035, R2 0.48323261737823486\n",
      "Eval loss 0.01243612077087164, R2 0.5209733247756958\n",
      "epoch 5942, loss 0.01243236381560564, R2 0.4832356572151184\n",
      "Eval loss 0.012436040677130222, R2 0.5209769606590271\n",
      "epoch 5943, loss 0.01243229117244482, R2 0.4832388162612915\n",
      "Eval loss 0.012435960583388805, R2 0.5209798812866211\n",
      "epoch 5944, loss 0.012432217597961426, R2 0.4832417368888855\n",
      "Eval loss 0.012435880489647388, R2 0.5209832787513733\n",
      "epoch 5945, loss 0.012432144954800606, R2 0.4832448959350586\n",
      "Eval loss 0.012435801327228546, R2 0.5209857225418091\n",
      "epoch 5946, loss 0.012432071380317211, R2 0.48324793577194214\n",
      "Eval loss 0.012435722164809704, R2 0.5209890604019165\n",
      "epoch 5947, loss 0.012431999668478966, R2 0.48325061798095703\n",
      "Eval loss 0.012435642071068287, R2 0.5209924578666687\n",
      "epoch 5948, loss 0.012431927025318146, R2 0.4832537770271301\n",
      "Eval loss 0.012435562908649445, R2 0.5209952592849731\n",
      "epoch 5949, loss 0.012431854382157326, R2 0.48325663805007935\n",
      "Eval loss 0.012435483746230602, R2 0.5209984183311462\n",
      "epoch 5950, loss 0.012431781738996506, R2 0.48325979709625244\n",
      "Eval loss 0.01243540458381176, R2 0.5210010409355164\n",
      "epoch 5951, loss 0.012431709095835686, R2 0.483262836933136\n",
      "Eval loss 0.012435324490070343, R2 0.5210042595863342\n",
      "epoch 5952, loss 0.012431636452674866, R2 0.48326635360717773\n",
      "Eval loss 0.01243524719029665, R2 0.521007239818573\n",
      "epoch 5953, loss 0.01243156474083662, R2 0.4832693338394165\n",
      "Eval loss 0.012435165233910084, R2 0.5210105180740356\n",
      "epoch 5954, loss 0.0124314920976758, R2 0.48327189683914185\n",
      "Eval loss 0.012435087002813816, R2 0.5210136771202087\n",
      "epoch 5955, loss 0.01243141945451498, R2 0.48327505588531494\n",
      "Eval loss 0.012435007840394974, R2 0.5210164785385132\n",
      "epoch 5956, loss 0.012431347742676735, R2 0.4832780361175537\n",
      "Eval loss 0.012434928677976131, R2 0.5210198163986206\n",
      "epoch 5957, loss 0.01243127416819334, R2 0.4832814335823059\n",
      "Eval loss 0.012434848584234715, R2 0.5210224390029907\n",
      "epoch 5958, loss 0.01243120152503252, R2 0.4832835793495178\n",
      "Eval loss 0.012434769421815872, R2 0.5210256576538086\n",
      "epoch 5959, loss 0.01243113074451685, R2 0.48328691720962524\n",
      "Eval loss 0.012434691190719604, R2 0.5210289359092712\n",
      "epoch 5960, loss 0.01243105810135603, R2 0.4832903742790222\n",
      "Eval loss 0.012434612028300762, R2 0.5210315585136414\n",
      "epoch 5961, loss 0.012430986389517784, R2 0.48329269886016846\n",
      "Eval loss 0.01243453286588192, R2 0.5210347175598145\n",
      "epoch 5962, loss 0.012430914677679539, R2 0.4832960367202759\n",
      "Eval loss 0.012434455566108227, R2 0.5210378766059875\n",
      "epoch 5963, loss 0.012430842965841293, R2 0.4832993149757385\n",
      "Eval loss 0.01243437547236681, R2 0.5210409164428711\n",
      "epoch 5964, loss 0.012430770322680473, R2 0.48330169916152954\n",
      "Eval loss 0.012434297241270542, R2 0.5210443735122681\n",
      "epoch 5965, loss 0.012430698610842228, R2 0.48330485820770264\n",
      "Eval loss 0.012434219010174274, R2 0.5210468769073486\n",
      "epoch 5966, loss 0.012430627830326557, R2 0.48330777883529663\n",
      "Eval loss 0.012434138916432858, R2 0.521050214767456\n",
      "epoch 5967, loss 0.012430556118488312, R2 0.4833107590675354\n",
      "Eval loss 0.01243406068533659, R2 0.5210530757904053\n",
      "epoch 5968, loss 0.012430484406650066, R2 0.4833139181137085\n",
      "Eval loss 0.012433982454240322, R2 0.521056056022644\n",
      "epoch 5969, loss 0.012430412694811821, R2 0.4833166003227234\n",
      "Eval loss 0.012433905154466629, R2 0.5210590362548828\n",
      "epoch 5970, loss 0.012430340051651001, R2 0.48331958055496216\n",
      "Eval loss 0.012433825992047787, R2 0.5210618376731873\n",
      "epoch 5971, loss 0.012430267408490181, R2 0.48332279920578003\n",
      "Eval loss 0.012433746829628944, R2 0.5210652947425842\n",
      "epoch 5972, loss 0.01243019662797451, R2 0.483325719833374\n",
      "Eval loss 0.012433669529855251, R2 0.5210682153701782\n",
      "epoch 5973, loss 0.012430124916136265, R2 0.4833287000656128\n",
      "Eval loss 0.012433591298758984, R2 0.521071195602417\n",
      "epoch 5974, loss 0.012430054135620594, R2 0.48333150148391724\n",
      "Eval loss 0.01243351399898529, R2 0.5210741758346558\n",
      "epoch 5975, loss 0.012429981492459774, R2 0.48333466053009033\n",
      "Eval loss 0.012433434836566448, R2 0.5210773348808289\n",
      "epoch 5976, loss 0.012429910711944103, R2 0.48333775997161865\n",
      "Eval loss 0.012433357536792755, R2 0.5210798978805542\n",
      "epoch 5977, loss 0.012429839000105858, R2 0.4833405613899231\n",
      "Eval loss 0.012433278374373913, R2 0.5210831165313721\n",
      "epoch 5978, loss 0.012429768219590187, R2 0.4833436608314514\n",
      "Eval loss 0.01243320107460022, R2 0.5210864543914795\n",
      "epoch 5979, loss 0.012429697439074516, R2 0.48334646224975586\n",
      "Eval loss 0.012433121912181377, R2 0.5210890769958496\n",
      "epoch 5980, loss 0.012429625727236271, R2 0.48334944248199463\n",
      "Eval loss 0.01243304368108511, R2 0.521092414855957\n",
      "epoch 5981, loss 0.0124295549467206, R2 0.48335254192352295\n",
      "Eval loss 0.012432966381311417, R2 0.5210953950881958\n",
      "epoch 5982, loss 0.01242948230355978, R2 0.48335540294647217\n",
      "Eval loss 0.012432888150215149, R2 0.5210980176925659\n",
      "epoch 5983, loss 0.01242941152304411, R2 0.4833580255508423\n",
      "Eval loss 0.012432810850441456, R2 0.5211012363433838\n",
      "epoch 5984, loss 0.012429340742528439, R2 0.48336130380630493\n",
      "Eval loss 0.012432732619345188, R2 0.5211039781570435\n",
      "epoch 5985, loss 0.012429269962012768, R2 0.4833640456199646\n",
      "Eval loss 0.012432655319571495, R2 0.5211074948310852\n",
      "epoch 5986, loss 0.012429199181497097, R2 0.48336732387542725\n",
      "Eval loss 0.012432577088475227, R2 0.5211104154586792\n",
      "epoch 5987, loss 0.012429127469658852, R2 0.4833701252937317\n",
      "Eval loss 0.012432499788701534, R2 0.5211133360862732\n",
      "epoch 5988, loss 0.01242905668914318, R2 0.483373761177063\n",
      "Eval loss 0.012432422488927841, R2 0.521115779876709\n",
      "epoch 5989, loss 0.01242898590862751, R2 0.4833762049674988\n",
      "Eval loss 0.012432344257831573, R2 0.5211190581321716\n",
      "epoch 5990, loss 0.01242891512811184, R2 0.4833790063858032\n",
      "Eval loss 0.01243226695805788, R2 0.5211221575737\n",
      "epoch 5991, loss 0.012428844347596169, R2 0.4833819270133972\n",
      "Eval loss 0.012432188726961613, R2 0.521125316619873\n",
      "epoch 5992, loss 0.012428773567080498, R2 0.48338502645492554\n",
      "Eval loss 0.01243211142718792, R2 0.5211282968521118\n",
      "epoch 5993, loss 0.012428703717887402, R2 0.48338842391967773\n",
      "Eval loss 0.012432035058736801, R2 0.5211312770843506\n",
      "epoch 5994, loss 0.01242863293737173, R2 0.48339134454727173\n",
      "Eval loss 0.012431956827640533, R2 0.5211339592933655\n",
      "epoch 5995, loss 0.012428561225533485, R2 0.4833938479423523\n",
      "Eval loss 0.012431878596544266, R2 0.5211372375488281\n",
      "epoch 5996, loss 0.01242849137634039, R2 0.4833967685699463\n",
      "Eval loss 0.012431802228093147, R2 0.5211399793624878\n",
      "epoch 5997, loss 0.012428419664502144, R2 0.48340004682540894\n",
      "Eval loss 0.012431725859642029, R2 0.5211428999900818\n",
      "epoch 5998, loss 0.012428350746631622, R2 0.48340296745300293\n",
      "Eval loss 0.012431647628545761, R2 0.5211461782455444\n",
      "epoch 5999, loss 0.012428279966115952, R2 0.4834054112434387\n",
      "Eval loss 0.012431572191417217, R2 0.5211490988731384\n",
      "epoch 6000, loss 0.012428210116922855, R2 0.48340797424316406\n",
      "Eval loss 0.01243149396032095, R2 0.5211519598960876\n",
      "epoch 6001, loss 0.012428139336407185, R2 0.48341089487075806\n",
      "Eval loss 0.012431417591869831, R2 0.5211549997329712\n",
      "epoch 6002, loss 0.012428069487214088, R2 0.4834141731262207\n",
      "Eval loss 0.012431339360773563, R2 0.5211577415466309\n",
      "epoch 6003, loss 0.012427998706698418, R2 0.4834170937538147\n",
      "Eval loss 0.012431262992322445, R2 0.5211606025695801\n",
      "epoch 6004, loss 0.012427926994860172, R2 0.48342055082321167\n",
      "Eval loss 0.012431186623871326, R2 0.5211639404296875\n",
      "epoch 6005, loss 0.012427857145667076, R2 0.4834229350090027\n",
      "Eval loss 0.012431108392775059, R2 0.5211668014526367\n",
      "epoch 6006, loss 0.012427786365151405, R2 0.4834258556365967\n",
      "Eval loss 0.01243103202432394, R2 0.5211700201034546\n",
      "epoch 6007, loss 0.012427718378603458, R2 0.4834293723106384\n",
      "Eval loss 0.012430956587195396, R2 0.5211724042892456\n",
      "epoch 6008, loss 0.012427649460732937, R2 0.4834315776824951\n",
      "Eval loss 0.012430878356099129, R2 0.5211758017539978\n",
      "epoch 6009, loss 0.012427577748894691, R2 0.4834352135658264\n",
      "Eval loss 0.01243080198764801, R2 0.5211786031723022\n",
      "epoch 6010, loss 0.012427507899701595, R2 0.48343712091445923\n",
      "Eval loss 0.012430725619196892, R2 0.5211812257766724\n",
      "epoch 6011, loss 0.012427438981831074, R2 0.4834405183792114\n",
      "Eval loss 0.012430650182068348, R2 0.5211844444274902\n",
      "epoch 6012, loss 0.012427369132637978, R2 0.4834432601928711\n",
      "Eval loss 0.012430572882294655, R2 0.5211870670318604\n",
      "epoch 6013, loss 0.012427297420799732, R2 0.48344624042510986\n",
      "Eval loss 0.012430496513843536, R2 0.5211905241012573\n",
      "epoch 6014, loss 0.01242722850292921, R2 0.48344969749450684\n",
      "Eval loss 0.012430420145392418, R2 0.5211933255195618\n",
      "epoch 6015, loss 0.012427158653736115, R2 0.4834520220756531\n",
      "Eval loss 0.012430342845618725, R2 0.5211964845657349\n",
      "epoch 6016, loss 0.012427089735865593, R2 0.48345500230789185\n",
      "Eval loss 0.012430267408490181, R2 0.5211992263793945\n",
      "epoch 6017, loss 0.012427018955349922, R2 0.4834578037261963\n",
      "Eval loss 0.012430191040039062, R2 0.5212019681930542\n",
      "epoch 6018, loss 0.0124269500374794, R2 0.4834606647491455\n",
      "Eval loss 0.012430114671587944, R2 0.521204948425293\n",
      "epoch 6019, loss 0.012426881119608879, R2 0.48346400260925293\n",
      "Eval loss 0.012430038303136826, R2 0.5212077498435974\n",
      "epoch 6020, loss 0.012426811270415783, R2 0.4834664463996887\n",
      "Eval loss 0.012429961934685707, R2 0.5212110877037048\n",
      "epoch 6021, loss 0.012426742352545261, R2 0.4834694266319275\n",
      "Eval loss 0.012429886497557163, R2 0.5212142467498779\n",
      "epoch 6022, loss 0.01242667157202959, R2 0.48347222805023193\n",
      "Eval loss 0.01242981106042862, R2 0.5212167501449585\n",
      "epoch 6023, loss 0.012426603585481644, R2 0.48347508907318115\n",
      "Eval loss 0.012429735623300076, R2 0.5212197303771973\n",
      "epoch 6024, loss 0.012426533736288548, R2 0.48347848653793335\n",
      "Eval loss 0.012429657392203808, R2 0.5212225914001465\n",
      "epoch 6025, loss 0.012426464818418026, R2 0.4834812879562378\n",
      "Eval loss 0.012429583817720413, R2 0.5212256908416748\n",
      "epoch 6026, loss 0.01242639496922493, R2 0.4834837317466736\n",
      "Eval loss 0.01242950651794672, R2 0.5212286710739136\n",
      "epoch 6027, loss 0.012426326051354408, R2 0.48348677158355713\n",
      "Eval loss 0.012429431080818176, R2 0.5212314128875732\n",
      "epoch 6028, loss 0.012426256202161312, R2 0.4834896922111511\n",
      "Eval loss 0.012429354712367058, R2 0.5212345123291016\n",
      "epoch 6029, loss 0.012426186352968216, R2 0.48349255323410034\n",
      "Eval loss 0.012429280206561089, R2 0.5212370753288269\n",
      "epoch 6030, loss 0.012426117435097694, R2 0.48349523544311523\n",
      "Eval loss 0.01242920383810997, R2 0.5212398767471313\n",
      "epoch 6031, loss 0.012426049448549747, R2 0.48349857330322266\n",
      "Eval loss 0.012429128400981426, R2 0.5212432146072388\n",
      "epoch 6032, loss 0.012425979599356651, R2 0.48350101709365845\n",
      "Eval loss 0.012429052032530308, R2 0.5212457180023193\n",
      "epoch 6033, loss 0.012425911612808704, R2 0.4835038185119629\n",
      "Eval loss 0.01242897566407919, R2 0.5212488174438477\n",
      "epoch 6034, loss 0.012425842694938183, R2 0.4835066795349121\n",
      "Eval loss 0.01242890115827322, R2 0.5212516784667969\n",
      "epoch 6035, loss 0.012425773777067661, R2 0.48350989818573\n",
      "Eval loss 0.01242882665246725, R2 0.5212547183036804\n",
      "epoch 6036, loss 0.01242570485919714, R2 0.48351240158081055\n",
      "Eval loss 0.012428750284016132, R2 0.521257758140564\n",
      "epoch 6037, loss 0.012425635941326618, R2 0.4835149645805359\n",
      "Eval loss 0.012428674846887589, R2 0.5212603807449341\n",
      "epoch 6038, loss 0.012425567023456097, R2 0.4835183024406433\n",
      "Eval loss 0.01242860034108162, R2 0.521263599395752\n",
      "epoch 6039, loss 0.012425498105585575, R2 0.483521044254303\n",
      "Eval loss 0.012428524903953075, R2 0.5212665796279907\n",
      "epoch 6040, loss 0.012425429187715054, R2 0.4835238456726074\n",
      "Eval loss 0.012428449466824532, R2 0.5212692022323608\n",
      "epoch 6041, loss 0.012425360269844532, R2 0.48352688550949097\n",
      "Eval loss 0.012428374029695988, R2 0.5212721228599548\n",
      "epoch 6042, loss 0.01242529321461916, R2 0.4835295081138611\n",
      "Eval loss 0.01242829766124487, R2 0.5212749242782593\n",
      "epoch 6043, loss 0.012425223365426064, R2 0.4835324287414551\n",
      "Eval loss 0.012428222224116325, R2 0.5212781429290771\n",
      "epoch 6044, loss 0.012425153516232967, R2 0.4835354685783386\n",
      "Eval loss 0.01242814864963293, R2 0.5212806463241577\n",
      "epoch 6045, loss 0.01242508739233017, R2 0.48353856801986694\n",
      "Eval loss 0.012428074143826962, R2 0.5212833881378174\n",
      "epoch 6046, loss 0.012425019405782223, R2 0.4835408926010132\n",
      "Eval loss 0.012427998706698418, R2 0.5212863087654114\n",
      "epoch 6047, loss 0.012424949556589127, R2 0.4835442900657654\n",
      "Eval loss 0.012427924200892448, R2 0.5212897062301636\n",
      "epoch 6048, loss 0.01242488157004118, R2 0.4835466146469116\n",
      "Eval loss 0.012427848763763905, R2 0.5212922096252441\n",
      "epoch 6049, loss 0.012424811720848083, R2 0.48354941606521606\n",
      "Eval loss 0.012427774257957935, R2 0.5212949514389038\n",
      "epoch 6050, loss 0.012424744665622711, R2 0.48355233669281006\n",
      "Eval loss 0.012427698820829391, R2 0.5212981700897217\n",
      "epoch 6051, loss 0.012424677610397339, R2 0.4835556149482727\n",
      "Eval loss 0.012427624315023422, R2 0.5213010311126709\n",
      "epoch 6052, loss 0.012424608692526817, R2 0.48355770111083984\n",
      "Eval loss 0.012427549809217453, R2 0.5213037729263306\n",
      "epoch 6053, loss 0.012424541637301445, R2 0.4835609197616577\n",
      "Eval loss 0.012427475303411484, R2 0.5213068723678589\n",
      "epoch 6054, loss 0.012424472719430923, R2 0.4835636019706726\n",
      "Eval loss 0.01242739986628294, R2 0.5213092565536499\n",
      "epoch 6055, loss 0.012424403801560402, R2 0.48356711864471436\n",
      "Eval loss 0.012427326291799545, R2 0.5213122367858887\n",
      "epoch 6056, loss 0.01242433674633503, R2 0.4835694432258606\n",
      "Eval loss 0.012427251785993576, R2 0.5213150978088379\n",
      "epoch 6057, loss 0.012424267828464508, R2 0.48357266187667847\n",
      "Eval loss 0.012427176348865032, R2 0.521318256855011\n",
      "epoch 6058, loss 0.012424200773239136, R2 0.4835752248764038\n",
      "Eval loss 0.012427100911736488, R2 0.5213208794593811\n",
      "epoch 6059, loss 0.012424133718013763, R2 0.4835777282714844\n",
      "Eval loss 0.012427027337253094, R2 0.5213238596916199\n",
      "epoch 6060, loss 0.012424063868820667, R2 0.4835808277130127\n",
      "Eval loss 0.012426953762769699, R2 0.5213271379470825\n",
      "epoch 6061, loss 0.012423996813595295, R2 0.48358339071273804\n",
      "Eval loss 0.012426880188286304, R2 0.521329402923584\n",
      "epoch 6062, loss 0.012423927895724773, R2 0.4835869073867798\n",
      "Eval loss 0.01242680475115776, R2 0.5213322639465332\n",
      "epoch 6063, loss 0.01242386270314455, R2 0.4835889935493469\n",
      "Eval loss 0.012426730245351791, R2 0.5213353037834167\n",
      "epoch 6064, loss 0.012423794716596603, R2 0.48359179496765137\n",
      "Eval loss 0.012426656670868397, R2 0.5213384628295898\n",
      "epoch 6065, loss 0.012423725798726082, R2 0.4835946559906006\n",
      "Eval loss 0.012426582165062428, R2 0.5213413238525391\n",
      "epoch 6066, loss 0.01242365874350071, R2 0.48359745740890503\n",
      "Eval loss 0.012426508590579033, R2 0.5213444232940674\n",
      "epoch 6067, loss 0.012423591688275337, R2 0.4836007356643677\n",
      "Eval loss 0.012426433153450489, R2 0.5213465690612793\n",
      "epoch 6068, loss 0.012423522770404816, R2 0.4836031198501587\n",
      "Eval loss 0.012426360510289669, R2 0.5213496685028076\n",
      "epoch 6069, loss 0.012423456646502018, R2 0.48360586166381836\n",
      "Eval loss 0.012426286935806274, R2 0.5213526487350464\n",
      "epoch 6070, loss 0.012423387728631496, R2 0.4836087226867676\n",
      "Eval loss 0.012426212430000305, R2 0.521355390548706\n",
      "epoch 6071, loss 0.012423320673406124, R2 0.4836120009422302\n",
      "Eval loss 0.012426139786839485, R2 0.5213584899902344\n",
      "epoch 6072, loss 0.012423253618180752, R2 0.48361480236053467\n",
      "Eval loss 0.012426064349710941, R2 0.521361231803894\n",
      "epoch 6073, loss 0.01242318656295538, R2 0.48361724615097046\n",
      "Eval loss 0.012425991706550121, R2 0.5213640332221985\n",
      "epoch 6074, loss 0.012423119507730007, R2 0.4836198687553406\n",
      "Eval loss 0.012425918132066727, R2 0.5213667154312134\n",
      "epoch 6075, loss 0.01242305152118206, R2 0.483622670173645\n",
      "Eval loss 0.012425842694938183, R2 0.5213699340820312\n",
      "epoch 6076, loss 0.012422984465956688, R2 0.48362553119659424\n",
      "Eval loss 0.012425770051777363, R2 0.5213723182678223\n",
      "epoch 6077, loss 0.01242291834205389, R2 0.48362821340560913\n",
      "Eval loss 0.012425697408616543, R2 0.5213749408721924\n",
      "epoch 6078, loss 0.012422850355505943, R2 0.4836311936378479\n",
      "Eval loss 0.012425622902810574, R2 0.5213785171508789\n",
      "epoch 6079, loss 0.012422784231603146, R2 0.48363417387008667\n",
      "Eval loss 0.012425550259649754, R2 0.5213809013366699\n",
      "epoch 6080, loss 0.012422716245055199, R2 0.48363614082336426\n",
      "Eval loss 0.012425475753843784, R2 0.5213836431503296\n",
      "epoch 6081, loss 0.012422649189829826, R2 0.4836394190788269\n",
      "Eval loss 0.01242540217936039, R2 0.521386444568634\n",
      "epoch 6082, loss 0.012422583065927029, R2 0.48364269733428955\n",
      "Eval loss 0.01242532953619957, R2 0.5213895440101624\n",
      "epoch 6083, loss 0.012422515079379082, R2 0.4836452007293701\n",
      "Eval loss 0.012425255961716175, R2 0.5213922262191772\n",
      "epoch 6084, loss 0.01242244802415371, R2 0.48364776372909546\n",
      "Eval loss 0.01242518238723278, R2 0.5213950872421265\n",
      "epoch 6085, loss 0.012422380968928337, R2 0.48365074396133423\n",
      "Eval loss 0.01242510974407196, R2 0.521397590637207\n",
      "epoch 6086, loss 0.012422315776348114, R2 0.4836534857749939\n",
      "Eval loss 0.012425036169588566, R2 0.5214006900787354\n",
      "epoch 6087, loss 0.012422248721122742, R2 0.48365604877471924\n",
      "Eval loss 0.012424963526427746, R2 0.5214036107063293\n",
      "epoch 6088, loss 0.01242218166589737, R2 0.4836588501930237\n",
      "Eval loss 0.012424889951944351, R2 0.5214061737060547\n",
      "epoch 6089, loss 0.012422115541994572, R2 0.48366159200668335\n",
      "Eval loss 0.012424817308783531, R2 0.5214091539382935\n",
      "epoch 6090, loss 0.0124220484867692, R2 0.4836643934249878\n",
      "Eval loss 0.012424742802977562, R2 0.5214120149612427\n",
      "epoch 6091, loss 0.012421980500221252, R2 0.48366719484329224\n",
      "Eval loss 0.012424668297171593, R2 0.5214149951934814\n",
      "epoch 6092, loss 0.01242191530764103, R2 0.4836699366569519\n",
      "Eval loss 0.012424597516655922, R2 0.5214178562164307\n",
      "epoch 6093, loss 0.012421849183738232, R2 0.4836728572845459\n",
      "Eval loss 0.012424523942172527, R2 0.521420419216156\n",
      "epoch 6094, loss 0.01242178212851286, R2 0.483675479888916\n",
      "Eval loss 0.012424451299011707, R2 0.5214231014251709\n",
      "epoch 6095, loss 0.012421716004610062, R2 0.4836782217025757\n",
      "Eval loss 0.012424378655850887, R2 0.521425724029541\n",
      "epoch 6096, loss 0.01242164894938469, R2 0.48368167877197266\n",
      "Eval loss 0.012424306944012642, R2 0.5214285254478455\n",
      "epoch 6097, loss 0.012421582825481892, R2 0.48368388414382935\n",
      "Eval loss 0.012424233369529247, R2 0.5214316248893738\n",
      "epoch 6098, loss 0.012421517632901669, R2 0.48368632793426514\n",
      "Eval loss 0.012424160726368427, R2 0.5214341282844543\n",
      "epoch 6099, loss 0.012421450577676296, R2 0.48368972539901733\n",
      "Eval loss 0.012424088083207607, R2 0.5214370489120483\n",
      "epoch 6100, loss 0.012421384453773499, R2 0.4836919903755188\n",
      "Eval loss 0.012424015440046787, R2 0.5214399099349976\n",
      "epoch 6101, loss 0.012421319261193275, R2 0.48369520902633667\n",
      "Eval loss 0.012423943728208542, R2 0.5214427709579468\n",
      "epoch 6102, loss 0.012421252205967903, R2 0.48369747400283813\n",
      "Eval loss 0.012423872016370296, R2 0.5214456915855408\n",
      "epoch 6103, loss 0.01242118701338768, R2 0.4837002158164978\n",
      "Eval loss 0.012423797510564327, R2 0.52144855260849\n",
      "epoch 6104, loss 0.012421120889484882, R2 0.4837031364440918\n",
      "Eval loss 0.012423724867403507, R2 0.5214512348175049\n",
      "epoch 6105, loss 0.012421054765582085, R2 0.48370617628097534\n",
      "Eval loss 0.012423653155565262, R2 0.5214536786079407\n",
      "epoch 6106, loss 0.012420986779034138, R2 0.4837085008621216\n",
      "Eval loss 0.012423581443727016, R2 0.5214570164680481\n",
      "epoch 6107, loss 0.012420923449099064, R2 0.4837113618850708\n",
      "Eval loss 0.012423508800566196, R2 0.5214592218399048\n",
      "epoch 6108, loss 0.012420856393873692, R2 0.48371392488479614\n",
      "Eval loss 0.012423435226082802, R2 0.5214622020721436\n",
      "epoch 6109, loss 0.01242078933864832, R2 0.4837167263031006\n",
      "Eval loss 0.012423364445567131, R2 0.5214652419090271\n",
      "epoch 6110, loss 0.01242072507739067, R2 0.4837194085121155\n",
      "Eval loss 0.012423289939761162, R2 0.521467924118042\n",
      "epoch 6111, loss 0.012420658953487873, R2 0.4837228059768677\n",
      "Eval loss 0.012423219159245491, R2 0.5214706659317017\n",
      "epoch 6112, loss 0.012420592829585075, R2 0.4837248921394348\n",
      "Eval loss 0.012423147447407246, R2 0.5214734673500061\n",
      "epoch 6113, loss 0.012420527637004852, R2 0.4837278127670288\n",
      "Eval loss 0.012423075735569, R2 0.5214762091636658\n",
      "epoch 6114, loss 0.012420461513102055, R2 0.48373037576675415\n",
      "Eval loss 0.01242300309240818, R2 0.5214788913726807\n",
      "epoch 6115, loss 0.012420395389199257, R2 0.483733594417572\n",
      "Eval loss 0.01242293044924736, R2 0.5214818120002747\n",
      "epoch 6116, loss 0.012420332059264183, R2 0.48373574018478394\n",
      "Eval loss 0.012422858737409115, R2 0.5214849710464478\n",
      "epoch 6117, loss 0.012420264072716236, R2 0.4837385416030884\n",
      "Eval loss 0.012422787956893444, R2 0.5214871764183044\n",
      "epoch 6118, loss 0.012420199811458588, R2 0.4837414026260376\n",
      "Eval loss 0.012422715313732624, R2 0.5214898586273193\n",
      "epoch 6119, loss 0.01242013555020094, R2 0.4837445616722107\n",
      "Eval loss 0.012422643601894379, R2 0.5214927196502686\n",
      "epoch 6120, loss 0.012420067563652992, R2 0.48374688625335693\n",
      "Eval loss 0.012422571890056133, R2 0.5214955806732178\n",
      "epoch 6121, loss 0.012420004233717918, R2 0.4837495684623718\n",
      "Eval loss 0.012422500178217888, R2 0.5214983820915222\n",
      "epoch 6122, loss 0.01241993810981512, R2 0.48375195264816284\n",
      "Eval loss 0.012422429397702217, R2 0.5215011239051819\n",
      "epoch 6123, loss 0.012419873848557472, R2 0.4837547540664673\n",
      "Eval loss 0.012422357685863972, R2 0.5215038657188416\n",
      "epoch 6124, loss 0.01241980865597725, R2 0.4837576746940613\n",
      "Eval loss 0.012422285974025726, R2 0.521506667137146\n",
      "epoch 6125, loss 0.012419742532074451, R2 0.4837602376937866\n",
      "Eval loss 0.012422213330864906, R2 0.5215091705322266\n",
      "epoch 6126, loss 0.012419676408171654, R2 0.48376280069351196\n",
      "Eval loss 0.012422142550349236, R2 0.5215121507644653\n",
      "epoch 6127, loss 0.01241961307823658, R2 0.4837656617164612\n",
      "Eval loss 0.012422071769833565, R2 0.5215146541595459\n",
      "epoch 6128, loss 0.012419547885656357, R2 0.4837685227394104\n",
      "Eval loss 0.012422000989317894, R2 0.5215173363685608\n",
      "epoch 6129, loss 0.012419482693076134, R2 0.48377108573913574\n",
      "Eval loss 0.012421929277479649, R2 0.5215206146240234\n",
      "epoch 6130, loss 0.01241941750049591, R2 0.48377442359924316\n",
      "Eval loss 0.012421857565641403, R2 0.5215229988098145\n",
      "epoch 6131, loss 0.012419352307915688, R2 0.4837764501571655\n",
      "Eval loss 0.012421785853803158, R2 0.5215258598327637\n",
      "epoch 6132, loss 0.012419288046658039, R2 0.4837796688079834\n",
      "Eval loss 0.012421715073287487, R2 0.5215284824371338\n",
      "epoch 6133, loss 0.012419221922755241, R2 0.4837820529937744\n",
      "Eval loss 0.012421643361449242, R2 0.5215311050415039\n",
      "epoch 6134, loss 0.012419157661497593, R2 0.4837847352027893\n",
      "Eval loss 0.01242157258093357, R2 0.5215340852737427\n",
      "epoch 6135, loss 0.012419093400239944, R2 0.4837875962257385\n",
      "Eval loss 0.0124215018004179, R2 0.5215368270874023\n",
      "epoch 6136, loss 0.012419027276337147, R2 0.48378998041152954\n",
      "Eval loss 0.01242143101990223, R2 0.521539568901062\n",
      "epoch 6137, loss 0.012418963015079498, R2 0.48379313945770264\n",
      "Eval loss 0.012421360239386559, R2 0.5215421915054321\n",
      "epoch 6138, loss 0.012418897822499275, R2 0.48379552364349365\n",
      "Eval loss 0.012421287596225739, R2 0.5215450525283813\n",
      "epoch 6139, loss 0.012418833561241627, R2 0.4837978482246399\n",
      "Eval loss 0.012421216815710068, R2 0.5215476751327515\n",
      "epoch 6140, loss 0.012418769299983978, R2 0.4838011860847473\n",
      "Eval loss 0.012421146035194397, R2 0.5215502381324768\n",
      "epoch 6141, loss 0.01241870503872633, R2 0.48380351066589355\n",
      "Eval loss 0.0124210761860013, R2 0.521553099155426\n",
      "epoch 6142, loss 0.012418639846146107, R2 0.48380571603775024\n",
      "Eval loss 0.012421004474163055, R2 0.5215559005737305\n",
      "epoch 6143, loss 0.012418576516211033, R2 0.483808696269989\n",
      "Eval loss 0.012420933693647385, R2 0.521558403968811\n",
      "epoch 6144, loss 0.01241851132363081, R2 0.4838120937347412\n",
      "Eval loss 0.012420863844454288, R2 0.5215613842010498\n",
      "epoch 6145, loss 0.012418447993695736, R2 0.4838140606880188\n",
      "Eval loss 0.012420793995261192, R2 0.5215641260147095\n",
      "epoch 6146, loss 0.012418382801115513, R2 0.483816921710968\n",
      "Eval loss 0.012420721352100372, R2 0.5215670466423035\n",
      "epoch 6147, loss 0.012418318539857864, R2 0.4838194251060486\n",
      "Eval loss 0.012420650571584702, R2 0.5215697884559631\n",
      "epoch 6148, loss 0.012418254278600216, R2 0.4838221073150635\n",
      "Eval loss 0.012420580722391605, R2 0.5215719938278198\n",
      "epoch 6149, loss 0.012418189086019993, R2 0.483825147151947\n",
      "Eval loss 0.01242051087319851, R2 0.5215747356414795\n",
      "epoch 6150, loss 0.012418124824762344, R2 0.48382747173309326\n",
      "Eval loss 0.012420440092682838, R2 0.5215780735015869\n",
      "epoch 6151, loss 0.012418060563504696, R2 0.4838303327560425\n",
      "Eval loss 0.012420370243489742, R2 0.5215802788734436\n",
      "epoch 6152, loss 0.012417996302247047, R2 0.48383283615112305\n",
      "Eval loss 0.012420297600328922, R2 0.5215831995010376\n",
      "epoch 6153, loss 0.012417932972311974, R2 0.48383545875549316\n",
      "Eval loss 0.012420227751135826, R2 0.5215854644775391\n",
      "epoch 6154, loss 0.012417868711054325, R2 0.48383814096450806\n",
      "Eval loss 0.012420158833265305, R2 0.5215884447097778\n",
      "epoch 6155, loss 0.012417804449796677, R2 0.4838411211967468\n",
      "Eval loss 0.012420088052749634, R2 0.521591305732727\n",
      "epoch 6156, loss 0.012417740188539028, R2 0.4838433265686035\n",
      "Eval loss 0.012420017272233963, R2 0.5215940475463867\n",
      "epoch 6157, loss 0.012417676858603954, R2 0.48384612798690796\n",
      "Eval loss 0.012419947423040867, R2 0.5215966701507568\n",
      "epoch 6158, loss 0.01241761352866888, R2 0.4838488698005676\n",
      "Eval loss 0.012419876642525196, R2 0.5215994119644165\n",
      "epoch 6159, loss 0.012417550198733807, R2 0.4838513731956482\n",
      "Eval loss 0.012419807724654675, R2 0.5216020941734314\n",
      "epoch 6160, loss 0.012417485937476158, R2 0.4838540554046631\n",
      "Eval loss 0.012419737875461578, R2 0.5216043591499329\n",
      "epoch 6161, loss 0.012417422607541084, R2 0.4838566780090332\n",
      "Eval loss 0.012419667094945908, R2 0.5216076374053955\n",
      "epoch 6162, loss 0.012417358346283436, R2 0.4838598370552063\n",
      "Eval loss 0.012419597245752811, R2 0.5216097831726074\n",
      "epoch 6163, loss 0.012417294085025787, R2 0.48386263847351074\n",
      "Eval loss 0.01241952832788229, R2 0.5216128826141357\n",
      "epoch 6164, loss 0.012417230755090714, R2 0.4838644862174988\n",
      "Eval loss 0.012419457547366619, R2 0.5216158628463745\n",
      "epoch 6165, loss 0.01241716742515564, R2 0.48386746644973755\n",
      "Eval loss 0.012419387698173523, R2 0.521618127822876\n",
      "epoch 6166, loss 0.012417104095220566, R2 0.48386991024017334\n",
      "Eval loss 0.012419317848980427, R2 0.5216209888458252\n",
      "epoch 6167, loss 0.012417039833962917, R2 0.48387306928634644\n",
      "Eval loss 0.01241924799978733, R2 0.5216232538223267\n",
      "epoch 6168, loss 0.012416975572705269, R2 0.48387521505355835\n",
      "Eval loss 0.012419179081916809, R2 0.5216259956359863\n",
      "epoch 6169, loss 0.01241691317409277, R2 0.48387783765792847\n",
      "Eval loss 0.012419110164046288, R2 0.5216289758682251\n",
      "epoch 6170, loss 0.012416848912835121, R2 0.4838806986808777\n",
      "Eval loss 0.012419038452208042, R2 0.5216317176818848\n",
      "epoch 6171, loss 0.012416786514222622, R2 0.4838831424713135\n",
      "Eval loss 0.01241896953433752, R2 0.521634578704834\n",
      "epoch 6172, loss 0.012416722252964973, R2 0.4838859438896179\n",
      "Eval loss 0.012418899685144424, R2 0.5216370820999146\n",
      "epoch 6173, loss 0.0124166589230299, R2 0.48388874530792236\n",
      "Eval loss 0.012418830767273903, R2 0.5216397047042847\n",
      "epoch 6174, loss 0.0124165965244174, R2 0.48389101028442383\n",
      "Eval loss 0.012418760918080807, R2 0.5216424465179443\n",
      "epoch 6175, loss 0.012416532263159752, R2 0.4838936924934387\n",
      "Eval loss 0.012418692000210285, R2 0.5216449499130249\n",
      "epoch 6176, loss 0.012416469864547253, R2 0.4838964343070984\n",
      "Eval loss 0.012418622151017189, R2 0.5216478109359741\n",
      "epoch 6177, loss 0.012416405603289604, R2 0.4838991165161133\n",
      "Eval loss 0.012418554164469242, R2 0.5216503739356995\n",
      "epoch 6178, loss 0.01241634413599968, R2 0.4839015007019043\n",
      "Eval loss 0.012418483383953571, R2 0.5216530561447144\n",
      "epoch 6179, loss 0.012416280806064606, R2 0.4839041233062744\n",
      "Eval loss 0.01241841446608305, R2 0.5216556787490845\n",
      "epoch 6180, loss 0.012416217476129532, R2 0.48390692472457886\n",
      "Eval loss 0.012418344616889954, R2 0.5216584205627441\n",
      "epoch 6181, loss 0.012416154146194458, R2 0.483909547328949\n",
      "Eval loss 0.012418276630342007, R2 0.5216610431671143\n",
      "epoch 6182, loss 0.012416091747581959, R2 0.48391181230545044\n",
      "Eval loss 0.012418205849826336, R2 0.5216637849807739\n",
      "epoch 6183, loss 0.012416028417646885, R2 0.48391467332839966\n",
      "Eval loss 0.012418138794600964, R2 0.5216665267944336\n",
      "epoch 6184, loss 0.012415966019034386, R2 0.483917236328125\n",
      "Eval loss 0.012418068014085293, R2 0.5216692686080933\n",
      "epoch 6185, loss 0.012415902689099312, R2 0.48391997814178467\n",
      "Eval loss 0.012418000027537346, R2 0.5216718912124634\n",
      "epoch 6186, loss 0.012415839359164238, R2 0.48392295837402344\n",
      "Eval loss 0.01241793017834425, R2 0.521674394607544\n",
      "epoch 6187, loss 0.012415776029229164, R2 0.48392510414123535\n",
      "Eval loss 0.012417862191796303, R2 0.5216771960258484\n",
      "epoch 6188, loss 0.012415713630616665, R2 0.483928382396698\n",
      "Eval loss 0.012417793273925781, R2 0.5216794013977051\n",
      "epoch 6189, loss 0.012415651232004166, R2 0.483930766582489\n",
      "Eval loss 0.01241772435605526, R2 0.5216823220252991\n",
      "epoch 6190, loss 0.012415589764714241, R2 0.4839335083961487\n",
      "Eval loss 0.012417656369507313, R2 0.5216851234436035\n",
      "epoch 6191, loss 0.012415527366101742, R2 0.4839355945587158\n",
      "Eval loss 0.012417587451636791, R2 0.5216876268386841\n",
      "epoch 6192, loss 0.012415462173521519, R2 0.48393815755844116\n",
      "Eval loss 0.01241751853376627, R2 0.5216907262802124\n",
      "epoch 6193, loss 0.012415400706231594, R2 0.4839407205581665\n",
      "Eval loss 0.012417449615895748, R2 0.5216926336288452\n",
      "epoch 6194, loss 0.012415338307619095, R2 0.48394346237182617\n",
      "Eval loss 0.012417380698025227, R2 0.5216957330703735\n",
      "epoch 6195, loss 0.01241527684032917, R2 0.4839460253715515\n",
      "Eval loss 0.01241731084883213, R2 0.5216982364654541\n",
      "epoch 6196, loss 0.012415213510394096, R2 0.4839484691619873\n",
      "Eval loss 0.012417242862284184, R2 0.5217010378837585\n",
      "epoch 6197, loss 0.012415150180459023, R2 0.4839510917663574\n",
      "Eval loss 0.012417173013091087, R2 0.5217036008834839\n",
      "epoch 6198, loss 0.012415089644491673, R2 0.4839538335800171\n",
      "Eval loss 0.012417105957865715, R2 0.5217061638832092\n",
      "epoch 6199, loss 0.012415026314556599, R2 0.4839562773704529\n",
      "Eval loss 0.012417037971317768, R2 0.521708607673645\n",
      "epoch 6200, loss 0.012414964847266674, R2 0.4839591979980469\n",
      "Eval loss 0.012416969984769821, R2 0.5217111110687256\n",
      "epoch 6201, loss 0.0124149015173316, R2 0.4839622974395752\n",
      "Eval loss 0.0124169010668993, R2 0.5217139720916748\n",
      "epoch 6202, loss 0.01241484098136425, R2 0.48396414518356323\n",
      "Eval loss 0.012416832149028778, R2 0.5217165946960449\n",
      "epoch 6203, loss 0.012414776720106602, R2 0.48396700620651245\n",
      "Eval loss 0.012416765093803406, R2 0.5217189788818359\n",
      "epoch 6204, loss 0.012414715252816677, R2 0.48396921157836914\n",
      "Eval loss 0.012416696175932884, R2 0.5217219591140747\n",
      "epoch 6205, loss 0.012414653785526752, R2 0.48397159576416016\n",
      "Eval loss 0.012416629120707512, R2 0.5217245817184448\n",
      "epoch 6206, loss 0.012414590455591679, R2 0.4839745759963989\n",
      "Eval loss 0.012416561134159565, R2 0.5217273235321045\n",
      "epoch 6207, loss 0.012414529919624329, R2 0.4839770793914795\n",
      "Eval loss 0.012416492216289043, R2 0.5217295289039612\n",
      "epoch 6208, loss 0.01241446752101183, R2 0.48397964239120483\n",
      "Eval loss 0.012416424229741096, R2 0.5217323303222656\n",
      "epoch 6209, loss 0.01241440512239933, R2 0.4839820861816406\n",
      "Eval loss 0.012416355311870575, R2 0.5217348337173462\n",
      "epoch 6210, loss 0.012414343655109406, R2 0.48398464918136597\n",
      "Eval loss 0.012416288256645203, R2 0.5217375159263611\n",
      "epoch 6211, loss 0.012414282187819481, R2 0.48398739099502563\n",
      "Eval loss 0.012416219338774681, R2 0.5217404365539551\n",
      "epoch 6212, loss 0.012414218857884407, R2 0.4839898347854614\n",
      "Eval loss 0.012416153214871883, R2 0.5217428803443909\n",
      "epoch 6213, loss 0.012414158321917057, R2 0.48399221897125244\n",
      "Eval loss 0.012416085228323936, R2 0.5217454433441162\n",
      "epoch 6214, loss 0.012414095923304558, R2 0.4839954376220703\n",
      "Eval loss 0.012416016310453415, R2 0.5217481255531311\n",
      "epoch 6215, loss 0.012414034456014633, R2 0.4839974641799927\n",
      "Eval loss 0.012415948323905468, R2 0.5217504501342773\n",
      "epoch 6216, loss 0.012413973920047283, R2 0.48400020599365234\n",
      "Eval loss 0.01241588220000267, R2 0.5217530131340027\n",
      "epoch 6217, loss 0.012413911521434784, R2 0.48400312662124634\n",
      "Eval loss 0.012415813282132149, R2 0.5217558145523071\n",
      "epoch 6218, loss 0.01241385005414486, R2 0.4840052127838135\n",
      "Eval loss 0.012415745295584202, R2 0.5217586755752563\n",
      "epoch 6219, loss 0.012413788586854935, R2 0.48400789499282837\n",
      "Eval loss 0.01241567637771368, R2 0.521761417388916\n",
      "epoch 6220, loss 0.01241372711956501, R2 0.4840104579925537\n",
      "Eval loss 0.012415611185133457, R2 0.5217641592025757\n",
      "epoch 6221, loss 0.01241366472095251, R2 0.48401302099227905\n",
      "Eval loss 0.012415542267262936, R2 0.5217667818069458\n",
      "epoch 6222, loss 0.01241360418498516, R2 0.4840155839920044\n",
      "Eval loss 0.012415475212037563, R2 0.5217692852020264\n",
      "epoch 6223, loss 0.012413542717695236, R2 0.4840179681777954\n",
      "Eval loss 0.012415407225489616, R2 0.5217714309692383\n",
      "epoch 6224, loss 0.012413481250405312, R2 0.48402130603790283\n",
      "Eval loss 0.01241533923894167, R2 0.5217746496200562\n",
      "epoch 6225, loss 0.012413420714437962, R2 0.4840230345726013\n",
      "Eval loss 0.012415272183716297, R2 0.5217769145965576\n",
      "epoch 6226, loss 0.012413356453180313, R2 0.48402583599090576\n",
      "Eval loss 0.012415205128490925, R2 0.5217794179916382\n",
      "epoch 6227, loss 0.012413297779858112, R2 0.48402827978134155\n",
      "Eval loss 0.012415138073265553, R2 0.5217821002006531\n",
      "epoch 6228, loss 0.012413235381245613, R2 0.48403090238571167\n",
      "Eval loss 0.012415069155395031, R2 0.5217843055725098\n",
      "epoch 6229, loss 0.012413174845278263, R2 0.4840332269668579\n",
      "Eval loss 0.012415003962814808, R2 0.5217874050140381\n",
      "epoch 6230, loss 0.012413113377988338, R2 0.4840359687805176\n",
      "Eval loss 0.012414935976266861, R2 0.5217894315719604\n",
      "epoch 6231, loss 0.012413052842020988, R2 0.48403769731521606\n",
      "Eval loss 0.012414868921041489, R2 0.5217921733856201\n",
      "epoch 6232, loss 0.012412991374731064, R2 0.48404085636138916\n",
      "Eval loss 0.012414801865816116, R2 0.5217947959899902\n",
      "epoch 6233, loss 0.012412930838763714, R2 0.48404353857040405\n",
      "Eval loss 0.01241473387926817, R2 0.521797239780426\n",
      "epoch 6234, loss 0.01241286937147379, R2 0.48404639959335327\n",
      "Eval loss 0.012414667755365372, R2 0.5218002200126648\n",
      "epoch 6235, loss 0.012412807904183865, R2 0.48404866456985474\n",
      "Eval loss 0.012414601631462574, R2 0.5218026638031006\n",
      "epoch 6236, loss 0.01241274643689394, R2 0.48405104875564575\n",
      "Eval loss 0.012414533644914627, R2 0.521804928779602\n",
      "epoch 6237, loss 0.01241268776357174, R2 0.48405349254608154\n",
      "Eval loss 0.012414466589689255, R2 0.5218077898025513\n",
      "epoch 6238, loss 0.01241262536495924, R2 0.48405641317367554\n",
      "Eval loss 0.012414399534463882, R2 0.521809995174408\n",
      "epoch 6239, loss 0.012412563897669315, R2 0.4840586185455322\n",
      "Eval loss 0.012414333410561085, R2 0.5218127965927124\n",
      "epoch 6240, loss 0.012412505224347115, R2 0.484061062335968\n",
      "Eval loss 0.012414266355335712, R2 0.5218155384063721\n",
      "epoch 6241, loss 0.012412442825734615, R2 0.48406362533569336\n",
      "Eval loss 0.012414200231432915, R2 0.5218179225921631\n",
      "epoch 6242, loss 0.012412382289767265, R2 0.4840661883354187\n",
      "Eval loss 0.012414134107530117, R2 0.5218203663825989\n",
      "epoch 6243, loss 0.012412321753799915, R2 0.4840688705444336\n",
      "Eval loss 0.012414065189659595, R2 0.5218231678009033\n",
      "epoch 6244, loss 0.01241226214915514, R2 0.4840714931488037\n",
      "Eval loss 0.012414000928401947, R2 0.5218250751495361\n",
      "epoch 6245, loss 0.01241219975054264, R2 0.48407411575317383\n",
      "Eval loss 0.012413932941854, R2 0.5218284130096436\n",
      "epoch 6246, loss 0.01241214107722044, R2 0.48407673835754395\n",
      "Eval loss 0.012413866817951202, R2 0.5218309164047241\n",
      "epoch 6247, loss 0.012412079609930515, R2 0.4840787649154663\n",
      "Eval loss 0.012413800694048405, R2 0.5218335390090942\n",
      "epoch 6248, loss 0.012412019073963165, R2 0.4840811491012573\n",
      "Eval loss 0.012413734570145607, R2 0.5218359231948853\n",
      "epoch 6249, loss 0.012411958537995815, R2 0.4840841293334961\n",
      "Eval loss 0.012413667514920235, R2 0.5218387842178345\n",
      "epoch 6250, loss 0.01241189893335104, R2 0.484086275100708\n",
      "Eval loss 0.012413600459694862, R2 0.5218414664268494\n",
      "epoch 6251, loss 0.012411837466061115, R2 0.484088659286499\n",
      "Eval loss 0.012413534335792065, R2 0.5218440294265747\n",
      "epoch 6252, loss 0.01241177786141634, R2 0.4840918183326721\n",
      "Eval loss 0.012413467280566692, R2 0.5218462944030762\n",
      "epoch 6253, loss 0.01241171732544899, R2 0.48409366607666016\n",
      "Eval loss 0.01241340022534132, R2 0.5218491554260254\n",
      "epoch 6254, loss 0.012411657720804214, R2 0.4840964674949646\n",
      "Eval loss 0.012413335964083672, R2 0.5218510627746582\n",
      "epoch 6255, loss 0.01241159625351429, R2 0.48409950733184814\n",
      "Eval loss 0.0124132689088583, R2 0.5218536853790283\n",
      "epoch 6256, loss 0.012411537580192089, R2 0.4841018319129944\n",
      "Eval loss 0.012413202784955502, R2 0.5218564867973328\n",
      "epoch 6257, loss 0.01241147518157959, R2 0.4841040372848511\n",
      "Eval loss 0.012413137592375278, R2 0.5218587517738342\n",
      "epoch 6258, loss 0.012411417439579964, R2 0.48410648107528687\n",
      "Eval loss 0.012413070537149906, R2 0.5218616127967834\n",
      "epoch 6259, loss 0.012411355040967464, R2 0.48410922288894653\n",
      "Eval loss 0.012413003481924534, R2 0.5218641757965088\n",
      "epoch 6260, loss 0.012411295436322689, R2 0.4841111898422241\n",
      "Eval loss 0.01241293828934431, R2 0.5218668580055237\n",
      "epoch 6261, loss 0.012411235831677914, R2 0.48411381244659424\n",
      "Eval loss 0.012412873096764088, R2 0.5218689441680908\n",
      "epoch 6262, loss 0.012411176227033138, R2 0.4841163158416748\n",
      "Eval loss 0.012412807904183865, R2 0.5218715667724609\n",
      "epoch 6263, loss 0.012411116622388363, R2 0.4841192960739136\n",
      "Eval loss 0.012412740848958492, R2 0.5218740701675415\n",
      "epoch 6264, loss 0.012411056086421013, R2 0.48412126302719116\n",
      "Eval loss 0.01241267565637827, R2 0.5218770503997803\n",
      "epoch 6265, loss 0.012410996481776237, R2 0.4841243028640747\n",
      "Eval loss 0.012412609532475471, R2 0.5218793749809265\n",
      "epoch 6266, loss 0.012410936877131462, R2 0.4841267466545105\n",
      "Eval loss 0.012412544339895248, R2 0.5218816995620728\n",
      "epoch 6267, loss 0.012410877272486687, R2 0.48412925004959106\n",
      "Eval loss 0.012412477284669876, R2 0.5218844413757324\n",
      "epoch 6268, loss 0.012410816736519337, R2 0.48413175344467163\n",
      "Eval loss 0.012412412092089653, R2 0.5218867063522339\n",
      "epoch 6269, loss 0.012410757131874561, R2 0.48413407802581787\n",
      "Eval loss 0.012412345968186855, R2 0.5218896865844727\n",
      "epoch 6270, loss 0.01241069845855236, R2 0.4841366410255432\n",
      "Eval loss 0.012412279844284058, R2 0.5218920707702637\n",
      "epoch 6271, loss 0.012410638853907585, R2 0.4841386675834656\n",
      "Eval loss 0.012412216514348984, R2 0.5218948125839233\n",
      "epoch 6272, loss 0.012410578317940235, R2 0.48414117097854614\n",
      "Eval loss 0.012412149459123611, R2 0.5218968391418457\n",
      "epoch 6273, loss 0.012410519644618034, R2 0.48414379358291626\n",
      "Eval loss 0.012412084266543388, R2 0.5218997001647949\n",
      "epoch 6274, loss 0.012410459108650684, R2 0.4841459393501282\n",
      "Eval loss 0.01241201814264059, R2 0.5219024419784546\n",
      "epoch 6275, loss 0.012410399504005909, R2 0.48414891958236694\n",
      "Eval loss 0.012411953881382942, R2 0.5219047665596008\n",
      "epoch 6276, loss 0.012410339899361134, R2 0.4841512441635132\n",
      "Eval loss 0.012411887757480145, R2 0.5219069719314575\n",
      "epoch 6277, loss 0.012410280294716358, R2 0.48415374755859375\n",
      "Eval loss 0.012411821633577347, R2 0.5219097137451172\n",
      "epoch 6278, loss 0.012410221621394157, R2 0.48415595293045044\n",
      "Eval loss 0.012411758303642273, R2 0.521912157535553\n",
      "epoch 6279, loss 0.012410162016749382, R2 0.48415863513946533\n",
      "Eval loss 0.012411692179739475, R2 0.5219147205352783\n",
      "epoch 6280, loss 0.012410103343427181, R2 0.4841609001159668\n",
      "Eval loss 0.012411626987159252, R2 0.5219172239303589\n",
      "epoch 6281, loss 0.012410043738782406, R2 0.48416388034820557\n",
      "Eval loss 0.012411561794579029, R2 0.5219194293022156\n",
      "epoch 6282, loss 0.012409983202815056, R2 0.48416590690612793\n",
      "Eval loss 0.01241149753332138, R2 0.5219223499298096\n",
      "epoch 6283, loss 0.01240992546081543, R2 0.48416846990585327\n",
      "Eval loss 0.012411431409418583, R2 0.5219250321388245\n",
      "epoch 6284, loss 0.012409866787493229, R2 0.4841712713241577\n",
      "Eval loss 0.012411367148160934, R2 0.5219269394874573\n",
      "epoch 6285, loss 0.012409807182848454, R2 0.4841732382774353\n",
      "Eval loss 0.012411301024258137, R2 0.5219297409057617\n",
      "epoch 6286, loss 0.012409748509526253, R2 0.4841758608818054\n",
      "Eval loss 0.012411236763000488, R2 0.5219322443008423\n",
      "epoch 6287, loss 0.012409687973558903, R2 0.484178364276886\n",
      "Eval loss 0.012411171570420265, R2 0.5219347476959229\n",
      "epoch 6288, loss 0.012409630231559277, R2 0.4841805696487427\n",
      "Eval loss 0.012411106377840042, R2 0.5219370126724243\n",
      "epoch 6289, loss 0.012409571558237076, R2 0.48418283462524414\n",
      "Eval loss 0.012411040253937244, R2 0.5219398140907288\n",
      "epoch 6290, loss 0.012409512884914875, R2 0.4841856360435486\n",
      "Eval loss 0.01241097692400217, R2 0.5219422578811646\n",
      "epoch 6291, loss 0.0124094532802701, R2 0.4841880798339844\n",
      "Eval loss 0.012410911731421947, R2 0.521945059299469\n",
      "epoch 6292, loss 0.012409394606947899, R2 0.48419052362442017\n",
      "Eval loss 0.012410847470164299, R2 0.5219472646713257\n",
      "epoch 6293, loss 0.012409335002303123, R2 0.4841928482055664\n",
      "Eval loss 0.01241078320890665, R2 0.5219498872756958\n",
      "epoch 6294, loss 0.012409276328980923, R2 0.4841952919960022\n",
      "Eval loss 0.012410719878971577, R2 0.5219523906707764\n",
      "epoch 6295, loss 0.012409217655658722, R2 0.48419785499572754\n",
      "Eval loss 0.012410653755068779, R2 0.5219549536705017\n",
      "epoch 6296, loss 0.012409158982336521, R2 0.4842003583908081\n",
      "Eval loss 0.01241058949381113, R2 0.5219571590423584\n",
      "epoch 6297, loss 0.01240910030901432, R2 0.48420292139053345\n",
      "Eval loss 0.012410524301230907, R2 0.5219595432281494\n",
      "epoch 6298, loss 0.012409043498337269, R2 0.48420512676239014\n",
      "Eval loss 0.012410459108650684, R2 0.5219621658325195\n",
      "epoch 6299, loss 0.012408982962369919, R2 0.4842074513435364\n",
      "Eval loss 0.012410394847393036, R2 0.521964967250824\n",
      "epoch 6300, loss 0.012408924289047718, R2 0.4842106103897095\n",
      "Eval loss 0.012410329654812813, R2 0.5219671726226807\n",
      "epoch 6301, loss 0.012408864684402943, R2 0.48421239852905273\n",
      "Eval loss 0.012410266324877739, R2 0.5219697952270508\n",
      "epoch 6302, loss 0.012408808805048466, R2 0.484214723110199\n",
      "Eval loss 0.01241020206362009, R2 0.5219722390174866\n",
      "epoch 6303, loss 0.012408748269081116, R2 0.48421722650527954\n",
      "Eval loss 0.012410136871039867, R2 0.5219745635986328\n",
      "epoch 6304, loss 0.01240869052708149, R2 0.4842197895050049\n",
      "Eval loss 0.012410072609782219, R2 0.5219769477844238\n",
      "epoch 6305, loss 0.012408631853759289, R2 0.48422205448150635\n",
      "Eval loss 0.012410009279847145, R2 0.5219793915748596\n",
      "epoch 6306, loss 0.012408573180437088, R2 0.48422449827194214\n",
      "Eval loss 0.012409944087266922, R2 0.5219817161560059\n",
      "epoch 6307, loss 0.012408516369760036, R2 0.48422688245773315\n",
      "Eval loss 0.012409880757331848, R2 0.5219844579696655\n",
      "epoch 6308, loss 0.012408457696437836, R2 0.48422980308532715\n",
      "Eval loss 0.0124098164960742, R2 0.5219866633415222\n",
      "epoch 6309, loss 0.01240839995443821, R2 0.48423224687576294\n",
      "Eval loss 0.012409752234816551, R2 0.5219894647598267\n",
      "epoch 6310, loss 0.012408340349793434, R2 0.4842343330383301\n",
      "Eval loss 0.012409687973558903, R2 0.521992027759552\n",
      "epoch 6311, loss 0.012408282607793808, R2 0.48423659801483154\n",
      "Eval loss 0.012409623712301254, R2 0.5219943523406982\n",
      "epoch 6312, loss 0.012408223934471607, R2 0.4842391610145569\n",
      "Eval loss 0.01240956038236618, R2 0.5219969749450684\n",
      "epoch 6313, loss 0.012408167123794556, R2 0.48424142599105835\n",
      "Eval loss 0.012409496121108532, R2 0.5219991207122803\n",
      "epoch 6314, loss 0.01240810751914978, R2 0.48424404859542847\n",
      "Eval loss 0.012409432791173458, R2 0.5220016241073608\n",
      "epoch 6315, loss 0.012408050708472729, R2 0.48424625396728516\n",
      "Eval loss 0.01240936852991581, R2 0.522004246711731\n",
      "epoch 6316, loss 0.012407992966473103, R2 0.4842487573623657\n",
      "Eval loss 0.012409305199980736, R2 0.522006630897522\n",
      "epoch 6317, loss 0.012407933361828327, R2 0.48425161838531494\n",
      "Eval loss 0.012409240938723087, R2 0.522009551525116\n",
      "epoch 6318, loss 0.012407875619828701, R2 0.484253466129303\n",
      "Eval loss 0.012409175746142864, R2 0.5220117568969727\n",
      "epoch 6319, loss 0.0124078169465065, R2 0.48425596952438354\n",
      "Eval loss 0.01240911427885294, R2 0.5220141410827637\n",
      "epoch 6320, loss 0.012407761067152023, R2 0.4842584729194641\n",
      "Eval loss 0.012409049086272717, R2 0.5220165252685547\n",
      "epoch 6321, loss 0.012407702393829823, R2 0.4842603802680969\n",
      "Eval loss 0.012408985756337643, R2 0.52201908826828\n",
      "epoch 6322, loss 0.012407644651830196, R2 0.4842631220817566\n",
      "Eval loss 0.012408922426402569, R2 0.5220215320587158\n",
      "epoch 6323, loss 0.01240758690983057, R2 0.48426568508148193\n",
      "Eval loss 0.012408857233822346, R2 0.522023618221283\n",
      "epoch 6324, loss 0.012407529167830944, R2 0.48426806926727295\n",
      "Eval loss 0.012408795766532421, R2 0.5220264196395874\n",
      "epoch 6325, loss 0.012407470494508743, R2 0.4842708110809326\n",
      "Eval loss 0.012408730573952198, R2 0.5220285058021545\n",
      "epoch 6326, loss 0.012407413683831692, R2 0.48427289724349976\n",
      "Eval loss 0.012408668175339699, R2 0.5220311880111694\n",
      "epoch 6327, loss 0.012407355941832066, R2 0.48427510261535645\n",
      "Eval loss 0.0124086057767272, R2 0.5220333337783813\n",
      "epoch 6328, loss 0.01240729819983244, R2 0.48427814245224\n",
      "Eval loss 0.012408540584146976, R2 0.522036075592041\n",
      "epoch 6329, loss 0.012407240457832813, R2 0.4842800498008728\n",
      "Eval loss 0.012408478185534477, R2 0.5220385193824768\n",
      "epoch 6330, loss 0.012407182715833187, R2 0.4842824935913086\n",
      "Eval loss 0.012408414855599403, R2 0.5220412015914917\n",
      "epoch 6331, loss 0.012407127767801285, R2 0.48428475856781006\n",
      "Eval loss 0.01240835152566433, R2 0.5220432281494141\n",
      "epoch 6332, loss 0.01240706816315651, R2 0.48428773880004883\n",
      "Eval loss 0.012408286333084106, R2 0.5220454931259155\n",
      "epoch 6333, loss 0.012407012283802032, R2 0.48429006338119507\n",
      "Eval loss 0.012408224865794182, R2 0.5220483541488647\n",
      "epoch 6334, loss 0.012406953610479832, R2 0.484291672706604\n",
      "Eval loss 0.012408161535859108, R2 0.5220510959625244\n",
      "epoch 6335, loss 0.01240689679980278, R2 0.48429417610168457\n",
      "Eval loss 0.012408098205924034, R2 0.5220530033111572\n",
      "epoch 6336, loss 0.012406839989125729, R2 0.4842965602874756\n",
      "Eval loss 0.01240803487598896, R2 0.5220556855201721\n",
      "epoch 6337, loss 0.012406781315803528, R2 0.4842991828918457\n",
      "Eval loss 0.012407972477376461, R2 0.5220581293106079\n",
      "epoch 6338, loss 0.012406723573803902, R2 0.4843013882637024\n",
      "Eval loss 0.012407909147441387, R2 0.5220606923103333\n",
      "epoch 6339, loss 0.012406667694449425, R2 0.48430371284484863\n",
      "Eval loss 0.012407846748828888, R2 0.5220626592636108\n",
      "epoch 6340, loss 0.012406609952449799, R2 0.484306275844574\n",
      "Eval loss 0.012407783418893814, R2 0.522065281867981\n",
      "epoch 6341, loss 0.012406552210450172, R2 0.484308660030365\n",
      "Eval loss 0.012407721020281315, R2 0.5220674276351929\n",
      "epoch 6342, loss 0.012406496331095695, R2 0.484311044216156\n",
      "Eval loss 0.012407657690346241, R2 0.5220704078674316\n",
      "epoch 6343, loss 0.01240643858909607, R2 0.4843132495880127\n",
      "Eval loss 0.012407594360411167, R2 0.5220725536346436\n",
      "epoch 6344, loss 0.012406381778419018, R2 0.4843154549598694\n",
      "Eval loss 0.012407533824443817, R2 0.5220745801925659\n",
      "epoch 6345, loss 0.012406324036419392, R2 0.48431795835494995\n",
      "Eval loss 0.012407469563186169, R2 0.5220777988433838\n",
      "epoch 6346, loss 0.01240626722574234, R2 0.48432034254074097\n",
      "Eval loss 0.01240740716457367, R2 0.5220800638198853\n",
      "epoch 6347, loss 0.012406209483742714, R2 0.484322726726532\n",
      "Eval loss 0.012407342903316021, R2 0.5220822095870972\n",
      "epoch 6348, loss 0.012406153604388237, R2 0.48432523012161255\n",
      "Eval loss 0.012407281436026096, R2 0.5220845937728882\n",
      "epoch 6349, loss 0.01240609586238861, R2 0.484327495098114\n",
      "Eval loss 0.012407219037413597, R2 0.5220867395401001\n",
      "epoch 6350, loss 0.012406039983034134, R2 0.4843299388885498\n",
      "Eval loss 0.012407154776155949, R2 0.5220898985862732\n",
      "epoch 6351, loss 0.012405983172357082, R2 0.48433202505111694\n",
      "Eval loss 0.01240709237754345, R2 0.522092342376709\n",
      "epoch 6352, loss 0.012405927293002605, R2 0.48433446884155273\n",
      "Eval loss 0.01240702997893095, R2 0.5220942497253418\n",
      "epoch 6353, loss 0.01240586955100298, R2 0.4843369126319885\n",
      "Eval loss 0.0124069694429636, R2 0.5220967531204224\n",
      "epoch 6354, loss 0.012405813671648502, R2 0.48433905839920044\n",
      "Eval loss 0.012406906113028526, R2 0.5220990180969238\n",
      "epoch 6355, loss 0.01240575686097145, R2 0.4843420386314392\n",
      "Eval loss 0.012406842783093452, R2 0.522101640701294\n",
      "epoch 6356, loss 0.0124057000502944, R2 0.484343945980072\n",
      "Eval loss 0.012406782247126102, R2 0.5221041440963745\n",
      "epoch 6357, loss 0.012405643239617348, R2 0.48434627056121826\n",
      "Eval loss 0.012406718917191029, R2 0.522106409072876\n",
      "epoch 6358, loss 0.01240558736026287, R2 0.4843486547470093\n",
      "Eval loss 0.012406657449901104, R2 0.5221083164215088\n",
      "epoch 6359, loss 0.01240553054958582, R2 0.4843512773513794\n",
      "Eval loss 0.01240659411996603, R2 0.5221110582351685\n",
      "epoch 6360, loss 0.012405473738908768, R2 0.48435336351394653\n",
      "Eval loss 0.012406532652676105, R2 0.5221134424209595\n",
      "epoch 6361, loss 0.01240541785955429, R2 0.4843556880950928\n",
      "Eval loss 0.012406469322741032, R2 0.5221155881881714\n",
      "epoch 6362, loss 0.012405360117554665, R2 0.4843580722808838\n",
      "Eval loss 0.012406409718096256, R2 0.5221182107925415\n",
      "epoch 6363, loss 0.012405305169522762, R2 0.48436033725738525\n",
      "Eval loss 0.012406345456838608, R2 0.5221203565597534\n",
      "epoch 6364, loss 0.01240524835884571, R2 0.4843628406524658\n",
      "Eval loss 0.012406283989548683, R2 0.5221229791641235\n",
      "epoch 6365, loss 0.01240519154816866, R2 0.48436522483825684\n",
      "Eval loss 0.012406222522258759, R2 0.5221251249313354\n",
      "epoch 6366, loss 0.012405134737491608, R2 0.48436760902404785\n",
      "Eval loss 0.01240616012364626, R2 0.522127628326416\n",
      "epoch 6367, loss 0.012405079789459705, R2 0.484369695186615\n",
      "Eval loss 0.01240609772503376, R2 0.5221304893493652\n",
      "epoch 6368, loss 0.012405023910105228, R2 0.4843723773956299\n",
      "Eval loss 0.012406036257743835, R2 0.5221326947212219\n",
      "epoch 6369, loss 0.012404968030750751, R2 0.4843745231628418\n",
      "Eval loss 0.012405975721776485, R2 0.5221347808837891\n",
      "epoch 6370, loss 0.012404910288751125, R2 0.48437678813934326\n",
      "Eval loss 0.012405912391841412, R2 0.5221370458602905\n",
      "epoch 6371, loss 0.012404854409396648, R2 0.4843785762786865\n",
      "Eval loss 0.012405850924551487, R2 0.5221396684646606\n",
      "epoch 6372, loss 0.012404798530042171, R2 0.4843815565109253\n",
      "Eval loss 0.012405788525938988, R2 0.522142231464386\n",
      "epoch 6373, loss 0.01240474358201027, R2 0.4843841791152954\n",
      "Eval loss 0.012405727058649063, R2 0.5221444368362427\n",
      "epoch 6374, loss 0.012404685840010643, R2 0.48438626527786255\n",
      "Eval loss 0.012405665591359138, R2 0.5221465826034546\n",
      "epoch 6375, loss 0.01240463089197874, R2 0.48438841104507446\n",
      "Eval loss 0.01240560319274664, R2 0.5221492052078247\n",
      "epoch 6376, loss 0.012404575012624264, R2 0.48439133167266846\n",
      "Eval loss 0.01240554265677929, R2 0.5221517086029053\n",
      "epoch 6377, loss 0.012404519133269787, R2 0.4843928813934326\n",
      "Eval loss 0.012405479326844215, R2 0.5221539735794067\n",
      "epoch 6378, loss 0.012404464185237885, R2 0.4843953251838684\n",
      "Eval loss 0.01240541972219944, R2 0.5221563577651978\n",
      "epoch 6379, loss 0.012404407374560833, R2 0.4843983054161072\n",
      "Eval loss 0.012405358254909515, R2 0.5221590995788574\n",
      "epoch 6380, loss 0.012404350563883781, R2 0.48440003395080566\n",
      "Eval loss 0.01240529678761959, R2 0.5221607685089111\n",
      "epoch 6381, loss 0.012404295615851879, R2 0.48440200090408325\n",
      "Eval loss 0.012405234389007092, R2 0.522163450717926\n",
      "epoch 6382, loss 0.012404238805174828, R2 0.48440468311309814\n",
      "Eval loss 0.012405172921717167, R2 0.5221654176712036\n",
      "epoch 6383, loss 0.0124041847884655, R2 0.4844074249267578\n",
      "Eval loss 0.012405113317072392, R2 0.5221682786941528\n",
      "epoch 6384, loss 0.012404128909111023, R2 0.4844092130661011\n",
      "Eval loss 0.012405049987137318, R2 0.5221707820892334\n",
      "epoch 6385, loss 0.012404073029756546, R2 0.48441141843795776\n",
      "Eval loss 0.012404990382492542, R2 0.5221731662750244\n",
      "epoch 6386, loss 0.012404017150402069, R2 0.4844142198562622\n",
      "Eval loss 0.012404927983880043, R2 0.5221753716468811\n",
      "epoch 6387, loss 0.012403963133692741, R2 0.48441630601882935\n",
      "Eval loss 0.012404867447912693, R2 0.5221774578094482\n",
      "epoch 6388, loss 0.01240390632301569, R2 0.48441851139068604\n",
      "Eval loss 0.012404805980622768, R2 0.5221799612045288\n",
      "epoch 6389, loss 0.012403850443661213, R2 0.4844208359718323\n",
      "Eval loss 0.012404744513332844, R2 0.5221824645996094\n",
      "epoch 6390, loss 0.012403796426951885, R2 0.4844233989715576\n",
      "Eval loss 0.01240468304604292, R2 0.5221843719482422\n",
      "epoch 6391, loss 0.012403738684952259, R2 0.4844256043434143\n",
      "Eval loss 0.01240462251007557, R2 0.5221871733665466\n",
      "epoch 6392, loss 0.012403685599565506, R2 0.48442769050598145\n",
      "Eval loss 0.01240456011146307, R2 0.5221893787384033\n",
      "epoch 6393, loss 0.012403629720211029, R2 0.4844304919242859\n",
      "Eval loss 0.012404500506818295, R2 0.5221917033195496\n",
      "epoch 6394, loss 0.012403574772179127, R2 0.48443228006362915\n",
      "Eval loss 0.01240443903952837, R2 0.5221937894821167\n",
      "epoch 6395, loss 0.012403517961502075, R2 0.48443466424942017\n",
      "Eval loss 0.01240437850356102, R2 0.5221962928771973\n",
      "epoch 6396, loss 0.012403463013470173, R2 0.4844372272491455\n",
      "Eval loss 0.012404318898916245, R2 0.5221985578536987\n",
      "epoch 6397, loss 0.012403407134115696, R2 0.4844397306442261\n",
      "Eval loss 0.012404256500303745, R2 0.5222010612487793\n",
      "epoch 6398, loss 0.012403354048728943, R2 0.48444193601608276\n",
      "Eval loss 0.012404195964336395, R2 0.5222037434577942\n",
      "epoch 6399, loss 0.012403297238051891, R2 0.48444443941116333\n",
      "Eval loss 0.012404135428369045, R2 0.5222060680389404\n",
      "epoch 6400, loss 0.012403243221342564, R2 0.48444604873657227\n",
      "Eval loss 0.01240407396107912, R2 0.5222084522247314\n",
      "epoch 6401, loss 0.012403187341988087, R2 0.4844483733177185\n",
      "Eval loss 0.012404014356434345, R2 0.5222105979919434\n",
      "epoch 6402, loss 0.012403133325278759, R2 0.4844508171081543\n",
      "Eval loss 0.01240395288914442, R2 0.5222128629684448\n",
      "epoch 6403, loss 0.012403078377246857, R2 0.48445290327072144\n",
      "Eval loss 0.012403893284499645, R2 0.5222148299217224\n",
      "epoch 6404, loss 0.01240302249789238, R2 0.4844552278518677\n",
      "Eval loss 0.01240383181720972, R2 0.5222175121307373\n",
      "epoch 6405, loss 0.012402968481183052, R2 0.48445749282836914\n",
      "Eval loss 0.01240377128124237, R2 0.5222196578979492\n",
      "epoch 6406, loss 0.012402912601828575, R2 0.4844599962234497\n",
      "Eval loss 0.012403711676597595, R2 0.5222223997116089\n",
      "epoch 6407, loss 0.012402857653796673, R2 0.48446208238601685\n",
      "Eval loss 0.01240365020930767, R2 0.5222244262695312\n",
      "epoch 6408, loss 0.012402801774442196, R2 0.4844648838043213\n",
      "Eval loss 0.012403590604662895, R2 0.5222270488739014\n",
      "epoch 6409, loss 0.012402747757732868, R2 0.4844664931297302\n",
      "Eval loss 0.01240352913737297, R2 0.5222291350364685\n",
      "epoch 6410, loss 0.01240269374102354, R2 0.4844687581062317\n",
      "Eval loss 0.01240347046405077, R2 0.5222312808036804\n",
      "epoch 6411, loss 0.012402638792991638, R2 0.4844713807106018\n",
      "Eval loss 0.012403408996760845, R2 0.5222339034080505\n",
      "epoch 6412, loss 0.012402583844959736, R2 0.48447364568710327\n",
      "Eval loss 0.012403348460793495, R2 0.5222362279891968\n",
      "epoch 6413, loss 0.012402528896927834, R2 0.4844757318496704\n",
      "Eval loss 0.01240328885614872, R2 0.5222384929656982\n",
      "epoch 6414, loss 0.012402473948895931, R2 0.4844781756401062\n",
      "Eval loss 0.01240322832018137, R2 0.5222408771514893\n",
      "epoch 6415, loss 0.012402419932186604, R2 0.48448044061660767\n",
      "Eval loss 0.01240316778421402, R2 0.5222430229187012\n",
      "epoch 6416, loss 0.01240236684679985, R2 0.48448246717453003\n",
      "Eval loss 0.01240310724824667, R2 0.5222454071044922\n",
      "epoch 6417, loss 0.012402310036122799, R2 0.48448532819747925\n",
      "Eval loss 0.012403048574924469, R2 0.5222473740577698\n",
      "epoch 6418, loss 0.012402256019413471, R2 0.4844871163368225\n",
      "Eval loss 0.012402987107634544, R2 0.5222502946853638\n",
      "epoch 6419, loss 0.012402202934026718, R2 0.4844895005226135\n",
      "Eval loss 0.012402927502989769, R2 0.5222523212432861\n",
      "epoch 6420, loss 0.012402147054672241, R2 0.484491765499115\n",
      "Eval loss 0.012402866035699844, R2 0.5222543478012085\n",
      "epoch 6421, loss 0.012402092106640339, R2 0.4844939112663269\n",
      "Eval loss 0.012402807362377644, R2 0.5222570896148682\n",
      "epoch 6422, loss 0.012402039021253586, R2 0.4844961166381836\n",
      "Eval loss 0.012402746826410294, R2 0.5222592353820801\n",
      "epoch 6423, loss 0.012401985004544258, R2 0.4844983220100403\n",
      "Eval loss 0.012402687221765518, R2 0.522261381149292\n",
      "epoch 6424, loss 0.012401930056512356, R2 0.4845006465911865\n",
      "Eval loss 0.012402626685798168, R2 0.5222641229629517\n",
      "epoch 6425, loss 0.012401875108480453, R2 0.484502911567688\n",
      "Eval loss 0.012402568012475967, R2 0.522266149520874\n",
      "epoch 6426, loss 0.012401821091771126, R2 0.48450517654418945\n",
      "Eval loss 0.012402508407831192, R2 0.5222684144973755\n",
      "epoch 6427, loss 0.012401767075061798, R2 0.4845074415206909\n",
      "Eval loss 0.012402447871863842, R2 0.5222705006599426\n",
      "epoch 6428, loss 0.012401712127029896, R2 0.4845097064971924\n",
      "Eval loss 0.012402387335896492, R2 0.5222732424736023\n",
      "epoch 6429, loss 0.012401659972965717, R2 0.4845118522644043\n",
      "Eval loss 0.012402328662574291, R2 0.5222755074501038\n",
      "epoch 6430, loss 0.01240160595625639, R2 0.48451411724090576\n",
      "Eval loss 0.01240226998925209, R2 0.5222777724266052\n",
      "epoch 6431, loss 0.012401550076901913, R2 0.48451608419418335\n",
      "Eval loss 0.012402210384607315, R2 0.522280216217041\n",
      "epoch 6432, loss 0.01240149699151516, R2 0.48451846837997437\n",
      "Eval loss 0.012402149848639965, R2 0.5222820043563843\n",
      "epoch 6433, loss 0.012401442974805832, R2 0.48452091217041016\n",
      "Eval loss 0.01240209024399519, R2 0.5222841501235962\n",
      "epoch 6434, loss 0.012401388958096504, R2 0.48452329635620117\n",
      "Eval loss 0.012402030639350414, R2 0.5222870111465454\n",
      "epoch 6435, loss 0.012401335872709751, R2 0.484525203704834\n",
      "Eval loss 0.012401971966028214, R2 0.5222891569137573\n",
      "epoch 6436, loss 0.012401280924677849, R2 0.48452794551849365\n",
      "Eval loss 0.012401911430060863, R2 0.5222915410995483\n",
      "epoch 6437, loss 0.012401226907968521, R2 0.48453032970428467\n",
      "Eval loss 0.012401853688061237, R2 0.5222938060760498\n",
      "epoch 6438, loss 0.012401172891259193, R2 0.48453277349472046\n",
      "Eval loss 0.012401792220771313, R2 0.5222957134246826\n",
      "epoch 6439, loss 0.012401118874549866, R2 0.48453450202941895\n",
      "Eval loss 0.012401734478771687, R2 0.522298276424408\n",
      "epoch 6440, loss 0.012401065789163113, R2 0.48453670740127563\n",
      "Eval loss 0.012401673942804337, R2 0.5223004817962646\n",
      "epoch 6441, loss 0.01240101270377636, R2 0.4845387935638428\n",
      "Eval loss 0.012401615269482136, R2 0.5223028659820557\n",
      "epoch 6442, loss 0.012400957755744457, R2 0.48454105854034424\n",
      "Eval loss 0.01240155566483736, R2 0.5223053693771362\n",
      "epoch 6443, loss 0.01240090373903513, R2 0.4845433235168457\n",
      "Eval loss 0.012401497922837734, R2 0.5223075151443481\n",
      "epoch 6444, loss 0.012400850653648376, R2 0.4845457077026367\n",
      "Eval loss 0.012401435524225235, R2 0.5223097801208496\n",
      "epoch 6445, loss 0.012400796636939049, R2 0.4845477342605591\n",
      "Eval loss 0.012401378713548183, R2 0.5223122835159302\n",
      "epoch 6446, loss 0.01240074448287487, R2 0.48455023765563965\n",
      "Eval loss 0.012401319108903408, R2 0.5223145484924316\n",
      "epoch 6447, loss 0.012400690466165543, R2 0.48455220460891724\n",
      "Eval loss 0.012401260435581207, R2 0.5223162174224854\n",
      "epoch 6448, loss 0.01240063551813364, R2 0.4845544695854187\n",
      "Eval loss 0.012401200830936432, R2 0.522318959236145\n",
      "epoch 6449, loss 0.012400581501424313, R2 0.4845566749572754\n",
      "Eval loss 0.012401143088936806, R2 0.5223208665847778\n",
      "epoch 6450, loss 0.012400529347360134, R2 0.4845593571662903\n",
      "Eval loss 0.012401082552969456, R2 0.522323489189148\n",
      "epoch 6451, loss 0.012400475330650806, R2 0.4845612645149231\n",
      "Eval loss 0.01240102481096983, R2 0.5223257541656494\n",
      "epoch 6452, loss 0.012400421313941479, R2 0.48456335067749023\n",
      "Eval loss 0.012400966137647629, R2 0.5223278999328613\n",
      "epoch 6453, loss 0.012400370091199875, R2 0.48456549644470215\n",
      "Eval loss 0.012400905601680279, R2 0.5223301649093628\n",
      "epoch 6454, loss 0.012400316074490547, R2 0.48456770181655884\n",
      "Eval loss 0.012400848791003227, R2 0.5223324298858643\n",
      "epoch 6455, loss 0.01240026205778122, R2 0.4845699667930603\n",
      "Eval loss 0.012400787323713303, R2 0.5223344564437866\n",
      "epoch 6456, loss 0.012400208972394466, R2 0.4845723509788513\n",
      "Eval loss 0.012400731444358826, R2 0.5223367810249329\n",
      "epoch 6457, loss 0.012400155887007713, R2 0.484574556350708\n",
      "Eval loss 0.012400672771036625, R2 0.5223391056060791\n",
      "epoch 6458, loss 0.012400103732943535, R2 0.4845770597457886\n",
      "Eval loss 0.01240061316639185, R2 0.5223411917686462\n",
      "epoch 6459, loss 0.012400048784911633, R2 0.48457884788513184\n",
      "Eval loss 0.012400554493069649, R2 0.5223433971405029\n",
      "epoch 6460, loss 0.012399996630847454, R2 0.4845806956291199\n",
      "Eval loss 0.012400494888424873, R2 0.522346019744873\n",
      "epoch 6461, loss 0.012399943545460701, R2 0.4845837354660034\n",
      "Eval loss 0.012400437146425247, R2 0.5223485231399536\n",
      "epoch 6462, loss 0.012399890460073948, R2 0.48458540439605713\n",
      "Eval loss 0.012400379404425621, R2 0.5223501920700073\n",
      "epoch 6463, loss 0.01239983644336462, R2 0.4845876693725586\n",
      "Eval loss 0.012400318868458271, R2 0.5223528146743774\n",
      "epoch 6464, loss 0.012399785220623016, R2 0.4845898151397705\n",
      "Eval loss 0.01240026205778122, R2 0.5223548412322998\n",
      "epoch 6465, loss 0.012399730272591114, R2 0.4845921993255615\n",
      "Eval loss 0.012400203384459019, R2 0.5223572254180908\n",
      "epoch 6466, loss 0.01239967904984951, R2 0.4845942258834839\n",
      "Eval loss 0.012400144711136818, R2 0.5223593711853027\n",
      "epoch 6467, loss 0.012399625964462757, R2 0.4845966100692749\n",
      "Eval loss 0.012400086037814617, R2 0.5223617553710938\n",
      "epoch 6468, loss 0.012399572879076004, R2 0.484599232673645\n",
      "Eval loss 0.012400028295814991, R2 0.5223641395568848\n",
      "epoch 6469, loss 0.012399520725011826, R2 0.4846014380455017\n",
      "Eval loss 0.012399968691170216, R2 0.5223661065101624\n",
      "epoch 6470, loss 0.012399466708302498, R2 0.48460304737091064\n",
      "Eval loss 0.012399910017848015, R2 0.5223685503005981\n",
      "epoch 6471, loss 0.012399413622915745, R2 0.4846058487892151\n",
      "Eval loss 0.012399854138493538, R2 0.522370457649231\n",
      "epoch 6472, loss 0.012399360537528992, R2 0.4846079349517822\n",
      "Eval loss 0.012399794533848763, R2 0.5223729610443115\n",
      "epoch 6473, loss 0.012399309314787388, R2 0.48461025953292847\n",
      "Eval loss 0.012399737723171711, R2 0.522375226020813\n",
      "epoch 6474, loss 0.01239925529807806, R2 0.48461228609085083\n",
      "Eval loss 0.01239967904984951, R2 0.5223777294158936\n",
      "epoch 6475, loss 0.012399202212691307, R2 0.4846140146255493\n",
      "Eval loss 0.012399621307849884, R2 0.5223796367645264\n",
      "epoch 6476, loss 0.012399151921272278, R2 0.48461663722991943\n",
      "Eval loss 0.012399562634527683, R2 0.5223820209503174\n",
      "epoch 6477, loss 0.01239909790456295, R2 0.48461854457855225\n",
      "Eval loss 0.012399503961205482, R2 0.5223840475082397\n",
      "epoch 6478, loss 0.012399044819176197, R2 0.4846205711364746\n",
      "Eval loss 0.012399446219205856, R2 0.5223862528800964\n",
      "epoch 6479, loss 0.012398992665112019, R2 0.48462337255477905\n",
      "Eval loss 0.012399387545883656, R2 0.522388219833374\n",
      "epoch 6480, loss 0.01239894051104784, R2 0.4846249222755432\n",
      "Eval loss 0.012399330735206604, R2 0.5223908424377441\n",
      "epoch 6481, loss 0.012398888356983662, R2 0.4846270680427551\n",
      "Eval loss 0.012399272061884403, R2 0.5223931074142456\n",
      "epoch 6482, loss 0.012398835271596909, R2 0.48462945222854614\n",
      "Eval loss 0.012399215251207352, R2 0.5223950147628784\n",
      "epoch 6483, loss 0.01239878311753273, R2 0.4846314787864685\n",
      "Eval loss 0.012399156577885151, R2 0.5223973989486694\n",
      "epoch 6484, loss 0.012398730032145977, R2 0.48463380336761475\n",
      "Eval loss 0.012399100698530674, R2 0.5223994255065918\n",
      "epoch 6485, loss 0.012398677878081799, R2 0.48463648557662964\n",
      "Eval loss 0.012399042025208473, R2 0.522402286529541\n",
      "epoch 6486, loss 0.012398626655340195, R2 0.48463815450668335\n",
      "Eval loss 0.012398984283208847, R2 0.5224044919013977\n",
      "epoch 6487, loss 0.012398574501276016, R2 0.48464012145996094\n",
      "Eval loss 0.012398925609886646, R2 0.5224061608314514\n",
      "epoch 6488, loss 0.012398522347211838, R2 0.48464226722717285\n",
      "Eval loss 0.012398868799209595, R2 0.522408664226532\n",
      "epoch 6489, loss 0.01239847019314766, R2 0.48464447259902954\n",
      "Eval loss 0.012398811057209969, R2 0.5224107503890991\n",
      "epoch 6490, loss 0.012398417107760906, R2 0.48464715480804443\n",
      "Eval loss 0.012398754246532917, R2 0.5224127769470215\n",
      "epoch 6491, loss 0.012398364953696728, R2 0.48464900255203247\n",
      "Eval loss 0.012398695573210716, R2 0.522415041923523\n",
      "epoch 6492, loss 0.012398313730955124, R2 0.48465096950531006\n",
      "Eval loss 0.012398638762533665, R2 0.5224176645278931\n",
      "epoch 6493, loss 0.012398261576890945, R2 0.484653115272522\n",
      "Eval loss 0.012398581951856613, R2 0.5224195718765259\n",
      "epoch 6494, loss 0.012398208491504192, R2 0.48465579748153687\n",
      "Eval loss 0.012398523278534412, R2 0.5224219560623169\n",
      "epoch 6495, loss 0.012398157268762589, R2 0.4846569895744324\n",
      "Eval loss 0.012398465536534786, R2 0.5224239230155945\n",
      "epoch 6496, loss 0.012398106046020985, R2 0.4846596121788025\n",
      "Eval loss 0.01239840779453516, R2 0.5224262475967407\n",
      "epoch 6497, loss 0.012398052960634232, R2 0.48466211557388306\n",
      "Eval loss 0.012398350983858109, R2 0.5224281549453735\n",
      "epoch 6498, loss 0.012398001737892628, R2 0.4846644401550293\n",
      "Eval loss 0.012398293241858482, R2 0.522430956363678\n",
      "epoch 6499, loss 0.01239794958382845, R2 0.4846665859222412\n",
      "Eval loss 0.012398237362504005, R2 0.5224332809448242\n",
      "epoch 6500, loss 0.01239789742976427, R2 0.4846682548522949\n",
      "Eval loss 0.012398178689181805, R2 0.5224350690841675\n",
      "epoch 6501, loss 0.012397845275700092, R2 0.4846702814102173\n",
      "Eval loss 0.012398121878504753, R2 0.522437572479248\n",
      "epoch 6502, loss 0.012397794052958488, R2 0.4846727252006531\n",
      "Eval loss 0.012398065067827702, R2 0.5224395990371704\n",
      "epoch 6503, loss 0.012397742830216885, R2 0.4846748113632202\n",
      "Eval loss 0.01239800825715065, R2 0.5224417448043823\n",
      "epoch 6504, loss 0.012397689744830132, R2 0.484676718711853\n",
      "Eval loss 0.012397951446473598, R2 0.5224440097808838\n",
      "epoch 6505, loss 0.012397639453411102, R2 0.4846791625022888\n",
      "Eval loss 0.012397893704473972, R2 0.5224461555480957\n",
      "epoch 6506, loss 0.01239758636802435, R2 0.4846818447113037\n",
      "Eval loss 0.01239783689379692, R2 0.5224483013153076\n",
      "epoch 6507, loss 0.01239753607660532, R2 0.4846832752227783\n",
      "Eval loss 0.012397779151797295, R2 0.5224505662918091\n",
      "epoch 6508, loss 0.012397482991218567, R2 0.48468565940856934\n",
      "Eval loss 0.012397722341120243, R2 0.5224525332450867\n",
      "epoch 6509, loss 0.012397432699799538, R2 0.4846874475479126\n",
      "Eval loss 0.01239766739308834, R2 0.5224546194076538\n",
      "epoch 6510, loss 0.01239738054573536, R2 0.48468971252441406\n",
      "Eval loss 0.012397609651088715, R2 0.5224572420120239\n",
      "epoch 6511, loss 0.01239733025431633, R2 0.4846925139427185\n",
      "Eval loss 0.012397552840411663, R2 0.5224590301513672\n",
      "epoch 6512, loss 0.012397278100252151, R2 0.48469436168670654\n",
      "Eval loss 0.012397495098412037, R2 0.5224615335464478\n",
      "epoch 6513, loss 0.012397225014865398, R2 0.4846962094306946\n",
      "Eval loss 0.012397438287734985, R2 0.5224637389183044\n",
      "epoch 6514, loss 0.012397175654768944, R2 0.48469823598861694\n",
      "Eval loss 0.012397382408380508, R2 0.522465705871582\n",
      "epoch 6515, loss 0.012397123500704765, R2 0.48470044136047363\n",
      "Eval loss 0.012397323735058308, R2 0.5224683284759521\n",
      "epoch 6516, loss 0.012397072277963161, R2 0.4847027659416199\n",
      "Eval loss 0.01239726785570383, R2 0.5224705934524536\n",
      "epoch 6517, loss 0.012397020123898983, R2 0.48470473289489746\n",
      "Eval loss 0.012397211976349354, R2 0.5224721431732178\n",
      "epoch 6518, loss 0.012396969832479954, R2 0.4847072958946228\n",
      "Eval loss 0.012397155165672302, R2 0.5224745273590088\n",
      "epoch 6519, loss 0.01239691860973835, R2 0.4847094416618347\n",
      "Eval loss 0.01239709835499525, R2 0.5224767923355103\n",
      "epoch 6520, loss 0.012396865524351597, R2 0.4847109913825989\n",
      "Eval loss 0.0123970415443182, R2 0.5224795937538147\n",
      "epoch 6521, loss 0.012396817095577717, R2 0.4847128391265869\n",
      "Eval loss 0.012396984733641148, R2 0.5224813222885132\n",
      "epoch 6522, loss 0.012396764941513538, R2 0.4847153425216675\n",
      "Eval loss 0.01239692885428667, R2 0.5224834680557251\n",
      "epoch 6523, loss 0.012396713718771935, R2 0.4847174882888794\n",
      "Eval loss 0.012396873906254768, R2 0.5224857926368713\n",
      "epoch 6524, loss 0.01239666249603033, R2 0.48471957445144653\n",
      "Eval loss 0.012396816164255142, R2 0.5224875211715698\n",
      "epoch 6525, loss 0.012396613135933876, R2 0.4847217798233032\n",
      "Eval loss 0.01239675935357809, R2 0.5224900245666504\n",
      "epoch 6526, loss 0.012396560981869698, R2 0.4847239851951599\n",
      "Eval loss 0.012396702542901039, R2 0.5224923491477966\n",
      "epoch 6527, loss 0.012396508827805519, R2 0.4847261309623718\n",
      "Eval loss 0.012396646663546562, R2 0.5224940776824951\n",
      "epoch 6528, loss 0.01239645853638649, R2 0.48472803831100464\n",
      "Eval loss 0.012396588921546936, R2 0.5224964618682861\n",
      "epoch 6529, loss 0.01239640824496746, R2 0.48473066091537476\n",
      "Eval loss 0.012396533973515034, R2 0.5224984288215637\n",
      "epoch 6530, loss 0.012396357022225857, R2 0.48473262786865234\n",
      "Eval loss 0.012396479025483131, R2 0.5225008726119995\n",
      "epoch 6531, loss 0.012396304868161678, R2 0.48473429679870605\n",
      "Eval loss 0.012396421283483505, R2 0.5225027799606323\n",
      "epoch 6532, loss 0.012396255508065224, R2 0.48473650217056274\n",
      "Eval loss 0.012396366335451603, R2 0.5225050449371338\n",
      "epoch 6533, loss 0.01239620428532362, R2 0.484738826751709\n",
      "Eval loss 0.012396308593451977, R2 0.5225070714950562\n",
      "epoch 6534, loss 0.012396153062582016, R2 0.4847407341003418\n",
      "Eval loss 0.012396253645420074, R2 0.5225090980529785\n",
      "epoch 6535, loss 0.012396101839840412, R2 0.4847433567047119\n",
      "Eval loss 0.012396195903420448, R2 0.5225114822387695\n",
      "epoch 6536, loss 0.012396051548421383, R2 0.48474550247192383\n",
      "Eval loss 0.012396140024065971, R2 0.5225139856338501\n",
      "epoch 6537, loss 0.012396001257002354, R2 0.48474788665771484\n",
      "Eval loss 0.012396086007356644, R2 0.5225157141685486\n",
      "epoch 6538, loss 0.012395950965583324, R2 0.4847491383552551\n",
      "Eval loss 0.012396028265357018, R2 0.5225179195404053\n",
      "epoch 6539, loss 0.012395898811519146, R2 0.48475098609924316\n",
      "Eval loss 0.012395973317325115, R2 0.5225198864936829\n",
      "epoch 6540, loss 0.012395849451422691, R2 0.4847533702850342\n",
      "Eval loss 0.012395916506648064, R2 0.5225224494934082\n",
      "epoch 6541, loss 0.012395798228681087, R2 0.48475563526153564\n",
      "Eval loss 0.012395861558616161, R2 0.522524893283844\n",
      "epoch 6542, loss 0.012395747937262058, R2 0.48475760221481323\n",
      "Eval loss 0.012395805679261684, R2 0.5225263833999634\n",
      "epoch 6543, loss 0.012395698577165604, R2 0.4847598075866699\n",
      "Eval loss 0.012395749799907207, R2 0.5225285291671753\n",
      "epoch 6544, loss 0.012395648285746574, R2 0.4847617745399475\n",
      "Eval loss 0.01239569392055273, R2 0.5225309133529663\n",
      "epoch 6545, loss 0.012395596131682396, R2 0.484764039516449\n",
      "Eval loss 0.012395637109875679, R2 0.5225332379341125\n",
      "epoch 6546, loss 0.012395545840263367, R2 0.4847661852836609\n",
      "Eval loss 0.012395583093166351, R2 0.5225350856781006\n",
      "epoch 6547, loss 0.012395496480166912, R2 0.48476821184158325\n",
      "Eval loss 0.012395527213811874, R2 0.522537112236023\n",
      "epoch 6548, loss 0.012395446188747883, R2 0.48477017879486084\n",
      "Eval loss 0.012395472265779972, R2 0.5225391387939453\n",
      "epoch 6549, loss 0.012395394966006279, R2 0.484772264957428\n",
      "Eval loss 0.012395414523780346, R2 0.5225415229797363\n",
      "epoch 6550, loss 0.01239534467458725, R2 0.4847743511199951\n",
      "Eval loss 0.012395360507071018, R2 0.5225437879562378\n",
      "epoch 6551, loss 0.01239529624581337, R2 0.4847763776779175\n",
      "Eval loss 0.012395304627716541, R2 0.5225460529327393\n",
      "epoch 6552, loss 0.01239524595439434, R2 0.4847784638404846\n",
      "Eval loss 0.012395250611007214, R2 0.522547721862793\n",
      "epoch 6553, loss 0.012395193800330162, R2 0.48478060960769653\n",
      "Eval loss 0.012395193800330162, R2 0.5225500464439392\n",
      "epoch 6554, loss 0.012395143508911133, R2 0.4847825765609741\n",
      "Eval loss 0.012395137920975685, R2 0.5225518941879272\n",
      "epoch 6555, loss 0.012395094148814678, R2 0.4847847819328308\n",
      "Eval loss 0.012395082041621208, R2 0.5225542187690735\n",
      "epoch 6556, loss 0.012395044788718224, R2 0.4847870469093323\n",
      "Eval loss 0.012395026162266731, R2 0.5225567817687988\n",
      "epoch 6557, loss 0.01239499356597662, R2 0.48478877544403076\n",
      "Eval loss 0.012394972145557404, R2 0.5225584506988525\n",
      "epoch 6558, loss 0.01239494327455759, R2 0.48479169607162476\n",
      "Eval loss 0.012394916266202927, R2 0.5225610136985779\n",
      "epoch 6559, loss 0.012394893914461136, R2 0.48479360342025757\n",
      "Eval loss 0.012394861318171024, R2 0.5225631594657898\n",
      "epoch 6560, loss 0.012394843623042107, R2 0.48479533195495605\n",
      "Eval loss 0.012394804507493973, R2 0.5225653052330017\n",
      "epoch 6561, loss 0.012394794262945652, R2 0.48479723930358887\n",
      "Eval loss 0.012394752353429794, R2 0.5225673317909241\n",
      "epoch 6562, loss 0.012394744902849197, R2 0.48479944467544556\n",
      "Eval loss 0.012394695542752743, R2 0.5225694179534912\n",
      "epoch 6563, loss 0.012394693680107594, R2 0.4848019480705261\n",
      "Eval loss 0.012394639663398266, R2 0.5225713849067688\n",
      "epoch 6564, loss 0.012394643388688564, R2 0.4848034977912903\n",
      "Eval loss 0.012394584715366364, R2 0.5225735306739807\n",
      "epoch 6565, loss 0.012394594959914684, R2 0.484805703163147\n",
      "Eval loss 0.012394530698657036, R2 0.522575855255127\n",
      "epoch 6566, loss 0.012394544668495655, R2 0.4848076105117798\n",
      "Eval loss 0.012394474819302559, R2 0.5225777626037598\n",
      "epoch 6567, loss 0.0123944953083992, R2 0.48481035232543945\n",
      "Eval loss 0.012394420802593231, R2 0.5225799679756165\n",
      "epoch 6568, loss 0.012394445016980171, R2 0.4848119020462036\n",
      "Eval loss 0.012394364923238754, R2 0.5225822925567627\n",
      "epoch 6569, loss 0.012394394725561142, R2 0.4848138689994812\n",
      "Eval loss 0.012394309975206852, R2 0.522584080696106\n",
      "epoch 6570, loss 0.012394345365464687, R2 0.4848165512084961\n",
      "Eval loss 0.012394255958497524, R2 0.5225860476493835\n",
      "epoch 6571, loss 0.012394296936690807, R2 0.48481810092926025\n",
      "Eval loss 0.012394200079143047, R2 0.5225881934165955\n",
      "epoch 6572, loss 0.012394245713949203, R2 0.48482000827789307\n",
      "Eval loss 0.01239414606243372, R2 0.5225903987884521\n",
      "epoch 6573, loss 0.012394197285175323, R2 0.48482203483581543\n",
      "Eval loss 0.012394092045724392, R2 0.5225925445556641\n",
      "epoch 6574, loss 0.012394147925078869, R2 0.48482412099838257\n",
      "Eval loss 0.01239403523504734, R2 0.522594690322876\n",
      "epoch 6575, loss 0.012394098564982414, R2 0.4848266839981079\n",
      "Eval loss 0.012393982149660587, R2 0.5225968360900879\n",
      "epoch 6576, loss 0.01239404734224081, R2 0.48482847213745117\n",
      "Eval loss 0.01239392627030611, R2 0.5225987434387207\n",
      "epoch 6577, loss 0.01239399891346693, R2 0.4848307967185974\n",
      "Eval loss 0.012393872253596783, R2 0.5226009488105774\n",
      "epoch 6578, loss 0.012393949553370476, R2 0.4848328232765198\n",
      "Eval loss 0.012393818236887455, R2 0.5226033329963684\n",
      "epoch 6579, loss 0.012393900193274021, R2 0.4848347306251526\n",
      "Eval loss 0.012393762357532978, R2 0.5226051807403564\n",
      "epoch 6580, loss 0.012393849901854992, R2 0.4848364591598511\n",
      "Eval loss 0.012393707409501076, R2 0.5226077437400818\n",
      "epoch 6581, loss 0.012393800541758537, R2 0.48483920097351074\n",
      "Eval loss 0.012393654324114323, R2 0.5226092338562012\n",
      "epoch 6582, loss 0.012393751181662083, R2 0.48484063148498535\n",
      "Eval loss 0.012393598444759846, R2 0.5226116180419922\n",
      "epoch 6583, loss 0.012393701821565628, R2 0.4848426580429077\n",
      "Eval loss 0.012393543496727943, R2 0.5226136445999146\n",
      "epoch 6584, loss 0.012393651530146599, R2 0.48484522104263306\n",
      "Eval loss 0.012393489480018616, R2 0.5226159691810608\n",
      "epoch 6585, loss 0.012393604032695293, R2 0.48484671115875244\n",
      "Eval loss 0.012393435463309288, R2 0.5226178169250488\n",
      "epoch 6586, loss 0.012393554672598839, R2 0.4848487973213196\n",
      "Eval loss 0.01239338330924511, R2 0.5226199626922607\n",
      "epoch 6587, loss 0.012393506243824959, R2 0.4848509430885315\n",
      "Eval loss 0.012393327429890633, R2 0.5226218104362488\n",
      "epoch 6588, loss 0.012393457815051079, R2 0.48485279083251953\n",
      "Eval loss 0.012393273413181305, R2 0.522624135017395\n",
      "epoch 6589, loss 0.01239340752363205, R2 0.48485487699508667\n",
      "Eval loss 0.012393218465149403, R2 0.5226261615753174\n",
      "epoch 6590, loss 0.01239335909485817, R2 0.48485690355300903\n",
      "Eval loss 0.0123931635171175, R2 0.522628128528595\n",
      "epoch 6591, loss 0.01239330880343914, R2 0.4848591685295105\n",
      "Eval loss 0.012393110431730747, R2 0.5226305723190308\n",
      "epoch 6592, loss 0.01239326037466526, R2 0.48486167192459106\n",
      "Eval loss 0.01239305455237627, R2 0.5226328372955322\n",
      "epoch 6593, loss 0.012393211014568806, R2 0.4848633408546448\n",
      "Eval loss 0.012393001466989517, R2 0.5226346254348755\n",
      "epoch 6594, loss 0.012393161654472351, R2 0.48486512899398804\n",
      "Eval loss 0.012392948381602764, R2 0.5226368308067322\n",
      "epoch 6595, loss 0.012393113225698471, R2 0.4848669767379761\n",
      "Eval loss 0.012392893433570862, R2 0.5226390361785889\n",
      "epoch 6596, loss 0.012393064796924591, R2 0.4848693013191223\n",
      "Eval loss 0.012392839416861534, R2 0.5226408839225769\n",
      "epoch 6597, loss 0.012393015436828136, R2 0.4848710298538208\n",
      "Eval loss 0.012392786331474781, R2 0.522642970085144\n",
      "epoch 6598, loss 0.012392966076731682, R2 0.4848734140396118\n",
      "Eval loss 0.012392732314765453, R2 0.5226452946662903\n",
      "epoch 6599, loss 0.012392916716635227, R2 0.484874963760376\n",
      "Eval loss 0.012392675504088402, R2 0.5226469039916992\n",
      "epoch 6600, loss 0.012392871081829071, R2 0.4848778247833252\n",
      "Eval loss 0.012392624281346798, R2 0.5226494669914246\n",
      "epoch 6601, loss 0.012392820790410042, R2 0.4848793148994446\n",
      "Eval loss 0.012392569333314896, R2 0.5226511359214783\n",
      "epoch 6602, loss 0.012392771430313587, R2 0.4848819971084595\n",
      "Eval loss 0.012392516247928143, R2 0.5226534605026245\n",
      "epoch 6603, loss 0.012392723001539707, R2 0.4848833680152893\n",
      "Eval loss 0.012392462231218815, R2 0.5226552486419678\n",
      "epoch 6604, loss 0.012392673641443253, R2 0.4848858714103699\n",
      "Eval loss 0.012392407283186913, R2 0.5226575136184692\n",
      "epoch 6605, loss 0.012392626143991947, R2 0.48488789796829224\n",
      "Eval loss 0.01239235419780016, R2 0.5226600170135498\n",
      "epoch 6606, loss 0.012392575852572918, R2 0.4848894476890564\n",
      "Eval loss 0.012392299249768257, R2 0.522661566734314\n",
      "epoch 6607, loss 0.012392529286444187, R2 0.484891414642334\n",
      "Eval loss 0.012392246164381504, R2 0.5226635932922363\n",
      "epoch 6608, loss 0.012392479926347733, R2 0.48489344120025635\n",
      "Eval loss 0.012392193078994751, R2 0.5226659178733826\n",
      "epoch 6609, loss 0.012392431497573853, R2 0.4848954677581787\n",
      "Eval loss 0.012392138130962849, R2 0.5226674675941467\n",
      "epoch 6610, loss 0.012392381206154823, R2 0.48489755392074585\n",
      "Eval loss 0.012392085045576096, R2 0.5226700901985168\n",
      "epoch 6611, loss 0.012392333708703518, R2 0.48490017652511597\n",
      "Eval loss 0.012392031960189342, R2 0.5226718187332153\n",
      "epoch 6612, loss 0.012392286211252213, R2 0.48490196466445923\n",
      "Eval loss 0.01239197701215744, R2 0.5226739645004272\n",
      "epoch 6613, loss 0.012392236851155758, R2 0.4849036931991577\n",
      "Eval loss 0.012391924858093262, R2 0.5226761102676392\n",
      "epoch 6614, loss 0.012392189353704453, R2 0.48490601778030396\n",
      "Eval loss 0.012391871772706509, R2 0.5226783156394958\n",
      "epoch 6615, loss 0.012392140924930573, R2 0.48490768671035767\n",
      "Eval loss 0.012391817755997181, R2 0.522680401802063\n",
      "epoch 6616, loss 0.012392092496156693, R2 0.4849095344543457\n",
      "Eval loss 0.012391765601933002, R2 0.5226824283599854\n",
      "epoch 6617, loss 0.012392044067382812, R2 0.48491138219833374\n",
      "Eval loss 0.012391711585223675, R2 0.5226843357086182\n",
      "epoch 6618, loss 0.012391996569931507, R2 0.48491406440734863\n",
      "Eval loss 0.012391656637191772, R2 0.5226868391036987\n",
      "epoch 6619, loss 0.012391949072480202, R2 0.484915554523468\n",
      "Eval loss 0.012391604483127594, R2 0.5226883292198181\n",
      "epoch 6620, loss 0.012391899712383747, R2 0.48491817712783813\n",
      "Eval loss 0.012391552329063416, R2 0.52269047498703\n",
      "epoch 6621, loss 0.012391851283609867, R2 0.4849199056625366\n",
      "Eval loss 0.012391497381031513, R2 0.5226924419403076\n",
      "epoch 6622, loss 0.012391803786158562, R2 0.484921395778656\n",
      "Eval loss 0.012391445226967335, R2 0.5226947069168091\n",
      "epoch 6623, loss 0.012391756288707256, R2 0.4849235415458679\n",
      "Eval loss 0.012391392141580582, R2 0.5226964950561523\n",
      "epoch 6624, loss 0.012391706928610802, R2 0.4849255681037903\n",
      "Eval loss 0.01239133719354868, R2 0.5226984620094299\n",
      "epoch 6625, loss 0.012391659431159496, R2 0.48492753505706787\n",
      "Eval loss 0.012391285970807076, R2 0.5227009057998657\n",
      "epoch 6626, loss 0.012391611933708191, R2 0.4849298596382141\n",
      "Eval loss 0.012391231954097748, R2 0.5227029323577881\n",
      "epoch 6627, loss 0.012391564436256886, R2 0.4849315285682678\n",
      "Eval loss 0.012391178868710995, R2 0.5227047204971313\n",
      "epoch 6628, loss 0.012391516007483006, R2 0.4849334955215454\n",
      "Eval loss 0.012391124852001667, R2 0.5227072238922119\n",
      "epoch 6629, loss 0.0123914685100317, R2 0.4849361777305603\n",
      "Eval loss 0.012391072697937489, R2 0.5227087736129761\n",
      "epoch 6630, loss 0.012391419149935246, R2 0.48493754863739014\n",
      "Eval loss 0.012391019612550735, R2 0.5227106809616089\n",
      "epoch 6631, loss 0.012391372583806515, R2 0.4849401116371155\n",
      "Eval loss 0.012390966527163982, R2 0.522712767124176\n",
      "epoch 6632, loss 0.01239132322371006, R2 0.4849420189857483\n",
      "Eval loss 0.01239091344177723, R2 0.5227149724960327\n",
      "epoch 6633, loss 0.012391275726258755, R2 0.4849435091018677\n",
      "Eval loss 0.012390860356390476, R2 0.5227172374725342\n",
      "epoch 6634, loss 0.012391227297484875, R2 0.48494547605514526\n",
      "Eval loss 0.012390808202326298, R2 0.522719144821167\n",
      "epoch 6635, loss 0.012391180731356144, R2 0.48494791984558105\n",
      "Eval loss 0.01239075604826212, R2 0.5227211713790894\n",
      "epoch 6636, loss 0.012391133233904839, R2 0.4849497675895691\n",
      "Eval loss 0.012390701100230217, R2 0.5227235555648804\n",
      "epoch 6637, loss 0.012391084805130959, R2 0.4849514365196228\n",
      "Eval loss 0.012390649877488613, R2 0.522724986076355\n",
      "epoch 6638, loss 0.012391037307679653, R2 0.4849534034729004\n",
      "Eval loss 0.01239059679210186, R2 0.5227271318435669\n",
      "epoch 6639, loss 0.012390988878905773, R2 0.48495543003082275\n",
      "Eval loss 0.012390542775392532, R2 0.5227291584014893\n",
      "epoch 6640, loss 0.012390942312777042, R2 0.4849575161933899\n",
      "Eval loss 0.012390491552650928, R2 0.5227313041687012\n",
      "epoch 6641, loss 0.012390894815325737, R2 0.48495984077453613\n",
      "Eval loss 0.01239043939858675, R2 0.522733211517334\n",
      "epoch 6642, loss 0.012390846386551857, R2 0.4849613308906555\n",
      "Eval loss 0.012390385381877422, R2 0.5227349996566772\n",
      "epoch 6643, loss 0.0123908007517457, R2 0.484963595867157\n",
      "Eval loss 0.012390333227813244, R2 0.5227371454238892\n",
      "epoch 6644, loss 0.012390753254294395, R2 0.4849652647972107\n",
      "Eval loss 0.01239028014242649, R2 0.5227394700050354\n",
      "epoch 6645, loss 0.012390704825520515, R2 0.4849672317504883\n",
      "Eval loss 0.012390228919684887, R2 0.5227413177490234\n",
      "epoch 6646, loss 0.012390656396746635, R2 0.48496925830841064\n",
      "Eval loss 0.012390175834298134, R2 0.5227432250976562\n",
      "epoch 6647, loss 0.012390609830617905, R2 0.48497170209884644\n",
      "Eval loss 0.012390123680233955, R2 0.5227454900741577\n",
      "epoch 6648, loss 0.0123905623331666, R2 0.4849730134010315\n",
      "Eval loss 0.012390071526169777, R2 0.5227472186088562\n",
      "epoch 6649, loss 0.012390516698360443, R2 0.48497503995895386\n",
      "Eval loss 0.012390019372105598, R2 0.5227491855621338\n",
      "epoch 6650, loss 0.012390468269586563, R2 0.48497670888900757\n",
      "Eval loss 0.012389966286718845, R2 0.5227516889572144\n",
      "epoch 6651, loss 0.012390419840812683, R2 0.48497891426086426\n",
      "Eval loss 0.012389913201332092, R2 0.5227538347244263\n",
      "epoch 6652, loss 0.012390373274683952, R2 0.4849814772605896\n",
      "Eval loss 0.012389861978590488, R2 0.5227557420730591\n",
      "epoch 6653, loss 0.012390325777232647, R2 0.4849831461906433\n",
      "Eval loss 0.01238980982452631, R2 0.5227575898170471\n",
      "epoch 6654, loss 0.01239028014242649, R2 0.4849854111671448\n",
      "Eval loss 0.012389756739139557, R2 0.5227594375610352\n",
      "epoch 6655, loss 0.012390229851007462, R2 0.48498713970184326\n",
      "Eval loss 0.012389704585075378, R2 0.5227616429328918\n",
      "epoch 6656, loss 0.012390184216201305, R2 0.4849890470504761\n",
      "Eval loss 0.012389651499688625, R2 0.5227638483047485\n",
      "epoch 6657, loss 0.012390137650072575, R2 0.4849909543991089\n",
      "Eval loss 0.012389599345624447, R2 0.5227656364440918\n",
      "epoch 6658, loss 0.01239009015262127, R2 0.4849928021430969\n",
      "Eval loss 0.012389547191560268, R2 0.5227678418159485\n",
      "epoch 6659, loss 0.012390043586492538, R2 0.48499488830566406\n",
      "Eval loss 0.012389495968818665, R2 0.5227696895599365\n",
      "epoch 6660, loss 0.012389996089041233, R2 0.4849966764450073\n",
      "Eval loss 0.01238944474607706, R2 0.5227713584899902\n",
      "epoch 6661, loss 0.012389949522912502, R2 0.4849984645843506\n",
      "Eval loss 0.012389391660690308, R2 0.5227733850479126\n",
      "epoch 6662, loss 0.012389902956783772, R2 0.4850010275840759\n",
      "Eval loss 0.01238933950662613, R2 0.5227757096290588\n",
      "epoch 6663, loss 0.01238985639065504, R2 0.4850029945373535\n",
      "Eval loss 0.012389288283884525, R2 0.5227776765823364\n",
      "epoch 6664, loss 0.01238980796188116, R2 0.48500436544418335\n",
      "Eval loss 0.012389235198497772, R2 0.5227797031402588\n",
      "epoch 6665, loss 0.01238976325839758, R2 0.4850063920021057\n",
      "Eval loss 0.012389183975756168, R2 0.5227815508842468\n",
      "epoch 6666, loss 0.012389713898301125, R2 0.4850085973739624\n",
      "Eval loss 0.012389133684337139, R2 0.5227832198143005\n",
      "epoch 6667, loss 0.012389667332172394, R2 0.48501020669937134\n",
      "Eval loss 0.012389079667627811, R2 0.5227859616279602\n",
      "epoch 6668, loss 0.012389620766043663, R2 0.4850122928619385\n",
      "Eval loss 0.012389028444886208, R2 0.5227876901626587\n",
      "epoch 6669, loss 0.012389575131237507, R2 0.48501449823379517\n",
      "Eval loss 0.012388976290822029, R2 0.5227898359298706\n",
      "epoch 6670, loss 0.012389528565108776, R2 0.4850162863731384\n",
      "Eval loss 0.01238892413675785, R2 0.5227916836738586\n",
      "epoch 6671, loss 0.012389480136334896, R2 0.48501813411712646\n",
      "Eval loss 0.012388872914016247, R2 0.5227934122085571\n",
      "epoch 6672, loss 0.01238943450152874, R2 0.48502016067504883\n",
      "Eval loss 0.012388820759952068, R2 0.5227957963943481\n",
      "epoch 6673, loss 0.01238938793540001, R2 0.4850219488143921\n",
      "Eval loss 0.012388770468533039, R2 0.5227977633476257\n",
      "epoch 6674, loss 0.012389341369271278, R2 0.485024094581604\n",
      "Eval loss 0.01238871831446886, R2 0.5227996110916138\n",
      "epoch 6675, loss 0.012389294803142548, R2 0.4850255250930786\n",
      "Eval loss 0.012388667091727257, R2 0.5228017568588257\n",
      "epoch 6676, loss 0.012389248237013817, R2 0.48502761125564575\n",
      "Eval loss 0.012388615868985653, R2 0.5228034257888794\n",
      "epoch 6677, loss 0.012389200739562511, R2 0.48502975702285767\n",
      "Eval loss 0.0123885627835989, R2 0.5228053331375122\n",
      "epoch 6678, loss 0.012389153242111206, R2 0.4850319027900696\n",
      "Eval loss 0.012388511560857296, R2 0.5228078365325928\n",
      "epoch 6679, loss 0.01238910760730505, R2 0.48503363132476807\n",
      "Eval loss 0.012388461269438267, R2 0.5228092670440674\n",
      "epoch 6680, loss 0.012389062903821468, R2 0.4850353002548218\n",
      "Eval loss 0.012388410046696663, R2 0.5228115320205688\n",
      "epoch 6681, loss 0.012389014475047588, R2 0.48503798246383667\n",
      "Eval loss 0.01238835696130991, R2 0.5228136777877808\n",
      "epoch 6682, loss 0.012388969771564007, R2 0.4850391745567322\n",
      "Eval loss 0.012388305738568306, R2 0.5228155255317688\n",
      "epoch 6683, loss 0.012388922274112701, R2 0.4850417971611023\n",
      "Eval loss 0.012388255447149277, R2 0.5228173732757568\n",
      "epoch 6684, loss 0.012388874776661396, R2 0.4850432872772217\n",
      "Eval loss 0.012388202361762524, R2 0.5228192210197449\n",
      "epoch 6685, loss 0.012388830073177814, R2 0.4850451350212097\n",
      "Eval loss 0.012388152070343494, R2 0.5228215456008911\n",
      "epoch 6686, loss 0.012388783507049084, R2 0.48504728078842163\n",
      "Eval loss 0.01238810084760189, R2 0.5228238105773926\n",
      "epoch 6687, loss 0.012388736940920353, R2 0.4850490093231201\n",
      "Eval loss 0.012388049624860287, R2 0.5228251218795776\n",
      "epoch 6688, loss 0.012388691306114197, R2 0.48505091667175293\n",
      "Eval loss 0.012387998402118683, R2 0.5228273868560791\n",
      "epoch 6689, loss 0.012388644739985466, R2 0.48505300283432007\n",
      "Eval loss 0.012387947179377079, R2 0.5228290557861328\n",
      "epoch 6690, loss 0.012388598173856735, R2 0.48505479097366333\n",
      "Eval loss 0.012387895956635475, R2 0.5228310823440552\n",
      "epoch 6691, loss 0.012388553470373154, R2 0.48505699634552\n",
      "Eval loss 0.012387843802571297, R2 0.5228331685066223\n",
      "epoch 6692, loss 0.012388505972921848, R2 0.48505914211273193\n",
      "Eval loss 0.012387794442474842, R2 0.522835373878479\n",
      "epoch 6693, loss 0.012388460338115692, R2 0.48506051301956177\n",
      "Eval loss 0.012387743219733238, R2 0.5228371620178223\n",
      "epoch 6694, loss 0.012388413771986961, R2 0.4850621223449707\n",
      "Eval loss 0.012387691996991634, R2 0.5228391289710999\n",
      "epoch 6695, loss 0.01238836906850338, R2 0.4850645065307617\n",
      "Eval loss 0.01238764077425003, R2 0.5228412747383118\n",
      "epoch 6696, loss 0.012388321571052074, R2 0.4850664734840393\n",
      "Eval loss 0.012387589551508427, R2 0.5228428244590759\n",
      "epoch 6697, loss 0.012388275936245918, R2 0.4850682020187378\n",
      "Eval loss 0.012387540191411972, R2 0.5228447318077087\n",
      "epoch 6698, loss 0.012388230301439762, R2 0.4850701093673706\n",
      "Eval loss 0.012387488037347794, R2 0.5228466987609863\n",
      "epoch 6699, loss 0.012388184666633606, R2 0.4850720167160034\n",
      "Eval loss 0.01238743681460619, R2 0.5228489637374878\n",
      "epoch 6700, loss 0.012388139963150024, R2 0.48507434129714966\n",
      "Eval loss 0.01238738652318716, R2 0.5228511095046997\n",
      "epoch 6701, loss 0.012388093397021294, R2 0.4850759506225586\n",
      "Eval loss 0.012387334369122982, R2 0.5228527784347534\n",
      "epoch 6702, loss 0.012388047762215137, R2 0.4850776791572571\n",
      "Eval loss 0.012387285940349102, R2 0.5228545069694519\n",
      "epoch 6703, loss 0.012388000264763832, R2 0.48507964611053467\n",
      "Eval loss 0.012387232854962349, R2 0.5228569507598877\n",
      "epoch 6704, loss 0.01238795556128025, R2 0.48508119583129883\n",
      "Eval loss 0.012387183494865894, R2 0.5228585004806519\n",
      "epoch 6705, loss 0.01238790899515152, R2 0.4850839376449585\n",
      "Eval loss 0.01238713227212429, R2 0.5228604078292847\n",
      "epoch 6706, loss 0.012387864291667938, R2 0.48508596420288086\n",
      "Eval loss 0.012387082912027836, R2 0.522862434387207\n",
      "epoch 6707, loss 0.012387819588184357, R2 0.4850878119468689\n",
      "Eval loss 0.012387030757963657, R2 0.5228646397590637\n",
      "epoch 6708, loss 0.012387773022055626, R2 0.48508912324905396\n",
      "Eval loss 0.012386981397867203, R2 0.5228665471076965\n",
      "epoch 6709, loss 0.01238772738724947, R2 0.485090970993042\n",
      "Eval loss 0.012386930175125599, R2 0.5228685140609741\n",
      "epoch 6710, loss 0.012387681752443314, R2 0.48509353399276733\n",
      "Eval loss 0.012386878952383995, R2 0.5228701829910278\n",
      "epoch 6711, loss 0.012387636117637157, R2 0.4850956201553345\n",
      "Eval loss 0.012386828660964966, R2 0.5228724479675293\n",
      "epoch 6712, loss 0.012387590482831001, R2 0.48509669303894043\n",
      "Eval loss 0.012386780232191086, R2 0.5228742957115173\n",
      "epoch 6713, loss 0.01238754391670227, R2 0.48509860038757324\n",
      "Eval loss 0.012386729009449482, R2 0.522875964641571\n",
      "epoch 6714, loss 0.012387499213218689, R2 0.4851011633872986\n",
      "Eval loss 0.012386677786707878, R2 0.5228779315948486\n",
      "epoch 6715, loss 0.012387452647089958, R2 0.4851025342941284\n",
      "Eval loss 0.012386627495288849, R2 0.5228798985481262\n",
      "epoch 6716, loss 0.012387408874928951, R2 0.48510420322418213\n",
      "Eval loss 0.01238657720386982, R2 0.522881805896759\n",
      "epoch 6717, loss 0.012387361377477646, R2 0.4851061701774597\n",
      "Eval loss 0.012386527843773365, R2 0.5228841304779053\n",
      "epoch 6718, loss 0.012387317605316639, R2 0.48510849475860596\n",
      "Eval loss 0.012386477552354336, R2 0.5228856801986694\n",
      "epoch 6719, loss 0.012387271970510483, R2 0.48510992527008057\n",
      "Eval loss 0.012386427260935307, R2 0.5228880643844604\n",
      "epoch 6720, loss 0.012387226335704327, R2 0.4851118326187134\n",
      "Eval loss 0.012386376038193703, R2 0.5228898525238037\n",
      "epoch 6721, loss 0.012387181632220745, R2 0.4851136803627014\n",
      "Eval loss 0.012386325746774673, R2 0.5228915214538574\n",
      "epoch 6722, loss 0.012387135997414589, R2 0.4851158857345581\n",
      "Eval loss 0.01238627452403307, R2 0.5228939056396484\n",
      "epoch 6723, loss 0.012387091293931007, R2 0.48511791229248047\n",
      "Eval loss 0.012386225163936615, R2 0.5228959321975708\n",
      "epoch 6724, loss 0.012387045659124851, R2 0.48511916399002075\n",
      "Eval loss 0.012386173941195011, R2 0.5228976011276245\n",
      "epoch 6725, loss 0.01238700095564127, R2 0.4851211905479431\n",
      "Eval loss 0.012386124581098557, R2 0.5228993892669678\n",
      "epoch 6726, loss 0.012386955320835114, R2 0.48512357473373413\n",
      "Eval loss 0.012386075221002102, R2 0.5229014158248901\n",
      "epoch 6727, loss 0.012386909686028957, R2 0.48512500524520874\n",
      "Eval loss 0.012386024929583073, R2 0.5229030847549438\n",
      "epoch 6728, loss 0.01238686591386795, R2 0.485126793384552\n",
      "Eval loss 0.012385974638164043, R2 0.5229054689407349\n",
      "epoch 6729, loss 0.012386820279061794, R2 0.48512887954711914\n",
      "Eval loss 0.012385924346745014, R2 0.5229073762893677\n",
      "epoch 6730, loss 0.012386774644255638, R2 0.48513054847717285\n",
      "Eval loss 0.012385875917971134, R2 0.5229088068008423\n",
      "epoch 6731, loss 0.012386729940772057, R2 0.48513245582580566\n",
      "Eval loss 0.012385823763906956, R2 0.5229110717773438\n",
      "epoch 6732, loss 0.0123866843059659, R2 0.4851348400115967\n",
      "Eval loss 0.012385773472487926, R2 0.5229129195213318\n",
      "epoch 6733, loss 0.012386640533804893, R2 0.48513662815093994\n",
      "Eval loss 0.012385725043714046, R2 0.5229146480560303\n",
      "epoch 6734, loss 0.012386595830321312, R2 0.48513805866241455\n",
      "Eval loss 0.012385675683617592, R2 0.5229165554046631\n",
      "epoch 6735, loss 0.012386550195515156, R2 0.4851400852203369\n",
      "Eval loss 0.012385625392198563, R2 0.5229185819625854\n",
      "epoch 6736, loss 0.012386506423354149, R2 0.4851417541503906\n",
      "Eval loss 0.012385574169456959, R2 0.5229208469390869\n",
      "epoch 6737, loss 0.012386459857225418, R2 0.48514413833618164\n",
      "Eval loss 0.012385526672005653, R2 0.5229225754737854\n",
      "epoch 6738, loss 0.012386416085064411, R2 0.4851455092430115\n",
      "Eval loss 0.01238547544926405, R2 0.5229243040084839\n",
      "epoch 6739, loss 0.012386370450258255, R2 0.4851473569869995\n",
      "Eval loss 0.01238542515784502, R2 0.5229262113571167\n",
      "epoch 6740, loss 0.012386325746774673, R2 0.4851492643356323\n",
      "Eval loss 0.012385375797748566, R2 0.5229283571243286\n",
      "epoch 6741, loss 0.012386281974613667, R2 0.4851510524749756\n",
      "Eval loss 0.01238532830029726, R2 0.5229299068450928\n",
      "epoch 6742, loss 0.01238623633980751, R2 0.4851536154747009\n",
      "Eval loss 0.012385277077555656, R2 0.5229320526123047\n",
      "epoch 6743, loss 0.012386191636323929, R2 0.48515480756759644\n",
      "Eval loss 0.012385226786136627, R2 0.522934079170227\n",
      "epoch 6744, loss 0.012386146932840347, R2 0.48515719175338745\n",
      "Eval loss 0.012385178357362747, R2 0.5229359865188599\n",
      "epoch 6745, loss 0.012386102229356766, R2 0.4851590394973755\n",
      "Eval loss 0.012385129928588867, R2 0.5229378342628479\n",
      "epoch 6746, loss 0.012386057525873184, R2 0.4851604104042053\n",
      "Eval loss 0.012385079637169838, R2 0.5229390859603882\n",
      "epoch 6747, loss 0.012386012822389603, R2 0.4851629137992859\n",
      "Eval loss 0.012385028414428234, R2 0.5229414701461792\n",
      "epoch 6748, loss 0.012385969050228596, R2 0.4851640462875366\n",
      "Eval loss 0.012384980916976929, R2 0.5229436159133911\n",
      "epoch 6749, loss 0.01238592341542244, R2 0.48516595363616943\n",
      "Eval loss 0.012384931556880474, R2 0.5229451656341553\n",
      "epoch 6750, loss 0.012385879643261433, R2 0.4851679801940918\n",
      "Eval loss 0.01238488219678402, R2 0.5229475498199463\n",
      "epoch 6751, loss 0.012385835871100426, R2 0.48517030477523804\n",
      "Eval loss 0.012384832836687565, R2 0.5229493379592896\n",
      "epoch 6752, loss 0.012385792098939419, R2 0.48517143726348877\n",
      "Eval loss 0.012384782545268536, R2 0.5229509472846985\n",
      "epoch 6753, loss 0.012385746464133263, R2 0.48517316579818726\n",
      "Eval loss 0.012384734116494656, R2 0.5229532718658447\n",
      "epoch 6754, loss 0.012385702691972256, R2 0.4851757884025574\n",
      "Eval loss 0.012384683825075626, R2 0.5229549407958984\n",
      "epoch 6755, loss 0.0123856570571661, R2 0.48517704010009766\n",
      "Eval loss 0.012384636327624321, R2 0.5229568481445312\n",
      "epoch 6756, loss 0.012385613285005093, R2 0.4851788878440857\n",
      "Eval loss 0.012384586036205292, R2 0.5229589343070984\n",
      "epoch 6757, loss 0.012385567650198936, R2 0.48518073558807373\n",
      "Eval loss 0.012384536676108837, R2 0.5229606628417969\n",
      "epoch 6758, loss 0.012385524809360504, R2 0.485182523727417\n",
      "Eval loss 0.012384488247334957, R2 0.5229628086090088\n",
      "epoch 6759, loss 0.012385481037199497, R2 0.48518455028533936\n",
      "Eval loss 0.012384439818561077, R2 0.5229644775390625\n",
      "epoch 6760, loss 0.012385435402393341, R2 0.48518627882003784\n",
      "Eval loss 0.012384389527142048, R2 0.5229663848876953\n",
      "epoch 6761, loss 0.012385391630232334, R2 0.48518824577331543\n",
      "Eval loss 0.012384341098368168, R2 0.5229682326316833\n",
      "epoch 6762, loss 0.012385347858071327, R2 0.48518985509872437\n",
      "Eval loss 0.012384291738271713, R2 0.5229703187942505\n",
      "epoch 6763, loss 0.012385303154587746, R2 0.48519188165664673\n",
      "Eval loss 0.012384242378175259, R2 0.5229724645614624\n",
      "epoch 6764, loss 0.012385259382426739, R2 0.4851934313774109\n",
      "Eval loss 0.012384193018078804, R2 0.5229736566543579\n",
      "epoch 6765, loss 0.012385215610265732, R2 0.4851953983306885\n",
      "Eval loss 0.01238414365798235, R2 0.5229754447937012\n",
      "epoch 6766, loss 0.01238517090678215, R2 0.4851972460746765\n",
      "Eval loss 0.01238409522920847, R2 0.5229777097702026\n",
      "epoch 6767, loss 0.012385126203298569, R2 0.4851992726325989\n",
      "Eval loss 0.012384047731757164, R2 0.5229793787002563\n",
      "epoch 6768, loss 0.012385084293782711, R2 0.485201358795166\n",
      "Eval loss 0.01238399837166071, R2 0.5229812860488892\n",
      "epoch 6769, loss 0.01238503959029913, R2 0.48520320653915405\n",
      "Eval loss 0.012383949011564255, R2 0.5229837894439697\n",
      "epoch 6770, loss 0.012384994886815548, R2 0.4852045774459839\n",
      "Eval loss 0.01238390151411295, R2 0.5229851603507996\n",
      "epoch 6771, loss 0.012384952045977116, R2 0.4852065443992615\n",
      "Eval loss 0.012383852154016495, R2 0.5229870676994324\n",
      "epoch 6772, loss 0.012384907342493534, R2 0.4852082133293152\n",
      "Eval loss 0.01238380279392004, R2 0.5229888558387756\n",
      "epoch 6773, loss 0.012384863570332527, R2 0.4852101802825928\n",
      "Eval loss 0.012383753433823586, R2 0.5229910016059875\n",
      "epoch 6774, loss 0.012384818866848946, R2 0.48521190881729126\n",
      "Eval loss 0.012383705005049706, R2 0.5229931473731995\n",
      "epoch 6775, loss 0.012384776957333088, R2 0.48521363735198975\n",
      "Eval loss 0.012383656576275826, R2 0.5229942798614502\n",
      "epoch 6776, loss 0.012384732253849506, R2 0.4852156639099121\n",
      "Eval loss 0.01238360907882452, R2 0.5229963064193726\n",
      "epoch 6777, loss 0.0123846884816885, R2 0.4852173328399658\n",
      "Eval loss 0.012383559718728065, R2 0.522998034954071\n",
      "epoch 6778, loss 0.012384645640850067, R2 0.4852191209793091\n",
      "Eval loss 0.012383511289954185, R2 0.5229998826980591\n",
      "epoch 6779, loss 0.012384600006043911, R2 0.48522084951400757\n",
      "Eval loss 0.012383462861180305, R2 0.523002028465271\n",
      "epoch 6780, loss 0.012384557165205479, R2 0.48522329330444336\n",
      "Eval loss 0.012383415363729, R2 0.5230035781860352\n",
      "epoch 6781, loss 0.012384513393044472, R2 0.48522424697875977\n",
      "Eval loss 0.012383366003632545, R2 0.5230059623718262\n",
      "epoch 6782, loss 0.012384467758238316, R2 0.4852266311645508\n",
      "Eval loss 0.01238331664353609, R2 0.5230078101158142\n",
      "epoch 6783, loss 0.012384424917399883, R2 0.48522889614105225\n",
      "Eval loss 0.012383269146084785, R2 0.5230098962783813\n",
      "epoch 6784, loss 0.012384382076561451, R2 0.485230028629303\n",
      "Eval loss 0.012383220717310905, R2 0.5230115652084351\n",
      "epoch 6785, loss 0.012384338304400444, R2 0.485231876373291\n",
      "Eval loss 0.0123831732198596, R2 0.5230131149291992\n",
      "epoch 6786, loss 0.012384295463562012, R2 0.4852336645126343\n",
      "Eval loss 0.01238312479108572, R2 0.5230151414871216\n",
      "epoch 6787, loss 0.01238425262272358, R2 0.48523545265197754\n",
      "Eval loss 0.012383075430989265, R2 0.5230169892311096\n",
      "epoch 6788, loss 0.012384207919239998, R2 0.4852377772331238\n",
      "Eval loss 0.01238302793353796, R2 0.5230185389518738\n",
      "epoch 6789, loss 0.012384165078401566, R2 0.4852392077445984\n",
      "Eval loss 0.01238297950476408, R2 0.5230205059051514\n",
      "epoch 6790, loss 0.012384122237563133, R2 0.48524153232574463\n",
      "Eval loss 0.012382930144667625, R2 0.5230224132537842\n",
      "epoch 6791, loss 0.012384078465402126, R2 0.48524266481399536\n",
      "Eval loss 0.01238288264721632, R2 0.5230243802070618\n",
      "epoch 6792, loss 0.012384033761918545, R2 0.4852449893951416\n",
      "Eval loss 0.01238283421844244, R2 0.5230259895324707\n",
      "epoch 6793, loss 0.012383990921080112, R2 0.48524612188339233\n",
      "Eval loss 0.01238278578966856, R2 0.5230287313461304\n",
      "epoch 6794, loss 0.012383947148919106, R2 0.4852480888366699\n",
      "Eval loss 0.01238273922353983, R2 0.5230296850204468\n",
      "epoch 6795, loss 0.012383903376758099, R2 0.48524993658065796\n",
      "Eval loss 0.01238269079476595, R2 0.5230318307876587\n",
      "epoch 6796, loss 0.012383861467242241, R2 0.48525166511535645\n",
      "Eval loss 0.01238264236599207, R2 0.523033618927002\n",
      "epoch 6797, loss 0.012383818626403809, R2 0.48525410890579224\n",
      "Eval loss 0.01238259393721819, R2 0.5230352878570557\n",
      "epoch 6798, loss 0.012383775785565376, R2 0.48525571823120117\n",
      "Eval loss 0.01238254550844431, R2 0.5230370759963989\n",
      "epoch 6799, loss 0.012383731082081795, R2 0.485257625579834\n",
      "Eval loss 0.012382498942315578, R2 0.5230394601821899\n",
      "epoch 6800, loss 0.012383688241243362, R2 0.4852590560913086\n",
      "Eval loss 0.012382450513541698, R2 0.5230408906936646\n",
      "epoch 6801, loss 0.01238364540040493, R2 0.4852604866027832\n",
      "Eval loss 0.012382402084767818, R2 0.523042619228363\n",
      "epoch 6802, loss 0.012383603490889072, R2 0.485262393951416\n",
      "Eval loss 0.012382353655993938, R2 0.5230444669723511\n",
      "epoch 6803, loss 0.012383558787405491, R2 0.48526424169540405\n",
      "Eval loss 0.012382305227220058, R2 0.523046612739563\n",
      "epoch 6804, loss 0.012383515946567059, R2 0.4852660298347473\n",
      "Eval loss 0.012382259592413902, R2 0.5230485200881958\n",
      "epoch 6805, loss 0.012383473105728626, R2 0.48526811599731445\n",
      "Eval loss 0.012382211163640022, R2 0.5230503082275391\n",
      "epoch 6806, loss 0.01238342933356762, R2 0.4852694869041443\n",
      "Eval loss 0.012382162734866142, R2 0.5230522751808167\n",
      "epoch 6807, loss 0.012383385561406612, R2 0.4852716326713562\n",
      "Eval loss 0.012382114306092262, R2 0.5230541229248047\n",
      "epoch 6808, loss 0.012383343651890755, R2 0.48527371883392334\n",
      "Eval loss 0.012382067739963531, R2 0.5230555534362793\n",
      "epoch 6809, loss 0.012383300811052322, R2 0.48527514934539795\n",
      "Eval loss 0.012382020242512226, R2 0.5230576395988464\n",
      "epoch 6810, loss 0.012383258901536465, R2 0.48527687788009644\n",
      "Eval loss 0.012381971813738346, R2 0.5230593681335449\n",
      "epoch 6811, loss 0.012383215129375458, R2 0.48527854681015015\n",
      "Eval loss 0.012381923384964466, R2 0.5230610370635986\n",
      "epoch 6812, loss 0.012383172288537025, R2 0.48528051376342773\n",
      "Eval loss 0.01238187775015831, R2 0.5230631232261658\n",
      "epoch 6813, loss 0.012383129447698593, R2 0.48528242111206055\n",
      "Eval loss 0.01238182745873928, R2 0.5230650901794434\n",
      "epoch 6814, loss 0.012383087538182735, R2 0.4852840304374695\n",
      "Eval loss 0.01238178089261055, R2 0.5230668783187866\n",
      "epoch 6815, loss 0.012383042834699154, R2 0.4852861762046814\n",
      "Eval loss 0.012381735257804394, R2 0.5230687856674194\n",
      "epoch 6816, loss 0.01238300185650587, R2 0.48528754711151123\n",
      "Eval loss 0.012381686829030514, R2 0.523070216178894\n",
      "epoch 6817, loss 0.01238295715302229, R2 0.4852892756462097\n",
      "Eval loss 0.012381639331579208, R2 0.5230721235275269\n",
      "epoch 6818, loss 0.012382915243506432, R2 0.4852910041809082\n",
      "Eval loss 0.012381591834127903, R2 0.5230741500854492\n",
      "epoch 6819, loss 0.012382873333990574, R2 0.4852927327156067\n",
      "Eval loss 0.012381545267999172, R2 0.5230759382247925\n",
      "epoch 6820, loss 0.012382830493152142, R2 0.48529452085494995\n",
      "Eval loss 0.012381496839225292, R2 0.523077666759491\n",
      "epoch 6821, loss 0.01238278765231371, R2 0.4852963089942932\n",
      "Eval loss 0.012381448410451412, R2 0.5230796337127686\n",
      "epoch 6822, loss 0.012382745742797852, R2 0.485298216342926\n",
      "Eval loss 0.012381402775645256, R2 0.5230814218521118\n",
      "epoch 6823, loss 0.01238270290195942, R2 0.4853000044822693\n",
      "Eval loss 0.01238135527819395, R2 0.5230835676193237\n",
      "epoch 6824, loss 0.012382660061120987, R2 0.4853016138076782\n",
      "Eval loss 0.012381309643387794, R2 0.5230847597122192\n",
      "epoch 6825, loss 0.01238261815160513, R2 0.48530352115631104\n",
      "Eval loss 0.012381261214613914, R2 0.5230866074562073\n",
      "epoch 6826, loss 0.012382576242089272, R2 0.48530513048171997\n",
      "Eval loss 0.012381212785840034, R2 0.5230884552001953\n",
      "epoch 6827, loss 0.012382532469928265, R2 0.48530709743499756\n",
      "Eval loss 0.012381167151033878, R2 0.5230903625488281\n",
      "epoch 6828, loss 0.012382490560412407, R2 0.48530882596969604\n",
      "Eval loss 0.012381118722259998, R2 0.5230920314788818\n",
      "epoch 6829, loss 0.01238244865089655, R2 0.485310435295105\n",
      "Eval loss 0.012381073087453842, R2 0.523094117641449\n",
      "epoch 6830, loss 0.012382404878735542, R2 0.48531222343444824\n",
      "Eval loss 0.012381025590002537, R2 0.5230956673622131\n",
      "epoch 6831, loss 0.012382362969219685, R2 0.48531413078308105\n",
      "Eval loss 0.012380978092551231, R2 0.5230977535247803\n",
      "epoch 6832, loss 0.012382321059703827, R2 0.48531603813171387\n",
      "Eval loss 0.012380930595099926, R2 0.5230993032455444\n",
      "epoch 6833, loss 0.01238227915018797, R2 0.485317587852478\n",
      "Eval loss 0.012380884028971195, R2 0.5231012105941772\n",
      "epoch 6834, loss 0.012382235378026962, R2 0.4853196144104004\n",
      "Eval loss 0.01238083653151989, R2 0.5231032371520996\n",
      "epoch 6835, loss 0.01238219439983368, R2 0.4853209853172302\n",
      "Eval loss 0.012380789965391159, R2 0.5231047868728638\n",
      "epoch 6836, loss 0.012382153421640396, R2 0.48532283306121826\n",
      "Eval loss 0.012380744330585003, R2 0.5231066942214966\n",
      "epoch 6837, loss 0.01238210964947939, R2 0.4853246808052063\n",
      "Eval loss 0.012380695901811123, R2 0.5231086015701294\n",
      "epoch 6838, loss 0.012382066808640957, R2 0.4853265881538391\n",
      "Eval loss 0.012380647473037243, R2 0.5231105089187622\n",
      "epoch 6839, loss 0.0123820248991251, R2 0.4853280186653137\n",
      "Eval loss 0.012380602769553661, R2 0.5231122374534607\n",
      "epoch 6840, loss 0.012381982989609241, R2 0.48532992601394653\n",
      "Eval loss 0.01238055620342493, R2 0.523114025592804\n",
      "epoch 6841, loss 0.01238194014877081, R2 0.485332190990448\n",
      "Eval loss 0.0123805096372962, R2 0.5231158137321472\n",
      "epoch 6842, loss 0.012381899170577526, R2 0.48533356189727783\n",
      "Eval loss 0.012380463071167469, R2 0.5231177806854248\n",
      "epoch 6843, loss 0.012381857261061668, R2 0.48533499240875244\n",
      "Eval loss 0.012380415573716164, R2 0.5231194496154785\n",
      "epoch 6844, loss 0.01238181535154581, R2 0.4853367209434509\n",
      "Eval loss 0.012380369007587433, R2 0.5231212377548218\n",
      "epoch 6845, loss 0.012381773442029953, R2 0.48533862829208374\n",
      "Eval loss 0.012380322441458702, R2 0.5231229066848755\n",
      "epoch 6846, loss 0.01238173060119152, R2 0.4853407144546509\n",
      "Eval loss 0.012380274944007397, R2 0.523125171661377\n",
      "epoch 6847, loss 0.012381688691675663, R2 0.48534196615219116\n",
      "Eval loss 0.01238022930920124, R2 0.5231263637542725\n",
      "epoch 6848, loss 0.012381646782159805, R2 0.485343873500824\n",
      "Eval loss 0.01238018088042736, R2 0.52312833070755\n",
      "epoch 6849, loss 0.012381603941321373, R2 0.48534536361694336\n",
      "Eval loss 0.012380135245621204, R2 0.5231303572654724\n",
      "epoch 6850, loss 0.012381562031805515, R2 0.48534727096557617\n",
      "Eval loss 0.012380088679492474, R2 0.5231322050094604\n",
      "epoch 6851, loss 0.012381521053612232, R2 0.4853494167327881\n",
      "Eval loss 0.012380041182041168, R2 0.5231337547302246\n",
      "epoch 6852, loss 0.012381479144096375, R2 0.48535019159317017\n",
      "Eval loss 0.012379994615912437, R2 0.5231356620788574\n",
      "epoch 6853, loss 0.012381437234580517, R2 0.48535245656967163\n",
      "Eval loss 0.012379949912428856, R2 0.5231372117996216\n",
      "epoch 6854, loss 0.012381396256387234, R2 0.48535412549972534\n",
      "Eval loss 0.01237990241497755, R2 0.5231388807296753\n",
      "epoch 6855, loss 0.01238135527819395, R2 0.4853556752204895\n",
      "Eval loss 0.012379856780171394, R2 0.5231409668922424\n",
      "epoch 6856, loss 0.012381313368678093, R2 0.4853580594062805\n",
      "Eval loss 0.012379810214042664, R2 0.5231425762176514\n",
      "epoch 6857, loss 0.012381271459162235, R2 0.4853594899177551\n",
      "Eval loss 0.012379764579236507, R2 0.5231442451477051\n",
      "epoch 6858, loss 0.012381228618323803, R2 0.4853612780570984\n",
      "Eval loss 0.012379718013107777, R2 0.5231463313102722\n",
      "epoch 6859, loss 0.012381188571453094, R2 0.48536330461502075\n",
      "Eval loss 0.01237967237830162, R2 0.5231483578681946\n",
      "epoch 6860, loss 0.012381145730614662, R2 0.48536455631256104\n",
      "Eval loss 0.012379624880850315, R2 0.523149847984314\n",
      "epoch 6861, loss 0.012381104752421379, R2 0.48536646366119385\n",
      "Eval loss 0.012379579246044159, R2 0.5231516361236572\n",
      "epoch 6862, loss 0.012381062842905521, R2 0.485368013381958\n",
      "Eval loss 0.012379531748592854, R2 0.5231533050537109\n",
      "epoch 6863, loss 0.012381020933389664, R2 0.4853699207305908\n",
      "Eval loss 0.012379487045109272, R2 0.5231550931930542\n",
      "epoch 6864, loss 0.012380980886518955, R2 0.4853714108467102\n",
      "Eval loss 0.012379440478980541, R2 0.5231567621231079\n",
      "epoch 6865, loss 0.012380938977003098, R2 0.48537302017211914\n",
      "Eval loss 0.012379394844174385, R2 0.5231589078903198\n",
      "epoch 6866, loss 0.012380896136164665, R2 0.4853747487068176\n",
      "Eval loss 0.012379349209368229, R2 0.5231606960296631\n",
      "epoch 6867, loss 0.012380855157971382, R2 0.48537659645080566\n",
      "Eval loss 0.012379301711916924, R2 0.5231624841690063\n",
      "epoch 6868, loss 0.012380813248455524, R2 0.4853783845901489\n",
      "Eval loss 0.012379255145788193, R2 0.5231641530990601\n",
      "epoch 6869, loss 0.012380772270262241, R2 0.48538023233413696\n",
      "Eval loss 0.012379209510982037, R2 0.5231658816337585\n",
      "epoch 6870, loss 0.012380731292068958, R2 0.4853822588920593\n",
      "Eval loss 0.012379164807498455, R2 0.523167610168457\n",
      "epoch 6871, loss 0.012380690313875675, R2 0.4853839874267578\n",
      "Eval loss 0.012379118241369724, R2 0.5231691598892212\n",
      "epoch 6872, loss 0.012380648404359818, R2 0.485385537147522\n",
      "Eval loss 0.012379072606563568, R2 0.5231711864471436\n",
      "epoch 6873, loss 0.01238060649484396, R2 0.48538732528686523\n",
      "Eval loss 0.012379026971757412, R2 0.5231726169586182\n",
      "epoch 6874, loss 0.012380564585328102, R2 0.48538869619369507\n",
      "Eval loss 0.012378979474306107, R2 0.5231747627258301\n",
      "epoch 6875, loss 0.012380524538457394, R2 0.4853910207748413\n",
      "Eval loss 0.01237893383949995, R2 0.5231761932373047\n",
      "epoch 6876, loss 0.01238048356026411, R2 0.4853922724723816\n",
      "Eval loss 0.012378889136016369, R2 0.523177981376648\n",
      "epoch 6877, loss 0.012380441650748253, R2 0.48539429903030396\n",
      "Eval loss 0.012378842569887638, R2 0.5231800079345703\n",
      "epoch 6878, loss 0.012380401603877544, R2 0.4853963255882263\n",
      "Eval loss 0.012378796935081482, R2 0.523181676864624\n",
      "epoch 6879, loss 0.012380359694361687, R2 0.4853973984718323\n",
      "Eval loss 0.012378749437630177, R2 0.5231837034225464\n",
      "epoch 6880, loss 0.012380317784845829, R2 0.48539912700653076\n",
      "Eval loss 0.01237870566546917, R2 0.5231856107711792\n",
      "epoch 6881, loss 0.012380276806652546, R2 0.48540085554122925\n",
      "Eval loss 0.012378660030663013, R2 0.5231870412826538\n",
      "epoch 6882, loss 0.012380237691104412, R2 0.48540228605270386\n",
      "Eval loss 0.012378614395856857, R2 0.5231884121894836\n",
      "epoch 6883, loss 0.012380195781588554, R2 0.4854040741920471\n",
      "Eval loss 0.012378568761050701, R2 0.5231902599334717\n",
      "epoch 6884, loss 0.012380153872072697, R2 0.4854058027267456\n",
      "Eval loss 0.012378523126244545, R2 0.5231920480728149\n",
      "epoch 6885, loss 0.012380113825201988, R2 0.48540765047073364\n",
      "Eval loss 0.012378477491438389, R2 0.5231942534446716\n",
      "epoch 6886, loss 0.012380072847008705, R2 0.48540931940078735\n",
      "Eval loss 0.012378430925309658, R2 0.5231959819793701\n",
      "epoch 6887, loss 0.012380031868815422, R2 0.4854108691215515\n",
      "Eval loss 0.012378386221826077, R2 0.5231977701187134\n",
      "epoch 6888, loss 0.012379990890622139, R2 0.4854130744934082\n",
      "Eval loss 0.012378339655697346, R2 0.5231994390487671\n",
      "epoch 6889, loss 0.012379948981106281, R2 0.48541396856307983\n",
      "Eval loss 0.012378294952213764, R2 0.5232011079788208\n",
      "epoch 6890, loss 0.012379909865558147, R2 0.4854159355163574\n",
      "Eval loss 0.012378250248730183, R2 0.5232025384902954\n",
      "epoch 6891, loss 0.012379868887364864, R2 0.48541849851608276\n",
      "Eval loss 0.012378203682601452, R2 0.5232042074203491\n",
      "epoch 6892, loss 0.012379827909171581, R2 0.48541921377182007\n",
      "Eval loss 0.01237815897911787, R2 0.523206353187561\n",
      "epoch 6893, loss 0.012379787862300873, R2 0.48542100191116333\n",
      "Eval loss 0.01237811241298914, R2 0.5232081413269043\n",
      "epoch 6894, loss 0.01237974502146244, R2 0.48542290925979614\n",
      "Eval loss 0.012378066778182983, R2 0.5232099294662476\n",
      "epoch 6895, loss 0.012379704974591732, R2 0.4854244589805603\n",
      "Eval loss 0.012378022074699402, R2 0.5232115983963013\n",
      "epoch 6896, loss 0.012379663065075874, R2 0.4854263663291931\n",
      "Eval loss 0.012377976439893246, R2 0.5232135057449341\n",
      "epoch 6897, loss 0.012379623018205166, R2 0.4854283332824707\n",
      "Eval loss 0.012377931736409664, R2 0.523215115070343\n",
      "epoch 6898, loss 0.012379582971334457, R2 0.4854295253753662\n",
      "Eval loss 0.012377887032926083, R2 0.523216724395752\n",
      "epoch 6899, loss 0.012379541993141174, R2 0.48543137311935425\n",
      "Eval loss 0.012377841398119926, R2 0.5232188701629639\n",
      "epoch 6900, loss 0.01237950287759304, R2 0.4854331612586975\n",
      "Eval loss 0.012377796694636345, R2 0.5232201814651489\n",
      "epoch 6901, loss 0.012379460968077183, R2 0.48543524742126465\n",
      "Eval loss 0.012377751059830189, R2 0.5232222080230713\n",
      "epoch 6902, loss 0.0123794199898839, R2 0.48543643951416016\n",
      "Eval loss 0.012377706356346607, R2 0.5232234001159668\n",
      "epoch 6903, loss 0.012379379011690617, R2 0.4854384660720825\n",
      "Eval loss 0.012377661652863026, R2 0.5232255458831787\n",
      "epoch 6904, loss 0.012379338033497334, R2 0.4854397177696228\n",
      "Eval loss 0.01237761601805687, R2 0.5232274532318115\n",
      "epoch 6905, loss 0.0123792989179492, R2 0.48544132709503174\n",
      "Eval loss 0.012377571314573288, R2 0.5232291221618652\n",
      "epoch 6906, loss 0.012379257939755917, R2 0.48544299602508545\n",
      "Eval loss 0.012377526611089706, R2 0.5232309103012085\n",
      "epoch 6907, loss 0.012379218824207783, R2 0.4854453206062317\n",
      "Eval loss 0.012377481907606125, R2 0.5232322216033936\n",
      "epoch 6908, loss 0.0123791778460145, R2 0.485446035861969\n",
      "Eval loss 0.012377436272799969, R2 0.5232341885566711\n",
      "epoch 6909, loss 0.012379136867821217, R2 0.48544806241989136\n",
      "Eval loss 0.012377390637993813, R2 0.5232359170913696\n",
      "epoch 6910, loss 0.012379096820950508, R2 0.48544973134994507\n",
      "Eval loss 0.012377345934510231, R2 0.523237407207489\n",
      "epoch 6911, loss 0.0123790567740798, R2 0.485451877117157\n",
      "Eval loss 0.01237730123102665, R2 0.5232391357421875\n",
      "epoch 6912, loss 0.012379016727209091, R2 0.4854530692100525\n",
      "Eval loss 0.012377255596220493, R2 0.5232412815093994\n",
      "epoch 6913, loss 0.012378975749015808, R2 0.4854552745819092\n",
      "Eval loss 0.012377212755382061, R2 0.5232428312301636\n",
      "epoch 6914, loss 0.012378934770822525, R2 0.4854564666748047\n",
      "Eval loss 0.01237716618925333, R2 0.5232446193695068\n",
      "epoch 6915, loss 0.012378895655274391, R2 0.48545825481414795\n",
      "Eval loss 0.012377121485769749, R2 0.5232462882995605\n",
      "epoch 6916, loss 0.012378854677081108, R2 0.4854602813720703\n",
      "Eval loss 0.012377076782286167, R2 0.5232477188110352\n",
      "epoch 6917, loss 0.0123788146302104, R2 0.4854614734649658\n",
      "Eval loss 0.01237703301012516, R2 0.5232494473457336\n",
      "epoch 6918, loss 0.012378774583339691, R2 0.4854632616043091\n",
      "Eval loss 0.01237698644399643, R2 0.5232516527175903\n",
      "epoch 6919, loss 0.012378733605146408, R2 0.48546481132507324\n",
      "Eval loss 0.012376943603157997, R2 0.523253321647644\n",
      "epoch 6920, loss 0.012378694489598274, R2 0.4854665994644165\n",
      "Eval loss 0.012376897968351841, R2 0.5232545137405396\n",
      "epoch 6921, loss 0.012378654442727566, R2 0.4854680895805359\n",
      "Eval loss 0.01237685326486826, R2 0.5232564210891724\n",
      "epoch 6922, loss 0.012378614395856857, R2 0.4854697585105896\n",
      "Eval loss 0.012376809492707253, R2 0.5232580900192261\n",
      "epoch 6923, loss 0.012378574348986149, R2 0.4854714274406433\n",
      "Eval loss 0.012376764789223671, R2 0.5232597589492798\n",
      "epoch 6924, loss 0.01237853430211544, R2 0.48547327518463135\n",
      "Eval loss 0.012376719154417515, R2 0.5232617855072021\n",
      "epoch 6925, loss 0.012378495186567307, R2 0.4854745864868164\n",
      "Eval loss 0.012376676313579082, R2 0.5232632160186768\n",
      "epoch 6926, loss 0.012378455139696598, R2 0.48547637462615967\n",
      "Eval loss 0.012376630678772926, R2 0.5232649445533752\n",
      "epoch 6927, loss 0.012378414161503315, R2 0.48547810316085815\n",
      "Eval loss 0.01237658690661192, R2 0.5232669115066528\n",
      "epoch 6928, loss 0.012378374114632607, R2 0.48548024892807007\n",
      "Eval loss 0.012376541271805763, R2 0.5232686996459961\n",
      "epoch 6929, loss 0.012378333136439323, R2 0.4854816198348999\n",
      "Eval loss 0.012376498430967331, R2 0.5232703685760498\n",
      "epoch 6930, loss 0.01237829402089119, R2 0.4854832887649536\n",
      "Eval loss 0.01237645372748375, R2 0.5232720375061035\n",
      "epoch 6931, loss 0.012378254905343056, R2 0.48548489809036255\n",
      "Eval loss 0.012376409024000168, R2 0.5232734680175781\n",
      "epoch 6932, loss 0.012378214858472347, R2 0.48548686504364014\n",
      "Eval loss 0.012376364320516586, R2 0.5232751965522766\n",
      "epoch 6933, loss 0.012378174811601639, R2 0.48548823595046997\n",
      "Eval loss 0.01237631868571043, R2 0.5232769250869751\n",
      "epoch 6934, loss 0.01237813476473093, R2 0.48548972606658936\n",
      "Eval loss 0.012376274913549423, R2 0.5232786536216736\n",
      "epoch 6935, loss 0.012378096580505371, R2 0.4854918122291565\n",
      "Eval loss 0.012376231141388416, R2 0.5232803821563721\n",
      "epoch 6936, loss 0.012378055602312088, R2 0.485493004322052\n",
      "Eval loss 0.01237618736922741, R2 0.5232820510864258\n",
      "epoch 6937, loss 0.012378016486763954, R2 0.48549461364746094\n",
      "Eval loss 0.012376143597066402, R2 0.523283839225769\n",
      "epoch 6938, loss 0.012377976439893246, R2 0.485496461391449\n",
      "Eval loss 0.012376098893582821, R2 0.5232857465744019\n",
      "epoch 6939, loss 0.012377937324345112, R2 0.48549777269363403\n",
      "Eval loss 0.012376056052744389, R2 0.5232870578765869\n",
      "epoch 6940, loss 0.012377897277474403, R2 0.4854995608329773\n",
      "Eval loss 0.012376011349260807, R2 0.5232892036437988\n",
      "epoch 6941, loss 0.012377859093248844, R2 0.48550182580947876\n",
      "Eval loss 0.012375964783132076, R2 0.5232909917831421\n",
      "epoch 6942, loss 0.012377818115055561, R2 0.4855033755302429\n",
      "Eval loss 0.012375921942293644, R2 0.5232925415039062\n",
      "epoch 6943, loss 0.012377778068184853, R2 0.4855045676231384\n",
      "Eval loss 0.012375878170132637, R2 0.52329421043396\n",
      "epoch 6944, loss 0.012377738952636719, R2 0.4855058193206787\n",
      "Eval loss 0.01237583439797163, R2 0.5232958793640137\n",
      "epoch 6945, loss 0.01237769890576601, R2 0.4855078458786011\n",
      "Eval loss 0.012375790625810623, R2 0.5232973098754883\n",
      "epoch 6946, loss 0.012377659790217876, R2 0.48550945520401\n",
      "Eval loss 0.012375745922327042, R2 0.5232993364334106\n",
      "epoch 6947, loss 0.012377620674669743, R2 0.4855118989944458\n",
      "Eval loss 0.012375702150166035, R2 0.5233007669448853\n",
      "epoch 6948, loss 0.012377581559121609, R2 0.48551273345947266\n",
      "Eval loss 0.012375658378005028, R2 0.523302435874939\n",
      "epoch 6949, loss 0.0123775415122509, R2 0.48551487922668457\n",
      "Eval loss 0.01237561460584402, R2 0.5233043432235718\n",
      "epoch 6950, loss 0.012377503328025341, R2 0.4855159521102905\n",
      "Eval loss 0.01237556990236044, R2 0.5233056545257568\n",
      "epoch 6951, loss 0.012377462349832058, R2 0.485517680644989\n",
      "Eval loss 0.012375526130199432, R2 0.5233074426651001\n",
      "epoch 6952, loss 0.01237742230296135, R2 0.4855193495750427\n",
      "Eval loss 0.012375482358038425, R2 0.523309588432312\n",
      "epoch 6953, loss 0.012377385050058365, R2 0.4855215549468994\n",
      "Eval loss 0.012375438585877419, R2 0.5233110189437866\n",
      "epoch 6954, loss 0.012377345934510231, R2 0.4855225086212158\n",
      "Eval loss 0.012375393882393837, R2 0.523313045501709\n",
      "epoch 6955, loss 0.012377305887639523, R2 0.48552435636520386\n",
      "Eval loss 0.01237535197287798, R2 0.523314356803894\n",
      "epoch 6956, loss 0.012377266772091389, R2 0.48552578687667847\n",
      "Eval loss 0.012375308200716972, R2 0.5233163237571716\n",
      "epoch 6957, loss 0.01237722672522068, R2 0.4855274558067322\n",
      "Eval loss 0.012375264428555965, R2 0.5233179926872253\n",
      "epoch 6958, loss 0.012377187609672546, R2 0.4855296015739441\n",
      "Eval loss 0.012375220656394958, R2 0.5233195424079895\n",
      "epoch 6959, loss 0.012377148494124413, R2 0.4855310320854187\n",
      "Eval loss 0.012375177815556526, R2 0.5233209133148193\n",
      "epoch 6960, loss 0.012377109378576279, R2 0.48553234338760376\n",
      "Eval loss 0.012375133112072945, R2 0.523322582244873\n",
      "epoch 6961, loss 0.012377070263028145, R2 0.4855339527130127\n",
      "Eval loss 0.012375089339911938, R2 0.5233243107795715\n",
      "epoch 6962, loss 0.012377031147480011, R2 0.48553574085235596\n",
      "Eval loss 0.01237504556775093, R2 0.5233263969421387\n",
      "epoch 6963, loss 0.012376991100609303, R2 0.48553723096847534\n",
      "Eval loss 0.012375002726912498, R2 0.5233280658721924\n",
      "epoch 6964, loss 0.012376952916383743, R2 0.4855388402938843\n",
      "Eval loss 0.012374958954751492, R2 0.5233293771743774\n",
      "epoch 6965, loss 0.012376914732158184, R2 0.48554104566574097\n",
      "Eval loss 0.012374915182590485, R2 0.5233315825462341\n",
      "epoch 6966, loss 0.012376874685287476, R2 0.48554205894470215\n",
      "Eval loss 0.012374871410429478, R2 0.5233331322669983\n",
      "epoch 6967, loss 0.012376835569739342, R2 0.4855438470840454\n",
      "Eval loss 0.01237482950091362, R2 0.5233343243598938\n",
      "epoch 6968, loss 0.012376797385513783, R2 0.485545814037323\n",
      "Eval loss 0.012374785728752613, R2 0.5233361721038818\n",
      "epoch 6969, loss 0.012376758269965649, R2 0.48554694652557373\n",
      "Eval loss 0.01237474288791418, R2 0.523337721824646\n",
      "epoch 6970, loss 0.012376719154417515, R2 0.48554837703704834\n",
      "Eval loss 0.0123746981844306, R2 0.5233395099639893\n",
      "epoch 6971, loss 0.012376680038869381, R2 0.48555052280426025\n",
      "Eval loss 0.012374655343592167, R2 0.5233410596847534\n",
      "epoch 6972, loss 0.012376641854643822, R2 0.48555177450180054\n",
      "Eval loss 0.01237461157143116, R2 0.5233429670333862\n",
      "epoch 6973, loss 0.012376602739095688, R2 0.4855533838272095\n",
      "Eval loss 0.012374567799270153, R2 0.5233449935913086\n",
      "epoch 6974, loss 0.012376563623547554, R2 0.48555564880371094\n",
      "Eval loss 0.012374524027109146, R2 0.5233463644981384\n",
      "epoch 6975, loss 0.012376525439321995, R2 0.48555707931518555\n",
      "Eval loss 0.012374482117593288, R2 0.5233480930328369\n",
      "epoch 6976, loss 0.012376485392451286, R2 0.48555827140808105\n",
      "Eval loss 0.012374438345432281, R2 0.5233495235443115\n",
      "epoch 6977, loss 0.012376448139548302, R2 0.48556047677993774\n",
      "Eval loss 0.01237439550459385, R2 0.5233516097068787\n",
      "epoch 6978, loss 0.012376409955322742, R2 0.4855616092681885\n",
      "Eval loss 0.012374351732432842, R2 0.523352861404419\n",
      "epoch 6979, loss 0.012376369908452034, R2 0.48556357622146606\n",
      "Eval loss 0.01237430889159441, R2 0.5233546495437622\n",
      "epoch 6980, loss 0.012376331724226475, R2 0.4855654835700989\n",
      "Eval loss 0.012374266982078552, R2 0.5233561396598816\n",
      "epoch 6981, loss 0.012376293540000916, R2 0.48556625843048096\n",
      "Eval loss 0.012374223209917545, R2 0.5233576893806458\n",
      "epoch 6982, loss 0.012376254424452782, R2 0.48556840419769287\n",
      "Eval loss 0.012374180369079113, R2 0.5233595967292786\n",
      "epoch 6983, loss 0.012376216240227222, R2 0.4855692982673645\n",
      "Eval loss 0.012374136596918106, R2 0.523361325263977\n",
      "epoch 6984, loss 0.012376177124679089, R2 0.4855709671974182\n",
      "Eval loss 0.012374093756079674, R2 0.5233632326126099\n",
      "epoch 6985, loss 0.01237613894045353, R2 0.4855726957321167\n",
      "Eval loss 0.012374051846563816, R2 0.5233643054962158\n",
      "epoch 6986, loss 0.01237610075622797, R2 0.4855744242668152\n",
      "Eval loss 0.01237400807440281, R2 0.5233660936355591\n",
      "epoch 6987, loss 0.012376060709357262, R2 0.48557591438293457\n",
      "Eval loss 0.012373964302241802, R2 0.5233679413795471\n",
      "epoch 6988, loss 0.012376022525131702, R2 0.48557770252227783\n",
      "Eval loss 0.01237392332404852, R2 0.5233697891235352\n",
      "epoch 6989, loss 0.012375983409583569, R2 0.4855790138244629\n",
      "Eval loss 0.012373879551887512, R2 0.5233709216117859\n",
      "epoch 6990, loss 0.012375947088003159, R2 0.48558080196380615\n",
      "Eval loss 0.01237383671104908, R2 0.5233725905418396\n",
      "epoch 6991, loss 0.012375907972455025, R2 0.48558229207992554\n",
      "Eval loss 0.012373792938888073, R2 0.5233741402626038\n",
      "epoch 6992, loss 0.012375868856906891, R2 0.4855839014053345\n",
      "Eval loss 0.01237375009804964, R2 0.5233762264251709\n",
      "epoch 6993, loss 0.012375830672681332, R2 0.4855858087539673\n",
      "Eval loss 0.012373707257211208, R2 0.5233778953552246\n",
      "epoch 6994, loss 0.012375793419778347, R2 0.48558706045150757\n",
      "Eval loss 0.01237366534769535, R2 0.5233794450759888\n",
      "epoch 6995, loss 0.012375753372907639, R2 0.48558855056762695\n",
      "Eval loss 0.012373620644211769, R2 0.5233812928199768\n",
      "epoch 6996, loss 0.01237571518868208, R2 0.4855901002883911\n",
      "Eval loss 0.012373579666018486, R2 0.5233824253082275\n",
      "epoch 6997, loss 0.01237567700445652, R2 0.4855923652648926\n",
      "Eval loss 0.012373537756502628, R2 0.5233846306800842\n",
      "epoch 6998, loss 0.01237563882023096, R2 0.48559343814849854\n",
      "Eval loss 0.01237349584698677, R2 0.5233860015869141\n",
      "epoch 6999, loss 0.012375600636005402, R2 0.4855955243110657\n",
      "Eval loss 0.012373451143503189, R2 0.5233874320983887\n",
      "epoch 7000, loss 0.012375562451779842, R2 0.48559677600860596\n",
      "Eval loss 0.012373409233987331, R2 0.5233890414237976\n",
      "epoch 7001, loss 0.012375524267554283, R2 0.4855979084968567\n",
      "Eval loss 0.012373366393148899, R2 0.5233910083770752\n",
      "epoch 7002, loss 0.012375486083328724, R2 0.4856002926826477\n",
      "Eval loss 0.012373324483633041, R2 0.5233927369117737\n",
      "epoch 7003, loss 0.012375447899103165, R2 0.485601544380188\n",
      "Eval loss 0.012373280711472034, R2 0.523393988609314\n",
      "epoch 7004, loss 0.012375409714877605, R2 0.48560279607772827\n",
      "Eval loss 0.012373238801956177, R2 0.5233957767486572\n",
      "epoch 7005, loss 0.01237537246197462, R2 0.48560452461242676\n",
      "Eval loss 0.012373195961117744, R2 0.52339768409729\n",
      "epoch 7006, loss 0.012375333346426487, R2 0.4856066107749939\n",
      "Eval loss 0.012373154051601887, R2 0.5233989953994751\n",
      "epoch 7007, loss 0.012375296093523502, R2 0.48560822010040283\n",
      "Eval loss 0.012373112142086029, R2 0.5234006643295288\n",
      "epoch 7008, loss 0.012375258840620518, R2 0.4856092929840088\n",
      "Eval loss 0.012373068369925022, R2 0.5234025716781616\n",
      "epoch 7009, loss 0.012375220656394958, R2 0.4856113791465759\n",
      "Eval loss 0.012373027391731739, R2 0.5234037637710571\n",
      "epoch 7010, loss 0.012375183403491974, R2 0.4856123924255371\n",
      "Eval loss 0.012372984550893307, R2 0.5234053730964661\n",
      "epoch 7011, loss 0.01237514428794384, R2 0.4856146574020386\n",
      "Eval loss 0.012372941710054874, R2 0.5234071612358093\n",
      "epoch 7012, loss 0.012375107035040855, R2 0.48561573028564453\n",
      "Eval loss 0.012372898869216442, R2 0.5234091281890869\n",
      "epoch 7013, loss 0.012375068850815296, R2 0.48561733961105347\n",
      "Eval loss 0.012372856959700584, R2 0.5234103202819824\n",
      "epoch 7014, loss 0.012375029735267162, R2 0.4856189489364624\n",
      "Eval loss 0.012372815981507301, R2 0.5234123468399048\n",
      "epoch 7015, loss 0.012374992482364178, R2 0.48562079668045044\n",
      "Eval loss 0.012372774071991444, R2 0.5234137773513794\n",
      "epoch 7016, loss 0.012374955229461193, R2 0.4856218695640564\n",
      "Eval loss 0.012372730299830437, R2 0.5234156250953674\n",
      "epoch 7017, loss 0.012374917976558208, R2 0.4856235980987549\n",
      "Eval loss 0.012372688390314579, R2 0.5234171152114868\n",
      "epoch 7018, loss 0.012374878861010075, R2 0.4856250286102295\n",
      "Eval loss 0.012372646480798721, R2 0.523418664932251\n",
      "epoch 7019, loss 0.01237484160810709, R2 0.485626757144928\n",
      "Eval loss 0.012372604571282864, R2 0.5234205722808838\n",
      "epoch 7020, loss 0.01237480342388153, R2 0.4856286644935608\n",
      "Eval loss 0.012372562661767006, R2 0.5234217643737793\n",
      "epoch 7021, loss 0.012374765239655972, R2 0.4856299161911011\n",
      "Eval loss 0.012372520752251148, R2 0.5234233140945435\n",
      "epoch 7022, loss 0.012374727055430412, R2 0.4856313467025757\n",
      "Eval loss 0.01237247884273529, R2 0.5234251618385315\n",
      "epoch 7023, loss 0.012374690733850002, R2 0.4856327176094055\n",
      "Eval loss 0.012372436933219433, R2 0.5234266519546509\n",
      "epoch 7024, loss 0.012374652549624443, R2 0.48563462495803833\n",
      "Eval loss 0.012372395023703575, R2 0.5234280824661255\n",
      "epoch 7025, loss 0.012374615296721458, R2 0.4856361746788025\n",
      "Eval loss 0.012372352182865143, R2 0.5234296321868896\n",
      "epoch 7026, loss 0.012374578043818474, R2 0.4856380224227905\n",
      "Eval loss 0.012372310273349285, R2 0.5234318971633911\n",
      "epoch 7027, loss 0.01237454079091549, R2 0.4856390953063965\n",
      "Eval loss 0.012372268363833427, R2 0.5234333872795105\n",
      "epoch 7028, loss 0.01237450260668993, R2 0.4856407046318054\n",
      "Eval loss 0.012372225522994995, R2 0.5234348773956299\n",
      "epoch 7029, loss 0.012374465353786945, R2 0.48564207553863525\n",
      "Eval loss 0.012372185476124287, R2 0.5234363079071045\n",
      "epoch 7030, loss 0.012374427169561386, R2 0.4856436848640442\n",
      "Eval loss 0.012372142635285854, R2 0.5234379768371582\n",
      "epoch 7031, loss 0.012374389916658401, R2 0.48564523458480835\n",
      "Eval loss 0.012372099794447422, R2 0.5234397649765015\n",
      "epoch 7032, loss 0.012374352663755417, R2 0.4856477379798889\n",
      "Eval loss 0.012372058816254139, R2 0.5234411954879761\n",
      "epoch 7033, loss 0.012374314479529858, R2 0.4856486916542053\n",
      "Eval loss 0.012372015975415707, R2 0.5234428644180298\n",
      "epoch 7034, loss 0.012374277226626873, R2 0.48565006256103516\n",
      "Eval loss 0.012371976859867573, R2 0.5234447717666626\n",
      "epoch 7035, loss 0.012374240905046463, R2 0.48565155267715454\n",
      "Eval loss 0.012371934950351715, R2 0.523446261882782\n",
      "epoch 7036, loss 0.012374202720820904, R2 0.4856531620025635\n",
      "Eval loss 0.012371892109513283, R2 0.5234476327896118\n",
      "epoch 7037, loss 0.012374166399240494, R2 0.48565465211868286\n",
      "Eval loss 0.012371850199997425, R2 0.5234492421150208\n",
      "epoch 7038, loss 0.012374128215014935, R2 0.4856564402580261\n",
      "Eval loss 0.012371808290481567, R2 0.5234508514404297\n",
      "epoch 7039, loss 0.01237409096211195, R2 0.48565781116485596\n",
      "Eval loss 0.012371768243610859, R2 0.5234525203704834\n",
      "epoch 7040, loss 0.01237405464053154, R2 0.48565948009490967\n",
      "Eval loss 0.012371727265417576, R2 0.5234541296958923\n",
      "epoch 7041, loss 0.01237401645630598, R2 0.48566102981567383\n",
      "Eval loss 0.012371682561933994, R2 0.5234557390213013\n",
      "epoch 7042, loss 0.012373978272080421, R2 0.48566246032714844\n",
      "Eval loss 0.012371642515063286, R2 0.5234575271606445\n",
      "epoch 7043, loss 0.012373941019177437, R2 0.4856641888618469\n",
      "Eval loss 0.012371601536870003, R2 0.52345871925354\n",
      "epoch 7044, loss 0.012373903766274452, R2 0.48566555976867676\n",
      "Eval loss 0.01237155869603157, R2 0.5234602689743042\n",
      "epoch 7045, loss 0.012373868376016617, R2 0.48566722869873047\n",
      "Eval loss 0.012371516786515713, R2 0.5234622955322266\n",
      "epoch 7046, loss 0.012373831123113632, R2 0.4856691360473633\n",
      "Eval loss 0.012371474876999855, R2 0.5234636664390564\n",
      "epoch 7047, loss 0.012373793870210648, R2 0.485670804977417\n",
      "Eval loss 0.012371433898806572, R2 0.5234655141830444\n",
      "epoch 7048, loss 0.012373755685985088, R2 0.4856717586517334\n",
      "Eval loss 0.012371393851935863, R2 0.523466944694519\n",
      "epoch 7049, loss 0.012373719364404678, R2 0.4856739044189453\n",
      "Eval loss 0.012371351011097431, R2 0.5234684348106384\n",
      "epoch 7050, loss 0.012373682111501694, R2 0.48567479848861694\n",
      "Eval loss 0.012371310032904148, R2 0.5234697461128235\n",
      "epoch 7051, loss 0.012373643927276134, R2 0.48567670583724976\n",
      "Eval loss 0.012371269054710865, R2 0.523471474647522\n",
      "epoch 7052, loss 0.012373607605695724, R2 0.48567819595336914\n",
      "Eval loss 0.012371228076517582, R2 0.52347332239151\n",
      "epoch 7053, loss 0.012373571284115314, R2 0.48567938804626465\n",
      "Eval loss 0.012371187098324299, R2 0.523474931716919\n",
      "epoch 7054, loss 0.012373533099889755, R2 0.4856816530227661\n",
      "Eval loss 0.012371145188808441, R2 0.5234764814376831\n",
      "epoch 7055, loss 0.012373496778309345, R2 0.4856831431388855\n",
      "Eval loss 0.012371104210615158, R2 0.5234782695770264\n",
      "epoch 7056, loss 0.01237345952540636, R2 0.48568403720855713\n",
      "Eval loss 0.012371063232421875, R2 0.5234798192977905\n",
      "epoch 7057, loss 0.01237342320382595, R2 0.48568636178970337\n",
      "Eval loss 0.012371021322906017, R2 0.5234813094139099\n",
      "epoch 7058, loss 0.012373385019600391, R2 0.48568761348724365\n",
      "Eval loss 0.01237097941339016, R2 0.523483157157898\n",
      "epoch 7059, loss 0.012373349629342556, R2 0.4856889843940735\n",
      "Eval loss 0.012370938435196877, R2 0.5234843492507935\n",
      "epoch 7060, loss 0.012373312376439571, R2 0.485690176486969\n",
      "Eval loss 0.012370897457003593, R2 0.5234862565994263\n",
      "epoch 7061, loss 0.012373276054859161, R2 0.48569220304489136\n",
      "Eval loss 0.01237085647881031, R2 0.5234876871109009\n",
      "epoch 7062, loss 0.012373239733278751, R2 0.48569369316101074\n",
      "Eval loss 0.012370815500617027, R2 0.523489236831665\n",
      "epoch 7063, loss 0.012373202480375767, R2 0.4856952428817749\n",
      "Eval loss 0.012370775453746319, R2 0.5234910249710083\n",
      "epoch 7064, loss 0.012373165227472782, R2 0.48569631576538086\n",
      "Eval loss 0.012370733544230461, R2 0.5234923958778381\n",
      "epoch 7065, loss 0.012373127974569798, R2 0.48569798469543457\n",
      "Eval loss 0.012370693497359753, R2 0.5234939455986023\n",
      "epoch 7066, loss 0.012373092584311962, R2 0.48569929599761963\n",
      "Eval loss 0.012370651587843895, R2 0.5234956741333008\n",
      "epoch 7067, loss 0.012373055331408978, R2 0.485701322555542\n",
      "Eval loss 0.012370611540973186, R2 0.5234971046447754\n",
      "epoch 7068, loss 0.012373019009828568, R2 0.4857022166252136\n",
      "Eval loss 0.012370570562779903, R2 0.5234984159469604\n",
      "epoch 7069, loss 0.012372981756925583, R2 0.485703706741333\n",
      "Eval loss 0.012370530515909195, R2 0.5235002040863037\n",
      "epoch 7070, loss 0.012372945435345173, R2 0.48570555448532104\n",
      "Eval loss 0.012370489537715912, R2 0.5235019326210022\n",
      "epoch 7071, loss 0.012372907251119614, R2 0.48570698499679565\n",
      "Eval loss 0.012370448559522629, R2 0.5235035419464111\n",
      "epoch 7072, loss 0.012372872792184353, R2 0.4857085943222046\n",
      "Eval loss 0.012370406650006771, R2 0.5235046744346619\n",
      "epoch 7073, loss 0.012372837401926517, R2 0.48570990562438965\n",
      "Eval loss 0.012370365671813488, R2 0.5235066413879395\n",
      "epoch 7074, loss 0.012372800149023533, R2 0.48571163415908813\n",
      "Eval loss 0.01237032376229763, R2 0.5235083103179932\n",
      "epoch 7075, loss 0.012372762896120548, R2 0.4857131838798523\n",
      "Eval loss 0.012370283715426922, R2 0.5235098600387573\n",
      "epoch 7076, loss 0.012372726574540138, R2 0.4857146739959717\n",
      "Eval loss 0.012370243668556213, R2 0.523511528968811\n",
      "epoch 7077, loss 0.012372690252959728, R2 0.4857160449028015\n",
      "Eval loss 0.012370201759040356, R2 0.5235129594802856\n",
      "epoch 7078, loss 0.012372653000056744, R2 0.48571741580963135\n",
      "Eval loss 0.012370161712169647, R2 0.5235143899917603\n",
      "epoch 7079, loss 0.012372616678476334, R2 0.48571908473968506\n",
      "Eval loss 0.012370121665298939, R2 0.5235159397125244\n",
      "epoch 7080, loss 0.012372581288218498, R2 0.48572057485580444\n",
      "Eval loss 0.01237008161842823, R2 0.523517370223999\n",
      "epoch 7081, loss 0.012372544966638088, R2 0.48572254180908203\n",
      "Eval loss 0.012370039708912373, R2 0.5235189199447632\n",
      "epoch 7082, loss 0.012372506782412529, R2 0.4857237935066223\n",
      "Eval loss 0.012370000593364239, R2 0.5235204696655273\n",
      "epoch 7083, loss 0.012372471392154694, R2 0.4857252836227417\n",
      "Eval loss 0.012369958683848381, R2 0.5235220789909363\n",
      "epoch 7084, loss 0.012372436001896858, R2 0.4857274293899536\n",
      "Eval loss 0.012369918636977673, R2 0.5235239267349243\n",
      "epoch 7085, loss 0.012372399680316448, R2 0.4857286214828491\n",
      "Eval loss 0.01236987765878439, R2 0.5235254764556885\n",
      "epoch 7086, loss 0.012372362427413464, R2 0.4857296347618103\n",
      "Eval loss 0.012369837611913681, R2 0.5235267877578735\n",
      "epoch 7087, loss 0.012372326105833054, R2 0.48573118448257446\n",
      "Eval loss 0.012369796633720398, R2 0.5235283374786377\n",
      "epoch 7088, loss 0.012372289784252644, R2 0.48573267459869385\n",
      "Eval loss 0.01236975658684969, R2 0.5235300064086914\n",
      "epoch 7089, loss 0.012372253462672234, R2 0.48573434352874756\n",
      "Eval loss 0.012369716539978981, R2 0.5235312581062317\n",
      "epoch 7090, loss 0.012372218072414398, R2 0.48573631048202515\n",
      "Eval loss 0.012369675561785698, R2 0.5235329866409302\n",
      "epoch 7091, loss 0.012372180819511414, R2 0.4857372045516968\n",
      "Eval loss 0.01236963551491499, R2 0.5235345363616943\n",
      "epoch 7092, loss 0.012372145429253578, R2 0.4857386350631714\n",
      "Eval loss 0.012369595468044281, R2 0.5235363245010376\n",
      "epoch 7093, loss 0.012372110038995743, R2 0.4857398271560669\n",
      "Eval loss 0.012369555421173573, R2 0.5235377550125122\n",
      "epoch 7094, loss 0.012372073717415333, R2 0.4857414960861206\n",
      "Eval loss 0.01236951444298029, R2 0.5235393047332764\n",
      "epoch 7095, loss 0.012372037395834923, R2 0.48574286699295044\n",
      "Eval loss 0.012369474396109581, R2 0.5235408544540405\n",
      "epoch 7096, loss 0.012372002936899662, R2 0.48574507236480713\n",
      "Eval loss 0.012369433417916298, R2 0.5235421657562256\n",
      "epoch 7097, loss 0.012371965683996677, R2 0.4857463240623474\n",
      "Eval loss 0.012369394302368164, R2 0.5235438346862793\n",
      "epoch 7098, loss 0.012371929362416267, R2 0.48574817180633545\n",
      "Eval loss 0.012369353324174881, R2 0.5235453844070435\n",
      "epoch 7099, loss 0.012371894903481007, R2 0.4857490658760071\n",
      "Eval loss 0.012369312345981598, R2 0.5235471129417419\n",
      "epoch 7100, loss 0.012371858581900597, R2 0.4857507348060608\n",
      "Eval loss 0.012369273230433464, R2 0.5235483646392822\n",
      "epoch 7101, loss 0.012371821328997612, R2 0.4857526421546936\n",
      "Eval loss 0.012369233183562756, R2 0.5235501527786255\n",
      "epoch 7102, loss 0.012371785938739777, R2 0.4857534170150757\n",
      "Eval loss 0.012369192205369473, R2 0.5235514640808105\n",
      "epoch 7103, loss 0.012371749617159367, R2 0.48575544357299805\n",
      "Eval loss 0.012369152158498764, R2 0.5235531330108643\n",
      "epoch 7104, loss 0.012371714226901531, R2 0.4857572317123413\n",
      "Eval loss 0.012369112111628056, R2 0.5235546827316284\n",
      "epoch 7105, loss 0.01237167976796627, R2 0.4857581853866577\n",
      "Eval loss 0.012369072064757347, R2 0.5235565304756165\n",
      "epoch 7106, loss 0.01237164344638586, R2 0.4857598543167114\n",
      "Eval loss 0.012369032949209213, R2 0.5235577821731567\n",
      "epoch 7107, loss 0.01237160712480545, R2 0.48576152324676514\n",
      "Eval loss 0.01236899383366108, R2 0.5235593914985657\n",
      "epoch 7108, loss 0.01237157080322504, R2 0.4857625365257263\n",
      "Eval loss 0.012368952855467796, R2 0.5235611200332642\n",
      "epoch 7109, loss 0.01237153448164463, R2 0.4857640862464905\n",
      "Eval loss 0.012368912808597088, R2 0.5235625505447388\n",
      "epoch 7110, loss 0.012371499091386795, R2 0.4857655167579651\n",
      "Eval loss 0.01236887276172638, R2 0.5235637426376343\n",
      "epoch 7111, loss 0.01237146370112896, R2 0.48576682806015015\n",
      "Eval loss 0.012368831783533096, R2 0.5235655307769775\n",
      "epoch 7112, loss 0.012371428310871124, R2 0.48576831817626953\n",
      "Eval loss 0.012368793599307537, R2 0.5235669612884521\n",
      "epoch 7113, loss 0.01237139105796814, R2 0.485770046710968\n",
      "Eval loss 0.012368753552436829, R2 0.5235689282417297\n",
      "epoch 7114, loss 0.012371356599032879, R2 0.48577141761779785\n",
      "Eval loss 0.01236871350556612, R2 0.5235704779624939\n",
      "epoch 7115, loss 0.012371320277452469, R2 0.485772967338562\n",
      "Eval loss 0.012368673458695412, R2 0.5235717296600342\n",
      "epoch 7116, loss 0.012371284887194633, R2 0.4857751131057739\n",
      "Eval loss 0.012368634343147278, R2 0.5235729217529297\n",
      "epoch 7117, loss 0.012371250428259373, R2 0.485775887966156\n",
      "Eval loss 0.012368593364953995, R2 0.5235748291015625\n",
      "epoch 7118, loss 0.012371215038001537, R2 0.48577749729156494\n",
      "Eval loss 0.012368555180728436, R2 0.5235761404037476\n",
      "epoch 7119, loss 0.012371179647743702, R2 0.4857789874076843\n",
      "Eval loss 0.012368514202535152, R2 0.5235775709152222\n",
      "epoch 7120, loss 0.012371143326163292, R2 0.4857802987098694\n",
      "Eval loss 0.012368474155664444, R2 0.5235795974731445\n",
      "epoch 7121, loss 0.012371107935905457, R2 0.4857819676399231\n",
      "Eval loss 0.01236843504011631, R2 0.5235804915428162\n",
      "epoch 7122, loss 0.012371072545647621, R2 0.48578375577926636\n",
      "Eval loss 0.012368394993245602, R2 0.5235824584960938\n",
      "epoch 7123, loss 0.012371037155389786, R2 0.48578470945358276\n",
      "Eval loss 0.012368355877697468, R2 0.5235836505889893\n",
      "epoch 7124, loss 0.01237100176513195, R2 0.48578619956970215\n",
      "Eval loss 0.012368316762149334, R2 0.5235852003097534\n",
      "epoch 7125, loss 0.012370966374874115, R2 0.4857887029647827\n",
      "Eval loss 0.012368276715278625, R2 0.5235867500305176\n",
      "epoch 7126, loss 0.012370931915938854, R2 0.4857889413833618\n",
      "Eval loss 0.012368236668407917, R2 0.5235885381698608\n",
      "epoch 7127, loss 0.012370895594358444, R2 0.48579078912734985\n",
      "Eval loss 0.012368197552859783, R2 0.5235897898674011\n",
      "epoch 7128, loss 0.012370861135423183, R2 0.48579204082489014\n",
      "Eval loss 0.01236815843731165, R2 0.5235913991928101\n",
      "epoch 7129, loss 0.012370825745165348, R2 0.4857935309410095\n",
      "Eval loss 0.01236811839044094, R2 0.5235929489135742\n",
      "epoch 7130, loss 0.012370789423584938, R2 0.48579519987106323\n",
      "Eval loss 0.012368078343570232, R2 0.5235946178436279\n",
      "epoch 7131, loss 0.012370754033327103, R2 0.48579633235931396\n",
      "Eval loss 0.012368038296699524, R2 0.5235960483551025\n",
      "epoch 7132, loss 0.012370719574391842, R2 0.4857982397079468\n",
      "Eval loss 0.012368000112473965, R2 0.5235973596572876\n",
      "epoch 7133, loss 0.012370683252811432, R2 0.4857999086380005\n",
      "Eval loss 0.012367960065603256, R2 0.5235990881919861\n",
      "epoch 7134, loss 0.012370648793876171, R2 0.48580092191696167\n",
      "Eval loss 0.012367920950055122, R2 0.5236004590988159\n",
      "epoch 7135, loss 0.012370613403618336, R2 0.4858025312423706\n",
      "Eval loss 0.012367880903184414, R2 0.5236020088195801\n",
      "epoch 7136, loss 0.012370578944683075, R2 0.4858037829399109\n",
      "Eval loss 0.012367842718958855, R2 0.5236034393310547\n",
      "epoch 7137, loss 0.01237054355442524, R2 0.4858052730560303\n",
      "Eval loss 0.01236780360341072, R2 0.5236049890518188\n",
      "epoch 7138, loss 0.012370508164167404, R2 0.4858068823814392\n",
      "Eval loss 0.012367762625217438, R2 0.5236067771911621\n",
      "epoch 7139, loss 0.012370471842586994, R2 0.48580873012542725\n",
      "Eval loss 0.012367725372314453, R2 0.5236079692840576\n",
      "epoch 7140, loss 0.012370437383651733, R2 0.48580968379974365\n",
      "Eval loss 0.012367685325443745, R2 0.5236096382141113\n",
      "epoch 7141, loss 0.012370401993393898, R2 0.48581093549728394\n",
      "Eval loss 0.012367647141218185, R2 0.523611307144165\n",
      "epoch 7142, loss 0.012370366603136063, R2 0.485812246799469\n",
      "Eval loss 0.012367607094347477, R2 0.5236126780509949\n",
      "epoch 7143, loss 0.012370333075523376, R2 0.48581451177597046\n",
      "Eval loss 0.012367568910121918, R2 0.5236142873764038\n",
      "epoch 7144, loss 0.012370297685265541, R2 0.4858156442642212\n",
      "Eval loss 0.012367527931928635, R2 0.5236154794692993\n",
      "epoch 7145, loss 0.012370262295007706, R2 0.48581695556640625\n",
      "Eval loss 0.0123674888163805, R2 0.5236170887947083\n",
      "epoch 7146, loss 0.012370227836072445, R2 0.48581838607788086\n",
      "Eval loss 0.012367450632154942, R2 0.5236185789108276\n",
      "epoch 7147, loss 0.012370193377137184, R2 0.48581981658935547\n",
      "Eval loss 0.012367410585284233, R2 0.5236201286315918\n",
      "epoch 7148, loss 0.012370157055556774, R2 0.48582130670547485\n",
      "Eval loss 0.0123673714697361, R2 0.5236217379570007\n",
      "epoch 7149, loss 0.012370122596621513, R2 0.48582321405410767\n",
      "Eval loss 0.012367332354187965, R2 0.5236231088638306\n",
      "epoch 7150, loss 0.012370088137686253, R2 0.4858241677284241\n",
      "Eval loss 0.012367294169962406, R2 0.5236248970031738\n",
      "epoch 7151, loss 0.012370053678750992, R2 0.48582565784454346\n",
      "Eval loss 0.012367254123091698, R2 0.5236263871192932\n",
      "epoch 7152, loss 0.012370020151138306, R2 0.4858270287513733\n",
      "Eval loss 0.012367215938866138, R2 0.5236278772354126\n",
      "epoch 7153, loss 0.012369983829557896, R2 0.4858285188674927\n",
      "Eval loss 0.012367176823318005, R2 0.5236297845840454\n",
      "epoch 7154, loss 0.012369949370622635, R2 0.4858301281929016\n",
      "Eval loss 0.01236713770776987, R2 0.5236307382583618\n",
      "epoch 7155, loss 0.012369914911687374, R2 0.4858318567276001\n",
      "Eval loss 0.012367098592221737, R2 0.5236321091651917\n",
      "epoch 7156, loss 0.012369879521429539, R2 0.4858335256576538\n",
      "Eval loss 0.012367060407996178, R2 0.5236338376998901\n",
      "epoch 7157, loss 0.012369844131171703, R2 0.4858344793319702\n",
      "Eval loss 0.012367021292448044, R2 0.5236350893974304\n",
      "epoch 7158, loss 0.012369809672236443, R2 0.4858357310295105\n",
      "Eval loss 0.012366983108222485, R2 0.5236369371414185\n",
      "epoch 7159, loss 0.012369776144623756, R2 0.4858371615409851\n",
      "Eval loss 0.01236694399267435, R2 0.5236380696296692\n",
      "epoch 7160, loss 0.012369740754365921, R2 0.485839307308197\n",
      "Eval loss 0.012366904877126217, R2 0.5236398577690125\n",
      "epoch 7161, loss 0.01236970629543066, R2 0.4858400821685791\n",
      "Eval loss 0.012366865761578083, R2 0.5236411094665527\n",
      "epoch 7162, loss 0.012369670905172825, R2 0.4858415126800537\n",
      "Eval loss 0.01236682664602995, R2 0.5236427187919617\n",
      "epoch 7163, loss 0.012369637377560139, R2 0.4858434200286865\n",
      "Eval loss 0.01236678846180439, R2 0.5236443281173706\n",
      "epoch 7164, loss 0.012369601987302303, R2 0.48584502935409546\n",
      "Eval loss 0.012366749346256256, R2 0.5236459970474243\n",
      "epoch 7165, loss 0.012369567528367043, R2 0.48584598302841187\n",
      "Eval loss 0.012366710230708122, R2 0.5236473083496094\n",
      "epoch 7166, loss 0.012369534000754356, R2 0.48584723472595215\n",
      "Eval loss 0.012366672977805138, R2 0.5236487984657288\n",
      "epoch 7167, loss 0.012369498610496521, R2 0.48584872484207153\n",
      "Eval loss 0.012366634793579578, R2 0.5236502885818481\n",
      "epoch 7168, loss 0.01236946415156126, R2 0.4858502745628357\n",
      "Eval loss 0.01236659660935402, R2 0.5236517190933228\n",
      "epoch 7169, loss 0.012369429692626, R2 0.48585206270217896\n",
      "Eval loss 0.01236655656248331, R2 0.5236537456512451\n",
      "epoch 7170, loss 0.012369395233690739, R2 0.4858529567718506\n",
      "Eval loss 0.012366518378257751, R2 0.5236544609069824\n",
      "epoch 7171, loss 0.012369360774755478, R2 0.4858543872833252\n",
      "Eval loss 0.012366480194032192, R2 0.5236557722091675\n",
      "epoch 7172, loss 0.012369325384497643, R2 0.4858560562133789\n",
      "Eval loss 0.012366443872451782, R2 0.5236577987670898\n",
      "epoch 7173, loss 0.012369291856884956, R2 0.4858572483062744\n",
      "Eval loss 0.012366403825581074, R2 0.523659348487854\n",
      "epoch 7174, loss 0.012369257397949696, R2 0.485859215259552\n",
      "Eval loss 0.012366365641355515, R2 0.5236605405807495\n",
      "epoch 7175, loss 0.012369224801659584, R2 0.4858602285385132\n",
      "Eval loss 0.012366325594484806, R2 0.5236622095108032\n",
      "epoch 7176, loss 0.012369189411401749, R2 0.48586153984069824\n",
      "Eval loss 0.012366287410259247, R2 0.5236640572547913\n",
      "epoch 7177, loss 0.012369155883789062, R2 0.4858630895614624\n",
      "Eval loss 0.012366249226033688, R2 0.5236651301383972\n",
      "epoch 7178, loss 0.012369120493531227, R2 0.48586505651474\n",
      "Eval loss 0.012366211041808128, R2 0.5236666202545166\n",
      "epoch 7179, loss 0.012369086034595966, R2 0.4858660101890564\n",
      "Eval loss 0.012366172857582569, R2 0.5236679315567017\n",
      "epoch 7180, loss 0.012369051575660706, R2 0.4858672618865967\n",
      "Eval loss 0.01236613467335701, R2 0.5236695408821106\n",
      "epoch 7181, loss 0.01236901804804802, R2 0.4858688712120056\n",
      "Eval loss 0.01236609648913145, R2 0.5236711502075195\n",
      "epoch 7182, loss 0.012368983589112759, R2 0.4858701229095459\n",
      "Eval loss 0.012366059236228466, R2 0.5236724615097046\n",
      "epoch 7183, loss 0.012368949130177498, R2 0.4858715534210205\n",
      "Eval loss 0.012366018258035183, R2 0.5236741304397583\n",
      "epoch 7184, loss 0.012368914671242237, R2 0.4858734607696533\n",
      "Eval loss 0.012365981005132198, R2 0.5236756801605225\n",
      "epoch 7185, loss 0.012368882074952126, R2 0.4858747720718384\n",
      "Eval loss 0.012365942820906639, R2 0.5236770510673523\n",
      "epoch 7186, loss 0.01236884668469429, R2 0.48587578535079956\n",
      "Eval loss 0.01236590463668108, R2 0.5236779451370239\n",
      "epoch 7187, loss 0.012368813157081604, R2 0.4858771562576294\n",
      "Eval loss 0.012365865521132946, R2 0.5236797332763672\n",
      "epoch 7188, loss 0.012368777766823769, R2 0.4858786463737488\n",
      "Eval loss 0.012365828268229961, R2 0.5236810445785522\n",
      "epoch 7189, loss 0.012368745170533657, R2 0.4858804941177368\n",
      "Eval loss 0.012365790084004402, R2 0.5236829519271851\n",
      "epoch 7190, loss 0.012368711642920971, R2 0.4858812689781189\n",
      "Eval loss 0.012365751899778843, R2 0.5236843824386597\n",
      "epoch 7191, loss 0.01236867718398571, R2 0.4858826994895935\n",
      "Eval loss 0.012365714646875858, R2 0.5236856937408447\n",
      "epoch 7192, loss 0.012368643656373024, R2 0.48588424921035767\n",
      "Eval loss 0.012365676462650299, R2 0.5236871242523193\n",
      "epoch 7193, loss 0.012368609197437763, R2 0.48588627576828003\n",
      "Eval loss 0.01236563827842474, R2 0.523688793182373\n",
      "epoch 7194, loss 0.012368573807179928, R2 0.4858872890472412\n",
      "Eval loss 0.01236560009419918, R2 0.523690402507782\n",
      "epoch 7195, loss 0.012368542142212391, R2 0.4858884811401367\n",
      "Eval loss 0.012365562841296196, R2 0.5236915349960327\n",
      "epoch 7196, loss 0.01236850768327713, R2 0.4858902096748352\n",
      "Eval loss 0.012365524657070637, R2 0.523693323135376\n",
      "epoch 7197, loss 0.012368474155664444, R2 0.48589128255844116\n",
      "Eval loss 0.012365487404167652, R2 0.5236943364143372\n",
      "epoch 7198, loss 0.012368440628051758, R2 0.485892653465271\n",
      "Eval loss 0.012365448288619518, R2 0.523695707321167\n",
      "epoch 7199, loss 0.012368406169116497, R2 0.4858940839767456\n",
      "Eval loss 0.012365411035716534, R2 0.523697555065155\n",
      "epoch 7200, loss 0.012368372641503811, R2 0.48589545488357544\n",
      "Eval loss 0.012365372851490974, R2 0.5236988663673401\n",
      "epoch 7201, loss 0.012368337251245975, R2 0.48589664697647095\n",
      "Eval loss 0.012365334667265415, R2 0.5237005949020386\n",
      "epoch 7202, loss 0.012368305586278439, R2 0.48589831590652466\n",
      "Eval loss 0.012365296483039856, R2 0.5237019658088684\n",
      "epoch 7203, loss 0.012368272058665752, R2 0.485900342464447\n",
      "Eval loss 0.012365258298814297, R2 0.5237032771110535\n",
      "epoch 7204, loss 0.012368236668407917, R2 0.4859011769294739\n",
      "Eval loss 0.012365221045911312, R2 0.5237047076225281\n",
      "epoch 7205, loss 0.01236820314079523, R2 0.4859025478363037\n",
      "Eval loss 0.012365182861685753, R2 0.5237060785293579\n",
      "epoch 7206, loss 0.012368169613182545, R2 0.4859037399291992\n",
      "Eval loss 0.012365145608782768, R2 0.5237078666687012\n",
      "epoch 7207, loss 0.012368136085569859, R2 0.48590612411499023\n",
      "Eval loss 0.012365108355879784, R2 0.5237094759941101\n",
      "epoch 7208, loss 0.012368101626634598, R2 0.485906720161438\n",
      "Eval loss 0.012365070171654224, R2 0.5237108469009399\n",
      "epoch 7209, loss 0.012368068099021912, R2 0.4859086275100708\n",
      "Eval loss 0.01236503291875124, R2 0.5237116813659668\n",
      "epoch 7210, loss 0.012368034571409225, R2 0.48591017723083496\n",
      "Eval loss 0.01236499473452568, R2 0.5237134099006653\n",
      "epoch 7211, loss 0.01236800104379654, R2 0.48591095209121704\n",
      "Eval loss 0.012364957481622696, R2 0.5237150192260742\n",
      "epoch 7212, loss 0.012367967516183853, R2 0.4859123229980469\n",
      "Eval loss 0.012364919297397137, R2 0.5237163305282593\n",
      "epoch 7213, loss 0.012367933988571167, R2 0.4859136939048767\n",
      "Eval loss 0.012364882044494152, R2 0.5237176418304443\n",
      "epoch 7214, loss 0.01236790046095848, R2 0.485914945602417\n",
      "Eval loss 0.012364843860268593, R2 0.5237189531326294\n",
      "epoch 7215, loss 0.012367866933345795, R2 0.4859168529510498\n",
      "Eval loss 0.012364807538688183, R2 0.523720383644104\n",
      "epoch 7216, loss 0.012367834337055683, R2 0.4859180450439453\n",
      "Eval loss 0.012364768423140049, R2 0.5237221717834473\n",
      "epoch 7217, loss 0.012367798946797848, R2 0.4859195351600647\n",
      "Eval loss 0.012364733032882214, R2 0.5237234830856323\n",
      "epoch 7218, loss 0.012367766350507736, R2 0.4859212040901184\n",
      "Eval loss 0.012364694848656654, R2 0.5237252712249756\n",
      "epoch 7219, loss 0.01236773282289505, R2 0.48592209815979004\n",
      "Eval loss 0.012364656664431095, R2 0.5237267017364502\n",
      "epoch 7220, loss 0.012367699295282364, R2 0.4859234690666199\n",
      "Eval loss 0.01236461941152811, R2 0.5237276554107666\n",
      "epoch 7221, loss 0.012367667630314827, R2 0.4859251379966736\n",
      "Eval loss 0.012364582158625126, R2 0.5237292051315308\n",
      "epoch 7222, loss 0.012367633171379566, R2 0.4859267473220825\n",
      "Eval loss 0.012364543974399567, R2 0.5237305164337158\n",
      "epoch 7223, loss 0.012367600575089455, R2 0.4859280586242676\n",
      "Eval loss 0.012364507652819157, R2 0.5237323045730591\n",
      "epoch 7224, loss 0.012367566116154194, R2 0.485929012298584\n",
      "Eval loss 0.012364471331238747, R2 0.5237337350845337\n",
      "epoch 7225, loss 0.012367533519864082, R2 0.4859303832054138\n",
      "Eval loss 0.012364432215690613, R2 0.5237351059913635\n",
      "epoch 7226, loss 0.012367499060928822, R2 0.4859321117401123\n",
      "Eval loss 0.012364394962787628, R2 0.5237365365028381\n",
      "epoch 7227, loss 0.012367465533316135, R2 0.48593318462371826\n",
      "Eval loss 0.012364356778562069, R2 0.5237378478050232\n",
      "epoch 7228, loss 0.012367433868348598, R2 0.48593437671661377\n",
      "Eval loss 0.012364321388304234, R2 0.5237393379211426\n",
      "epoch 7229, loss 0.012367401272058487, R2 0.48593586683273315\n",
      "Eval loss 0.0123642822727561, R2 0.5237410068511963\n",
      "epoch 7230, loss 0.012367366813123226, R2 0.48593777418136597\n",
      "Eval loss 0.012364245019853115, R2 0.5237424373626709\n",
      "epoch 7231, loss 0.01236733514815569, R2 0.4859389066696167\n",
      "Eval loss 0.012364210560917854, R2 0.5237438082695007\n",
      "epoch 7232, loss 0.012367299757897854, R2 0.48594021797180176\n",
      "Eval loss 0.01236417330801487, R2 0.5237447023391724\n",
      "epoch 7233, loss 0.012367267161607742, R2 0.48594123125076294\n",
      "Eval loss 0.012364136055111885, R2 0.52374666929245\n",
      "epoch 7234, loss 0.012367235496640205, R2 0.4859427809715271\n",
      "Eval loss 0.012364097870886326, R2 0.5237481594085693\n",
      "epoch 7235, loss 0.01236720196902752, R2 0.48594415187835693\n",
      "Eval loss 0.012364061549305916, R2 0.5237495303153992\n",
      "epoch 7236, loss 0.012367168441414833, R2 0.4859461784362793\n",
      "Eval loss 0.012364023365080357, R2 0.5237511396408081\n",
      "epoch 7237, loss 0.012367134913802147, R2 0.4859469532966614\n",
      "Eval loss 0.012363986112177372, R2 0.5237524509429932\n",
      "epoch 7238, loss 0.01236710138618946, R2 0.48594897985458374\n",
      "Eval loss 0.012363948859274387, R2 0.5237538814544678\n",
      "epoch 7239, loss 0.012367067858576775, R2 0.4859495162963867\n",
      "Eval loss 0.012363911606371403, R2 0.5237550139427185\n",
      "epoch 7240, loss 0.012367036193609238, R2 0.48595088720321655\n",
      "Eval loss 0.012363876216113567, R2 0.5237562656402588\n",
      "epoch 7241, loss 0.012367002665996552, R2 0.4859529137611389\n",
      "Eval loss 0.012363838031888008, R2 0.5237579941749573\n",
      "epoch 7242, loss 0.01236697006970644, R2 0.48595380783081055\n",
      "Eval loss 0.012363801710307598, R2 0.5237593650817871\n",
      "epoch 7243, loss 0.012366936542093754, R2 0.4859553575515747\n",
      "Eval loss 0.012363763526082039, R2 0.523760974407196\n",
      "epoch 7244, loss 0.012366903945803642, R2 0.4859565496444702\n",
      "Eval loss 0.012363727204501629, R2 0.5237623453140259\n",
      "epoch 7245, loss 0.012366869486868382, R2 0.4859579801559448\n",
      "Eval loss 0.012363689951598644, R2 0.5237639546394348\n",
      "epoch 7246, loss 0.012366837821900845, R2 0.4859592914581299\n",
      "Eval loss 0.012363654561340809, R2 0.5237650871276855\n",
      "epoch 7247, loss 0.012366805225610733, R2 0.48596084117889404\n",
      "Eval loss 0.012363617308437824, R2 0.5237665176391602\n",
      "epoch 7248, loss 0.012366773560643196, R2 0.4859620928764343\n",
      "Eval loss 0.01236358005553484, R2 0.5237676501274109\n",
      "epoch 7249, loss 0.01236674003303051, R2 0.4859633445739746\n",
      "Eval loss 0.01236354373395443, R2 0.5237690210342407\n",
      "epoch 7250, loss 0.012366706505417824, R2 0.48596471548080444\n",
      "Eval loss 0.01236350741237402, R2 0.5237705707550049\n",
      "epoch 7251, loss 0.012366675771772861, R2 0.4859658479690552\n",
      "Eval loss 0.012363470159471035, R2 0.5237718820571899\n",
      "epoch 7252, loss 0.012366642244160175, R2 0.4859674572944641\n",
      "Eval loss 0.012363433837890625, R2 0.5237733125686646\n",
      "epoch 7253, loss 0.012366609647870064, R2 0.4859689474105835\n",
      "Eval loss 0.012363395653665066, R2 0.5237754583358765\n",
      "epoch 7254, loss 0.012366577051579952, R2 0.4859706163406372\n",
      "Eval loss 0.01236336026340723, R2 0.523776113986969\n",
      "epoch 7255, loss 0.012366542592644691, R2 0.48597168922424316\n",
      "Eval loss 0.012363323010504246, R2 0.5237778425216675\n",
      "epoch 7256, loss 0.01236650999635458, R2 0.485973060131073\n",
      "Eval loss 0.012363286688923836, R2 0.5237792134284973\n",
      "epoch 7257, loss 0.012366478331387043, R2 0.48597443103790283\n",
      "Eval loss 0.012363250367343426, R2 0.5237805843353271\n",
      "epoch 7258, loss 0.012366445735096931, R2 0.4859757423400879\n",
      "Eval loss 0.012363212183117867, R2 0.5237822532653809\n",
      "epoch 7259, loss 0.01236641313880682, R2 0.4859774112701416\n",
      "Eval loss 0.012363176792860031, R2 0.5237832069396973\n",
      "epoch 7260, loss 0.012366380542516708, R2 0.48597830533981323\n",
      "Eval loss 0.012363139539957047, R2 0.523784875869751\n",
      "epoch 7261, loss 0.012366347946226597, R2 0.4859796166419983\n",
      "Eval loss 0.012363104149699211, R2 0.5237864255905151\n",
      "epoch 7262, loss 0.01236631628125906, R2 0.48598116636276245\n",
      "Eval loss 0.012363067828118801, R2 0.5237876772880554\n",
      "epoch 7263, loss 0.012366282753646374, R2 0.48598283529281616\n",
      "Eval loss 0.012363030575215816, R2 0.5237892270088196\n",
      "epoch 7264, loss 0.012366250157356262, R2 0.4859837293624878\n",
      "Eval loss 0.012362993322312832, R2 0.5237906575202942\n",
      "epoch 7265, loss 0.01236621756106615, R2 0.48598504066467285\n",
      "Eval loss 0.012362957000732422, R2 0.5237919092178345\n",
      "epoch 7266, loss 0.012366184964776039, R2 0.4859864115715027\n",
      "Eval loss 0.012362920679152012, R2 0.5237934589385986\n",
      "epoch 7267, loss 0.012366152368485928, R2 0.4859877824783325\n",
      "Eval loss 0.012362885288894176, R2 0.523794412612915\n",
      "epoch 7268, loss 0.01236612070351839, R2 0.4859890937805176\n",
      "Eval loss 0.012362848035991192, R2 0.5237960815429688\n",
      "epoch 7269, loss 0.012366088107228279, R2 0.4859909415245056\n",
      "Eval loss 0.012362810783088207, R2 0.523797869682312\n",
      "epoch 7270, loss 0.012366055510938168, R2 0.48599231243133545\n",
      "Eval loss 0.012362774461507797, R2 0.523798942565918\n",
      "epoch 7271, loss 0.01236602384597063, R2 0.4859939217567444\n",
      "Eval loss 0.012362740933895111, R2 0.523800253868103\n",
      "epoch 7272, loss 0.012365990318357944, R2 0.48599451780319214\n",
      "Eval loss 0.012362702749669552, R2 0.5238017439842224\n",
      "epoch 7273, loss 0.012365958653390408, R2 0.4859960079193115\n",
      "Eval loss 0.012362666428089142, R2 0.5238032341003418\n",
      "epoch 7274, loss 0.01236592698842287, R2 0.48599761724472046\n",
      "Eval loss 0.012362630106508732, R2 0.5238044261932373\n",
      "epoch 7275, loss 0.012365893460810184, R2 0.48599857091903687\n",
      "Eval loss 0.012362593784928322, R2 0.5238057374954224\n",
      "epoch 7276, loss 0.012365860864520073, R2 0.4859998822212219\n",
      "Eval loss 0.012362556532025337, R2 0.5238075256347656\n",
      "epoch 7277, loss 0.012365828268229961, R2 0.48600107431411743\n",
      "Eval loss 0.012362522073090076, R2 0.5238087177276611\n",
      "epoch 7278, loss 0.012365797534584999, R2 0.48600250482559204\n",
      "Eval loss 0.012362485751509666, R2 0.5238100290298462\n",
      "epoch 7279, loss 0.012365764938294888, R2 0.48600471019744873\n",
      "Eval loss 0.012362447567284107, R2 0.5238116979598999\n",
      "epoch 7280, loss 0.012365732342004776, R2 0.4860052466392517\n",
      "Eval loss 0.012362413108348846, R2 0.5238125920295715\n",
      "epoch 7281, loss 0.012365699745714664, R2 0.48600709438323975\n",
      "Eval loss 0.012362376786768436, R2 0.5238139629364014\n",
      "epoch 7282, loss 0.012365667149424553, R2 0.48600828647613525\n",
      "Eval loss 0.012362339533865452, R2 0.5238156914710999\n",
      "epoch 7283, loss 0.012365635484457016, R2 0.48600971698760986\n",
      "Eval loss 0.012362305074930191, R2 0.5238171815872192\n",
      "epoch 7284, loss 0.012365604750812054, R2 0.4860103726387024\n",
      "Eval loss 0.012362269684672356, R2 0.5238181352615356\n",
      "epoch 7285, loss 0.012365571223199368, R2 0.4860121011734009\n",
      "Eval loss 0.012362233363091946, R2 0.5238198041915894\n",
      "epoch 7286, loss 0.01236553955823183, R2 0.4860137701034546\n",
      "Eval loss 0.012362197041511536, R2 0.5238213539123535\n",
      "epoch 7287, loss 0.012365506961941719, R2 0.486015260219574\n",
      "Eval loss 0.012362158857285976, R2 0.5238227844238281\n",
      "epoch 7288, loss 0.012365475296974182, R2 0.48601657152175903\n",
      "Eval loss 0.012362124398350716, R2 0.5238237380981445\n",
      "epoch 7289, loss 0.01236544456332922, R2 0.48601704835891724\n",
      "Eval loss 0.01236208900809288, R2 0.5238255262374878\n",
      "epoch 7290, loss 0.012365411035716534, R2 0.4860185980796814\n",
      "Eval loss 0.01236205268651247, R2 0.5238265991210938\n",
      "epoch 7291, loss 0.012365379370748997, R2 0.48602038621902466\n",
      "Eval loss 0.012362017296254635, R2 0.523828387260437\n",
      "epoch 7292, loss 0.01236534770578146, R2 0.4860212206840515\n",
      "Eval loss 0.012361980974674225, R2 0.5238295197486877\n",
      "epoch 7293, loss 0.012365314178168774, R2 0.48602259159088135\n",
      "Eval loss 0.01236194558441639, R2 0.5238306522369385\n",
      "epoch 7294, loss 0.012365283444523811, R2 0.4860239028930664\n",
      "Eval loss 0.01236190926283598, R2 0.5238323211669922\n",
      "epoch 7295, loss 0.012365251779556274, R2 0.48602521419525146\n",
      "Eval loss 0.012361873872578144, R2 0.5238333940505981\n",
      "epoch 7296, loss 0.012365219183266163, R2 0.4860267639160156\n",
      "Eval loss 0.012361837550997734, R2 0.523835301399231\n",
      "epoch 7297, loss 0.0123651884496212, R2 0.4860283136367798\n",
      "Eval loss 0.012361802160739899, R2 0.5238365530967712\n",
      "epoch 7298, loss 0.012365155853331089, R2 0.4860296845436096\n",
      "Eval loss 0.012361765839159489, R2 0.5238379240036011\n",
      "epoch 7299, loss 0.012365124188363552, R2 0.4860306978225708\n",
      "Eval loss 0.012361730448901653, R2 0.5238394737243652\n",
      "epoch 7300, loss 0.01236509345471859, R2 0.4860319495201111\n",
      "Eval loss 0.012361694127321243, R2 0.5238405466079712\n",
      "epoch 7301, loss 0.012365059927105904, R2 0.48603367805480957\n",
      "Eval loss 0.012361657805740833, R2 0.5238420963287354\n",
      "epoch 7302, loss 0.012365029193460941, R2 0.48603445291519165\n",
      "Eval loss 0.012361622415482998, R2 0.5238432884216309\n",
      "epoch 7303, loss 0.012364995665848255, R2 0.486036479473114\n",
      "Eval loss 0.012361587956547737, R2 0.523844838142395\n",
      "epoch 7304, loss 0.012364965863525867, R2 0.4860377907752991\n",
      "Eval loss 0.012361553497612476, R2 0.5238460302352905\n",
      "epoch 7305, loss 0.01236493419855833, R2 0.4860384464263916\n",
      "Eval loss 0.012361517176032066, R2 0.5238474011421204\n",
      "epoch 7306, loss 0.012364902533590794, R2 0.48603975772857666\n",
      "Eval loss 0.012361480854451656, R2 0.5238488912582397\n",
      "epoch 7307, loss 0.012364870868623257, R2 0.4860410690307617\n",
      "Eval loss 0.012361444532871246, R2 0.523849606513977\n",
      "epoch 7308, loss 0.012364838272333145, R2 0.4860423803329468\n",
      "Eval loss 0.012361410073935986, R2 0.5238515138626099\n",
      "epoch 7309, loss 0.012364806607365608, R2 0.4860437512397766\n",
      "Eval loss 0.01236137468367815, R2 0.5238527059555054\n",
      "epoch 7310, loss 0.012364775873720646, R2 0.4860450029373169\n",
      "Eval loss 0.01236133836209774, R2 0.52385413646698\n",
      "epoch 7311, loss 0.012364743277430534, R2 0.48604637384414673\n",
      "Eval loss 0.01236130390316248, R2 0.5238553285598755\n",
      "epoch 7312, loss 0.012364712543785572, R2 0.486047625541687\n",
      "Eval loss 0.01236126758158207, R2 0.5238569974899292\n",
      "epoch 7313, loss 0.012364680878818035, R2 0.48604893684387207\n",
      "Eval loss 0.01236123126000166, R2 0.5238584280014038\n",
      "epoch 7314, loss 0.012364650145173073, R2 0.48605072498321533\n",
      "Eval loss 0.012361196801066399, R2 0.5238595008850098\n",
      "epoch 7315, loss 0.012364617548882961, R2 0.4860515594482422\n",
      "Eval loss 0.012361161410808563, R2 0.5238612294197083\n",
      "epoch 7316, loss 0.012364586815237999, R2 0.4860529899597168\n",
      "Eval loss 0.012361125089228153, R2 0.5238627791404724\n",
      "epoch 7317, loss 0.012364555150270462, R2 0.48605430126190186\n",
      "Eval loss 0.012361090630292892, R2 0.5238639116287231\n",
      "epoch 7318, loss 0.012364523485302925, R2 0.4860556125640869\n",
      "Eval loss 0.012361055240035057, R2 0.5238653421401978\n",
      "epoch 7319, loss 0.012364492751657963, R2 0.4860568046569824\n",
      "Eval loss 0.012361019849777222, R2 0.5238666534423828\n",
      "epoch 7320, loss 0.012364460155367851, R2 0.4860585927963257\n",
      "Eval loss 0.012360984459519386, R2 0.5238680243492126\n",
      "epoch 7321, loss 0.012364428490400314, R2 0.4860592484474182\n",
      "Eval loss 0.012360948137938976, R2 0.5238692760467529\n",
      "epoch 7322, loss 0.012364396825432777, R2 0.4860609173774719\n",
      "Eval loss 0.01236091461032629, R2 0.5238703489303589\n",
      "epoch 7323, loss 0.012364366091787815, R2 0.4860626459121704\n",
      "Eval loss 0.012360879220068455, R2 0.5238719582557678\n",
      "epoch 7324, loss 0.012364335358142853, R2 0.48606348037719727\n",
      "Eval loss 0.012360842898488045, R2 0.5238736867904663\n",
      "epoch 7325, loss 0.012364303693175316, R2 0.4860650897026062\n",
      "Eval loss 0.012360809370875359, R2 0.5238744020462036\n",
      "epoch 7326, loss 0.012364272028207779, R2 0.4860661029815674\n",
      "Eval loss 0.012360773049294949, R2 0.5238759517669678\n",
      "epoch 7327, loss 0.012364240363240242, R2 0.4860672354698181\n",
      "Eval loss 0.012360737659037113, R2 0.5238773822784424\n",
      "epoch 7328, loss 0.012364211492240429, R2 0.4860689640045166\n",
      "Eval loss 0.012360702268779278, R2 0.5238789319992065\n",
      "epoch 7329, loss 0.012364178895950317, R2 0.48606979846954346\n",
      "Eval loss 0.012360666878521442, R2 0.5238802433013916\n",
      "epoch 7330, loss 0.01236414723098278, R2 0.4860711097717285\n",
      "Eval loss 0.012360634282231331, R2 0.5238811373710632\n",
      "epoch 7331, loss 0.012364115566015244, R2 0.4860724210739136\n",
      "Eval loss 0.012360598891973495, R2 0.5238825082778931\n",
      "epoch 7332, loss 0.012364084832370281, R2 0.48607373237609863\n",
      "Eval loss 0.012360562570393085, R2 0.5238840579986572\n",
      "epoch 7333, loss 0.012364053167402744, R2 0.48607486486434937\n",
      "Eval loss 0.012360528111457825, R2 0.5238853693008423\n",
      "epoch 7334, loss 0.012364023365080357, R2 0.48607611656188965\n",
      "Eval loss 0.012360491789877415, R2 0.5238872766494751\n",
      "epoch 7335, loss 0.012363990768790245, R2 0.48607760667800903\n",
      "Eval loss 0.01236045639961958, R2 0.523888349533081\n",
      "epoch 7336, loss 0.012363959103822708, R2 0.4860790967941284\n",
      "Eval loss 0.012360421940684319, R2 0.5238895416259766\n",
      "epoch 7337, loss 0.01236392930150032, R2 0.4860803484916687\n",
      "Eval loss 0.012360386550426483, R2 0.5238907933235168\n",
      "epoch 7338, loss 0.012363898567855358, R2 0.48608165979385376\n",
      "Eval loss 0.012360352091491222, R2 0.5238922834396362\n",
      "epoch 7339, loss 0.012363867834210396, R2 0.4860832095146179\n",
      "Eval loss 0.012360316701233387, R2 0.5238936543464661\n",
      "epoch 7340, loss 0.012363836169242859, R2 0.4860840439796448\n",
      "Eval loss 0.012360284104943275, R2 0.5238949060440063\n",
      "epoch 7341, loss 0.012363803572952747, R2 0.4860854148864746\n",
      "Eval loss 0.01236024871468544, R2 0.5238962173461914\n",
      "epoch 7342, loss 0.012363774701952934, R2 0.48608678579330444\n",
      "Eval loss 0.012360213324427605, R2 0.5238972902297974\n",
      "epoch 7343, loss 0.012363743036985397, R2 0.4860885739326477\n",
      "Eval loss 0.01236017793416977, R2 0.523898720741272\n",
      "epoch 7344, loss 0.01236371137201786, R2 0.4860890507698059\n",
      "Eval loss 0.012360143475234509, R2 0.5239002704620361\n",
      "epoch 7345, loss 0.012363681569695473, R2 0.4860907793045044\n",
      "Eval loss 0.012360108084976673, R2 0.523901641368866\n",
      "epoch 7346, loss 0.012363648973405361, R2 0.4860917925834656\n",
      "Eval loss 0.012360073626041412, R2 0.5239030122756958\n",
      "epoch 7347, loss 0.012363618239760399, R2 0.48609328269958496\n",
      "Eval loss 0.012360039167106152, R2 0.5239040851593018\n",
      "epoch 7348, loss 0.012363588437438011, R2 0.4860943555831909\n",
      "Eval loss 0.012360003776848316, R2 0.5239056944847107\n",
      "epoch 7349, loss 0.012363557703793049, R2 0.4860960841178894\n",
      "Eval loss 0.01235997024923563, R2 0.5239071249961853\n",
      "epoch 7350, loss 0.012363526970148087, R2 0.48609739542007446\n",
      "Eval loss 0.01235993579030037, R2 0.5239084362983704\n",
      "epoch 7351, loss 0.012363497167825699, R2 0.48609817028045654\n",
      "Eval loss 0.01235989946871996, R2 0.5239096879959106\n",
      "epoch 7352, loss 0.012363465502858162, R2 0.4860994815826416\n",
      "Eval loss 0.012359865009784698, R2 0.5239111185073853\n",
      "epoch 7353, loss 0.012363433837890625, R2 0.48610079288482666\n",
      "Eval loss 0.012359831482172012, R2 0.5239123106002808\n",
      "epoch 7354, loss 0.012363402172923088, R2 0.4861021041870117\n",
      "Eval loss 0.012359796091914177, R2 0.5239138007164001\n",
      "epoch 7355, loss 0.0123633723706007, R2 0.4861036539077759\n",
      "Eval loss 0.012359760701656342, R2 0.52391517162323\n",
      "epoch 7356, loss 0.012363341636955738, R2 0.48610448837280273\n",
      "Eval loss 0.01235972810536623, R2 0.5239163041114807\n",
      "epoch 7357, loss 0.012363310903310776, R2 0.486105740070343\n",
      "Eval loss 0.012359692715108395, R2 0.5239177942276001\n",
      "epoch 7358, loss 0.012363280169665813, R2 0.4861071705818176\n",
      "Eval loss 0.012359658256173134, R2 0.5239187479019165\n",
      "epoch 7359, loss 0.012363248504698277, R2 0.4861084818840027\n",
      "Eval loss 0.012359623797237873, R2 0.5239201784133911\n",
      "epoch 7360, loss 0.012363218702375889, R2 0.48610973358154297\n",
      "Eval loss 0.012359589338302612, R2 0.5239216089248657\n",
      "epoch 7361, loss 0.012363187968730927, R2 0.48611098527908325\n",
      "Eval loss 0.012359553948044777, R2 0.523922860622406\n",
      "epoch 7362, loss 0.01236315630376339, R2 0.48611265420913696\n",
      "Eval loss 0.012359521351754665, R2 0.5239244103431702\n",
      "epoch 7363, loss 0.012363127432763577, R2 0.48611336946487427\n",
      "Eval loss 0.012359485030174255, R2 0.52392578125\n",
      "epoch 7364, loss 0.012363097630441189, R2 0.4861144423484802\n",
      "Eval loss 0.01235945150256157, R2 0.5239266753196716\n",
      "epoch 7365, loss 0.012363065965473652, R2 0.48611605167388916\n",
      "Eval loss 0.012359417043626308, R2 0.523928165435791\n",
      "epoch 7366, loss 0.01236303523182869, R2 0.4861173629760742\n",
      "Eval loss 0.012359383516013622, R2 0.5239295959472656\n",
      "epoch 7367, loss 0.012363004498183727, R2 0.4861184358596802\n",
      "Eval loss 0.012359348125755787, R2 0.5239312052726746\n",
      "epoch 7368, loss 0.012362973764538765, R2 0.48612022399902344\n",
      "Eval loss 0.012359313666820526, R2 0.5239319801330566\n",
      "epoch 7369, loss 0.012362943962216377, R2 0.48612117767333984\n",
      "Eval loss 0.012359279207885265, R2 0.5239335894584656\n",
      "epoch 7370, loss 0.012362913228571415, R2 0.4861224293708801\n",
      "Eval loss 0.012359246611595154, R2 0.5239346027374268\n",
      "epoch 7371, loss 0.012362881563603878, R2 0.4861237406730652\n",
      "Eval loss 0.012359212152659893, R2 0.5239361524581909\n",
      "epoch 7372, loss 0.012362852692604065, R2 0.486125111579895\n",
      "Eval loss 0.012359176762402058, R2 0.5239374041557312\n",
      "epoch 7373, loss 0.012362821958959103, R2 0.48612624406814575\n",
      "Eval loss 0.012359142303466797, R2 0.5239388942718506\n",
      "epoch 7374, loss 0.012362792156636715, R2 0.48612797260284424\n",
      "Eval loss 0.012359109707176685, R2 0.5239400863647461\n",
      "epoch 7375, loss 0.012362760491669178, R2 0.4861287474632263\n",
      "Eval loss 0.012359073385596275, R2 0.5239413976669312\n",
      "epoch 7376, loss 0.012362729758024216, R2 0.4861297011375427\n",
      "Eval loss 0.01235903985798359, R2 0.5239431858062744\n",
      "epoch 7377, loss 0.012362699955701828, R2 0.48613131046295166\n",
      "Eval loss 0.012359006330370903, R2 0.5239440202713013\n",
      "epoch 7378, loss 0.01236267015337944, R2 0.48613250255584717\n",
      "Eval loss 0.012358971871435642, R2 0.523945689201355\n",
      "epoch 7379, loss 0.012362640351057053, R2 0.4861339330673218\n",
      "Eval loss 0.012358938343822956, R2 0.5239464044570923\n",
      "epoch 7380, loss 0.012362610548734665, R2 0.4861353039741516\n",
      "Eval loss 0.01235890295356512, R2 0.523948073387146\n",
      "epoch 7381, loss 0.012362577952444553, R2 0.4861363172531128\n",
      "Eval loss 0.01235886849462986, R2 0.523949384689331\n",
      "epoch 7382, loss 0.01236254908144474, R2 0.4861375689506531\n",
      "Eval loss 0.012358834967017174, R2 0.523950457572937\n",
      "epoch 7383, loss 0.012362519279122353, R2 0.4861389398574829\n",
      "Eval loss 0.012358801439404488, R2 0.5239520072937012\n",
      "epoch 7384, loss 0.012362487614154816, R2 0.48614007234573364\n",
      "Eval loss 0.012358766980469227, R2 0.5239530801773071\n",
      "epoch 7385, loss 0.012362457811832428, R2 0.48614197969436646\n",
      "Eval loss 0.012358734384179115, R2 0.5239546298980713\n",
      "epoch 7386, loss 0.01236242800951004, R2 0.4861431121826172\n",
      "Eval loss 0.012358699925243855, R2 0.5239556431770325\n",
      "epoch 7387, loss 0.012362398207187653, R2 0.48614364862442017\n",
      "Eval loss 0.012358666397631168, R2 0.5239571928977966\n",
      "epoch 7388, loss 0.01236236747354269, R2 0.4861452579498291\n",
      "Eval loss 0.012358631938695908, R2 0.5239585041999817\n",
      "epoch 7389, loss 0.012362336739897728, R2 0.4861465096473694\n",
      "Eval loss 0.012358598411083221, R2 0.5239596366882324\n",
      "epoch 7390, loss 0.012362307868897915, R2 0.48614805936813354\n",
      "Eval loss 0.01235856395214796, R2 0.5239611268043518\n",
      "epoch 7391, loss 0.012362277135252953, R2 0.4861488342285156\n",
      "Eval loss 0.012358530424535275, R2 0.523962140083313\n",
      "epoch 7392, loss 0.012362247332930565, R2 0.48615074157714844\n",
      "Eval loss 0.012358495965600014, R2 0.5239640474319458\n",
      "epoch 7393, loss 0.012362217530608177, R2 0.4861513376235962\n",
      "Eval loss 0.012358462437987328, R2 0.5239647626876831\n",
      "epoch 7394, loss 0.012362188659608364, R2 0.4861525297164917\n",
      "Eval loss 0.012358427979052067, R2 0.5239666700363159\n",
      "epoch 7395, loss 0.012362156994640827, R2 0.48615384101867676\n",
      "Eval loss 0.01235839445143938, R2 0.5239676237106323\n",
      "epoch 7396, loss 0.012362126260995865, R2 0.48615562915802\n",
      "Eval loss 0.01235835999250412, R2 0.5239688158035278\n",
      "epoch 7397, loss 0.012362096458673477, R2 0.4861563444137573\n",
      "Eval loss 0.012358326464891434, R2 0.5239704251289368\n",
      "epoch 7398, loss 0.012362067587673664, R2 0.48615825176239014\n",
      "Eval loss 0.012358293868601322, R2 0.523971676826477\n",
      "epoch 7399, loss 0.012362037785351276, R2 0.4861587882041931\n",
      "Eval loss 0.012358259409666061, R2 0.5239728689193726\n",
      "epoch 7400, loss 0.012362007983028889, R2 0.4861606955528259\n",
      "Eval loss 0.0123582249507308, R2 0.5239741802215576\n",
      "epoch 7401, loss 0.012361977249383926, R2 0.4861612915992737\n",
      "Eval loss 0.012358192354440689, R2 0.5239756107330322\n",
      "epoch 7402, loss 0.012361948378384113, R2 0.48616254329681396\n",
      "Eval loss 0.012358157895505428, R2 0.5239766240119934\n",
      "epoch 7403, loss 0.012361917644739151, R2 0.4861636161804199\n",
      "Eval loss 0.012358124367892742, R2 0.5239778757095337\n",
      "epoch 7404, loss 0.012361887842416763, R2 0.48616504669189453\n",
      "Eval loss 0.01235809177160263, R2 0.5239790678024292\n",
      "epoch 7405, loss 0.012361857108771801, R2 0.48616647720336914\n",
      "Eval loss 0.012358058243989944, R2 0.5239802002906799\n",
      "epoch 7406, loss 0.012361828237771988, R2 0.4861675500869751\n",
      "Eval loss 0.012358024716377258, R2 0.5239823460578918\n",
      "epoch 7407, loss 0.012361797504127026, R2 0.4861694574356079\n",
      "Eval loss 0.012357990257441998, R2 0.523983359336853\n",
      "epoch 7408, loss 0.012361768633127213, R2 0.4861699938774109\n",
      "Eval loss 0.012357957661151886, R2 0.5239841938018799\n",
      "epoch 7409, loss 0.01236173789948225, R2 0.48617124557495117\n",
      "Eval loss 0.012357923202216625, R2 0.5239859819412231\n",
      "epoch 7410, loss 0.012361709028482437, R2 0.48617249727249146\n",
      "Eval loss 0.012357889674603939, R2 0.5239869356155396\n",
      "epoch 7411, loss 0.01236167922616005, R2 0.48617368936538696\n",
      "Eval loss 0.012357857078313828, R2 0.5239884853363037\n",
      "epoch 7412, loss 0.012361648492515087, R2 0.486175000667572\n",
      "Eval loss 0.012357824482023716, R2 0.5239897966384888\n",
      "epoch 7413, loss 0.012361619621515274, R2 0.48617619276046753\n",
      "Eval loss 0.012357790023088455, R2 0.5239905118942261\n",
      "epoch 7414, loss 0.012361590750515461, R2 0.48617738485336304\n",
      "Eval loss 0.012357756495475769, R2 0.5239920616149902\n",
      "epoch 7415, loss 0.012361560948193073, R2 0.4861791133880615\n",
      "Eval loss 0.012357722967863083, R2 0.5239936709403992\n",
      "epoch 7416, loss 0.012361531145870686, R2 0.4861798882484436\n",
      "Eval loss 0.012357688508927822, R2 0.5239948034286499\n",
      "epoch 7417, loss 0.012361501343548298, R2 0.4861815571784973\n",
      "Eval loss 0.01235765591263771, R2 0.5239965915679932\n",
      "epoch 7418, loss 0.012361472472548485, R2 0.4861823320388794\n",
      "Eval loss 0.012357623316347599, R2 0.5239970684051514\n",
      "epoch 7419, loss 0.012361442670226097, R2 0.48618388175964355\n",
      "Eval loss 0.012357589788734913, R2 0.5239987969398499\n",
      "epoch 7420, loss 0.012361411936581135, R2 0.48618483543395996\n",
      "Eval loss 0.012357556261122227, R2 0.5239999294281006\n",
      "epoch 7421, loss 0.012361383065581322, R2 0.48618602752685547\n",
      "Eval loss 0.01235752273350954, R2 0.5240012407302856\n",
      "epoch 7422, loss 0.012361353263258934, R2 0.48618775606155396\n",
      "Eval loss 0.01235748827457428, R2 0.5240025520324707\n",
      "epoch 7423, loss 0.012361324392259121, R2 0.48618847131729126\n",
      "Eval loss 0.012357456609606743, R2 0.524003803730011\n",
      "epoch 7424, loss 0.012361294589936733, R2 0.48618966341018677\n",
      "Eval loss 0.012357423081994057, R2 0.5240048170089722\n",
      "epoch 7425, loss 0.012361263856291771, R2 0.4861909747123718\n",
      "Eval loss 0.012357390485703945, R2 0.5240063667297363\n",
      "epoch 7426, loss 0.012361236847937107, R2 0.4861922860145569\n",
      "Eval loss 0.012357356958091259, R2 0.5240079164505005\n",
      "epoch 7427, loss 0.012361206114292145, R2 0.48619335889816284\n",
      "Eval loss 0.012357322499155998, R2 0.5240089893341064\n",
      "epoch 7428, loss 0.012361177243292332, R2 0.48619455099105835\n",
      "Eval loss 0.012357289902865887, R2 0.5240099430084229\n",
      "epoch 7429, loss 0.01236114650964737, R2 0.4861963391304016\n",
      "Eval loss 0.012357257306575775, R2 0.524011492729187\n",
      "epoch 7430, loss 0.012361117638647556, R2 0.4861980676651001\n",
      "Eval loss 0.012357225641608238, R2 0.5240124464035034\n",
      "epoch 7431, loss 0.012361087836325169, R2 0.4861984848976135\n",
      "Eval loss 0.012357191182672977, R2 0.524013876914978\n",
      "epoch 7432, loss 0.01236105989664793, R2 0.48619943857192993\n",
      "Eval loss 0.012357158586382866, R2 0.5240153074264526\n",
      "epoch 7433, loss 0.012361029163002968, R2 0.4862006902694702\n",
      "Eval loss 0.01235712505877018, R2 0.5240168571472168\n",
      "epoch 7434, loss 0.012361000292003155, R2 0.4862019419670105\n",
      "Eval loss 0.012357090599834919, R2 0.5240181684494019\n",
      "epoch 7435, loss 0.012360971421003342, R2 0.4862034320831299\n",
      "Eval loss 0.012357059866189957, R2 0.5240191221237183\n",
      "epoch 7436, loss 0.012360941618680954, R2 0.4862043857574463\n",
      "Eval loss 0.012357025407254696, R2 0.5240201354026794\n",
      "epoch 7437, loss 0.012360912747681141, R2 0.4862062335014343\n",
      "Eval loss 0.012356992810964584, R2 0.5240218043327332\n",
      "epoch 7438, loss 0.012360883876681328, R2 0.48620694875717163\n",
      "Eval loss 0.012356960214674473, R2 0.5240229368209839\n",
      "epoch 7439, loss 0.012360853143036366, R2 0.4862080216407776\n",
      "Eval loss 0.012356927618384361, R2 0.5240238904953003\n",
      "epoch 7440, loss 0.012360825203359127, R2 0.4862092137336731\n",
      "Eval loss 0.012356894090771675, R2 0.524025559425354\n",
      "epoch 7441, loss 0.012360796332359314, R2 0.4862104058265686\n",
      "Eval loss 0.012356862425804138, R2 0.5240263938903809\n",
      "epoch 7442, loss 0.012360767461359501, R2 0.4862115979194641\n",
      "Eval loss 0.012356828898191452, R2 0.5240281224250793\n",
      "epoch 7443, loss 0.012360737659037113, R2 0.4862128496170044\n",
      "Eval loss 0.01235679630190134, R2 0.5240293741226196\n",
      "epoch 7444, loss 0.012360706925392151, R2 0.4862141013145447\n",
      "Eval loss 0.012356763705611229, R2 0.524030327796936\n",
      "epoch 7445, loss 0.012360679917037487, R2 0.4862157702445984\n",
      "Eval loss 0.012356730177998543, R2 0.5240318775177002\n",
      "epoch 7446, loss 0.012360649183392525, R2 0.48621666431427\n",
      "Eval loss 0.012356695719063282, R2 0.5240333676338196\n",
      "epoch 7447, loss 0.012360620312392712, R2 0.4862183928489685\n",
      "Eval loss 0.01235666312277317, R2 0.5240343809127808\n",
      "epoch 7448, loss 0.012360591441392899, R2 0.4862189292907715\n",
      "Eval loss 0.012356632389128208, R2 0.5240352749824524\n",
      "epoch 7449, loss 0.01236056350171566, R2 0.486220121383667\n",
      "Eval loss 0.012356599792838097, R2 0.5240367650985718\n",
      "epoch 7450, loss 0.012360533699393272, R2 0.4862214922904968\n",
      "Eval loss 0.012356565333902836, R2 0.5240382552146912\n",
      "epoch 7451, loss 0.01236050482839346, R2 0.486222505569458\n",
      "Eval loss 0.01235653180629015, R2 0.5240392684936523\n",
      "epoch 7452, loss 0.012360475957393646, R2 0.4862237572669983\n",
      "Eval loss 0.012356501072645187, R2 0.5240407586097717\n",
      "epoch 7453, loss 0.012360447086393833, R2 0.48622506856918335\n",
      "Eval loss 0.012356468476355076, R2 0.5240418910980225\n",
      "epoch 7454, loss 0.01236041821539402, R2 0.48622626066207886\n",
      "Eval loss 0.012356435880064964, R2 0.5240429639816284\n",
      "epoch 7455, loss 0.012360388413071632, R2 0.48622751235961914\n",
      "Eval loss 0.012356402352452278, R2 0.524044394493103\n",
      "epoch 7456, loss 0.01236035954207182, R2 0.4862287640571594\n",
      "Eval loss 0.012356369756162167, R2 0.5240458250045776\n",
      "epoch 7457, loss 0.012360332533717155, R2 0.48622971773147583\n",
      "Eval loss 0.012356337159872055, R2 0.5240468978881836\n",
      "epoch 7458, loss 0.012360302731394768, R2 0.48623108863830566\n",
      "Eval loss 0.012356304563581944, R2 0.5240483283996582\n",
      "epoch 7459, loss 0.01236027292907238, R2 0.4862319827079773\n",
      "Eval loss 0.012356273829936981, R2 0.5240490436553955\n",
      "epoch 7460, loss 0.012360244058072567, R2 0.48623400926589966\n",
      "Eval loss 0.01235624123364687, R2 0.5240505933761597\n",
      "epoch 7461, loss 0.012360216118395329, R2 0.48623502254486084\n",
      "Eval loss 0.012356206774711609, R2 0.5240525007247925\n",
      "epoch 7462, loss 0.012360187247395515, R2 0.4862363934516907\n",
      "Eval loss 0.012356173247098923, R2 0.5240533351898193\n",
      "epoch 7463, loss 0.012360158376395702, R2 0.4862375855445862\n",
      "Eval loss 0.012356141582131386, R2 0.5240545272827148\n",
      "epoch 7464, loss 0.01236012950539589, R2 0.4862383008003235\n",
      "Eval loss 0.012356109917163849, R2 0.5240553617477417\n",
      "epoch 7465, loss 0.01236010156571865, R2 0.4862394332885742\n",
      "Eval loss 0.012356077320873737, R2 0.5240567922592163\n",
      "epoch 7466, loss 0.012360069900751114, R2 0.4862402677536011\n",
      "Eval loss 0.012356044724583626, R2 0.52405846118927\n",
      "epoch 7467, loss 0.012360043823719025, R2 0.4862421751022339\n",
      "Eval loss 0.012356013059616089, R2 0.5240594148635864\n",
      "epoch 7468, loss 0.012360014021396637, R2 0.48624294996261597\n",
      "Eval loss 0.012355980463325977, R2 0.5240603685379028\n",
      "epoch 7469, loss 0.012359985150396824, R2 0.4862441420555115\n",
      "Eval loss 0.01235594879835844, R2 0.5240616202354431\n",
      "epoch 7470, loss 0.01235995814204216, R2 0.4862452745437622\n",
      "Eval loss 0.012355915270745754, R2 0.5240631699562073\n",
      "epoch 7471, loss 0.012359928339719772, R2 0.4862465262413025\n",
      "Eval loss 0.012355883605778217, R2 0.5240644216537476\n",
      "epoch 7472, loss 0.01235989946871996, R2 0.4862480163574219\n",
      "Eval loss 0.012355851009488106, R2 0.5240656137466431\n",
      "epoch 7473, loss 0.01235987152904272, R2 0.48624885082244873\n",
      "Eval loss 0.012355819344520569, R2 0.5240668058395386\n",
      "epoch 7474, loss 0.012359842658042908, R2 0.48625022172927856\n",
      "Eval loss 0.012355786748230457, R2 0.5240681171417236\n",
      "epoch 7475, loss 0.01235981471836567, R2 0.4862513542175293\n",
      "Eval loss 0.01235575508326292, R2 0.5240691900253296\n",
      "epoch 7476, loss 0.012359785847365856, R2 0.4862525463104248\n",
      "Eval loss 0.012355722486972809, R2 0.5240704417228699\n",
      "epoch 7477, loss 0.012359756976366043, R2 0.4862537384033203\n",
      "Eval loss 0.012355689890682697, R2 0.5240718126296997\n",
      "epoch 7478, loss 0.01235972810536623, R2 0.48625481128692627\n",
      "Eval loss 0.012355657294392586, R2 0.5240731239318848\n",
      "epoch 7479, loss 0.012359699234366417, R2 0.4862561821937561\n",
      "Eval loss 0.012355626560747623, R2 0.5240743160247803\n",
      "epoch 7480, loss 0.012359670363366604, R2 0.4862571954727173\n",
      "Eval loss 0.012355593033134937, R2 0.5240752696990967\n",
      "epoch 7481, loss 0.01235964335501194, R2 0.48625868558883667\n",
      "Eval loss 0.0123555613681674, R2 0.5240770578384399\n",
      "epoch 7482, loss 0.012359615415334702, R2 0.4862595200538635\n",
      "Eval loss 0.012355528771877289, R2 0.5240781903266907\n",
      "epoch 7483, loss 0.012359585613012314, R2 0.48626089096069336\n",
      "Eval loss 0.012355497106909752, R2 0.5240790843963623\n",
      "epoch 7484, loss 0.0123595567420125, R2 0.48626190423965454\n",
      "Eval loss 0.01235546451061964, R2 0.5240802764892578\n",
      "epoch 7485, loss 0.012359529733657837, R2 0.4862632155418396\n",
      "Eval loss 0.012355434708297253, R2 0.5240817070007324\n",
      "epoch 7486, loss 0.012359500862658024, R2 0.48626410961151123\n",
      "Eval loss 0.012355401180684566, R2 0.5240828394889832\n",
      "epoch 7487, loss 0.01235947199165821, R2 0.4862654209136963\n",
      "Eval loss 0.012355368584394455, R2 0.524084210395813\n",
      "epoch 7488, loss 0.012359444051980972, R2 0.4862666130065918\n",
      "Eval loss 0.012355335988104343, R2 0.5240854620933533\n",
      "epoch 7489, loss 0.012359417043626308, R2 0.48626792430877686\n",
      "Eval loss 0.012355304323136806, R2 0.5240864157676697\n",
      "epoch 7490, loss 0.012359386309981346, R2 0.486269474029541\n",
      "Eval loss 0.012355273589491844, R2 0.5240877866744995\n",
      "epoch 7491, loss 0.012359358370304108, R2 0.48627030849456787\n",
      "Eval loss 0.012355240061879158, R2 0.5240890383720398\n",
      "epoch 7492, loss 0.01235933043062687, R2 0.48627185821533203\n",
      "Eval loss 0.012355208396911621, R2 0.5240905284881592\n",
      "epoch 7493, loss 0.012359301559627056, R2 0.48627305030822754\n",
      "Eval loss 0.012355176731944084, R2 0.5240916013717651\n",
      "epoch 7494, loss 0.012359273619949818, R2 0.48627370595932007\n",
      "Eval loss 0.012355144135653973, R2 0.5240930318832397\n",
      "epoch 7495, loss 0.01235924568027258, R2 0.4862753748893738\n",
      "Eval loss 0.01235511340200901, R2 0.5240941047668457\n",
      "epoch 7496, loss 0.012359218671917915, R2 0.486275851726532\n",
      "Eval loss 0.012355081737041473, R2 0.5240954160690308\n",
      "epoch 7497, loss 0.012359188869595528, R2 0.4862770438194275\n",
      "Eval loss 0.012355051003396511, R2 0.5240961313247681\n",
      "epoch 7498, loss 0.012359162792563438, R2 0.4862784743309021\n",
      "Eval loss 0.012355017475783825, R2 0.524097740650177\n",
      "epoch 7499, loss 0.01235913299024105, R2 0.48627954721450806\n",
      "Eval loss 0.012354984879493713, R2 0.5240988731384277\n",
      "epoch 7500, loss 0.012359104119241238, R2 0.48628073930740356\n",
      "Eval loss 0.012354955077171326, R2 0.5241001844406128\n",
      "epoch 7501, loss 0.012359076179564, R2 0.4862818717956543\n",
      "Eval loss 0.012354923412203789, R2 0.5241011381149292\n",
      "epoch 7502, loss 0.012359049171209335, R2 0.486283540725708\n",
      "Eval loss 0.012354890815913677, R2 0.5241023302078247\n",
      "epoch 7503, loss 0.012359020300209522, R2 0.4862842559814453\n",
      "Eval loss 0.01235485915094614, R2 0.5241039991378784\n",
      "epoch 7504, loss 0.012358993291854858, R2 0.4862855076789856\n",
      "Eval loss 0.012354827485978603, R2 0.5241047739982605\n",
      "epoch 7505, loss 0.012358964420855045, R2 0.4862872362136841\n",
      "Eval loss 0.012354795821011066, R2 0.5241062641143799\n",
      "epoch 7506, loss 0.012358936481177807, R2 0.4862881898880005\n",
      "Eval loss 0.012354763224720955, R2 0.5241072773933411\n",
      "epoch 7507, loss 0.012358907610177994, R2 0.486289381980896\n",
      "Eval loss 0.012354732491075993, R2 0.5241087675094604\n",
      "epoch 7508, loss 0.012358879670500755, R2 0.48629021644592285\n",
      "Eval loss 0.012354699894785881, R2 0.524109959602356\n",
      "epoch 7509, loss 0.012358852662146091, R2 0.4862915277481079\n",
      "Eval loss 0.012354669161140919, R2 0.5241115689277649\n",
      "epoch 7510, loss 0.012358823791146278, R2 0.48629289865493774\n",
      "Eval loss 0.012354637496173382, R2 0.5241125822067261\n",
      "epoch 7511, loss 0.012358796782791615, R2 0.4862934947013855\n",
      "Eval loss 0.012354605831205845, R2 0.5241134762763977\n",
      "epoch 7512, loss 0.012358768843114376, R2 0.48629486560821533\n",
      "Eval loss 0.012354575097560883, R2 0.5241147875785828\n",
      "epoch 7513, loss 0.012358740903437138, R2 0.48629599809646606\n",
      "Eval loss 0.012354542501270771, R2 0.5241157412528992\n",
      "epoch 7514, loss 0.012358712032437325, R2 0.4862971901893616\n",
      "Eval loss 0.012354511767625809, R2 0.5241167545318604\n",
      "epoch 7515, loss 0.012358684092760086, R2 0.4862988591194153\n",
      "Eval loss 0.012354480102658272, R2 0.5241183042526245\n",
      "epoch 7516, loss 0.012358656153082848, R2 0.4863000512123108\n",
      "Eval loss 0.012354448437690735, R2 0.5241198539733887\n",
      "epoch 7517, loss 0.012358629144728184, R2 0.4863004684448242\n",
      "Eval loss 0.012354415841400623, R2 0.5241209268569946\n",
      "epoch 7518, loss 0.01235860027372837, R2 0.48630231618881226\n",
      "Eval loss 0.012354385107755661, R2 0.5241221189498901\n",
      "epoch 7519, loss 0.012358574196696281, R2 0.48630279302597046\n",
      "Eval loss 0.012354353442788124, R2 0.5241230726242065\n",
      "epoch 7520, loss 0.012358545325696468, R2 0.48630446195602417\n",
      "Eval loss 0.012354323640465736, R2 0.5241243839263916\n",
      "epoch 7521, loss 0.01235851738601923, R2 0.4863051176071167\n",
      "Eval loss 0.012354291044175625, R2 0.5241254568099976\n",
      "epoch 7522, loss 0.012358490377664566, R2 0.48630690574645996\n",
      "Eval loss 0.012354260310530663, R2 0.5241270065307617\n",
      "epoch 7523, loss 0.012358462437987328, R2 0.48630809783935547\n",
      "Eval loss 0.0123542295768857, R2 0.5241278409957886\n",
      "epoch 7524, loss 0.01235843263566494, R2 0.4863088130950928\n",
      "Eval loss 0.012354196980595589, R2 0.5241290926933289\n",
      "epoch 7525, loss 0.012358408421278, R2 0.4863100051879883\n",
      "Eval loss 0.012354165315628052, R2 0.5241305828094482\n",
      "epoch 7526, loss 0.012358378618955612, R2 0.48631107807159424\n",
      "Eval loss 0.01235413458198309, R2 0.5241317749023438\n",
      "epoch 7527, loss 0.012358350679278374, R2 0.4863128662109375\n",
      "Eval loss 0.012354102917015553, R2 0.5241330862045288\n",
      "epoch 7528, loss 0.012358322739601135, R2 0.4863128662109375\n",
      "Eval loss 0.012354071252048016, R2 0.5241341590881348\n",
      "epoch 7529, loss 0.012358296662569046, R2 0.4863142967224121\n",
      "Eval loss 0.012354041449725628, R2 0.5241357684135437\n",
      "epoch 7530, loss 0.012358267791569233, R2 0.48631566762924194\n",
      "Eval loss 0.012354009784758091, R2 0.5241364240646362\n",
      "epoch 7531, loss 0.012358241714537144, R2 0.4863167405128479\n",
      "Eval loss 0.012353978119790554, R2 0.5241377949714661\n",
      "epoch 7532, loss 0.01235821284353733, R2 0.48631763458251953\n",
      "Eval loss 0.012353947386145592, R2 0.5241388082504272\n",
      "epoch 7533, loss 0.012358185835182667, R2 0.4863194227218628\n",
      "Eval loss 0.012353915721178055, R2 0.5241400003433228\n",
      "epoch 7534, loss 0.012358156964182854, R2 0.4863201379776001\n",
      "Eval loss 0.012353884987533092, R2 0.5241413712501526\n",
      "epoch 7535, loss 0.012358130887150764, R2 0.48632121086120605\n",
      "Eval loss 0.01235385425388813, R2 0.5241425633430481\n",
      "epoch 7536, loss 0.012358102016150951, R2 0.48632240295410156\n",
      "Eval loss 0.012353822588920593, R2 0.5241437554359436\n",
      "epoch 7537, loss 0.012358075939118862, R2 0.48632383346557617\n",
      "Eval loss 0.012353790923953056, R2 0.5241450071334839\n",
      "epoch 7538, loss 0.012358047068119049, R2 0.486324667930603\n",
      "Eval loss 0.012353762052953243, R2 0.5241462588310242\n",
      "epoch 7539, loss 0.012358020059764385, R2 0.48632580041885376\n",
      "Eval loss 0.012353729456663132, R2 0.5241475105285645\n",
      "epoch 7540, loss 0.012357992120087147, R2 0.48632699251174927\n",
      "Eval loss 0.012353697791695595, R2 0.52414870262146\n",
      "epoch 7541, loss 0.012357965111732483, R2 0.4863284230232239\n",
      "Eval loss 0.012353667058050632, R2 0.5241498947143555\n",
      "epoch 7542, loss 0.012357938103377819, R2 0.4863298535346985\n",
      "Eval loss 0.012353635393083096, R2 0.5241508483886719\n",
      "epoch 7543, loss 0.012357911095023155, R2 0.4863309860229492\n",
      "Eval loss 0.012353606522083282, R2 0.524152398109436\n",
      "epoch 7544, loss 0.012357883155345917, R2 0.48633164167404175\n",
      "Eval loss 0.012353572994470596, R2 0.5241537094116211\n",
      "epoch 7545, loss 0.012357856146991253, R2 0.48633265495300293\n",
      "Eval loss 0.012353542260825634, R2 0.5241543054580688\n",
      "epoch 7546, loss 0.012357828207314014, R2 0.48633378744125366\n",
      "Eval loss 0.012353513389825821, R2 0.5241554975509644\n",
      "epoch 7547, loss 0.012357800267636776, R2 0.4863350987434387\n",
      "Eval loss 0.012353481724858284, R2 0.524156928062439\n",
      "epoch 7548, loss 0.012357774190604687, R2 0.48633670806884766\n",
      "Eval loss 0.012353450991213322, R2 0.5241581201553345\n",
      "epoch 7549, loss 0.012357747182250023, R2 0.48633766174316406\n",
      "Eval loss 0.01235342025756836, R2 0.52415931224823\n",
      "epoch 7550, loss 0.01235771831125021, R2 0.4863389730453491\n",
      "Eval loss 0.012353388592600822, R2 0.524160623550415\n",
      "epoch 7551, loss 0.012357690371572971, R2 0.4863399863243103\n",
      "Eval loss 0.012353358790278435, R2 0.5241619348526001\n",
      "epoch 7552, loss 0.012357664294540882, R2 0.48634111881256104\n",
      "Eval loss 0.012353327125310898, R2 0.524162769317627\n",
      "epoch 7553, loss 0.012357637286186218, R2 0.486342191696167\n",
      "Eval loss 0.012353296391665936, R2 0.5241639018058777\n",
      "epoch 7554, loss 0.012357608415186405, R2 0.4863429069519043\n",
      "Eval loss 0.012353265658020973, R2 0.5241652131080627\n",
      "epoch 7555, loss 0.01235758326947689, R2 0.48634445667266846\n",
      "Eval loss 0.012353234924376011, R2 0.5241664052009583\n",
      "epoch 7556, loss 0.012357555329799652, R2 0.4863452911376953\n",
      "Eval loss 0.012353205122053623, R2 0.5241677165031433\n",
      "epoch 7557, loss 0.012357528321444988, R2 0.48634690046310425\n",
      "Eval loss 0.012353172525763512, R2 0.5241685509681702\n",
      "epoch 7558, loss 0.012357501313090324, R2 0.48634785413742065\n",
      "Eval loss 0.012353142723441124, R2 0.524169921875\n",
      "epoch 7559, loss 0.012357472442090511, R2 0.48634856939315796\n",
      "Eval loss 0.012353111058473587, R2 0.5241711735725403\n",
      "epoch 7560, loss 0.012357446365058422, R2 0.4863496422767639\n",
      "Eval loss 0.012353080324828625, R2 0.5241721868515015\n",
      "epoch 7561, loss 0.012357420288026333, R2 0.48635077476501465\n",
      "Eval loss 0.012353049591183662, R2 0.5241732597351074\n",
      "epoch 7562, loss 0.012357392348349094, R2 0.4863520860671997\n",
      "Eval loss 0.012353019788861275, R2 0.524174690246582\n",
      "epoch 7563, loss 0.012357364408671856, R2 0.4863530397415161\n",
      "Eval loss 0.012352989055216312, R2 0.5241760015487671\n",
      "epoch 7564, loss 0.012357338331639767, R2 0.48635435104370117\n",
      "Eval loss 0.01235295832157135, R2 0.5241765975952148\n",
      "epoch 7565, loss 0.012357312254607677, R2 0.486355721950531\n",
      "Eval loss 0.012352927587926388, R2 0.5241783857345581\n",
      "epoch 7566, loss 0.012357283383607864, R2 0.4863564372062683\n",
      "Eval loss 0.012352897785604, R2 0.5241793990135193\n",
      "epoch 7567, loss 0.01235725823789835, R2 0.4863576292991638\n",
      "Eval loss 0.012352867051959038, R2 0.5241805911064148\n",
      "epoch 7568, loss 0.012357231229543686, R2 0.48635876178741455\n",
      "Eval loss 0.012352836318314075, R2 0.5241814851760864\n",
      "epoch 7569, loss 0.012357202358543873, R2 0.48635977506637573\n",
      "Eval loss 0.012352806515991688, R2 0.5241826772689819\n",
      "epoch 7570, loss 0.012357178144156933, R2 0.4863608479499817\n",
      "Eval loss 0.012352775782346725, R2 0.5241842269897461\n",
      "epoch 7571, loss 0.01235714927315712, R2 0.4863625168800354\n",
      "Eval loss 0.012352745048701763, R2 0.524185299873352\n",
      "epoch 7572, loss 0.012357122264802456, R2 0.48636311292648315\n",
      "Eval loss 0.0123527143150568, R2 0.5241863131523132\n",
      "epoch 7573, loss 0.012357095256447792, R2 0.48636436462402344\n",
      "Eval loss 0.012352683581411839, R2 0.5241878032684326\n",
      "epoch 7574, loss 0.012357069179415703, R2 0.48636549711227417\n",
      "Eval loss 0.01235265377908945, R2 0.5241888165473938\n",
      "epoch 7575, loss 0.012357042171061039, R2 0.4863666296005249\n",
      "Eval loss 0.012352622114121914, R2 0.5241904258728027\n",
      "epoch 7576, loss 0.012357015162706375, R2 0.4863675832748413\n",
      "Eval loss 0.0123525932431221, R2 0.5241912603378296\n",
      "epoch 7577, loss 0.012356987223029137, R2 0.48636889457702637\n",
      "Eval loss 0.012352561578154564, R2 0.5241920948028564\n",
      "epoch 7578, loss 0.012356961145997047, R2 0.486370325088501\n",
      "Eval loss 0.01235253270715475, R2 0.5241936445236206\n",
      "epoch 7579, loss 0.012356934137642384, R2 0.4863707423210144\n",
      "Eval loss 0.012352501042187214, R2 0.5241948366165161\n",
      "epoch 7580, loss 0.01235690712928772, R2 0.4863722324371338\n",
      "Eval loss 0.012352470308542252, R2 0.5241958498954773\n",
      "epoch 7581, loss 0.012356878258287907, R2 0.48637324571609497\n",
      "Eval loss 0.012352440506219864, R2 0.5241968631744385\n",
      "epoch 7582, loss 0.012356854043900967, R2 0.4863744378089905\n",
      "Eval loss 0.012352409772574902, R2 0.524198055267334\n",
      "epoch 7583, loss 0.012356827966868877, R2 0.48637551069259644\n",
      "Eval loss 0.012352379970252514, R2 0.524199366569519\n",
      "epoch 7584, loss 0.012356800027191639, R2 0.4863765239715576\n",
      "Eval loss 0.012352349236607552, R2 0.5242002010345459\n",
      "epoch 7585, loss 0.01235677395015955, R2 0.48637789487838745\n",
      "Eval loss 0.012352320365607738, R2 0.5242016315460205\n",
      "epoch 7586, loss 0.012356746941804886, R2 0.48637890815734863\n",
      "Eval loss 0.012352288700640202, R2 0.5242033004760742\n",
      "epoch 7587, loss 0.012356719933450222, R2 0.48638033866882324\n",
      "Eval loss 0.012352258898317814, R2 0.5242042541503906\n",
      "epoch 7588, loss 0.012356691993772984, R2 0.4863811731338501\n",
      "Eval loss 0.012352229095995426, R2 0.5242051482200623\n",
      "epoch 7589, loss 0.012356665916740894, R2 0.4863824248313904\n",
      "Eval loss 0.012352199293673038, R2 0.5242066979408264\n",
      "epoch 7590, loss 0.01235663890838623, R2 0.48638319969177246\n",
      "Eval loss 0.01235216949135065, R2 0.5242071747779846\n",
      "epoch 7591, loss 0.012356612831354141, R2 0.48638415336608887\n",
      "Eval loss 0.012352138757705688, R2 0.5242083668708801\n",
      "epoch 7592, loss 0.012356586754322052, R2 0.48638540506362915\n",
      "Eval loss 0.012352108024060726, R2 0.5242099761962891\n",
      "epoch 7593, loss 0.012356560677289963, R2 0.48638665676116943\n",
      "Eval loss 0.012352077290415764, R2 0.5242109894752502\n",
      "epoch 7594, loss 0.012356533668935299, R2 0.4863877296447754\n",
      "Eval loss 0.012352047488093376, R2 0.5242122411727905\n",
      "epoch 7595, loss 0.01235650759190321, R2 0.4863886833190918\n",
      "Eval loss 0.012352018617093563, R2 0.5242133140563965\n",
      "epoch 7596, loss 0.012356479652225971, R2 0.48638981580734253\n",
      "Eval loss 0.0123519878834486, R2 0.5242144465446472\n",
      "epoch 7597, loss 0.012356452643871307, R2 0.4863911271095276\n",
      "Eval loss 0.012351957149803638, R2 0.5242160558700562\n",
      "epoch 7598, loss 0.012356428429484367, R2 0.4863918423652649\n",
      "Eval loss 0.01235192734748125, R2 0.5242170095443726\n",
      "epoch 7599, loss 0.012356400489807129, R2 0.48639315366744995\n",
      "Eval loss 0.012351898476481438, R2 0.5242176055908203\n",
      "epoch 7600, loss 0.012356373481452465, R2 0.4863942265510559\n",
      "Eval loss 0.01235186867415905, R2 0.5242190361022949\n",
      "epoch 7601, loss 0.01235634833574295, R2 0.48639529943466187\n",
      "Eval loss 0.012351837940514088, R2 0.5242201089859009\n",
      "epoch 7602, loss 0.012356319464743137, R2 0.4863964915275574\n",
      "Eval loss 0.0123518081381917, R2 0.5242213606834412\n",
      "epoch 7603, loss 0.012356294319033623, R2 0.4863976836204529\n",
      "Eval loss 0.012351779267191887, R2 0.5242222547531128\n",
      "epoch 7604, loss 0.012356268242001534, R2 0.4863991141319275\n",
      "Eval loss 0.01235174760222435, R2 0.5242234468460083\n",
      "epoch 7605, loss 0.01235624123364687, R2 0.48639976978302\n",
      "Eval loss 0.012351718731224537, R2 0.5242246389389038\n",
      "epoch 7606, loss 0.012356214225292206, R2 0.4864007234573364\n",
      "Eval loss 0.012351687997579575, R2 0.5242258310317993\n",
      "epoch 7607, loss 0.01235619094222784, R2 0.48640167713165283\n",
      "Eval loss 0.012351659126579762, R2 0.5242271423339844\n",
      "epoch 7608, loss 0.012356162071228027, R2 0.486403226852417\n",
      "Eval loss 0.012351629324257374, R2 0.5242282152175903\n",
      "epoch 7609, loss 0.012356135994195938, R2 0.4864041209220886\n",
      "Eval loss 0.012351599521934986, R2 0.5242294073104858\n",
      "epoch 7610, loss 0.012356109917163849, R2 0.4864053726196289\n",
      "Eval loss 0.012351568788290024, R2 0.5242305994033813\n",
      "epoch 7611, loss 0.01235608384013176, R2 0.48640626668930054\n",
      "Eval loss 0.012351538985967636, R2 0.5242316126823425\n",
      "epoch 7612, loss 0.01235605776309967, R2 0.4864075183868408\n",
      "Eval loss 0.012351510114967823, R2 0.5242328643798828\n",
      "epoch 7613, loss 0.012356031686067581, R2 0.4864084720611572\n",
      "Eval loss 0.01235147938132286, R2 0.5242341756820679\n",
      "epoch 7614, loss 0.012356004677712917, R2 0.4864097237586975\n",
      "Eval loss 0.012351449579000473, R2 0.5242353081703186\n",
      "epoch 7615, loss 0.012355978600680828, R2 0.48641079664230347\n",
      "Eval loss 0.01235142070800066, R2 0.5242362022399902\n",
      "epoch 7616, loss 0.012355952523648739, R2 0.4864117503166199\n",
      "Eval loss 0.012351390905678272, R2 0.5242374539375305\n",
      "epoch 7617, loss 0.01235592644661665, R2 0.48641282320022583\n",
      "Eval loss 0.01235136203467846, R2 0.5242384076118469\n",
      "epoch 7618, loss 0.012355899438261986, R2 0.4864140748977661\n",
      "Eval loss 0.012351331301033497, R2 0.5242398977279663\n",
      "epoch 7619, loss 0.012355875223875046, R2 0.48641544580459595\n",
      "Eval loss 0.01235130149871111, R2 0.524241030216217\n",
      "epoch 7620, loss 0.012355848215520382, R2 0.486416757106781\n",
      "Eval loss 0.012351271696388721, R2 0.5242422819137573\n",
      "epoch 7621, loss 0.012355821207165718, R2 0.48641735315322876\n",
      "Eval loss 0.012351242825388908, R2 0.5242434144020081\n",
      "epoch 7622, loss 0.012355796061456203, R2 0.4864187240600586\n",
      "Eval loss 0.01235121302306652, R2 0.5242444276809692\n",
      "epoch 7623, loss 0.012355769984424114, R2 0.4864199757575989\n",
      "Eval loss 0.012351184152066708, R2 0.5242452621459961\n",
      "epoch 7624, loss 0.012355742044746876, R2 0.4864204525947571\n",
      "Eval loss 0.01235115248709917, R2 0.5242466330528259\n",
      "epoch 7625, loss 0.012355717830359936, R2 0.4864213466644287\n",
      "Eval loss 0.012351123616099358, R2 0.5242475867271423\n",
      "epoch 7626, loss 0.012355691753327847, R2 0.486422598361969\n",
      "Eval loss 0.012351094745099545, R2 0.5242489576339722\n",
      "epoch 7627, loss 0.012355664744973183, R2 0.4864243268966675\n",
      "Eval loss 0.012351064942777157, R2 0.5242500305175781\n",
      "epoch 7628, loss 0.012355638667941093, R2 0.48642498254776\n",
      "Eval loss 0.012351036071777344, R2 0.5242509841918945\n",
      "epoch 7629, loss 0.012355613522231579, R2 0.48642581701278687\n",
      "Eval loss 0.012351006269454956, R2 0.5242522358894348\n",
      "epoch 7630, loss 0.01235558744519949, R2 0.48642677068710327\n",
      "Eval loss 0.012350977398455143, R2 0.524253249168396\n",
      "epoch 7631, loss 0.012355562299489975, R2 0.4864279627799988\n",
      "Eval loss 0.012350947596132755, R2 0.524254560470581\n",
      "epoch 7632, loss 0.012355535291135311, R2 0.4864290952682495\n",
      "Eval loss 0.012350917793810368, R2 0.524255633354187\n",
      "epoch 7633, loss 0.012355509214103222, R2 0.48643016815185547\n",
      "Eval loss 0.01235088799148798, R2 0.524256706237793\n",
      "epoch 7634, loss 0.012355484068393707, R2 0.486431360244751\n",
      "Eval loss 0.012350859120488167, R2 0.5242578983306885\n",
      "epoch 7635, loss 0.012355457060039043, R2 0.4864323139190674\n",
      "Eval loss 0.012350830249488354, R2 0.5242590308189392\n",
      "epoch 7636, loss 0.012355430983006954, R2 0.4864334464073181\n",
      "Eval loss 0.012350800447165966, R2 0.5242601633071899\n",
      "epoch 7637, loss 0.01235540583729744, R2 0.4864346385002136\n",
      "Eval loss 0.012350770644843578, R2 0.5242613554000854\n",
      "epoch 7638, loss 0.01235537976026535, R2 0.4864357113838196\n",
      "Eval loss 0.012350741773843765, R2 0.5242626070976257\n",
      "epoch 7639, loss 0.012355354614555836, R2 0.4864366054534912\n",
      "Eval loss 0.012350712902843952, R2 0.5242633819580078\n",
      "epoch 7640, loss 0.012355327606201172, R2 0.48643773794174194\n",
      "Eval loss 0.012350683100521564, R2 0.5242644548416138\n",
      "epoch 7641, loss 0.012355301529169083, R2 0.4864392876625061\n",
      "Eval loss 0.012350653298199177, R2 0.5242658257484436\n",
      "epoch 7642, loss 0.012355276383459568, R2 0.4864395260810852\n",
      "Eval loss 0.012350624427199364, R2 0.5242667198181152\n",
      "epoch 7643, loss 0.012355249375104904, R2 0.4864409565925598\n",
      "Eval loss 0.012350593693554401, R2 0.5242682695388794\n",
      "epoch 7644, loss 0.01235522422939539, R2 0.486442506313324\n",
      "Eval loss 0.012350566685199738, R2 0.5242692232131958\n",
      "epoch 7645, loss 0.012355199083685875, R2 0.4864438772201538\n",
      "Eval loss 0.01235053688287735, R2 0.5242702960968018\n",
      "epoch 7646, loss 0.012355173006653786, R2 0.4864441156387329\n",
      "Eval loss 0.012350507080554962, R2 0.5242716073989868\n",
      "epoch 7647, loss 0.012355145998299122, R2 0.4864450693130493\n",
      "Eval loss 0.012350479140877724, R2 0.5242725610733032\n",
      "epoch 7648, loss 0.012355120852589607, R2 0.4864463210105896\n",
      "Eval loss 0.012350449338555336, R2 0.524273693561554\n",
      "epoch 7649, loss 0.012355095706880093, R2 0.4864479899406433\n",
      "Eval loss 0.012350419536232948, R2 0.5242748260498047\n",
      "epoch 7650, loss 0.012355070561170578, R2 0.48644840717315674\n",
      "Eval loss 0.012350390665233135, R2 0.5242756605148315\n",
      "epoch 7651, loss 0.012355044484138489, R2 0.4864491820335388\n",
      "Eval loss 0.012350360862910748, R2 0.5242772698402405\n",
      "epoch 7652, loss 0.0123550184071064, R2 0.486450731754303\n",
      "Eval loss 0.012350332923233509, R2 0.5242781639099121\n",
      "epoch 7653, loss 0.01235499419271946, R2 0.4864521026611328\n",
      "Eval loss 0.012350304052233696, R2 0.5242795944213867\n",
      "epoch 7654, loss 0.01235496811568737, R2 0.48645299673080444\n",
      "Eval loss 0.012350275181233883, R2 0.5242801308631897\n",
      "epoch 7655, loss 0.012354942038655281, R2 0.4864542484283447\n",
      "Eval loss 0.01235024631023407, R2 0.5242817997932434\n",
      "epoch 7656, loss 0.012354915961623192, R2 0.48645466566085815\n",
      "Eval loss 0.012350215576589108, R2 0.5242828130722046\n",
      "epoch 7657, loss 0.012354890815913677, R2 0.486456036567688\n",
      "Eval loss 0.012350187636911869, R2 0.524283766746521\n",
      "epoch 7658, loss 0.012354864738881588, R2 0.48645710945129395\n",
      "Eval loss 0.012350158765912056, R2 0.5242847204208374\n",
      "epoch 7659, loss 0.012354841455817223, R2 0.48645859956741333\n",
      "Eval loss 0.012350129894912243, R2 0.5242858529090881\n",
      "epoch 7660, loss 0.012354814447462559, R2 0.48645907640457153\n",
      "Eval loss 0.012350100092589855, R2 0.5242871046066284\n",
      "epoch 7661, loss 0.01235478837043047, R2 0.48645997047424316\n",
      "Eval loss 0.012350073084235191, R2 0.5242883563041687\n",
      "epoch 7662, loss 0.012354763224720955, R2 0.48646116256713867\n",
      "Eval loss 0.012350044213235378, R2 0.5242891311645508\n",
      "epoch 7663, loss 0.012354737147688866, R2 0.48646223545074463\n",
      "Eval loss 0.012350015342235565, R2 0.524290144443512\n",
      "epoch 7664, loss 0.012354711070656776, R2 0.4864633083343506\n",
      "Eval loss 0.012349985539913177, R2 0.5242915749549866\n",
      "epoch 7665, loss 0.012354686856269836, R2 0.4864645004272461\n",
      "Eval loss 0.012349956668913364, R2 0.5242924094200134\n",
      "epoch 7666, loss 0.012354661710560322, R2 0.4864653944969177\n",
      "Eval loss 0.012349928729236126, R2 0.5242936611175537\n",
      "epoch 7667, loss 0.012354636564850807, R2 0.4864664673805237\n",
      "Eval loss 0.012349897995591164, R2 0.5242947936058044\n",
      "epoch 7668, loss 0.012354610487818718, R2 0.48646771907806396\n",
      "Eval loss 0.012349870055913925, R2 0.5242961645126343\n",
      "epoch 7669, loss 0.012354585342109203, R2 0.4864685535430908\n",
      "Eval loss 0.012349841184914112, R2 0.5242968797683716\n",
      "epoch 7670, loss 0.012354559265077114, R2 0.48646968603134155\n",
      "Eval loss 0.012349812313914299, R2 0.5242982506752014\n",
      "epoch 7671, loss 0.012354535050690174, R2 0.4864712953567505\n",
      "Eval loss 0.012349782511591911, R2 0.5242990851402283\n",
      "epoch 7672, loss 0.01235450804233551, R2 0.4864717721939087\n",
      "Eval loss 0.012349756434559822, R2 0.5243002772331238\n",
      "epoch 7673, loss 0.012354484759271145, R2 0.4864727854728699\n",
      "Eval loss 0.012349726632237434, R2 0.5243016481399536\n",
      "epoch 7674, loss 0.012354457750916481, R2 0.48647385835647583\n",
      "Eval loss 0.012349696829915047, R2 0.5243025422096252\n",
      "epoch 7675, loss 0.012354431673884392, R2 0.4864751100540161\n",
      "Eval loss 0.012349669821560383, R2 0.5243034362792969\n",
      "epoch 7676, loss 0.012354408390820026, R2 0.4864766001701355\n",
      "Eval loss 0.012349640019237995, R2 0.5243051648139954\n",
      "epoch 7677, loss 0.012354383245110512, R2 0.4864776134490967\n",
      "Eval loss 0.012349611148238182, R2 0.5243062973022461\n",
      "epoch 7678, loss 0.012354358099400997, R2 0.48647868633270264\n",
      "Eval loss 0.012349582277238369, R2 0.524307131767273\n",
      "epoch 7679, loss 0.012354332022368908, R2 0.48647892475128174\n",
      "Eval loss 0.012349555268883705, R2 0.5243080258369446\n",
      "epoch 7680, loss 0.012354306876659393, R2 0.4864807724952698\n",
      "Eval loss 0.012349525466561317, R2 0.5243090391159058\n",
      "epoch 7681, loss 0.012354281730949879, R2 0.48648136854171753\n",
      "Eval loss 0.012349497526884079, R2 0.5243103504180908\n",
      "epoch 7682, loss 0.01235425565391779, R2 0.48648226261138916\n",
      "Eval loss 0.012349468655884266, R2 0.5243111848831177\n",
      "epoch 7683, loss 0.012354230508208275, R2 0.4864833354949951\n",
      "Eval loss 0.012349440716207027, R2 0.5243125557899475\n",
      "epoch 7684, loss 0.012354206293821335, R2 0.4864845275878906\n",
      "Eval loss 0.01234941091388464, R2 0.5243134498596191\n",
      "epoch 7685, loss 0.012354180216789246, R2 0.48648524284362793\n",
      "Eval loss 0.012349382042884827, R2 0.5243149399757385\n",
      "epoch 7686, loss 0.012354156002402306, R2 0.48648661375045776\n",
      "Eval loss 0.012349353171885014, R2 0.5243159532546997\n",
      "epoch 7687, loss 0.012354129925370216, R2 0.48648732900619507\n",
      "Eval loss 0.012349325232207775, R2 0.5243170261383057\n",
      "epoch 7688, loss 0.012354104779660702, R2 0.48648905754089355\n",
      "Eval loss 0.012349297292530537, R2 0.5243178009986877\n",
      "epoch 7689, loss 0.012354079633951187, R2 0.48648959398269653\n",
      "Eval loss 0.012349270284175873, R2 0.5243191719055176\n",
      "epoch 7690, loss 0.012354055419564247, R2 0.4864909052848816\n",
      "Eval loss 0.01234924141317606, R2 0.5243202447891235\n",
      "epoch 7691, loss 0.012354030273854733, R2 0.4864922761917114\n",
      "Eval loss 0.012349211610853672, R2 0.5243210792541504\n",
      "epoch 7692, loss 0.012354006059467793, R2 0.48649269342422485\n",
      "Eval loss 0.012349181808531284, R2 0.5243222713470459\n",
      "epoch 7693, loss 0.012353980913758278, R2 0.48649370670318604\n",
      "Eval loss 0.012349155731499195, R2 0.524323582649231\n",
      "epoch 7694, loss 0.012353955768048763, R2 0.48649537563323975\n",
      "Eval loss 0.012349126860499382, R2 0.5243247747421265\n",
      "epoch 7695, loss 0.012353930622339249, R2 0.4864957928657532\n",
      "Eval loss 0.012349097989499569, R2 0.5243257284164429\n",
      "epoch 7696, loss 0.01235390454530716, R2 0.48649686574935913\n",
      "Eval loss 0.012349070981144905, R2 0.5243270993232727\n",
      "epoch 7697, loss 0.01235388033092022, R2 0.48649805784225464\n",
      "Eval loss 0.012349043041467667, R2 0.5243278741836548\n",
      "epoch 7698, loss 0.012353855185210705, R2 0.4864996075630188\n",
      "Eval loss 0.012349014170467854, R2 0.5243291258811951\n",
      "epoch 7699, loss 0.012353829108178616, R2 0.4865001440048218\n",
      "Eval loss 0.012348984368145466, R2 0.5243300199508667\n",
      "epoch 7700, loss 0.012353804893791676, R2 0.4865018129348755\n",
      "Eval loss 0.012348957359790802, R2 0.5243312120437622\n",
      "epoch 7701, loss 0.012353779748082161, R2 0.4865027070045471\n",
      "Eval loss 0.012348928488790989, R2 0.524332582950592\n",
      "epoch 7702, loss 0.012353755533695221, R2 0.48650306463241577\n",
      "Eval loss 0.01234890054911375, R2 0.5243329405784607\n",
      "epoch 7703, loss 0.012353731319308281, R2 0.4865047335624695\n",
      "Eval loss 0.012348872609436512, R2 0.5243343114852905\n",
      "epoch 7704, loss 0.012353706173598766, R2 0.48650479316711426\n",
      "Eval loss 0.012348843738436699, R2 0.5243355631828308\n",
      "epoch 7705, loss 0.012353681027889252, R2 0.4865063428878784\n",
      "Eval loss 0.01234881579875946, R2 0.5243366360664368\n",
      "epoch 7706, loss 0.012353654950857162, R2 0.48650771379470825\n",
      "Eval loss 0.012348787859082222, R2 0.5243378281593323\n",
      "epoch 7707, loss 0.012353631667792797, R2 0.48650819063186646\n",
      "Eval loss 0.012348760850727558, R2 0.5243391990661621\n",
      "epoch 7708, loss 0.012353606522083282, R2 0.4865097403526306\n",
      "Eval loss 0.012348731979727745, R2 0.524340033531189\n",
      "epoch 7709, loss 0.012353581376373768, R2 0.4865102767944336\n",
      "Eval loss 0.012348703108727932, R2 0.524340808391571\n",
      "epoch 7710, loss 0.012353557161986828, R2 0.48651111125946045\n",
      "Eval loss 0.012348675169050694, R2 0.5243418216705322\n",
      "epoch 7711, loss 0.012353532947599888, R2 0.48651283979415894\n",
      "Eval loss 0.01234864629805088, R2 0.5243430137634277\n",
      "epoch 7712, loss 0.012353506870567799, R2 0.4865133762359619\n",
      "Eval loss 0.012348619289696217, R2 0.5243442058563232\n",
      "epoch 7713, loss 0.012353482656180859, R2 0.4865151047706604\n",
      "Eval loss 0.012348590418696404, R2 0.5243452787399292\n",
      "epoch 7714, loss 0.012353458441793919, R2 0.4865154027938843\n",
      "Eval loss 0.012348562479019165, R2 0.5243468284606934\n",
      "epoch 7715, loss 0.01235343236476183, R2 0.48651665449142456\n",
      "Eval loss 0.012348535470664501, R2 0.5243478417396545\n",
      "epoch 7716, loss 0.012353409081697464, R2 0.4865174889564514\n",
      "Eval loss 0.012348506599664688, R2 0.5243484973907471\n",
      "epoch 7717, loss 0.012353383004665375, R2 0.4865185618400574\n",
      "Eval loss 0.012348479591310024, R2 0.5243493318557739\n",
      "epoch 7718, loss 0.012353358790278435, R2 0.4865201711654663\n",
      "Eval loss 0.012348449788987637, R2 0.5243508815765381\n",
      "epoch 7719, loss 0.012353334575891495, R2 0.48652058839797974\n",
      "Eval loss 0.012348422780632973, R2 0.5243515968322754\n",
      "epoch 7720, loss 0.01235330943018198, R2 0.48652178049087524\n",
      "Eval loss 0.012348394840955734, R2 0.5243528485298157\n",
      "epoch 7721, loss 0.01235328521579504, R2 0.4865227937698364\n",
      "Eval loss 0.012348366901278496, R2 0.5243536233901978\n",
      "epoch 7722, loss 0.0123532610014081, R2 0.48652344942092896\n",
      "Eval loss 0.012348338030278683, R2 0.5243547558784485\n",
      "epoch 7723, loss 0.012353235855698586, R2 0.48652464151382446\n",
      "Eval loss 0.012348311021924019, R2 0.5243561863899231\n",
      "epoch 7724, loss 0.012353210709989071, R2 0.48652637004852295\n",
      "Eval loss 0.01234828308224678, R2 0.5243573188781738\n",
      "epoch 7725, loss 0.012353186495602131, R2 0.4865269064903259\n",
      "Eval loss 0.012348255142569542, R2 0.5243583917617798\n",
      "epoch 7726, loss 0.012353163212537766, R2 0.486527681350708\n",
      "Eval loss 0.012348226271569729, R2 0.5243593454360962\n",
      "epoch 7727, loss 0.012353138066828251, R2 0.4865288734436035\n",
      "Eval loss 0.01234820019453764, R2 0.5243605375289917\n",
      "epoch 7728, loss 0.012353112921118736, R2 0.4865299463272095\n",
      "Eval loss 0.012348172254860401, R2 0.5243614315986633\n",
      "epoch 7729, loss 0.012353088706731796, R2 0.48653125762939453\n",
      "Eval loss 0.012348142452538013, R2 0.5243625640869141\n",
      "epoch 7730, loss 0.012353064492344856, R2 0.4865317940711975\n",
      "Eval loss 0.012348116375505924, R2 0.5243637561798096\n",
      "epoch 7731, loss 0.012353040277957916, R2 0.48653340339660645\n",
      "Eval loss 0.01234808936715126, R2 0.5243648886680603\n",
      "epoch 7732, loss 0.012353016063570976, R2 0.4865338206291199\n",
      "Eval loss 0.012348060496151447, R2 0.5243657231330872\n",
      "epoch 7733, loss 0.012352990917861462, R2 0.4865350127220154\n",
      "Eval loss 0.012348032556474209, R2 0.5243669152259827\n",
      "epoch 7734, loss 0.012352966703474522, R2 0.48653602600097656\n",
      "Eval loss 0.01234800647944212, R2 0.5243681073188782\n",
      "epoch 7735, loss 0.012352942489087582, R2 0.4865368604660034\n",
      "Eval loss 0.012347976677119732, R2 0.5243686437606812\n",
      "epoch 7736, loss 0.012352918274700642, R2 0.48653799295425415\n",
      "Eval loss 0.012347949668765068, R2 0.5243702530860901\n",
      "epoch 7737, loss 0.012352894060313702, R2 0.4865388870239258\n",
      "Eval loss 0.012347922660410404, R2 0.5243709087371826\n",
      "epoch 7738, loss 0.012352869845926762, R2 0.48653990030288696\n",
      "Eval loss 0.012347893789410591, R2 0.524371862411499\n",
      "epoch 7739, loss 0.012352846562862396, R2 0.48654067516326904\n",
      "Eval loss 0.012347865849733353, R2 0.5243736505508423\n",
      "epoch 7740, loss 0.012352821417152882, R2 0.4865420460700989\n",
      "Eval loss 0.012347839772701263, R2 0.5243741273880005\n",
      "epoch 7741, loss 0.012352796271443367, R2 0.4865429401397705\n",
      "Eval loss 0.012347811833024025, R2 0.5243754386901855\n",
      "epoch 7742, loss 0.012352772988379002, R2 0.4865444302558899\n",
      "Eval loss 0.012347782962024212, R2 0.5243765115737915\n",
      "epoch 7743, loss 0.012352747842669487, R2 0.4865454435348511\n",
      "Eval loss 0.012347755953669548, R2 0.524377703666687\n",
      "epoch 7744, loss 0.012352723628282547, R2 0.4865459203720093\n",
      "Eval loss 0.012347729876637459, R2 0.5243782997131348\n",
      "epoch 7745, loss 0.012352700345218182, R2 0.48654693365097046\n",
      "Eval loss 0.012347701005637646, R2 0.5243796110153198\n",
      "epoch 7746, loss 0.012352676130831242, R2 0.48654794692993164\n",
      "Eval loss 0.012347673065960407, R2 0.5243805050849915\n",
      "epoch 7747, loss 0.012352650985121727, R2 0.4865490198135376\n",
      "Eval loss 0.012347646057605743, R2 0.5243815183639526\n",
      "epoch 7748, loss 0.012352627702057362, R2 0.4865504503250122\n",
      "Eval loss 0.01234761904925108, R2 0.524382472038269\n",
      "epoch 7749, loss 0.012352602556347847, R2 0.4865511655807495\n",
      "Eval loss 0.012347591109573841, R2 0.5243836641311646\n",
      "epoch 7750, loss 0.012352578341960907, R2 0.4865521788597107\n",
      "Eval loss 0.012347564101219177, R2 0.5243850946426392\n",
      "epoch 7751, loss 0.012352554127573967, R2 0.4865531325340271\n",
      "Eval loss 0.012347536161541939, R2 0.524385929107666\n",
      "epoch 7752, loss 0.012352530844509602, R2 0.4865541458129883\n",
      "Eval loss 0.0123475082218647, R2 0.5243868827819824\n",
      "epoch 7753, loss 0.012352506630122662, R2 0.48655498027801514\n",
      "Eval loss 0.012347482144832611, R2 0.5243877172470093\n",
      "epoch 7754, loss 0.012352481484413147, R2 0.4865559935569763\n",
      "Eval loss 0.012347455136477947, R2 0.5243891477584839\n",
      "epoch 7755, loss 0.012352459132671356, R2 0.48655712604522705\n",
      "Eval loss 0.012347426265478134, R2 0.5243898630142212\n",
      "epoch 7756, loss 0.012352433986961842, R2 0.48655813932418823\n",
      "Eval loss 0.012347398325800896, R2 0.5243911743164062\n",
      "epoch 7757, loss 0.012352409772574902, R2 0.48655951023101807\n",
      "Eval loss 0.012347374111413956, R2 0.5243924856185913\n",
      "epoch 7758, loss 0.012352386489510536, R2 0.48655998706817627\n",
      "Eval loss 0.012347345240414143, R2 0.5243935585021973\n",
      "epoch 7759, loss 0.012352362275123596, R2 0.48656129837036133\n",
      "Eval loss 0.01234731636941433, R2 0.5243945121765137\n",
      "epoch 7760, loss 0.012352338060736656, R2 0.48656201362609863\n",
      "Eval loss 0.01234729029238224, R2 0.5243954062461853\n",
      "epoch 7761, loss 0.012352313846349716, R2 0.48656296730041504\n",
      "Eval loss 0.012347261421382427, R2 0.524396538734436\n",
      "epoch 7762, loss 0.012352288700640202, R2 0.486564040184021\n",
      "Eval loss 0.012347235344350338, R2 0.5243974924087524\n",
      "epoch 7763, loss 0.012352265417575836, R2 0.48656487464904785\n",
      "Eval loss 0.012347208335995674, R2 0.5243984460830688\n",
      "epoch 7764, loss 0.012352243065834045, R2 0.4865659475326538\n",
      "Eval loss 0.012347180396318436, R2 0.5243997573852539\n",
      "epoch 7765, loss 0.01235221792012453, R2 0.48656749725341797\n",
      "Eval loss 0.012347154319286346, R2 0.524401068687439\n",
      "epoch 7766, loss 0.012352192774415016, R2 0.4865682125091553\n",
      "Eval loss 0.012347127310931683, R2 0.5244014263153076\n",
      "epoch 7767, loss 0.0123521713539958, R2 0.4865691065788269\n",
      "Eval loss 0.012347099371254444, R2 0.5244027376174927\n",
      "epoch 7768, loss 0.012352146208286285, R2 0.4865705966949463\n",
      "Eval loss 0.012347071431577206, R2 0.5244036912918091\n",
      "epoch 7769, loss 0.012352121993899345, R2 0.48657095432281494\n",
      "Eval loss 0.012347044423222542, R2 0.5244044661521912\n",
      "epoch 7770, loss 0.01235209871083498, R2 0.4865719676017761\n",
      "Eval loss 0.012347016483545303, R2 0.5244059562683105\n",
      "epoch 7771, loss 0.01235207263380289, R2 0.4865730404853821\n",
      "Eval loss 0.012346990406513214, R2 0.5244067907333374\n",
      "epoch 7772, loss 0.012352051213383675, R2 0.4865739345550537\n",
      "Eval loss 0.012346964329481125, R2 0.5244078636169434\n",
      "epoch 7773, loss 0.012352026998996735, R2 0.48657506704330444\n",
      "Eval loss 0.012346937321126461, R2 0.5244089961051941\n",
      "epoch 7774, loss 0.012352002784609795, R2 0.4865759015083313\n",
      "Eval loss 0.012346908450126648, R2 0.5244102478027344\n",
      "epoch 7775, loss 0.012351980432868004, R2 0.4865768551826477\n",
      "Eval loss 0.012346881441771984, R2 0.5244108438491821\n",
      "epoch 7776, loss 0.01235195528715849, R2 0.4865778684616089\n",
      "Eval loss 0.01234685443341732, R2 0.5244125127792358\n",
      "epoch 7777, loss 0.012351932004094124, R2 0.48657935857772827\n",
      "Eval loss 0.012346827425062656, R2 0.5244133472442627\n",
      "epoch 7778, loss 0.012351907789707184, R2 0.4865800142288208\n",
      "Eval loss 0.012346799485385418, R2 0.5244144201278687\n",
      "epoch 7779, loss 0.012351883575320244, R2 0.4865815043449402\n",
      "Eval loss 0.012346774339675903, R2 0.5244155526161194\n",
      "epoch 7780, loss 0.012351860292255878, R2 0.48658186197280884\n",
      "Eval loss 0.012346746399998665, R2 0.5244163274765015\n",
      "epoch 7781, loss 0.012351837009191513, R2 0.4865834712982178\n",
      "Eval loss 0.012346718460321426, R2 0.5244172811508179\n",
      "epoch 7782, loss 0.012351812794804573, R2 0.4865838289260864\n",
      "Eval loss 0.012346692383289337, R2 0.524418830871582\n",
      "epoch 7783, loss 0.012351789511740208, R2 0.48658478260040283\n",
      "Eval loss 0.012346665374934673, R2 0.5244191884994507\n",
      "epoch 7784, loss 0.012351766228675842, R2 0.48658591508865356\n",
      "Eval loss 0.01234663836658001, R2 0.5244204998016357\n",
      "epoch 7785, loss 0.012351742014288902, R2 0.4865867495536804\n",
      "Eval loss 0.01234661228954792, R2 0.5244213938713074\n",
      "epoch 7786, loss 0.012351716868579388, R2 0.4865878224372864\n",
      "Eval loss 0.012346586212515831, R2 0.5244222283363342\n",
      "epoch 7787, loss 0.012351694516837597, R2 0.4865887761116028\n",
      "Eval loss 0.012346558272838593, R2 0.5244233012199402\n",
      "epoch 7788, loss 0.012351670302450657, R2 0.4865899085998535\n",
      "Eval loss 0.012346533127129078, R2 0.5244243144989014\n",
      "epoch 7789, loss 0.012351647019386292, R2 0.4865906834602356\n",
      "Eval loss 0.012346504256129265, R2 0.5244258046150208\n",
      "epoch 7790, loss 0.0123516246676445, R2 0.486591637134552\n",
      "Eval loss 0.012346477247774601, R2 0.5244264602661133\n",
      "epoch 7791, loss 0.01235160045325756, R2 0.4865931272506714\n",
      "Eval loss 0.012346451170742512, R2 0.524428129196167\n",
      "epoch 7792, loss 0.012351577170193195, R2 0.4865937829017639\n",
      "Eval loss 0.012346423231065273, R2 0.5244287848472595\n",
      "epoch 7793, loss 0.012351552955806255, R2 0.4865947961807251\n",
      "Eval loss 0.012346398085355759, R2 0.5244297385215759\n",
      "epoch 7794, loss 0.01235152967274189, R2 0.48659539222717285\n",
      "Eval loss 0.012346371077001095, R2 0.5244308114051819\n",
      "epoch 7795, loss 0.012351506389677525, R2 0.48659658432006836\n",
      "Eval loss 0.012346344068646431, R2 0.5244317054748535\n",
      "epoch 7796, loss 0.012351482175290585, R2 0.4865978956222534\n",
      "Eval loss 0.012346316128969193, R2 0.524432897567749\n",
      "epoch 7797, loss 0.012351459823548794, R2 0.48659849166870117\n",
      "Eval loss 0.012346290051937103, R2 0.5244340896606445\n",
      "epoch 7798, loss 0.012351435609161854, R2 0.48659950494766235\n",
      "Eval loss 0.012346263974905014, R2 0.5244348049163818\n",
      "epoch 7799, loss 0.012351411394774914, R2 0.4866003394126892\n",
      "Eval loss 0.012346235103905201, R2 0.5244359970092773\n",
      "epoch 7800, loss 0.012351388111710548, R2 0.4866012930870056\n",
      "Eval loss 0.012346210889518261, R2 0.5244373083114624\n",
      "epoch 7801, loss 0.012351364828646183, R2 0.4866030812263489\n",
      "Eval loss 0.012346183881163597, R2 0.5244381427764893\n",
      "epoch 7802, loss 0.012351341545581818, R2 0.48660343885421753\n",
      "Eval loss 0.012346155941486359, R2 0.5244387984275818\n",
      "epoch 7803, loss 0.012351318262517452, R2 0.48660439252853394\n",
      "Eval loss 0.012346131727099419, R2 0.5244399309158325\n",
      "epoch 7804, loss 0.012351294979453087, R2 0.48660534620285034\n",
      "Eval loss 0.012346104718744755, R2 0.5244412422180176\n",
      "epoch 7805, loss 0.012351271696388721, R2 0.48660629987716675\n",
      "Eval loss 0.012346076779067516, R2 0.5244419574737549\n",
      "epoch 7806, loss 0.012351248413324356, R2 0.48660707473754883\n",
      "Eval loss 0.012346050702035427, R2 0.5244430303573608\n",
      "epoch 7807, loss 0.01235122513025999, R2 0.48660874366760254\n",
      "Eval loss 0.012346023693680763, R2 0.5244442224502563\n",
      "epoch 7808, loss 0.01235120091587305, R2 0.48660939931869507\n",
      "Eval loss 0.0123459966853261, R2 0.5244453549385071\n",
      "epoch 7809, loss 0.01235117856413126, R2 0.4866105318069458\n",
      "Eval loss 0.012345971539616585, R2 0.5244460701942444\n",
      "epoch 7810, loss 0.012351155281066895, R2 0.48661112785339355\n",
      "Eval loss 0.012345944531261921, R2 0.5244472026824951\n",
      "epoch 7811, loss 0.012351131066679955, R2 0.48661261796951294\n",
      "Eval loss 0.012345916591584682, R2 0.5244482755661011\n",
      "epoch 7812, loss 0.012351108714938164, R2 0.48661309480667114\n",
      "Eval loss 0.012345891445875168, R2 0.5244489908218384\n",
      "epoch 7813, loss 0.012351084500551224, R2 0.4866141080856323\n",
      "Eval loss 0.012345864437520504, R2 0.5244501829147339\n",
      "epoch 7814, loss 0.012351063080132008, R2 0.4866151809692383\n",
      "Eval loss 0.012345838360488415, R2 0.5244516134262085\n",
      "epoch 7815, loss 0.012351038865745068, R2 0.4866161346435547\n",
      "Eval loss 0.0123458132147789, R2 0.5244524478912354\n",
      "epoch 7816, loss 0.012351014651358128, R2 0.48661696910858154\n",
      "Eval loss 0.012345785275101662, R2 0.5244531035423279\n",
      "epoch 7817, loss 0.012350993230938911, R2 0.4866182208061218\n",
      "Eval loss 0.012345758266746998, R2 0.5244543552398682\n",
      "epoch 7818, loss 0.012350968085229397, R2 0.48661959171295166\n",
      "Eval loss 0.012345732189714909, R2 0.5244556665420532\n",
      "epoch 7819, loss 0.012350945733487606, R2 0.48661988973617554\n",
      "Eval loss 0.012345707044005394, R2 0.5244563817977905\n",
      "epoch 7820, loss 0.01235092245042324, R2 0.4866206645965576\n",
      "Eval loss 0.012345679104328156, R2 0.5244571566581726\n",
      "epoch 7821, loss 0.01235090009868145, R2 0.486621618270874\n",
      "Eval loss 0.012345653958618641, R2 0.5244581699371338\n",
      "epoch 7822, loss 0.012350876815617085, R2 0.48662275075912476\n",
      "Eval loss 0.012345626950263977, R2 0.5244600176811218\n",
      "epoch 7823, loss 0.012350853532552719, R2 0.48662418127059937\n",
      "Eval loss 0.012345600873231888, R2 0.5244605541229248\n",
      "epoch 7824, loss 0.012350830249488354, R2 0.48662465810775757\n",
      "Eval loss 0.012345574796199799, R2 0.5244612097740173\n",
      "epoch 7825, loss 0.012350806966423988, R2 0.4866260886192322\n",
      "Eval loss 0.01234554871916771, R2 0.5244622230529785\n",
      "epoch 7826, loss 0.012350784614682198, R2 0.4866270422935486\n",
      "Eval loss 0.012345521710813046, R2 0.5244633555412292\n",
      "epoch 7827, loss 0.012350759468972683, R2 0.4866277575492859\n",
      "Eval loss 0.012345495633780956, R2 0.5244642496109009\n",
      "epoch 7828, loss 0.012350738979876041, R2 0.4866284728050232\n",
      "Eval loss 0.012345470488071442, R2 0.5244652032852173\n",
      "epoch 7829, loss 0.012350714765489101, R2 0.4866294860839844\n",
      "Eval loss 0.012345442548394203, R2 0.5244662761688232\n",
      "epoch 7830, loss 0.012350691482424736, R2 0.4866304397583008\n",
      "Eval loss 0.012345418334007263, R2 0.5244672298431396\n",
      "epoch 7831, loss 0.012350669130682945, R2 0.4866313934326172\n",
      "Eval loss 0.0123453913256526, R2 0.5244684219360352\n",
      "epoch 7832, loss 0.01235064584761858, R2 0.4866328239440918\n",
      "Eval loss 0.01234536524862051, R2 0.5244698524475098\n",
      "epoch 7833, loss 0.012350623495876789, R2 0.4866332411766052\n",
      "Eval loss 0.012345338240265846, R2 0.5244707465171814\n",
      "epoch 7834, loss 0.012350599281489849, R2 0.4866342544555664\n",
      "Eval loss 0.012345312163233757, R2 0.5244715809822083\n",
      "epoch 7835, loss 0.012350577861070633, R2 0.48663562536239624\n",
      "Eval loss 0.012345286086201668, R2 0.5244722962379456\n",
      "epoch 7836, loss 0.012350553646683693, R2 0.48663681745529175\n",
      "Eval loss 0.012345260940492153, R2 0.5244733095169067\n",
      "epoch 7837, loss 0.012350530363619328, R2 0.4866371154785156\n",
      "Eval loss 0.01234523393213749, R2 0.5244745016098022\n",
      "epoch 7838, loss 0.012350509874522686, R2 0.48663800954818726\n",
      "Eval loss 0.012345206923782825, R2 0.5244758129119873\n",
      "epoch 7839, loss 0.012350484728813171, R2 0.48663949966430664\n",
      "Eval loss 0.012345180846750736, R2 0.5244764089584351\n",
      "epoch 7840, loss 0.012350463308393955, R2 0.4866405725479126\n",
      "Eval loss 0.012345156632363796, R2 0.5244774222373962\n",
      "epoch 7841, loss 0.012350440956652164, R2 0.48664116859436035\n",
      "Eval loss 0.012345129624009132, R2 0.5244786143302917\n",
      "epoch 7842, loss 0.012350416742265224, R2 0.4866420030593872\n",
      "Eval loss 0.012345103546977043, R2 0.5244796276092529\n",
      "epoch 7843, loss 0.012350393459200859, R2 0.48664283752441406\n",
      "Eval loss 0.01234507653862238, R2 0.5244805216789246\n",
      "epoch 7844, loss 0.012350371107459068, R2 0.4866437315940857\n",
      "Eval loss 0.01234505232423544, R2 0.5244817137718201\n",
      "epoch 7845, loss 0.012350349687039852, R2 0.48664528131484985\n",
      "Eval loss 0.01234502624720335, R2 0.5244823694229126\n",
      "epoch 7846, loss 0.012350325472652912, R2 0.4866456389427185\n",
      "Eval loss 0.012344999238848686, R2 0.5244832038879395\n",
      "epoch 7847, loss 0.012350303120911121, R2 0.48664623498916626\n",
      "Eval loss 0.012344971299171448, R2 0.524484395980835\n",
      "epoch 7848, loss 0.012350279837846756, R2 0.48664772510528564\n",
      "Eval loss 0.012344948947429657, R2 0.5244852900505066\n",
      "epoch 7849, loss 0.012350257486104965, R2 0.48664844036102295\n",
      "Eval loss 0.012344921939074993, R2 0.5244864821434021\n",
      "epoch 7850, loss 0.012350236065685749, R2 0.4866493344306946\n",
      "Eval loss 0.01234489493072033, R2 0.5244874954223633\n",
      "epoch 7851, loss 0.012350211851298809, R2 0.48665082454681396\n",
      "Eval loss 0.01234487071633339, R2 0.5244885683059692\n",
      "epoch 7852, loss 0.012350189499557018, R2 0.48665130138397217\n",
      "Eval loss 0.012344843707978725, R2 0.5244895219802856\n",
      "epoch 7853, loss 0.012350166216492653, R2 0.4866522550582886\n",
      "Eval loss 0.01234481856226921, R2 0.5244903564453125\n",
      "epoch 7854, loss 0.012350142933428288, R2 0.48665326833724976\n",
      "Eval loss 0.012344792485237122, R2 0.5244917869567871\n",
      "epoch 7855, loss 0.012350121513009071, R2 0.4866545796394348\n",
      "Eval loss 0.012344766408205032, R2 0.524492621421814\n",
      "epoch 7856, loss 0.012350098229944706, R2 0.486655056476593\n",
      "Eval loss 0.012344741262495518, R2 0.5244933366775513\n",
      "epoch 7857, loss 0.012350075878202915, R2 0.4866560101509094\n",
      "Eval loss 0.012344716116786003, R2 0.5244946479797363\n",
      "epoch 7858, loss 0.01235005259513855, R2 0.48665744066238403\n",
      "Eval loss 0.012344688177108765, R2 0.5244956016540527\n",
      "epoch 7859, loss 0.012350031174719334, R2 0.4866580367088318\n",
      "Eval loss 0.012344663962721825, R2 0.5244965553283691\n",
      "epoch 7860, loss 0.012350008822977543, R2 0.48665910959243774\n",
      "Eval loss 0.012344637885689735, R2 0.524497389793396\n",
      "epoch 7861, loss 0.012349986471235752, R2 0.4866604208946228\n",
      "Eval loss 0.012344611808657646, R2 0.5244985818862915\n",
      "epoch 7862, loss 0.012349963188171387, R2 0.4866607189178467\n",
      "Eval loss 0.012344587594270706, R2 0.5244996547698975\n",
      "epoch 7863, loss 0.012349939905107021, R2 0.4866621494293213\n",
      "Eval loss 0.012344560585916042, R2 0.5245005488395691\n",
      "epoch 7864, loss 0.01234991755336523, R2 0.4866626262664795\n",
      "Eval loss 0.012344533577561378, R2 0.5245011448860168\n",
      "epoch 7865, loss 0.012349894270300865, R2 0.4866635799407959\n",
      "Eval loss 0.012344509363174438, R2 0.5245025157928467\n",
      "epoch 7866, loss 0.012349872849881649, R2 0.48666447401046753\n",
      "Eval loss 0.01234448328614235, R2 0.5245033502578735\n",
      "epoch 7867, loss 0.012349850498139858, R2 0.4866655468940735\n",
      "Eval loss 0.01234445720911026, R2 0.5245041847229004\n",
      "epoch 7868, loss 0.012349828146398067, R2 0.48666679859161377\n",
      "Eval loss 0.012344432063400745, R2 0.5245052576065063\n",
      "epoch 7869, loss 0.012349804863333702, R2 0.486667275428772\n",
      "Eval loss 0.012344405986368656, R2 0.5245068073272705\n",
      "epoch 7870, loss 0.012349782511591911, R2 0.48666882514953613\n",
      "Eval loss 0.012344380840659142, R2 0.5245071649551392\n",
      "epoch 7871, loss 0.01234976015985012, R2 0.48666930198669434\n",
      "Eval loss 0.012344354763627052, R2 0.5245085954666138\n",
      "epoch 7872, loss 0.01234973780810833, R2 0.48667019605636597\n",
      "Eval loss 0.012344329617917538, R2 0.5245095491409302\n",
      "epoch 7873, loss 0.012349715456366539, R2 0.4866710305213928\n",
      "Eval loss 0.012344304472208023, R2 0.5245101451873779\n",
      "epoch 7874, loss 0.012349694035947323, R2 0.486672043800354\n",
      "Eval loss 0.012344278395175934, R2 0.5245113968849182\n",
      "epoch 7875, loss 0.012349671684205532, R2 0.4866732954978943\n",
      "Eval loss 0.012344252318143845, R2 0.5245121121406555\n",
      "epoch 7876, loss 0.012349648401141167, R2 0.486674427986145\n",
      "Eval loss 0.01234422717243433, R2 0.5245132446289062\n",
      "epoch 7877, loss 0.012349626049399376, R2 0.4866747260093689\n",
      "Eval loss 0.012344202026724815, R2 0.5245141983032227\n",
      "epoch 7878, loss 0.012349603697657585, R2 0.48667627573013306\n",
      "Eval loss 0.0123441768810153, R2 0.5245152711868286\n",
      "epoch 7879, loss 0.01234958041459322, R2 0.48667657375335693\n",
      "Eval loss 0.012344150803983212, R2 0.5245160460472107\n",
      "epoch 7880, loss 0.012349558994174004, R2 0.48667818307876587\n",
      "Eval loss 0.012344125658273697, R2 0.5245171785354614\n",
      "epoch 7881, loss 0.012349535711109638, R2 0.4866787791252136\n",
      "Eval loss 0.012344099581241608, R2 0.5245181322097778\n",
      "epoch 7882, loss 0.012349514290690422, R2 0.4866793751716614\n",
      "Eval loss 0.012344074435532093, R2 0.5245194435119629\n",
      "epoch 7883, loss 0.012349491938948631, R2 0.48668092489242554\n",
      "Eval loss 0.012344049289822578, R2 0.5245200991630554\n",
      "epoch 7884, loss 0.01234946958720684, R2 0.48668140172958374\n",
      "Eval loss 0.012344024144113064, R2 0.5245212316513062\n",
      "epoch 7885, loss 0.012349448166787624, R2 0.4866819381713867\n",
      "Eval loss 0.012343998067080975, R2 0.5245221853256226\n",
      "epoch 7886, loss 0.012349424883723259, R2 0.48668307065963745\n",
      "Eval loss 0.01234397292137146, R2 0.5245229005813599\n",
      "epoch 7887, loss 0.012349403463304043, R2 0.4866841435432434\n",
      "Eval loss 0.01234394684433937, R2 0.5245242118835449\n",
      "epoch 7888, loss 0.012349379248917103, R2 0.4866856336593628\n",
      "Eval loss 0.012343921698629856, R2 0.5245251655578613\n",
      "epoch 7889, loss 0.012349358759820461, R2 0.48668551445007324\n",
      "Eval loss 0.012343896552920341, R2 0.5245263576507568\n",
      "epoch 7890, loss 0.01234933640807867, R2 0.4866868853569031\n",
      "Eval loss 0.012343872338533401, R2 0.5245267748832703\n",
      "epoch 7891, loss 0.01234931405633688, R2 0.48668771982192993\n",
      "Eval loss 0.012343846261501312, R2 0.5245283246040344\n",
      "epoch 7892, loss 0.012349291704595089, R2 0.48668861389160156\n",
      "Eval loss 0.012343820184469223, R2 0.5245288610458374\n",
      "epoch 7893, loss 0.012349268421530724, R2 0.4866897463798523\n",
      "Eval loss 0.012343795038759708, R2 0.5245300531387329\n",
      "epoch 7894, loss 0.012349247001111507, R2 0.4866902828216553\n",
      "Eval loss 0.012343770824372768, R2 0.5245309472084045\n",
      "epoch 7895, loss 0.012349225580692291, R2 0.48669183254241943\n",
      "Eval loss 0.012343746609985828, R2 0.5245316028594971\n",
      "epoch 7896, loss 0.012349204160273075, R2 0.48669224977493286\n",
      "Eval loss 0.012343719601631165, R2 0.5245329141616821\n",
      "epoch 7897, loss 0.012349179945886135, R2 0.4866933822631836\n",
      "Eval loss 0.01234369445592165, R2 0.5245339870452881\n",
      "epoch 7898, loss 0.012349159456789494, R2 0.4866946339607239\n",
      "Eval loss 0.01234367024153471, R2 0.5245347023010254\n",
      "epoch 7899, loss 0.012349136173725128, R2 0.4866950511932373\n",
      "Eval loss 0.012343643233180046, R2 0.5245358943939209\n",
      "epoch 7900, loss 0.012349115684628487, R2 0.48669612407684326\n",
      "Eval loss 0.012343618087470531, R2 0.5245369672775269\n",
      "epoch 7901, loss 0.012349093332886696, R2 0.48669731616973877\n",
      "Eval loss 0.012343593873083591, R2 0.5245375037193298\n",
      "epoch 7902, loss 0.01234907191246748, R2 0.486697793006897\n",
      "Eval loss 0.012343568727374077, R2 0.5245388746261597\n",
      "epoch 7903, loss 0.012349048629403114, R2 0.48669886589050293\n",
      "Eval loss 0.012343543581664562, R2 0.5245398283004761\n",
      "epoch 7904, loss 0.012349027208983898, R2 0.48669975996017456\n",
      "Eval loss 0.012343518435955048, R2 0.5245407819747925\n",
      "epoch 7905, loss 0.012349005788564682, R2 0.48670047521591187\n",
      "Eval loss 0.012343492358922958, R2 0.5245418548583984\n",
      "epoch 7906, loss 0.012348981574177742, R2 0.48670148849487305\n",
      "Eval loss 0.012343468144536018, R2 0.5245428085327148\n",
      "epoch 7907, loss 0.0123489610850811, R2 0.48670315742492676\n",
      "Eval loss 0.012343442998826504, R2 0.5245437622070312\n",
      "epoch 7908, loss 0.012348939664661884, R2 0.48670369386672974\n",
      "Eval loss 0.012343416921794415, R2 0.5245447158813477\n",
      "epoch 7909, loss 0.012348917312920094, R2 0.48670417070388794\n",
      "Eval loss 0.012343392707407475, R2 0.5245455503463745\n",
      "epoch 7910, loss 0.012348894961178303, R2 0.48670512437820435\n",
      "Eval loss 0.012343368493020535, R2 0.5245466232299805\n",
      "epoch 7911, loss 0.012348873540759087, R2 0.48670631647109985\n",
      "Eval loss 0.012343342415988445, R2 0.5245473384857178\n",
      "epoch 7912, loss 0.012348850257694721, R2 0.4867071509361267\n",
      "Eval loss 0.01234331913292408, R2 0.5245485305786133\n",
      "epoch 7913, loss 0.01234882976859808, R2 0.48670798540115356\n",
      "Eval loss 0.01234329305589199, R2 0.5245490670204163\n",
      "epoch 7914, loss 0.012348807416856289, R2 0.48670876026153564\n",
      "Eval loss 0.012343266978859901, R2 0.5245500802993774\n",
      "epoch 7915, loss 0.012348785065114498, R2 0.4867098331451416\n",
      "Eval loss 0.012343243695795536, R2 0.5245511531829834\n",
      "epoch 7916, loss 0.012348762713372707, R2 0.4867106080055237\n",
      "Eval loss 0.012343217618763447, R2 0.5245519876480103\n",
      "epoch 7917, loss 0.012348742224276066, R2 0.48671162128448486\n",
      "Eval loss 0.012343193404376507, R2 0.5245533585548401\n",
      "epoch 7918, loss 0.01234872080385685, R2 0.48671233654022217\n",
      "Eval loss 0.012343168258666992, R2 0.5245538949966431\n",
      "epoch 7919, loss 0.012348699383437634, R2 0.486713707447052\n",
      "Eval loss 0.012343142181634903, R2 0.5245550274848938\n",
      "epoch 7920, loss 0.012348677963018417, R2 0.48671430349349976\n",
      "Eval loss 0.012343117035925388, R2 0.5245562791824341\n",
      "epoch 7921, loss 0.012348655611276627, R2 0.4867156744003296\n",
      "Eval loss 0.012343093752861023, R2 0.5245569944381714\n",
      "epoch 7922, loss 0.012348633259534836, R2 0.48671597242355347\n",
      "Eval loss 0.012343069538474083, R2 0.5245578289031982\n",
      "epoch 7923, loss 0.01234861183911562, R2 0.4867175221443176\n",
      "Eval loss 0.012343044392764568, R2 0.5245589017868042\n",
      "epoch 7924, loss 0.012348589487373829, R2 0.48671776056289673\n",
      "Eval loss 0.012343019247055054, R2 0.5245598554611206\n",
      "epoch 7925, loss 0.012348568066954613, R2 0.48671871423721313\n",
      "Eval loss 0.012342995032668114, R2 0.5245606899261475\n",
      "epoch 7926, loss 0.012348544783890247, R2 0.48671966791152954\n",
      "Eval loss 0.012342969886958599, R2 0.5245619416236877\n",
      "epoch 7927, loss 0.012348524294793606, R2 0.4867205023765564\n",
      "Eval loss 0.012342944741249084, R2 0.5245625972747803\n",
      "epoch 7928, loss 0.01234850287437439, R2 0.486721396446228\n",
      "Eval loss 0.012342920526862144, R2 0.5245634317398071\n",
      "epoch 7929, loss 0.012348482385277748, R2 0.48672229051589966\n",
      "Eval loss 0.01234289538115263, R2 0.5245646834373474\n",
      "epoch 7930, loss 0.012348459102213383, R2 0.4867231845855713\n",
      "Eval loss 0.01234287116676569, R2 0.5245652198791504\n",
      "epoch 7931, loss 0.012348438613116741, R2 0.4867240786552429\n",
      "Eval loss 0.012342846021056175, R2 0.5245667099952698\n",
      "epoch 7932, loss 0.012348417192697525, R2 0.4867252707481384\n",
      "Eval loss 0.01234282087534666, R2 0.5245675444602966\n",
      "epoch 7933, loss 0.012348394840955734, R2 0.48672592639923096\n",
      "Eval loss 0.01234279666095972, R2 0.5245686769485474\n",
      "epoch 7934, loss 0.012348373420536518, R2 0.4867267608642578\n",
      "Eval loss 0.01234277244657278, R2 0.5245693922042847\n",
      "epoch 7935, loss 0.012348352000117302, R2 0.48672765493392944\n",
      "Eval loss 0.012342745438218117, R2 0.5245704650878906\n",
      "epoch 7936, loss 0.012348329648375511, R2 0.4867292642593384\n",
      "Eval loss 0.012342722155153751, R2 0.5245710611343384\n",
      "epoch 7937, loss 0.012348308227956295, R2 0.4867295026779175\n",
      "Eval loss 0.012342697009444237, R2 0.5245726108551025\n",
      "epoch 7938, loss 0.012348287738859653, R2 0.48673033714294434\n",
      "Eval loss 0.012342673726379871, R2 0.5245730876922607\n",
      "epoch 7939, loss 0.012348264455795288, R2 0.4867311120033264\n",
      "Eval loss 0.012342649511992931, R2 0.5245738625526428\n",
      "epoch 7940, loss 0.012348243966698647, R2 0.4867321848869324\n",
      "Eval loss 0.012342623434960842, R2 0.5245752930641174\n",
      "epoch 7941, loss 0.01234822254627943, R2 0.48673301935195923\n",
      "Eval loss 0.012342600151896477, R2 0.5245761871337891\n",
      "epoch 7942, loss 0.012348201125860214, R2 0.48673397302627563\n",
      "Eval loss 0.012342575006186962, R2 0.524577260017395\n",
      "epoch 7943, loss 0.012348179705440998, R2 0.4867348074913025\n",
      "Eval loss 0.012342549860477448, R2 0.5245779752731323\n",
      "epoch 7944, loss 0.012348159216344357, R2 0.4867355227470398\n",
      "Eval loss 0.012342527508735657, R2 0.5245791673660278\n",
      "epoch 7945, loss 0.012348136864602566, R2 0.48673659563064575\n",
      "Eval loss 0.012342500500380993, R2 0.5245800018310547\n",
      "epoch 7946, loss 0.012348116375505924, R2 0.4867374897003174\n",
      "Eval loss 0.012342477217316628, R2 0.5245809555053711\n",
      "epoch 7947, loss 0.012348094023764133, R2 0.486738383769989\n",
      "Eval loss 0.012342451140284538, R2 0.524582028388977\n",
      "epoch 7948, loss 0.012348072603344917, R2 0.48673927783966064\n",
      "Eval loss 0.012342428788542747, R2 0.5245823860168457\n",
      "epoch 7949, loss 0.012348050251603127, R2 0.4867400527000427\n",
      "Eval loss 0.012342403642833233, R2 0.5245837569236755\n",
      "epoch 7950, loss 0.012348029762506485, R2 0.4867408871650696\n",
      "Eval loss 0.012342379428446293, R2 0.5245845317840576\n",
      "epoch 7951, loss 0.012348007410764694, R2 0.4867420196533203\n",
      "Eval loss 0.012342354282736778, R2 0.5245853662490845\n",
      "epoch 7952, loss 0.012347986921668053, R2 0.48674362897872925\n",
      "Eval loss 0.012342330999672413, R2 0.5245864391326904\n",
      "epoch 7953, loss 0.012347965501248837, R2 0.4867437481880188\n",
      "Eval loss 0.012342305853962898, R2 0.5245872735977173\n",
      "epoch 7954, loss 0.012347945012152195, R2 0.48674476146698\n",
      "Eval loss 0.012342280708253384, R2 0.5245888233184814\n",
      "epoch 7955, loss 0.012347922660410404, R2 0.48674601316452026\n",
      "Eval loss 0.012342256493866444, R2 0.5245891809463501\n",
      "epoch 7956, loss 0.012347902171313763, R2 0.4867463707923889\n",
      "Eval loss 0.012342232279479504, R2 0.5245901942253113\n",
      "epoch 7957, loss 0.012347879819571972, R2 0.4867473244667053\n",
      "Eval loss 0.012342208065092564, R2 0.5245911478996277\n",
      "epoch 7958, loss 0.01234785933047533, R2 0.4867481589317322\n",
      "Eval loss 0.012342183850705624, R2 0.5245918035507202\n",
      "epoch 7959, loss 0.012347837910056114, R2 0.486749529838562\n",
      "Eval loss 0.012342159636318684, R2 0.5245929956436157\n",
      "epoch 7960, loss 0.012347816489636898, R2 0.48675042390823364\n",
      "Eval loss 0.012342135421931744, R2 0.5245940685272217\n",
      "epoch 7961, loss 0.012347795069217682, R2 0.4867507815361023\n",
      "Eval loss 0.012342111207544804, R2 0.5245952606201172\n",
      "epoch 7962, loss 0.01234777458012104, R2 0.4867516756057739\n",
      "Eval loss 0.012342086993157864, R2 0.5245958566665649\n",
      "epoch 7963, loss 0.012347751297056675, R2 0.48675280809402466\n",
      "Eval loss 0.012342061847448349, R2 0.5245965123176575\n",
      "epoch 7964, loss 0.012347730807960033, R2 0.4867536425590515\n",
      "Eval loss 0.012342037633061409, R2 0.5245981216430664\n",
      "epoch 7965, loss 0.012347709387540817, R2 0.4867543578147888\n",
      "Eval loss 0.012342014349997044, R2 0.5245985984802246\n",
      "epoch 7966, loss 0.012347688898444176, R2 0.4867554306983948\n",
      "Eval loss 0.012341989204287529, R2 0.5245996713638306\n",
      "epoch 7967, loss 0.012347666546702385, R2 0.4867561459541321\n",
      "Eval loss 0.012341965921223164, R2 0.5246005058288574\n",
      "epoch 7968, loss 0.012347646988928318, R2 0.48675745725631714\n",
      "Eval loss 0.012341940775513649, R2 0.5246014595031738\n",
      "epoch 7969, loss 0.012347625568509102, R2 0.48675787448883057\n",
      "Eval loss 0.012341917492449284, R2 0.5246020555496216\n",
      "epoch 7970, loss 0.012347603216767311, R2 0.4867593050003052\n",
      "Eval loss 0.012341892346739769, R2 0.5246034860610962\n",
      "epoch 7971, loss 0.01234758272767067, R2 0.48675912618637085\n",
      "Eval loss 0.012341868132352829, R2 0.5246045589447021\n",
      "epoch 7972, loss 0.012347562238574028, R2 0.486760675907135\n",
      "Eval loss 0.012341843917965889, R2 0.5246050357818604\n",
      "epoch 7973, loss 0.012347540818154812, R2 0.486761212348938\n",
      "Eval loss 0.012341820634901524, R2 0.5246056318283081\n",
      "epoch 7974, loss 0.012347518466413021, R2 0.48676228523254395\n",
      "Eval loss 0.012341795489192009, R2 0.5246067047119141\n",
      "epoch 7975, loss 0.012347499839961529, R2 0.486763596534729\n",
      "Eval loss 0.012341772206127644, R2 0.5246082544326782\n",
      "epoch 7976, loss 0.012347477488219738, R2 0.4867636561393738\n",
      "Eval loss 0.012341747991740704, R2 0.524608850479126\n",
      "epoch 7977, loss 0.012347456999123096, R2 0.48676538467407227\n",
      "Eval loss 0.012341724708676338, R2 0.5246098041534424\n",
      "epoch 7978, loss 0.012347434647381306, R2 0.48676592111587524\n",
      "Eval loss 0.012341699562966824, R2 0.5246108770370483\n",
      "epoch 7979, loss 0.012347416020929813, R2 0.4867672324180603\n",
      "Eval loss 0.012341675348579884, R2 0.5246114730834961\n",
      "epoch 7980, loss 0.012347392737865448, R2 0.4867675304412842\n",
      "Eval loss 0.012341652996838093, R2 0.5246124267578125\n",
      "epoch 7981, loss 0.012347372248768806, R2 0.48676908016204834\n",
      "Eval loss 0.012341626919806004, R2 0.5246132612228394\n",
      "epoch 7982, loss 0.012347351759672165, R2 0.48676973581314087\n",
      "Eval loss 0.012341604568064213, R2 0.5246145725250244\n",
      "epoch 7983, loss 0.012347330339252949, R2 0.4867701530456543\n",
      "Eval loss 0.012341579422354698, R2 0.5246150493621826\n",
      "epoch 7984, loss 0.012347308918833733, R2 0.4867706894874573\n",
      "Eval loss 0.012341555207967758, R2 0.524616003036499\n",
      "epoch 7985, loss 0.012347289361059666, R2 0.4867718815803528\n",
      "Eval loss 0.012341530993580818, R2 0.5246175527572632\n",
      "epoch 7986, loss 0.012347267009317875, R2 0.4867727756500244\n",
      "Eval loss 0.012341506779193878, R2 0.5246181488037109\n",
      "epoch 7987, loss 0.012347247451543808, R2 0.4867737293243408\n",
      "Eval loss 0.012341483496129513, R2 0.5246193408966064\n",
      "epoch 7988, loss 0.012347226962447166, R2 0.4867744445800781\n",
      "Eval loss 0.012341459281742573, R2 0.524619996547699\n",
      "epoch 7989, loss 0.01234720554202795, R2 0.4867755174636841\n",
      "Eval loss 0.012341435998678207, R2 0.524620771408081\n",
      "epoch 7990, loss 0.012347185052931309, R2 0.4867761731147766\n",
      "Eval loss 0.012341412715613842, R2 0.5246214866638184\n",
      "epoch 7991, loss 0.012347163632512093, R2 0.48677724599838257\n",
      "Eval loss 0.012341386638581753, R2 0.5246227979660034\n",
      "epoch 7992, loss 0.012347142212092876, R2 0.4867779612541199\n",
      "Eval loss 0.012341365218162537, R2 0.5246236324310303\n",
      "epoch 7993, loss 0.01234712079167366, R2 0.4867788553237915\n",
      "Eval loss 0.012341341935098171, R2 0.524624228477478\n",
      "epoch 7994, loss 0.012347100302577019, R2 0.48677968978881836\n",
      "Eval loss 0.012341316789388657, R2 0.5246254801750183\n",
      "epoch 7995, loss 0.012347079813480377, R2 0.4867803454399109\n",
      "Eval loss 0.012341292575001717, R2 0.5246262550354004\n",
      "epoch 7996, loss 0.01234706025570631, R2 0.48678135871887207\n",
      "Eval loss 0.012341269291937351, R2 0.5246273279190063\n",
      "epoch 7997, loss 0.01234703790396452, R2 0.4867827892303467\n",
      "Eval loss 0.012341246008872986, R2 0.5246279239654541\n",
      "epoch 7998, loss 0.012347017414867878, R2 0.48678380250930786\n",
      "Eval loss 0.012341220863163471, R2 0.5246293544769287\n",
      "epoch 7999, loss 0.012346997857093811, R2 0.48678410053253174\n",
      "Eval loss 0.012341197580099106, R2 0.524630069732666\n",
      "epoch 8000, loss 0.012346976436674595, R2 0.48678499460220337\n",
      "Eval loss 0.012341175228357315, R2 0.5246306657791138\n",
      "epoch 8001, loss 0.012346955947577953, R2 0.4867857098579407\n",
      "Eval loss 0.0123411500826478, R2 0.5246317386627197\n",
      "epoch 8002, loss 0.012346935458481312, R2 0.48678654432296753\n",
      "Eval loss 0.01234112773090601, R2 0.5246328115463257\n",
      "epoch 8003, loss 0.012346914038062096, R2 0.48678791522979736\n",
      "Eval loss 0.01234110165387392, R2 0.5246337652206421\n",
      "epoch 8004, loss 0.012346894480288029, R2 0.486788272857666\n",
      "Eval loss 0.01234107930213213, R2 0.524634599685669\n",
      "epoch 8005, loss 0.012346872128546238, R2 0.48678916692733765\n",
      "Eval loss 0.01234105508774519, R2 0.5246356725692749\n",
      "epoch 8006, loss 0.012346851639449596, R2 0.4867900609970093\n",
      "Eval loss 0.012341032736003399, R2 0.5246363878250122\n",
      "epoch 8007, loss 0.012346831150352955, R2 0.48679155111312866\n",
      "Eval loss 0.012341009452939034, R2 0.5246372222900391\n",
      "epoch 8008, loss 0.012346811592578888, R2 0.4867923855781555\n",
      "Eval loss 0.012340984307229519, R2 0.5246379971504211\n",
      "epoch 8009, loss 0.012346790172159672, R2 0.48679256439208984\n",
      "Eval loss 0.012340960092842579, R2 0.5246392488479614\n",
      "epoch 8010, loss 0.01234676968306303, R2 0.4867936372756958\n",
      "Eval loss 0.012340936809778214, R2 0.5246398448944092\n",
      "epoch 8011, loss 0.012346749193966389, R2 0.48679476976394653\n",
      "Eval loss 0.012340913526713848, R2 0.5246407389640808\n",
      "epoch 8012, loss 0.012346726842224598, R2 0.48679524660110474\n",
      "Eval loss 0.012340889312326908, R2 0.5246423482894897\n",
      "epoch 8013, loss 0.012346708215773106, R2 0.4867960214614868\n",
      "Eval loss 0.012340866960585117, R2 0.5246427059173584\n",
      "epoch 8014, loss 0.012346687726676464, R2 0.486797034740448\n",
      "Eval loss 0.012340841814875603, R2 0.5246437788009644\n",
      "epoch 8015, loss 0.012346666306257248, R2 0.48679792881011963\n",
      "Eval loss 0.012340819463133812, R2 0.5246444940567017\n",
      "epoch 8016, loss 0.012346647679805756, R2 0.4867985248565674\n",
      "Eval loss 0.012340795248746872, R2 0.5246453285217285\n",
      "epoch 8017, loss 0.012346625328063965, R2 0.48679929971694946\n",
      "Eval loss 0.012340772897005081, R2 0.5246461629867554\n",
      "epoch 8018, loss 0.012346604838967323, R2 0.48680078983306885\n",
      "Eval loss 0.012340748682618141, R2 0.5246473550796509\n",
      "epoch 8019, loss 0.012346584349870682, R2 0.4868011474609375\n",
      "Eval loss 0.012340723536908627, R2 0.5246480703353882\n",
      "epoch 8020, loss 0.01234656386077404, R2 0.4868021607398987\n",
      "Eval loss 0.012340701185166836, R2 0.5246492028236389\n",
      "epoch 8021, loss 0.012346543371677399, R2 0.486802875995636\n",
      "Eval loss 0.01234067790210247, R2 0.524649977684021\n",
      "epoch 8022, loss 0.012346522882580757, R2 0.48680371046066284\n",
      "Eval loss 0.012340654619038105, R2 0.5246509909629822\n",
      "epoch 8023, loss 0.01234650332480669, R2 0.4868045449256897\n",
      "Eval loss 0.01234063133597374, R2 0.5246518850326538\n",
      "epoch 8024, loss 0.012346480041742325, R2 0.48680543899536133\n",
      "Eval loss 0.012340608052909374, R2 0.5246524810791016\n",
      "epoch 8025, loss 0.012346462346613407, R2 0.48680686950683594\n",
      "Eval loss 0.012340583838522434, R2 0.5246535539627075\n",
      "epoch 8026, loss 0.012346441857516766, R2 0.4868072271347046\n",
      "Eval loss 0.012340560555458069, R2 0.5246546268463135\n",
      "epoch 8027, loss 0.01234642043709755, R2 0.4868079423904419\n",
      "Eval loss 0.012340537272393703, R2 0.5246556997299194\n",
      "epoch 8028, loss 0.012346400879323483, R2 0.48680925369262695\n",
      "Eval loss 0.012340513989329338, R2 0.5246562957763672\n",
      "epoch 8029, loss 0.012346380390226841, R2 0.4868100881576538\n",
      "Eval loss 0.012340490706264973, R2 0.5246576070785522\n",
      "epoch 8030, loss 0.012346358969807625, R2 0.4868103265762329\n",
      "Eval loss 0.012340467423200607, R2 0.524658203125\n",
      "epoch 8031, loss 0.012346339412033558, R2 0.4868113398551941\n",
      "Eval loss 0.012340444140136242, R2 0.5246590375900269\n",
      "epoch 8032, loss 0.012346317991614342, R2 0.4868123531341553\n",
      "Eval loss 0.012340420857071877, R2 0.5246601104736328\n",
      "epoch 8033, loss 0.01234629936516285, R2 0.4868130087852478\n",
      "Eval loss 0.012340397574007511, R2 0.5246607065200806\n",
      "epoch 8034, loss 0.012346277944743633, R2 0.486814022064209\n",
      "Eval loss 0.012340374290943146, R2 0.5246617794036865\n",
      "epoch 8035, loss 0.012346259318292141, R2 0.48681533336639404\n",
      "Eval loss 0.012340350076556206, R2 0.5246631503105164\n",
      "epoch 8036, loss 0.012346237897872925, R2 0.48681551218032837\n",
      "Eval loss 0.012340327724814415, R2 0.5246634483337402\n",
      "epoch 8037, loss 0.012346216477453709, R2 0.4868164658546448\n",
      "Eval loss 0.01234030444175005, R2 0.5246645212173462\n",
      "epoch 8038, loss 0.012346197851002216, R2 0.4868173599243164\n",
      "Eval loss 0.012340279296040535, R2 0.524665117263794\n",
      "epoch 8039, loss 0.012346176430583, R2 0.4868181347846985\n",
      "Eval loss 0.012340258806943893, R2 0.5246659517288208\n",
      "epoch 8040, loss 0.012346157804131508, R2 0.4868188500404358\n",
      "Eval loss 0.012340234592556953, R2 0.5246671438217163\n",
      "epoch 8041, loss 0.012346137315034866, R2 0.4868202209472656\n",
      "Eval loss 0.012340212240815163, R2 0.5246679186820984\n",
      "epoch 8042, loss 0.012346116825938225, R2 0.4868210554122925\n",
      "Eval loss 0.012340188957750797, R2 0.5246689319610596\n",
      "epoch 8043, loss 0.012346096336841583, R2 0.48682206869125366\n",
      "Eval loss 0.012340165674686432, R2 0.5246697664260864\n",
      "epoch 8044, loss 0.012346074916422367, R2 0.48682212829589844\n",
      "Eval loss 0.012340143322944641, R2 0.5246707201004028\n",
      "epoch 8045, loss 0.012346056289970875, R2 0.48682326078414917\n",
      "Eval loss 0.012340119108557701, R2 0.5246713161468506\n",
      "epoch 8046, loss 0.012346034869551659, R2 0.4868239760398865\n",
      "Eval loss 0.012340095825493336, R2 0.5246725082397461\n",
      "epoch 8047, loss 0.012346015311777592, R2 0.4868251085281372\n",
      "Eval loss 0.01234007254242897, R2 0.5246729850769043\n",
      "epoch 8048, loss 0.01234599482268095, R2 0.4868258237838745\n",
      "Eval loss 0.01234004832804203, R2 0.5246742963790894\n",
      "epoch 8049, loss 0.012345974333584309, R2 0.4868265390396118\n",
      "Eval loss 0.012340025044977665, R2 0.5246749520301819\n",
      "epoch 8050, loss 0.012345954775810242, R2 0.4868279695510864\n",
      "Eval loss 0.012340002693235874, R2 0.5246762037277222\n",
      "epoch 8051, loss 0.012345935218036175, R2 0.48682814836502075\n",
      "Eval loss 0.012339980341494083, R2 0.5246769785881042\n",
      "epoch 8052, loss 0.012345914728939533, R2 0.4868289828300476\n",
      "Eval loss 0.012339957989752293, R2 0.5246775150299072\n",
      "epoch 8053, loss 0.012345893308520317, R2 0.48682987689971924\n",
      "Eval loss 0.012339933775365353, R2 0.5246784687042236\n",
      "epoch 8054, loss 0.012345874682068825, R2 0.4868306517601013\n",
      "Eval loss 0.012339911423623562, R2 0.5246796607971191\n",
      "epoch 8055, loss 0.012345854192972183, R2 0.48683202266693115\n",
      "Eval loss 0.012339887209236622, R2 0.5246802568435669\n",
      "epoch 8056, loss 0.012345834635198116, R2 0.48683232069015503\n",
      "Eval loss 0.012339864857494831, R2 0.5246814489364624\n",
      "epoch 8057, loss 0.0123458132147789, R2 0.486833393573761\n",
      "Eval loss 0.01233984250575304, R2 0.5246824026107788\n",
      "epoch 8058, loss 0.012345793657004833, R2 0.48683416843414307\n",
      "Eval loss 0.01233982015401125, R2 0.524682879447937\n",
      "epoch 8059, loss 0.012345774099230766, R2 0.4868350028991699\n",
      "Eval loss 0.01233979593962431, R2 0.5246840715408325\n",
      "epoch 8060, loss 0.0123457545414567, R2 0.48683565855026245\n",
      "Eval loss 0.012339772656559944, R2 0.5246848464012146\n",
      "epoch 8061, loss 0.012345734052360058, R2 0.4868369698524475\n",
      "Eval loss 0.012339751236140728, R2 0.5246856212615967\n",
      "epoch 8062, loss 0.012345713563263416, R2 0.48683732748031616\n",
      "Eval loss 0.012339727021753788, R2 0.5246869921684265\n",
      "epoch 8063, loss 0.01234569400548935, R2 0.48683834075927734\n",
      "Eval loss 0.012339704670011997, R2 0.5246877670288086\n",
      "epoch 8064, loss 0.012345674447715282, R2 0.4868391156196594\n",
      "Eval loss 0.012339681386947632, R2 0.5246884822845459\n",
      "epoch 8065, loss 0.012345653958618641, R2 0.48684000968933105\n",
      "Eval loss 0.012339659035205841, R2 0.5246893167495728\n",
      "epoch 8066, loss 0.012345634400844574, R2 0.4868406057357788\n",
      "Eval loss 0.012339635752141476, R2 0.5246903896331787\n",
      "epoch 8067, loss 0.012345614843070507, R2 0.48684144020080566\n",
      "Eval loss 0.012339613400399685, R2 0.5246908664703369\n",
      "epoch 8068, loss 0.01234559528529644, R2 0.48684239387512207\n",
      "Eval loss 0.01233959011733532, R2 0.5246918201446533\n",
      "epoch 8069, loss 0.012345574796199799, R2 0.4868432879447937\n",
      "Eval loss 0.012339567765593529, R2 0.5246928334236145\n",
      "epoch 8070, loss 0.012345555238425732, R2 0.4868440628051758\n",
      "Eval loss 0.012339544482529163, R2 0.524693489074707\n",
      "epoch 8071, loss 0.012345535680651665, R2 0.4868452548980713\n",
      "Eval loss 0.012339521199464798, R2 0.5246949195861816\n",
      "epoch 8072, loss 0.012345514260232449, R2 0.48684579133987427\n",
      "Eval loss 0.012339499779045582, R2 0.524695873260498\n",
      "epoch 8073, loss 0.012345495633780956, R2 0.4868467450141907\n",
      "Eval loss 0.012339475564658642, R2 0.524696409702301\n",
      "epoch 8074, loss 0.01234547607600689, R2 0.4868474006652832\n",
      "Eval loss 0.012339454144239426, R2 0.5246972441673279\n",
      "epoch 8075, loss 0.012345455586910248, R2 0.48684871196746826\n",
      "Eval loss 0.01233943086117506, R2 0.52469801902771\n",
      "epoch 8076, loss 0.012345436029136181, R2 0.4868495464324951\n",
      "Eval loss 0.01233940850943327, R2 0.5246987342834473\n",
      "epoch 8077, loss 0.01234541554003954, R2 0.48684990406036377\n",
      "Eval loss 0.012339385226368904, R2 0.5247000455856323\n",
      "epoch 8078, loss 0.012345395982265472, R2 0.48685121536254883\n",
      "Eval loss 0.012339361011981964, R2 0.5247005224227905\n",
      "epoch 8079, loss 0.01234537735581398, R2 0.4868515133857727\n",
      "Eval loss 0.012339339591562748, R2 0.5247019529342651\n",
      "epoch 8080, loss 0.012345355935394764, R2 0.4868525266647339\n",
      "Eval loss 0.012339317239820957, R2 0.5247021913528442\n",
      "epoch 8081, loss 0.012345338240265846, R2 0.4868529438972473\n",
      "Eval loss 0.012339294888079166, R2 0.5247031450271606\n",
      "epoch 8082, loss 0.01234531681984663, R2 0.48685401678085327\n",
      "Eval loss 0.012339271605014801, R2 0.5247042775154114\n",
      "epoch 8083, loss 0.012345298193395138, R2 0.48685508966445923\n",
      "Eval loss 0.01233924925327301, R2 0.5247051119804382\n",
      "epoch 8084, loss 0.012345276772975922, R2 0.48685580492019653\n",
      "Eval loss 0.01233922690153122, R2 0.5247058868408203\n",
      "epoch 8085, loss 0.012345257215201855, R2 0.48685646057128906\n",
      "Eval loss 0.012339203618466854, R2 0.524707019329071\n",
      "epoch 8086, loss 0.012345237657427788, R2 0.4868571162223816\n",
      "Eval loss 0.012339182198047638, R2 0.5247074365615845\n",
      "epoch 8087, loss 0.012345219030976295, R2 0.48685789108276367\n",
      "Eval loss 0.012339157983660698, R2 0.52470862865448\n",
      "epoch 8088, loss 0.012345198541879654, R2 0.48685890436172485\n",
      "Eval loss 0.012339136563241482, R2 0.5247095823287964\n",
      "epoch 8089, loss 0.012345180846750736, R2 0.48685967922210693\n",
      "Eval loss 0.012339113280177116, R2 0.5247100591659546\n",
      "epoch 8090, loss 0.01234515942633152, R2 0.486860990524292\n",
      "Eval loss 0.012339090928435326, R2 0.5247112512588501\n",
      "epoch 8091, loss 0.012345140799880028, R2 0.4868611693382263\n",
      "Eval loss 0.012339068576693535, R2 0.524712085723877\n",
      "epoch 8092, loss 0.012345120310783386, R2 0.4868620038032532\n",
      "Eval loss 0.012339046224951744, R2 0.5247126817703247\n",
      "epoch 8093, loss 0.01234510075300932, R2 0.4868636131286621\n",
      "Eval loss 0.012339023873209953, R2 0.5247135162353516\n",
      "epoch 8094, loss 0.012345081195235252, R2 0.4868640899658203\n",
      "Eval loss 0.012339001521468163, R2 0.5247145295143127\n",
      "epoch 8095, loss 0.01234506256878376, R2 0.4868643879890442\n",
      "Eval loss 0.012338980101048946, R2 0.524715781211853\n",
      "epoch 8096, loss 0.012345043011009693, R2 0.48686522245407104\n",
      "Eval loss 0.012338956817984581, R2 0.5247164964675903\n",
      "epoch 8097, loss 0.012345023453235626, R2 0.4868658781051636\n",
      "Eval loss 0.012338933534920216, R2 0.5247164368629456\n",
      "epoch 8098, loss 0.01234500203281641, R2 0.48686689138412476\n",
      "Eval loss 0.01233891025185585, R2 0.5247182846069336\n",
      "epoch 8099, loss 0.012344982475042343, R2 0.4868677258491516\n",
      "Eval loss 0.012338889762759209, R2 0.5247188210487366\n",
      "epoch 8100, loss 0.01234496384859085, R2 0.486868679523468\n",
      "Eval loss 0.012338867411017418, R2 0.5247195959091187\n",
      "epoch 8101, loss 0.012344944290816784, R2 0.48686933517456055\n",
      "Eval loss 0.012338844127953053, R2 0.5247205495834351\n",
      "epoch 8102, loss 0.012344924733042717, R2 0.48687028884887695\n",
      "Eval loss 0.012338822707533836, R2 0.5247212648391724\n",
      "epoch 8103, loss 0.012344906106591225, R2 0.4868714213371277\n",
      "Eval loss 0.012338798493146896, R2 0.5247224569320679\n",
      "epoch 8104, loss 0.012344885617494583, R2 0.4868718981742859\n",
      "Eval loss 0.012338778004050255, R2 0.5247230529785156\n",
      "epoch 8105, loss 0.01234486699104309, R2 0.4868725538253784\n",
      "Eval loss 0.01233875472098589, R2 0.5247238874435425\n",
      "epoch 8106, loss 0.01234484650194645, R2 0.4868735074996948\n",
      "Eval loss 0.012338731437921524, R2 0.524725079536438\n",
      "epoch 8107, loss 0.012344827875494957, R2 0.48687416315078735\n",
      "Eval loss 0.012338710017502308, R2 0.524726152420044\n",
      "epoch 8108, loss 0.012344807386398315, R2 0.48687517642974854\n",
      "Eval loss 0.012338688597083092, R2 0.5247267484664917\n",
      "epoch 8109, loss 0.012344788759946823, R2 0.4868757724761963\n",
      "Eval loss 0.012338664382696152, R2 0.5247273445129395\n",
      "epoch 8110, loss 0.012344769202172756, R2 0.4868762493133545\n",
      "Eval loss 0.012338642962276936, R2 0.5247284770011902\n",
      "epoch 8111, loss 0.012344750575721264, R2 0.48687803745269775\n",
      "Eval loss 0.012338620610535145, R2 0.5247290730476379\n",
      "epoch 8112, loss 0.012344730086624622, R2 0.4868782162666321\n",
      "Eval loss 0.012338598258793354, R2 0.5247297883033752\n",
      "epoch 8113, loss 0.01234471146017313, R2 0.48687881231307983\n",
      "Eval loss 0.012338574975728989, R2 0.5247308015823364\n",
      "epoch 8114, loss 0.012344690971076488, R2 0.486879825592041\n",
      "Eval loss 0.012338553555309772, R2 0.5247319340705872\n",
      "epoch 8115, loss 0.01234467327594757, R2 0.48688071966171265\n",
      "Eval loss 0.012338531203567982, R2 0.524732768535614\n",
      "epoch 8116, loss 0.012344651855528355, R2 0.48688143491744995\n",
      "Eval loss 0.012338509783148766, R2 0.5247334837913513\n",
      "epoch 8117, loss 0.012344635091722012, R2 0.48688215017318726\n",
      "Eval loss 0.012338487431406975, R2 0.5247341394424438\n",
      "epoch 8118, loss 0.012344613671302795, R2 0.4868836998939514\n",
      "Eval loss 0.012338466010987759, R2 0.5247355699539185\n",
      "epoch 8119, loss 0.012344595044851303, R2 0.48688429594039917\n",
      "Eval loss 0.012338442727923393, R2 0.5247360467910767\n",
      "epoch 8120, loss 0.012344575487077236, R2 0.486885130405426\n",
      "Eval loss 0.012338420376181602, R2 0.5247370600700378\n",
      "epoch 8121, loss 0.01234455592930317, R2 0.4868854880332947\n",
      "Eval loss 0.012338398955762386, R2 0.5247378349304199\n",
      "epoch 8122, loss 0.012344537302851677, R2 0.4868863821029663\n",
      "Eval loss 0.012338376604020596, R2 0.5247385501861572\n",
      "epoch 8123, loss 0.01234451774507761, R2 0.48688703775405884\n",
      "Eval loss 0.01233835518360138, R2 0.5247398614883423\n",
      "epoch 8124, loss 0.012344500049948692, R2 0.48688799142837524\n",
      "Eval loss 0.012338331900537014, R2 0.5247401595115662\n",
      "epoch 8125, loss 0.012344480492174625, R2 0.486888587474823\n",
      "Eval loss 0.012338310480117798, R2 0.5247417092323303\n",
      "epoch 8126, loss 0.01234445907175541, R2 0.48688948154449463\n",
      "Eval loss 0.012338288128376007, R2 0.5247420072555542\n",
      "epoch 8127, loss 0.012344441376626492, R2 0.4868907332420349\n",
      "Eval loss 0.012338264845311642, R2 0.5247429609298706\n",
      "epoch 8128, loss 0.012344422750175, R2 0.4868910312652588\n",
      "Eval loss 0.012338243424892426, R2 0.5247438549995422\n",
      "epoch 8129, loss 0.012344402261078358, R2 0.4868919849395752\n",
      "Eval loss 0.01233822200447321, R2 0.5247447490692139\n",
      "epoch 8130, loss 0.012344383634626865, R2 0.4868931174278259\n",
      "Eval loss 0.012338201515376568, R2 0.5247454643249512\n",
      "epoch 8131, loss 0.012344365008175373, R2 0.4868934154510498\n",
      "Eval loss 0.012338177300989628, R2 0.524746298789978\n",
      "epoch 8132, loss 0.01234434638172388, R2 0.4868943691253662\n",
      "Eval loss 0.012338155880570412, R2 0.5247471332550049\n",
      "epoch 8133, loss 0.012344326823949814, R2 0.48689496517181396\n",
      "Eval loss 0.012338133528828621, R2 0.5247478485107422\n",
      "epoch 8134, loss 0.012344307266175747, R2 0.4868956208229065\n",
      "Eval loss 0.012338112108409405, R2 0.5247485041618347\n",
      "epoch 8135, loss 0.01234428770840168, R2 0.48689645528793335\n",
      "Eval loss 0.01233808882534504, R2 0.5247495174407959\n",
      "epoch 8136, loss 0.012344269081950188, R2 0.4868975281715393\n",
      "Eval loss 0.012338069267570972, R2 0.5247505903244019\n",
      "epoch 8137, loss 0.01234424952417612, R2 0.48689883947372437\n",
      "Eval loss 0.012338045053184032, R2 0.5247516632080078\n",
      "epoch 8138, loss 0.012344230897724628, R2 0.48689883947372437\n",
      "Eval loss 0.012338023632764816, R2 0.5247523784637451\n",
      "epoch 8139, loss 0.012344211339950562, R2 0.48689979314804077\n",
      "Eval loss 0.012338001281023026, R2 0.5247529149055481\n",
      "epoch 8140, loss 0.012344193644821644, R2 0.4869005084037781\n",
      "Eval loss 0.01233797986060381, R2 0.5247536897659302\n",
      "epoch 8141, loss 0.012344173155725002, R2 0.4869017004966736\n",
      "Eval loss 0.012337958440184593, R2 0.5247550010681152\n",
      "epoch 8142, loss 0.012344155460596085, R2 0.48690229654312134\n",
      "Eval loss 0.012337936088442802, R2 0.5247554183006287\n",
      "epoch 8143, loss 0.012344135902822018, R2 0.4869035482406616\n",
      "Eval loss 0.012337914668023586, R2 0.524756669998169\n",
      "epoch 8144, loss 0.012344117276370525, R2 0.48690420389175415\n",
      "Eval loss 0.012337892316281796, R2 0.5247571468353271\n",
      "epoch 8145, loss 0.012344097718596458, R2 0.48690468072891235\n",
      "Eval loss 0.012337869964540005, R2 0.5247581005096436\n",
      "epoch 8146, loss 0.012344078160822392, R2 0.4869053363800049\n",
      "Eval loss 0.012337848544120789, R2 0.52475905418396\n",
      "epoch 8147, loss 0.012344058603048325, R2 0.48690658807754517\n",
      "Eval loss 0.012337828055024147, R2 0.5247598886489868\n",
      "epoch 8148, loss 0.012344040907919407, R2 0.4869075417518616\n",
      "Eval loss 0.012337804771959782, R2 0.5247604846954346\n",
      "epoch 8149, loss 0.01234402321279049, R2 0.4869077801704407\n",
      "Eval loss 0.012337782420217991, R2 0.524761438369751\n",
      "epoch 8150, loss 0.012344002723693848, R2 0.48690927028656006\n",
      "Eval loss 0.012337760999798775, R2 0.5247624516487122\n",
      "epoch 8151, loss 0.01234398502856493, R2 0.4869092106819153\n",
      "Eval loss 0.012337738648056984, R2 0.524763286113739\n",
      "epoch 8152, loss 0.012343965470790863, R2 0.4869101643562317\n",
      "Eval loss 0.012337718158960342, R2 0.5247637033462524\n",
      "epoch 8153, loss 0.012343945913016796, R2 0.48691099882125854\n",
      "Eval loss 0.012337696738541126, R2 0.524764895439148\n",
      "epoch 8154, loss 0.012343927286565304, R2 0.4869115948677063\n",
      "Eval loss 0.012337674386799335, R2 0.5247654914855957\n",
      "epoch 8155, loss 0.012343908660113811, R2 0.48691272735595703\n",
      "Eval loss 0.01233765296638012, R2 0.5247665643692017\n",
      "epoch 8156, loss 0.012343889102339745, R2 0.48691314458847046\n",
      "Eval loss 0.012337631545960903, R2 0.5247675776481628\n",
      "epoch 8157, loss 0.012343871407210827, R2 0.48691457509994507\n",
      "Eval loss 0.012337609194219112, R2 0.5247681140899658\n",
      "epoch 8158, loss 0.012343852780759335, R2 0.4869146943092346\n",
      "Eval loss 0.012337587773799896, R2 0.5247693061828613\n",
      "epoch 8159, loss 0.012343834154307842, R2 0.4869154691696167\n",
      "Eval loss 0.01233756635338068, R2 0.5247699022293091\n",
      "epoch 8160, loss 0.012343814596533775, R2 0.4869164228439331\n",
      "Eval loss 0.012337544932961464, R2 0.5247704982757568\n",
      "epoch 8161, loss 0.012343795970082283, R2 0.4869173765182495\n",
      "Eval loss 0.012337523512542248, R2 0.5247714519500732\n",
      "epoch 8162, loss 0.012343776412308216, R2 0.4869178533554077\n",
      "Eval loss 0.012337501160800457, R2 0.5247727632522583\n",
      "epoch 8163, loss 0.012343757785856724, R2 0.4869186282157898\n",
      "Eval loss 0.01233747974038124, R2 0.5247735977172852\n",
      "epoch 8164, loss 0.012343738228082657, R2 0.48691946268081665\n",
      "Eval loss 0.012337456457316875, R2 0.524773895740509\n",
      "epoch 8165, loss 0.01234372053295374, R2 0.48692017793655396\n",
      "Eval loss 0.012337436899542809, R2 0.5247749090194702\n",
      "epoch 8166, loss 0.012343700043857098, R2 0.4869210124015808\n",
      "Eval loss 0.012337414547801018, R2 0.5247757434844971\n",
      "epoch 8167, loss 0.012343683280050755, R2 0.48692142963409424\n",
      "Eval loss 0.012337392196059227, R2 0.5247766971588135\n",
      "epoch 8168, loss 0.012343663722276688, R2 0.4869225025177002\n",
      "Eval loss 0.01233737077564001, R2 0.5247772932052612\n",
      "epoch 8169, loss 0.012343645095825195, R2 0.48692333698272705\n",
      "Eval loss 0.012337349355220795, R2 0.5247781872749329\n",
      "epoch 8170, loss 0.012343626469373703, R2 0.48692429065704346\n",
      "Eval loss 0.012337327934801579, R2 0.5247794389724731\n",
      "epoch 8171, loss 0.012343608774244785, R2 0.4869244694709778\n",
      "Eval loss 0.012337306514382362, R2 0.5247797966003418\n",
      "epoch 8172, loss 0.012343590147793293, R2 0.48692625761032104\n",
      "Eval loss 0.012337284162640572, R2 0.5247806906700134\n",
      "epoch 8173, loss 0.0123435715213418, R2 0.4869265556335449\n",
      "Eval loss 0.012337262742221355, R2 0.5247814655303955\n",
      "epoch 8174, loss 0.012343552894890308, R2 0.4869271516799927\n",
      "Eval loss 0.012337242253124714, R2 0.5247827172279358\n",
      "epoch 8175, loss 0.012343533337116241, R2 0.4869281053543091\n",
      "Eval loss 0.012337221764028072, R2 0.5247828960418701\n",
      "epoch 8176, loss 0.012343515641987324, R2 0.48692888021469116\n",
      "Eval loss 0.012337199412286282, R2 0.5247836709022522\n",
      "epoch 8177, loss 0.012343496084213257, R2 0.48692965507507324\n",
      "Eval loss 0.01233717892318964, R2 0.5247848629951477\n",
      "epoch 8178, loss 0.012343477457761765, R2 0.48693031072616577\n",
      "Eval loss 0.012337158434092999, R2 0.5247853994369507\n",
      "epoch 8179, loss 0.012343457899987698, R2 0.48693108558654785\n",
      "Eval loss 0.012337135151028633, R2 0.5247865915298462\n",
      "epoch 8180, loss 0.012343441136181355, R2 0.4869319796562195\n",
      "Eval loss 0.012337114661931992, R2 0.5247870683670044\n",
      "epoch 8181, loss 0.012343420647084713, R2 0.4869324564933777\n",
      "Eval loss 0.0123370923101902, R2 0.524788498878479\n",
      "epoch 8182, loss 0.012343402951955795, R2 0.48693352937698364\n",
      "Eval loss 0.012337072752416134, R2 0.5247892141342163\n",
      "epoch 8183, loss 0.012343384325504303, R2 0.4869343042373657\n",
      "Eval loss 0.012337049469351768, R2 0.5247901678085327\n",
      "epoch 8184, loss 0.012343366630375385, R2 0.4869353771209717\n",
      "Eval loss 0.012337028048932552, R2 0.5247907638549805\n",
      "epoch 8185, loss 0.012343348935246468, R2 0.48693549633026123\n",
      "Eval loss 0.012337006628513336, R2 0.5247916579246521\n",
      "epoch 8186, loss 0.0123433293774724, R2 0.48693645000457764\n",
      "Eval loss 0.01233698520809412, R2 0.5247920751571655\n",
      "epoch 8187, loss 0.012343310751020908, R2 0.48693740367889404\n",
      "Eval loss 0.012336964718997478, R2 0.5247928500175476\n",
      "epoch 8188, loss 0.012343291193246841, R2 0.4869379997253418\n",
      "Eval loss 0.012336942367255688, R2 0.5247939825057983\n",
      "epoch 8189, loss 0.012343273498117924, R2 0.4869384765625\n",
      "Eval loss 0.01233692280948162, R2 0.5247946977615356\n",
      "epoch 8190, loss 0.012343254871666431, R2 0.48693954944610596\n",
      "Eval loss 0.01233690045773983, R2 0.5247952938079834\n",
      "epoch 8191, loss 0.012343236245214939, R2 0.48694032430648804\n",
      "Eval loss 0.012336879037320614, R2 0.5247964262962341\n",
      "epoch 8192, loss 0.012343219481408596, R2 0.4869416356086731\n",
      "Eval loss 0.012336857616901398, R2 0.5247970819473267\n",
      "epoch 8193, loss 0.012343199923634529, R2 0.4869421720504761\n",
      "Eval loss 0.012336837127804756, R2 0.5247979164123535\n",
      "epoch 8194, loss 0.012343181297183037, R2 0.48694276809692383\n",
      "Eval loss 0.012336816638708115, R2 0.5247989892959595\n",
      "epoch 8195, loss 0.012343162670731544, R2 0.4869433641433716\n",
      "Eval loss 0.012336795218288898, R2 0.5247997045516968\n",
      "epoch 8196, loss 0.012343144044280052, R2 0.48694461584091187\n",
      "Eval loss 0.012336772866547108, R2 0.5248006582260132\n",
      "epoch 8197, loss 0.012343127280473709, R2 0.48694485425949097\n",
      "Eval loss 0.012336752377450466, R2 0.5248013138771057\n",
      "epoch 8198, loss 0.012343106791377068, R2 0.48694586753845215\n",
      "Eval loss 0.01233673095703125, R2 0.5248020887374878\n",
      "epoch 8199, loss 0.012343088164925575, R2 0.4869469404220581\n",
      "Eval loss 0.01233670860528946, R2 0.5248029828071594\n",
      "epoch 8200, loss 0.012343071401119232, R2 0.4869476556777954\n",
      "Eval loss 0.012336688116192818, R2 0.5248040556907654\n",
      "epoch 8201, loss 0.01234305277466774, R2 0.48694807291030884\n",
      "Eval loss 0.012336665764451027, R2 0.5248044729232788\n",
      "epoch 8202, loss 0.012343034148216248, R2 0.48694872856140137\n",
      "Eval loss 0.01233664620667696, R2 0.5248057842254639\n",
      "epoch 8203, loss 0.01234301645308733, R2 0.48694944381713867\n",
      "Eval loss 0.012336625717580318, R2 0.5248061418533325\n",
      "epoch 8204, loss 0.012342997826635838, R2 0.48695021867752075\n",
      "Eval loss 0.012336604297161102, R2 0.5248071551322937\n",
      "epoch 8205, loss 0.012342979200184345, R2 0.48695099353790283\n",
      "Eval loss 0.012336581945419312, R2 0.5248081684112549\n",
      "epoch 8206, loss 0.012342961505055428, R2 0.4869515895843506\n",
      "Eval loss 0.01233656145632267, R2 0.524808943271637\n",
      "epoch 8207, loss 0.012342942878603935, R2 0.48695266246795654\n",
      "Eval loss 0.012336541898548603, R2 0.5248091220855713\n",
      "epoch 8208, loss 0.012342925183475018, R2 0.4869532585144043\n",
      "Eval loss 0.012336519546806812, R2 0.5248105525970459\n",
      "epoch 8209, loss 0.01234290562570095, R2 0.4869540333747864\n",
      "Eval loss 0.012336498126387596, R2 0.5248112678527832\n",
      "epoch 8210, loss 0.012342887930572033, R2 0.48695528507232666\n",
      "Eval loss 0.012336477637290955, R2 0.5248117446899414\n",
      "epoch 8211, loss 0.01234286930412054, R2 0.4869554042816162\n",
      "Eval loss 0.012336457148194313, R2 0.5248126983642578\n",
      "epoch 8212, loss 0.012342850677669048, R2 0.4869563579559326\n",
      "Eval loss 0.012336435727775097, R2 0.5248132348060608\n",
      "epoch 8213, loss 0.012342833913862705, R2 0.4869570732116699\n",
      "Eval loss 0.012336415238678455, R2 0.5248144268989563\n",
      "epoch 8214, loss 0.012342815287411213, R2 0.4869577884674072\n",
      "Eval loss 0.01233639381825924, R2 0.5248152613639832\n",
      "epoch 8215, loss 0.01234279666095972, R2 0.48695874214172363\n",
      "Eval loss 0.012336373329162598, R2 0.5248161554336548\n",
      "epoch 8216, loss 0.012342778034508228, R2 0.4869590401649475\n",
      "Eval loss 0.012336350977420807, R2 0.524816632270813\n",
      "epoch 8217, loss 0.01234276033937931, R2 0.4869602918624878\n",
      "Eval loss 0.01233632955700159, R2 0.5248175859451294\n",
      "epoch 8218, loss 0.012342741712927818, R2 0.48696082830429077\n",
      "Eval loss 0.01233630906790495, R2 0.5248181819915771\n",
      "epoch 8219, loss 0.0123427240177989, R2 0.4869622588157654\n",
      "Eval loss 0.012336288578808308, R2 0.5248188972473145\n",
      "epoch 8220, loss 0.012342706322669983, R2 0.48696231842041016\n",
      "Eval loss 0.012336268089711666, R2 0.5248199701309204\n",
      "epoch 8221, loss 0.012342686764895916, R2 0.48696333169937134\n",
      "Eval loss 0.012336247600615025, R2 0.5248209238052368\n",
      "epoch 8222, loss 0.012342669069766998, R2 0.48696404695510864\n",
      "Eval loss 0.012336225248873234, R2 0.5248214602470398\n",
      "epoch 8223, loss 0.01234265137463808, R2 0.4869651198387146\n",
      "Eval loss 0.012336204759776592, R2 0.5248224139213562\n",
      "epoch 8224, loss 0.012342634610831738, R2 0.4869653582572937\n",
      "Eval loss 0.01233618427067995, R2 0.5248228907585144\n",
      "epoch 8225, loss 0.012342615984380245, R2 0.486966073513031\n",
      "Eval loss 0.01233616378158331, R2 0.5248238444328308\n",
      "epoch 8226, loss 0.012342598289251328, R2 0.4869673252105713\n",
      "Eval loss 0.012336143292486668, R2 0.524824857711792\n",
      "epoch 8227, loss 0.01234257873147726, R2 0.48696810007095337\n",
      "Eval loss 0.012336122803390026, R2 0.5248254537582397\n",
      "epoch 8228, loss 0.012342561036348343, R2 0.48696839809417725\n",
      "Eval loss 0.01233609952032566, R2 0.524826169013977\n",
      "epoch 8229, loss 0.01234254240989685, R2 0.4869692921638489\n",
      "Eval loss 0.012336080893874168, R2 0.5248270034790039\n",
      "epoch 8230, loss 0.012342525646090508, R2 0.48696982860565186\n",
      "Eval loss 0.012336058542132378, R2 0.5248278975486755\n",
      "epoch 8231, loss 0.012342505156993866, R2 0.4869706630706787\n",
      "Eval loss 0.01233603898435831, R2 0.5248285531997681\n",
      "epoch 8232, loss 0.012342489324510098, R2 0.4869718551635742\n",
      "Eval loss 0.012336017563939095, R2 0.5248293280601501\n",
      "epoch 8233, loss 0.01234246976673603, R2 0.4869721531867981\n",
      "Eval loss 0.012335997074842453, R2 0.5248304009437561\n",
      "epoch 8234, loss 0.012342453002929688, R2 0.4869726896286011\n",
      "Eval loss 0.012335976585745811, R2 0.5248309373855591\n",
      "epoch 8235, loss 0.01234243530780077, R2 0.48697376251220703\n",
      "Eval loss 0.01233595423400402, R2 0.524832010269165\n",
      "epoch 8236, loss 0.012342416681349277, R2 0.4869745373725891\n",
      "Eval loss 0.012335934676229954, R2 0.524833083152771\n",
      "epoch 8237, loss 0.01234239898622036, R2 0.48697513341903687\n",
      "Eval loss 0.012335913255810738, R2 0.524833619594574\n",
      "epoch 8238, loss 0.012342380359768867, R2 0.48697590827941895\n",
      "Eval loss 0.012335892766714096, R2 0.5248345136642456\n",
      "epoch 8239, loss 0.012342363595962524, R2 0.48697662353515625\n",
      "Eval loss 0.012335872277617455, R2 0.5248352289199829\n",
      "epoch 8240, loss 0.012342344038188457, R2 0.48697739839553833\n",
      "Eval loss 0.012335850857198238, R2 0.5248360633850098\n",
      "epoch 8241, loss 0.012342327274382114, R2 0.48697811365127563\n",
      "Eval loss 0.012335832230746746, R2 0.5248365998268127\n",
      "epoch 8242, loss 0.012342310510575771, R2 0.48697876930236816\n",
      "Eval loss 0.012335809879004955, R2 0.5248377323150635\n",
      "epoch 8243, loss 0.012342290952801704, R2 0.4869800806045532\n",
      "Eval loss 0.012335789389908314, R2 0.5248382091522217\n",
      "epoch 8244, loss 0.012342273257672787, R2 0.4869806170463562\n",
      "Eval loss 0.012335769832134247, R2 0.5248392820358276\n",
      "epoch 8245, loss 0.012342256493866444, R2 0.48698103427886963\n",
      "Eval loss 0.01233574841171503, R2 0.5248399972915649\n",
      "epoch 8246, loss 0.0123422397300601, R2 0.48698174953460693\n",
      "Eval loss 0.012335727922618389, R2 0.5248409509658813\n",
      "epoch 8247, loss 0.012342220172286034, R2 0.486982524394989\n",
      "Eval loss 0.012335706502199173, R2 0.5248415470123291\n",
      "epoch 8248, loss 0.01234220340847969, R2 0.48698359727859497\n",
      "Eval loss 0.012335686944425106, R2 0.524842381477356\n",
      "epoch 8249, loss 0.012342183850705624, R2 0.4869846701622009\n",
      "Eval loss 0.012335666455328465, R2 0.5248432755470276\n",
      "epoch 8250, loss 0.012342166155576706, R2 0.4869849681854248\n",
      "Eval loss 0.012335645966231823, R2 0.5248439311981201\n",
      "epoch 8251, loss 0.012342149391770363, R2 0.486985981464386\n",
      "Eval loss 0.012335624545812607, R2 0.5248444676399231\n",
      "epoch 8252, loss 0.01234213076531887, R2 0.48698627948760986\n",
      "Eval loss 0.01233560498803854, R2 0.5248454809188843\n",
      "epoch 8253, loss 0.012342113070189953, R2 0.48698747158050537\n",
      "Eval loss 0.012335584498941898, R2 0.5248461365699768\n",
      "epoch 8254, loss 0.01234209630638361, R2 0.486988365650177\n",
      "Eval loss 0.012335564009845257, R2 0.5248469114303589\n",
      "epoch 8255, loss 0.012342077679932117, R2 0.4869886636734009\n",
      "Eval loss 0.01233554258942604, R2 0.5248480439186096\n",
      "epoch 8256, loss 0.012342060916125774, R2 0.4869891405105591\n",
      "Eval loss 0.012335523031651974, R2 0.5248486995697021\n",
      "epoch 8257, loss 0.012342042289674282, R2 0.48699045181274414\n",
      "Eval loss 0.012335502542555332, R2 0.5248495936393738\n",
      "epoch 8258, loss 0.01234202366322279, R2 0.48699116706848145\n",
      "Eval loss 0.01233548205345869, R2 0.5248500108718872\n",
      "epoch 8259, loss 0.012342007830739021, R2 0.4869917035102844\n",
      "Eval loss 0.012335460633039474, R2 0.5248507857322693\n",
      "epoch 8260, loss 0.012341989204287529, R2 0.48699212074279785\n",
      "Eval loss 0.012335441075265408, R2 0.5248517990112305\n",
      "epoch 8261, loss 0.012341971509158611, R2 0.48699355125427246\n",
      "Eval loss 0.012335419654846191, R2 0.5248528718948364\n",
      "epoch 8262, loss 0.012341954745352268, R2 0.48699355125427246\n",
      "Eval loss 0.01233539916574955, R2 0.5248534083366394\n",
      "epoch 8263, loss 0.012341936118900776, R2 0.48699432611465454\n",
      "Eval loss 0.012335379607975483, R2 0.5248541831970215\n",
      "epoch 8264, loss 0.012341919355094433, R2 0.48699522018432617\n",
      "Eval loss 0.012335359118878841, R2 0.5248552560806274\n",
      "epoch 8265, loss 0.01234190072864294, R2 0.4869958162307739\n",
      "Eval loss 0.012335339561104774, R2 0.5248556137084961\n",
      "epoch 8266, loss 0.012341883033514023, R2 0.4869964122772217\n",
      "Eval loss 0.012335318140685558, R2 0.5248562097549438\n",
      "epoch 8267, loss 0.01234186440706253, R2 0.4869973063468933\n",
      "Eval loss 0.012335298582911491, R2 0.5248574018478394\n",
      "epoch 8268, loss 0.012341848574578762, R2 0.4869984984397888\n",
      "Eval loss 0.012335277162492275, R2 0.5248582363128662\n",
      "epoch 8269, loss 0.012341830879449844, R2 0.4869987368583679\n",
      "Eval loss 0.012335257604718208, R2 0.5248585939407349\n",
      "epoch 8270, loss 0.012341813184320927, R2 0.48699963092803955\n",
      "Eval loss 0.012335238046944141, R2 0.5248593091964722\n",
      "epoch 8271, loss 0.012341794557869434, R2 0.48699986934661865\n",
      "Eval loss 0.012335216626524925, R2 0.5248606204986572\n",
      "epoch 8272, loss 0.012341777794063091, R2 0.4870009422302246\n",
      "Eval loss 0.012335196137428284, R2 0.5248610973358154\n",
      "epoch 8273, loss 0.012341760098934174, R2 0.4870021939277649\n",
      "Eval loss 0.012335176579654217, R2 0.5248621702194214\n",
      "epoch 8274, loss 0.012341742403805256, R2 0.48700255155563354\n",
      "Eval loss 0.012335155159235, R2 0.5248628258705139\n",
      "epoch 8275, loss 0.012341724708676338, R2 0.4870031476020813\n",
      "Eval loss 0.012335135601460934, R2 0.524863600730896\n",
      "epoch 8276, loss 0.01234170701354742, R2 0.4870038628578186\n",
      "Eval loss 0.012335116043686867, R2 0.5248644351959229\n",
      "epoch 8277, loss 0.012341690249741077, R2 0.4870045781135559\n",
      "Eval loss 0.01233509462326765, R2 0.524865448474884\n",
      "epoch 8278, loss 0.012341671623289585, R2 0.48700547218322754\n",
      "Eval loss 0.012335075065493584, R2 0.5248658657073975\n",
      "epoch 8279, loss 0.012341654859483242, R2 0.4870060682296753\n",
      "Eval loss 0.012335054576396942, R2 0.5248665809631348\n",
      "epoch 8280, loss 0.01234163623303175, R2 0.4870067834854126\n",
      "Eval loss 0.012335033155977726, R2 0.5248675346374512\n",
      "epoch 8281, loss 0.012341621331870556, R2 0.48700809478759766\n",
      "Eval loss 0.012335014529526234, R2 0.5248684883117676\n",
      "epoch 8282, loss 0.012341602705419064, R2 0.48700839281082153\n",
      "Eval loss 0.012334994040429592, R2 0.5248690247535706\n",
      "epoch 8283, loss 0.012341585010290146, R2 0.4870089292526245\n",
      "Eval loss 0.0123349754139781, R2 0.5248697400093079\n",
      "epoch 8284, loss 0.012341568246483803, R2 0.4870096445083618\n",
      "Eval loss 0.012334953062236309, R2 0.5248704552650452\n",
      "epoch 8285, loss 0.012341550551354885, R2 0.48701101541519165\n",
      "Eval loss 0.012334932573139668, R2 0.5248711109161377\n",
      "epoch 8286, loss 0.012341533787548542, R2 0.4870110750198364\n",
      "Eval loss 0.0123349130153656, R2 0.524871826171875\n",
      "epoch 8287, loss 0.01234151516109705, R2 0.48701196908950806\n",
      "Eval loss 0.012334893457591534, R2 0.5248731970787048\n",
      "epoch 8288, loss 0.012341497465968132, R2 0.4870125651359558\n",
      "Eval loss 0.012334872968494892, R2 0.524873673915863\n",
      "epoch 8289, loss 0.012341480702161789, R2 0.48701345920562744\n",
      "Eval loss 0.01233485247939825, R2 0.524874210357666\n",
      "epoch 8290, loss 0.012341463007032871, R2 0.4870145320892334\n",
      "Eval loss 0.012334832921624184, R2 0.5248752236366272\n",
      "epoch 8291, loss 0.012341446243226528, R2 0.4870147109031677\n",
      "Eval loss 0.012334814295172691, R2 0.5248761177062988\n",
      "epoch 8292, loss 0.012341426685452461, R2 0.4870155453681946\n",
      "Eval loss 0.012334792874753475, R2 0.5248769521713257\n",
      "epoch 8293, loss 0.012341410852968693, R2 0.48701637983322144\n",
      "Eval loss 0.012334772385656834, R2 0.5248775482177734\n",
      "epoch 8294, loss 0.01234139408916235, R2 0.48701703548431396\n",
      "Eval loss 0.012334753759205341, R2 0.5248788595199585\n",
      "epoch 8295, loss 0.012341377325356007, R2 0.48701804876327515\n",
      "Eval loss 0.012334732338786125, R2 0.5248790979385376\n",
      "epoch 8296, loss 0.01234135776758194, R2 0.48701852560043335\n",
      "Eval loss 0.012334712781012058, R2 0.5248798131942749\n",
      "epoch 8297, loss 0.012341341003775597, R2 0.48701924085617065\n",
      "Eval loss 0.012334693223237991, R2 0.5248807668685913\n",
      "epoch 8298, loss 0.012341324239969254, R2 0.48701977729797363\n",
      "Eval loss 0.012334671802818775, R2 0.5248814225196838\n",
      "epoch 8299, loss 0.01234130747616291, R2 0.48702049255371094\n",
      "Eval loss 0.012334654107689857, R2 0.5248821377754211\n",
      "epoch 8300, loss 0.012341289781033993, R2 0.4870215654373169\n",
      "Eval loss 0.012334632687270641, R2 0.5248827934265137\n",
      "epoch 8301, loss 0.012341272085905075, R2 0.4870225787162781\n",
      "Eval loss 0.012334612198174, R2 0.524883508682251\n",
      "epoch 8302, loss 0.012341253459453583, R2 0.48702287673950195\n",
      "Eval loss 0.012334592640399933, R2 0.5248844623565674\n",
      "epoch 8303, loss 0.012341237626969814, R2 0.4870235323905945\n",
      "Eval loss 0.012334573082625866, R2 0.5248851180076599\n",
      "epoch 8304, loss 0.012341220863163471, R2 0.48702406883239746\n",
      "Eval loss 0.012334553524851799, R2 0.5248857140541077\n",
      "epoch 8305, loss 0.012341204099357128, R2 0.4870249629020691\n",
      "Eval loss 0.012334533967077732, R2 0.5248869061470032\n",
      "epoch 8306, loss 0.012341185472905636, R2 0.4870262145996094\n",
      "Eval loss 0.01233451347798109, R2 0.5248876810073853\n",
      "epoch 8307, loss 0.012341169640421867, R2 0.4870262145996094\n",
      "Eval loss 0.012334493920207024, R2 0.5248884558677673\n",
      "epoch 8308, loss 0.01234115194529295, R2 0.487027108669281\n",
      "Eval loss 0.012334473431110382, R2 0.5248891115188599\n",
      "epoch 8309, loss 0.012341135181486607, R2 0.48702800273895264\n",
      "Eval loss 0.012334453873336315, R2 0.5248895883560181\n",
      "epoch 8310, loss 0.012341117486357689, R2 0.4870283603668213\n",
      "Eval loss 0.012334433384239674, R2 0.5248903036117554\n",
      "epoch 8311, loss 0.012341098859906197, R2 0.48702913522720337\n",
      "Eval loss 0.012334413826465607, R2 0.5248913764953613\n",
      "epoch 8312, loss 0.012341083958745003, R2 0.4870297908782959\n",
      "Eval loss 0.01233439426869154, R2 0.5248920917510986\n",
      "epoch 8313, loss 0.012341066263616085, R2 0.4870305061340332\n",
      "Eval loss 0.012334374710917473, R2 0.5248929262161255\n",
      "epoch 8314, loss 0.012341049499809742, R2 0.48703140020370483\n",
      "Eval loss 0.012334354221820831, R2 0.5248938202857971\n",
      "epoch 8315, loss 0.012341031804680824, R2 0.4870319366455078\n",
      "Eval loss 0.012334335595369339, R2 0.5248947143554688\n",
      "epoch 8316, loss 0.012341015040874481, R2 0.4870326519012451\n",
      "Eval loss 0.012334316037595272, R2 0.5248955488204956\n",
      "epoch 8317, loss 0.012340998277068138, R2 0.4870331287384033\n",
      "Eval loss 0.01233429554849863, R2 0.5248959064483643\n",
      "epoch 8318, loss 0.012340979650616646, R2 0.4870341420173645\n",
      "Eval loss 0.012334275990724564, R2 0.5248963832855225\n",
      "epoch 8319, loss 0.012340963818132877, R2 0.48703479766845703\n",
      "Eval loss 0.012334255501627922, R2 0.5248972177505493\n",
      "epoch 8320, loss 0.012340947054326534, R2 0.48703545331954956\n",
      "Eval loss 0.01233423501253128, R2 0.5248979926109314\n",
      "epoch 8321, loss 0.012340929359197617, R2 0.48703670501708984\n",
      "Eval loss 0.012334215454757214, R2 0.5248987674713135\n",
      "epoch 8322, loss 0.012340912595391273, R2 0.48703688383102417\n",
      "Eval loss 0.012334195896983147, R2 0.5249000787734985\n",
      "epoch 8323, loss 0.01234089583158493, R2 0.487038254737854\n",
      "Eval loss 0.012334175407886505, R2 0.5249003171920776\n",
      "epoch 8324, loss 0.012340878136456013, R2 0.4870383143424988\n",
      "Eval loss 0.012334155850112438, R2 0.5249011516571045\n",
      "epoch 8325, loss 0.012340860441327095, R2 0.48703908920288086\n",
      "Eval loss 0.01233413815498352, R2 0.5249021053314209\n",
      "epoch 8326, loss 0.012340842746198177, R2 0.48703962564468384\n",
      "Eval loss 0.012334116734564304, R2 0.5249029397964478\n",
      "epoch 8327, loss 0.012340827845036983, R2 0.4870404005050659\n",
      "Eval loss 0.012334098108112812, R2 0.5249036550521851\n",
      "epoch 8328, loss 0.01234081108123064, R2 0.4870415925979614\n",
      "Eval loss 0.01233407761901617, R2 0.5249043107032776\n",
      "epoch 8329, loss 0.012340793386101723, R2 0.4870418310165405\n",
      "Eval loss 0.012334058992564678, R2 0.5249053239822388\n",
      "epoch 8330, loss 0.012340775690972805, R2 0.4870426058769226\n",
      "Eval loss 0.012334037572145462, R2 0.5249058604240417\n",
      "epoch 8331, loss 0.012340758927166462, R2 0.48704326152801514\n",
      "Eval loss 0.012334019877016544, R2 0.5249063968658447\n",
      "epoch 8332, loss 0.012340743094682693, R2 0.487044095993042\n",
      "Eval loss 0.012333998456597328, R2 0.5249074697494507\n",
      "epoch 8333, loss 0.01234072633087635, R2 0.48704516887664795\n",
      "Eval loss 0.012333979830145836, R2 0.5249082446098328\n",
      "epoch 8334, loss 0.012340708635747433, R2 0.4870455265045166\n",
      "Eval loss 0.012333960272371769, R2 0.5249085426330566\n",
      "epoch 8335, loss 0.01234069187194109, R2 0.4870460629463196\n",
      "Eval loss 0.012333941645920277, R2 0.5249096155166626\n",
      "epoch 8336, loss 0.012340674176812172, R2 0.4870467782020569\n",
      "Eval loss 0.012333921156823635, R2 0.5249103307723999\n",
      "epoch 8337, loss 0.012340657413005829, R2 0.4870479702949524\n",
      "Eval loss 0.012333902530372143, R2 0.5249112248420715\n",
      "epoch 8338, loss 0.012340640649199486, R2 0.487048864364624\n",
      "Eval loss 0.012333882041275501, R2 0.5249118804931641\n",
      "epoch 8339, loss 0.012340624816715717, R2 0.48704904317855835\n",
      "Eval loss 0.012333863414824009, R2 0.5249124765396118\n",
      "epoch 8340, loss 0.012340608052909374, R2 0.48705005645751953\n",
      "Eval loss 0.012333842925727367, R2 0.5249131917953491\n",
      "epoch 8341, loss 0.012340590357780457, R2 0.4870501160621643\n",
      "Eval loss 0.0123338233679533, R2 0.5249141454696655\n",
      "epoch 8342, loss 0.012340574525296688, R2 0.48705142736434937\n",
      "Eval loss 0.012333803810179234, R2 0.5249148607254028\n",
      "epoch 8343, loss 0.01234055683016777, R2 0.4870518445968628\n",
      "Eval loss 0.012333784252405167, R2 0.5249156355857849\n",
      "epoch 8344, loss 0.012340540997684002, R2 0.4870525002479553\n",
      "Eval loss 0.0123337646946311, R2 0.5249162316322327\n",
      "epoch 8345, loss 0.012340523302555084, R2 0.48705369234085083\n",
      "Eval loss 0.012333746068179607, R2 0.5249176621437073\n",
      "epoch 8346, loss 0.012340506538748741, R2 0.48705393075942993\n",
      "Eval loss 0.012333725579082966, R2 0.5249178409576416\n",
      "epoch 8347, loss 0.012340489774942398, R2 0.48705512285232544\n",
      "Eval loss 0.012333706952631474, R2 0.5249186158180237\n",
      "epoch 8348, loss 0.012340473011136055, R2 0.4870551824569702\n",
      "Eval loss 0.012333687394857407, R2 0.524919331073761\n",
      "epoch 8349, loss 0.012340457178652287, R2 0.4870561361312866\n",
      "Eval loss 0.012333668768405914, R2 0.5249202251434326\n",
      "epoch 8350, loss 0.012340438552200794, R2 0.4870566129684448\n",
      "Eval loss 0.012333648279309273, R2 0.5249207019805908\n",
      "epoch 8351, loss 0.0123404236510396, R2 0.4870573878288269\n",
      "Eval loss 0.012333630584180355, R2 0.5249215364456177\n",
      "epoch 8352, loss 0.012340405024588108, R2 0.48705846071243286\n",
      "Eval loss 0.012333610095083714, R2 0.5249223113059998\n",
      "epoch 8353, loss 0.01234038732945919, R2 0.48705875873565674\n",
      "Eval loss 0.012333589605987072, R2 0.5249234437942505\n",
      "epoch 8354, loss 0.012340372428297997, R2 0.4870595335960388\n",
      "Eval loss 0.012333570048213005, R2 0.52492356300354\n",
      "epoch 8355, loss 0.012340355664491653, R2 0.48706018924713135\n",
      "Eval loss 0.012333552353084087, R2 0.5249243974685669\n",
      "epoch 8356, loss 0.01234033890068531, R2 0.4870607256889343\n",
      "Eval loss 0.01233353279531002, R2 0.5249254703521729\n",
      "epoch 8357, loss 0.012340321205556393, R2 0.48706144094467163\n",
      "Eval loss 0.012333513237535954, R2 0.5249260663986206\n",
      "epoch 8358, loss 0.012340305373072624, R2 0.48706215620040894\n",
      "Eval loss 0.012333493679761887, R2 0.524927020072937\n",
      "epoch 8359, loss 0.012340288609266281, R2 0.4870629906654358\n",
      "Eval loss 0.012333475053310394, R2 0.5249278545379639\n",
      "epoch 8360, loss 0.012340271845459938, R2 0.4870637059211731\n",
      "Eval loss 0.012333455495536327, R2 0.5249285697937012\n",
      "epoch 8361, loss 0.01234025601297617, R2 0.48706454038619995\n",
      "Eval loss 0.012333435006439686, R2 0.5249289274215698\n",
      "epoch 8362, loss 0.012340238317847252, R2 0.4870653748512268\n",
      "Eval loss 0.012333416379988194, R2 0.524929940700531\n",
      "epoch 8363, loss 0.012340223416686058, R2 0.48706573247909546\n",
      "Eval loss 0.012333396822214127, R2 0.5249307155609131\n",
      "epoch 8364, loss 0.01234020572155714, R2 0.48706644773483276\n",
      "Eval loss 0.012333378195762634, R2 0.524931013584137\n",
      "epoch 8365, loss 0.012340188957750797, R2 0.48706763982772827\n",
      "Eval loss 0.012333357706665993, R2 0.5249320268630981\n",
      "epoch 8366, loss 0.012340172193944454, R2 0.48706769943237305\n",
      "Eval loss 0.012333340011537075, R2 0.5249326229095459\n",
      "epoch 8367, loss 0.012340156361460686, R2 0.4870688319206238\n",
      "Eval loss 0.012333320453763008, R2 0.5249336361885071\n",
      "epoch 8368, loss 0.012340139597654343, R2 0.48706918954849243\n",
      "Eval loss 0.012333301827311516, R2 0.5249342322349548\n",
      "epoch 8369, loss 0.012340122833848, R2 0.4870700240135193\n",
      "Eval loss 0.012333282269537449, R2 0.5249349474906921\n",
      "epoch 8370, loss 0.012340106070041656, R2 0.4870704412460327\n",
      "Eval loss 0.012333262711763382, R2 0.5249360203742981\n",
      "epoch 8371, loss 0.012340091168880463, R2 0.48707103729248047\n",
      "Eval loss 0.012333243153989315, R2 0.5249366164207458\n",
      "epoch 8372, loss 0.012340073473751545, R2 0.4870719313621521\n",
      "Eval loss 0.012333223596215248, R2 0.524937093257904\n",
      "epoch 8373, loss 0.012340056709945202, R2 0.4870726466178894\n",
      "Eval loss 0.012333204969763756, R2 0.5249379277229309\n",
      "epoch 8374, loss 0.012340039014816284, R2 0.4870733618736267\n",
      "Eval loss 0.012333187274634838, R2 0.5249386429786682\n",
      "epoch 8375, loss 0.01234002411365509, R2 0.4870736598968506\n",
      "Eval loss 0.012333165854215622, R2 0.5249391794204712\n",
      "epoch 8376, loss 0.012340007349848747, R2 0.48707419633865356\n",
      "Eval loss 0.012333149090409279, R2 0.5249402523040771\n",
      "epoch 8377, loss 0.012339990586042404, R2 0.48707520961761475\n",
      "Eval loss 0.012333127669990063, R2 0.5249414443969727\n",
      "epoch 8378, loss 0.012339974753558636, R2 0.4870758652687073\n",
      "Eval loss 0.012333109974861145, R2 0.5249419212341309\n",
      "epoch 8379, loss 0.012339957989752293, R2 0.48707735538482666\n",
      "Eval loss 0.012333090417087078, R2 0.5249423384666443\n",
      "epoch 8380, loss 0.01233994122594595, R2 0.48707741498947144\n",
      "Eval loss 0.012333071790635586, R2 0.524942934513092\n",
      "epoch 8381, loss 0.012339923530817032, R2 0.4870780110359192\n",
      "Eval loss 0.012333052232861519, R2 0.5249438285827637\n",
      "epoch 8382, loss 0.012339908629655838, R2 0.48707878589630127\n",
      "Eval loss 0.012333033606410027, R2 0.5249446630477905\n",
      "epoch 8383, loss 0.01233989279717207, R2 0.48707932233810425\n",
      "Eval loss 0.01233301404863596, R2 0.5249451398849487\n",
      "epoch 8384, loss 0.012339876033365726, R2 0.48707979917526245\n",
      "Eval loss 0.012332995422184467, R2 0.5249457359313965\n",
      "epoch 8385, loss 0.012339860200881958, R2 0.4870806336402893\n",
      "Eval loss 0.012332976795732975, R2 0.524946928024292\n",
      "epoch 8386, loss 0.01233984250575304, R2 0.48708200454711914\n",
      "Eval loss 0.012332957237958908, R2 0.5249476432800293\n",
      "epoch 8387, loss 0.012339826673269272, R2 0.4870820641517639\n",
      "Eval loss 0.012332938611507416, R2 0.5249484777450562\n",
      "epoch 8388, loss 0.012339808978140354, R2 0.4870827794075012\n",
      "Eval loss 0.012332919053733349, R2 0.5249488353729248\n",
      "epoch 8389, loss 0.012339793145656586, R2 0.4870832562446594\n",
      "Eval loss 0.012332900427281857, R2 0.5249497890472412\n",
      "epoch 8390, loss 0.012339778244495392, R2 0.48708420991897583\n",
      "Eval loss 0.01233288086950779, R2 0.5249501466751099\n",
      "epoch 8391, loss 0.012339760549366474, R2 0.4870847463607788\n",
      "Eval loss 0.012332863174378872, R2 0.5249509811401367\n",
      "epoch 8392, loss 0.012339744716882706, R2 0.4870859384536743\n",
      "Eval loss 0.012332843616604805, R2 0.524951696395874\n",
      "epoch 8393, loss 0.012339727953076363, R2 0.48708629608154297\n",
      "Eval loss 0.012332824990153313, R2 0.52495276927948\n",
      "epoch 8394, loss 0.012339712120592594, R2 0.4870869517326355\n",
      "Eval loss 0.012332805432379246, R2 0.5249530076980591\n",
      "epoch 8395, loss 0.012339695356786251, R2 0.4870874881744385\n",
      "Eval loss 0.012332786805927753, R2 0.5249544382095337\n",
      "epoch 8396, loss 0.012339679524302483, R2 0.487088143825531\n",
      "Eval loss 0.012332768179476261, R2 0.5249544382095337\n",
      "epoch 8397, loss 0.01233966276049614, R2 0.4870893359184265\n",
      "Eval loss 0.012332748621702194, R2 0.5249558091163635\n",
      "epoch 8398, loss 0.012339645996689796, R2 0.4870893955230713\n",
      "Eval loss 0.012332731857895851, R2 0.5249562859535217\n",
      "epoch 8399, loss 0.012339630164206028, R2 0.48709022998809814\n",
      "Eval loss 0.012332710437476635, R2 0.5249567031860352\n",
      "epoch 8400, loss 0.01233961433172226, R2 0.4870908856391907\n",
      "Eval loss 0.012332691811025143, R2 0.5249580144882202\n",
      "epoch 8401, loss 0.012339597567915916, R2 0.4870915412902832\n",
      "Eval loss 0.012332674115896225, R2 0.5249582529067993\n",
      "epoch 8402, loss 0.012339582666754723, R2 0.48709219694137573\n",
      "Eval loss 0.012332653626799583, R2 0.5249588489532471\n",
      "epoch 8403, loss 0.01233956590294838, R2 0.48709285259246826\n",
      "Eval loss 0.012332635931670666, R2 0.5249596834182739\n",
      "epoch 8404, loss 0.012339550070464611, R2 0.4870936870574951\n",
      "Eval loss 0.012332617305219173, R2 0.5249605178833008\n",
      "epoch 8405, loss 0.012339534237980843, R2 0.4870942234992981\n",
      "Eval loss 0.012332597747445107, R2 0.5249613523483276\n",
      "epoch 8406, loss 0.0123395174741745, R2 0.4870948791503906\n",
      "Eval loss 0.012332579120993614, R2 0.5249621868133545\n",
      "epoch 8407, loss 0.012339502573013306, R2 0.4870956540107727\n",
      "Eval loss 0.012332560494542122, R2 0.5249626636505127\n",
      "epoch 8408, loss 0.012339483946561813, R2 0.48709625005722046\n",
      "Eval loss 0.012332542799413204, R2 0.5249631404876709\n",
      "epoch 8409, loss 0.012339468114078045, R2 0.48709696531295776\n",
      "Eval loss 0.012332523241639137, R2 0.5249643325805664\n",
      "epoch 8410, loss 0.012339450418949127, R2 0.48709815740585327\n",
      "Eval loss 0.012332504615187645, R2 0.5249649286270142\n",
      "epoch 8411, loss 0.012339436449110508, R2 0.4870983958244324\n",
      "Eval loss 0.012332485057413578, R2 0.5249661207199097\n",
      "epoch 8412, loss 0.012339419685304165, R2 0.48709946870803833\n",
      "Eval loss 0.012332466430962086, R2 0.5249663591384888\n",
      "epoch 8413, loss 0.012339403852820396, R2 0.4870997667312622\n",
      "Eval loss 0.012332447804510593, R2 0.5249667167663574\n",
      "epoch 8414, loss 0.012339387089014053, R2 0.4871007800102234\n",
      "Eval loss 0.012332430109381676, R2 0.5249675512313843\n",
      "epoch 8415, loss 0.01233937032520771, R2 0.4871014952659607\n",
      "Eval loss 0.012332410551607609, R2 0.5249682664871216\n",
      "epoch 8416, loss 0.012339355424046516, R2 0.48710161447525024\n",
      "Eval loss 0.012332391925156116, R2 0.5249689817428589\n",
      "epoch 8417, loss 0.012339340522885323, R2 0.4871022701263428\n",
      "Eval loss 0.012332373298704624, R2 0.5249702334403992\n",
      "epoch 8418, loss 0.012339322827756405, R2 0.4871029853820801\n",
      "Eval loss 0.012332354672253132, R2 0.524970293045044\n",
      "epoch 8419, loss 0.012339306995272636, R2 0.4871041178703308\n",
      "Eval loss 0.012332335114479065, R2 0.5249714255332947\n",
      "epoch 8420, loss 0.012339291162788868, R2 0.48710477352142334\n",
      "Eval loss 0.012332318350672722, R2 0.5249720811843872\n",
      "epoch 8421, loss 0.0123392753303051, R2 0.48710477352142334\n",
      "Eval loss 0.01233229786157608, R2 0.5249731540679932\n",
      "epoch 8422, loss 0.012339258566498756, R2 0.4871061444282532\n",
      "Eval loss 0.012332280166447163, R2 0.5249733924865723\n",
      "epoch 8423, loss 0.012339241802692413, R2 0.4871063232421875\n",
      "Eval loss 0.012332262471318245, R2 0.5249744057655334\n",
      "epoch 8424, loss 0.012339227832853794, R2 0.48710691928863525\n",
      "Eval loss 0.012332241982221603, R2 0.524975061416626\n",
      "epoch 8425, loss 0.012339212000370026, R2 0.48710739612579346\n",
      "Eval loss 0.012332226149737835, R2 0.5249755382537842\n",
      "epoch 8426, loss 0.012339195236563683, R2 0.4871084690093994\n",
      "Eval loss 0.012332206591963768, R2 0.5249764919281006\n",
      "epoch 8427, loss 0.01233917847275734, R2 0.4871091842651367\n",
      "Eval loss 0.01233218889683485, R2 0.5249776840209961\n",
      "epoch 8428, loss 0.012339163571596146, R2 0.48710960149765015\n",
      "Eval loss 0.012332168407738209, R2 0.5249780416488647\n",
      "epoch 8429, loss 0.012339147739112377, R2 0.487110435962677\n",
      "Eval loss 0.012332149781286716, R2 0.5249782800674438\n",
      "epoch 8430, loss 0.012339130975306034, R2 0.4871109127998352\n",
      "Eval loss 0.012332132086157799, R2 0.5249794125556946\n",
      "epoch 8431, loss 0.01233911607414484, R2 0.48711156845092773\n",
      "Eval loss 0.012332112528383732, R2 0.5249803066253662\n",
      "epoch 8432, loss 0.012339100241661072, R2 0.48711222410202026\n",
      "Eval loss 0.012332094833254814, R2 0.5249806642532349\n",
      "epoch 8433, loss 0.012339082546532154, R2 0.48711347579956055\n",
      "Eval loss 0.012332075275480747, R2 0.5249818563461304\n",
      "epoch 8434, loss 0.01233906764537096, R2 0.4871137738227844\n",
      "Eval loss 0.012332056649029255, R2 0.5249821543693542\n",
      "epoch 8435, loss 0.012339051812887192, R2 0.4871142506599426\n",
      "Eval loss 0.012332038953900337, R2 0.5249828696250916\n",
      "epoch 8436, loss 0.012339035980403423, R2 0.48711472749710083\n",
      "Eval loss 0.012332020327448845, R2 0.5249834060668945\n",
      "epoch 8437, loss 0.012339020147919655, R2 0.4871155619621277\n",
      "Eval loss 0.012332001700997353, R2 0.5249843001365662\n",
      "epoch 8438, loss 0.012339004315435886, R2 0.48711687326431274\n",
      "Eval loss 0.01233198307454586, R2 0.5249850153923035\n",
      "epoch 8439, loss 0.012338988482952118, R2 0.4871172308921814\n",
      "Eval loss 0.012331965379416943, R2 0.5249854326248169\n",
      "epoch 8440, loss 0.01233897265046835, R2 0.4871177077293396\n",
      "Eval loss 0.01233194675296545, R2 0.5249862670898438\n",
      "epoch 8441, loss 0.012338955886662006, R2 0.4871187210083008\n",
      "Eval loss 0.012331929057836533, R2 0.5249868035316467\n",
      "epoch 8442, loss 0.012338940985500813, R2 0.48711884021759033\n",
      "Eval loss 0.012331911362707615, R2 0.5249879360198975\n",
      "epoch 8443, loss 0.01233892422169447, R2 0.48712068796157837\n",
      "Eval loss 0.012331891804933548, R2 0.5249885320663452\n",
      "epoch 8444, loss 0.012338909320533276, R2 0.4871201515197754\n",
      "Eval loss 0.012331873178482056, R2 0.524989128112793\n",
      "epoch 8445, loss 0.012338892556726933, R2 0.48712068796157837\n",
      "Eval loss 0.012331854552030563, R2 0.5249899625778198\n",
      "epoch 8446, loss 0.012338877655565739, R2 0.48712146282196045\n",
      "Eval loss 0.012331836856901646, R2 0.5249906778335571\n",
      "epoch 8447, loss 0.01233886182308197, R2 0.4871222972869873\n",
      "Eval loss 0.012331818230450153, R2 0.5249913334846497\n",
      "epoch 8448, loss 0.012338845990598202, R2 0.48712342977523804\n",
      "Eval loss 0.012331799603998661, R2 0.5249918103218079\n",
      "epoch 8449, loss 0.012338831089437008, R2 0.48712342977523804\n",
      "Eval loss 0.012331780977547169, R2 0.5249929428100586\n",
      "epoch 8450, loss 0.01233881339430809, R2 0.48712414503097534\n",
      "Eval loss 0.012331763282418251, R2 0.5249937772750854\n",
      "epoch 8451, loss 0.012338799424469471, R2 0.4871247410774231\n",
      "Eval loss 0.012331744655966759, R2 0.5249943733215332\n",
      "epoch 8452, loss 0.012338782660663128, R2 0.4871256351470947\n",
      "Eval loss 0.012331726960837841, R2 0.524994969367981\n",
      "epoch 8453, loss 0.012338767759501934, R2 0.48712652921676636\n",
      "Eval loss 0.012331709265708923, R2 0.5249954462051392\n",
      "epoch 8454, loss 0.012338751927018166, R2 0.4871267080307007\n",
      "Eval loss 0.012331689707934856, R2 0.5249960422515869\n",
      "epoch 8455, loss 0.012338736094534397, R2 0.48712748289108276\n",
      "Eval loss 0.012331672012805939, R2 0.5249965786933899\n",
      "epoch 8456, loss 0.012338720262050629, R2 0.48712801933288574\n",
      "Eval loss 0.012331653386354446, R2 0.5249972939491272\n",
      "epoch 8457, loss 0.012338703498244286, R2 0.4871293902397156\n",
      "Eval loss 0.012331635691225529, R2 0.524998128414154\n",
      "epoch 8458, loss 0.012338687665760517, R2 0.4871298670768738\n",
      "Eval loss 0.012331617064774036, R2 0.5249987840652466\n",
      "epoch 8459, loss 0.012338672764599323, R2 0.48713046312332153\n",
      "Eval loss 0.012331599369645119, R2 0.5249994993209839\n",
      "epoch 8460, loss 0.012338656932115555, R2 0.4871308207511902\n",
      "Eval loss 0.012331579811871052, R2 0.5250003933906555\n",
      "epoch 8461, loss 0.012338642030954361, R2 0.4871312975883484\n",
      "Eval loss 0.012331562116742134, R2 0.5250012278556824\n",
      "epoch 8462, loss 0.012338626198470592, R2 0.4871324300765991\n",
      "Eval loss 0.012331543490290642, R2 0.5250020623207092\n",
      "epoch 8463, loss 0.012338610365986824, R2 0.48713308572769165\n",
      "Eval loss 0.012331526726484299, R2 0.5250025987625122\n",
      "epoch 8464, loss 0.01233859546482563, R2 0.4871329069137573\n",
      "Eval loss 0.012331508100032806, R2 0.52500319480896\n",
      "epoch 8465, loss 0.012338579632341862, R2 0.48713386058807373\n",
      "Eval loss 0.012331489473581314, R2 0.5250041484832764\n",
      "epoch 8466, loss 0.012338563799858093, R2 0.4871346950531006\n",
      "Eval loss 0.012331471778452396, R2 0.5250045657157898\n",
      "epoch 8467, loss 0.012338547967374325, R2 0.4871353507041931\n",
      "Eval loss 0.012331454083323479, R2 0.5250053405761719\n",
      "epoch 8468, loss 0.012338531203567982, R2 0.48713600635528564\n",
      "Eval loss 0.012331434525549412, R2 0.5250064134597778\n",
      "epoch 8469, loss 0.012338516302406788, R2 0.4871366620063782\n",
      "Eval loss 0.012331416830420494, R2 0.5250067710876465\n",
      "epoch 8470, loss 0.012338502332568169, R2 0.48713696002960205\n",
      "Eval loss 0.012331399135291576, R2 0.5250076055526733\n",
      "epoch 8471, loss 0.012338484637439251, R2 0.4871377944946289\n",
      "Eval loss 0.012331380508840084, R2 0.525008499622345\n",
      "epoch 8472, loss 0.012338470667600632, R2 0.4871390461921692\n",
      "Eval loss 0.012331361882388592, R2 0.5250087976455688\n",
      "epoch 8473, loss 0.012338454835116863, R2 0.4871395230293274\n",
      "Eval loss 0.0123313432559371, R2 0.5250093936920166\n",
      "epoch 8474, loss 0.012338439002633095, R2 0.4871397018432617\n",
      "Eval loss 0.012331325560808182, R2 0.5250100493431091\n",
      "epoch 8475, loss 0.012338423170149326, R2 0.487140417098999\n",
      "Eval loss 0.012331309728324413, R2 0.5250112414360046\n",
      "epoch 8476, loss 0.012338407337665558, R2 0.4871410131454468\n",
      "Eval loss 0.012331290170550346, R2 0.5250116586685181\n",
      "epoch 8477, loss 0.012338392436504364, R2 0.4871416687965393\n",
      "Eval loss 0.012331273406744003, R2 0.5250120759010315\n",
      "epoch 8478, loss 0.012338375672698021, R2 0.48714250326156616\n",
      "Eval loss 0.012331254780292511, R2 0.5250129103660583\n",
      "epoch 8479, loss 0.012338361702859402, R2 0.4871429204940796\n",
      "Eval loss 0.012331236153841019, R2 0.5250140428543091\n",
      "epoch 8480, loss 0.012338345870375633, R2 0.48714345693588257\n",
      "Eval loss 0.012331218458712101, R2 0.5250141620635986\n",
      "epoch 8481, loss 0.01233832910656929, R2 0.4871447682380676\n",
      "Eval loss 0.012331200763583183, R2 0.525015115737915\n",
      "epoch 8482, loss 0.012338314205408096, R2 0.4871450662612915\n",
      "Eval loss 0.012331181205809116, R2 0.5250160098075867\n",
      "epoch 8483, loss 0.012338299304246902, R2 0.48714548349380493\n",
      "Eval loss 0.012331163510680199, R2 0.525016725063324\n",
      "epoch 8484, loss 0.012338283471763134, R2 0.4871463179588318\n",
      "Eval loss 0.012331146746873856, R2 0.5250172019004822\n",
      "epoch 8485, loss 0.012338267639279366, R2 0.48714685440063477\n",
      "Eval loss 0.012331128120422363, R2 0.5250179767608643\n",
      "epoch 8486, loss 0.012338252738118172, R2 0.4871477484703064\n",
      "Eval loss 0.01233111135661602, R2 0.5250183343887329\n",
      "epoch 8487, loss 0.012338237836956978, R2 0.48714810609817505\n",
      "Eval loss 0.012331091798841953, R2 0.5250190496444702\n",
      "epoch 8488, loss 0.012338222935795784, R2 0.487149178981781\n",
      "Eval loss 0.01233107503503561, R2 0.5250197052955627\n",
      "epoch 8489, loss 0.012338207103312016, R2 0.48714983463287354\n",
      "Eval loss 0.012331057339906693, R2 0.5250203609466553\n",
      "epoch 8490, loss 0.012338192202150822, R2 0.4871499538421631\n",
      "Eval loss 0.012331039644777775, R2 0.525021493434906\n",
      "epoch 8491, loss 0.012338176369667053, R2 0.4871506094932556\n",
      "Eval loss 0.012331020087003708, R2 0.5250222682952881\n",
      "epoch 8492, loss 0.012338160537183285, R2 0.48715144395828247\n",
      "Eval loss 0.012331003323197365, R2 0.5250225067138672\n",
      "epoch 8493, loss 0.012338145636022091, R2 0.4871516227722168\n",
      "Eval loss 0.012330983765423298, R2 0.5250237584114075\n",
      "epoch 8494, loss 0.012338129803538322, R2 0.4871530532836914\n",
      "Eval loss 0.01233096793293953, R2 0.5250241160392761\n",
      "epoch 8495, loss 0.012338113971054554, R2 0.4871530532836914\n",
      "Eval loss 0.012330950237810612, R2 0.5250247716903687\n",
      "epoch 8496, loss 0.012338098138570786, R2 0.4871540069580078\n",
      "Eval loss 0.01233093161135912, R2 0.5250253677368164\n",
      "epoch 8497, loss 0.012338083237409592, R2 0.48715513944625854\n",
      "Eval loss 0.012330913916230202, R2 0.5250265002250671\n",
      "epoch 8498, loss 0.012338069267570972, R2 0.48715507984161377\n",
      "Eval loss 0.01233089528977871, R2 0.5250269174575806\n",
      "epoch 8499, loss 0.012338053435087204, R2 0.4871557354927063\n",
      "Eval loss 0.012330877594649792, R2 0.525027334690094\n",
      "epoch 8500, loss 0.012338037602603436, R2 0.48715656995773315\n",
      "Eval loss 0.012330859899520874, R2 0.5250279903411865\n",
      "epoch 8501, loss 0.012338021770119667, R2 0.48715686798095703\n",
      "Eval loss 0.012330843135714531, R2 0.5250290632247925\n",
      "epoch 8502, loss 0.012338006868958473, R2 0.4871571660041809\n",
      "Eval loss 0.012330824509263039, R2 0.5250296592712402\n",
      "epoch 8503, loss 0.01233799196779728, R2 0.48715829849243164\n",
      "Eval loss 0.012330806814134121, R2 0.5250304937362671\n",
      "epoch 8504, loss 0.012337977066636086, R2 0.4871590733528137\n",
      "Eval loss 0.012330789119005203, R2 0.5250308513641357\n",
      "epoch 8505, loss 0.012337960302829742, R2 0.487159788608551\n",
      "Eval loss 0.012330770492553711, R2 0.5250316858291626\n",
      "epoch 8506, loss 0.012337947264313698, R2 0.487160325050354\n",
      "Eval loss 0.012330755591392517, R2 0.5250324010848999\n",
      "epoch 8507, loss 0.01233793143182993, R2 0.48716098070144653\n",
      "Eval loss 0.01233073603361845, R2 0.5250327587127686\n",
      "epoch 8508, loss 0.012337916530668736, R2 0.4871615767478943\n",
      "Eval loss 0.012330719269812107, R2 0.5250335931777954\n",
      "epoch 8509, loss 0.012337900698184967, R2 0.48716241121292114\n",
      "Eval loss 0.012330700643360615, R2 0.5250341296195984\n",
      "epoch 8510, loss 0.012337884865701199, R2 0.4871627688407898\n",
      "Eval loss 0.012330682948231697, R2 0.5250350832939148\n",
      "epoch 8511, loss 0.012337869964540005, R2 0.48716336488723755\n",
      "Eval loss 0.01233066525310278, R2 0.5250357389450073\n",
      "epoch 8512, loss 0.012337855063378811, R2 0.48716461658477783\n",
      "Eval loss 0.012330647557973862, R2 0.5250365734100342\n",
      "epoch 8513, loss 0.012337840162217617, R2 0.48716479539871216\n",
      "Eval loss 0.01233062893152237, R2 0.5250370502471924\n",
      "epoch 8514, loss 0.012337826192378998, R2 0.4871652126312256\n",
      "Eval loss 0.012330612167716026, R2 0.5250378251075745\n",
      "epoch 8515, loss 0.01233781035989523, R2 0.4871656894683838\n",
      "Eval loss 0.012330594472587109, R2 0.525038480758667\n",
      "epoch 8516, loss 0.012337793596088886, R2 0.48716652393341064\n",
      "Eval loss 0.012330576777458191, R2 0.5250393152236938\n",
      "epoch 8517, loss 0.012337779626250267, R2 0.4871675968170166\n",
      "Eval loss 0.012330559082329273, R2 0.5250401496887207\n",
      "epoch 8518, loss 0.012337765656411648, R2 0.4871678352355957\n",
      "Eval loss 0.012330541387200356, R2 0.5250402688980103\n",
      "epoch 8519, loss 0.012337748892605305, R2 0.48716843128204346\n",
      "Eval loss 0.012330525554716587, R2 0.525040864944458\n",
      "epoch 8520, loss 0.012337733991444111, R2 0.48716968297958374\n",
      "Eval loss 0.01233050599694252, R2 0.5250417590141296\n",
      "epoch 8521, loss 0.012337719090282917, R2 0.4871702790260315\n",
      "Eval loss 0.012330488301813602, R2 0.5250424146652222\n",
      "epoch 8522, loss 0.012337703257799149, R2 0.4871704578399658\n",
      "Eval loss 0.012330470606684685, R2 0.5250433683395386\n",
      "epoch 8523, loss 0.012337688356637955, R2 0.4871707558631897\n",
      "Eval loss 0.012330452911555767, R2 0.5250439643859863\n",
      "epoch 8524, loss 0.01233767531812191, R2 0.4871717691421509\n",
      "Eval loss 0.012330437079071999, R2 0.5250445604324341\n",
      "epoch 8525, loss 0.012337658554315567, R2 0.48717260360717773\n",
      "Eval loss 0.012330418452620506, R2 0.5250451564788818\n",
      "epoch 8526, loss 0.012337642721831799, R2 0.48717284202575684\n",
      "Eval loss 0.012330401688814163, R2 0.5250459313392639\n",
      "epoch 8527, loss 0.01233762875199318, R2 0.48717403411865234\n",
      "Eval loss 0.012330383062362671, R2 0.5250465869903564\n",
      "epoch 8528, loss 0.012337613850831985, R2 0.48717451095581055\n",
      "Eval loss 0.012330366298556328, R2 0.5250470042228699\n",
      "epoch 8529, loss 0.012337598018348217, R2 0.48717451095581055\n",
      "Eval loss 0.01233034860342741, R2 0.5250476598739624\n",
      "epoch 8530, loss 0.012337584048509598, R2 0.48717570304870605\n",
      "Eval loss 0.012330331839621067, R2 0.5250484943389893\n",
      "epoch 8531, loss 0.01233756821602583, R2 0.4871758818626404\n",
      "Eval loss 0.012330312281847, R2 0.52504962682724\n",
      "epoch 8532, loss 0.01233755424618721, R2 0.48717665672302246\n",
      "Eval loss 0.012330295518040657, R2 0.5250500440597534\n",
      "epoch 8533, loss 0.012337538413703442, R2 0.48717761039733887\n",
      "Eval loss 0.012330278754234314, R2 0.5250504016876221\n",
      "epoch 8534, loss 0.012337523512542248, R2 0.4871777892112732\n",
      "Eval loss 0.012330261059105396, R2 0.5250513553619385\n",
      "epoch 8535, loss 0.012337509542703629, R2 0.48717838525772095\n",
      "Eval loss 0.012330243363976479, R2 0.5250520706176758\n",
      "epoch 8536, loss 0.01233749371021986, R2 0.4871789813041687\n",
      "Eval loss 0.012330225668847561, R2 0.525052547454834\n",
      "epoch 8537, loss 0.012337478809058666, R2 0.48718011379241943\n",
      "Eval loss 0.012330208905041218, R2 0.5250533819198608\n",
      "epoch 8538, loss 0.012337463907897472, R2 0.487180233001709\n",
      "Eval loss 0.012330190278589725, R2 0.5250537395477295\n",
      "epoch 8539, loss 0.012337449006736279, R2 0.48718100786209106\n",
      "Eval loss 0.012330174446105957, R2 0.5250548124313354\n",
      "epoch 8540, loss 0.01233743503689766, R2 0.48718130588531494\n",
      "Eval loss 0.01233015675097704, R2 0.5250555276870728\n",
      "epoch 8541, loss 0.012337418273091316, R2 0.4871823191642761\n",
      "Eval loss 0.012330140918493271, R2 0.5250556468963623\n",
      "epoch 8542, loss 0.012337403371930122, R2 0.4871829152107239\n",
      "Eval loss 0.012330121360719204, R2 0.5250566005706787\n",
      "epoch 8543, loss 0.012337390333414078, R2 0.48718351125717163\n",
      "Eval loss 0.012330104596912861, R2 0.5250571966171265\n",
      "epoch 8544, loss 0.01233737450093031, R2 0.48718446493148804\n",
      "Eval loss 0.012330087833106518, R2 0.5250584483146667\n",
      "epoch 8545, loss 0.01233736053109169, R2 0.4871845245361328\n",
      "Eval loss 0.012330069206655025, R2 0.5250588655471802\n",
      "epoch 8546, loss 0.012337344698607922, R2 0.487185001373291\n",
      "Eval loss 0.012330053374171257, R2 0.5250591039657593\n",
      "epoch 8547, loss 0.012337329797446728, R2 0.48718583583831787\n",
      "Eval loss 0.012330036610364914, R2 0.5250599980354309\n",
      "epoch 8548, loss 0.012337314896285534, R2 0.4871869683265686\n",
      "Eval loss 0.012330017983913422, R2 0.5250608325004578\n",
      "epoch 8549, loss 0.01233729999512434, R2 0.4871875047683716\n",
      "Eval loss 0.012330001220107079, R2 0.5250610709190369\n",
      "epoch 8550, loss 0.01233728602528572, R2 0.48718762397766113\n",
      "Eval loss 0.01232998352497816, R2 0.5250620245933533\n",
      "epoch 8551, loss 0.012337271124124527, R2 0.48718827962875366\n",
      "Eval loss 0.012329964898526669, R2 0.5250630378723145\n",
      "epoch 8552, loss 0.012337255291640759, R2 0.48718905448913574\n",
      "Eval loss 0.01232994720339775, R2 0.5250630378723145\n",
      "epoch 8553, loss 0.012337242253124714, R2 0.48718947172164917\n",
      "Eval loss 0.012329932302236557, R2 0.5250641107559204\n",
      "epoch 8554, loss 0.01233722548931837, R2 0.48719078302383423\n",
      "Eval loss 0.012329913675785065, R2 0.5250644683837891\n",
      "epoch 8555, loss 0.012337211519479752, R2 0.48719072341918945\n",
      "Eval loss 0.012329895980656147, R2 0.5250654220581055\n",
      "epoch 8556, loss 0.012337196618318558, R2 0.4871915578842163\n",
      "Eval loss 0.012329880148172379, R2 0.5250661373138428\n",
      "epoch 8557, loss 0.01233718078583479, R2 0.48719215393066406\n",
      "Eval loss 0.01232986245304346, R2 0.5250665545463562\n",
      "epoch 8558, loss 0.01233716681599617, R2 0.4871925711631775\n",
      "Eval loss 0.012329845689237118, R2 0.5250676274299622\n",
      "epoch 8559, loss 0.012337153777480125, R2 0.48719316720962524\n",
      "Eval loss 0.0123298279941082, R2 0.5250676870346069\n",
      "epoch 8560, loss 0.012337137013673782, R2 0.4871940016746521\n",
      "Eval loss 0.012329811230301857, R2 0.5250691175460815\n",
      "epoch 8561, loss 0.012337122112512589, R2 0.48719513416290283\n",
      "Eval loss 0.01232979353517294, R2 0.5250694751739502\n",
      "epoch 8562, loss 0.012337109073996544, R2 0.4871951937675476\n",
      "Eval loss 0.012329776771366596, R2 0.5250698328018188\n",
      "epoch 8563, loss 0.0123370923101902, R2 0.487196147441864\n",
      "Eval loss 0.012329759076237679, R2 0.5250704288482666\n",
      "epoch 8564, loss 0.012337079271674156, R2 0.4871963858604431\n",
      "Eval loss 0.012329742312431335, R2 0.5250713229179382\n",
      "epoch 8565, loss 0.012337064370512962, R2 0.4871974587440491\n",
      "Eval loss 0.012329724617302418, R2 0.5250716209411621\n",
      "epoch 8566, loss 0.012337049469351768, R2 0.4871974587440491\n",
      "Eval loss 0.0123297069221735, R2 0.5250726938247681\n",
      "epoch 8567, loss 0.012337034568190575, R2 0.4871981143951416\n",
      "Eval loss 0.012329690158367157, R2 0.5250734686851501\n",
      "epoch 8568, loss 0.01233701966702938, R2 0.48719871044158936\n",
      "Eval loss 0.012329673394560814, R2 0.5250738263130188\n",
      "epoch 8569, loss 0.012337005697190762, R2 0.4871991276741028\n",
      "Eval loss 0.01232965663075447, R2 0.5250746011734009\n",
      "epoch 8570, loss 0.012336990796029568, R2 0.4872000813484192\n",
      "Eval loss 0.012329640798270702, R2 0.5250752568244934\n",
      "epoch 8571, loss 0.012336975894868374, R2 0.4872004985809326\n",
      "Eval loss 0.01232962217181921, R2 0.5250759124755859\n",
      "epoch 8572, loss 0.012336961925029755, R2 0.48720109462738037\n",
      "Eval loss 0.012329604476690292, R2 0.5250769257545471\n",
      "epoch 8573, loss 0.012336945161223412, R2 0.4872019290924072\n",
      "Eval loss 0.01232958771288395, R2 0.5250771045684814\n",
      "epoch 8574, loss 0.012336933054029942, R2 0.4872022867202759\n",
      "Eval loss 0.01232957188040018, R2 0.5250780582427979\n",
      "epoch 8575, loss 0.012336918152868748, R2 0.4872027635574341\n",
      "Eval loss 0.012329553253948689, R2 0.5250785946846008\n",
      "epoch 8576, loss 0.01233690232038498, R2 0.48720359802246094\n",
      "Eval loss 0.012329538352787495, R2 0.5250791907310486\n",
      "epoch 8577, loss 0.01233688835054636, R2 0.4872046709060669\n",
      "Eval loss 0.012329519726336002, R2 0.5250799059867859\n",
      "epoch 8578, loss 0.01233687438070774, R2 0.487204909324646\n",
      "Eval loss 0.012329502031207085, R2 0.5250803232192993\n",
      "epoch 8579, loss 0.012336858548223972, R2 0.4872053861618042\n",
      "Eval loss 0.012329485267400742, R2 0.5250812768936157\n",
      "epoch 8580, loss 0.012336845509707928, R2 0.4872061014175415\n",
      "Eval loss 0.012329468503594398, R2 0.5250818729400635\n",
      "epoch 8581, loss 0.01233682781457901, R2 0.4872068166732788\n",
      "Eval loss 0.01232945080846548, R2 0.5250824093818665\n",
      "epoch 8582, loss 0.012336814776062965, R2 0.48720723390579224\n",
      "Eval loss 0.012329433113336563, R2 0.525083065032959\n",
      "epoch 8583, loss 0.012336800806224346, R2 0.4872083067893982\n",
      "Eval loss 0.01232941821217537, R2 0.5250839591026306\n",
      "epoch 8584, loss 0.012336785905063152, R2 0.48720842599868774\n",
      "Eval loss 0.012329401448369026, R2 0.5250841975212097\n",
      "epoch 8585, loss 0.012336771003901958, R2 0.4872090220451355\n",
      "Eval loss 0.012329384684562683, R2 0.5250853300094604\n",
      "epoch 8586, loss 0.01233675703406334, R2 0.4872097969055176\n",
      "Eval loss 0.012329366989433765, R2 0.5250858068466187\n",
      "epoch 8587, loss 0.012336742132902145, R2 0.487210214138031\n",
      "Eval loss 0.012329350225627422, R2 0.525086522102356\n",
      "epoch 8588, loss 0.012336728163063526, R2 0.48721081018447876\n",
      "Eval loss 0.012329332530498505, R2 0.5250868201255798\n",
      "epoch 8589, loss 0.012336715124547482, R2 0.48721206188201904\n",
      "Eval loss 0.012329315766692162, R2 0.5250877141952515\n",
      "epoch 8590, loss 0.012336699292063713, R2 0.48721200227737427\n",
      "Eval loss 0.012329299002885818, R2 0.5250884294509888\n",
      "epoch 8591, loss 0.012336683459579945, R2 0.4872131943702698\n",
      "Eval loss 0.012329284101724625, R2 0.5250891447067261\n",
      "epoch 8592, loss 0.012336669489741325, R2 0.4872139096260071\n",
      "Eval loss 0.012329265475273132, R2 0.5250898599624634\n",
      "epoch 8593, loss 0.01233665645122528, R2 0.48721396923065186\n",
      "Eval loss 0.012329249642789364, R2 0.5250903367996216\n",
      "epoch 8594, loss 0.012336641550064087, R2 0.48721444606781006\n",
      "Eval loss 0.012329231016337872, R2 0.5250908732414246\n",
      "epoch 8595, loss 0.012336626648902893, R2 0.48721522092819214\n",
      "Eval loss 0.012329215183854103, R2 0.5250911712646484\n",
      "epoch 8596, loss 0.012336612679064274, R2 0.48721545934677124\n",
      "Eval loss 0.012329197488725185, R2 0.5250920057296753\n",
      "epoch 8597, loss 0.01233659777790308, R2 0.48721641302108765\n",
      "Eval loss 0.012329181656241417, R2 0.5250927805900574\n",
      "epoch 8598, loss 0.01233658380806446, R2 0.48721665143966675\n",
      "Eval loss 0.012329164892435074, R2 0.525093674659729\n",
      "epoch 8599, loss 0.012336568906903267, R2 0.48721742630004883\n",
      "Eval loss 0.012329147197306156, R2 0.5250943899154663\n",
      "epoch 8600, loss 0.012336555868387222, R2 0.48721814155578613\n",
      "Eval loss 0.012329130433499813, R2 0.5250951647758484\n",
      "epoch 8601, loss 0.012336540035903454, R2 0.4872191548347473\n",
      "Eval loss 0.01232911366969347, R2 0.525095522403717\n",
      "epoch 8602, loss 0.01233652699738741, R2 0.4872192144393921\n",
      "Eval loss 0.012329096905887127, R2 0.5250959396362305\n",
      "epoch 8603, loss 0.01233651302754879, R2 0.48722028732299805\n",
      "Eval loss 0.012329080142080784, R2 0.525097131729126\n",
      "epoch 8604, loss 0.012336498126387596, R2 0.4872204065322876\n",
      "Eval loss 0.012329064309597015, R2 0.5250972509384155\n",
      "epoch 8605, loss 0.012336483225226402, R2 0.48722100257873535\n",
      "Eval loss 0.012329046614468098, R2 0.5250979661941528\n",
      "epoch 8606, loss 0.012336469255387783, R2 0.48722171783447266\n",
      "Eval loss 0.012329029850661755, R2 0.5250986218452454\n",
      "epoch 8607, loss 0.012336455285549164, R2 0.48722219467163086\n",
      "Eval loss 0.012329013086855412, R2 0.5250993967056274\n",
      "epoch 8608, loss 0.012336441315710545, R2 0.4872232675552368\n",
      "Eval loss 0.012328996323049068, R2 0.5250998735427856\n",
      "epoch 8609, loss 0.012336425483226776, R2 0.4872240424156189\n",
      "Eval loss 0.0123289804905653, R2 0.5251007080078125\n",
      "epoch 8610, loss 0.012336411513388157, R2 0.48722416162490845\n",
      "Eval loss 0.012328961864113808, R2 0.525101363658905\n",
      "epoch 8611, loss 0.012336397543549538, R2 0.4872245788574219\n",
      "Eval loss 0.01232894603163004, R2 0.5251017212867737\n",
      "epoch 8612, loss 0.012336382642388344, R2 0.48722535371780396\n",
      "Eval loss 0.012328929267823696, R2 0.5251026153564453\n",
      "epoch 8613, loss 0.012336368672549725, R2 0.4872259497642517\n",
      "Eval loss 0.012328913435339928, R2 0.5251032114028931\n",
      "epoch 8614, loss 0.01233635563403368, R2 0.4872261881828308\n",
      "Eval loss 0.01232889574021101, R2 0.5251038074493408\n",
      "epoch 8615, loss 0.012336340732872486, R2 0.4872274398803711\n",
      "Eval loss 0.012328878976404667, R2 0.5251044034957886\n",
      "epoch 8616, loss 0.012336326763033867, R2 0.48722749948501587\n",
      "Eval loss 0.012328862212598324, R2 0.525105357170105\n",
      "epoch 8617, loss 0.012336311861872673, R2 0.4872280955314636\n",
      "Eval loss 0.01232884731143713, R2 0.5251059532165527\n",
      "epoch 8618, loss 0.01233629696071148, R2 0.48722904920578003\n",
      "Eval loss 0.012328830547630787, R2 0.5251063108444214\n",
      "epoch 8619, loss 0.01233628299087286, R2 0.48722952604293823\n",
      "Eval loss 0.012328813783824444, R2 0.5251069664955139\n",
      "epoch 8620, loss 0.01233626902103424, R2 0.4872298836708069\n",
      "Eval loss 0.0123287970200181, R2 0.5251076221466064\n",
      "epoch 8621, loss 0.012336255982518196, R2 0.4872303009033203\n",
      "Eval loss 0.012328780256211758, R2 0.5251082181930542\n",
      "epoch 8622, loss 0.012336241081357002, R2 0.4872310757637024\n",
      "Eval loss 0.01232876442372799, R2 0.5251091718673706\n",
      "epoch 8623, loss 0.012336226180195808, R2 0.48723167181015015\n",
      "Eval loss 0.012328746728599072, R2 0.5251095294952393\n",
      "epoch 8624, loss 0.012336213141679764, R2 0.4872322082519531\n",
      "Eval loss 0.012328731827437878, R2 0.52511066198349\n",
      "epoch 8625, loss 0.01233619824051857, R2 0.48723268508911133\n",
      "Eval loss 0.012328713200986385, R2 0.5251110792160034\n",
      "epoch 8626, loss 0.012336183339357376, R2 0.48723316192626953\n",
      "Eval loss 0.012328697368502617, R2 0.5251113176345825\n",
      "epoch 8627, loss 0.012336170300841331, R2 0.4872339963912964\n",
      "Eval loss 0.0123286796733737, R2 0.5251123905181885\n",
      "epoch 8628, loss 0.012336156331002712, R2 0.48723477125167847\n",
      "Eval loss 0.01232866384088993, R2 0.5251128673553467\n",
      "epoch 8629, loss 0.012336141429841518, R2 0.4872353672981262\n",
      "Eval loss 0.012328647077083588, R2 0.5251133441925049\n",
      "epoch 8630, loss 0.012336128391325474, R2 0.48723578453063965\n",
      "Eval loss 0.01232863124459982, R2 0.525114119052887\n",
      "epoch 8631, loss 0.012336112558841705, R2 0.48723655939102173\n",
      "Eval loss 0.01232861541211605, R2 0.5251147747039795\n",
      "epoch 8632, loss 0.01233609952032566, R2 0.48723697662353516\n",
      "Eval loss 0.012328598648309708, R2 0.5251153707504272\n",
      "epoch 8633, loss 0.012336085550487041, R2 0.4872375726699829\n",
      "Eval loss 0.012328580021858215, R2 0.525115966796875\n",
      "epoch 8634, loss 0.012336070649325848, R2 0.48723816871643066\n",
      "Eval loss 0.012328566052019596, R2 0.5251166820526123\n",
      "epoch 8635, loss 0.012336055748164654, R2 0.48723942041397095\n",
      "Eval loss 0.012328548356890678, R2 0.525117039680481\n",
      "epoch 8636, loss 0.01233604270964861, R2 0.4872399568557739\n",
      "Eval loss 0.01232853252440691, R2 0.5251179337501526\n",
      "epoch 8637, loss 0.01233602873980999, R2 0.4872400760650635\n",
      "Eval loss 0.012328515760600567, R2 0.5251185894012451\n",
      "epoch 8638, loss 0.01233601476997137, R2 0.4872403144836426\n",
      "Eval loss 0.012328498996794224, R2 0.5251193642616272\n",
      "epoch 8639, loss 0.012336000800132751, R2 0.48724108934402466\n",
      "Eval loss 0.012328483164310455, R2 0.525119960308075\n",
      "epoch 8640, loss 0.012335985898971558, R2 0.4872416853904724\n",
      "Eval loss 0.012328466400504112, R2 0.5251204967498779\n",
      "epoch 8641, loss 0.012335973791778088, R2 0.4872421622276306\n",
      "Eval loss 0.01232844963669777, R2 0.5251210927963257\n",
      "epoch 8642, loss 0.012335958890616894, R2 0.48724299669265747\n",
      "Eval loss 0.012328432872891426, R2 0.5251217484474182\n",
      "epoch 8643, loss 0.012335944920778275, R2 0.48724353313446045\n",
      "Eval loss 0.012328417040407658, R2 0.5251227617263794\n",
      "epoch 8644, loss 0.012335930950939655, R2 0.48724365234375\n",
      "Eval loss 0.012328400276601315, R2 0.525123119354248\n",
      "epoch 8645, loss 0.012335916981101036, R2 0.48724454641342163\n",
      "Eval loss 0.012328384444117546, R2 0.5251234769821167\n",
      "epoch 8646, loss 0.012335902079939842, R2 0.4872451424598694\n",
      "Eval loss 0.012328367680311203, R2 0.5251244306564331\n",
      "epoch 8647, loss 0.012335888110101223, R2 0.48724591732025146\n",
      "Eval loss 0.012328351847827435, R2 0.5251251459121704\n",
      "epoch 8648, loss 0.012335874140262604, R2 0.4872463345527649\n",
      "Eval loss 0.012328336015343666, R2 0.5251255035400391\n",
      "epoch 8649, loss 0.01233586110174656, R2 0.4872473478317261\n",
      "Eval loss 0.012328317388892174, R2 0.5251260995864868\n",
      "epoch 8650, loss 0.01233584713190794, R2 0.48724764585494995\n",
      "Eval loss 0.012328301556408405, R2 0.5251266956329346\n",
      "epoch 8651, loss 0.012335832230746746, R2 0.4872480630874634\n",
      "Eval loss 0.012328284792602062, R2 0.5251274108886719\n",
      "epoch 8652, loss 0.012335819192230701, R2 0.4872487783432007\n",
      "Eval loss 0.012328268960118294, R2 0.5251278877258301\n",
      "epoch 8653, loss 0.012335805222392082, R2 0.4872491955757141\n",
      "Eval loss 0.0123282540589571, R2 0.5251286625862122\n",
      "epoch 8654, loss 0.012335791252553463, R2 0.4872497320175171\n",
      "Eval loss 0.012328237295150757, R2 0.5251293182373047\n",
      "epoch 8655, loss 0.012335777282714844, R2 0.4872508645057678\n",
      "Eval loss 0.012328221462666988, R2 0.5251299142837524\n",
      "epoch 8656, loss 0.012335763312876225, R2 0.48725175857543945\n",
      "Eval loss 0.012328204698860645, R2 0.5251306295394897\n",
      "epoch 8657, loss 0.012335749343037605, R2 0.48725152015686035\n",
      "Eval loss 0.012328188866376877, R2 0.5251314640045166\n",
      "epoch 8658, loss 0.012335735373198986, R2 0.48725223541259766\n",
      "Eval loss 0.012328172102570534, R2 0.5251319408416748\n",
      "epoch 8659, loss 0.012335722334682941, R2 0.4872526526451111\n",
      "Eval loss 0.01232815533876419, R2 0.5251325964927673\n",
      "epoch 8660, loss 0.012335708364844322, R2 0.48725318908691406\n",
      "Eval loss 0.012328140437602997, R2 0.5251328945159912\n",
      "epoch 8661, loss 0.012335693463683128, R2 0.4872538447380066\n",
      "Eval loss 0.012328122742474079, R2 0.5251333713531494\n",
      "epoch 8662, loss 0.012335680425167084, R2 0.4872550368309021\n",
      "Eval loss 0.01232810690999031, R2 0.5251344442367554\n",
      "epoch 8663, loss 0.012335666455328465, R2 0.487254798412323\n",
      "Eval loss 0.012328091077506542, R2 0.5251350402832031\n",
      "epoch 8664, loss 0.012335652485489845, R2 0.4872555732727051\n",
      "Eval loss 0.012328073382377625, R2 0.5251357555389404\n",
      "epoch 8665, loss 0.012335637584328651, R2 0.4872558116912842\n",
      "Eval loss 0.012328059412539005, R2 0.5251359939575195\n",
      "epoch 8666, loss 0.012335624545812607, R2 0.48725688457489014\n",
      "Eval loss 0.012328041717410088, R2 0.5251367092132568\n",
      "epoch 8667, loss 0.012335611507296562, R2 0.4872572422027588\n",
      "Eval loss 0.012328026816248894, R2 0.5251374244689941\n",
      "epoch 8668, loss 0.012335597537457943, R2 0.4872584342956543\n",
      "Eval loss 0.012328010983765125, R2 0.5251380205154419\n",
      "epoch 8669, loss 0.012335583567619324, R2 0.4872589111328125\n",
      "Eval loss 0.012327994219958782, R2 0.5251384973526001\n",
      "epoch 8670, loss 0.012335570529103279, R2 0.48725974559783936\n",
      "Eval loss 0.012327978387475014, R2 0.5251394510269165\n",
      "epoch 8671, loss 0.01233555655926466, R2 0.4872596859931946\n",
      "Eval loss 0.012327960692346096, R2 0.5251403450965881\n",
      "epoch 8672, loss 0.01233554258942604, R2 0.4872605800628662\n",
      "Eval loss 0.012327945791184902, R2 0.5251402854919434\n",
      "epoch 8673, loss 0.012335528619587421, R2 0.48726117610931396\n",
      "Eval loss 0.012327929958701134, R2 0.5251412987709045\n",
      "epoch 8674, loss 0.012335514649748802, R2 0.48726141452789307\n",
      "Eval loss 0.01232791319489479, R2 0.5251417756080627\n",
      "epoch 8675, loss 0.012335500679910183, R2 0.4872620105743408\n",
      "Eval loss 0.012327898293733597, R2 0.5251426696777344\n",
      "epoch 8676, loss 0.012335487641394138, R2 0.4872622489929199\n",
      "Eval loss 0.012327880598604679, R2 0.5251427292823792\n",
      "epoch 8677, loss 0.012335473671555519, R2 0.4872629642486572\n",
      "Eval loss 0.01232786476612091, R2 0.5251433849334717\n",
      "epoch 8678, loss 0.012335458770394325, R2 0.48726409673690796\n",
      "Eval loss 0.012327848002314568, R2 0.525144100189209\n",
      "epoch 8679, loss 0.01233544573187828, R2 0.48726415634155273\n",
      "Eval loss 0.012327832169830799, R2 0.5251444578170776\n",
      "epoch 8680, loss 0.012335431762039661, R2 0.48726487159729004\n",
      "Eval loss 0.012327817268669605, R2 0.5251453518867493\n",
      "epoch 8681, loss 0.012335417792201042, R2 0.48726528882980347\n",
      "Eval loss 0.012327801436185837, R2 0.5251460671424866\n",
      "epoch 8682, loss 0.012335404753684998, R2 0.487266480922699\n",
      "Eval loss 0.012327785603702068, R2 0.5251468420028687\n",
      "epoch 8683, loss 0.012335390783846378, R2 0.4872671961784363\n",
      "Eval loss 0.01232776790857315, R2 0.5251469612121582\n",
      "epoch 8684, loss 0.012335376814007759, R2 0.48726701736450195\n",
      "Eval loss 0.012327753007411957, R2 0.5251473784446716\n",
      "epoch 8685, loss 0.012335364706814289, R2 0.48726749420166016\n",
      "Eval loss 0.012327737174928188, R2 0.5251485705375671\n",
      "epoch 8686, loss 0.01233535073697567, R2 0.48726820945739746\n",
      "Eval loss 0.01232771947979927, R2 0.5251495242118835\n",
      "epoch 8687, loss 0.012335335835814476, R2 0.48726886510849\n",
      "Eval loss 0.012327704578638077, R2 0.5251498222351074\n",
      "epoch 8688, loss 0.012335323728621006, R2 0.48726922273635864\n",
      "Eval loss 0.012327688746154308, R2 0.5251504182815552\n",
      "epoch 8689, loss 0.012335309758782387, R2 0.48726993799209595\n",
      "Eval loss 0.01232767291367054, R2 0.5251510143280029\n",
      "epoch 8690, loss 0.012335295788943768, R2 0.4872705340385437\n",
      "Eval loss 0.012327657081186771, R2 0.5251513719558716\n",
      "epoch 8691, loss 0.012335281819105148, R2 0.48727160692214966\n",
      "Eval loss 0.012327640317380428, R2 0.5251522660255432\n",
      "epoch 8692, loss 0.012335267849266529, R2 0.48727166652679443\n",
      "Eval loss 0.012327625416219234, R2 0.5251530408859253\n",
      "epoch 8693, loss 0.012335254810750484, R2 0.48727208375930786\n",
      "Eval loss 0.012327608652412891, R2 0.5251533389091492\n",
      "epoch 8694, loss 0.01233524177223444, R2 0.48727279901504517\n",
      "Eval loss 0.012327593751251698, R2 0.5251537561416626\n",
      "epoch 8695, loss 0.012335228733718395, R2 0.48727333545684814\n",
      "Eval loss 0.012327577918767929, R2 0.5251548290252686\n",
      "epoch 8696, loss 0.012335213832557201, R2 0.4872737526893616\n",
      "Eval loss 0.012327561154961586, R2 0.5251554250717163\n",
      "epoch 8697, loss 0.012335199862718582, R2 0.48727482557296753\n",
      "Eval loss 0.012327545322477818, R2 0.5251559615135193\n",
      "epoch 8698, loss 0.012335185892879963, R2 0.4872749447822571\n",
      "Eval loss 0.012327528558671474, R2 0.5251566171646118\n",
      "epoch 8699, loss 0.012335173785686493, R2 0.4872754216194153\n",
      "Eval loss 0.01232751365751028, R2 0.5251573324203491\n",
      "epoch 8700, loss 0.012335158884525299, R2 0.48727667331695557\n",
      "Eval loss 0.012327497825026512, R2 0.5251578092575073\n",
      "epoch 8701, loss 0.012335146777331829, R2 0.487276554107666\n",
      "Eval loss 0.012327481992542744, R2 0.5251581072807312\n",
      "epoch 8702, loss 0.01233513280749321, R2 0.48727715015411377\n",
      "Eval loss 0.012327466160058975, R2 0.5251590013504028\n",
      "epoch 8703, loss 0.012335119768977165, R2 0.48727768659591675\n",
      "Eval loss 0.012327449396252632, R2 0.5251593589782715\n",
      "epoch 8704, loss 0.012335105799138546, R2 0.4872781038284302\n",
      "Eval loss 0.012327434495091438, R2 0.5251599550247192\n",
      "epoch 8705, loss 0.012335093691945076, R2 0.4872787594795227\n",
      "Eval loss 0.01232741866260767, R2 0.5251611471176147\n",
      "epoch 8706, loss 0.012335078790783882, R2 0.48727983236312866\n",
      "Eval loss 0.012327401898801327, R2 0.5251613259315491\n",
      "epoch 8707, loss 0.012335065752267838, R2 0.4872802495956421\n",
      "Eval loss 0.012327386997640133, R2 0.5251620411872864\n",
      "epoch 8708, loss 0.012335052713751793, R2 0.4872804880142212\n",
      "Eval loss 0.012327371165156364, R2 0.5251628160476685\n",
      "epoch 8709, loss 0.012335038743913174, R2 0.48728102445602417\n",
      "Eval loss 0.012327355332672596, R2 0.5251634120941162\n",
      "epoch 8710, loss 0.012335024774074554, R2 0.4872816205024719\n",
      "Eval loss 0.012327339500188828, R2 0.5251637697219849\n",
      "epoch 8711, loss 0.01233501173555851, R2 0.48728233575820923\n",
      "Eval loss 0.012327322736382484, R2 0.5251644849777222\n",
      "epoch 8712, loss 0.01233499776571989, R2 0.48728275299072266\n",
      "Eval loss 0.012327308766543865, R2 0.5251650810241699\n",
      "epoch 8713, loss 0.012334983795881271, R2 0.4872833490371704\n",
      "Eval loss 0.012327291071414948, R2 0.5251657366752625\n",
      "epoch 8714, loss 0.012334971688687801, R2 0.4872838258743286\n",
      "Eval loss 0.012327276170253754, R2 0.5251662731170654\n",
      "epoch 8715, loss 0.012334956787526608, R2 0.48728495836257935\n",
      "Eval loss 0.012327260337769985, R2 0.525166928768158\n",
      "epoch 8716, loss 0.012334944680333138, R2 0.48728495836257935\n",
      "Eval loss 0.012327244505286217, R2 0.5251678228378296\n",
      "epoch 8717, loss 0.012334931641817093, R2 0.4872853755950928\n",
      "Eval loss 0.012327230535447598, R2 0.5251679420471191\n",
      "epoch 8718, loss 0.012334916740655899, R2 0.48728644847869873\n",
      "Eval loss 0.01232721284031868, R2 0.525168776512146\n",
      "epoch 8719, loss 0.01233490277081728, R2 0.48728686571121216\n",
      "Eval loss 0.012327197007834911, R2 0.5251694917678833\n",
      "epoch 8720, loss 0.01233489066362381, R2 0.4872872233390808\n",
      "Eval loss 0.012327182106673717, R2 0.5251699686050415\n",
      "epoch 8721, loss 0.01233487855643034, R2 0.487287700176239\n",
      "Eval loss 0.012327166274189949, R2 0.5251703858375549\n",
      "epoch 8722, loss 0.012334865517914295, R2 0.487288236618042\n",
      "Eval loss 0.012327151373028755, R2 0.525171160697937\n",
      "epoch 8723, loss 0.012334851548075676, R2 0.4872890114784241\n",
      "Eval loss 0.012327133677899837, R2 0.525172233581543\n",
      "epoch 8724, loss 0.012334836646914482, R2 0.48728978633880615\n",
      "Eval loss 0.012327118776738644, R2 0.5251726508140564\n",
      "epoch 8725, loss 0.012334823608398438, R2 0.4872901439666748\n",
      "Eval loss 0.01232710387557745, R2 0.5251730680465698\n",
      "epoch 8726, loss 0.012334811501204967, R2 0.487291157245636\n",
      "Eval loss 0.012327087111771107, R2 0.525173544883728\n",
      "epoch 8727, loss 0.012334797531366348, R2 0.48729103803634644\n",
      "Eval loss 0.012327073141932487, R2 0.5251741409301758\n",
      "epoch 8728, loss 0.012334785424172878, R2 0.4872913956642151\n",
      "Eval loss 0.012327057309448719, R2 0.5251747369766235\n",
      "epoch 8729, loss 0.012334770523011684, R2 0.4872923493385315\n",
      "Eval loss 0.012327040545642376, R2 0.5251752138137817\n",
      "epoch 8730, loss 0.012334756553173065, R2 0.4872927665710449\n",
      "Eval loss 0.012327026575803757, R2 0.525175929069519\n",
      "epoch 8731, loss 0.012334744445979595, R2 0.4872931241989136\n",
      "Eval loss 0.012327009811997414, R2 0.5251767039299011\n",
      "epoch 8732, loss 0.012334732338786125, R2 0.4872937798500061\n",
      "Eval loss 0.01232699304819107, R2 0.5251771211624146\n",
      "epoch 8733, loss 0.012334718368947506, R2 0.48729485273361206\n",
      "Eval loss 0.012326980009675026, R2 0.5251777172088623\n",
      "epoch 8734, loss 0.012334705330431461, R2 0.48729491233825684\n",
      "Eval loss 0.012326962314546108, R2 0.5251782536506653\n",
      "epoch 8735, loss 0.012334692291915417, R2 0.4872954487800598\n",
      "Eval loss 0.012326947413384914, R2 0.5251791477203369\n",
      "epoch 8736, loss 0.012334678322076797, R2 0.4872961640357971\n",
      "Eval loss 0.012326930649578571, R2 0.5251796245574951\n",
      "epoch 8737, loss 0.012334665283560753, R2 0.48729705810546875\n",
      "Eval loss 0.012326916679739952, R2 0.5251801609992981\n",
      "epoch 8738, loss 0.012334652245044708, R2 0.4872971177101135\n",
      "Eval loss 0.012326900847256184, R2 0.5251805186271667\n",
      "epoch 8739, loss 0.012334639206528664, R2 0.48729729652404785\n",
      "Eval loss 0.012326885014772415, R2 0.5251810550689697\n",
      "epoch 8740, loss 0.012334625236690044, R2 0.48729825019836426\n",
      "Eval loss 0.012326871044933796, R2 0.5251818895339966\n",
      "epoch 8741, loss 0.012334612198174, R2 0.48729878664016724\n",
      "Eval loss 0.012326853349804878, R2 0.5251826047897339\n",
      "epoch 8742, loss 0.01233459822833538, R2 0.48729950189590454\n",
      "Eval loss 0.012326839379966259, R2 0.5251836776733398\n",
      "epoch 8743, loss 0.012334585189819336, R2 0.48730039596557617\n",
      "Eval loss 0.012326824478805065, R2 0.5251837372779846\n",
      "epoch 8744, loss 0.012334572151303291, R2 0.4873007535934448\n",
      "Eval loss 0.012326807714998722, R2 0.5251843929290771\n",
      "epoch 8745, loss 0.012334560044109821, R2 0.4873011112213135\n",
      "Eval loss 0.012326791882514954, R2 0.5251849889755249\n",
      "epoch 8746, loss 0.012334547005593777, R2 0.48730164766311646\n",
      "Eval loss 0.012326777912676334, R2 0.5251855254173279\n",
      "epoch 8747, loss 0.012334533967077732, R2 0.4873025417327881\n",
      "Eval loss 0.012326762080192566, R2 0.5251858234405518\n",
      "epoch 8748, loss 0.012334519997239113, R2 0.48730307817459106\n",
      "Eval loss 0.012326746247708797, R2 0.5251867771148682\n",
      "epoch 8749, loss 0.012334506958723068, R2 0.48730313777923584\n",
      "Eval loss 0.012326730415225029, R2 0.5251871943473816\n",
      "epoch 8750, loss 0.012334493920207024, R2 0.4873037338256836\n",
      "Eval loss 0.012326715514063835, R2 0.5251877307891846\n",
      "epoch 8751, loss 0.012334479950368404, R2 0.4873049259185791\n",
      "Eval loss 0.012326700612902641, R2 0.5251883268356323\n",
      "epoch 8752, loss 0.01233446691185236, R2 0.48730480670928955\n",
      "Eval loss 0.012326683849096298, R2 0.5251889824867249\n",
      "epoch 8753, loss 0.01233445480465889, R2 0.48730534315109253\n",
      "Eval loss 0.012326668947935104, R2 0.5251898765563965\n",
      "epoch 8754, loss 0.01233444083482027, R2 0.4873057007789612\n",
      "Eval loss 0.012326653115451336, R2 0.5251902937889099\n",
      "epoch 8755, loss 0.0123344287276268, R2 0.4873065948486328\n",
      "Eval loss 0.012326638214290142, R2 0.5251905918121338\n",
      "epoch 8756, loss 0.012334415689110756, R2 0.4873071312904358\n",
      "Eval loss 0.012326624244451523, R2 0.5251915454864502\n",
      "epoch 8757, loss 0.012334400787949562, R2 0.48730772733688354\n",
      "Eval loss 0.01232660748064518, R2 0.5251920223236084\n",
      "epoch 8758, loss 0.012334388680756092, R2 0.4873080849647522\n",
      "Eval loss 0.012326591648161411, R2 0.5251929759979248\n",
      "epoch 8759, loss 0.012334376573562622, R2 0.48730939626693726\n",
      "Eval loss 0.012326576747000217, R2 0.5251932144165039\n",
      "epoch 8760, loss 0.012334362603724003, R2 0.4873092770576477\n",
      "Eval loss 0.012326560914516449, R2 0.5251938700675964\n",
      "epoch 8761, loss 0.012334349565207958, R2 0.48730987310409546\n",
      "Eval loss 0.01232654694467783, R2 0.5251942873001099\n",
      "epoch 8762, loss 0.012334336526691914, R2 0.4873102307319641\n",
      "Eval loss 0.012326530180871487, R2 0.5251954793930054\n",
      "epoch 8763, loss 0.012334323488175869, R2 0.4873109459877014\n",
      "Eval loss 0.012326515279710293, R2 0.5251955986022949\n",
      "epoch 8764, loss 0.012334310449659824, R2 0.48731130361557007\n",
      "Eval loss 0.012326500378549099, R2 0.525195837020874\n",
      "epoch 8765, loss 0.01233429741114378, R2 0.487312376499176\n",
      "Eval loss 0.01232648454606533, R2 0.5251965522766113\n",
      "epoch 8766, loss 0.01233428530395031, R2 0.487312376499176\n",
      "Eval loss 0.012326468713581562, R2 0.5251973867416382\n",
      "epoch 8767, loss 0.012334272265434265, R2 0.487312912940979\n",
      "Eval loss 0.012326453812420368, R2 0.5251976847648621\n",
      "epoch 8768, loss 0.01233425922691822, R2 0.487313449382782\n",
      "Eval loss 0.012326438911259174, R2 0.5251985788345337\n",
      "epoch 8769, loss 0.012334245257079601, R2 0.4873146414756775\n",
      "Eval loss 0.01232642401009798, R2 0.5251991748809814\n",
      "epoch 8770, loss 0.012334232218563557, R2 0.4873145818710327\n",
      "Eval loss 0.012326407246291637, R2 0.5251997709274292\n",
      "epoch 8771, loss 0.012334220111370087, R2 0.48731493949890137\n",
      "Eval loss 0.012326393276453018, R2 0.5252001881599426\n",
      "epoch 8772, loss 0.012334207072854042, R2 0.4873160719871521\n",
      "Eval loss 0.01232637744396925, R2 0.5252009034156799\n",
      "epoch 8773, loss 0.012334194034337997, R2 0.4873161315917969\n",
      "Eval loss 0.012326362542808056, R2 0.5252014994621277\n",
      "epoch 8774, loss 0.012334181927144527, R2 0.4873173236846924\n",
      "Eval loss 0.012326348572969437, R2 0.5252020359039307\n",
      "epoch 8775, loss 0.012334169819951057, R2 0.48731720447540283\n",
      "Eval loss 0.012326331809163094, R2 0.5252022743225098\n",
      "epoch 8776, loss 0.012334154918789864, R2 0.48731762170791626\n",
      "Eval loss 0.012326317839324474, R2 0.5252032279968262\n",
      "epoch 8777, loss 0.012334142811596394, R2 0.4873184561729431\n",
      "Eval loss 0.01232630293816328, R2 0.5252037048339844\n",
      "epoch 8778, loss 0.012334129773080349, R2 0.48731887340545654\n",
      "Eval loss 0.012326287105679512, R2 0.5252041220664978\n",
      "epoch 8779, loss 0.01233411580324173, R2 0.4873194098472595\n",
      "Eval loss 0.012326271273195744, R2 0.5252050161361694\n",
      "epoch 8780, loss 0.01233410369604826, R2 0.4873199462890625\n",
      "Eval loss 0.01232625637203455, R2 0.5252054333686829\n",
      "epoch 8781, loss 0.01233409158885479, R2 0.4873202443122864\n",
      "Eval loss 0.012326240539550781, R2 0.5252059698104858\n",
      "epoch 8782, loss 0.012334076687693596, R2 0.48732084035873413\n",
      "Eval loss 0.012326225638389587, R2 0.5252066850662231\n",
      "epoch 8783, loss 0.0123340655118227, R2 0.48732197284698486\n",
      "Eval loss 0.012326209805905819, R2 0.52520751953125\n",
      "epoch 8784, loss 0.012334052473306656, R2 0.48732203245162964\n",
      "Eval loss 0.0123261958360672, R2 0.5252077579498291\n",
      "epoch 8785, loss 0.012334038503468037, R2 0.4873226284980774\n",
      "Eval loss 0.012326180003583431, R2 0.5252085328102112\n",
      "epoch 8786, loss 0.012334027327597141, R2 0.4873232841491699\n",
      "Eval loss 0.012326166033744812, R2 0.5252090692520142\n",
      "epoch 8787, loss 0.012334013357758522, R2 0.4873243570327759\n",
      "Eval loss 0.012326151132583618, R2 0.5252096652984619\n",
      "epoch 8788, loss 0.012334000319242477, R2 0.4873248338699341\n",
      "Eval loss 0.012326134368777275, R2 0.5252102613449097\n",
      "epoch 8789, loss 0.012333987280726433, R2 0.4873252511024475\n",
      "Eval loss 0.012326120398938656, R2 0.5252105593681335\n",
      "epoch 8790, loss 0.012333977036178112, R2 0.4873250126838684\n",
      "Eval loss 0.012326105497777462, R2 0.5252111554145813\n",
      "epoch 8791, loss 0.012333961203694344, R2 0.48732584714889526\n",
      "Eval loss 0.012326090596616268, R2 0.5252118110656738\n",
      "epoch 8792, loss 0.012333948165178299, R2 0.48732638359069824\n",
      "Eval loss 0.0123260747641325, R2 0.5252125859260559\n",
      "epoch 8793, loss 0.012333937920629978, R2 0.48732680082321167\n",
      "Eval loss 0.012326059862971306, R2 0.5252130031585693\n",
      "epoch 8794, loss 0.012333923019468784, R2 0.4873272776603699\n",
      "Eval loss 0.012326044030487537, R2 0.5252134799957275\n",
      "epoch 8795, loss 0.012333910912275314, R2 0.48732858896255493\n",
      "Eval loss 0.012326029129326344, R2 0.5252140760421753\n",
      "epoch 8796, loss 0.012333898805081844, R2 0.4873284697532654\n",
      "Eval loss 0.012326015159487724, R2 0.5252150297164917\n",
      "epoch 8797, loss 0.0123338857665658, R2 0.4873294234275818\n",
      "Eval loss 0.01232600025832653, R2 0.5252155065536499\n",
      "epoch 8798, loss 0.012333872728049755, R2 0.48732954263687134\n",
      "Eval loss 0.012325985357165337, R2 0.5252159237861633\n",
      "epoch 8799, loss 0.012333860620856285, R2 0.4873303771018982\n",
      "Eval loss 0.012325970456004143, R2 0.5252166986465454\n",
      "epoch 8800, loss 0.01233384758234024, R2 0.4873305559158325\n",
      "Eval loss 0.012325954623520374, R2 0.5252173542976379\n",
      "epoch 8801, loss 0.01233383547514677, R2 0.48733168840408325\n",
      "Eval loss 0.01232593972235918, R2 0.5252177715301514\n",
      "epoch 8802, loss 0.012333822436630726, R2 0.487331748008728\n",
      "Eval loss 0.012325925752520561, R2 0.5252184867858887\n",
      "epoch 8803, loss 0.012333808466792107, R2 0.48733264207839966\n",
      "Eval loss 0.012325908988714218, R2 0.5252188444137573\n",
      "epoch 8804, loss 0.012333796359598637, R2 0.48733270168304443\n",
      "Eval loss 0.012325895018875599, R2 0.5252196788787842\n",
      "epoch 8805, loss 0.012333784252405167, R2 0.48733383417129517\n",
      "Eval loss 0.01232588104903698, R2 0.5252199172973633\n",
      "epoch 8806, loss 0.012333772145211697, R2 0.4873337149620056\n",
      "Eval loss 0.012325865216553211, R2 0.5252206921577454\n",
      "epoch 8807, loss 0.012333759106695652, R2 0.4873342514038086\n",
      "Eval loss 0.012325850315392017, R2 0.5252213478088379\n",
      "epoch 8808, loss 0.012333746068179607, R2 0.48733460903167725\n",
      "Eval loss 0.012325835414230824, R2 0.5252219438552856\n",
      "epoch 8809, loss 0.012333733029663563, R2 0.487335205078125\n",
      "Eval loss 0.01232582051306963, R2 0.5252223610877991\n",
      "epoch 8810, loss 0.012333720922470093, R2 0.48733580112457275\n",
      "Eval loss 0.012325805611908436, R2 0.5252227783203125\n",
      "epoch 8811, loss 0.012333708815276623, R2 0.48733633756637573\n",
      "Eval loss 0.012325790710747242, R2 0.5252238512039185\n",
      "epoch 8812, loss 0.012333696708083153, R2 0.4873368740081787\n",
      "Eval loss 0.012325774878263474, R2 0.5252240896224976\n",
      "epoch 8813, loss 0.012333682738244534, R2 0.487337589263916\n",
      "Eval loss 0.012325760908424854, R2 0.5252246856689453\n",
      "epoch 8814, loss 0.012333669699728489, R2 0.487338125705719\n",
      "Eval loss 0.012325746938586235, R2 0.5252251029014587\n",
      "epoch 8815, loss 0.012333658523857594, R2 0.4873386025428772\n",
      "Eval loss 0.012325730174779892, R2 0.5252255797386169\n",
      "epoch 8816, loss 0.012333644554018974, R2 0.4873388409614563\n",
      "Eval loss 0.012325716204941273, R2 0.5252265930175781\n",
      "epoch 8817, loss 0.012333632446825504, R2 0.48733949661254883\n",
      "Eval loss 0.012325700372457504, R2 0.5252270102500916\n",
      "epoch 8818, loss 0.012333620339632034, R2 0.4873400330543518\n",
      "Eval loss 0.012325686402618885, R2 0.525227427482605\n",
      "epoch 8819, loss 0.012333606369793415, R2 0.4873407483100891\n",
      "Eval loss 0.012325672432780266, R2 0.5252281427383423\n",
      "epoch 8820, loss 0.012333594262599945, R2 0.48734188079833984\n",
      "Eval loss 0.012325656600296497, R2 0.5252283811569214\n",
      "epoch 8821, loss 0.012333582155406475, R2 0.48734211921691895\n",
      "Eval loss 0.012325642630457878, R2 0.5252293348312378\n",
      "epoch 8822, loss 0.01233356911689043, R2 0.48734259605407715\n",
      "Eval loss 0.012325627729296684, R2 0.5252299308776855\n",
      "epoch 8823, loss 0.01233355700969696, R2 0.4873426556587219\n",
      "Eval loss 0.01232561282813549, R2 0.5252304077148438\n",
      "epoch 8824, loss 0.012333543971180916, R2 0.4873431921005249\n",
      "Eval loss 0.012325597926974297, R2 0.5252305269241333\n",
      "epoch 8825, loss 0.012333530932664871, R2 0.48734354972839355\n",
      "Eval loss 0.012325583025813103, R2 0.5252314805984497\n",
      "epoch 8826, loss 0.012333518825471401, R2 0.4873443841934204\n",
      "Eval loss 0.012325567193329334, R2 0.5252319574356079\n",
      "epoch 8827, loss 0.012333507649600506, R2 0.4873448610305786\n",
      "Eval loss 0.012325553223490715, R2 0.5252329707145691\n",
      "epoch 8828, loss 0.012333495542407036, R2 0.4873453974723816\n",
      "Eval loss 0.012325537391006947, R2 0.5252330303192139\n",
      "epoch 8829, loss 0.012333481572568417, R2 0.48734575510025024\n",
      "Eval loss 0.012325523421168327, R2 0.5252338647842407\n",
      "epoch 8830, loss 0.012333469465374947, R2 0.4873465895652771\n",
      "Eval loss 0.012325509451329708, R2 0.5252342224121094\n",
      "epoch 8831, loss 0.012333457358181477, R2 0.48734694719314575\n",
      "Eval loss 0.01232549361884594, R2 0.525235116481781\n",
      "epoch 8832, loss 0.012333443388342857, R2 0.4873473644256592\n",
      "Eval loss 0.01232547964900732, R2 0.5252357721328735\n",
      "epoch 8833, loss 0.012333431281149387, R2 0.4873480200767517\n",
      "Eval loss 0.012325463816523552, R2 0.5252358317375183\n",
      "epoch 8834, loss 0.012333419173955917, R2 0.48734837770462036\n",
      "Eval loss 0.012325449846684933, R2 0.5252367854118347\n",
      "epoch 8835, loss 0.012333407066762447, R2 0.4873490333557129\n",
      "Eval loss 0.012325434945523739, R2 0.525236964225769\n",
      "epoch 8836, loss 0.012333394959568977, R2 0.48734939098358154\n",
      "Eval loss 0.01232542097568512, R2 0.5252374410629272\n",
      "epoch 8837, loss 0.012333382852375507, R2 0.4873504042625427\n",
      "Eval loss 0.0123254070058465, R2 0.5252382755279541\n",
      "epoch 8838, loss 0.012333367951214314, R2 0.48735082149505615\n",
      "Eval loss 0.012325392104685307, R2 0.5252391695976257\n",
      "epoch 8839, loss 0.012333356775343418, R2 0.48735129833221436\n",
      "Eval loss 0.012325376272201538, R2 0.5252394676208496\n",
      "epoch 8840, loss 0.012333343736827374, R2 0.48735183477401733\n",
      "Eval loss 0.012325361371040344, R2 0.5252400636672974\n",
      "epoch 8841, loss 0.012333330698311329, R2 0.48735201358795166\n",
      "Eval loss 0.012325347401201725, R2 0.5252403616905212\n",
      "epoch 8842, loss 0.012333319522440434, R2 0.48735296726226807\n",
      "Eval loss 0.012325333431363106, R2 0.5252412557601929\n",
      "epoch 8843, loss 0.012333308346569538, R2 0.48735296726226807\n",
      "Eval loss 0.012325318530201912, R2 0.5252414345741272\n",
      "epoch 8844, loss 0.012333294376730919, R2 0.4873535633087158\n",
      "Eval loss 0.012325303629040718, R2 0.5252425670623779\n",
      "epoch 8845, loss 0.012333282269537449, R2 0.48735421895980835\n",
      "Eval loss 0.012325288727879524, R2 0.5252426862716675\n",
      "epoch 8846, loss 0.012333270162343979, R2 0.48735475540161133\n",
      "Eval loss 0.01232527382671833, R2 0.5252434611320496\n",
      "epoch 8847, loss 0.012333258055150509, R2 0.4873550534248352\n",
      "Eval loss 0.012325259856879711, R2 0.5252443552017212\n",
      "epoch 8848, loss 0.012333245947957039, R2 0.48735541105270386\n",
      "Eval loss 0.012325245887041092, R2 0.5252448320388794\n",
      "epoch 8849, loss 0.012333232909440994, R2 0.48735612630844116\n",
      "Eval loss 0.012325231917202473, R2 0.5252450704574585\n",
      "epoch 8850, loss 0.012333220802247524, R2 0.4873567819595337\n",
      "Eval loss 0.01232521515339613, R2 0.5252455472946167\n",
      "epoch 8851, loss 0.012333208695054054, R2 0.48735707998275757\n",
      "Eval loss 0.012325202114880085, R2 0.5252463817596436\n",
      "epoch 8852, loss 0.01233319565653801, R2 0.4873576760292053\n",
      "Eval loss 0.012325185351073742, R2 0.5252466797828674\n",
      "epoch 8853, loss 0.012333184480667114, R2 0.4873581528663635\n",
      "Eval loss 0.012325173243880272, R2 0.52524733543396\n",
      "epoch 8854, loss 0.01233317144215107, R2 0.48735880851745605\n",
      "Eval loss 0.012325157411396503, R2 0.5252479314804077\n",
      "epoch 8855, loss 0.012333158403635025, R2 0.48735934495925903\n",
      "Eval loss 0.012325143441557884, R2 0.5252485871315002\n",
      "epoch 8856, loss 0.01233314722776413, R2 0.48735952377319336\n",
      "Eval loss 0.012325129471719265, R2 0.5252490043640137\n",
      "epoch 8857, loss 0.012333134189248085, R2 0.4873601794242859\n",
      "Eval loss 0.012325115501880646, R2 0.5252498388290405\n",
      "epoch 8858, loss 0.012333122082054615, R2 0.48736053705215454\n",
      "Eval loss 0.012325099669396877, R2 0.5252499580383301\n",
      "epoch 8859, loss 0.01233310904353857, R2 0.48736172914505005\n",
      "Eval loss 0.012325085699558258, R2 0.5252506732940674\n",
      "epoch 8860, loss 0.012333097867667675, R2 0.4873619079589844\n",
      "Eval loss 0.012325070798397064, R2 0.5252512693405151\n",
      "epoch 8861, loss 0.012333083897829056, R2 0.487362802028656\n",
      "Eval loss 0.01232505589723587, R2 0.5252518653869629\n",
      "epoch 8862, loss 0.012333073653280735, R2 0.48736274242401123\n",
      "Eval loss 0.012325041927397251, R2 0.525252103805542\n",
      "epoch 8863, loss 0.01233306061476469, R2 0.4873637557029724\n",
      "Eval loss 0.012325027026236057, R2 0.525252640247345\n",
      "epoch 8864, loss 0.012333049438893795, R2 0.4873642325401306\n",
      "Eval loss 0.012325013056397438, R2 0.525253176689148\n",
      "epoch 8865, loss 0.01233303640037775, R2 0.48736459016799927\n",
      "Eval loss 0.012324999086558819, R2 0.5252541303634644\n",
      "epoch 8866, loss 0.012333023361861706, R2 0.4873650074005127\n",
      "Eval loss 0.012324984185397625, R2 0.5252546072006226\n",
      "epoch 8867, loss 0.012333011254668236, R2 0.4873653054237366\n",
      "Eval loss 0.012324970215559006, R2 0.5252548456192017\n",
      "epoch 8868, loss 0.012332998216152191, R2 0.48736637830734253\n",
      "Eval loss 0.012324954383075237, R2 0.5252554416656494\n",
      "epoch 8869, loss 0.012332987040281296, R2 0.4873664975166321\n",
      "Eval loss 0.012324941344559193, R2 0.5252563953399658\n",
      "epoch 8870, loss 0.012332976795732975, R2 0.48736679553985596\n",
      "Eval loss 0.012324927374720573, R2 0.5252571105957031\n",
      "epoch 8871, loss 0.012332961894571781, R2 0.4873673915863037\n",
      "Eval loss 0.012324913404881954, R2 0.5252573490142822\n",
      "epoch 8872, loss 0.01233295165002346, R2 0.4873676300048828\n",
      "Eval loss 0.012324897572398186, R2 0.5252580642700195\n",
      "epoch 8873, loss 0.01233293954282999, R2 0.487368643283844\n",
      "Eval loss 0.012324882671236992, R2 0.5252580642700195\n",
      "epoch 8874, loss 0.012332925572991371, R2 0.4873695373535156\n",
      "Eval loss 0.012324868701398373, R2 0.5252590179443359\n",
      "epoch 8875, loss 0.012332913465797901, R2 0.48736971616744995\n",
      "Eval loss 0.012324854731559753, R2 0.5252594351768494\n",
      "epoch 8876, loss 0.01233290322124958, R2 0.48737001419067383\n",
      "Eval loss 0.012324840761721134, R2 0.5252605676651001\n",
      "epoch 8877, loss 0.012332890182733536, R2 0.48737049102783203\n",
      "Eval loss 0.012324824929237366, R2 0.5252605676651001\n",
      "epoch 8878, loss 0.012332878075540066, R2 0.4873708486557007\n",
      "Eval loss 0.012324810959398746, R2 0.525261402130127\n",
      "epoch 8879, loss 0.01233286689966917, R2 0.4873713254928589\n",
      "Eval loss 0.012324796989560127, R2 0.5252618193626404\n",
      "epoch 8880, loss 0.012332852929830551, R2 0.4873717427253723\n",
      "Eval loss 0.012324783019721508, R2 0.5252624750137329\n",
      "epoch 8881, loss 0.012332841753959656, R2 0.4873725175857544\n",
      "Eval loss 0.012324768118560314, R2 0.5252626538276672\n",
      "epoch 8882, loss 0.01233283057808876, R2 0.4873736500740051\n",
      "Eval loss 0.01232475508004427, R2 0.5252634286880493\n",
      "epoch 8883, loss 0.012332817539572716, R2 0.48737388849258423\n",
      "Eval loss 0.012324739247560501, R2 0.5252642631530762\n",
      "epoch 8884, loss 0.012332805432379246, R2 0.48737454414367676\n",
      "Eval loss 0.012324726209044456, R2 0.5252648591995239\n",
      "epoch 8885, loss 0.012332793325185776, R2 0.48737454414367676\n",
      "Eval loss 0.012324713170528412, R2 0.5252649188041687\n",
      "epoch 8886, loss 0.012332781217992306, R2 0.4873749017715454\n",
      "Eval loss 0.012324697338044643, R2 0.525265634059906\n",
      "epoch 8887, loss 0.012332769110798836, R2 0.48737603425979614\n",
      "Eval loss 0.01232468243688345, R2 0.5252662301063538\n",
      "epoch 8888, loss 0.012332756072282791, R2 0.4873765707015991\n",
      "Eval loss 0.01232466846704483, R2 0.5252667665481567\n",
      "epoch 8889, loss 0.012332744896411896, R2 0.4873763918876648\n",
      "Eval loss 0.012324653565883636, R2 0.525267481803894\n",
      "epoch 8890, loss 0.012332731857895851, R2 0.4873769283294678\n",
      "Eval loss 0.012324640527367592, R2 0.5252678394317627\n",
      "epoch 8891, loss 0.012332720682024956, R2 0.487377405166626\n",
      "Eval loss 0.012324625626206398, R2 0.5252679586410522\n",
      "epoch 8892, loss 0.012332708574831486, R2 0.48737841844558716\n",
      "Eval loss 0.012324610725045204, R2 0.5252691507339478\n",
      "epoch 8893, loss 0.012332696467638016, R2 0.48737841844558716\n",
      "Eval loss 0.01232459768652916, R2 0.5252697467803955\n",
      "epoch 8894, loss 0.012332684360444546, R2 0.48737937211990356\n",
      "Eval loss 0.012324584648013115, R2 0.5252702236175537\n",
      "epoch 8895, loss 0.012332672253251076, R2 0.48737943172454834\n",
      "Eval loss 0.012324568815529346, R2 0.5252705812454224\n",
      "epoch 8896, loss 0.01233266107738018, R2 0.48738008737564087\n",
      "Eval loss 0.012324554845690727, R2 0.5252707004547119\n",
      "epoch 8897, loss 0.01233264897018671, R2 0.48738038539886475\n",
      "Eval loss 0.012324539944529533, R2 0.5252714157104492\n",
      "epoch 8898, loss 0.01233263686299324, R2 0.48738086223602295\n",
      "Eval loss 0.012324525974690914, R2 0.525272011756897\n",
      "epoch 8899, loss 0.012332625687122345, R2 0.4873815178871155\n",
      "Eval loss 0.01232451293617487, R2 0.5252727270126343\n",
      "epoch 8900, loss 0.0123326126486063, R2 0.48738253116607666\n",
      "Eval loss 0.012324497103691101, R2 0.5252735018730164\n",
      "epoch 8901, loss 0.012332599610090256, R2 0.48738229274749756\n",
      "Eval loss 0.012324484065175056, R2 0.5252735614776611\n",
      "epoch 8902, loss 0.012332589365541935, R2 0.4873833656311035\n",
      "Eval loss 0.012324470095336437, R2 0.525274395942688\n",
      "epoch 8903, loss 0.01233257632702589, R2 0.4873833656311035\n",
      "Eval loss 0.012324455194175243, R2 0.5252749919891357\n",
      "epoch 8904, loss 0.012332565151154995, R2 0.4873846769332886\n",
      "Eval loss 0.01232444029301405, R2 0.5252755880355835\n",
      "epoch 8905, loss 0.0123325539752841, R2 0.48738402128219604\n",
      "Eval loss 0.01232442632317543, R2 0.5252760648727417\n",
      "epoch 8906, loss 0.012332540936768055, R2 0.4873849153518677\n",
      "Eval loss 0.012324413284659386, R2 0.5252768993377686\n",
      "epoch 8907, loss 0.012332528829574585, R2 0.4873858690261841\n",
      "Eval loss 0.012324398383498192, R2 0.5252774953842163\n",
      "epoch 8908, loss 0.012332516722381115, R2 0.4873860478401184\n",
      "Eval loss 0.012324385344982147, R2 0.5252777934074402\n",
      "epoch 8909, loss 0.012332506477832794, R2 0.4873863458633423\n",
      "Eval loss 0.012324371375143528, R2 0.5252782106399536\n",
      "epoch 8910, loss 0.012332492507994175, R2 0.48738688230514526\n",
      "Eval loss 0.012324357405304909, R2 0.5252790451049805\n",
      "epoch 8911, loss 0.01233248133212328, R2 0.48738735914230347\n",
      "Eval loss 0.01232434157282114, R2 0.5252790451049805\n",
      "epoch 8912, loss 0.012332470156252384, R2 0.48738783597946167\n",
      "Eval loss 0.012324328534305096, R2 0.5252798795700073\n",
      "epoch 8913, loss 0.01233245711773634, R2 0.4873883128166199\n",
      "Eval loss 0.012324315495789051, R2 0.5252803564071655\n",
      "epoch 8914, loss 0.012332446873188019, R2 0.4873887896537781\n",
      "Eval loss 0.012324300594627857, R2 0.5252809524536133\n",
      "epoch 8915, loss 0.012332434765994549, R2 0.4873892664909363\n",
      "Eval loss 0.012324286624789238, R2 0.5252813100814819\n",
      "epoch 8916, loss 0.012332422658801079, R2 0.48739027976989746\n",
      "Eval loss 0.012324271723628044, R2 0.5252820253372192\n",
      "epoch 8917, loss 0.012332410551607609, R2 0.48739027976989746\n",
      "Eval loss 0.012324257753789425, R2 0.525282621383667\n",
      "epoch 8918, loss 0.012332398444414139, R2 0.48739081621170044\n",
      "Eval loss 0.01232424471527338, R2 0.5252829790115356\n",
      "epoch 8919, loss 0.012332387268543243, R2 0.48739176988601685\n",
      "Eval loss 0.012324230745434761, R2 0.5252835750579834\n",
      "epoch 8920, loss 0.012332374230027199, R2 0.4873921275138855\n",
      "Eval loss 0.012324215844273567, R2 0.5252841711044312\n",
      "epoch 8921, loss 0.012332362122833729, R2 0.4873921275138855\n",
      "Eval loss 0.012324202805757523, R2 0.5252848863601685\n",
      "epoch 8922, loss 0.012332350946962833, R2 0.487392783164978\n",
      "Eval loss 0.012324187904596329, R2 0.525285005569458\n",
      "epoch 8923, loss 0.012332338839769363, R2 0.48739326000213623\n",
      "Eval loss 0.01232417393475771, R2 0.525285542011261\n",
      "epoch 8924, loss 0.012332326732575893, R2 0.4873942732810974\n",
      "Eval loss 0.012324160896241665, R2 0.5252863168716431\n",
      "epoch 8925, loss 0.012332315556704998, R2 0.48739391565322876\n",
      "Eval loss 0.012324145995080471, R2 0.5252870321273804\n",
      "epoch 8926, loss 0.012332305312156677, R2 0.48739469051361084\n",
      "Eval loss 0.012324132956564426, R2 0.5252871513366699\n",
      "epoch 8927, loss 0.012332292273640633, R2 0.487395703792572\n",
      "Eval loss 0.012324118986725807, R2 0.5252881050109863\n",
      "epoch 8928, loss 0.012332280166447163, R2 0.487395703792572\n",
      "Eval loss 0.012324104085564613, R2 0.5252886414527893\n",
      "epoch 8929, loss 0.012332268059253693, R2 0.4873964190483093\n",
      "Eval loss 0.012324091978371143, R2 0.5252887010574341\n",
      "epoch 8930, loss 0.012332255952060223, R2 0.4873967170715332\n",
      "Eval loss 0.012324076145887375, R2 0.5252895951271057\n",
      "epoch 8931, loss 0.012332245707511902, R2 0.48739778995513916\n",
      "Eval loss 0.01232406310737133, R2 0.5252900719642639\n",
      "epoch 8932, loss 0.012332233600318432, R2 0.48739778995513916\n",
      "Eval loss 0.012324049137532711, R2 0.5252904891967773\n",
      "epoch 8933, loss 0.012332222424447536, R2 0.4873979687690735\n",
      "Eval loss 0.012324036099016666, R2 0.5252911448478699\n",
      "epoch 8934, loss 0.012332208454608917, R2 0.4873988628387451\n",
      "Eval loss 0.012324020266532898, R2 0.5252916812896729\n",
      "epoch 8935, loss 0.012332197278738022, R2 0.487399160861969\n",
      "Eval loss 0.012324008159339428, R2 0.5252920389175415\n",
      "epoch 8936, loss 0.012332186102867126, R2 0.48739945888519287\n",
      "Eval loss 0.012323994189500809, R2 0.5252927541732788\n",
      "epoch 8937, loss 0.012332174926996231, R2 0.4874000549316406\n",
      "Eval loss 0.01232398021966219, R2 0.525293231010437\n",
      "epoch 8938, loss 0.012332161888480186, R2 0.4874006509780884\n",
      "Eval loss 0.01232396624982357, R2 0.5252938270568848\n",
      "epoch 8939, loss 0.012332151643931866, R2 0.4874013662338257\n",
      "Eval loss 0.012323952279984951, R2 0.525294303894043\n",
      "epoch 8940, loss 0.012332138605415821, R2 0.48740142583847046\n",
      "Eval loss 0.012323938310146332, R2 0.5252950191497803\n",
      "epoch 8941, loss 0.012332127429544926, R2 0.48740220069885254\n",
      "Eval loss 0.012323924340307713, R2 0.5252952575683594\n",
      "epoch 8942, loss 0.01233211625367403, R2 0.48740315437316895\n",
      "Eval loss 0.012323910370469093, R2 0.5252960920333862\n",
      "epoch 8943, loss 0.01233210414648056, R2 0.4874030351638794\n",
      "Eval loss 0.012323897331953049, R2 0.5252965688705444\n",
      "epoch 8944, loss 0.012332092970609665, R2 0.48740410804748535\n",
      "Eval loss 0.012323882430791855, R2 0.5252970457077026\n",
      "epoch 8945, loss 0.012332080863416195, R2 0.4874039888381958\n",
      "Eval loss 0.01232386939227581, R2 0.5252972841262817\n",
      "epoch 8946, loss 0.012332070618867874, R2 0.48740458488464355\n",
      "Eval loss 0.012323856353759766, R2 0.5252977609634399\n",
      "epoch 8947, loss 0.01233205758035183, R2 0.48740512132644653\n",
      "Eval loss 0.012323841452598572, R2 0.5252985954284668\n",
      "epoch 8948, loss 0.01233204547315836, R2 0.4874054789543152\n",
      "Eval loss 0.012323828414082527, R2 0.525299072265625\n",
      "epoch 8949, loss 0.012332034297287464, R2 0.48740607500076294\n",
      "Eval loss 0.012323813512921333, R2 0.5252999663352966\n",
      "epoch 8950, loss 0.012332023121416569, R2 0.4874064326286316\n",
      "Eval loss 0.012323801405727863, R2 0.5252999067306519\n",
      "epoch 8951, loss 0.012332011014223099, R2 0.48740673065185547\n",
      "Eval loss 0.01232378650456667, R2 0.5253007411956787\n",
      "epoch 8952, loss 0.012331999838352203, R2 0.487407386302948\n",
      "Eval loss 0.012323773466050625, R2 0.5253012180328369\n",
      "epoch 8953, loss 0.012331988662481308, R2 0.4874078631401062\n",
      "Eval loss 0.01232376042753458, R2 0.525301456451416\n",
      "epoch 8954, loss 0.012331976555287838, R2 0.4874086380004883\n",
      "Eval loss 0.012323744595050812, R2 0.5253021717071533\n",
      "epoch 8955, loss 0.012331965379416943, R2 0.4874092936515808\n",
      "Eval loss 0.012323732487857342, R2 0.5253031253814697\n",
      "epoch 8956, loss 0.012331953272223473, R2 0.4874092936515808\n",
      "Eval loss 0.012323718518018723, R2 0.5253034830093384\n",
      "epoch 8957, loss 0.012331941165030003, R2 0.48740994930267334\n",
      "Eval loss 0.012323704548180103, R2 0.5253036022186279\n",
      "epoch 8958, loss 0.012331929989159107, R2 0.48741060495376587\n",
      "Eval loss 0.012323691509664059, R2 0.5253040790557861\n",
      "epoch 8959, loss 0.012331919744610786, R2 0.4874107241630554\n",
      "Eval loss 0.012323678471148014, R2 0.5253055095672607\n",
      "epoch 8960, loss 0.012331907637417316, R2 0.48741137981414795\n",
      "Eval loss 0.012323661707341671, R2 0.5253056883811951\n",
      "epoch 8961, loss 0.012331895530223846, R2 0.4874116778373718\n",
      "Eval loss 0.012323649600148201, R2 0.525305986404419\n",
      "epoch 8962, loss 0.012331884354352951, R2 0.48741263151168823\n",
      "Eval loss 0.012323635630309582, R2 0.5253063440322876\n",
      "epoch 8963, loss 0.012331873178482056, R2 0.48741263151168823\n",
      "Eval loss 0.012323622591793537, R2 0.5253069400787354\n",
      "epoch 8964, loss 0.012331861071288586, R2 0.48741281032562256\n",
      "Eval loss 0.012323608621954918, R2 0.5253075957298279\n",
      "epoch 8965, loss 0.012331848964095116, R2 0.48741382360458374\n",
      "Eval loss 0.012323593720793724, R2 0.5253081321716309\n",
      "epoch 8966, loss 0.012331838719546795, R2 0.48741424083709717\n",
      "Eval loss 0.01232358068227768, R2 0.5253086686134338\n",
      "epoch 8967, loss 0.012331826612353325, R2 0.487415075302124\n",
      "Eval loss 0.01232356857508421, R2 0.5253094434738159\n",
      "epoch 8968, loss 0.01233181543648243, R2 0.4874151945114136\n",
      "Eval loss 0.01232355460524559, R2 0.5253093838691711\n",
      "epoch 8969, loss 0.01233180332928896, R2 0.4874156713485718\n",
      "Eval loss 0.012323539704084396, R2 0.5253102779388428\n",
      "epoch 8970, loss 0.012331792153418064, R2 0.48741650581359863\n",
      "Eval loss 0.012323526665568352, R2 0.5253106355667114\n",
      "epoch 8971, loss 0.012331780977547169, R2 0.48741674423217773\n",
      "Eval loss 0.012323512695729733, R2 0.5253114104270935\n",
      "epoch 8972, loss 0.012331768870353699, R2 0.48741698265075684\n",
      "Eval loss 0.012323498725891113, R2 0.5253115296363831\n",
      "epoch 8973, loss 0.012331756763160229, R2 0.48741763830184937\n",
      "Eval loss 0.012323486618697643, R2 0.5253121852874756\n",
      "epoch 8974, loss 0.012331747449934483, R2 0.48741787672042847\n",
      "Eval loss 0.012323472648859024, R2 0.5253126621246338\n",
      "epoch 8975, loss 0.012331734411418438, R2 0.4874180555343628\n",
      "Eval loss 0.012323458679020405, R2 0.5253133773803711\n",
      "epoch 8976, loss 0.012331723235547543, R2 0.4874187111854553\n",
      "Eval loss 0.01232344564050436, R2 0.5253136157989502\n",
      "epoch 8977, loss 0.012331712059676647, R2 0.4874194860458374\n",
      "Eval loss 0.012323431670665741, R2 0.525314211845398\n",
      "epoch 8978, loss 0.012331699952483177, R2 0.4874199628829956\n",
      "Eval loss 0.012323418632149696, R2 0.5253149271011353\n",
      "epoch 8979, loss 0.012331688776612282, R2 0.48742032051086426\n",
      "Eval loss 0.012323404662311077, R2 0.5253154635429382\n",
      "epoch 8980, loss 0.012331677600741386, R2 0.487420916557312\n",
      "Eval loss 0.012323390692472458, R2 0.5253161191940308\n",
      "epoch 8981, loss 0.012331667356193066, R2 0.4874213933944702\n",
      "Eval loss 0.012323377653956413, R2 0.5253167748451233\n",
      "epoch 8982, loss 0.012331655248999596, R2 0.4874223470687866\n",
      "Eval loss 0.012323363684117794, R2 0.5253173112869263\n",
      "epoch 8983, loss 0.0123316440731287, R2 0.4874221682548523\n",
      "Eval loss 0.01232335064560175, R2 0.5253171920776367\n",
      "epoch 8984, loss 0.012331632897257805, R2 0.4874226450920105\n",
      "Eval loss 0.01232333667576313, R2 0.5253177881240845\n",
      "epoch 8985, loss 0.012331620790064335, R2 0.48742324113845825\n",
      "Eval loss 0.012323323637247086, R2 0.525318443775177\n",
      "epoch 8986, loss 0.012331608682870865, R2 0.4874241352081299\n",
      "Eval loss 0.012323309667408466, R2 0.5253188014030457\n",
      "epoch 8987, loss 0.012331598438322544, R2 0.4874245524406433\n",
      "Eval loss 0.012323296628892422, R2 0.5253194570541382\n",
      "epoch 8988, loss 0.012331587262451649, R2 0.4874250292778015\n",
      "Eval loss 0.012323283590376377, R2 0.5253199934959412\n",
      "epoch 8989, loss 0.012331576086580753, R2 0.48742496967315674\n",
      "Eval loss 0.012323269620537758, R2 0.5253206491470337\n",
      "epoch 8990, loss 0.012331563979387283, R2 0.48742562532424927\n",
      "Eval loss 0.012323255650699139, R2 0.5253208875656128\n",
      "epoch 8991, loss 0.012331553734838963, R2 0.48742592334747314\n",
      "Eval loss 0.012323242612183094, R2 0.5253214240074158\n",
      "epoch 8992, loss 0.012331540696322918, R2 0.4874264597892761\n",
      "Eval loss 0.012323228642344475, R2 0.5253220796585083\n",
      "epoch 8993, loss 0.012331528589129448, R2 0.48742759227752686\n",
      "Eval loss 0.01232321560382843, R2 0.5253227353096008\n",
      "epoch 8994, loss 0.012331519275903702, R2 0.48742735385894775\n",
      "Eval loss 0.01232320349663496, R2 0.525323212146759\n",
      "epoch 8995, loss 0.012331508100032806, R2 0.48742765188217163\n",
      "Eval loss 0.012323189526796341, R2 0.5253234505653381\n",
      "epoch 8996, loss 0.012331495061516762, R2 0.48742836713790894\n",
      "Eval loss 0.012323175556957722, R2 0.5253239870071411\n",
      "epoch 8997, loss 0.012331484816968441, R2 0.48742878437042236\n",
      "Eval loss 0.012323162518441677, R2 0.525324821472168\n",
      "epoch 8998, loss 0.012331473641097546, R2 0.48742926120758057\n",
      "Eval loss 0.012323148548603058, R2 0.5253250598907471\n",
      "epoch 8999, loss 0.01233146246522665, R2 0.48743003606796265\n",
      "Eval loss 0.012323136441409588, R2 0.5253257751464844\n",
      "epoch 9000, loss 0.012331449426710606, R2 0.4874303936958313\n",
      "Eval loss 0.012323123402893543, R2 0.5253260135650635\n",
      "epoch 9001, loss 0.01233144011348486, R2 0.4874306321144104\n",
      "Eval loss 0.01232310850173235, R2 0.5253267288208008\n",
      "epoch 9002, loss 0.012331428937613964, R2 0.4874311089515686\n",
      "Eval loss 0.01232309453189373, R2 0.5253273844718933\n",
      "epoch 9003, loss 0.012331416830420494, R2 0.4874314069747925\n",
      "Eval loss 0.012323080562055111, R2 0.5253281593322754\n",
      "epoch 9004, loss 0.012331406585872173, R2 0.4874318838119507\n",
      "Eval loss 0.012323068454861641, R2 0.5253282785415649\n",
      "epoch 9005, loss 0.012331395410001278, R2 0.48743265867233276\n",
      "Eval loss 0.012323056347668171, R2 0.5253287553787231\n",
      "epoch 9006, loss 0.012331382371485233, R2 0.48743319511413574\n",
      "Eval loss 0.012323042377829552, R2 0.5253294110298157\n",
      "epoch 9007, loss 0.012331373058259487, R2 0.48743391036987305\n",
      "Eval loss 0.012323028407990932, R2 0.5253296494483948\n",
      "epoch 9008, loss 0.012331361882388592, R2 0.48743438720703125\n",
      "Eval loss 0.012323015369474888, R2 0.5253303050994873\n",
      "epoch 9009, loss 0.012331349775195122, R2 0.48743438720703125\n",
      "Eval loss 0.012323001399636269, R2 0.5253311395645142\n",
      "epoch 9010, loss 0.012331337668001652, R2 0.48743486404418945\n",
      "Eval loss 0.012322988361120224, R2 0.5253317356109619\n",
      "epoch 9011, loss 0.012331327423453331, R2 0.4874359369277954\n",
      "Eval loss 0.01232297532260418, R2 0.5253320932388306\n",
      "epoch 9012, loss 0.012331316247582436, R2 0.48743581771850586\n",
      "Eval loss 0.01232296321541071, R2 0.5253325700759888\n",
      "epoch 9013, loss 0.01233130507171154, R2 0.4874362349510193\n",
      "Eval loss 0.012322948314249516, R2 0.5253331661224365\n",
      "epoch 9014, loss 0.012331293895840645, R2 0.4874367117881775\n",
      "Eval loss 0.01232293713837862, R2 0.5253332853317261\n",
      "epoch 9015, loss 0.01233128271996975, R2 0.4874378442764282\n",
      "Eval loss 0.012322923168540001, R2 0.5253340005874634\n",
      "epoch 9016, loss 0.012331272475421429, R2 0.4874376058578491\n",
      "Eval loss 0.012322908267378807, R2 0.5253345966339111\n",
      "epoch 9017, loss 0.012331260368227959, R2 0.4874376058578491\n",
      "Eval loss 0.012322895228862762, R2 0.5253348350524902\n",
      "epoch 9018, loss 0.012331249192357063, R2 0.48743873834609985\n",
      "Eval loss 0.012322881259024143, R2 0.5253357291221619\n",
      "epoch 9019, loss 0.012331238016486168, R2 0.48743951320648193\n",
      "Eval loss 0.012322870083153248, R2 0.5253357887268066\n",
      "epoch 9020, loss 0.012331225909292698, R2 0.48743999004364014\n",
      "Eval loss 0.012322856113314629, R2 0.5253365635871887\n",
      "epoch 9021, loss 0.012331214733421803, R2 0.48744064569473267\n",
      "Eval loss 0.012322844006121159, R2 0.5253372192382812\n",
      "epoch 9022, loss 0.012331204488873482, R2 0.48744040727615356\n",
      "Eval loss 0.01232283003628254, R2 0.5253374576568604\n",
      "epoch 9023, loss 0.012331194244325161, R2 0.4874405264854431\n",
      "Eval loss 0.012322816997766495, R2 0.5253382325172424\n",
      "epoch 9024, loss 0.012331183068454266, R2 0.4874415993690491\n",
      "Eval loss 0.01232280395925045, R2 0.5253382921218872\n",
      "epoch 9025, loss 0.012331170961260796, R2 0.4874418377876282\n",
      "Eval loss 0.012322789058089256, R2 0.5253392457962036\n",
      "epoch 9026, loss 0.01233116164803505, R2 0.4874421954154968\n",
      "Eval loss 0.012322777882218361, R2 0.5253397226333618\n",
      "epoch 9027, loss 0.012331150472164154, R2 0.48744267225265503\n",
      "Eval loss 0.012322762049734592, R2 0.5253404378890991\n",
      "epoch 9028, loss 0.012331136502325535, R2 0.4874432682991028\n",
      "Eval loss 0.012322749942541122, R2 0.5253407955169678\n",
      "epoch 9029, loss 0.012331128120422363, R2 0.4874442219734192\n",
      "Eval loss 0.012322736904025078, R2 0.5253411531448364\n",
      "epoch 9030, loss 0.012331116013228893, R2 0.48744457960128784\n",
      "Eval loss 0.012322725728154182, R2 0.5253415107727051\n",
      "epoch 9031, loss 0.012331105768680573, R2 0.48744499683380127\n",
      "Eval loss 0.012322710826992989, R2 0.5253422856330872\n",
      "epoch 9032, loss 0.012331093661487103, R2 0.48744499683380127\n",
      "Eval loss 0.012322697788476944, R2 0.5253425240516663\n",
      "epoch 9033, loss 0.012331082485616207, R2 0.4874454736709595\n",
      "Eval loss 0.012322685681283474, R2 0.5253431797027588\n",
      "epoch 9034, loss 0.012331073172390461, R2 0.4874458909034729\n",
      "Eval loss 0.01232267078012228, R2 0.5253437161445618\n",
      "epoch 9035, loss 0.012331061065196991, R2 0.4874468445777893\n",
      "Eval loss 0.012322659604251385, R2 0.5253442525863647\n",
      "epoch 9036, loss 0.012331049889326096, R2 0.48744702339172363\n",
      "Eval loss 0.01232264656573534, R2 0.5253443717956543\n",
      "epoch 9037, loss 0.0123310387134552, R2 0.48744750022888184\n",
      "Eval loss 0.012322632595896721, R2 0.5253454446792603\n",
      "epoch 9038, loss 0.01233102660626173, R2 0.4874477982521057\n",
      "Eval loss 0.012322619557380676, R2 0.5253458023071289\n",
      "epoch 9039, loss 0.01233101636171341, R2 0.48744839429855347\n",
      "Eval loss 0.012322606518864632, R2 0.5253463387489319\n",
      "epoch 9040, loss 0.012331005185842514, R2 0.48744869232177734\n",
      "Eval loss 0.012322592549026012, R2 0.5253463983535767\n",
      "epoch 9041, loss 0.012330994941294193, R2 0.4874492883682251\n",
      "Eval loss 0.012322580441832542, R2 0.5253472328186035\n",
      "epoch 9042, loss 0.012330983765423298, R2 0.4874497652053833\n",
      "Eval loss 0.012322567403316498, R2 0.5253477096557617\n",
      "epoch 9043, loss 0.012330973520874977, R2 0.4874504804611206\n",
      "Eval loss 0.012322553433477879, R2 0.5253480672836304\n",
      "epoch 9044, loss 0.012330963276326656, R2 0.4874509572982788\n",
      "Eval loss 0.012322540394961834, R2 0.5253487229347229\n",
      "epoch 9045, loss 0.012330950237810612, R2 0.4874514937400818\n",
      "Eval loss 0.012322528287768364, R2 0.5253491997718811\n",
      "epoch 9046, loss 0.012330940924584866, R2 0.4874510169029236\n",
      "Eval loss 0.01232251524925232, R2 0.5253496766090393\n",
      "epoch 9047, loss 0.012330928817391396, R2 0.4874526858329773\n",
      "Eval loss 0.01232250314205885, R2 0.5253498554229736\n",
      "epoch 9048, loss 0.012330918572843075, R2 0.48745298385620117\n",
      "Eval loss 0.01232248917222023, R2 0.5253504514694214\n",
      "epoch 9049, loss 0.01233090553432703, R2 0.4874533414840698\n",
      "Eval loss 0.012322475202381611, R2 0.5253511071205139\n",
      "epoch 9050, loss 0.012330896221101284, R2 0.4874532222747803\n",
      "Eval loss 0.012322462163865566, R2 0.5253517627716064\n",
      "epoch 9051, loss 0.012330885045230389, R2 0.4874541759490967\n",
      "Eval loss 0.012322450987994671, R2 0.5253521800041199\n",
      "epoch 9052, loss 0.012330874800682068, R2 0.48745399713516235\n",
      "Eval loss 0.012322437018156052, R2 0.5253527164459229\n",
      "epoch 9053, loss 0.012330863624811172, R2 0.48745477199554443\n",
      "Eval loss 0.012322423979640007, R2 0.5253528356552124\n",
      "epoch 9054, loss 0.012330852448940277, R2 0.4874550700187683\n",
      "Eval loss 0.012322410941123962, R2 0.525353729724884\n",
      "epoch 9055, loss 0.012330841273069382, R2 0.48745566606521606\n",
      "Eval loss 0.012322396971285343, R2 0.5253541469573975\n",
      "epoch 9056, loss 0.012330831028521061, R2 0.48745596408843994\n",
      "Eval loss 0.012322384864091873, R2 0.5253547430038452\n",
      "epoch 9057, loss 0.012330819852650166, R2 0.48745638132095337\n",
      "Eval loss 0.012322371825575829, R2 0.5253552198410034\n",
      "epoch 9058, loss 0.01233080867677927, R2 0.4874568581581116\n",
      "Eval loss 0.012322358787059784, R2 0.5253554582595825\n",
      "epoch 9059, loss 0.01233079843223095, R2 0.4874573349952698\n",
      "Eval loss 0.01232234574854374, R2 0.5253562331199646\n",
      "epoch 9060, loss 0.01233078632503748, R2 0.48745793104171753\n",
      "Eval loss 0.012322332710027695, R2 0.5253567695617676\n",
      "epoch 9061, loss 0.012330776080489159, R2 0.4874582290649414\n",
      "Eval loss 0.012322320602834225, R2 0.5253570079803467\n",
      "epoch 9062, loss 0.012330764904618263, R2 0.4874591827392578\n",
      "Eval loss 0.012322306632995605, R2 0.525357723236084\n",
      "epoch 9063, loss 0.012330755591392517, R2 0.4874592423439026\n",
      "Eval loss 0.012322294525802135, R2 0.5253579616546631\n",
      "epoch 9064, loss 0.012330743484199047, R2 0.48745959997177124\n",
      "Eval loss 0.01232228148728609, R2 0.5253584384918213\n",
      "epoch 9065, loss 0.012330732308328152, R2 0.48746055364608765\n",
      "Eval loss 0.01232226938009262, R2 0.525359034538269\n",
      "epoch 9066, loss 0.012330722995102406, R2 0.4874604344367981\n",
      "Eval loss 0.01232225727289915, R2 0.5253595113754272\n",
      "epoch 9067, loss 0.01233071181923151, R2 0.4874609112739563\n",
      "Eval loss 0.012322242371737957, R2 0.5253602266311646\n",
      "epoch 9068, loss 0.01233070157468319, R2 0.4874611496925354\n",
      "Eval loss 0.012322229333221912, R2 0.5253607034683228\n",
      "epoch 9069, loss 0.01233068946748972, R2 0.48746198415756226\n",
      "Eval loss 0.012322216294705868, R2 0.5253613591194153\n",
      "epoch 9070, loss 0.012330679222941399, R2 0.48746228218078613\n",
      "Eval loss 0.012322204187512398, R2 0.5253615379333496\n",
      "epoch 9071, loss 0.012330668047070503, R2 0.48746317625045776\n",
      "Eval loss 0.012322190217673779, R2 0.5253620743751526\n",
      "epoch 9072, loss 0.012330657802522182, R2 0.48746365308761597\n",
      "Eval loss 0.012322179973125458, R2 0.5253626108169556\n",
      "epoch 9073, loss 0.012330646626651287, R2 0.4874635934829712\n",
      "Eval loss 0.012322165071964264, R2 0.5253632068634033\n",
      "epoch 9074, loss 0.012330637313425541, R2 0.4874644875526428\n",
      "Eval loss 0.012322152964770794, R2 0.5253636837005615\n",
      "epoch 9075, loss 0.012330624274909496, R2 0.48746466636657715\n",
      "Eval loss 0.012322140857577324, R2 0.5253641605377197\n",
      "epoch 9076, loss 0.01233061496168375, R2 0.4874647855758667\n",
      "Eval loss 0.012322126887738705, R2 0.5253645181655884\n",
      "epoch 9077, loss 0.01233060471713543, R2 0.487465500831604\n",
      "Eval loss 0.012322114780545235, R2 0.5253649950027466\n",
      "epoch 9078, loss 0.012330593541264534, R2 0.4874659776687622\n",
      "Eval loss 0.012322102673351765, R2 0.5253655910491943\n",
      "epoch 9079, loss 0.012330582365393639, R2 0.4874667525291443\n",
      "Eval loss 0.012322088703513145, R2 0.5253660082817078\n",
      "epoch 9080, loss 0.012330572120845318, R2 0.4874666929244995\n",
      "Eval loss 0.0123220756649971, R2 0.5253666639328003\n",
      "epoch 9081, loss 0.012330560944974422, R2 0.48746687173843384\n",
      "Eval loss 0.01232206355780363, R2 0.525367259979248\n",
      "epoch 9082, loss 0.012330549769103527, R2 0.4874681234359741\n",
      "Eval loss 0.01232205145061016, R2 0.5253676176071167\n",
      "epoch 9083, loss 0.012330539524555206, R2 0.48746854066848755\n",
      "Eval loss 0.012322037480771542, R2 0.5253680944442749\n",
      "epoch 9084, loss 0.012330528348684311, R2 0.4874683618545532\n",
      "Eval loss 0.012322025373578072, R2 0.5253686904907227\n",
      "epoch 9085, loss 0.012330517172813416, R2 0.48746901750564575\n",
      "Eval loss 0.012322014197707176, R2 0.5253691673278809\n",
      "epoch 9086, loss 0.01233050785958767, R2 0.48746955394744873\n",
      "Eval loss 0.012321999296545982, R2 0.5253695249557495\n",
      "epoch 9087, loss 0.012330497615039349, R2 0.4874701499938965\n",
      "Eval loss 0.012321986258029938, R2 0.5253701210021973\n",
      "epoch 9088, loss 0.012330486439168453, R2 0.48747092485427856\n",
      "Eval loss 0.012321975082159042, R2 0.5253704786300659\n",
      "epoch 9089, loss 0.012330476194620132, R2 0.487471342086792\n",
      "Eval loss 0.012321961112320423, R2 0.5253711938858032\n",
      "epoch 9090, loss 0.012330465018749237, R2 0.48747116327285767\n",
      "Eval loss 0.012321948073804379, R2 0.5253713130950928\n",
      "epoch 9091, loss 0.012330452911555767, R2 0.4874718189239502\n",
      "Eval loss 0.012321936897933483, R2 0.5253720283508301\n",
      "epoch 9092, loss 0.012330443598330021, R2 0.4874720573425293\n",
      "Eval loss 0.012321922928094864, R2 0.5253726243972778\n",
      "epoch 9093, loss 0.012330431491136551, R2 0.4874725341796875\n",
      "Eval loss 0.012321911752223969, R2 0.5253729820251465\n",
      "epoch 9094, loss 0.01233042124658823, R2 0.48747313022613525\n",
      "Eval loss 0.01232189778238535, R2 0.5253734588623047\n",
      "epoch 9095, loss 0.01233041100203991, R2 0.4874732494354248\n",
      "Eval loss 0.01232188567519188, R2 0.5253736972808838\n",
      "epoch 9096, loss 0.012330400757491589, R2 0.48747384548187256\n",
      "Eval loss 0.012321872636675835, R2 0.5253750085830688\n",
      "epoch 9097, loss 0.012330389581620693, R2 0.48747509717941284\n",
      "Eval loss 0.01232185959815979, R2 0.5253751277923584\n",
      "epoch 9098, loss 0.012330379337072372, R2 0.4874747395515442\n",
      "Eval loss 0.01232184749096632, R2 0.5253753662109375\n",
      "epoch 9099, loss 0.012330370023846626, R2 0.48747557401657104\n",
      "Eval loss 0.0123218335211277, R2 0.5253756642341614\n",
      "epoch 9100, loss 0.012330357916653156, R2 0.4874754548072815\n",
      "Eval loss 0.012321822345256805, R2 0.5253760814666748\n",
      "epoch 9101, loss 0.012330345809459686, R2 0.4874765872955322\n",
      "Eval loss 0.012321808375418186, R2 0.5253770351409912\n",
      "epoch 9102, loss 0.01233033649623394, R2 0.48747652769088745\n",
      "Eval loss 0.012321796268224716, R2 0.5253775119781494\n",
      "epoch 9103, loss 0.01233032625168562, R2 0.4874771237373352\n",
      "Eval loss 0.012321783229708672, R2 0.5253780484199524\n",
      "epoch 9104, loss 0.012330316938459873, R2 0.4874773621559143\n",
      "Eval loss 0.012321772053837776, R2 0.5253783464431763\n",
      "epoch 9105, loss 0.012330304831266403, R2 0.4874783158302307\n",
      "Eval loss 0.012321758083999157, R2 0.5253787040710449\n",
      "epoch 9106, loss 0.012330295518040657, R2 0.4874783754348755\n",
      "Eval loss 0.012321745976805687, R2 0.5253793001174927\n",
      "epoch 9107, loss 0.012330285273492336, R2 0.4874786138534546\n",
      "Eval loss 0.012321733869612217, R2 0.5253797769546509\n",
      "epoch 9108, loss 0.012330273166298866, R2 0.4874790906906128\n",
      "Eval loss 0.012321720831096172, R2 0.5253801345825195\n",
      "epoch 9109, loss 0.012330262921750546, R2 0.487479567527771\n",
      "Eval loss 0.012321709655225277, R2 0.5253809690475464\n",
      "epoch 9110, loss 0.01233025174587965, R2 0.48748016357421875\n",
      "Eval loss 0.012321696616709232, R2 0.5253812074661255\n",
      "epoch 9111, loss 0.01233024150133133, R2 0.48748064041137695\n",
      "Eval loss 0.012321683578193188, R2 0.525381863117218\n",
      "epoch 9112, loss 0.012330232188105583, R2 0.4874809980392456\n",
      "Eval loss 0.012321670539677143, R2 0.525382399559021\n",
      "epoch 9113, loss 0.012330221012234688, R2 0.487481951713562\n",
      "Eval loss 0.012321658432483673, R2 0.5253828763961792\n",
      "epoch 9114, loss 0.012330210767686367, R2 0.4874817132949829\n",
      "Eval loss 0.012321646325290203, R2 0.5253831148147583\n",
      "epoch 9115, loss 0.012330200523138046, R2 0.4874821901321411\n",
      "Eval loss 0.012321634218096733, R2 0.5253833532333374\n",
      "epoch 9116, loss 0.012330188415944576, R2 0.4874826669692993\n",
      "Eval loss 0.012321621179580688, R2 0.5253840088844299\n",
      "epoch 9117, loss 0.012330178171396255, R2 0.48748308420181274\n",
      "Eval loss 0.012321608141064644, R2 0.5253844857215881\n",
      "epoch 9118, loss 0.012330167926847935, R2 0.4874839782714844\n",
      "Eval loss 0.012321596965193748, R2 0.5253853797912598\n",
      "epoch 9119, loss 0.012330159544944763, R2 0.487484335899353\n",
      "Eval loss 0.012321584858000278, R2 0.5253856778144836\n",
      "epoch 9120, loss 0.012330147437751293, R2 0.48748451471328735\n",
      "Eval loss 0.01232157088816166, R2 0.5253862142562866\n",
      "epoch 9121, loss 0.012330137193202972, R2 0.4874849319458008\n",
      "Eval loss 0.012321557849645615, R2 0.5253864526748657\n",
      "epoch 9122, loss 0.012330126948654652, R2 0.487485408782959\n",
      "Eval loss 0.012321545742452145, R2 0.5253870487213135\n",
      "epoch 9123, loss 0.012330115772783756, R2 0.48748570680618286\n",
      "Eval loss 0.012321533635258675, R2 0.5253876447677612\n",
      "epoch 9124, loss 0.012330103665590286, R2 0.48748618364334106\n",
      "Eval loss 0.01232152059674263, R2 0.5253880023956299\n",
      "epoch 9125, loss 0.012330095283687115, R2 0.4874865412712097\n",
      "Eval loss 0.012321509420871735, R2 0.5253881812095642\n",
      "epoch 9126, loss 0.01233008410781622, R2 0.48748719692230225\n",
      "Eval loss 0.012321495451033115, R2 0.5253889560699463\n",
      "epoch 9127, loss 0.012330073863267899, R2 0.4874873757362366\n",
      "Eval loss 0.012321483343839645, R2 0.525389552116394\n",
      "epoch 9128, loss 0.012330064550042152, R2 0.48748779296875\n",
      "Eval loss 0.0123214703053236, R2 0.5253899097442627\n",
      "epoch 9129, loss 0.012330052442848682, R2 0.48748844861984253\n",
      "Eval loss 0.012321459129452705, R2 0.5253905057907104\n",
      "epoch 9130, loss 0.012330042198300362, R2 0.4874885678291321\n",
      "Eval loss 0.012321447022259235, R2 0.5253909826278687\n",
      "epoch 9131, loss 0.012330032885074615, R2 0.48748958110809326\n",
      "Eval loss 0.01232143398374319, R2 0.5253913402557373\n",
      "epoch 9132, loss 0.012330022640526295, R2 0.4874895215034485\n",
      "Eval loss 0.012321420945227146, R2 0.5253915786743164\n",
      "epoch 9133, loss 0.012330010533332825, R2 0.48749005794525146\n",
      "Eval loss 0.012321408838033676, R2 0.5253921747207642\n",
      "epoch 9134, loss 0.012330002151429653, R2 0.4874902367591858\n",
      "Eval loss 0.012321395799517632, R2 0.5253925323486328\n",
      "epoch 9135, loss 0.012329991906881332, R2 0.48749130964279175\n",
      "Eval loss 0.012321383692324162, R2 0.5253933072090149\n",
      "epoch 9136, loss 0.012329980731010437, R2 0.48749178647994995\n",
      "Eval loss 0.01232137344777584, R2 0.5253934264183044\n",
      "epoch 9137, loss 0.01232997141778469, R2 0.4874918460845947\n",
      "Eval loss 0.012321359477937222, R2 0.5253939628601074\n",
      "epoch 9138, loss 0.01232996117323637, R2 0.4874921441078186\n",
      "Eval loss 0.012321347370743752, R2 0.5253946781158447\n",
      "epoch 9139, loss 0.012329948134720325, R2 0.48749250173568726\n",
      "Eval loss 0.012321335263550282, R2 0.5253951549530029\n",
      "epoch 9140, loss 0.01232993882149458, R2 0.48749351501464844\n",
      "Eval loss 0.012321323156356812, R2 0.5253953337669373\n",
      "epoch 9141, loss 0.012329929508268833, R2 0.48749345541000366\n",
      "Eval loss 0.012321309186518192, R2 0.525396466255188\n",
      "epoch 9142, loss 0.012329918332397938, R2 0.48749393224716187\n",
      "Eval loss 0.012321298010647297, R2 0.5253968834877014\n",
      "epoch 9143, loss 0.012329908087849617, R2 0.48749446868896484\n",
      "Eval loss 0.012321284972131252, R2 0.5253968238830566\n",
      "epoch 9144, loss 0.012329897843301296, R2 0.48749470710754395\n",
      "Eval loss 0.012321271933615208, R2 0.5253971815109253\n",
      "epoch 9145, loss 0.012329887598752975, R2 0.48749518394470215\n",
      "Eval loss 0.012321260757744312, R2 0.5253980159759521\n",
      "epoch 9146, loss 0.01232987828552723, R2 0.4874955415725708\n",
      "Eval loss 0.012321247719228268, R2 0.5253984928131104\n",
      "epoch 9147, loss 0.012329867109656334, R2 0.487496018409729\n",
      "Eval loss 0.012321236543357372, R2 0.5253989696502686\n",
      "epoch 9148, loss 0.012329855933785439, R2 0.48749715089797974\n",
      "Eval loss 0.012321223504841328, R2 0.5253992080688477\n",
      "epoch 9149, loss 0.012329846620559692, R2 0.4874970316886902\n",
      "Eval loss 0.012321210466325283, R2 0.5254002213478088\n",
      "epoch 9150, loss 0.012329835444688797, R2 0.48749715089797974\n",
      "Eval loss 0.012321199290454388, R2 0.5254002809524536\n",
      "epoch 9151, loss 0.01232982613146305, R2 0.4874976873397827\n",
      "Eval loss 0.012321187183260918, R2 0.5254008769989014\n",
      "epoch 9152, loss 0.01232981588691473, R2 0.48749881982803345\n",
      "Eval loss 0.012321175076067448, R2 0.52540123462677\n",
      "epoch 9153, loss 0.01232980564236641, R2 0.48749858140945435\n",
      "Eval loss 0.012321162037551403, R2 0.5254020690917969\n",
      "epoch 9154, loss 0.012329795397818089, R2 0.4874991178512573\n",
      "Eval loss 0.012321150861680508, R2 0.5254022479057312\n",
      "epoch 9155, loss 0.012329784221947193, R2 0.487499475479126\n",
      "Eval loss 0.012321138754487038, R2 0.5254026055335999\n",
      "epoch 9156, loss 0.012329774908721447, R2 0.4874997138977051\n",
      "Eval loss 0.012321124784648418, R2 0.5254034996032715\n",
      "epoch 9157, loss 0.012329763732850552, R2 0.4875011444091797\n",
      "Eval loss 0.012321112677454948, R2 0.5254034399986267\n",
      "epoch 9158, loss 0.012329754419624805, R2 0.4875006675720215\n",
      "Eval loss 0.012321101501584053, R2 0.5254038572311401\n",
      "epoch 9159, loss 0.01232974324375391, R2 0.487501323223114\n",
      "Eval loss 0.012321087531745434, R2 0.5254048109054565\n",
      "epoch 9160, loss 0.012329733930528164, R2 0.48750221729278564\n",
      "Eval loss 0.012321075424551964, R2 0.5254048705101013\n",
      "epoch 9161, loss 0.012329723685979843, R2 0.48750197887420654\n",
      "Eval loss 0.012321065180003643, R2 0.5254052877426147\n",
      "epoch 9162, loss 0.012329712510108948, R2 0.48750245571136475\n",
      "Eval loss 0.012321051210165024, R2 0.5254059433937073\n",
      "epoch 9163, loss 0.012329702265560627, R2 0.4875028729438782\n",
      "Eval loss 0.012321040034294128, R2 0.5254064798355103\n",
      "epoch 9164, loss 0.01232969295233488, R2 0.4875032305717468\n",
      "Eval loss 0.012321027927100658, R2 0.5254069566726685\n",
      "epoch 9165, loss 0.01232968270778656, R2 0.48750370740890503\n",
      "Eval loss 0.012321015819907188, R2 0.5254077911376953\n",
      "epoch 9166, loss 0.01232967246323824, R2 0.4875047206878662\n",
      "Eval loss 0.012321002781391144, R2 0.5254080891609192\n",
      "epoch 9167, loss 0.012329662218689919, R2 0.4875045418739319\n",
      "Eval loss 0.012320990674197674, R2 0.5254085659980774\n",
      "epoch 9168, loss 0.012329652905464172, R2 0.487504780292511\n",
      "Eval loss 0.012320979498326778, R2 0.5254086256027222\n",
      "epoch 9169, loss 0.012329642660915852, R2 0.48750537633895874\n",
      "Eval loss 0.012320967391133308, R2 0.5254093408584595\n",
      "epoch 9170, loss 0.012329631485044956, R2 0.4875064492225647\n",
      "Eval loss 0.012320954352617264, R2 0.5254096984863281\n",
      "epoch 9171, loss 0.012329621240496635, R2 0.48750609159469604\n",
      "Eval loss 0.012320944108068943, R2 0.5254102349281311\n",
      "epoch 9172, loss 0.01232961192727089, R2 0.48750728368759155\n",
      "Eval loss 0.012320930138230324, R2 0.5254105925559998\n",
      "epoch 9173, loss 0.012329601682722569, R2 0.4875069260597229\n",
      "Eval loss 0.012320918031036854, R2 0.5254113674163818\n",
      "epoch 9174, loss 0.012329590506851673, R2 0.4875076413154602\n",
      "Eval loss 0.012320905923843384, R2 0.5254112482070923\n",
      "epoch 9175, loss 0.012329581193625927, R2 0.4875083565711975\n",
      "Eval loss 0.01232089288532734, R2 0.5254119038581848\n",
      "epoch 9176, loss 0.012329570017755032, R2 0.4875083565711975\n",
      "Eval loss 0.012320881709456444, R2 0.5254123210906982\n",
      "epoch 9177, loss 0.01232956163585186, R2 0.48750919103622437\n",
      "Eval loss 0.012320870533585548, R2 0.5254130363464355\n",
      "epoch 9178, loss 0.01232955139130354, R2 0.4875091314315796\n",
      "Eval loss 0.012320858426392078, R2 0.5254135131835938\n",
      "epoch 9179, loss 0.012329541146755219, R2 0.4875103831291199\n",
      "Eval loss 0.012320845387876034, R2 0.5254137516021729\n",
      "epoch 9180, loss 0.012329530902206898, R2 0.4875100255012512\n",
      "Eval loss 0.012320833280682564, R2 0.5254145860671997\n",
      "epoch 9181, loss 0.012329520657658577, R2 0.48751091957092285\n",
      "Eval loss 0.012320822104811668, R2 0.5254150629043579\n",
      "epoch 9182, loss 0.012329510413110256, R2 0.4875108003616333\n",
      "Eval loss 0.012320809066295624, R2 0.5254154205322266\n",
      "epoch 9183, loss 0.012329500168561935, R2 0.4875110983848572\n",
      "Eval loss 0.012320796959102154, R2 0.5254158973693848\n",
      "epoch 9184, loss 0.012329489924013615, R2 0.4875115156173706\n",
      "Eval loss 0.012320785783231258, R2 0.5254160165786743\n",
      "epoch 9185, loss 0.012329480610787868, R2 0.48751240968704224\n",
      "Eval loss 0.012320773676037788, R2 0.5254166126251221\n",
      "epoch 9186, loss 0.012329469434916973, R2 0.48751300573349\n",
      "Eval loss 0.012320760637521744, R2 0.5254169702529907\n",
      "epoch 9187, loss 0.012329460121691227, R2 0.48751306533813477\n",
      "Eval loss 0.012320749461650848, R2 0.525417685508728\n",
      "epoch 9188, loss 0.012329449877142906, R2 0.48751354217529297\n",
      "Eval loss 0.012320738285779953, R2 0.5254181623458862\n",
      "epoch 9189, loss 0.01232943870127201, R2 0.48751378059387207\n",
      "Eval loss 0.012320724315941334, R2 0.5254186391830444\n",
      "epoch 9190, loss 0.012329429388046265, R2 0.487514853477478\n",
      "Eval loss 0.012320714071393013, R2 0.5254192352294922\n",
      "epoch 9191, loss 0.012329419143497944, R2 0.4875146150588989\n",
      "Eval loss 0.012320701032876968, R2 0.5254197120666504\n",
      "epoch 9192, loss 0.012329410761594772, R2 0.4875149726867676\n",
      "Eval loss 0.012320688925683498, R2 0.5254203081130981\n",
      "epoch 9193, loss 0.012329400517046452, R2 0.4875154495239258\n",
      "Eval loss 0.012320678681135178, R2 0.5254205465316772\n",
      "epoch 9194, loss 0.012329389341175556, R2 0.4875158667564392\n",
      "Eval loss 0.012320664711296558, R2 0.525421142578125\n",
      "epoch 9195, loss 0.01232938002794981, R2 0.48751628398895264\n",
      "Eval loss 0.012320652604103088, R2 0.5254210233688354\n",
      "epoch 9196, loss 0.012329370714724064, R2 0.4875168204307556\n",
      "Eval loss 0.012320641428232193, R2 0.5254218578338623\n",
      "epoch 9197, loss 0.012329360470175743, R2 0.48751693964004517\n",
      "Eval loss 0.012320629321038723, R2 0.525422215461731\n",
      "epoch 9198, loss 0.012329350225627422, R2 0.4875178337097168\n",
      "Eval loss 0.012320619076490402, R2 0.5254229307174683\n",
      "epoch 9199, loss 0.012329339981079102, R2 0.48751795291900635\n",
      "Eval loss 0.012320605106651783, R2 0.5254232883453369\n",
      "epoch 9200, loss 0.01232932973653078, R2 0.48751819133758545\n",
      "Eval loss 0.012320592999458313, R2 0.5254236459732056\n",
      "epoch 9201, loss 0.01232931949198246, R2 0.4875192642211914\n",
      "Eval loss 0.012320582754909992, R2 0.5254241228103638\n",
      "epoch 9202, loss 0.012329310178756714, R2 0.4875193238258362\n",
      "Eval loss 0.012320570647716522, R2 0.5254247188568115\n",
      "epoch 9203, loss 0.012329299934208393, R2 0.4875197410583496\n",
      "Eval loss 0.012320558540523052, R2 0.5254250764846802\n",
      "epoch 9204, loss 0.012329289689660072, R2 0.4875202178955078\n",
      "Eval loss 0.012320546433329582, R2 0.5254252552986145\n",
      "epoch 9205, loss 0.0123292813077569, R2 0.4875202178955078\n",
      "Eval loss 0.012320532463490963, R2 0.5254260301589966\n",
      "epoch 9206, loss 0.012329270131886005, R2 0.48752129077911377\n",
      "Eval loss 0.012320522218942642, R2 0.5254263281822205\n",
      "epoch 9207, loss 0.01232926081866026, R2 0.48752135038375854\n",
      "Eval loss 0.012320510111749172, R2 0.5254266262054443\n",
      "epoch 9208, loss 0.012329250574111938, R2 0.4875221252441406\n",
      "Eval loss 0.012320498004555702, R2 0.5254273414611816\n",
      "epoch 9209, loss 0.012329241260886192, R2 0.48752206563949585\n",
      "Eval loss 0.012320486828684807, R2 0.5254275798797607\n",
      "epoch 9210, loss 0.012329230085015297, R2 0.4875224828720093\n",
      "Eval loss 0.012320474721491337, R2 0.5254281759262085\n",
      "epoch 9211, loss 0.01232922077178955, R2 0.4875229001045227\n",
      "Eval loss 0.012320463545620441, R2 0.5254284739494324\n",
      "epoch 9212, loss 0.01232921052724123, R2 0.48752331733703613\n",
      "Eval loss 0.012320451438426971, R2 0.5254290103912354\n",
      "epoch 9213, loss 0.01232920028269291, R2 0.4875238537788391\n",
      "Eval loss 0.012320439331233501, R2 0.525429368019104\n",
      "epoch 9214, loss 0.012329190038144588, R2 0.4875243306159973\n",
      "Eval loss 0.01232042908668518, R2 0.5254297852516174\n",
      "epoch 9215, loss 0.012329181656241417, R2 0.48752498626708984\n",
      "Eval loss 0.012320415116846561, R2 0.5254306793212891\n",
      "epoch 9216, loss 0.012329171411693096, R2 0.48752492666244507\n",
      "Eval loss 0.012320403940975666, R2 0.5254310369491577\n",
      "epoch 9217, loss 0.01232916209846735, R2 0.4875258207321167\n",
      "Eval loss 0.012320391833782196, R2 0.5254316329956055\n",
      "epoch 9218, loss 0.012329150922596455, R2 0.4875257611274719\n",
      "Eval loss 0.012320378795266151, R2 0.5254319906234741\n",
      "epoch 9219, loss 0.012329140678048134, R2 0.4875262379646301\n",
      "Eval loss 0.012320367619395256, R2 0.525432288646698\n",
      "epoch 9220, loss 0.012329132296144962, R2 0.4875265955924988\n",
      "Eval loss 0.01232035644352436, R2 0.5254330039024353\n",
      "epoch 9221, loss 0.012329122051596642, R2 0.4875274896621704\n",
      "Eval loss 0.012320345267653465, R2 0.5254334211349487\n",
      "epoch 9222, loss 0.012329112738370895, R2 0.48752737045288086\n",
      "Eval loss 0.01232033409178257, R2 0.5254336595535278\n",
      "epoch 9223, loss 0.0123291015625, R2 0.48752784729003906\n",
      "Eval loss 0.012320321053266525, R2 0.5254342555999756\n",
      "epoch 9224, loss 0.012329093180596828, R2 0.4875286817550659\n",
      "Eval loss 0.012320308946073055, R2 0.5254348516464233\n",
      "epoch 9225, loss 0.012329082936048508, R2 0.48752862215042114\n",
      "Eval loss 0.012320296838879585, R2 0.5254352688789368\n",
      "epoch 9226, loss 0.012329072691500187, R2 0.48752903938293457\n",
      "Eval loss 0.01232028566300869, R2 0.5254355669021606\n",
      "epoch 9227, loss 0.01232906337827444, R2 0.48752957582473755\n",
      "Eval loss 0.012320274487137794, R2 0.5254360437393188\n",
      "epoch 9228, loss 0.012329054065048695, R2 0.487529993057251\n",
      "Eval loss 0.012320262379944324, R2 0.5254366397857666\n",
      "epoch 9229, loss 0.012329043820500374, R2 0.4875304102897644\n",
      "Eval loss 0.01232024934142828, R2 0.5254366993904114\n",
      "epoch 9230, loss 0.012329033575952053, R2 0.48753082752227783\n",
      "Eval loss 0.012320239096879959, R2 0.5254371166229248\n",
      "epoch 9231, loss 0.012329023331403732, R2 0.4875316023826599\n",
      "Eval loss 0.012320227921009064, R2 0.5254379510879517\n",
      "epoch 9232, loss 0.01232901494950056, R2 0.4875316023826599\n",
      "Eval loss 0.012320213951170444, R2 0.5254380702972412\n",
      "epoch 9233, loss 0.01232900470495224, R2 0.48753201961517334\n",
      "Eval loss 0.012320203706622124, R2 0.5254386067390442\n",
      "epoch 9234, loss 0.012328993529081345, R2 0.4875323176383972\n",
      "Eval loss 0.012320191599428654, R2 0.5254392027854919\n",
      "epoch 9235, loss 0.012328985147178173, R2 0.4875331521034241\n",
      "Eval loss 0.012320179492235184, R2 0.5254393815994263\n",
      "epoch 9236, loss 0.012328975833952427, R2 0.48753321170806885\n",
      "Eval loss 0.012320167385041714, R2 0.525439977645874\n",
      "epoch 9237, loss 0.01232896652072668, R2 0.48753345012664795\n",
      "Eval loss 0.012320158071815968, R2 0.5254404544830322\n",
      "epoch 9238, loss 0.01232895441353321, R2 0.48753440380096436\n",
      "Eval loss 0.012320145033299923, R2 0.5254408717155457\n",
      "epoch 9239, loss 0.012328945100307465, R2 0.4875343441963196\n",
      "Eval loss 0.012320131994783878, R2 0.5254415273666382\n",
      "epoch 9240, loss 0.012328936718404293, R2 0.48753470182418823\n",
      "Eval loss 0.012320120818912983, R2 0.5254416465759277\n",
      "epoch 9241, loss 0.012328927405178547, R2 0.4875352382659912\n",
      "Eval loss 0.012320110574364662, R2 0.5254424810409546\n",
      "epoch 9242, loss 0.012328916229307652, R2 0.48753535747528076\n",
      "Eval loss 0.012320096604526043, R2 0.5254429578781128\n",
      "epoch 9243, loss 0.012328906916081905, R2 0.48753589391708374\n",
      "Eval loss 0.012320086359977722, R2 0.5254432559013367\n",
      "epoch 9244, loss 0.01232889760285616, R2 0.48753631114959717\n",
      "Eval loss 0.012320074252784252, R2 0.5254437327384949\n",
      "epoch 9245, loss 0.012328888289630413, R2 0.4875372052192688\n",
      "Eval loss 0.012320063076913357, R2 0.5254440307617188\n",
      "epoch 9246, loss 0.012328878045082092, R2 0.48753708600997925\n",
      "Eval loss 0.012320051901042461, R2 0.5254448652267456\n",
      "epoch 9247, loss 0.012328866869211197, R2 0.4875379204750061\n",
      "Eval loss 0.012320040725171566, R2 0.5254448652267456\n",
      "epoch 9248, loss 0.012328858487308025, R2 0.48753827810287476\n",
      "Eval loss 0.012320028617978096, R2 0.5254459381103516\n",
      "epoch 9249, loss 0.01232884917408228, R2 0.48753833770751953\n",
      "Eval loss 0.012320016510784626, R2 0.5254456996917725\n",
      "epoch 9250, loss 0.012328837998211384, R2 0.48753875494003296\n",
      "Eval loss 0.01232000533491373, R2 0.5254460573196411\n",
      "epoch 9251, loss 0.012328829616308212, R2 0.4875391125679016\n",
      "Eval loss 0.012319994159042835, R2 0.5254469513893127\n",
      "epoch 9252, loss 0.012328820303082466, R2 0.48754000663757324\n",
      "Eval loss 0.01231998298317194, R2 0.5254473686218262\n",
      "epoch 9253, loss 0.012328810058534145, R2 0.48753994703292847\n",
      "Eval loss 0.01231997087597847, R2 0.5254477262496948\n",
      "epoch 9254, loss 0.01232879888266325, R2 0.4875409007072449\n",
      "Eval loss 0.012319957837462425, R2 0.525448203086853\n",
      "epoch 9255, loss 0.012328790500760078, R2 0.487540602684021\n",
      "Eval loss 0.012319947592914104, R2 0.5254486203193665\n",
      "epoch 9256, loss 0.012328781187534332, R2 0.4875416159629822\n",
      "Eval loss 0.012319935485720634, R2 0.5254493355751038\n",
      "epoch 9257, loss 0.012328771874308586, R2 0.48754173517227173\n",
      "Eval loss 0.012319924309849739, R2 0.5254492163658142\n",
      "epoch 9258, loss 0.01232876069843769, R2 0.48754245042800903\n",
      "Eval loss 0.012319913133978844, R2 0.5254502296447754\n",
      "epoch 9259, loss 0.01232875045388937, R2 0.4875430464744568\n",
      "Eval loss 0.012319900095462799, R2 0.5254501104354858\n",
      "epoch 9260, loss 0.012328742071986198, R2 0.48754292726516724\n",
      "Eval loss 0.012319889850914478, R2 0.5254507064819336\n",
      "epoch 9261, loss 0.012328733690083027, R2 0.4875428080558777\n",
      "Eval loss 0.012319878675043583, R2 0.5254514217376709\n",
      "epoch 9262, loss 0.012328723445534706, R2 0.4875437021255493\n",
      "Eval loss 0.012319866567850113, R2 0.5254520177841187\n",
      "epoch 9263, loss 0.01232871413230896, R2 0.4875439405441284\n",
      "Eval loss 0.012319854460656643, R2 0.5254523158073425\n",
      "epoch 9264, loss 0.01232870388776064, R2 0.48754435777664185\n",
      "Eval loss 0.012319842353463173, R2 0.5254523754119873\n",
      "epoch 9265, loss 0.012328695505857468, R2 0.4875448942184448\n",
      "Eval loss 0.012319832108914852, R2 0.5254527926445007\n",
      "epoch 9266, loss 0.012328685261309147, R2 0.4875456690788269\n",
      "Eval loss 0.012319820001721382, R2 0.5254536271095276\n",
      "epoch 9267, loss 0.012328674085438251, R2 0.4875463843345642\n",
      "Eval loss 0.012319808825850487, R2 0.5254536867141724\n",
      "epoch 9268, loss 0.01232866570353508, R2 0.48754626512527466\n",
      "Eval loss 0.012319797649979591, R2 0.5254542827606201\n",
      "epoch 9269, loss 0.012328656390309334, R2 0.4875468611717224\n",
      "Eval loss 0.012319786474108696, R2 0.5254548192024231\n",
      "epoch 9270, loss 0.012328646145761013, R2 0.48754680156707764\n",
      "Eval loss 0.012319774366915226, R2 0.525455117225647\n",
      "epoch 9271, loss 0.012328636832535267, R2 0.48754698038101196\n",
      "Eval loss 0.012319762259721756, R2 0.5254554748535156\n",
      "epoch 9272, loss 0.012328628450632095, R2 0.48754751682281494\n",
      "Eval loss 0.012319752015173435, R2 0.5254558324813843\n",
      "epoch 9273, loss 0.012328618206083775, R2 0.48754793405532837\n",
      "Eval loss 0.01231974083930254, R2 0.5254563093185425\n",
      "epoch 9274, loss 0.012328608892858028, R2 0.48754817247390747\n",
      "Eval loss 0.01231972873210907, R2 0.5254573225975037\n",
      "epoch 9275, loss 0.012328598648309708, R2 0.48754870891571045\n",
      "Eval loss 0.012319717556238174, R2 0.5254572033882141\n",
      "epoch 9276, loss 0.012328590266406536, R2 0.4875492453575134\n",
      "Eval loss 0.012319706380367279, R2 0.5254582166671753\n",
      "epoch 9277, loss 0.012328580021858215, R2 0.4875495433807373\n",
      "Eval loss 0.012319694273173809, R2 0.5254583358764648\n",
      "epoch 9278, loss 0.01232857070863247, R2 0.4875497817993164\n",
      "Eval loss 0.012319683097302914, R2 0.525458812713623\n",
      "epoch 9279, loss 0.012328560464084148, R2 0.4875504970550537\n",
      "Eval loss 0.012319671921432018, R2 0.5254594087600708\n",
      "epoch 9280, loss 0.012328550219535828, R2 0.48755139112472534\n",
      "Eval loss 0.012319659814238548, R2 0.5254598259925842\n",
      "epoch 9281, loss 0.012328541837632656, R2 0.4875512719154358\n",
      "Eval loss 0.012319648638367653, R2 0.5254598259925842\n",
      "epoch 9282, loss 0.01232853252440691, R2 0.4875515103340149\n",
      "Eval loss 0.012319637462496758, R2 0.525460422039032\n",
      "epoch 9283, loss 0.01232852227985859, R2 0.48755156993865967\n",
      "Eval loss 0.012319626286625862, R2 0.5254608392715454\n",
      "epoch 9284, loss 0.012328512966632843, R2 0.4875526428222656\n",
      "Eval loss 0.012319614179432392, R2 0.5254613161087036\n",
      "epoch 9285, loss 0.012328503653407097, R2 0.4875528812408447\n",
      "Eval loss 0.012319603003561497, R2 0.5254619121551514\n",
      "epoch 9286, loss 0.01232849434018135, R2 0.4875532388687134\n",
      "Eval loss 0.012319590896368027, R2 0.5254620909690857\n",
      "epoch 9287, loss 0.01232848409563303, R2 0.4875539541244507\n",
      "Eval loss 0.012319579720497131, R2 0.5254628658294678\n",
      "epoch 9288, loss 0.012328475713729858, R2 0.48755383491516113\n",
      "Eval loss 0.012319568544626236, R2 0.5254631042480469\n",
      "epoch 9289, loss 0.012328466400504112, R2 0.48755425214767456\n",
      "Eval loss 0.01231955736875534, R2 0.5254636406898499\n",
      "epoch 9290, loss 0.012328456155955791, R2 0.48755449056625366\n",
      "Eval loss 0.012319546192884445, R2 0.5254641771316528\n",
      "epoch 9291, loss 0.01232844777405262, R2 0.48755520582199097\n",
      "Eval loss 0.01231953501701355, R2 0.5254644751548767\n",
      "epoch 9292, loss 0.012328439392149448, R2 0.4875553250312805\n",
      "Eval loss 0.012319523841142654, R2 0.5254650712013245\n",
      "epoch 9293, loss 0.012328428216278553, R2 0.4875558018684387\n",
      "Eval loss 0.012319513596594334, R2 0.5254653692245483\n",
      "epoch 9294, loss 0.012328418903052807, R2 0.48755621910095215\n",
      "Eval loss 0.012319500558078289, R2 0.5254654884338379\n",
      "epoch 9295, loss 0.012328408658504486, R2 0.4875566363334656\n",
      "Eval loss 0.012319488450884819, R2 0.5254661440849304\n",
      "epoch 9296, loss 0.01232840120792389, R2 0.4875571131706238\n",
      "Eval loss 0.012319478206336498, R2 0.5254664421081543\n",
      "epoch 9297, loss 0.012328390963375568, R2 0.4875573515892029\n",
      "Eval loss 0.012319467961788177, R2 0.5254669189453125\n",
      "epoch 9298, loss 0.012328381650149822, R2 0.4875582456588745\n",
      "Eval loss 0.012319455854594707, R2 0.5254679918289185\n",
      "epoch 9299, loss 0.01232837326824665, R2 0.4875587821006775\n",
      "Eval loss 0.012319442816078663, R2 0.5254682302474976\n",
      "epoch 9300, loss 0.012328362092375755, R2 0.4875582456588745\n",
      "Eval loss 0.012319433502852917, R2 0.5254685878753662\n",
      "epoch 9301, loss 0.01232835277915001, R2 0.48755913972854614\n",
      "Eval loss 0.012319422326982021, R2 0.5254690647125244\n",
      "epoch 9302, loss 0.012328344397246838, R2 0.4875589609146118\n",
      "Eval loss 0.012319410219788551, R2 0.525469183921814\n",
      "epoch 9303, loss 0.012328335084021091, R2 0.48756033182144165\n",
      "Eval loss 0.012319399043917656, R2 0.5254696011543274\n",
      "epoch 9304, loss 0.01232832483947277, R2 0.48756009340286255\n",
      "Eval loss 0.012319388799369335, R2 0.5254698991775513\n",
      "epoch 9305, loss 0.012328315526247025, R2 0.4875609874725342\n",
      "Eval loss 0.01231937576085329, R2 0.5254706144332886\n",
      "epoch 9306, loss 0.012328306213021278, R2 0.48756152391433716\n",
      "Eval loss 0.01231936551630497, R2 0.525471031665802\n",
      "epoch 9307, loss 0.012328296899795532, R2 0.48756128549575806\n",
      "Eval loss 0.012319354340434074, R2 0.5254714488983154\n",
      "epoch 9308, loss 0.012328287586569786, R2 0.4875617027282715\n",
      "Eval loss 0.012319342233240604, R2 0.5254718065261841\n",
      "epoch 9309, loss 0.01232827827334404, R2 0.48756206035614014\n",
      "Eval loss 0.012319331988692284, R2 0.5254722833633423\n",
      "epoch 9310, loss 0.012328268960118294, R2 0.4875624179840088\n",
      "Eval loss 0.012319320812821388, R2 0.5254727602005005\n",
      "epoch 9311, loss 0.012328260578215122, R2 0.48756277561187744\n",
      "Eval loss 0.012319310568273067, R2 0.5254732966423035\n",
      "epoch 9312, loss 0.012328250333666801, R2 0.48756325244903564\n",
      "Eval loss 0.012319297529757023, R2 0.5254737734794617\n",
      "epoch 9313, loss 0.01232824195176363, R2 0.4875643849372864\n",
      "Eval loss 0.012319286353886127, R2 0.5254741907119751\n",
      "epoch 9314, loss 0.01232823170721531, R2 0.4875638484954834\n",
      "Eval loss 0.012319276109337807, R2 0.5254746079444885\n",
      "epoch 9315, loss 0.012328223325312138, R2 0.4875643253326416\n",
      "Eval loss 0.012319264002144337, R2 0.5254746675491333\n",
      "epoch 9316, loss 0.012328214012086391, R2 0.48756492137908936\n",
      "Eval loss 0.012319253757596016, R2 0.5254756212234497\n",
      "epoch 9317, loss 0.012328204698860645, R2 0.4875655770301819\n",
      "Eval loss 0.012319243513047695, R2 0.5254754424095154\n",
      "epoch 9318, loss 0.012328194454312325, R2 0.4875655174255371\n",
      "Eval loss 0.012319231405854225, R2 0.5254759192466736\n",
      "epoch 9319, loss 0.012328186072409153, R2 0.4875655770301819\n",
      "Eval loss 0.012319219298660755, R2 0.525476336479187\n",
      "epoch 9320, loss 0.012328176759183407, R2 0.4875662922859192\n",
      "Eval loss 0.012319209054112434, R2 0.5254770517349243\n",
      "epoch 9321, loss 0.012328168377280235, R2 0.48756664991378784\n",
      "Eval loss 0.012319197878241539, R2 0.5254771709442139\n",
      "epoch 9322, loss 0.01232815720140934, R2 0.4875672459602356\n",
      "Eval loss 0.012319186702370644, R2 0.5254784822463989\n",
      "epoch 9323, loss 0.012328148819506168, R2 0.48756808042526245\n",
      "Eval loss 0.012319175526499748, R2 0.5254784822463989\n",
      "epoch 9324, loss 0.012328138574957848, R2 0.4875680208206177\n",
      "Eval loss 0.012319165281951427, R2 0.525478720664978\n",
      "epoch 9325, loss 0.012328130193054676, R2 0.487568199634552\n",
      "Eval loss 0.012319153174757957, R2 0.5254791975021362\n",
      "epoch 9326, loss 0.01232812087982893, R2 0.48756909370422363\n",
      "Eval loss 0.012319141067564487, R2 0.5254796743392944\n",
      "epoch 9327, loss 0.012328110635280609, R2 0.48756903409957886\n",
      "Eval loss 0.012319130823016167, R2 0.5254802107810974\n",
      "epoch 9328, loss 0.012328102253377438, R2 0.4875693917274475\n",
      "Eval loss 0.012319120578467846, R2 0.5254801511764526\n",
      "epoch 9329, loss 0.012328092940151691, R2 0.48756974935531616\n",
      "Eval loss 0.01231910940259695, R2 0.5254810452461243\n",
      "epoch 9330, loss 0.01232808269560337, R2 0.4875703454017639\n",
      "Eval loss 0.01231909729540348, R2 0.525481104850769\n",
      "epoch 9331, loss 0.012328075245022774, R2 0.4875703454017639\n",
      "Eval loss 0.012319086119532585, R2 0.5254818201065063\n",
      "epoch 9332, loss 0.012328065931797028, R2 0.4875708818435669\n",
      "Eval loss 0.01231907494366169, R2 0.5254820585250854\n",
      "epoch 9333, loss 0.012328056618571281, R2 0.487571120262146\n",
      "Eval loss 0.012319063767790794, R2 0.5254826545715332\n",
      "epoch 9334, loss 0.01232804637402296, R2 0.4875718355178833\n",
      "Eval loss 0.012319051660597324, R2 0.525483250617981\n",
      "epoch 9335, loss 0.012328037060797215, R2 0.4875718951225281\n",
      "Eval loss 0.012319043278694153, R2 0.5254831314086914\n",
      "epoch 9336, loss 0.012328029610216618, R2 0.4875725507736206\n",
      "Eval loss 0.012319032102823257, R2 0.5254836082458496\n",
      "epoch 9337, loss 0.012328019365668297, R2 0.4875728487968445\n",
      "Eval loss 0.012319019064307213, R2 0.5254846811294556\n",
      "epoch 9338, loss 0.01232801005244255, R2 0.48757386207580566\n",
      "Eval loss 0.012319008819758892, R2 0.5254844427108765\n",
      "epoch 9339, loss 0.012328000739216805, R2 0.4875742197036743\n",
      "Eval loss 0.012318998575210571, R2 0.5254850387573242\n",
      "epoch 9340, loss 0.012327991425991058, R2 0.4875739812850952\n",
      "Eval loss 0.012318986468017101, R2 0.5254857540130615\n",
      "epoch 9341, loss 0.012327983044087887, R2 0.4875749945640564\n",
      "Eval loss 0.01231897622346878, R2 0.5254861116409302\n",
      "epoch 9342, loss 0.01232797373086214, R2 0.48757535219192505\n",
      "Eval loss 0.01231896411627531, R2 0.5254867672920227\n",
      "epoch 9343, loss 0.01232796348631382, R2 0.48757511377334595\n",
      "Eval loss 0.01231895387172699, R2 0.5254870653152466\n",
      "epoch 9344, loss 0.012327955104410648, R2 0.4875754714012146\n",
      "Eval loss 0.01231894176453352, R2 0.52548748254776\n",
      "epoch 9345, loss 0.012327945791184902, R2 0.4875757098197937\n",
      "Eval loss 0.012318931519985199, R2 0.5254881381988525\n",
      "epoch 9346, loss 0.012327936477959156, R2 0.48757630586624146\n",
      "Eval loss 0.012318920344114304, R2 0.5254881381988525\n",
      "epoch 9347, loss 0.012327928096055984, R2 0.48757678270339966\n",
      "Eval loss 0.012318910099565983, R2 0.5254886150360107\n",
      "epoch 9348, loss 0.012327916920185089, R2 0.48757725954055786\n",
      "Eval loss 0.012318898923695087, R2 0.5254888534545898\n",
      "epoch 9349, loss 0.012327909469604492, R2 0.4875773787498474\n",
      "Eval loss 0.012318887747824192, R2 0.525489330291748\n",
      "epoch 9350, loss 0.012327902019023895, R2 0.48757821321487427\n",
      "Eval loss 0.012318876571953297, R2 0.5254895687103271\n",
      "epoch 9351, loss 0.012327891774475574, R2 0.4875781536102295\n",
      "Eval loss 0.012318866327404976, R2 0.5254902243614197\n",
      "epoch 9352, loss 0.012327882461249828, R2 0.48757851123809814\n",
      "Eval loss 0.01231885515153408, R2 0.5254908204078674\n",
      "epoch 9353, loss 0.012327873148024082, R2 0.4875788688659668\n",
      "Eval loss 0.012318843975663185, R2 0.5254911184310913\n",
      "epoch 9354, loss 0.01232786476612091, R2 0.4875794053077698\n",
      "Eval loss 0.012318833731114864, R2 0.5254915356636047\n",
      "epoch 9355, loss 0.01232785452157259, R2 0.48757970333099365\n",
      "Eval loss 0.01231882069259882, R2 0.5254918336868286\n",
      "epoch 9356, loss 0.012327845208346844, R2 0.4875808358192444\n",
      "Eval loss 0.012318811379373074, R2 0.5254920721054077\n",
      "epoch 9357, loss 0.012327837757766247, R2 0.4875803589820862\n",
      "Eval loss 0.012318800203502178, R2 0.5254926681518555\n",
      "epoch 9358, loss 0.0123278284445405, R2 0.48758143186569214\n",
      "Eval loss 0.012318789027631283, R2 0.5254932045936584\n",
      "epoch 9359, loss 0.012327819131314754, R2 0.48758113384246826\n",
      "Eval loss 0.012318778783082962, R2 0.525493323802948\n",
      "epoch 9360, loss 0.012327808886766434, R2 0.4875822067260742\n",
      "Eval loss 0.012318767607212067, R2 0.5254939198493958\n",
      "epoch 9361, loss 0.012327800504863262, R2 0.48758208751678467\n",
      "Eval loss 0.012318757362663746, R2 0.5254944562911987\n",
      "epoch 9362, loss 0.012327793054282665, R2 0.48758286237716675\n",
      "Eval loss 0.012318745255470276, R2 0.5254949331283569\n",
      "epoch 9363, loss 0.012327782809734344, R2 0.4875826835632324\n",
      "Eval loss 0.012318735010921955, R2 0.5254952907562256\n",
      "epoch 9364, loss 0.012327774427831173, R2 0.48758363723754883\n",
      "Eval loss 0.012318724766373634, R2 0.5254958868026733\n",
      "epoch 9365, loss 0.012327764183282852, R2 0.4875833988189697\n",
      "Eval loss 0.012318713590502739, R2 0.5254958868026733\n",
      "epoch 9366, loss 0.01232775580137968, R2 0.4875837564468384\n",
      "Eval loss 0.012318701483309269, R2 0.5254963040351868\n",
      "epoch 9367, loss 0.012327747419476509, R2 0.48758411407470703\n",
      "Eval loss 0.012318691238760948, R2 0.5254971385002136\n",
      "epoch 9368, loss 0.012327738106250763, R2 0.48758453130722046\n",
      "Eval loss 0.012318680062890053, R2 0.5254974365234375\n",
      "epoch 9369, loss 0.012327727861702442, R2 0.48758506774902344\n",
      "Eval loss 0.012318667955696583, R2 0.5254979133605957\n",
      "epoch 9370, loss 0.01232771947979927, R2 0.48758524656295776\n",
      "Eval loss 0.012318658642470837, R2 0.52549809217453\n",
      "epoch 9371, loss 0.012327710166573524, R2 0.4875853657722473\n",
      "Eval loss 0.012318648397922516, R2 0.5254983901977539\n",
      "epoch 9372, loss 0.012327701784670353, R2 0.48758572340011597\n",
      "Eval loss 0.012318638153374195, R2 0.5254990458488464\n",
      "epoch 9373, loss 0.012327693402767181, R2 0.4875865578651428\n",
      "Eval loss 0.012318626046180725, R2 0.5254994630813599\n",
      "epoch 9374, loss 0.01232768315821886, R2 0.4875872731208801\n",
      "Eval loss 0.01231861487030983, R2 0.5254998207092285\n",
      "epoch 9375, loss 0.012327674776315689, R2 0.4875878095626831\n",
      "Eval loss 0.012318604625761509, R2 0.5255002975463867\n",
      "epoch 9376, loss 0.012327666394412518, R2 0.48758798837661743\n",
      "Eval loss 0.012318593449890614, R2 0.5255007743835449\n",
      "epoch 9377, loss 0.012327658012509346, R2 0.4875878691673279\n",
      "Eval loss 0.012318583205342293, R2 0.525501012802124\n",
      "epoch 9378, loss 0.012327647767961025, R2 0.48758840560913086\n",
      "Eval loss 0.012318572029471397, R2 0.5255017280578613\n",
      "epoch 9379, loss 0.012327638454735279, R2 0.4875892996788025\n",
      "Eval loss 0.012318561784923077, R2 0.5255019664764404\n",
      "epoch 9380, loss 0.012327630072832108, R2 0.48758918046951294\n",
      "Eval loss 0.012318549677729607, R2 0.5255023241043091\n",
      "epoch 9381, loss 0.012327621690928936, R2 0.48758983612060547\n",
      "Eval loss 0.01231854036450386, R2 0.5255028009414673\n",
      "epoch 9382, loss 0.01232761237770319, R2 0.4875902533531189\n",
      "Eval loss 0.012318529188632965, R2 0.5255032777786255\n",
      "epoch 9383, loss 0.012327603995800018, R2 0.48759007453918457\n",
      "Eval loss 0.01231851801276207, R2 0.5255036950111389\n",
      "epoch 9384, loss 0.012327594682574272, R2 0.48759031295776367\n",
      "Eval loss 0.012318507768213749, R2 0.5255042314529419\n",
      "epoch 9385, loss 0.0123275863006711, R2 0.4875909686088562\n",
      "Eval loss 0.012318496592342854, R2 0.5255045890808105\n",
      "epoch 9386, loss 0.012327576987445354, R2 0.48759138584136963\n",
      "Eval loss 0.012318486347794533, R2 0.5255053043365479\n",
      "epoch 9387, loss 0.012327566742897034, R2 0.48759180307388306\n",
      "Eval loss 0.012318476103246212, R2 0.5255054235458374\n",
      "epoch 9388, loss 0.012327558360993862, R2 0.4875919818878174\n",
      "Eval loss 0.012318463996052742, R2 0.5255061388015747\n",
      "epoch 9389, loss 0.01232754997909069, R2 0.48759233951568604\n",
      "Eval loss 0.012318453751504421, R2 0.5255061388015747\n",
      "epoch 9390, loss 0.012327541597187519, R2 0.4875926971435547\n",
      "Eval loss 0.012318444438278675, R2 0.5255064964294434\n",
      "epoch 9391, loss 0.012327531352639198, R2 0.48759323358535767\n",
      "Eval loss 0.012318430468440056, R2 0.5255070328712463\n",
      "epoch 9392, loss 0.012327523902058601, R2 0.4875938892364502\n",
      "Eval loss 0.012318422086536884, R2 0.5255074501037598\n",
      "epoch 9393, loss 0.01232751365751028, R2 0.4875936508178711\n",
      "Eval loss 0.012318410910665989, R2 0.525507926940918\n",
      "epoch 9394, loss 0.012327505275607109, R2 0.4875946640968323\n",
      "Eval loss 0.012318399734795094, R2 0.5255081057548523\n",
      "epoch 9395, loss 0.012327497825026512, R2 0.4875943660736084\n",
      "Eval loss 0.012318390421569347, R2 0.5255082845687866\n",
      "epoch 9396, loss 0.012327487580478191, R2 0.4875949025154114\n",
      "Eval loss 0.012318378314375877, R2 0.5255091786384583\n",
      "epoch 9397, loss 0.01232747919857502, R2 0.48759526014328003\n",
      "Eval loss 0.012318368069827557, R2 0.5255094766616821\n",
      "epoch 9398, loss 0.012327469885349274, R2 0.48759549856185913\n",
      "Eval loss 0.012318357825279236, R2 0.5255099534988403\n",
      "epoch 9399, loss 0.012327461503446102, R2 0.487596333026886\n",
      "Eval loss 0.01231834664940834, R2 0.5255105495452881\n",
      "epoch 9400, loss 0.012327454052865505, R2 0.4875965118408203\n",
      "Eval loss 0.01231833640486002, R2 0.5255106687545776\n",
      "epoch 9401, loss 0.01232744287699461, R2 0.48759663105010986\n",
      "Eval loss 0.012318327091634274, R2 0.5255109071731567\n",
      "epoch 9402, loss 0.012327435426414013, R2 0.4875972270965576\n",
      "Eval loss 0.012318314984440804, R2 0.5255115032196045\n",
      "epoch 9403, loss 0.012327426113188267, R2 0.48759764432907104\n",
      "Eval loss 0.012318303808569908, R2 0.5255122184753418\n",
      "epoch 9404, loss 0.01232741866260767, R2 0.4875979423522949\n",
      "Eval loss 0.012318293564021587, R2 0.5255122184753418\n",
      "epoch 9405, loss 0.012327408418059349, R2 0.48759788274765015\n",
      "Eval loss 0.012318283319473267, R2 0.5255125761032104\n",
      "epoch 9406, loss 0.012327399104833603, R2 0.48759907484054565\n",
      "Eval loss 0.012318272143602371, R2 0.5255128145217896\n",
      "epoch 9407, loss 0.012327390722930431, R2 0.48759859800338745\n",
      "Eval loss 0.01231826189905405, R2 0.5255139470100403\n",
      "epoch 9408, loss 0.012327381409704685, R2 0.48759937286376953\n",
      "Eval loss 0.012318250723183155, R2 0.5255136489868164\n",
      "epoch 9409, loss 0.012327373959124088, R2 0.48759984970092773\n",
      "Eval loss 0.012318240478634834, R2 0.5255143642425537\n",
      "epoch 9410, loss 0.012327365577220917, R2 0.4875999689102173\n",
      "Eval loss 0.012318230234086514, R2 0.5255141854286194\n",
      "epoch 9411, loss 0.01232735626399517, R2 0.48760104179382324\n",
      "Eval loss 0.012318219058215618, R2 0.5255153179168701\n",
      "epoch 9412, loss 0.012327346950769424, R2 0.48760128021240234\n",
      "Eval loss 0.012318208813667297, R2 0.5255153179168701\n",
      "epoch 9413, loss 0.012327338568866253, R2 0.487601101398468\n",
      "Eval loss 0.012318198569118977, R2 0.5255158543586731\n",
      "epoch 9414, loss 0.012327329255640507, R2 0.48760145902633667\n",
      "Eval loss 0.012318186461925507, R2 0.525516152381897\n",
      "epoch 9415, loss 0.01232732180505991, R2 0.4876018166542053\n",
      "Eval loss 0.01231817714869976, R2 0.5255168080329895\n",
      "epoch 9416, loss 0.012327312491834164, R2 0.4876025319099426\n",
      "Eval loss 0.012318165972828865, R2 0.5255172252655029\n",
      "epoch 9417, loss 0.012327303178608418, R2 0.48760294914245605\n",
      "Eval loss 0.012318157590925694, R2 0.525518000125885\n",
      "epoch 9418, loss 0.012327294796705246, R2 0.48760294914245605\n",
      "Eval loss 0.012318144552409649, R2 0.5255180597305298\n",
      "epoch 9419, loss 0.012327284552156925, R2 0.48760348558425903\n",
      "Eval loss 0.012318134307861328, R2 0.5255184173583984\n",
      "epoch 9420, loss 0.012327276170253754, R2 0.48760372400283813\n",
      "Eval loss 0.012318124063313007, R2 0.525518536567688\n",
      "epoch 9421, loss 0.012327268719673157, R2 0.48760467767715454\n",
      "Eval loss 0.012318113818764687, R2 0.5255192518234253\n",
      "epoch 9422, loss 0.01232725940644741, R2 0.4876042604446411\n",
      "Eval loss 0.012318103574216366, R2 0.5255194902420044\n",
      "epoch 9423, loss 0.01232724916189909, R2 0.4876054525375366\n",
      "Eval loss 0.01231809239834547, R2 0.5255202054977417\n",
      "epoch 9424, loss 0.012327241711318493, R2 0.4876052737236023\n",
      "Eval loss 0.01231808215379715, R2 0.5255204439163208\n",
      "epoch 9425, loss 0.012327233329415321, R2 0.4876055121421814\n",
      "Eval loss 0.012318069115281105, R2 0.5255206823348999\n",
      "epoch 9426, loss 0.01232722494751215, R2 0.48760586977005005\n",
      "Eval loss 0.012318060733377934, R2 0.5255215167999268\n",
      "epoch 9427, loss 0.012327216565608978, R2 0.48760634660720825\n",
      "Eval loss 0.012318049557507038, R2 0.5255217552185059\n",
      "epoch 9428, loss 0.012327207252383232, R2 0.4876072406768799\n",
      "Eval loss 0.012318040244281292, R2 0.5255222320556641\n",
      "epoch 9429, loss 0.012327197939157486, R2 0.487606942653656\n",
      "Eval loss 0.012318029068410397, R2 0.5255223512649536\n",
      "epoch 9430, loss 0.012327190488576889, R2 0.4876078963279724\n",
      "Eval loss 0.012318018823862076, R2 0.5255228877067566\n",
      "epoch 9431, loss 0.012327182106673717, R2 0.48760777711868286\n",
      "Eval loss 0.01231800764799118, R2 0.5255234837532043\n",
      "epoch 9432, loss 0.012327172793447971, R2 0.48760801553726196\n",
      "Eval loss 0.012317999266088009, R2 0.5255237817764282\n",
      "epoch 9433, loss 0.0123271644115448, R2 0.4876083731651306\n",
      "Eval loss 0.012317987158894539, R2 0.525523841381073\n",
      "epoch 9434, loss 0.012327155098319054, R2 0.4876089096069336\n",
      "Eval loss 0.012317975983023643, R2 0.5255245566368103\n",
      "epoch 9435, loss 0.012327147647738457, R2 0.4876095652580261\n",
      "Eval loss 0.012317967601120472, R2 0.5255246162414551\n",
      "epoch 9436, loss 0.012327139265835285, R2 0.4876095652580261\n",
      "Eval loss 0.012317956425249577, R2 0.5255252122879028\n",
      "epoch 9437, loss 0.012327129021286964, R2 0.4876098036766052\n",
      "Eval loss 0.012317945249378681, R2 0.5255255699157715\n",
      "epoch 9438, loss 0.012327120639383793, R2 0.4876106381416321\n",
      "Eval loss 0.012317935936152935, R2 0.5255259275436401\n",
      "epoch 9439, loss 0.012327112257480621, R2 0.48761069774627686\n",
      "Eval loss 0.01231792476028204, R2 0.5255264043807983\n",
      "epoch 9440, loss 0.012327104806900024, R2 0.4876108169555664\n",
      "Eval loss 0.012317914515733719, R2 0.5255268812179565\n",
      "epoch 9441, loss 0.012327096424996853, R2 0.48761117458343506\n",
      "Eval loss 0.012317903339862823, R2 0.5255273580551147\n",
      "epoch 9442, loss 0.012327086180448532, R2 0.4876115918159485\n",
      "Eval loss 0.012317894026637077, R2 0.5255274176597595\n",
      "epoch 9443, loss 0.01232707779854536, R2 0.48761194944381714\n",
      "Eval loss 0.012317882850766182, R2 0.5255280137062073\n",
      "epoch 9444, loss 0.012327070347964764, R2 0.487612247467041\n",
      "Eval loss 0.012317872606217861, R2 0.5255287885665894\n",
      "epoch 9445, loss 0.012327061966061592, R2 0.48761314153671265\n",
      "Eval loss 0.01231786236166954, R2 0.5255289077758789\n",
      "epoch 9446, loss 0.012327050790190697, R2 0.48761308193206787\n",
      "Eval loss 0.01231785211712122, R2 0.5255290269851685\n",
      "epoch 9447, loss 0.0123270433396101, R2 0.48761337995529175\n",
      "Eval loss 0.012317841872572899, R2 0.5255299806594849\n",
      "epoch 9448, loss 0.012327034957706928, R2 0.48761439323425293\n",
      "Eval loss 0.012317829765379429, R2 0.5255303382873535\n",
      "epoch 9449, loss 0.012327026575803757, R2 0.48761409521102905\n",
      "Eval loss 0.012317819520831108, R2 0.5255305767059326\n",
      "epoch 9450, loss 0.012327018193900585, R2 0.48761415481567383\n",
      "Eval loss 0.012317811138927937, R2 0.5255309343338013\n",
      "epoch 9451, loss 0.012327008880674839, R2 0.48761463165283203\n",
      "Eval loss 0.012317799963057041, R2 0.5255310535430908\n",
      "epoch 9452, loss 0.012327001430094242, R2 0.4876149892807007\n",
      "Eval loss 0.01231778971850872, R2 0.5255314111709595\n",
      "epoch 9453, loss 0.012326992116868496, R2 0.48761600255966187\n",
      "Eval loss 0.0123177794739604, R2 0.5255323648452759\n",
      "epoch 9454, loss 0.012326984666287899, R2 0.48761582374572754\n",
      "Eval loss 0.012317768298089504, R2 0.5255327224731445\n",
      "epoch 9455, loss 0.012326974421739578, R2 0.48761624097824097\n",
      "Eval loss 0.012317758053541183, R2 0.5255328416824341\n",
      "epoch 9456, loss 0.012326966971158981, R2 0.4876171946525574\n",
      "Eval loss 0.012317747808992863, R2 0.5255330801010132\n",
      "epoch 9457, loss 0.01232695858925581, R2 0.4876168966293335\n",
      "Eval loss 0.012317737564444542, R2 0.5255334377288818\n",
      "epoch 9458, loss 0.012326950207352638, R2 0.4876174330711365\n",
      "Eval loss 0.012317728251218796, R2 0.5255337953567505\n",
      "epoch 9459, loss 0.012326940894126892, R2 0.4876176714897156\n",
      "Eval loss 0.012317718006670475, R2 0.5255346298217773\n",
      "epoch 9460, loss 0.01232693251222372, R2 0.48761796951293945\n",
      "Eval loss 0.012317707762122154, R2 0.5255346298217773\n",
      "epoch 9461, loss 0.012326923198997974, R2 0.4876183867454529\n",
      "Eval loss 0.012317696586251259, R2 0.5255354642868042\n",
      "epoch 9462, loss 0.012326914817094803, R2 0.48761940002441406\n",
      "Eval loss 0.012317686341702938, R2 0.5255361199378967\n",
      "epoch 9463, loss 0.01232690829783678, R2 0.48761898279190063\n",
      "Eval loss 0.012317675165832043, R2 0.5255358219146729\n",
      "epoch 9464, loss 0.012326898984611034, R2 0.48761940002441406\n",
      "Eval loss 0.012317664921283722, R2 0.5255365371704102\n",
      "epoch 9465, loss 0.012326888740062714, R2 0.4876198172569275\n",
      "Eval loss 0.012317655608057976, R2 0.5255370140075684\n",
      "epoch 9466, loss 0.012326881289482117, R2 0.48762011528015137\n",
      "Eval loss 0.012317645363509655, R2 0.5255370140075684\n",
      "epoch 9467, loss 0.01232687383890152, R2 0.4876210689544678\n",
      "Eval loss 0.012317635118961334, R2 0.525537371635437\n",
      "epoch 9468, loss 0.012326865456998348, R2 0.48762065172195435\n",
      "Eval loss 0.012317624874413013, R2 0.5255380868911743\n",
      "epoch 9469, loss 0.012326857075095177, R2 0.48762112855911255\n",
      "Eval loss 0.012317615561187267, R2 0.525538444519043\n",
      "epoch 9470, loss 0.01232684776186943, R2 0.4876214861869812\n",
      "Eval loss 0.012317604385316372, R2 0.5255390405654907\n",
      "epoch 9471, loss 0.012326839379966259, R2 0.48762232065200806\n",
      "Eval loss 0.012317594140768051, R2 0.5255395174026489\n",
      "epoch 9472, loss 0.012326830998063087, R2 0.4876222014427185\n",
      "Eval loss 0.01231758389621973, R2 0.525539755821228\n",
      "epoch 9473, loss 0.012326822616159916, R2 0.48762303590774536\n",
      "Eval loss 0.01231757365167141, R2 0.5255402326583862\n",
      "epoch 9474, loss 0.012326814234256744, R2 0.4876227378845215\n",
      "Eval loss 0.012317563407123089, R2 0.5255401730537415\n",
      "epoch 9475, loss 0.012326806783676147, R2 0.4876232147216797\n",
      "Eval loss 0.012317554093897343, R2 0.5255406498908997\n",
      "epoch 9476, loss 0.012326798401772976, R2 0.48762357234954834\n",
      "Eval loss 0.012317543849349022, R2 0.5255409479141235\n",
      "epoch 9477, loss 0.012326788157224655, R2 0.48762398958206177\n",
      "Eval loss 0.012317533604800701, R2 0.5255417227745056\n",
      "epoch 9478, loss 0.012326780706644058, R2 0.4876243472099304\n",
      "Eval loss 0.01231752336025238, R2 0.5255417227745056\n",
      "epoch 9479, loss 0.012326772324740887, R2 0.4876246452331543\n",
      "Eval loss 0.012317512184381485, R2 0.5255424976348877\n",
      "epoch 9480, loss 0.012326763942837715, R2 0.4876246452331543\n",
      "Eval loss 0.012317501939833164, R2 0.5255429744720459\n",
      "epoch 9481, loss 0.012326754629611969, R2 0.4876255393028259\n",
      "Eval loss 0.012317492626607418, R2 0.5255434513092041\n",
      "epoch 9482, loss 0.012326747179031372, R2 0.48762571811676025\n",
      "Eval loss 0.012317482382059097, R2 0.5255436897277832\n",
      "epoch 9483, loss 0.0123267387971282, R2 0.4876260757446289\n",
      "Eval loss 0.012317471206188202, R2 0.5255435705184937\n",
      "epoch 9484, loss 0.012326729483902454, R2 0.4876266121864319\n",
      "Eval loss 0.012317460961639881, R2 0.5255444049835205\n",
      "epoch 9485, loss 0.012326721101999283, R2 0.48762744665145874\n",
      "Eval loss 0.01231745071709156, R2 0.5255447626113892\n",
      "epoch 9486, loss 0.012326713651418686, R2 0.4876270890235901\n",
      "Eval loss 0.012317441403865814, R2 0.5255451202392578\n",
      "epoch 9487, loss 0.012326706200838089, R2 0.4876272678375244\n",
      "Eval loss 0.012317429296672344, R2 0.5255448818206787\n",
      "epoch 9488, loss 0.012326695956289768, R2 0.4876278042793274\n",
      "Eval loss 0.012317419983446598, R2 0.5255457162857056\n",
      "epoch 9489, loss 0.012326687574386597, R2 0.48762816190719604\n",
      "Eval loss 0.012317410670220852, R2 0.5255461931228638\n",
      "epoch 9490, loss 0.012326679192483425, R2 0.487628698348999\n",
      "Eval loss 0.012317401356995106, R2 0.525546669960022\n",
      "epoch 9491, loss 0.012326670810580254, R2 0.48762935400009155\n",
      "Eval loss 0.01231739018112421, R2 0.5255467891693115\n",
      "epoch 9492, loss 0.012326664291322231, R2 0.48762965202331543\n",
      "Eval loss 0.01231737993657589, R2 0.5255473852157593\n",
      "epoch 9493, loss 0.01232665404677391, R2 0.4876297116279602\n",
      "Eval loss 0.012317370623350143, R2 0.5255482792854309\n",
      "epoch 9494, loss 0.012326646596193314, R2 0.48763036727905273\n",
      "Eval loss 0.012317361310124397, R2 0.525547981262207\n",
      "epoch 9495, loss 0.012326638214290142, R2 0.4876307249069214\n",
      "Eval loss 0.012317350134253502, R2 0.52554851770401\n",
      "epoch 9496, loss 0.01232662983238697, R2 0.48763060569763184\n",
      "Eval loss 0.012317339889705181, R2 0.525549054145813\n",
      "epoch 9497, loss 0.012326622381806374, R2 0.48763155937194824\n",
      "Eval loss 0.01231732964515686, R2 0.5255492925643921\n",
      "epoch 9498, loss 0.012326613068580627, R2 0.48763126134872437\n",
      "Eval loss 0.012317320331931114, R2 0.5255499482154846\n",
      "epoch 9499, loss 0.012326604686677456, R2 0.4876320958137512\n",
      "Eval loss 0.012317310087382793, R2 0.5255501866340637\n",
      "epoch 9500, loss 0.012326597236096859, R2 0.4876323938369751\n",
      "Eval loss 0.012317300774157047, R2 0.5255505442619324\n",
      "epoch 9501, loss 0.012326588854193687, R2 0.4876324534416199\n",
      "Eval loss 0.012317289598286152, R2 0.5255508422851562\n",
      "epoch 9502, loss 0.012326577678322792, R2 0.4876330494880676\n",
      "Eval loss 0.012317278422415257, R2 0.5255517959594727\n",
      "epoch 9503, loss 0.01232657115906477, R2 0.48763298988342285\n",
      "Eval loss 0.01231726910918951, R2 0.5255516171455383\n",
      "epoch 9504, loss 0.012326562777161598, R2 0.4876338243484497\n",
      "Eval loss 0.01231725886464119, R2 0.5255521535873413\n",
      "epoch 9505, loss 0.012326555326581001, R2 0.48763370513916016\n",
      "Eval loss 0.012317249551415443, R2 0.5255523920059204\n",
      "epoch 9506, loss 0.01232654694467783, R2 0.48763400316238403\n",
      "Eval loss 0.012317239306867123, R2 0.5255532264709473\n",
      "epoch 9507, loss 0.012326539494097233, R2 0.4876348376274109\n",
      "Eval loss 0.012317229062318802, R2 0.5255533456802368\n",
      "epoch 9508, loss 0.012326530180871487, R2 0.48763489723205566\n",
      "Eval loss 0.012317219749093056, R2 0.5255534052848816\n",
      "epoch 9509, loss 0.012326521798968315, R2 0.4876357316970825\n",
      "Eval loss 0.012317209504544735, R2 0.5255541801452637\n",
      "epoch 9510, loss 0.012326514348387718, R2 0.48763537406921387\n",
      "Eval loss 0.012317199259996414, R2 0.5255542993545532\n",
      "epoch 9511, loss 0.012326505966484547, R2 0.48763591051101685\n",
      "Eval loss 0.012317188084125519, R2 0.525554895401001\n",
      "epoch 9512, loss 0.012326497584581375, R2 0.4876365661621094\n",
      "Eval loss 0.012317178770899773, R2 0.5255552530288696\n",
      "epoch 9513, loss 0.012326488271355629, R2 0.48763662576675415\n",
      "Eval loss 0.012317169457674026, R2 0.5255557298660278\n",
      "epoch 9514, loss 0.012326480820775032, R2 0.487636923789978\n",
      "Eval loss 0.012317159213125706, R2 0.5255557298660278\n",
      "epoch 9515, loss 0.012326473370194435, R2 0.48763710260391235\n",
      "Eval loss 0.01231714989989996, R2 0.5255563259124756\n",
      "epoch 9516, loss 0.012326464056968689, R2 0.487637460231781\n",
      "Eval loss 0.012317138724029064, R2 0.5255566835403442\n",
      "epoch 9517, loss 0.012326455675065517, R2 0.4876379370689392\n",
      "Eval loss 0.012317128479480743, R2 0.5255571603775024\n",
      "epoch 9518, loss 0.012326447293162346, R2 0.48763829469680786\n",
      "Eval loss 0.012317117303609848, R2 0.5255573391914368\n",
      "epoch 9519, loss 0.012326438911259174, R2 0.48763853311538696\n",
      "Eval loss 0.012317108921706676, R2 0.5255576372146606\n",
      "epoch 9520, loss 0.012326431460678577, R2 0.48763883113861084\n",
      "Eval loss 0.012317097745835781, R2 0.5255581140518188\n",
      "epoch 9521, loss 0.012326423078775406, R2 0.4876396656036377\n",
      "Eval loss 0.01231708936393261, R2 0.525558590888977\n",
      "epoch 9522, loss 0.012326414696872234, R2 0.48763948678970337\n",
      "Eval loss 0.012317079119384289, R2 0.5255590677261353\n",
      "epoch 9523, loss 0.012326407246291637, R2 0.487639844417572\n",
      "Eval loss 0.012317068874835968, R2 0.5255591869354248\n",
      "epoch 9524, loss 0.012326398864388466, R2 0.4876403212547302\n",
      "Eval loss 0.012317059561610222, R2 0.5255600214004517\n",
      "epoch 9525, loss 0.012326390482485294, R2 0.4876405596733093\n",
      "Eval loss 0.012317050248384476, R2 0.5255600810050964\n",
      "epoch 9526, loss 0.012326382100582123, R2 0.487640917301178\n",
      "Eval loss 0.012317038141191006, R2 0.5255606174468994\n",
      "epoch 9527, loss 0.012326374650001526, R2 0.4876411557197571\n",
      "Eval loss 0.01231702882796526, R2 0.5255611538887024\n",
      "epoch 9528, loss 0.012326366268098354, R2 0.48764169216156006\n",
      "Eval loss 0.012317019514739513, R2 0.5255613923072815\n",
      "epoch 9529, loss 0.012326358817517757, R2 0.4876418709754944\n",
      "Eval loss 0.012317009270191193, R2 0.5255616903305054\n",
      "epoch 9530, loss 0.012326349504292011, R2 0.48764222860336304\n",
      "Eval loss 0.012316999025642872, R2 0.5255622863769531\n",
      "epoch 9531, loss 0.012326342053711414, R2 0.48764318227767944\n",
      "Eval loss 0.012316989712417126, R2 0.5255622863769531\n",
      "epoch 9532, loss 0.012326331809163094, R2 0.48764294385910034\n",
      "Eval loss 0.01231698039919138, R2 0.5255628824234009\n",
      "epoch 9533, loss 0.012326325289905071, R2 0.4876432418823242\n",
      "Eval loss 0.012316971085965633, R2 0.5255630016326904\n",
      "epoch 9534, loss 0.0123263169080019, R2 0.48764359951019287\n",
      "Eval loss 0.012316959910094738, R2 0.5255633592605591\n",
      "epoch 9535, loss 0.012326309457421303, R2 0.48764389753341675\n",
      "Eval loss 0.012316949665546417, R2 0.5255640745162964\n",
      "epoch 9536, loss 0.012326302006840706, R2 0.4876447319984436\n",
      "Eval loss 0.012316940352320671, R2 0.5255641937255859\n",
      "epoch 9537, loss 0.012326293624937534, R2 0.4876445531845093\n",
      "Eval loss 0.012316931039094925, R2 0.5255646705627441\n",
      "epoch 9538, loss 0.012326284311711788, R2 0.48764491081237793\n",
      "Eval loss 0.012316918931901455, R2 0.5255653858184814\n",
      "epoch 9539, loss 0.012326276861131191, R2 0.4876457452774048\n",
      "Eval loss 0.012316910549998283, R2 0.5255657434463501\n",
      "epoch 9540, loss 0.01232626847922802, R2 0.48764562606811523\n",
      "Eval loss 0.012316900305449963, R2 0.5255659818649292\n",
      "epoch 9541, loss 0.012326259166002274, R2 0.4876459836959839\n",
      "Eval loss 0.012316891923546791, R2 0.5255664587020874\n",
      "epoch 9542, loss 0.012326252646744251, R2 0.48764657974243164\n",
      "Eval loss 0.012316880747675896, R2 0.525566577911377\n",
      "epoch 9543, loss 0.01232624426484108, R2 0.48764675855636597\n",
      "Eval loss 0.012316872365772724, R2 0.5255669355392456\n",
      "epoch 9544, loss 0.012326235882937908, R2 0.4876469373703003\n",
      "Eval loss 0.012316861189901829, R2 0.5255675315856934\n",
      "epoch 9545, loss 0.012326229363679886, R2 0.48764723539352417\n",
      "Eval loss 0.012316852807998657, R2 0.5255678296089172\n",
      "epoch 9546, loss 0.012326219119131565, R2 0.4876483082771301\n",
      "Eval loss 0.012316841632127762, R2 0.5255682468414307\n",
      "epoch 9547, loss 0.012326211668550968, R2 0.4876481294631958\n",
      "Eval loss 0.01231683325022459, R2 0.525568425655365\n",
      "epoch 9548, loss 0.012326202355325222, R2 0.4876486659049988\n",
      "Eval loss 0.01231682300567627, R2 0.5255688428878784\n",
      "epoch 9549, loss 0.0123261958360672, R2 0.487648606300354\n",
      "Eval loss 0.012316812761127949, R2 0.5255692005157471\n",
      "epoch 9550, loss 0.012326188385486603, R2 0.48764878511428833\n",
      "Eval loss 0.012316802516579628, R2 0.52556973695755\n",
      "epoch 9551, loss 0.012326179072260857, R2 0.48764944076538086\n",
      "Eval loss 0.012316792272031307, R2 0.5255703926086426\n",
      "epoch 9552, loss 0.012326170690357685, R2 0.48764967918395996\n",
      "Eval loss 0.012316782958805561, R2 0.5255699157714844\n",
      "epoch 9553, loss 0.012326164171099663, R2 0.4876505732536316\n",
      "Eval loss 0.01231677457690239, R2 0.5255709886550903\n",
      "epoch 9554, loss 0.012326155789196491, R2 0.4876502752304077\n",
      "Eval loss 0.01231676246970892, R2 0.5255709886550903\n",
      "epoch 9555, loss 0.012326148338615894, R2 0.4876505732536316\n",
      "Eval loss 0.012316752225160599, R2 0.5255717039108276\n",
      "epoch 9556, loss 0.012326138094067574, R2 0.487650990486145\n",
      "Eval loss 0.012316743843257427, R2 0.5255720019340515\n",
      "epoch 9557, loss 0.012326130643486977, R2 0.4876519441604614\n",
      "Eval loss 0.012316734530031681, R2 0.5255724191665649\n",
      "epoch 9558, loss 0.012326124124228954, R2 0.4876515865325928\n",
      "Eval loss 0.012316725216805935, R2 0.5255724787712097\n",
      "epoch 9559, loss 0.012326114811003208, R2 0.4876524806022644\n",
      "Eval loss 0.012316714972257614, R2 0.5255728363990784\n",
      "epoch 9560, loss 0.012326107360422611, R2 0.4876524806022644\n",
      "Eval loss 0.012316704727709293, R2 0.5255734920501709\n",
      "epoch 9561, loss 0.012326099909842014, R2 0.48765289783477783\n",
      "Eval loss 0.012316695414483547, R2 0.5255741477012634\n",
      "epoch 9562, loss 0.012326091527938843, R2 0.48765307664871216\n",
      "Eval loss 0.012316685169935226, R2 0.5255739688873291\n",
      "epoch 9563, loss 0.012326083146035671, R2 0.4876534342765808\n",
      "Eval loss 0.01231667585670948, R2 0.5255747437477112\n",
      "epoch 9564, loss 0.012326076626777649, R2 0.48765408992767334\n",
      "Eval loss 0.01231666561216116, R2 0.5255747437477112\n",
      "epoch 9565, loss 0.012326068244874477, R2 0.487653911113739\n",
      "Eval loss 0.012316657230257988, R2 0.5255753397941589\n",
      "epoch 9566, loss 0.012326058931648731, R2 0.4876541495323181\n",
      "Eval loss 0.012316646985709667, R2 0.5255757570266724\n",
      "epoch 9567, loss 0.012326051481068134, R2 0.48765474557876587\n",
      "Eval loss 0.012316636741161346, R2 0.5255759954452515\n",
      "epoch 9568, loss 0.012326042167842388, R2 0.4876554608345032\n",
      "Eval loss 0.012316626496613026, R2 0.5255762338638306\n",
      "epoch 9569, loss 0.012326034717261791, R2 0.4876554608345032\n",
      "Eval loss 0.01231661718338728, R2 0.5255767107009888\n",
      "epoch 9570, loss 0.012326028198003769, R2 0.4876555800437927\n",
      "Eval loss 0.012316608801484108, R2 0.5255773663520813\n",
      "epoch 9571, loss 0.012326018884778023, R2 0.4876561164855957\n",
      "Eval loss 0.012316598556935787, R2 0.52557772397995\n",
      "epoch 9572, loss 0.012326011434197426, R2 0.4876564145088196\n",
      "Eval loss 0.012316588312387466, R2 0.5255782604217529\n",
      "epoch 9573, loss 0.012326003052294254, R2 0.4876566529273987\n",
      "Eval loss 0.01231657899916172, R2 0.5255782604217529\n",
      "epoch 9574, loss 0.012325995601713657, R2 0.48765695095062256\n",
      "Eval loss 0.012316569685935974, R2 0.5255786180496216\n",
      "epoch 9575, loss 0.01232598815113306, R2 0.48765742778778076\n",
      "Eval loss 0.012316559441387653, R2 0.5255788564682007\n",
      "epoch 9576, loss 0.012325978837907314, R2 0.4876577854156494\n",
      "Eval loss 0.012316549196839333, R2 0.5255792140960693\n",
      "epoch 9577, loss 0.012325972318649292, R2 0.48765838146209717\n",
      "Eval loss 0.012316539883613586, R2 0.5255800485610962\n",
      "epoch 9578, loss 0.01232596393674612, R2 0.48765891790390015\n",
      "Eval loss 0.01231653057038784, R2 0.5255802273750305\n",
      "epoch 9579, loss 0.012325955554842949, R2 0.48765844106674194\n",
      "Eval loss 0.012316521257162094, R2 0.5255802869796753\n",
      "epoch 9580, loss 0.012325947172939777, R2 0.4876589775085449\n",
      "Eval loss 0.012316511943936348, R2 0.525580644607544\n",
      "epoch 9581, loss 0.012325938791036606, R2 0.4876592755317688\n",
      "Eval loss 0.012316501699388027, R2 0.5255810618400574\n",
      "epoch 9582, loss 0.012325933203101158, R2 0.4876595139503479\n",
      "Eval loss 0.012316493317484856, R2 0.5255816578865051\n",
      "epoch 9583, loss 0.012325923889875412, R2 0.4876600503921509\n",
      "Eval loss 0.01231648214161396, R2 0.5255823135375977\n",
      "epoch 9584, loss 0.012325916439294815, R2 0.4876602292060852\n",
      "Eval loss 0.012316472828388214, R2 0.5255824327468872\n",
      "epoch 9585, loss 0.012325908988714218, R2 0.4876605272293091\n",
      "Eval loss 0.012316463515162468, R2 0.5255825519561768\n",
      "epoch 9586, loss 0.012325899675488472, R2 0.48766088485717773\n",
      "Eval loss 0.012316454201936722, R2 0.5255831480026245\n",
      "epoch 9587, loss 0.01232589315623045, R2 0.48766136169433594\n",
      "Eval loss 0.012316442094743252, R2 0.5255835056304932\n",
      "epoch 9588, loss 0.012325885705649853, R2 0.4876614809036255\n",
      "Eval loss 0.012316434644162655, R2 0.5255839228630066\n",
      "epoch 9589, loss 0.012325878255069256, R2 0.48766183853149414\n",
      "Eval loss 0.01231642346829176, R2 0.52558434009552\n",
      "epoch 9590, loss 0.01232586894184351, R2 0.4876621961593628\n",
      "Eval loss 0.012316415086388588, R2 0.5255846977233887\n",
      "epoch 9591, loss 0.012325861491262913, R2 0.487662672996521\n",
      "Eval loss 0.012316406704485416, R2 0.5255851745605469\n",
      "epoch 9592, loss 0.012325854040682316, R2 0.4876629710197449\n",
      "Eval loss 0.012316396459937096, R2 0.525585949420929\n",
      "epoch 9593, loss 0.012325846590101719, R2 0.48766326904296875\n",
      "Eval loss 0.012316386215388775, R2 0.5255856513977051\n",
      "epoch 9594, loss 0.012325838208198547, R2 0.4876629710197449\n",
      "Eval loss 0.012316375970840454, R2 0.5255860090255737\n",
      "epoch 9595, loss 0.012325831688940525, R2 0.4876639246940613\n",
      "Eval loss 0.012316366657614708, R2 0.5255864262580872\n",
      "epoch 9596, loss 0.012325822375714779, R2 0.4876645803451538\n",
      "Eval loss 0.012316357344388962, R2 0.525586724281311\n",
      "epoch 9597, loss 0.012325813993811607, R2 0.4876646399497986\n",
      "Eval loss 0.012316348031163216, R2 0.5255869626998901\n",
      "epoch 9598, loss 0.012325805611908436, R2 0.4876652956008911\n",
      "Eval loss 0.012316339649260044, R2 0.5255881547927856\n",
      "epoch 9599, loss 0.012325799092650414, R2 0.4876651167869568\n",
      "Eval loss 0.012316329404711723, R2 0.5255878567695618\n",
      "epoch 9600, loss 0.012325791642069817, R2 0.487665593624115\n",
      "Eval loss 0.012316319160163403, R2 0.5255879163742065\n",
      "epoch 9601, loss 0.01232578232884407, R2 0.4876657724380493\n",
      "Eval loss 0.012316310778260231, R2 0.5255883932113647\n",
      "epoch 9602, loss 0.012325774878263474, R2 0.48766613006591797\n",
      "Eval loss 0.01231630053371191, R2 0.525589108467102\n",
      "epoch 9603, loss 0.012325767427682877, R2 0.48766642808914185\n",
      "Eval loss 0.012316292151808739, R2 0.5255894064903259\n",
      "epoch 9604, loss 0.01232575997710228, R2 0.48766738176345825\n",
      "Eval loss 0.012316280975937843, R2 0.5255895853042603\n",
      "epoch 9605, loss 0.012325752526521683, R2 0.4876670241355896\n",
      "Eval loss 0.012316271662712097, R2 0.525590181350708\n",
      "epoch 9606, loss 0.012325743213295937, R2 0.48766738176345825\n",
      "Eval loss 0.012316263280808926, R2 0.5255905389785767\n",
      "epoch 9607, loss 0.01232573576271534, R2 0.4876677393913269\n",
      "Eval loss 0.012316253036260605, R2 0.5255907773971558\n",
      "epoch 9608, loss 0.012325729243457317, R2 0.48766863346099854\n",
      "Eval loss 0.012316243723034859, R2 0.525591254234314\n",
      "epoch 9609, loss 0.012325720861554146, R2 0.48766833543777466\n",
      "Eval loss 0.012316232547163963, R2 0.5255914330482483\n",
      "epoch 9610, loss 0.012325714342296124, R2 0.4876689910888672\n",
      "Eval loss 0.012316224165260792, R2 0.5255923271179199\n",
      "epoch 9611, loss 0.012325705960392952, R2 0.4876689910888672\n",
      "Eval loss 0.012316214852035046, R2 0.525592565536499\n",
      "epoch 9612, loss 0.012325696647167206, R2 0.48766982555389404\n",
      "Eval loss 0.012316204607486725, R2 0.5255930423736572\n",
      "epoch 9613, loss 0.012325690127909184, R2 0.4876699447631836\n",
      "Eval loss 0.012316196225583553, R2 0.5255932211875916\n",
      "epoch 9614, loss 0.012325681746006012, R2 0.4876701235771179\n",
      "Eval loss 0.012316186912357807, R2 0.5255934596061707\n",
      "epoch 9615, loss 0.012325674295425415, R2 0.4876706004142761\n",
      "Eval loss 0.012316177599132061, R2 0.5255938172340393\n",
      "epoch 9616, loss 0.012325666844844818, R2 0.4876711368560791\n",
      "Eval loss 0.012316168285906315, R2 0.5255943536758423\n",
      "epoch 9617, loss 0.012325658462941647, R2 0.4876716136932373\n",
      "Eval loss 0.012316158041357994, R2 0.5255945920944214\n",
      "epoch 9618, loss 0.012325651943683624, R2 0.4876711964607239\n",
      "Eval loss 0.012316149659454823, R2 0.5255950689315796\n",
      "epoch 9619, loss 0.012325642630457878, R2 0.48767179250717163\n",
      "Eval loss 0.012316139414906502, R2 0.5255951881408691\n",
      "epoch 9620, loss 0.012325636111199856, R2 0.4876719117164612\n",
      "Eval loss 0.012316130101680756, R2 0.5255957841873169\n",
      "epoch 9621, loss 0.012325627729296684, R2 0.48767226934432983\n",
      "Eval loss 0.012316121719777584, R2 0.5255959630012512\n",
      "epoch 9622, loss 0.012325620278716087, R2 0.48767298460006714\n",
      "Eval loss 0.012316112406551838, R2 0.5255963206291199\n",
      "epoch 9623, loss 0.012325611896812916, R2 0.4876730442047119\n",
      "Eval loss 0.012316101230680943, R2 0.5255969166755676\n",
      "epoch 9624, loss 0.012325605377554893, R2 0.48767316341400146\n",
      "Eval loss 0.012316091917455196, R2 0.5255968570709229\n",
      "epoch 9625, loss 0.012325596995651722, R2 0.48767417669296265\n",
      "Eval loss 0.01231608260422945, R2 0.525597333908081\n",
      "epoch 9626, loss 0.0123255904763937, R2 0.4876742959022522\n",
      "Eval loss 0.012316074222326279, R2 0.5255978107452393\n",
      "epoch 9627, loss 0.012325581163167953, R2 0.4876747727394104\n",
      "Eval loss 0.012316064909100533, R2 0.5255985856056213\n",
      "epoch 9628, loss 0.012325573712587357, R2 0.48767465353012085\n",
      "Eval loss 0.012316054664552212, R2 0.5255982875823975\n",
      "epoch 9629, loss 0.01232556626200676, R2 0.4876745939254761\n",
      "Eval loss 0.012316044420003891, R2 0.5255986452102661\n",
      "epoch 9630, loss 0.012325559742748737, R2 0.4876750707626343\n",
      "Eval loss 0.012316035106778145, R2 0.5255993008613586\n",
      "epoch 9631, loss 0.01232555229216814, R2 0.48767536878585815\n",
      "Eval loss 0.012316026724874973, R2 0.5255995988845825\n",
      "epoch 9632, loss 0.012325543910264969, R2 0.487676203250885\n",
      "Eval loss 0.012316017411649227, R2 0.5255999565124512\n",
      "epoch 9633, loss 0.012325535528361797, R2 0.4876760244369507\n",
      "Eval loss 0.012316007167100906, R2 0.5256000757217407\n",
      "epoch 9634, loss 0.0123255280777812, R2 0.4876766800880432\n",
      "Eval loss 0.012315998785197735, R2 0.5256006717681885\n",
      "epoch 9635, loss 0.012325521558523178, R2 0.4876765012741089\n",
      "Eval loss 0.012315988540649414, R2 0.525600790977478\n",
      "epoch 9636, loss 0.012325513176620007, R2 0.4876769781112671\n",
      "Eval loss 0.012315980158746243, R2 0.5256014466285706\n",
      "epoch 9637, loss 0.012325504794716835, R2 0.4876776337623596\n",
      "Eval loss 0.012315970845520496, R2 0.5256018042564392\n",
      "epoch 9638, loss 0.012325497344136238, R2 0.48767733573913574\n",
      "Eval loss 0.01231596153229475, R2 0.5256022810935974\n",
      "epoch 9639, loss 0.012325488962233067, R2 0.4876784682273865\n",
      "Eval loss 0.012315952219069004, R2 0.5256026387214661\n",
      "epoch 9640, loss 0.012325482442975044, R2 0.48767876625061035\n",
      "Eval loss 0.012315942905843258, R2 0.5256025791168213\n",
      "epoch 9641, loss 0.012325474992394447, R2 0.48767876625061035\n",
      "Eval loss 0.012315933592617512, R2 0.5256029367446899\n",
      "epoch 9642, loss 0.012325468473136425, R2 0.4876793622970581\n",
      "Eval loss 0.012315923348069191, R2 0.5256038904190063\n",
      "epoch 9643, loss 0.012325458228588104, R2 0.48767924308776855\n",
      "Eval loss 0.012315915897488594, R2 0.5256034731864929\n",
      "epoch 9644, loss 0.012325451709330082, R2 0.48767954111099243\n",
      "Eval loss 0.012315905652940273, R2 0.5256040096282959\n",
      "epoch 9645, loss 0.012325444258749485, R2 0.48767954111099243\n",
      "Eval loss 0.012315897271037102, R2 0.5256047248840332\n",
      "epoch 9646, loss 0.012325436808168888, R2 0.48768067359924316\n",
      "Eval loss 0.012315887026488781, R2 0.5256052613258362\n",
      "epoch 9647, loss 0.012325429357588291, R2 0.48768067359924316\n",
      "Eval loss 0.012315877713263035, R2 0.525605320930481\n",
      "epoch 9648, loss 0.012325420044362545, R2 0.4876808524131775\n",
      "Eval loss 0.012315869331359863, R2 0.5256056785583496\n",
      "epoch 9649, loss 0.012325412593781948, R2 0.48768115043640137\n",
      "Eval loss 0.012315859086811543, R2 0.5256063938140869\n",
      "epoch 9650, loss 0.0123254070058465, R2 0.48768186569213867\n",
      "Eval loss 0.012315848842263222, R2 0.5256061553955078\n",
      "epoch 9651, loss 0.012325399555265903, R2 0.487682044506073\n",
      "Eval loss 0.01231584046036005, R2 0.5256067514419556\n",
      "epoch 9652, loss 0.012325392104685307, R2 0.487682044506073\n",
      "Eval loss 0.012315832078456879, R2 0.5256072282791138\n",
      "epoch 9653, loss 0.01232538279145956, R2 0.4876830577850342\n",
      "Eval loss 0.012315821833908558, R2 0.5256072282791138\n",
      "epoch 9654, loss 0.012325375340878963, R2 0.48768287897109985\n",
      "Eval loss 0.012315812520682812, R2 0.5256077647209167\n",
      "epoch 9655, loss 0.012325368821620941, R2 0.4876832962036133\n",
      "Eval loss 0.01231580413877964, R2 0.5256083011627197\n",
      "epoch 9656, loss 0.012325361371040344, R2 0.4876829981803894\n",
      "Eval loss 0.012315794825553894, R2 0.5256083011627197\n",
      "epoch 9657, loss 0.012325354851782322, R2 0.4876842498779297\n",
      "Eval loss 0.012315784581005573, R2 0.5256086587905884\n",
      "epoch 9658, loss 0.012325345538556576, R2 0.4876839518547058\n",
      "Eval loss 0.012315777130424976, R2 0.5256092548370361\n",
      "epoch 9659, loss 0.012325338087975979, R2 0.4876842498779297\n",
      "Eval loss 0.012315766885876656, R2 0.5256096124649048\n",
      "epoch 9660, loss 0.012325330637395382, R2 0.4876852035522461\n",
      "Eval loss 0.01231575571000576, R2 0.5256099700927734\n",
      "epoch 9661, loss 0.012325323186814785, R2 0.48768502473831177\n",
      "Eval loss 0.012315748259425163, R2 0.5256102085113525\n",
      "epoch 9662, loss 0.012325316667556763, R2 0.48768532276153564\n",
      "Eval loss 0.012315738946199417, R2 0.5256106853485107\n",
      "epoch 9663, loss 0.012325309216976166, R2 0.48768532276153564\n",
      "Eval loss 0.012315729632973671, R2 0.5256108045578003\n",
      "epoch 9664, loss 0.01232529990375042, R2 0.4876859784126282\n",
      "Eval loss 0.012315720319747925, R2 0.5256115198135376\n",
      "epoch 9665, loss 0.012325294315814972, R2 0.4876860976219177\n",
      "Eval loss 0.012315711937844753, R2 0.5256117582321167\n",
      "epoch 9666, loss 0.012325285002589226, R2 0.4876866340637207\n",
      "Eval loss 0.012315700761973858, R2 0.5256118774414062\n",
      "epoch 9667, loss 0.012325278483331203, R2 0.48768675327301025\n",
      "Eval loss 0.012315693311393261, R2 0.5256125926971436\n",
      "epoch 9668, loss 0.012325271032750607, R2 0.48768723011016846\n",
      "Eval loss 0.012315683998167515, R2 0.5256128311157227\n",
      "epoch 9669, loss 0.012325262650847435, R2 0.48768752813339233\n",
      "Eval loss 0.012315673753619194, R2 0.52561354637146\n",
      "epoch 9670, loss 0.012325255200266838, R2 0.48768818378448486\n",
      "Eval loss 0.012315665371716022, R2 0.5256134271621704\n",
      "epoch 9671, loss 0.012325247749686241, R2 0.48768800497055054\n",
      "Eval loss 0.012315656989812851, R2 0.5256140232086182\n",
      "epoch 9672, loss 0.012325241230428219, R2 0.48768848180770874\n",
      "Eval loss 0.012315645813941956, R2 0.5256142616271973\n",
      "epoch 9673, loss 0.012325232848525047, R2 0.48768913745880127\n",
      "Eval loss 0.012315637432038784, R2 0.5256145000457764\n",
      "epoch 9674, loss 0.01232522539794445, R2 0.48768895864486694\n",
      "Eval loss 0.012315629050135612, R2 0.5256149768829346\n",
      "epoch 9675, loss 0.012325216084718704, R2 0.4876899719238281\n",
      "Eval loss 0.012315620668232441, R2 0.5256152749061584\n",
      "epoch 9676, loss 0.012325210496783257, R2 0.487689733505249\n",
      "Eval loss 0.01231561042368412, R2 0.5256155133247375\n",
      "epoch 9677, loss 0.012325203977525234, R2 0.4876898527145386\n",
      "Eval loss 0.0123156001791358, R2 0.5256161689758301\n",
      "epoch 9678, loss 0.012325195595622063, R2 0.48769068717956543\n",
      "Eval loss 0.012315591797232628, R2 0.5256162881851196\n",
      "epoch 9679, loss 0.01232518907636404, R2 0.48769110441207886\n",
      "Eval loss 0.012315583415329456, R2 0.5256167054176331\n",
      "epoch 9680, loss 0.012325180694460869, R2 0.48769062757492065\n",
      "Eval loss 0.012315573170781136, R2 0.5256171226501465\n",
      "epoch 9681, loss 0.012325173243880272, R2 0.48769110441207886\n",
      "Eval loss 0.012315564788877964, R2 0.5256174206733704\n",
      "epoch 9682, loss 0.01232516672462225, R2 0.48769205808639526\n",
      "Eval loss 0.012315554544329643, R2 0.5256179571151733\n",
      "epoch 9683, loss 0.012325158342719078, R2 0.4876917600631714\n",
      "Eval loss 0.012315546162426472, R2 0.5256181359291077\n",
      "epoch 9684, loss 0.012325151823461056, R2 0.4876919984817505\n",
      "Eval loss 0.012315536849200726, R2 0.5256185531616211\n",
      "epoch 9685, loss 0.01232514251023531, R2 0.48769235610961914\n",
      "Eval loss 0.012315526604652405, R2 0.5256192684173584\n",
      "epoch 9686, loss 0.012325135990977287, R2 0.487692654132843\n",
      "Eval loss 0.012315519154071808, R2 0.5256191492080688\n",
      "epoch 9687, loss 0.01232512854039669, R2 0.4876931309700012\n",
      "Eval loss 0.012315510772168636, R2 0.5256197452545166\n",
      "epoch 9688, loss 0.012325122021138668, R2 0.4876934289932251\n",
      "Eval loss 0.012315502390265465, R2 0.5256199836730957\n",
      "epoch 9689, loss 0.012325114570558071, R2 0.48769354820251465\n",
      "Eval loss 0.012315493077039719, R2 0.5256202220916748\n",
      "epoch 9690, loss 0.0123251061886549, R2 0.4876939058303833\n",
      "Eval loss 0.012315483763813972, R2 0.5256205201148987\n",
      "epoch 9691, loss 0.012325099669396877, R2 0.4876946806907654\n",
      "Eval loss 0.012315473519265652, R2 0.5256211757659912\n",
      "epoch 9692, loss 0.01232509221881628, R2 0.48769497871398926\n",
      "Eval loss 0.012315466068685055, R2 0.525621235370636\n",
      "epoch 9693, loss 0.012325085699558258, R2 0.4876949191093445\n",
      "Eval loss 0.012315456755459309, R2 0.525621771812439\n",
      "epoch 9694, loss 0.012325077317655087, R2 0.48769545555114746\n",
      "Eval loss 0.012315446510910988, R2 0.5256216526031494\n",
      "epoch 9695, loss 0.01232506986707449, R2 0.487695574760437\n",
      "Eval loss 0.012315438129007816, R2 0.5256221294403076\n",
      "epoch 9696, loss 0.012325061485171318, R2 0.48769575357437134\n",
      "Eval loss 0.012315429747104645, R2 0.5256223678588867\n",
      "epoch 9697, loss 0.012325054965913296, R2 0.48769670724868774\n",
      "Eval loss 0.012315420433878899, R2 0.5256228446960449\n",
      "epoch 9698, loss 0.012325049377977848, R2 0.4876967668533325\n",
      "Eval loss 0.012315411120653152, R2 0.5256232023239136\n",
      "epoch 9699, loss 0.012325040064752102, R2 0.48769664764404297\n",
      "Eval loss 0.012315401807427406, R2 0.5256236791610718\n",
      "epoch 9700, loss 0.012325032614171505, R2 0.4876970052719116\n",
      "Eval loss 0.012315393425524235, R2 0.52562415599823\n",
      "epoch 9701, loss 0.012325025163590908, R2 0.48769742250442505\n",
      "Eval loss 0.012315385043621063, R2 0.5256245136260986\n",
      "epoch 9702, loss 0.012325017713010311, R2 0.4876976013183594\n",
      "Eval loss 0.012315375730395317, R2 0.5256245732307434\n",
      "epoch 9703, loss 0.012325011193752289, R2 0.4876980185508728\n",
      "Eval loss 0.012315366417169571, R2 0.5256252288818359\n",
      "epoch 9704, loss 0.012325004674494267, R2 0.48769813776016235\n",
      "Eval loss 0.012315355241298676, R2 0.5256256461143494\n",
      "epoch 9705, loss 0.012324996292591095, R2 0.4876989722251892\n",
      "Eval loss 0.012315348722040653, R2 0.5256257653236389\n",
      "epoch 9706, loss 0.012324987910687923, R2 0.4876987934112549\n",
      "Eval loss 0.012315340340137482, R2 0.5256258249282837\n",
      "epoch 9707, loss 0.012324981391429901, R2 0.48769956827163696\n",
      "Eval loss 0.012315329164266586, R2 0.5256264805793762\n",
      "epoch 9708, loss 0.012324973940849304, R2 0.4876992106437683\n",
      "Eval loss 0.012315320782363415, R2 0.5256269574165344\n",
      "epoch 9709, loss 0.012324966490268707, R2 0.4877002239227295\n",
      "Eval loss 0.012315311469137669, R2 0.5256271958351135\n",
      "epoch 9710, loss 0.01232495903968811, R2 0.48770004510879517\n",
      "Eval loss 0.012315304018557072, R2 0.5256273746490479\n",
      "epoch 9711, loss 0.012324952520430088, R2 0.48770028352737427\n",
      "Eval loss 0.012315294705331326, R2 0.525627851486206\n",
      "epoch 9712, loss 0.012324945069849491, R2 0.48770058155059814\n",
      "Eval loss 0.012315284460783005, R2 0.5256277918815613\n",
      "epoch 9713, loss 0.012324937619268894, R2 0.487700879573822\n",
      "Eval loss 0.012315277010202408, R2 0.5256286859512329\n",
      "epoch 9714, loss 0.012324931100010872, R2 0.48770153522491455\n",
      "Eval loss 0.012315267696976662, R2 0.5256285667419434\n",
      "epoch 9715, loss 0.01232492458075285, R2 0.4877015948295593\n",
      "Eval loss 0.01231525931507349, R2 0.5256295204162598\n",
      "epoch 9716, loss 0.012324917130172253, R2 0.48770177364349365\n",
      "Eval loss 0.01231524907052517, R2 0.5256296992301941\n",
      "epoch 9717, loss 0.012324908748269081, R2 0.48770207166671753\n",
      "Eval loss 0.012315240688621998, R2 0.5256297588348389\n",
      "epoch 9718, loss 0.012324901297688484, R2 0.48770254850387573\n",
      "Eval loss 0.012315231375396252, R2 0.5256304144859314\n",
      "epoch 9719, loss 0.012324894778430462, R2 0.48770254850387573\n",
      "Eval loss 0.01231522299349308, R2 0.5256304740905762\n",
      "epoch 9720, loss 0.01232488825917244, R2 0.48770296573638916\n",
      "Eval loss 0.012315214611589909, R2 0.5256308317184448\n",
      "epoch 9721, loss 0.012324879877269268, R2 0.4877033233642578\n",
      "Eval loss 0.012315204367041588, R2 0.5256310105323792\n",
      "epoch 9722, loss 0.012324873358011246, R2 0.4877036213874817\n",
      "Eval loss 0.012315195985138416, R2 0.525631308555603\n",
      "epoch 9723, loss 0.012324864976108074, R2 0.48770391941070557\n",
      "Eval loss 0.012315187603235245, R2 0.5256320834159851\n",
      "epoch 9724, loss 0.012324857525527477, R2 0.48770421743392944\n",
      "Eval loss 0.012315180152654648, R2 0.5256319642066956\n",
      "epoch 9725, loss 0.01232485007494688, R2 0.4877045750617981\n",
      "Eval loss 0.012315169908106327, R2 0.5256325006484985\n",
      "epoch 9726, loss 0.012324843555688858, R2 0.4877054691314697\n",
      "Eval loss 0.012315160594880581, R2 0.5256327390670776\n",
      "epoch 9727, loss 0.012324837036430836, R2 0.4877051115036011\n",
      "Eval loss 0.012315151281654835, R2 0.5256332159042358\n",
      "epoch 9728, loss 0.012324828654527664, R2 0.48770540952682495\n",
      "Eval loss 0.012315141968429089, R2 0.5256339907646179\n",
      "epoch 9729, loss 0.012324822135269642, R2 0.48770570755004883\n",
      "Eval loss 0.012315132655203342, R2 0.5256340503692627\n",
      "epoch 9730, loss 0.012324814684689045, R2 0.4877060055732727\n",
      "Eval loss 0.012315125204622746, R2 0.5256343483924866\n",
      "epoch 9731, loss 0.012324808165431023, R2 0.4877063035964966\n",
      "Eval loss 0.012315116822719574, R2 0.5256349444389343\n",
      "epoch 9732, loss 0.012324800714850426, R2 0.48770660161972046\n",
      "Eval loss 0.012315108440816402, R2 0.525634765625\n",
      "epoch 9733, loss 0.012324793264269829, R2 0.4877069592475891\n",
      "Eval loss 0.012315098196268082, R2 0.5256355404853821\n",
      "epoch 9734, loss 0.012324784882366657, R2 0.4877074360847473\n",
      "Eval loss 0.01231508981436491, R2 0.5256356000900269\n",
      "epoch 9735, loss 0.01232477743178606, R2 0.48770755529403687\n",
      "Eval loss 0.01231507956981659, R2 0.5256360769271851\n",
      "epoch 9736, loss 0.012324771843850613, R2 0.48770779371261597\n",
      "Eval loss 0.012315071187913418, R2 0.5256364345550537\n",
      "epoch 9737, loss 0.012324764393270016, R2 0.4877086281776428\n",
      "Eval loss 0.012315063737332821, R2 0.5256364345550537\n",
      "epoch 9738, loss 0.012324757874011993, R2 0.4877092242240906\n",
      "Eval loss 0.0123150534927845, R2 0.5256370902061462\n",
      "epoch 9739, loss 0.012324749492108822, R2 0.4877092242240906\n",
      "Eval loss 0.012315045110881329, R2 0.5256373882293701\n",
      "epoch 9740, loss 0.0123247429728508, R2 0.4877091646194458\n",
      "Eval loss 0.012315037660300732, R2 0.5256374478340149\n",
      "epoch 9741, loss 0.012324735522270203, R2 0.48770928382873535\n",
      "Eval loss 0.012315028347074986, R2 0.5256379842758179\n",
      "epoch 9742, loss 0.01232472900301218, R2 0.4877094030380249\n",
      "Eval loss 0.01231501903384924, R2 0.5256383419036865\n",
      "epoch 9743, loss 0.012324722483754158, R2 0.4877097010612488\n",
      "Eval loss 0.012315009720623493, R2 0.5256385207176208\n",
      "epoch 9744, loss 0.012324714101850986, R2 0.4877103567123413\n",
      "Eval loss 0.012315000407397747, R2 0.5256388783454895\n",
      "epoch 9745, loss 0.01232470665127039, R2 0.48771047592163086\n",
      "Eval loss 0.01231499295681715, R2 0.5256391763687134\n",
      "epoch 9746, loss 0.012324699200689793, R2 0.4877108335494995\n",
      "Eval loss 0.012314984574913979, R2 0.5256397724151611\n",
      "epoch 9747, loss 0.01232469268143177, R2 0.48771125078201294\n",
      "Eval loss 0.012314974330365658, R2 0.5256398916244507\n",
      "epoch 9748, loss 0.012324684299528599, R2 0.4877116084098816\n",
      "Eval loss 0.012314965948462486, R2 0.5256403684616089\n",
      "epoch 9749, loss 0.012324678711593151, R2 0.4877113699913025\n",
      "Eval loss 0.012314957566559315, R2 0.5256409645080566\n",
      "epoch 9750, loss 0.012324672192335129, R2 0.48771196603775024\n",
      "Eval loss 0.012314950115978718, R2 0.5256410837173462\n",
      "epoch 9751, loss 0.012324663810431957, R2 0.48771214485168457\n",
      "Eval loss 0.012314939871430397, R2 0.525641679763794\n",
      "epoch 9752, loss 0.01232465635985136, R2 0.4877127408981323\n",
      "Eval loss 0.012314930558204651, R2 0.5256419777870178\n",
      "epoch 9753, loss 0.012324649840593338, R2 0.4877128601074219\n",
      "Eval loss 0.01231492217630148, R2 0.525641918182373\n",
      "epoch 9754, loss 0.012324642390012741, R2 0.4877133369445801\n",
      "Eval loss 0.012314913794398308, R2 0.5256427526473999\n",
      "epoch 9755, loss 0.012324635870754719, R2 0.48771363496780396\n",
      "Eval loss 0.012314905412495136, R2 0.525642991065979\n",
      "epoch 9756, loss 0.012324626557528973, R2 0.4877142906188965\n",
      "Eval loss 0.012314897030591965, R2 0.525642991065979\n",
      "epoch 9757, loss 0.012324620969593525, R2 0.4877140522003174\n",
      "Eval loss 0.012314886786043644, R2 0.5256437063217163\n",
      "epoch 9758, loss 0.012324614450335503, R2 0.48771435022354126\n",
      "Eval loss 0.012314878404140472, R2 0.5256438255310059\n",
      "epoch 9759, loss 0.012324606068432331, R2 0.4877145290374756\n",
      "Eval loss 0.012314870953559875, R2 0.5256441831588745\n",
      "epoch 9760, loss 0.012324600480496883, R2 0.48771512508392334\n",
      "Eval loss 0.012314862571656704, R2 0.5256444811820984\n",
      "epoch 9761, loss 0.012324593029916286, R2 0.4877152442932129\n",
      "Eval loss 0.012314853258430958, R2 0.5256446599960327\n",
      "epoch 9762, loss 0.01232458557933569, R2 0.48771554231643677\n",
      "Eval loss 0.012314843945205212, R2 0.5256450176239014\n",
      "epoch 9763, loss 0.012324579060077667, R2 0.4877156615257263\n",
      "Eval loss 0.01231483556330204, R2 0.5256451368331909\n",
      "epoch 9764, loss 0.01232457160949707, R2 0.48771679401397705\n",
      "Eval loss 0.01231482531875372, R2 0.5256460905075073\n",
      "epoch 9765, loss 0.012324565090239048, R2 0.48771655559539795\n",
      "Eval loss 0.012314818799495697, R2 0.5256463289260864\n",
      "epoch 9766, loss 0.012324557639658451, R2 0.4877163767814636\n",
      "Eval loss 0.012314808554947376, R2 0.5256465673446655\n",
      "epoch 9767, loss 0.012324550189077854, R2 0.4877169728279114\n",
      "Eval loss 0.01231479924172163, R2 0.5256469249725342\n",
      "epoch 9768, loss 0.012324543669819832, R2 0.4877174496650696\n",
      "Eval loss 0.012314791791141033, R2 0.5256475210189819\n",
      "epoch 9769, loss 0.012324536219239235, R2 0.48771774768829346\n",
      "Eval loss 0.012314784340560436, R2 0.5256478786468506\n",
      "epoch 9770, loss 0.012324529699981213, R2 0.4877183437347412\n",
      "Eval loss 0.012314774096012115, R2 0.5256479978561401\n",
      "epoch 9771, loss 0.012324522249400616, R2 0.48771870136260986\n",
      "Eval loss 0.012314765714108944, R2 0.5256483554840088\n",
      "epoch 9772, loss 0.012324516661465168, R2 0.4877185821533203\n",
      "Eval loss 0.012314756400883198, R2 0.5256487131118774\n",
      "epoch 9773, loss 0.012324508279561996, R2 0.48771876096725464\n",
      "Eval loss 0.012314748018980026, R2 0.5256491899490356\n",
      "epoch 9774, loss 0.0123245008289814, R2 0.4877190589904785\n",
      "Eval loss 0.012314741499722004, R2 0.5256491899490356\n",
      "epoch 9775, loss 0.012324494309723377, R2 0.4877201318740845\n",
      "Eval loss 0.012314731255173683, R2 0.5256496667861938\n",
      "epoch 9776, loss 0.012324487790465355, R2 0.4877197742462158\n",
      "Eval loss 0.012314722873270512, R2 0.5256498456001282\n",
      "epoch 9777, loss 0.012324480339884758, R2 0.4877197742462158\n",
      "Eval loss 0.01231471449136734, R2 0.525650143623352\n",
      "epoch 9778, loss 0.012324472889304161, R2 0.4877207279205322\n",
      "Eval loss 0.012314706109464169, R2 0.5256503820419312\n",
      "epoch 9779, loss 0.012324465438723564, R2 0.4877210259437561\n",
      "Eval loss 0.012314695864915848, R2 0.5256513357162476\n",
      "epoch 9780, loss 0.012324458919465542, R2 0.4877212643623352\n",
      "Eval loss 0.01231468841433525, R2 0.5256508588790894\n",
      "epoch 9781, loss 0.012324453331530094, R2 0.4877215027809143\n",
      "Eval loss 0.01231468003243208, R2 0.5256515145301819\n",
      "epoch 9782, loss 0.012324444949626923, R2 0.4877215623855591\n",
      "Eval loss 0.012314671650528908, R2 0.5256521105766296\n",
      "epoch 9783, loss 0.0123244384303689, R2 0.48772168159484863\n",
      "Eval loss 0.012314663268625736, R2 0.5256521701812744\n",
      "epoch 9784, loss 0.012324430979788303, R2 0.48772144317626953\n",
      "Eval loss 0.012314653024077415, R2 0.5256525278091431\n",
      "epoch 9785, loss 0.012324424460530281, R2 0.48772257566452026\n",
      "Eval loss 0.012314645573496819, R2 0.5256525278091431\n",
      "epoch 9786, loss 0.012324417941272259, R2 0.48772233724594116\n",
      "Eval loss 0.012314636260271072, R2 0.5256531834602356\n",
      "epoch 9787, loss 0.012324411422014236, R2 0.48772329092025757\n",
      "Eval loss 0.0123146278783679, R2 0.5256538391113281\n",
      "epoch 9788, loss 0.012324403040111065, R2 0.487723171710968\n",
      "Eval loss 0.012314621359109879, R2 0.5256537795066833\n",
      "epoch 9789, loss 0.012324396520853043, R2 0.48772358894348145\n",
      "Eval loss 0.012314610183238983, R2 0.5256538987159729\n",
      "epoch 9790, loss 0.01232439000159502, R2 0.4877236485481262\n",
      "Eval loss 0.012314601801335812, R2 0.5256547927856445\n",
      "epoch 9791, loss 0.012324381619691849, R2 0.4877240061759949\n",
      "Eval loss 0.012314594350755215, R2 0.5256549119949341\n",
      "epoch 9792, loss 0.012324376031756401, R2 0.4877249002456665\n",
      "Eval loss 0.012314585037529469, R2 0.5256551504135132\n",
      "epoch 9793, loss 0.012324369512498379, R2 0.4877247214317322\n",
      "Eval loss 0.012314575724303722, R2 0.5256555080413818\n",
      "epoch 9794, loss 0.012324362993240356, R2 0.4877246618270874\n",
      "Eval loss 0.01231456734240055, R2 0.52565598487854\n",
      "epoch 9795, loss 0.012324354611337185, R2 0.4877256751060486\n",
      "Eval loss 0.01231455896049738, R2 0.5256558656692505\n",
      "epoch 9796, loss 0.012324348092079163, R2 0.4877259135246277\n",
      "Eval loss 0.012314552441239357, R2 0.5256561040878296\n",
      "epoch 9797, loss 0.012324340641498566, R2 0.48772573471069336\n",
      "Eval loss 0.01231454312801361, R2 0.525656521320343\n",
      "epoch 9798, loss 0.012324334122240543, R2 0.48772597312927246\n",
      "Eval loss 0.01231453474611044, R2 0.525657057762146\n",
      "epoch 9799, loss 0.012324326671659946, R2 0.4877268075942993\n",
      "Eval loss 0.012314525432884693, R2 0.525657594203949\n",
      "epoch 9800, loss 0.012324321083724499, R2 0.4877268671989441\n",
      "Eval loss 0.012314516119658947, R2 0.5256579518318176\n",
      "epoch 9801, loss 0.012324314564466476, R2 0.48772716522216797\n",
      "Eval loss 0.012314509600400925, R2 0.5256577730178833\n",
      "epoch 9802, loss 0.012324306182563305, R2 0.4877272844314575\n",
      "Eval loss 0.012314499355852604, R2 0.525658369064331\n",
      "epoch 9803, loss 0.012324300594627857, R2 0.48772740364074707\n",
      "Eval loss 0.012314491905272007, R2 0.5256587266921997\n",
      "epoch 9804, loss 0.012324292212724686, R2 0.48772740364074707\n",
      "Eval loss 0.012314483523368835, R2 0.5256589651107788\n",
      "epoch 9805, loss 0.012324286624789238, R2 0.4877279996871948\n",
      "Eval loss 0.012314475141465664, R2 0.525659441947937\n",
      "epoch 9806, loss 0.012324279174208641, R2 0.48772895336151123\n",
      "Eval loss 0.012314466759562492, R2 0.5256597399711609\n",
      "epoch 9807, loss 0.012324271723628044, R2 0.4877285957336426\n",
      "Eval loss 0.012314457446336746, R2 0.5256606340408325\n",
      "epoch 9808, loss 0.012324265204370022, R2 0.4877288341522217\n",
      "Eval loss 0.012314449064433575, R2 0.525660514831543\n",
      "epoch 9809, loss 0.012324257753789425, R2 0.48772919178009033\n",
      "Eval loss 0.012314440682530403, R2 0.5256604552268982\n",
      "epoch 9810, loss 0.012324251234531403, R2 0.48772943019866943\n",
      "Eval loss 0.012314432300627232, R2 0.525661051273346\n",
      "epoch 9811, loss 0.01232424471527338, R2 0.4877297282218933\n",
      "Eval loss 0.012314424850046635, R2 0.5256613492965698\n",
      "epoch 9812, loss 0.012324238196015358, R2 0.48773014545440674\n",
      "Eval loss 0.012314415536820889, R2 0.5256617069244385\n",
      "epoch 9813, loss 0.012324230745434761, R2 0.4877302646636963\n",
      "Eval loss 0.012314406223595142, R2 0.5256623029708862\n",
      "epoch 9814, loss 0.012324223294854164, R2 0.4877312183380127\n",
      "Eval loss 0.012314398773014545, R2 0.5256623029708862\n",
      "epoch 9815, loss 0.012324217706918716, R2 0.4877309799194336\n",
      "Eval loss 0.012314390391111374, R2 0.5256624221801758\n",
      "epoch 9816, loss 0.01232421025633812, R2 0.48773133754730225\n",
      "Eval loss 0.012314381077885628, R2 0.5256631374359131\n",
      "epoch 9817, loss 0.012324203737080097, R2 0.48773157596588135\n",
      "Eval loss 0.012314372695982456, R2 0.5256634950637817\n",
      "epoch 9818, loss 0.0123241962864995, R2 0.4877324104309082\n",
      "Eval loss 0.012314364314079285, R2 0.5256636738777161\n",
      "epoch 9819, loss 0.012324189767241478, R2 0.4877321720123291\n",
      "Eval loss 0.012314355000853539, R2 0.525664210319519\n",
      "epoch 9820, loss 0.012324183247983456, R2 0.4877324104309082\n",
      "Eval loss 0.012314347550272942, R2 0.5256644487380981\n",
      "epoch 9821, loss 0.012324176728725433, R2 0.48773300647735596\n",
      "Eval loss 0.012314338237047195, R2 0.5256646871566772\n",
      "epoch 9822, loss 0.012324169278144836, R2 0.48773282766342163\n",
      "Eval loss 0.012314329855144024, R2 0.5256645679473877\n",
      "epoch 9823, loss 0.012324163690209389, R2 0.48773276805877686\n",
      "Eval loss 0.012314322404563427, R2 0.525665283203125\n",
      "epoch 9824, loss 0.012324156239628792, R2 0.4877334237098694\n",
      "Eval loss 0.012314314022660255, R2 0.5256653428077698\n",
      "epoch 9825, loss 0.012324148789048195, R2 0.4877336621284485\n",
      "Eval loss 0.012314306572079659, R2 0.5256656408309937\n",
      "epoch 9826, loss 0.012324142269790173, R2 0.48773396015167236\n",
      "Eval loss 0.012314296327531338, R2 0.5256661176681519\n",
      "epoch 9827, loss 0.012324134819209576, R2 0.48773425817489624\n",
      "Eval loss 0.01231428887695074, R2 0.5256665945053101\n",
      "epoch 9828, loss 0.012324129231274128, R2 0.48773449659347534\n",
      "Eval loss 0.012314279563724995, R2 0.5256666541099548\n",
      "epoch 9829, loss 0.01232412364333868, R2 0.487735390663147\n",
      "Eval loss 0.012314271181821823, R2 0.5256677865982056\n",
      "epoch 9830, loss 0.01232411339879036, R2 0.48773515224456787\n",
      "Eval loss 0.0123142646625638, R2 0.5256673693656921\n",
      "epoch 9831, loss 0.012324107810854912, R2 0.487735390663147\n",
      "Eval loss 0.012314255349338055, R2 0.5256675481796265\n",
      "epoch 9832, loss 0.01232410129159689, R2 0.48773568868637085\n",
      "Eval loss 0.012314246036112309, R2 0.5256683826446533\n",
      "epoch 9833, loss 0.012324094772338867, R2 0.48773592710494995\n",
      "Eval loss 0.012314239516854286, R2 0.5256685018539429\n",
      "epoch 9834, loss 0.01232408732175827, R2 0.48773694038391113\n",
      "Eval loss 0.01231423020362854, R2 0.5256688594818115\n",
      "epoch 9835, loss 0.012324081733822823, R2 0.4877365231513977\n",
      "Eval loss 0.012314222753047943, R2 0.5256688594818115\n",
      "epoch 9836, loss 0.012324073351919651, R2 0.4877369999885559\n",
      "Eval loss 0.012314214371144772, R2 0.525669276714325\n",
      "epoch 9837, loss 0.012324065901339054, R2 0.48773759603500366\n",
      "Eval loss 0.012314205057919025, R2 0.5256699323654175\n",
      "epoch 9838, loss 0.012324061244726181, R2 0.48773735761642456\n",
      "Eval loss 0.012314196676015854, R2 0.5256702899932861\n",
      "epoch 9839, loss 0.012324053794145584, R2 0.48773765563964844\n",
      "Eval loss 0.012314189225435257, R2 0.5256705284118652\n",
      "epoch 9840, loss 0.012324047274887562, R2 0.4877379536628723\n",
      "Eval loss 0.01231417991220951, R2 0.5256708860397339\n",
      "epoch 9841, loss 0.012324041686952114, R2 0.4877381920814514\n",
      "Eval loss 0.012314172461628914, R2 0.5256711840629578\n",
      "epoch 9842, loss 0.012324033305048943, R2 0.48773831129074097\n",
      "Eval loss 0.012314163148403168, R2 0.5256717205047607\n",
      "epoch 9843, loss 0.012324027717113495, R2 0.4877387285232544\n",
      "Eval loss 0.01231415569782257, R2 0.5256717205047607\n",
      "epoch 9844, loss 0.012324020266532898, R2 0.4877393841743469\n",
      "Eval loss 0.0123141473159194, R2 0.5256718993186951\n",
      "epoch 9845, loss 0.01232401467859745, R2 0.4877394437789917\n",
      "Eval loss 0.012314138934016228, R2 0.5256723165512085\n",
      "epoch 9846, loss 0.012324006296694279, R2 0.48773980140686035\n",
      "Eval loss 0.012314130552113056, R2 0.5256725549697876\n",
      "epoch 9847, loss 0.012324000708758831, R2 0.48774003982543945\n",
      "Eval loss 0.01231412123888731, R2 0.5256730318069458\n",
      "epoch 9848, loss 0.012323993258178234, R2 0.48774033784866333\n",
      "Eval loss 0.012314114719629288, R2 0.5256730318069458\n",
      "epoch 9849, loss 0.012323987670242786, R2 0.4877403974533081\n",
      "Eval loss 0.012314105406403542, R2 0.5256739258766174\n",
      "epoch 9850, loss 0.01232397835701704, R2 0.4877409338951111\n",
      "Eval loss 0.012314096093177795, R2 0.5256744027137756\n",
      "epoch 9851, loss 0.012323972769081593, R2 0.4877411723136902\n",
      "Eval loss 0.012314088642597198, R2 0.5256744027137756\n",
      "epoch 9852, loss 0.012323967181146145, R2 0.48774123191833496\n",
      "Eval loss 0.012314081192016602, R2 0.52567458152771\n",
      "epoch 9853, loss 0.012323959730565548, R2 0.4877415895462036\n",
      "Eval loss 0.01231407281011343, R2 0.5256749391555786\n",
      "epoch 9854, loss 0.012323952279984951, R2 0.4877423644065857\n",
      "Eval loss 0.012314065359532833, R2 0.5256752967834473\n",
      "epoch 9855, loss 0.012323945760726929, R2 0.4877421259880066\n",
      "Eval loss 0.012314055114984512, R2 0.5256760120391846\n",
      "epoch 9856, loss 0.012323940172791481, R2 0.48774218559265137\n",
      "Eval loss 0.012314047664403915, R2 0.525675892829895\n",
      "epoch 9857, loss 0.012323932722210884, R2 0.4877428412437439\n",
      "Eval loss 0.012314040213823318, R2 0.5256761312484741\n",
      "epoch 9858, loss 0.012323928065598011, R2 0.4877435564994812\n",
      "Eval loss 0.012314031831920147, R2 0.5256764888763428\n",
      "epoch 9859, loss 0.012323920615017414, R2 0.4877435564994812\n",
      "Eval loss 0.0123140225186944, R2 0.525676965713501\n",
      "epoch 9860, loss 0.012323913164436817, R2 0.4877434968948364\n",
      "Eval loss 0.01231401413679123, R2 0.5256773233413696\n",
      "epoch 9861, loss 0.012323906645178795, R2 0.48774391412734985\n",
      "Eval loss 0.012314005754888058, R2 0.5256772041320801\n",
      "epoch 9862, loss 0.012323900125920773, R2 0.4877445101737976\n",
      "Eval loss 0.012313997372984886, R2 0.5256778001785278\n",
      "epoch 9863, loss 0.012323892675340176, R2 0.4877443313598633\n",
      "Eval loss 0.012313990853726864, R2 0.5256785154342651\n",
      "epoch 9864, loss 0.012323887087404728, R2 0.48774492740631104\n",
      "Eval loss 0.012313980609178543, R2 0.5256785750389099\n",
      "epoch 9865, loss 0.012323879636824131, R2 0.4877450466156006\n",
      "Eval loss 0.012313972227275372, R2 0.5256788730621338\n",
      "epoch 9866, loss 0.012323872186243534, R2 0.48774516582489014\n",
      "Eval loss 0.01231396570801735, R2 0.5256789922714233\n",
      "epoch 9867, loss 0.012323866598308086, R2 0.48774540424346924\n",
      "Eval loss 0.012313957326114178, R2 0.525679349899292\n",
      "epoch 9868, loss 0.012323860079050064, R2 0.4877457022666931\n",
      "Eval loss 0.012313948012888432, R2 0.5256797075271606\n",
      "epoch 9869, loss 0.012323852628469467, R2 0.4877457022666931\n",
      "Eval loss 0.012313940562307835, R2 0.5256798267364502\n",
      "epoch 9870, loss 0.01232384704053402, R2 0.4877467155456543\n",
      "Eval loss 0.012313932180404663, R2 0.5256803035736084\n",
      "epoch 9871, loss 0.012323840521275997, R2 0.4877470135688782\n",
      "Eval loss 0.012313922867178917, R2 0.525680661201477\n",
      "epoch 9872, loss 0.0123238330706954, R2 0.4877476096153259\n",
      "Eval loss 0.01231391727924347, R2 0.5256810188293457\n",
      "epoch 9873, loss 0.012323826551437378, R2 0.48774707317352295\n",
      "Eval loss 0.012313908897340298, R2 0.5256808996200562\n",
      "epoch 9874, loss 0.012323820032179356, R2 0.48774755001068115\n",
      "Eval loss 0.012313898652791977, R2 0.5256816148757935\n",
      "epoch 9875, loss 0.012323814444243908, R2 0.4877476096153259\n",
      "Eval loss 0.012313892133533955, R2 0.5256815552711487\n",
      "epoch 9876, loss 0.012323806993663311, R2 0.4877479076385498\n",
      "Eval loss 0.012313882820308208, R2 0.5256824493408203\n",
      "epoch 9877, loss 0.012323801405727863, R2 0.48774832487106323\n",
      "Eval loss 0.012313875369727612, R2 0.5256825089454651\n",
      "epoch 9878, loss 0.012323794886469841, R2 0.48774856328964233\n",
      "Eval loss 0.01231386698782444, R2 0.5256829857826233\n",
      "epoch 9879, loss 0.01232378650456667, R2 0.487748920917511\n",
      "Eval loss 0.012313858605921268, R2 0.5256831645965576\n",
      "epoch 9880, loss 0.012323780916631222, R2 0.4877491593360901\n",
      "Eval loss 0.012313849292695522, R2 0.5256835222244263\n",
      "epoch 9881, loss 0.01232377253472805, R2 0.4877493381500244\n",
      "Eval loss 0.0123138427734375, R2 0.5256839990615845\n",
      "epoch 9882, loss 0.012323767878115177, R2 0.48775017261505127\n",
      "Eval loss 0.012313833460211754, R2 0.525684118270874\n",
      "epoch 9883, loss 0.012323761358857155, R2 0.48775047063827515\n",
      "Eval loss 0.012313826009631157, R2 0.5256845951080322\n",
      "epoch 9884, loss 0.012323752976953983, R2 0.4877502918243408\n",
      "Eval loss 0.01231381855905056, R2 0.5256843566894531\n",
      "epoch 9885, loss 0.012323746457695961, R2 0.487750768661499\n",
      "Eval loss 0.012313811108469963, R2 0.5256849527359009\n",
      "epoch 9886, loss 0.012323741801083088, R2 0.4877505898475647\n",
      "Eval loss 0.012313800863921642, R2 0.5256853699684143\n",
      "epoch 9887, loss 0.012323735281825066, R2 0.4877508878707886\n",
      "Eval loss 0.012313793413341045, R2 0.5256856679916382\n",
      "epoch 9888, loss 0.012323727831244469, R2 0.4877510666847229\n",
      "Eval loss 0.012313785962760448, R2 0.5256857872009277\n",
      "epoch 9889, loss 0.012323721311986446, R2 0.48775148391723633\n",
      "Eval loss 0.012313776649534702, R2 0.5256860256195068\n",
      "epoch 9890, loss 0.012323714792728424, R2 0.48775172233581543\n",
      "Eval loss 0.01231377013027668, R2 0.5256862640380859\n",
      "epoch 9891, loss 0.012323708273470402, R2 0.4877520203590393\n",
      "Eval loss 0.012313760817050934, R2 0.5256869196891785\n",
      "epoch 9892, loss 0.012323702685534954, R2 0.4877522587776184\n",
      "Eval loss 0.012313753366470337, R2 0.5256872177124023\n",
      "epoch 9893, loss 0.012323696166276932, R2 0.48775237798690796\n",
      "Eval loss 0.012313744984567165, R2 0.5256872177124023\n",
      "epoch 9894, loss 0.012323688715696335, R2 0.48775315284729004\n",
      "Eval loss 0.012313736602663994, R2 0.525688111782074\n",
      "epoch 9895, loss 0.012323682196438313, R2 0.48775309324264526\n",
      "Eval loss 0.012313729152083397, R2 0.525688111782074\n",
      "epoch 9896, loss 0.012323674745857716, R2 0.48775339126586914\n",
      "Eval loss 0.0123137217015028, R2 0.5256885290145874\n",
      "epoch 9897, loss 0.012323669157922268, R2 0.48775380849838257\n",
      "Eval loss 0.012313712388277054, R2 0.5256890654563904\n",
      "epoch 9898, loss 0.012323662638664246, R2 0.4877539277076721\n",
      "Eval loss 0.012313705869019032, R2 0.5256891846656799\n",
      "epoch 9899, loss 0.012323656119406223, R2 0.4877541661262512\n",
      "Eval loss 0.012313696555793285, R2 0.5256893634796143\n",
      "epoch 9900, loss 0.012323650531470776, R2 0.4877544045448303\n",
      "Eval loss 0.012313690036535263, R2 0.5256896018981934\n",
      "epoch 9901, loss 0.012323643080890179, R2 0.487754762172699\n",
      "Eval loss 0.012313679791986942, R2 0.5256901979446411\n",
      "epoch 9902, loss 0.012323636561632156, R2 0.4877550005912781\n",
      "Eval loss 0.012313672341406345, R2 0.5256900787353516\n",
      "epoch 9903, loss 0.012323630042374134, R2 0.48775607347488403\n",
      "Eval loss 0.012313664890825748, R2 0.5256906151771545\n",
      "epoch 9904, loss 0.012323623523116112, R2 0.48775553703308105\n",
      "Eval loss 0.012313655577600002, R2 0.5256909132003784\n",
      "epoch 9905, loss 0.01232361700385809, R2 0.48775583505630493\n",
      "Eval loss 0.01231364905834198, R2 0.5256912112236023\n",
      "epoch 9906, loss 0.012323610484600067, R2 0.48775655031204224\n",
      "Eval loss 0.012313640676438808, R2 0.5256912708282471\n",
      "epoch 9907, loss 0.01232360489666462, R2 0.48775631189346313\n",
      "Eval loss 0.012313633225858212, R2 0.5256919860839844\n",
      "epoch 9908, loss 0.012323596514761448, R2 0.48775678873062134\n",
      "Eval loss 0.01231362484395504, R2 0.5256919860839844\n",
      "epoch 9909, loss 0.012323590926826, R2 0.48775702714920044\n",
      "Eval loss 0.012313616462051868, R2 0.5256925821304321\n",
      "epoch 9910, loss 0.012323583476245403, R2 0.4877578616142273\n",
      "Eval loss 0.012313608080148697, R2 0.5256933569908142\n",
      "epoch 9911, loss 0.012323577888309956, R2 0.48775774240493774\n",
      "Eval loss 0.0123136006295681, R2 0.5256932377815247\n",
      "epoch 9912, loss 0.012323572300374508, R2 0.48775768280029297\n",
      "Eval loss 0.012313593178987503, R2 0.5256935358047485\n",
      "epoch 9913, loss 0.012323564849793911, R2 0.48775798082351685\n",
      "Eval loss 0.012313583865761757, R2 0.525693416595459\n",
      "epoch 9914, loss 0.012323560193181038, R2 0.4877580404281616\n",
      "Eval loss 0.012313575483858585, R2 0.5256941914558411\n",
      "epoch 9915, loss 0.012323551811277866, R2 0.48775917291641235\n",
      "Eval loss 0.012313568033277988, R2 0.5256941914558411\n",
      "epoch 9916, loss 0.012323545292019844, R2 0.4877587556838989\n",
      "Eval loss 0.012313560582697392, R2 0.5256943702697754\n",
      "epoch 9917, loss 0.012323540635406971, R2 0.48775964975357056\n",
      "Eval loss 0.01231355220079422, R2 0.5256949663162231\n",
      "epoch 9918, loss 0.012323533184826374, R2 0.48775947093963623\n",
      "Eval loss 0.012313545681536198, R2 0.5256950855255127\n",
      "epoch 9919, loss 0.012323527596890926, R2 0.4877600073814392\n",
      "Eval loss 0.012313535436987877, R2 0.5256958603858948\n",
      "epoch 9920, loss 0.012323521077632904, R2 0.4877598285675049\n",
      "Eval loss 0.01231352798640728, R2 0.5256959199905396\n",
      "epoch 9921, loss 0.012323514558374882, R2 0.48776018619537354\n",
      "Eval loss 0.012313520535826683, R2 0.5256959199905396\n",
      "epoch 9922, loss 0.012323507107794285, R2 0.48776018619537354\n",
      "Eval loss 0.012313512153923512, R2 0.5256966352462769\n",
      "epoch 9923, loss 0.012323500588536263, R2 0.48776113986968994\n",
      "Eval loss 0.01231350563466549, R2 0.5256966352462769\n",
      "epoch 9924, loss 0.01232349593192339, R2 0.4877610206604004\n",
      "Eval loss 0.012313495390117168, R2 0.5256971120834351\n",
      "epoch 9925, loss 0.012323489412665367, R2 0.48776113986968994\n",
      "Eval loss 0.012313488870859146, R2 0.5256972312927246\n",
      "epoch 9926, loss 0.012323482893407345, R2 0.4877612590789795\n",
      "Eval loss 0.012313480488955975, R2 0.5256975889205933\n",
      "epoch 9927, loss 0.012323476374149323, R2 0.4877614974975586\n",
      "Eval loss 0.012313473038375378, R2 0.5256977081298828\n",
      "epoch 9928, loss 0.012323468923568726, R2 0.4877619743347168\n",
      "Eval loss 0.012313463725149632, R2 0.5256983041763306\n",
      "epoch 9929, loss 0.012323462404310703, R2 0.4877626895904541\n",
      "Eval loss 0.01231345720589161, R2 0.5256983041763306\n",
      "epoch 9930, loss 0.012323455885052681, R2 0.4877625107765198\n",
      "Eval loss 0.012313449755311012, R2 0.5256991386413574\n",
      "epoch 9931, loss 0.012323450297117233, R2 0.4877627491950989\n",
      "Eval loss 0.01231344137340784, R2 0.5256987810134888\n",
      "epoch 9932, loss 0.012323444709181786, R2 0.4877634644508362\n",
      "Eval loss 0.012313433922827244, R2 0.525699257850647\n",
      "epoch 9933, loss 0.012323438189923763, R2 0.4877632260322571\n",
      "Eval loss 0.012313424609601498, R2 0.5256995558738708\n",
      "epoch 9934, loss 0.012323430739343166, R2 0.48776352405548096\n",
      "Eval loss 0.0123134171590209, R2 0.5257004499435425\n",
      "epoch 9935, loss 0.012323425151407719, R2 0.4877644181251526\n",
      "Eval loss 0.012313409708440304, R2 0.5257004499435425\n",
      "epoch 9936, loss 0.012323418632149696, R2 0.48776423931121826\n",
      "Eval loss 0.012313400395214558, R2 0.5257008075714111\n",
      "epoch 9937, loss 0.012323413044214249, R2 0.48776477575302124\n",
      "Eval loss 0.012313393875956535, R2 0.5257010459899902\n",
      "epoch 9938, loss 0.012323405593633652, R2 0.4877645969390869\n",
      "Eval loss 0.01231338456273079, R2 0.5257012844085693\n",
      "epoch 9939, loss 0.012323398143053055, R2 0.4877648949623108\n",
      "Eval loss 0.012313378043472767, R2 0.525701642036438\n",
      "epoch 9940, loss 0.012323392555117607, R2 0.48776543140411377\n",
      "Eval loss 0.012313369661569595, R2 0.5257019996643066\n",
      "epoch 9941, loss 0.012323386035859585, R2 0.48776525259017944\n",
      "Eval loss 0.012313362210988998, R2 0.5257019996643066\n",
      "epoch 9942, loss 0.012323380447924137, R2 0.4877662658691406\n",
      "Eval loss 0.012313353829085827, R2 0.5257023572921753\n",
      "epoch 9943, loss 0.01232337485998869, R2 0.4877660274505615\n",
      "Eval loss 0.012313345447182655, R2 0.5257030725479126\n",
      "epoch 9944, loss 0.012323368340730667, R2 0.4877665042877197\n",
      "Eval loss 0.012313338927924633, R2 0.5257030129432678\n",
      "epoch 9945, loss 0.01232336089015007, R2 0.4877666234970093\n",
      "Eval loss 0.012313330546021461, R2 0.525703489780426\n",
      "epoch 9946, loss 0.012323354370892048, R2 0.4877668619155884\n",
      "Eval loss 0.012313323095440865, R2 0.5257037878036499\n",
      "epoch 9947, loss 0.012323346920311451, R2 0.48776715993881226\n",
      "Eval loss 0.012313314713537693, R2 0.5257041454315186\n",
      "epoch 9948, loss 0.012323342263698578, R2 0.4877675771713257\n",
      "Eval loss 0.012313305400311947, R2 0.5257044434547424\n",
      "epoch 9949, loss 0.012323335744440556, R2 0.4877675175666809\n",
      "Eval loss 0.012313298881053925, R2 0.5257046818733215\n",
      "epoch 9950, loss 0.012323329225182533, R2 0.48776763677597046\n",
      "Eval loss 0.012313291430473328, R2 0.5257052779197693\n",
      "epoch 9951, loss 0.01232332456856966, R2 0.4877679944038391\n",
      "Eval loss 0.01231328397989273, R2 0.5257054567337036\n",
      "epoch 9952, loss 0.012323318049311638, R2 0.4877682328224182\n",
      "Eval loss 0.01231327559798956, R2 0.5257057547569275\n",
      "epoch 9953, loss 0.012323309667408466, R2 0.48776859045028687\n",
      "Eval loss 0.012313267216086388, R2 0.5257059335708618\n",
      "epoch 9954, loss 0.012323303148150444, R2 0.48776882886886597\n",
      "Eval loss 0.012313258834183216, R2 0.5257059335708618\n",
      "epoch 9955, loss 0.012323298491537571, R2 0.48776906728744507\n",
      "Eval loss 0.01231325138360262, R2 0.5257067680358887\n",
      "epoch 9956, loss 0.012323291972279549, R2 0.4877691864967346\n",
      "Eval loss 0.012313243933022022, R2 0.5257068276405334\n",
      "epoch 9957, loss 0.012323285453021526, R2 0.48776960372924805\n",
      "Eval loss 0.012313237413764, R2 0.525706946849823\n",
      "epoch 9958, loss 0.012323278933763504, R2 0.48777031898498535\n",
      "Eval loss 0.012313229031860828, R2 0.5257075428962708\n",
      "epoch 9959, loss 0.012323272414505482, R2 0.487770140171051\n",
      "Eval loss 0.012313220649957657, R2 0.5257078409194946\n",
      "epoch 9960, loss 0.01232326589524746, R2 0.4877703785896301\n",
      "Eval loss 0.01231321133673191, R2 0.5257076621055603\n",
      "epoch 9961, loss 0.012323259375989437, R2 0.487770676612854\n",
      "Eval loss 0.012313204817473888, R2 0.5257089138031006\n",
      "epoch 9962, loss 0.01232325378805399, R2 0.48777109384536743\n",
      "Eval loss 0.012313198298215866, R2 0.5257091522216797\n",
      "epoch 9963, loss 0.012323248200118542, R2 0.4877711534500122\n",
      "Eval loss 0.01231319084763527, R2 0.5257087349891663\n",
      "epoch 9964, loss 0.01232324168086052, R2 0.48777157068252563\n",
      "Eval loss 0.012313181534409523, R2 0.5257090330123901\n",
      "epoch 9965, loss 0.012323235161602497, R2 0.48777180910110474\n",
      "Eval loss 0.0123131750151515, R2 0.5257096290588379\n",
      "epoch 9966, loss 0.0123232277110219, R2 0.48777198791503906\n",
      "Eval loss 0.01231316663324833, R2 0.5257098078727722\n",
      "epoch 9967, loss 0.012323223985731602, R2 0.4877721667289734\n",
      "Eval loss 0.012313160113990307, R2 0.5257102251052856\n",
      "epoch 9968, loss 0.01232321560382843, R2 0.48777300119400024\n",
      "Eval loss 0.012313151732087135, R2 0.5257103443145752\n",
      "epoch 9969, loss 0.012323210947215557, R2 0.48777270317077637\n",
      "Eval loss 0.01231314241886139, R2 0.5257104635238647\n",
      "epoch 9970, loss 0.01232320535928011, R2 0.48777294158935547\n",
      "Eval loss 0.012313135899603367, R2 0.525710940361023\n",
      "epoch 9971, loss 0.012323196977376938, R2 0.48777341842651367\n",
      "Eval loss 0.01231312844902277, R2 0.5257109999656677\n",
      "epoch 9972, loss 0.01232319138944149, R2 0.4877736568450928\n",
      "Eval loss 0.012313120998442173, R2 0.5257115364074707\n",
      "epoch 9973, loss 0.012323183938860893, R2 0.4877737760543823\n",
      "Eval loss 0.012313111685216427, R2 0.5257120728492737\n",
      "epoch 9974, loss 0.012323180213570595, R2 0.48777395486831665\n",
      "Eval loss 0.01231310423463583, R2 0.5257121920585632\n",
      "epoch 9975, loss 0.012323173694312572, R2 0.48777443170547485\n",
      "Eval loss 0.012313097715377808, R2 0.5257128477096558\n",
      "epoch 9976, loss 0.01232316717505455, R2 0.48777449131011963\n",
      "Eval loss 0.012313088402152061, R2 0.5257126688957214\n",
      "epoch 9977, loss 0.012323159724473953, R2 0.48777496814727783\n",
      "Eval loss 0.012313080951571465, R2 0.5257128477096558\n",
      "epoch 9978, loss 0.012323154136538506, R2 0.4877748489379883\n",
      "Eval loss 0.012313074432313442, R2 0.5257130861282349\n",
      "epoch 9979, loss 0.012323148548603058, R2 0.48777544498443604\n",
      "Eval loss 0.01231306605041027, R2 0.5257138013839722\n",
      "epoch 9980, loss 0.012323142029345036, R2 0.4877755641937256\n",
      "Eval loss 0.0123130576685071, R2 0.5257138609886169\n",
      "epoch 9981, loss 0.012323135510087013, R2 0.487775981426239\n",
      "Eval loss 0.012313051149249077, R2 0.525714099407196\n",
      "epoch 9982, loss 0.01232313085347414, R2 0.4877760410308838\n",
      "Eval loss 0.012313042767345905, R2 0.5257142782211304\n",
      "epoch 9983, loss 0.012323124334216118, R2 0.4877769351005554\n",
      "Eval loss 0.012313035316765308, R2 0.5257148742675781\n",
      "epoch 9984, loss 0.012323116883635521, R2 0.48777705430984497\n",
      "Eval loss 0.012313026934862137, R2 0.5257149934768677\n",
      "epoch 9985, loss 0.012323110364377499, R2 0.48777735233306885\n",
      "Eval loss 0.012313020415604115, R2 0.5257152915000916\n",
      "epoch 9986, loss 0.012323105707764626, R2 0.48777687549591064\n",
      "Eval loss 0.012313012965023518, R2 0.5257160067558289\n",
      "epoch 9987, loss 0.012323099188506603, R2 0.48777735233306885\n",
      "Eval loss 0.01231300551444292, R2 0.5257163047790527\n",
      "epoch 9988, loss 0.012323091737926006, R2 0.4877781271934509\n",
      "Eval loss 0.01231299713253975, R2 0.5257163047790527\n",
      "epoch 9989, loss 0.012323086149990559, R2 0.48777806758880615\n",
      "Eval loss 0.012312990613281727, R2 0.5257164239883423\n",
      "epoch 9990, loss 0.012323078699409962, R2 0.4877781867980957\n",
      "Eval loss 0.01231298130005598, R2 0.5257166624069214\n",
      "epoch 9991, loss 0.012323074042797089, R2 0.48777884244918823\n",
      "Eval loss 0.012312974780797958, R2 0.5257171392440796\n",
      "epoch 9992, loss 0.012323067523539066, R2 0.48777931928634644\n",
      "Eval loss 0.012312967330217361, R2 0.5257173776626587\n",
      "epoch 9993, loss 0.012323061935603619, R2 0.487778902053833\n",
      "Eval loss 0.012312958016991615, R2 0.5257177352905273\n",
      "epoch 9994, loss 0.012323055416345596, R2 0.48777931928634644\n",
      "Eval loss 0.012312952429056168, R2 0.5257177948951721\n",
      "epoch 9995, loss 0.012323048897087574, R2 0.487779438495636\n",
      "Eval loss 0.012312943115830421, R2 0.5257179737091064\n",
      "epoch 9996, loss 0.012323044240474701, R2 0.4877800941467285\n",
      "Eval loss 0.012312935665249825, R2 0.5257186889648438\n",
      "epoch 9997, loss 0.012323037721216679, R2 0.48777973651885986\n",
      "Eval loss 0.012312927283346653, R2 0.5257190465927124\n",
      "epoch 9998, loss 0.012323030270636082, R2 0.48778021335601807\n",
      "Eval loss 0.01231292076408863, R2 0.5257192850112915\n",
      "epoch 9999, loss 0.012323024682700634, R2 0.4877805709838867\n",
      "Eval loss 0.012312914244830608, R2 0.5257195234298706\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(reward_train_x.shape[0])\n",
    "    reward_train_x = reward_train_x[idx, :]\n",
    "    reward_train_y = reward_train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    reward_test_x = reward_test_x[idx, :]\n",
    "    reward_test_y = reward_test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = reward_lr_model(reward_train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs.squeeze(), reward_train_y.squeeze())\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs.squeeze(), reward_train_y.squeeze())\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = reward_lr_model(reward_test_x)\n",
    "            test_loss = criterion(preds.squeeze(), reward_test_y.squeeze())\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds.squeeze(), reward_test_y.squeeze())\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].set_xlabel('train epoch')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = reward_lr_model(reward_train_x)\n",
    "    test_preds = reward_lr_model(reward_test_x)\n",
    "\n",
    "relative_train_preds = train_preds - reward_train_y\n",
    "relative_test_preds = test_preds - reward_test_y\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Train bar plots\n",
    "n_bins=100\n",
    "counts, bins = np.histogram(relative_train_preds, bins=n_bins)\n",
    "ax[0].hist(bins[:-1], bins, weights=counts)\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "n_bins=100\n",
    "counts, bins = np.histogram(relative_test_preds, bins=n_bins)\n",
    "ax[1].hist(bins[:-1], bins, weights=counts)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MBRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 10\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = lr_model\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL loop (with pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1, reward: 2.573672088377971.\n",
      "Trial: 2, reward: 2.705402098585997.\n",
      "Trial: 3, reward: 2.322563968184047.\n",
      "Trial: 4, reward: 2.677491420881591.\n",
      "Trial: 5, reward: 2.8242376956986925.\n",
      "Trial: 6, reward: 3.101734248162826.\n",
      "Trial: 7, reward: 1.843696885892893.\n",
      "Trial: 8, reward: 1.903616935018447.\n",
      "Trial: 9, reward: 2.8298462815498127.\n",
      "Trial: 10, reward: 2.7504850048228.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "\n",
    "while (\n",
    "    current_trial < num_episodes\n",
    "):\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "\n",
    "        # --- Doing env step using the agent ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "        \n",
    "        #print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative Reward')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sum(rewards, axis=-1))\n",
    "plt.xlabel('Day'); plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training MBRL agent with a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from mbrl.models.model import Model\n",
    "from mbrl.util.logger import Logger\n",
    "import mbrl.util.common\n",
    "import mbrl.models\n",
    "\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "from src.util.model_trainer import ModelTrainer, ModelTrainerOverriden\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.model.simple import Simple\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 20\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "optim_lr = 0.1\n",
    "model_wd = 0\n",
    "freq_train_model = 10\n",
    "model_batch_size = num_steps+initial_exploration_steps #Make sense for LR or GP\n",
    "validation_ratio = 0\n",
    "num_epochs = 25\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = Simple(in_size, out_size, device)\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "\n",
    "#Replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    num_steps+initial_exploration_steps,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=initial_exploration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training MBRL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500\n",
      "Epoch: 0 Train loss 5.370, Test loss 864915072.000\n",
      "1 500\n",
      "Epoch: 1 Train loss 864915200.000, Test loss 281.795\n",
      "1 500\n",
      "Epoch: 2 Train loss 281.795, Test loss 5.353\n",
      "1 500\n",
      "Epoch: 3 Train loss 5.353, Test loss 5.373\n",
      "1 500\n",
      "Epoch: 4 Train loss 5.373, Test loss 5.311\n",
      "1 500\n",
      "Epoch: 5 Train loss 5.311, Test loss 5.156\n",
      "1 500\n",
      "Epoch: 6 Train loss 5.156, Test loss 4.905\n",
      "1 500\n",
      "Epoch: 7 Train loss 4.905, Test loss 4.582\n",
      "1 500\n",
      "Epoch: 8 Train loss 4.582, Test loss 4.256\n",
      "1 500\n",
      "Epoch: 9 Train loss 4.256, Test loss 4.038\n",
      "1 500\n",
      "Epoch: 10 Train loss 4.038, Test loss 4.029\n",
      "1 500\n",
      "Epoch: 11 Train loss 4.029, Test loss 4.146\n",
      "1 500\n",
      "Epoch: 12 Train loss 4.146, Test loss 4.144\n",
      "1 500\n",
      "Epoch: 13 Train loss 4.144, Test loss 4.023\n",
      "1 500\n",
      "Epoch: 14 Train loss 4.023, Test loss 3.897\n",
      "1 500\n",
      "Epoch: 15 Train loss 3.897, Test loss 3.841\n",
      "1 500\n",
      "Epoch: 16 Train loss 3.841, Test loss 3.855\n",
      "1 500\n",
      "Epoch: 17 Train loss 3.855, Test loss 3.906\n",
      "1 500\n",
      "Epoch: 18 Train loss 3.906, Test loss 3.956\n",
      "1 500\n",
      "Epoch: 19 Train loss 3.956, Test loss 3.985\n",
      "1 500\n",
      "Epoch: 20 Train loss 3.985, Test loss 3.986\n",
      "1 500\n",
      "Epoch: 21 Train loss 3.986, Test loss 3.962\n",
      "1 500\n",
      "Epoch: 22 Train loss 3.962, Test loss 3.923\n",
      "1 500\n",
      "Epoch: 23 Train loss 3.923, Test loss 3.881\n",
      "1 500\n",
      "Epoch: 24 Train loss 3.881, Test loss 3.849\n",
      "Mean train loss: 34596623.267 Mean val score: 34596618.086\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  0.\n",
      "  -2. -2.  1.  2.  0. -1. -1.  1. -2.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.01303253]] False\n",
      "Step 1: Reward 0.000.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  1.\n",
      "  -2.  4.  0.  2. -1. -1. -1.  9. -2.  0. -2. -1. -1. -2. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.34636587]] False\n",
      "Step 2: Reward 0.333.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   1.  4. -1.  2. -2. -1.  2. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.23525475]] False\n",
      "Step 3: Reward 0.222.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  1.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   6.  4.  2.  5. -2.  1. -1. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.16520643]] False\n",
      "Step 4: Reward 0.152.\n",
      "[[ 0.  0.  0. -1.  0.  0. -1.  0.  0. -1.  1. -2.  0.  1. -1.  1. -2.  1.\n",
      "   6.  4.  5.  5. -2.  1. -1. 17. -2.  3. -2. -1.  2. -2. -2. -1.  0.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3071502]] False\n",
      "Step 5: Reward 0.294.\n",
      "[[ 0.  0.  0. -1.  0.  0.  0.  0.  0. -1.  3. -1.  1.  1. -1.  1. -2.  0.\n",
      "   5.  3.  5.  6. -2. -1. -1. 22. -2.  3. -2. -1.  2. -2. -2. -1.  1. -1.\n",
      "  -1. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.42212343]] False\n",
      "Step 6: Reward 0.409.\n",
      "[[ 1.  0.  0. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  1. -2.  0.\n",
      "   5.  3.  4.  4.  0.  0. -1. 24. -2.  1. -2.  2.  1. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.42969918]] False\n",
      "Step 7: Reward 0.417.\n",
      "[[ 1.  0.  2. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  4. -2.  3.\n",
      "   5.  3.  4.  8.  2. -1. -1. 24. -2.  1. -2.  2.  4. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.26303253]] True\n",
      "Step 8: Reward 0.250.\n",
      "Trial: 1, reward: 2.0776046914154334.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -2. -2.  1.\n",
      "  -2. -2.  1. -2.  7. -1. -1. -2. -2.  0. -2.  0. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.20350872]] False\n",
      "Step 9: Reward 0.190.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  1.  0. -1. -2. -2.  0.\n",
      "  -2.  0.  4.  3.  7. -1. -1. -2. -1.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.07824992]] False\n",
      "Step 10: Reward 0.065.\n",
      "1 510\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 1 Train loss 3.943, Test loss 3.948\n",
      "1 510\n",
      "Epoch: 2 Train loss 3.948, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 3 Train loss 3.943, Test loss 3.930\n",
      "1 510\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.918\n",
      "1 510\n",
      "Epoch: 5 Train loss 3.918, Test loss 3.914\n",
      "1 510\n",
      "Epoch: 6 Train loss 3.914, Test loss 3.919\n",
      "1 510\n",
      "Epoch: 7 Train loss 3.919, Test loss 3.928\n",
      "1 510\n",
      "Epoch: 8 Train loss 3.928, Test loss 3.935\n",
      "1 510\n",
      "Epoch: 9 Train loss 3.935, Test loss 3.936\n",
      "1 510\n",
      "Epoch: 10 Train loss 3.936, Test loss 3.931\n",
      "1 510\n",
      "Epoch: 11 Train loss 3.931, Test loss 3.921\n",
      "1 510\n",
      "Epoch: 12 Train loss 3.921, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 13 Train loss 3.911, Test loss 3.903\n",
      "1 510\n",
      "Epoch: 14 Train loss 3.903, Test loss 3.902\n",
      "1 510\n",
      "Epoch: 15 Train loss 3.902, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 16 Train loss 3.905, Test loss 3.909\n",
      "1 510\n",
      "Epoch: 17 Train loss 3.909, Test loss 3.912\n",
      "1 510\n",
      "Epoch: 18 Train loss 3.912, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 19 Train loss 3.911, Test loss 3.908\n",
      "1 510\n",
      "Epoch: 20 Train loss 3.908, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 21 Train loss 3.906, Test loss 3.904\n",
      "1 510\n",
      "Epoch: 22 Train loss 3.904, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 23 Train loss 3.905, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 24 Train loss 3.906, Test loss 3.906\n",
      "Mean train loss: 3.919 Mean val score: 3.918\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1.  0.  0. -1.  0. -1. -1.  0.\n",
      "  -1. -1.  7.  6. 10. -1. -1.  0. -1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20161985]] False\n",
      "Step 11: Reward 0.053.\n",
      "[[ 0.  0.  0.  1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -1. -1.  8.  8. 12. -1. -1.  0. -1.  0. -2. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.1531674]] False\n",
      "Step 12: Reward 0.102.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  0.\n",
      "  -1. -1.  8.  8. 11.  1. -1.  3.  1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12490711]] False\n",
      "Step 13: Reward 0.130.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  0.\n",
      "   3.  0.  8.  8.  9.  1. -1.  3.  2.  0. -2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -1. -1.  0.  0. -2. -1. -1.  0.  0.  0.]] [[-0.04456946]] False\n",
      "Step 14: Reward 0.211.\n",
      "[[ 0.  0.  2. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  2. -1. -1.  0.  0.\n",
      "   4.  0.  9.  9. 10. -1. -1.  3.  3.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1.  1.  6. -1. -2.  2. -1.  0.  0.  0.]] [[0.02421457]] False\n",
      "Step 15: Reward 0.279.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0. -1.  0. -1.  2.  0.  0. -1.  1. -1.  0.  0.\n",
      "   4. -1.  9.  8. 11.  0. -1.  3.  6.  0. -2. -1. -1. -1. -2.  0. -1.  0.\n",
      "  -2. -1.  6. -1. -1.  3. -1.  0.  0.  0.]] [[-0.01402435]] True\n",
      "Step 16: Reward 0.241.\n",
      "Trial: 2, reward: 1.2721946606988341.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   1.  1. -2. -1. -2.  1.  0. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13744873]] False\n",
      "Step 17: Reward 0.118.\n",
      "[[ 1.  0.  1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  1. -1. -1. -1.  4.\n",
      "   1.  1.  2.  1. -2.  1. -1. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.10204709]] False\n",
      "Step 18: Reward 0.357.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  3.\n",
      "   5.  1.  2. -1. -2.  5.  0.  0. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.06490421]] False\n",
      "Step 19: Reward 0.320.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0.\n",
      "   7.  1.  0.  2. -2.  6.  0.  1. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[0.00653213]] False\n",
      "Step 20: Reward 0.262.\n",
      "1 520\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.935\n",
      "1 520\n",
      "Epoch: 1 Train loss 3.935, Test loss 3.933\n",
      "1 520\n",
      "Epoch: 2 Train loss 3.933, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 3 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 5 Train loss 3.930, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 6 Train loss 3.931, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 7 Train loss 3.932, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 8 Train loss 3.932, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 9 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 10 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 11 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 12 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 13 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 14 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 15 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 16 Train loss 3.930, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 17 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 18 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 19 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 20 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 21 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 22 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 23 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 24 Train loss 3.929, Test loss 3.929\n",
      "Mean train loss: 3.931 Mean val score: 3.930\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1. -1. -1.  3. -1.\n",
      "   7.  4.  0.  5. -2.  4. -1.  1.  0.  1.  2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.0207608]] False\n",
      "Step 21: Reward 0.322.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  1.  3. -1.\n",
      "   7.  4.  0.  5.  0.  4. -1.  1.  6. -1.  6. -1. -1. -1. -2. -1. -1.  0.\n",
      "  -2.  2. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.04603767]] False\n",
      "Step 22: Reward 0.296.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1.  3. -1.  1. -1. -1.  0. -1.  2.  1.  3.  1.\n",
      "   7.  4. -2.  5.  2.  4. -1.  1.  9. -1.  4. -1. -1. -1. -2. -1. -1.  0.\n",
      "   0.  4.  0. -1. -1. -2. -1.  0.  0.  0.]] [[-0.00511685]] False\n",
      "Step 23: Reward 0.337.\n",
      "[[ 0.  1. -1. -1.  0. -1.  2.  3.  0. -1. -1. -1.  0.  2.  1.  0.  3.  0.\n",
      "   7.  5.  3.  3.  3.  4. -1.  1. 16.  0.  3. -1.  1.  0. -2. -1. -1. -1.\n",
      "   0.  4.  2. -1.  1. -1. -1.  0.  0.  0.]] [[-0.02454716]] True\n",
      "Step 24: Reward 0.318.\n",
      "Trial: 3, reward: 2.3298202934631616.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   5. -2.  3.  0. -2. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.34246624]] False\n",
      "Step 25: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   4. -2.  3.  6. -2. -1. -1. -2.  0. -1.  1.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3154392]] False\n",
      "Step 26: Reward 0.027.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "   3. -2.  3.  8.  4. -1. -1.  0.  0. -1.  1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.20785084]] False\n",
      "Step 27: Reward 0.135.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.\n",
      "   1.  1.  1. 11.  2. -1.  0.  0.  0.  0. -1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.03691068]] False\n",
      "Step 28: Reward 0.306.\n",
      "[[-1.  0. -1.  0.  0. -1. -1.  2.  0. -1.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   1.  1.  0. 12.  2. -1.  0.  0. -1.  3. -1. -1. -1. -1. -2.  0. -1.  1.\n",
      "  -2. -1.  2. -1. -2. -1.  1.  0.  0.  0.]] [[-0.00592777]] False\n",
      "Step 29: Reward 0.337.\n",
      "[[-1.  1. -1. -1.  1. -1. -1.  1. -1.  0.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   0.  0.  0. 12. -1. -1. -1.  3.  3.  2.  2. -1. -1. -1. -2. -1. -1.  3.\n",
      "   1.  0.  3. -1. -2. -1.  1.  0.  0.  0.]] [[0.0934312]] False\n",
      "Step 30: Reward 0.436.\n",
      "1 530\n",
      "Epoch: 0 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 1 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 2 Train loss 3.945, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 3 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 4 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 5 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 6 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 7 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 8 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 9 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 10 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 11 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 12 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 13 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 14 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 15 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 16 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 17 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 18 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 19 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 20 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 21 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 22 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 23 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 24 Train loss 3.944, Test loss 3.944\n",
      "Mean train loss: 3.944 Mean val score: 3.944\n",
      "[[-1.  0. -1.  0.  2. -1. -1.  0. -1. -1.  1. -1.  1.  1. -1.  1.  3. -1.\n",
      "  -1.  0.  1. 13. -2. -1.  0.  3.  5.  3.  2. -1. -1. -1. -2.  2. -1.  0.\n",
      "   0.  3.  4. -1. -2. -1.  1.  0.  0.  0.]] [[0.12568054]] False\n",
      "Step 31: Reward 0.442.\n",
      "[[-1.  0.  2.  0.  0. -1.  0.  0. -1. -1.  2. -1.  1.  2. -1.  3.  5.  0.\n",
      "  -1.  3.  1. 15. -2.  0.  0.  3.  6.  3.  4. -1. -1. -1. -2.  2.  1.  1.\n",
      "   1.  3.  4. -1. -2.  1.  1.  0.  0.  0.]] [[0.12454933]] True\n",
      "Step 32: Reward 0.441.\n",
      "Trial: 4, reward: 2.1231180275297925.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "  -2.  3. -2. -2.  3. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31662714]] False\n",
      "Step 33: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1.  0.\n",
      "  -2.  5. -2. -2.  7. -1.  2. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.2990833]] False\n",
      "Step 34: Reward 0.018.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1. -1.\n",
      "   1.  5. -2. -1.  7. -1. -1.  0.  1. -1. -2.  0.  2. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12615095]] False\n",
      "Step 35: Reward 0.190.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2.  1. -1. -1.\n",
      "   4.  5. -2. -1.  5. -1. -1. -1.  3. -1.  0. -1.  0. -1. -2. -1. -1. -1.\n",
      "  -2.  0. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.11095339]] False\n",
      "Step 36: Reward 0.206.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0.  3. -1.  4.\n",
      "   8.  5. -2. -2.  8. -1. -1.  0. -1. -1.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1.  0.  0.  0.  0.  0.]] [[-0.05114043]] False\n",
      "Step 37: Reward 0.265.\n",
      "[[-1.  0.  3. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -1.  1.  3.  0.  3.\n",
      "   8.  5. -2. -2. 10.  0. -1.  1. -1. -1. -2.  1. -1. -1. -2. -1.  1. -1.\n",
      "   0.  1. -1. -1. -1. -1.  0.  0.  0.  0.]] [[-0.04143333]] False\n",
      "Step 38: Reward 0.275.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  0.  3.  3.  6.\n",
      "   8.  5. -2. -1. 14. -1. -1.  1. -1. -1.  4.  5. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[0.11563092]] False\n",
      "Step 39: Reward 0.432.\n",
      "[[ 2.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  1.  3.  6.  6.\n",
      "   8.  7.  4. -1. 14. -1. -1.  1. -1. -1.  4.  4. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[-0.06662714]] True\n",
      "Step 40: Reward 0.250.\n",
      "Trial: 5, reward: 1.6366323976200197.\n",
      "1 540\n",
      "Epoch: 0 Train loss 3.981, Test loss 3.981\n",
      "1 540\n",
      "Epoch: 1 Train loss 3.981, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 2 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 3 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 4 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 5 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 6 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 7 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 8 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 9 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 10 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 11 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 12 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 13 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 14 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 15 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 16 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 17 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 18 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 19 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 20 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 21 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 22 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 23 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 24 Train loss 3.980, Test loss 3.980\n",
      "Mean train loss: 3.980 Mean val score: 3.980\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2. -1. -1.  1.\n",
      "  -2. -2.  0. -2.  0.  2. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3091395]] False\n",
      "Step 41: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -1.  3.\n",
      "   0.  0.  0.  0. -1.  2. -1.  1. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19086051]] False\n",
      "Step 42: Reward 0.500.\n",
      "[[-1.  0.  1. -1.  2. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0. -1. -1. -1.\n",
      "   0.  3.  1.  6. -2.  3. -1.  1. -2. -1. -2.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.10858202]] False\n",
      "Step 43: Reward 0.418.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1.  0.  0.  2.  0.\n",
      "   1.  2.  3.  5. -2.  2. -1.  3. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.00556806]] False\n",
      "Step 44: Reward 0.304.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1.  0. -1.  2.  3.\n",
      "   4.  4.  5.  7. -1. -1. -1.  3. -2.  0. -1. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.05224666]] False\n",
      "Step 45: Reward 0.361.\n",
      "[[-1.  0.  0.  1.  1.  0.  0. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1.  4.\n",
      "   4.  5.  5.  9.  2. -1. -1.  3. -2. -1.  3.  0. -1.  0. -2.  2. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.09974939]] False\n",
      "Step 46: Reward 0.409.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  0.  0.  1. -1.  0.  1.\n",
      "   7. 10.  7.  9.  1.  0. -1.  3. -1. -1.  3. -1. -1.  0. -1. -1. -1. -1.\n",
      "  -2.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.03917512]] False\n",
      "Step 47: Reward 0.348.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  3.  0.  1. -1.  3.  1.\n",
      "   7. 11.  7.  9.  2.  1. -1.  3. -1. -1.  3. -1. -1.  0. -1.  1. -1. -1.\n",
      "   0.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10913949]] True\n",
      "Step 48: Reward 0.200.\n",
      "Trial: 6, reward: 2.539882581803094.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0. -1.  1. -1. -1.\n",
      "  -2.  1. -2. -1.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.04247281]] False\n",
      "Step 49: Reward 0.267.\n",
      "[[-1.  1. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "  -2.  3.  0.  2. -2. -1.  1. -2. -2.  1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.0438017]] False\n",
      "Step 50: Reward 0.353.\n",
      "1 550\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  6. -1. -1.\n",
      "  -2.  8.  1.  2. -2. -1.  3. -2. -2.  1. -1. -1. -1. -1. -2.  1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03686938]] False\n",
      "Step 51: Reward 0.277.\n",
      "[[-1.  0. -1. -1.  0. -1.  1. -1. -1. -1. -1. -1.  0.  0. -1.  7. -1. -1.\n",
      "  -2.  8.  1.  2.  0.  2.  0. -2.  1.  0. -2.  0. -1. -1. -2.  0.  0. -1.\n",
      "  -2.  0. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02903968]] False\n",
      "Step 52: Reward 0.285.\n",
      "[[ 0.  0.  0. -1.  0. -1.  2. -1. -1. -1. -1.  0.  0. -1.  1.  7. -1. -1.\n",
      "   2. 10.  0.  1. -1. -1. -1.  2.  0. -1. -2. -1.  0. -1. -2.  0. -1.  3.\n",
      "  -1. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[0.01174176]] False\n",
      "Step 53: Reward 0.325.\n",
      "[[-1.  1.  0. -1.  0. -1.  1.  0. -1.  2. -1.  0.  0. -1.  6.  7.  1. -1.\n",
      "   4.  7.  0.  0.  2. -1. -1.  2.  2. -1. -1.  1.  0.  0. -2. -1. -1.  3.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.17787033]] False\n",
      "Step 54: Reward 0.492.\n",
      "[[ 1.  1. -1. -1.  0. -1. -1. -1. -1.  4.  0.  2.  0.  0.  5.  7.  1.  1.\n",
      "   4.  8.  0.  0.  2. -1. -1.  3.  4. -1.  1.  1.  1. -1. -2. -1. -1.  2.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.10942185]] False\n",
      "Step 55: Reward 0.423.\n",
      "[[ 0.  1. -1. -1.  1.  0. -1. -1. -1.  4.  2.  2.  0. -1.  7. 10.  1. -1.\n",
      "   4.  8.  0.  3.  2. -1. -1.  6.  4. -1.  1.  1.  2. -1. -2. -1. -1.  1.\n",
      "   2.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.18634492]] True\n",
      "Step 56: Reward 0.500.\n",
      "Trial: 7, reward: 2.921008114240916.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  4.\n",
      "   0. -2. -2. -2. -1. -1.  1. -2. -2. -1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.16365507]] False\n",
      "Step 57: Reward 0.150.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      "   3. -2. -2. -2. -1.  1.  0. -2. -1. -1. -1.  0. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.01540947]] False\n",
      "Step 58: Reward 0.298.\n",
      "[[-1.  0.  0. -1.  0. -1. -1.  0.  1.  0. -1. -1.  2. -1. -1. -1.  4. -1.\n",
      "   8. -1. -2. -2. -1.  1. -1.  1. -2.  0.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.00258425]] False\n",
      "Step 59: Reward 0.316.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0.  0.  0. -1. -1. -1.  7. -1.\n",
      "   7. -1. -1. -2. -2.  1. -1.  0. -2. -1. -2. -1.  1. -1. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -1. -1.  0.  0.  0.]] [[0.04913563]] False\n",
      "Step 60: Reward 0.363.\n",
      "1 560\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0.  1. -1.  1.  1. -1. -1. -1.  9.  0.\n",
      "   8. -2. -1. -1. -2.  2. -1.  0.  2. -1.  1.  1.  0. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -1.  1. -1.  0.  0.  0.]] [[-0.05258617]] False\n",
      "Step 61: Reward 0.261.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1. -1. -1. 14. -1.\n",
      "   9. -1.  1. -1. -2.  2. -1.  0.  6. -1.  0. -1. -1.  0. -1. -1. -1.  1.\n",
      "  -1.  0.  4. -1.  0.  1. -1.  0.  0.  0.]] [[-0.0411129]] False\n",
      "Step 62: Reward 0.272.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1.  1.  0.  1.  0.  1.  1.  0. 18. -1.\n",
      "   7.  0.  1.  1.  0.  0. -1.  0.  8. -1.  0. -1. -1.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  6. -1.  0.  2.  0.  0.  0.  0.]] [[-0.03090695]] False\n",
      "Step 63: Reward 0.283.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1.  0.  0.  0.  1.  0. 18. -1.\n",
      "   7.  1.  1.  2.  0.  0.  1.  3.  9. -1.  4. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1.  6. -1.  0.  4.  0.  0.  0.  0.]] [[-0.07549722]] True\n",
      "Step 64: Reward 0.238.\n",
      "Trial: 8, reward: 2.1815422317524256.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  -2.  4.  1. -2. -2. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31359246]] False\n",
      "Step 65: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   0.  4.  4.  1. -1. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.18640754]] False\n",
      "Step 66: Reward 0.500.\n",
      "[[-1.  0.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   4.  4.  6.  5. -2. -1. -1.  1. -2.  1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13637727]] False\n",
      "Step 67: Reward 0.177.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1. -1.  2.  0.  0.\n",
      "   2.  5.  5.  4. -2.  0.  0.  1. -2. -1. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.07549722]] False\n",
      "Step 68: Reward 0.238.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1. -1.  4.  4.  0.\n",
      "   2.  4.  5.  9. -2. -1. -1.  1. -2.  0. -1. -1.  1.  0. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.00323921]] False\n",
      "Step 69: Reward 0.317.\n",
      "[[ 0.  0. -1.  1.  1.  0.  1. -1. -1. -1. -1. -1.  0. -1.  0.  1.  3. -1.\n",
      "   3.  5.  5. 17. -1. -1. -1.  1. -1. -1.  1.  0.  1.  0. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.02862975]] False\n",
      "Step 70: Reward 0.342.\n",
      "1 570\n",
      "Epoch: 0 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 1 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 2 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 3 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 4 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 5 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 6 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 7 Train loss 4.014, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 8 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 9 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 10 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 11 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 12 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 13 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 14 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 15 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 16 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 17 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 18 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 19 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 20 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 21 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 22 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 23 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 24 Train loss 4.014, Test loss 4.014\n",
      "Mean train loss: 4.014 Mean val score: 4.014\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  0.  0.  0.  0. -1.  5.  1.  0.\n",
      "   3.  4.  6. 17.  3.  1. -1.  3.  2. -1.  1. -1.  1.  0. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.03747821]] False\n",
      "Step 71: Reward 0.275.\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  2.  0.  0.  3. -1.  5.  6.  0.\n",
      "   3.  4.  6. 17.  1.  2. -1.  3.  2. -1.  1. -1.  2.  0. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.01275909]] True\n",
      "Step 72: Reward 0.300.\n",
      "Trial: 9, reward: 2.1496452322355992.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2. -2. -2. -2.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09053688]] False\n",
      "Step 73: Reward 0.222.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "  -2.  1. -2. -2.  4.  2. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19609243]] False\n",
      "Step 74: Reward 0.117.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "  -2.  1. -2.  1.  3.  1. -1.  1.  1. -1.  0. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.0705716]] False\n",
      "Step 75: Reward 0.242.\n",
      "[[-1.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.  5.  0.\n",
      "  -2.  1.  3.  3.  4.  1. -1.  1.  3. -1. -1. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.13094091]] False\n",
      "Step 76: Reward 0.182.\n",
      "[[-1.  0. -1. -1.  1. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0.  1.  5. -1.\n",
      "   0.  1.  3.  4.  5. -1. -1.  1.  8. -1. -2. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.15935001]] False\n",
      "Step 77: Reward 0.153.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.  5.  5.  0.\n",
      "   0.  0.  2.  5.  1.  0. -1.  1. 13. -1. -1. -1.  0.  2.  4. -1. -1.  2.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.12814371]] False\n",
      "Step 78: Reward 0.185.\n",
      "[[-1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1.  2.  6.  5. -1.\n",
      "   0.  2.  0.  8.  3.  2. -1.  1. 13. -1. -2. -1.  0.  0.  4. -1. -1.  2.\n",
      "  -1.  0. -2. -1. -2. -2.  1.  0.  0.  0.]] [[-0.04515347]] False\n",
      "Step 79: Reward 0.268.\n",
      "[[-1.  0. -1. -1.  0.  0. -1.  0. -1.  1.  3.  0.  0. -1.  2.  6.  5.  2.\n",
      "  -1.  2.  0.  9.  5.  1. -1.  3. 14. -1. -2.  3.  0. -1.  4. -1. -1.  2.\n",
      "   0.  0. -1. -1. -2. -2.  2.  0.  0.  0.]] [[-0.07201836]] True\n",
      "Step 80: Reward 0.241.\n",
      "Trial: 10, reward: 1.609265420775104.\n",
      "1 580\n",
      "Epoch: 0 Train loss 4.045, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 1 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 2 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 3 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 4 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 5 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 6 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 7 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 8 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 9 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 10 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 11 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 12 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 13 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 14 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 15 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 16 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 17 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 18 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 19 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 20 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 21 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 22 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 23 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 24 Train loss 4.044, Test loss 4.044\n",
      "Mean train loss: 4.044 Mean val score: 4.044\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0. -1. -2. -1.  5. -2. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.23142757]] False\n",
      "Step 81: Reward 0.080.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1. -1. -1. -2.  4.\n",
      "   0. -2.  0. -1. -2. -1.  7. -2. -2.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.07458545]] False\n",
      "Step 82: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1. -1. -1.  0. -1. -1.  0. -1.  1. -1. -1. -1.  1.  3.\n",
      "   9. -2.  0. -2. -2. -1.  5.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.01615864]] False\n",
      "Step 83: Reward 0.328.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1.  2. -1.  0. -1. -1. -1.  1.  3.\n",
      "  11.  4.  0. -2. -2. -1. -1.  8. -2. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -1. -1. -2. -1. -2.  0.  0.  0.]] [[0.05220881]] False\n",
      "Step 84: Reward 0.364.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1.  0. -1.  1.  1.\n",
      "   9.  4.  2. -2. -2.  0. -1. 13. -2. -1. -2.  4. -1.  1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -1.  0. -2.  0.  0.  0.]] [[-0.08013505]] False\n",
      "Step 85: Reward 0.231.\n",
      "[[-1.  0.  0.  0.  0. -1.  1. -1. -1.  0. -1. -1.  2.  1.  3. -1.  2. -1.\n",
      "   8.  5.  2.  1. -1. -1.  0. 13. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2.  0. -2. -1. -2.  0. -2.  0.  0.  0.]] [[0.09568706]] False\n",
      "Step 86: Reward 0.407.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  6.  0.  0. -1.  2.  1.  4.  0.\n",
      "  10.  5.  1.  3.  0. -1. -1. 12. -2. -1.  4. -1.  0.  1. -1. -1. -1. -1.\n",
      "  -1. -1.  2. -1.  0.  0. -1.  0.  0.  0.]] [[0.07197165]] False\n",
      "Step 87: Reward 0.383.\n",
      "[[-1.  0.  0. -1.  0.  1. -1. -1.  0.  1.  3. -1.  0.  0.  3.  1.  4.  1.\n",
      "  10.  5.  1.  4. -2. -1.  0. 12. -2. -1.  7. -1. -1. -1.  0. -1. -1. -1.\n",
      "   3. -1.  2. -1.  4.  0. -1.  0.  0.  0.]] [[-0.0816139]] True\n",
      "Step 88: Reward 0.230.\n",
      "Trial: 11, reward: 2.2596846913912443.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  2. -1. -2.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19604295]] False\n",
      "Step 89: Reward 0.115.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  3.\n",
      "  -1.  2.  0. -2.  0.  6. -1. -2. -2.  1. -2. -1. -1. -1. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19142756]] False\n",
      "Step 90: Reward 0.120.\n",
      "1 590\n",
      "Epoch: 0 Train loss 4.066, Test loss 4.066\n",
      "1 590\n",
      "Epoch: 1 Train loss 4.066, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 2 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 3 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 4 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 5 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 6 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 7 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 8 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 9 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 10 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 11 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 12 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 13 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 14 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 15 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 16 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 17 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 18 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 19 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 20 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 21 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 22 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 23 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 24 Train loss 4.065, Test loss 4.065\n",
      "Mean train loss: 4.065 Mean val score: 4.065\n",
      "[[-1.  2. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  3.  3.  0.  0.  5. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.22695008]] False\n",
      "Step 91: Reward 0.083.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1. -1.\n",
      "   0.  3.  3.  3. -1.  9. -1.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2.  0. -2. -1. -1. -2. -2.  0.  0.  0.]] [[-0.13638127]] False\n",
      "Step 92: Reward 0.173.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1.  1. -1.\n",
      "  -1.  4.  7. -1.  0.  7. -1.  5. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.\n",
      "   2.  0. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.12800482]] False\n",
      "Step 93: Reward 0.182.\n",
      "[[-1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0.  0. -1. -1.  4. -1.\n",
      "  -1.  5.  9.  0. -2.  7. -1.  5. -2. -1.  6. -1. -1. -1. -1. -1. -1.  1.\n",
      "   5. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.07412507]] False\n",
      "Step 94: Reward 0.236.\n",
      "[[-1.  0. -1.  0.  0.  1.  0. -1. -1. -1.  0.  0.  1. -1.  4. -1.  5. -1.\n",
      "  -1.  2.  9. -2.  0.  8.  0.  7. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.\n",
      "   9. -1. -1. -1. -1. -2.  2.  0.  0.  0.]] [[-0.06379126]] False\n",
      "Step 95: Reward 0.246.\n",
      "[[-1.  0.  1.  1.  0. -1. -1.  0. -1. -1.  1. -1.  1. -1.  4.  1.  5. -1.\n",
      "   0.  2. 11. -2. -1.  8. -1.  8. -1. -1.  1. -1. -1. -1.  0. -1. -1.  1.\n",
      "  11. -1. -1. -1. -1. -2.  4.  0.  0.  0.]] [[0.02351034]] True\n",
      "Step 96: Reward 0.333.\n",
      "Trial: 12, reward: 1.4885804796654487.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  0.  3. -1.  0. -1. -1. -2. -2. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19217595]] False\n",
      "Step 97: Reward 0.118.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2.  0.\n",
      "  -2.  0.  3. -1. -1.  2. -1. -2.  1. -1.  3. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09243171]] False\n",
      "Step 98: Reward 0.217.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2. -1.\n",
      "  -2.  0.  3.  4. -2.  3.  0. -2.  1. -1.  3.  5. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.159823]] False\n",
      "Step 99: Reward 0.150.\n",
      "[[-1.  2.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  3. -2. -1.\n",
      "   1. -2.  4. 11. -1.  6. -1. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.15090828]] False\n",
      "Step 100: Reward 0.159.\n",
      "1 600\n",
      "Epoch: 0 Train loss 4.076, Test loss 4.076\n",
      "1 600\n",
      "Epoch: 1 Train loss 4.076, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 2 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 3 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 4 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 5 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 6 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 7 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 8 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 9 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 10 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 11 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 12 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 13 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 14 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 15 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 16 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 17 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 18 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 19 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 20 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 21 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 22 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 23 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 24 Train loss 4.075, Test loss 4.075\n",
      "Mean train loss: 4.075 Mean val score: 4.075\n",
      "[[ 0.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.  4. -2. -1.\n",
      "   2. -2.  5. 15. -1.  3. -1.  1. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2.  0. -2. -1. -2.  0.  0.  0.]] [[-0.07353824]] False\n",
      "Step 101: Reward 0.235.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  6. -2. -1.\n",
      "   3. -2.  5. 16. -2.  1.  0.  4.  2.  0. -2. -1. -1.  0. -1.  1.  1.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.05061775]] False\n",
      "Step 102: Reward 0.258.\n",
      "[[ 0.  0.  0. -1.  1. -1.  0. -1.  0.  2.  2. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  5. 18.  0.  5.  0.  5.  2. -1. -2.  1. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.03281066]] False\n",
      "Step 103: Reward 0.276.\n",
      "[[ 0.  0. -1. -1.  1. -1.  0. -1.  0.  2.  4. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  7. 18.  0.  5.  0.  5.  2.  4. -2.  5. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.01420319]] True\n",
      "Step 104: Reward 0.294.\n",
      "Trial: 13, reward: 1.7060666329221275.\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 4. 0. 0.\n",
      "  0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0.6412164]] False\n",
      "Step 105: Reward 0.250.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   0.  0.  0.  2.  1. -1.  0.  3. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19167915]] False\n",
      "Step 106: Reward 0.500.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1. -1.  1.  1.\n",
      "   0.  3. -2.  3.  0.  0. -1.  2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03309149]] False\n",
      "Step 107: Reward 0.275.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  1. -1. -1.  1.  2.\n",
      "   0.  5.  1.  5.  2. -1.  0.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02783304]] False\n",
      "Step 108: Reward 0.280.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0.  0. -1. -1.  7.  1.\n",
      "   2.  4.  0.  6.  0. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  0. -2.  0.  0.  0.  0.]] [[-0.03641751]] False\n",
      "Step 109: Reward 0.272.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  0.  3.  5. -1.\n",
      "   1.  4.  1.  8. -1. -1. -1.  0. -2. -1. -2. -1. -1. -1.  0. -1. -1.  0.\n",
      "  -2.  1.  1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.04998752]] False\n",
      "Step 110: Reward 0.258.\n",
      "1 610\n",
      "Epoch: 0 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 1 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 2 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 3 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 4 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 5 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 6 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 7 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 8 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 9 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 10 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 11 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 12 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 13 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 14 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 15 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 16 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 17 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 18 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 19 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 20 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 21 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 22 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 23 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 24 Train loss 4.097, Test loss 4.097\n",
      "Mean train loss: 4.097 Mean val score: 4.097\n",
      "[[ 0.  0.  0. -1.  0.  1.  0. -1.  2.  1. -1.  2.  0.  3. -1.  3.  7.  3.\n",
      "   1.  6.  1.  9. -2. -1. -1.  1. -1.  0. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[-0.07657747]] False\n",
      "Step 111: Reward 0.232.\n",
      "[[ 0.  0. -1. -1.  0.  1. -1. -1.  4.  1. -1.  2.  0.  3. -1.  6.  9.  6.\n",
      "   1.  6.  1.  9.  0. -1. -1.  1.  2.  0.  1. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[0.01433161]] True\n",
      "Step 112: Reward 0.323.\n",
      "Trial: 14, reward: 2.390206018685877.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0.  1.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20824903]] False\n",
      "Step 113: Reward 0.100.\n",
      "[[ 0.  1. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1. -1.  0. -2. -1.\n",
      "   1.  0.  1.  0.  3.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03165329]] False\n",
      "Step 114: Reward 0.277.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -2. -1.\n",
      "   2.  1.  1. -2.  4.  1.  1. -2. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -2.  0. -2. -1. -2.  0.  0.  0.]] [[0.0080775]] False\n",
      "Step 115: Reward 0.316.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0. -1.  0.  0. -1. -1. -2.  0.\n",
      "   8.  2.  2. -1.  5. -1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2.  0.  0. -1. -2.  0.  0.  0.]] [[0.07865575]] False\n",
      "Step 116: Reward 0.387.\n",
      "[[ 0.  0. -1. -1.  0. -1.  0. -1. -1.  0.  2.  1.  0.  0. -1. -1.  1.  1.\n",
      "   9.  1.  2. -1.  6.  0. -1. -2. -2. -1. -2. -1.  0. -1. -1.  2.  0. -1.\n",
      "  -2. -1. -2.  0.  0.  0. -2.  0.  0.  0.]] [[0.05909792]] False\n",
      "Step 117: Reward 0.367.\n",
      "[[ 0.  2. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0.  2.  1. -1.\n",
      "   9.  2.  5.  6.  5.  0.  0. -2. -1. -1. -2.  0.  0. -1. -1. -1. -1. -1.\n",
      "   1.  1. -2.  0.  2.  1. -2.  0.  0.  0.]] [[0.04360282]] False\n",
      "Step 118: Reward 0.352.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  1.  1.  6. -1.\n",
      "   9.  3.  5.  6.  4.  5. -1. -2.  1. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "   1.  1.  2.  1.  2.  1. -1.  0.  0.  0.]] [[-0.09203281]] False\n",
      "Step 119: Reward 0.216.\n",
      "[[ 0.  4. -1.  0.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  4.  0.  5.  0.\n",
      "   9.  2.  8.  7.  2.  9. -1. -2.  1. -1. -2. -1. -1. -1. -1. -1.  2. -1.\n",
      "   1.  2.  1.  1.  2.  1. -1.  0.  0.  0.]] [[-0.14450634]] True\n",
      "Step 120: Reward 0.164.\n",
      "Trial: 15, reward: 2.1789847340999158.\n",
      "1 620\n",
      "Epoch: 0 Train loss 4.102, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 1 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 2 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 3 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 4 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 5 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 6 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 7 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 8 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 9 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 10 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 11 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 12 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 13 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 14 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 15 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 16 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 17 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 18 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 19 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 20 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 21 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 22 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 23 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 24 Train loss 4.101, Test loss 4.101\n",
      "Mean train loss: 4.101 Mean val score: 4.101\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   1. -2.  2. -2.  0.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 121: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  2. -1.\n",
      "   0.  0.  5. -2.  2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19229937]] False\n",
      "Step 122: Reward 0.500.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  4.  1.\n",
      "  -1. -1.  7. -2.  2.  1. -1. -2.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.13378759]] False\n",
      "Step 123: Reward 0.174.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1.  4. -1.\n",
      "   1.  1.  7. -2. -2.  1. -1. -2.  4. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10890545]] False\n",
      "Step 124: Reward 0.199.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0.  1. -1. -1.  0.  0.  0. -1.  2.  7. -1.\n",
      "   0.  2.  6.  0. -2.  0.  0. -2.  3. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  0. -1.  0. -1. -2.  0.  0.  0.  0.]] [[-0.07061857]] False\n",
      "Step 125: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1.  3. -1.  0. -1. -1. -1.  1.  0.  3. -1.  2.  7. -1.\n",
      "   0.  1.  6.  0. -1. -1. -1. -2.  7.  2.  0. -1. -1. -1. -1. -1.  0. -1.\n",
      "   1. -1. -2. -1.  0. -2.  0.  0.  0.  0.]] [[-0.01636204]] False\n",
      "Step 126: Reward 0.291.\n",
      "[[ 1.  1. -1. -1.  1.  0.  0. -1. -1.  3. -1.  2.  0.  3. -1.  4.  7.  0.\n",
      "   2.  3.  6.  2. -1.  0.  2.  0.  8.  2. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.01676744]] False\n",
      "Step 127: Reward 0.324.\n",
      "[[ 1.  2. -1. -1.  2.  0. -1. -1. -1.  3. -1.  2.  1.  3. -1.  4. 10.  0.\n",
      "   2.  3.  6.  5. -1.  0.  2.  3. 12.  1. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.02563271]] True\n",
      "Step 128: Reward 0.333.\n",
      "Trial: 16, reward: 2.058930292187335.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   2.  3. -2. -2. -2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 129: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   3.  2.  3. -2. -1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.11722444]] False\n",
      "Step 130: Reward 0.190.\n",
      "1 630\n",
      "Epoch: 0 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 1 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 2 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 3 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 4 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 5 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 6 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 7 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 8 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 9 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 10 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 11 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 12 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 13 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 14 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 15 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 16 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 17 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 18 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 19 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 20 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 21 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 22 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 23 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 24 Train loss 4.092, Test loss 4.092\n",
      "Mean train loss: 4.092 Mean val score: 4.092\n",
      "[[ 1.  0.  0. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "   7.  1.  5. -2. -1.  1. -1. -2. -1. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.05655059]] False\n",
      "Step 131: Reward 0.250.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1.  2. -1.\n",
      "   6.  3.  6. -1. -1.  0.  0. -2.  2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.06126757]] False\n",
      "Step 132: Reward 0.245.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   5.  5.  6.  4.  0. -1.  0. -2.  4. -1. -2.  0. -1. -1. -1.  1. -1.  0.\n",
      "   0. -1. -2. -1. -1. -1. -2.  0.  0.  0.]] [[-0.0413332]] False\n",
      "Step 133: Reward 0.265.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "   7.  6.  5. 10. -1. -1. -1. -2.  4. -1. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "   0.  1. -2. -1.  1.  0. -1.  0.  0.  0.]] [[-0.11754715]] False\n",
      "Step 134: Reward 0.189.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1.  0.  5. -1.\n",
      "   7.  9.  3. 10. -2. -1. -1.  2.  6. -1. -2. -1. -1.  0. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.18106039]] False\n",
      "Step 135: Reward 0.125.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1.  0. -1.  0. -1. -1.  4.  5. -1.\n",
      "   7. 10.  6.  7. -1. -1. -1.  2.  7.  0. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.08027323]] True\n",
      "Step 136: Reward 0.226.\n",
      "Trial: 17, reward: 1.4917476054157848.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  1.  1.  1.  1. -1.  2. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3065506]] False\n",
      "Step 137: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -2. -1.\n",
      "  -2.  4.  1.  1.  3. -1.  3. -2. -2. -1. -2.  2. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3601161]] False\n",
      "Step 138: Reward 0.667.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  2. -1. -1. -1. -1.  0. -1.  0. -1. -2. -1.\n",
      "  -2.  6.  3.  5. -2.  0.  0.  3. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.04639059]] False\n",
      "Step 139: Reward 0.353.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  1. -1.  3. -1.  1. -1.\n",
      "  -2.  9.  2.  7. -2. -1. -1.  3. -2. -1. -1. -1.  2. -1. -1. -1.  0.  0.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[-0.00655058]] False\n",
      "Step 140: Reward 0.300.\n",
      "1 640\n",
      "Epoch: 0 Train loss 4.094, Test loss 4.094\n",
      "1 640\n",
      "Epoch: 1 Train loss 4.094, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 2 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 3 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 4 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 5 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 6 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 7 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 8 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 9 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 10 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 11 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 12 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 13 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 14 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 15 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 16 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 17 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 18 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 19 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 20 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 21 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 22 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 23 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 24 Train loss 4.093, Test loss 4.093\n",
      "Mean train loss: 4.093 Mean val score: 4.093\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0. -1.  4.  0.  2. -1.\n",
      "  -1. 11.  2.  7. -2. -1. -1.  2.  0.  0.  0. -1.  5. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1.  1. -1.  0.  0.  0.  0.]] [[-0.04637557]] False\n",
      "Step 141: Reward 0.259.\n",
      "[[ 0.  1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  5.  0.  3.  0.\n",
      "  -1.  9.  4.  7.  3. -1. -1.  2.  1.  0.  0. -1.  3. -1. -1.  1. -1. -1.\n",
      "  -2.  0. -2. -1.  1.  0.  0.  0.  0.  0.]] [[-0.06239158]] False\n",
      "Step 142: Reward 0.243.\n",
      "[[ 1.  0. -1.  1.  0. -1. -1. -1. -1.  0. -1.  1.  0.  0.  4.  0.  4.  0.\n",
      "   1. 14.  4.  7.  8.  1. -1.  3.  3. -1. -1. -1.  3. -1. -1. -1.  0.  0.\n",
      "  -1.  0. -2. -1.  1.  0.  2.  0.  0.  0.]] [[0.00300714]] False\n",
      "Step 143: Reward 0.309.\n",
      "[[ 1.  0. -1.  2.  0. -1. -1. -1. -1.  0. -1.  1.  0.  3.  4.  0.  4.  0.\n",
      "   1. 14.  5.  7. 11.  1. -1.  8.  3. -1. -2. -1.  3.  1. -1. -1. -1. -1.\n",
      "  -1.  0. -2. -1.  1.  0.  3.  0.  0.  0.]] [[0.08325407]] True\n",
      "Step 144: Reward 0.389.\n",
      "Trial: 18, reward: 2.519641209837288.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  4. -2. -1.  1. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.2671733]] False\n",
      "Step 145: Reward 0.038.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   2.  6. -1. -1.  0.  0. -1.  1. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.18563482]] False\n",
      "Step 146: Reward 0.120.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  7. -1.  2.  2. -1.  0.  4.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -1.  0.  0.  0.]] [[-0.21723703]] False\n",
      "Step 147: Reward 0.088.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1. -1.\n",
      "   4.  8. -1.  2.  4.  0. -1.  6. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.14845325]] False\n",
      "Step 148: Reward 0.157.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1. -1. -1.\n",
      "   6. 11.  4. -3.  3. -1. -1.  6. -1. -1.  1. -1. -1.  0. -1. -1. -1. -1.\n",
      "   2. -1. -2. -1. -2.  0.  0.  0.  0.  0.]] [[-0.11958832]] False\n",
      "Step 149: Reward 0.186.\n",
      "[[ 0.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   6. 11.  5. -1.  1.  0. -1.  7. -2. -1.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "   4. -1.  1. -1. -2.  0.  0.  0.  0.  0.]] [[-0.13629845]] False\n",
      "Step 150: Reward 0.169.\n",
      "1 650\n",
      "Epoch: 0 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 1 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 2 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 3 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 4 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 5 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 6 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 7 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 8 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 9 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 10 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 11 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 12 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 13 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 14 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 15 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 16 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 17 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 18 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 19 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 20 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 21 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 22 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 23 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 24 Train loss 4.115, Test loss 4.115\n",
      "Mean train loss: 4.115 Mean val score: 4.115\n",
      "[[ 0.  0. -1.  0.  0.  1.  0. -1. -1. -1. -1.  0.  0. -1.  1. -1.  0. -1.\n",
      "   6.  8.  6. -3.  0. -1. -1. 10. -1.  0.  0. -1. -1.  0.  0. -1. -1. -1.\n",
      "   8. -1.  1. -1. -1. -1.  3.  0.  0.  0.]] [[-0.00797501]] False\n",
      "Step 151: Reward 0.296.\n",
      "[[ 0.  0.  1.  1.  1. -1. -1.  0.  0. -1.  1. -1.  0. -1.  1. -1.  3. -1.\n",
      "   8.  8.  6. -3. -2.  3. -1. 14. -1.  2.  0. -1. -1. -1.  0. -1. -1. -1.\n",
      "  10. -1.  0. -1. -1. -2.  5.  0.  0.  0.]] [[0.00049061]] True\n",
      "Step 152: Reward 0.305.\n",
      "Trial: 19, reward: 1.3604819974579725.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2.  1. -2.  4. -2. -1. -1. -2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.3042713]] False\n",
      "Step 153: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  1.\n",
      "  -2.  3. -2.  4.  0.  2.  2.  1. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 154: Reward 0.500.\n",
      "[[ 0.  1.  2. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1.  2. -1. -2. -1.\n",
      "   1.  3. -2.  4.  2.  2.  0.  1. -2.  2. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 155: Reward 0.500.\n",
      "[[ 0.  0.  1. -1.  1. -1. -1.  0. -1.  1.  1. -1.  0. -1.  2. -1. -2. -1.\n",
      "   1.  3.  3.  6.  2.  2.  0.  1. -2.  0. -2.  0.  1. -1.  1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.09046555]] False\n",
      "Step 156: Reward 0.395.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  1. -1. -1. -1.  2. -1.  2. -1. -2. -1.\n",
      "   0.  3.  3.  7.  2.  6.  1.  1. -2.  0.  1. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.03641418]] False\n",
      "Step 157: Reward 0.268.\n",
      "[[ 0.  0. -1.  2.  0. -1.  1. -1. -1.  0. -1.  0.  2.  0.  2. -1. -2. -1.\n",
      "   0.  3.  3.  6.  2. 11.  0.  1.  0.  0.  2. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.02685452]] False\n",
      "Step 158: Reward 0.331.\n",
      "[[ 0.  0. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  0.  2.  0.  2. -1.\n",
      "   0.  3.  4.  6.  3. 15. -1.  1.  2.  2.  1.  0.  3. -1.  4. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[-0.12780072]] False\n",
      "Step 159: Reward 0.176.\n",
      "[[ 0.  1. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  2.  5.  2.  2. -1.\n",
      "   3.  3.  4.  6.  3. 16. -1.  1.  3.  1.  1.  0. -1. -1.  2. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.2671573]] True\n",
      "Step 160: Reward 0.571.\n",
      "Trial: 20, reward: 2.7416189724408406.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "all_training_losses = []\n",
    "all_val_scores = []\n",
    "\n",
    "while current_trial < num_episodes:\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "        # --------------- Model Training -----------------\n",
    "        if env_steps % freq_train_model == 0:\n",
    "            dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                model_batch_size,\n",
    "                validation_ratio,\n",
    "                ensemble_size=len(dynamics_model),\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,\n",
    "            )\n",
    "            if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "            training_losses, val_scores = model_trainer.train(\n",
    "                dataset_train,\n",
    "                dataset_val=dataset_val,\n",
    "                num_epochs=num_epochs,\n",
    "                patience=num_epochs,\n",
    "                improvement_threshold=0.01,\n",
    "            )\n",
    "            all_training_losses += training_losses\n",
    "            all_val_scores += val_scores\n",
    "            print(f\"Mean train loss: {np.mean(training_losses):.3f} Mean val score: {np.mean(val_scores):.3f}\")\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        replay_buffer.add(obs, action, next_obs, reward, terminated, truncated)\n",
    "\n",
    "        model_action = action[None, ...]\n",
    "        model_observation = {\n",
    "            \"obs\": obs[None, ...],\n",
    "            \"propagation_indices\": None,\n",
    "        }\n",
    "        model_next_obs, model_reward, model_dones, next_model_state = model_env.step(model_action, model_observation)\n",
    "        #TODO: compare actual next_obs and rewards with model env\n",
    "        print(next_obs-torch.round(model_next_obs).numpy(), reward-model_reward.numpy(), terminated)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "\n",
    "        print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(all_training_losses, 'b', label='Training loss')\n",
    "ax[0].plot(all_val_scores, 'g', label='Eval score')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].plot(np.mean(rewards, axis=-1), 'b', label='Cumulative reward')\n",
    "ax[1].set_xlabel('Episode')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
