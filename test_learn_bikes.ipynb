{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "from mbrl.models import Model\n",
    "from src.env.bikes import Bikes\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load env dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 46]) torch.Size([10000, 10])\n",
      "{'bikes_distr': slice(0, 43, None), 'day': slice(43, 44, None), 'month': slice(44, 45, None), 'time_counter': slice(45, 46, None)} {'truck_centroid': slice(0, 5, None), 'truck_num_bikes': slice(5, 10, None)}\n"
     ]
    }
   ],
   "source": [
    "load_dir = \"datasets/Bikes/None\"\n",
    "path = pathlib.Path(load_dir) / \"replay_buffer.npz\"\n",
    "buffer=np.load(path)\n",
    "next_obs = torch.tensor(buffer[\"next_obs\"], dtype=torch.float32)\n",
    "obs = torch.tensor(buffer[\"obs\"], dtype=torch.float32)\n",
    "act = torch.round(torch.tensor(buffer[\"action\"], dtype=torch.float32))\n",
    "reward = torch.tensor(buffer[\"reward\"], dtype=torch.float32)\n",
    "print(obs.shape, act.shape)\n",
    "\n",
    "num_centroids = 43\n",
    "map_obs = {\n",
    "    \"bikes_distr\": slice(0, num_centroids),\n",
    "    \"day\": slice(num_centroids, 44),\n",
    "    \"month\": slice(44, 45),\n",
    "    \"time_counter\": slice(45, 46),\n",
    "}\n",
    "num_trucks = act.shape[-1]//2\n",
    "map_act={\n",
    "    \"truck_centroid\": slice(0, num_trucks),\n",
    "    \"truck_num_bikes\": slice(num_trucks, 2*num_trucks),\n",
    "}\n",
    "\n",
    "print(map_obs, map_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute delta bikes and obs += delta_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n",
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n"
     ]
    }
   ],
   "source": [
    "obs_before_action = obs.clone()\n",
    "\n",
    "resize = False\n",
    "while obs.ndim < 3:\n",
    "    assert act.ndim == obs.ndim\n",
    "    obs = obs[None, ...]\n",
    "    act = act[None, ...]\n",
    "    resize = True\n",
    "\n",
    "ensemble_size = obs.shape[0]\n",
    "batch_size = obs.shape[1]\n",
    "distr_size = len(obs[0, 0, map_obs[\"bikes_distr\"]]) #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "# Compute delta_bikes in a parallel way\n",
    "delta_bikes = np.zeros((ensemble_size, batch_size, distr_size), dtype=int)\n",
    "truck_centroids = act[..., map_act[\"truck_centroid\"]] #self.map_act[\"truck_centroid\"]\n",
    "truck_bikes = act[..., map_act[\"truck_num_bikes\"]] #self.map_act[\"truck_num_bikes\"]\n",
    "n = distr_size\n",
    "truck_centroids = np.reshape(\n",
    "    truck_centroids, (truck_centroids.shape[0] * truck_centroids.shape[1], -1)\n",
    ")\n",
    "offset = np.arange(truck_centroids.shape[0])[..., None]\n",
    "truck_centroids_offset = truck_centroids + offset * n\n",
    "unq, inv = np.unique(truck_centroids_offset.ravel(), return_inverse=True)\n",
    "unq = unq.astype(int)\n",
    "sol = np.bincount(inv, truck_bikes.ravel())\n",
    "delta_bikes[\n",
    "    unq // (batch_size * n),\n",
    "    (unq % (batch_size * n)) // n,\n",
    "    (unq % (batch_size * n)) % n,\n",
    "] = sol\n",
    "\n",
    "if resize:\n",
    "    delta_bikes = delta_bikes.reshape((batch_size, -1))\n",
    "    act = act.reshape((batch_size, -1))\n",
    "    obs = obs.reshape((batch_size, -1))\n",
    "\n",
    "# Update obs\n",
    "obs[..., map_obs[\"bikes_distr\"]] += delta_bikes #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "print(torch.sum(obs-obs_before_action, axis=-1))\n",
    "print(torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "assert torch.all(torch.sum(obs-obs_before_action, axis=-1) == torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "# Super long check to see if preprocess is good, and it is so far\n",
    "for obs_, previous_obs_, truck_centroids, truck_num_bikes in zip(obs, obs_before_action, act[...,map_act[\"truck_centroid\"]], act[...,map_act[\"truck_num_bikes\"]]):\n",
    "    bikes_idx = torch.nonzero(truck_num_bikes, as_tuple=True)[0]\n",
    "    truck_centroids = truck_centroids[bikes_idx]\n",
    "    centroids_new_bikes = torch.nonzero(obs_-previous_obs_, as_tuple=True)[0]\n",
    "    # print(torch.sort(centroids_new_bikes).values)\n",
    "    # print(torch.unique(truck_centroids))\n",
    "    # print(torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids))\n",
    "    assert torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y from obs and next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 44]) torch.Size([10000, 43])\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 10000\n",
    "input_obs_keys = [\"bikes_distr\", \"time_counter\"]\n",
    "input_act_keys = [] #not implemented\n",
    "output_keys = [\"bikes_distr\"]\n",
    "\n",
    "input_mask = np.zeros(obs.shape[-1])\n",
    "for key in input_obs_keys:\n",
    "    input_mask[map_obs[key]] = 1\n",
    "input_mask = np.ma.make_mask(input_mask)\n",
    "\n",
    "output_mask = np.zeros(obs.shape[-1])\n",
    "for key in output_keys:\n",
    "    output_mask[map_obs[key]] = 1\n",
    "output_mask = np.ma.make_mask(output_mask)\n",
    "\n",
    "assert obs.ndim == 2\n",
    "assert next_obs.ndim == 2\n",
    "x = obs[:dataset_size, input_mask]\n",
    "y = next_obs[:dataset_size, output_mask]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe add a date proxy, weather ? holiday, week-end ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional output preds (e.g. reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 43]) torch.Size([10000, 1])\n",
      "torch.Size([10000, 44])\n"
     ]
    }
   ],
   "source": [
    "learned_rewards = True\n",
    "if learned_rewards:\n",
    "    reward_ = reward[:dataset_size, ...].unsqueeze(-1)\n",
    "    print(y.shape, reward_.shape)\n",
    "    y = torch.cat([y, reward_], dim=-1)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_x, test_x and train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n"
     ]
    }
   ],
   "source": [
    "test_split_ratio = 0.2\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# idx = torch.randperm(x.shape[0])\n",
    "# x = x[idx, :]\n",
    "# y = y[idx, :]\n",
    "\n",
    "train_x = x[int(test_split_ratio*dataset_size):, ...]\n",
    "train_y = y[int(test_split_ratio*dataset_size):, ...]\n",
    "test_x = x[:int(test_split_ratio*dataset_size), ...]\n",
    "test_y = y[:int(test_split_ratio*dataset_size), ...]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.linear_regression import LinearRegression\n",
    "\n",
    "learningRate = 0.01 \n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 7.188284873962402, R2 -2.206334352493286\n",
      "Eval loss 6.941813945770264, R2 -1.9536854028701782\n",
      "epoch 1, loss 6.912776947021484, R2 -2.1389591693878174\n",
      "Eval loss 6.691708087921143, R2 -1.8928974866867065\n",
      "epoch 2, loss 6.661588191986084, R2 -2.0771067142486572\n",
      "Eval loss 6.463588714599609, R2 -1.8370894193649292\n",
      "epoch 3, loss 6.432467460632324, R2 -2.0202696323394775\n",
      "Eval loss 6.255420684814453, R2 -1.785797119140625\n",
      "epoch 4, loss 6.223369121551514, R2 -1.9679820537567139\n",
      "Eval loss 6.065353870391846, R2 -1.7386103868484497\n",
      "epoch 5, loss 6.03244161605835, R2 -1.919825792312622\n",
      "Eval loss 5.891716480255127, R2 -1.695146083831787\n",
      "epoch 6, loss 5.858002185821533, R2 -1.8754196166992188\n",
      "Eval loss 5.732985973358154, R2 -1.6550629138946533\n",
      "epoch 7, loss 5.698524475097656, R2 -1.8344193696975708\n",
      "Eval loss 5.587784767150879, R2 -1.6180499792099\n",
      "epoch 8, loss 5.5526251792907715, R2 -1.7965086698532104\n",
      "Eval loss 5.454861640930176, R2 -1.5838249921798706\n",
      "epoch 9, loss 5.419047832489014, R2 -1.7614086866378784\n",
      "Eval loss 5.333081245422363, R2 -1.5521330833435059\n",
      "epoch 10, loss 5.29665470123291, R2 -1.728857398033142\n",
      "Eval loss 5.221414566040039, R2 -1.5227385759353638\n",
      "epoch 11, loss 5.184411525726318, R2 -1.6986216306686401\n",
      "Eval loss 5.1189284324646, R2 -1.4954334497451782\n",
      "epoch 12, loss 5.081381320953369, R2 -1.6704894304275513\n",
      "Eval loss 5.02477502822876, R2 -1.4700270891189575\n",
      "epoch 13, loss 4.9867143630981445, R2 -1.6442681550979614\n",
      "Eval loss 4.938185214996338, R2 -1.4463449716567993\n",
      "epoch 14, loss 4.899638652801514, R2 -1.619783878326416\n",
      "Eval loss 4.858463287353516, R2 -1.4242287874221802\n",
      "epoch 15, loss 4.819454193115234, R2 -1.596878170967102\n",
      "Eval loss 4.784975528717041, R2 -1.4035359621047974\n",
      "epoch 16, loss 4.7455267906188965, R2 -1.5754050016403198\n",
      "Eval loss 4.7171478271484375, R2 -1.384137749671936\n",
      "epoch 17, loss 4.677281379699707, R2 -1.555237889289856\n",
      "Eval loss 4.654460906982422, R2 -1.3659168481826782\n",
      "epoch 18, loss 4.614193439483643, R2 -1.5362540483474731\n",
      "Eval loss 4.596441745758057, R2 -1.3487662076950073\n",
      "epoch 19, loss 4.555790424346924, R2 -1.5183517932891846\n",
      "Eval loss 4.542661190032959, R2 -1.332589030265808\n",
      "epoch 20, loss 4.501642227172852, R2 -1.5014287233352661\n",
      "Eval loss 4.492733001708984, R2 -1.3172961473464966\n",
      "epoch 21, loss 4.451358795166016, R2 -1.4853984117507935\n",
      "Eval loss 4.4463019371032715, R2 -1.3028109073638916\n",
      "epoch 22, loss 4.4045867919921875, R2 -1.4701805114746094\n",
      "Eval loss 4.403049945831299, R2 -1.2890585660934448\n",
      "epoch 23, loss 4.36100435256958, R2 -1.455702304840088\n",
      "Eval loss 4.362684726715088, R2 -1.275974154472351\n",
      "epoch 24, loss 4.320319652557373, R2 -1.4418981075286865\n",
      "Eval loss 4.324944496154785, R2 -1.2634981870651245\n",
      "epoch 25, loss 4.2822699546813965, R2 -1.4287077188491821\n",
      "Eval loss 4.289591312408447, R2 -1.2515771389007568\n",
      "epoch 26, loss 4.246615886688232, R2 -1.4160774946212769\n",
      "Eval loss 4.256406307220459, R2 -1.2401602268218994\n",
      "epoch 27, loss 4.213138103485107, R2 -1.403956413269043\n",
      "Eval loss 4.225194454193115, R2 -1.2292054891586304\n",
      "epoch 28, loss 4.181640148162842, R2 -1.392301321029663\n",
      "Eval loss 4.195775985717773, R2 -1.2186713218688965\n",
      "epoch 29, loss 4.151943683624268, R2 -1.38107168674469\n",
      "Eval loss 4.167990684509277, R2 -1.208521842956543\n",
      "epoch 30, loss 4.123886585235596, R2 -1.3702294826507568\n",
      "Eval loss 4.141689777374268, R2 -1.198723316192627\n",
      "epoch 31, loss 4.097319602966309, R2 -1.3597428798675537\n",
      "Eval loss 4.1167426109313965, R2 -1.1892447471618652\n",
      "epoch 32, loss 4.072111129760742, R2 -1.34958016872406\n",
      "Eval loss 4.0930256843566895, R2 -1.1800589561462402\n",
      "epoch 33, loss 4.048138618469238, R2 -1.3397150039672852\n",
      "Eval loss 4.070431232452393, R2 -1.1711417436599731\n",
      "epoch 34, loss 4.02529239654541, R2 -1.330121636390686\n",
      "Eval loss 4.048858165740967, R2 -1.162472128868103\n",
      "epoch 35, loss 4.003473281860352, R2 -1.3207770586013794\n",
      "Eval loss 4.02821683883667, R2 -1.1540261507034302\n",
      "epoch 36, loss 3.9825899600982666, R2 -1.311661720275879\n",
      "Eval loss 4.008426189422607, R2 -1.1457875967025757\n",
      "epoch 37, loss 3.962559700012207, R2 -1.3027548789978027\n",
      "Eval loss 3.989410400390625, R2 -1.1377393007278442\n",
      "epoch 38, loss 3.9433088302612305, R2 -1.2940438985824585\n",
      "Eval loss 3.971102714538574, R2 -1.1298651695251465\n",
      "epoch 39, loss 3.924769639968872, R2 -1.2855082750320435\n",
      "Eval loss 3.9534425735473633, R2 -1.1221513748168945\n",
      "epoch 40, loss 3.9068799018859863, R2 -1.277138590812683\n",
      "Eval loss 3.936372756958008, R2 -1.1145867109298706\n",
      "epoch 41, loss 3.88958477973938, R2 -1.2689181566238403\n",
      "Eval loss 3.9198436737060547, R2 -1.1071572303771973\n",
      "epoch 42, loss 3.872833013534546, R2 -1.2608370780944824\n",
      "Eval loss 3.903810739517212, R2 -1.0998553037643433\n",
      "epoch 43, loss 3.856579542160034, R2 -1.25288724899292\n",
      "Eval loss 3.888230562210083, R2 -1.09266996383667\n",
      "epoch 44, loss 3.840782403945923, R2 -1.2450553178787231\n",
      "Eval loss 3.873065948486328, R2 -1.0855927467346191\n",
      "epoch 45, loss 3.8254029750823975, R2 -1.2373350858688354\n",
      "Eval loss 3.858283042907715, R2 -1.0786153078079224\n",
      "epoch 46, loss 3.8104076385498047, R2 -1.2297173738479614\n",
      "Eval loss 3.84385085105896, R2 -1.071731686592102\n",
      "epoch 47, loss 3.795764923095703, R2 -1.222196102142334\n",
      "Eval loss 3.8297407627105713, R2 -1.0649346113204956\n",
      "epoch 48, loss 3.781446695327759, R2 -1.2147632837295532\n",
      "Eval loss 3.815927505493164, R2 -1.0582187175750732\n",
      "epoch 49, loss 3.767427921295166, R2 -1.2074156999588013\n",
      "Eval loss 3.8023877143859863, R2 -1.0515769720077515\n",
      "epoch 50, loss 3.7536840438842773, R2 -1.200145959854126\n",
      "Eval loss 3.789100170135498, R2 -1.0450079441070557\n",
      "epoch 51, loss 3.74019455909729, R2 -1.1929497718811035\n",
      "Eval loss 3.776045083999634, R2 -1.0385042428970337\n",
      "epoch 52, loss 3.726940631866455, R2 -1.1858220100402832\n",
      "Eval loss 3.763206720352173, R2 -1.0320640802383423\n",
      "epoch 53, loss 3.7139041423797607, R2 -1.1787587404251099\n",
      "Eval loss 3.75056791305542, R2 -1.025681972503662\n",
      "epoch 54, loss 3.701068878173828, R2 -1.171758770942688\n",
      "Eval loss 3.738114356994629, R2 -1.0193556547164917\n",
      "epoch 55, loss 3.6884219646453857, R2 -1.1648160219192505\n",
      "Eval loss 3.7258338928222656, R2 -1.0130805969238281\n",
      "epoch 56, loss 3.6759488582611084, R2 -1.1579275131225586\n",
      "Eval loss 3.7137134075164795, R2 -1.0068563222885132\n",
      "epoch 57, loss 3.66363787651062, R2 -1.15109121799469\n",
      "Eval loss 3.7017428874969482, R2 -1.0006792545318604\n",
      "epoch 58, loss 3.6514787673950195, R2 -1.1443063020706177\n",
      "Eval loss 3.6899125576019287, R2 -0.9945465326309204\n",
      "epoch 59, loss 3.6394617557525635, R2 -1.1375681161880493\n",
      "Eval loss 3.678213119506836, R2 -0.988456130027771\n",
      "epoch 60, loss 3.6275765895843506, R2 -1.130874514579773\n",
      "Eval loss 3.6666371822357178, R2 -0.982408344745636\n",
      "epoch 61, loss 3.615817070007324, R2 -1.124224305152893\n",
      "Eval loss 3.655176877975464, R2 -0.9763987064361572\n",
      "epoch 62, loss 3.6041746139526367, R2 -1.1176172494888306\n",
      "Eval loss 3.6438255310058594, R2 -0.9704267382621765\n",
      "epoch 63, loss 3.5926427841186523, R2 -1.1110495328903198\n",
      "Eval loss 3.6325767040252686, R2 -0.964490532875061\n",
      "epoch 64, loss 3.5812151432037354, R2 -1.1045206785202026\n",
      "Eval loss 3.6214253902435303, R2 -0.9585897326469421\n",
      "epoch 65, loss 3.569887161254883, R2 -1.0980286598205566\n",
      "Eval loss 3.6103665828704834, R2 -0.9527223706245422\n",
      "epoch 66, loss 3.558652400970459, R2 -1.0915734767913818\n",
      "Eval loss 3.599395513534546, R2 -0.9468882083892822\n",
      "epoch 67, loss 3.547506809234619, R2 -1.0851529836654663\n",
      "Eval loss 3.5885074138641357, R2 -0.9410839676856995\n",
      "epoch 68, loss 3.5364468097686768, R2 -1.0787668228149414\n",
      "Eval loss 3.577699661254883, R2 -0.9353125095367432\n",
      "epoch 69, loss 3.5254673957824707, R2 -1.0724139213562012\n",
      "Eval loss 3.566967248916626, R2 -0.9295701384544373\n",
      "epoch 70, loss 3.514565944671631, R2 -1.066092610359192\n",
      "Eval loss 3.5563082695007324, R2 -0.9238560199737549\n",
      "epoch 71, loss 3.503739356994629, R2 -1.0598036050796509\n",
      "Eval loss 3.5457193851470947, R2 -0.9181708693504333\n",
      "epoch 72, loss 3.492983341217041, R2 -1.0535448789596558\n",
      "Eval loss 3.535198211669922, R2 -0.9125133752822876\n",
      "epoch 73, loss 3.482296943664551, R2 -1.0473167896270752\n",
      "Eval loss 3.5247414112091064, R2 -0.9068824648857117\n",
      "epoch 74, loss 3.471676826477051, R2 -1.0411176681518555\n",
      "Eval loss 3.514348030090332, R2 -0.9012775421142578\n",
      "epoch 75, loss 3.4611215591430664, R2 -1.0349472761154175\n",
      "Eval loss 3.5040152072906494, R2 -0.8956995010375977\n",
      "epoch 76, loss 3.4506278038024902, R2 -1.0288059711456299\n",
      "Eval loss 3.493741512298584, R2 -0.8901475667953491\n",
      "epoch 77, loss 3.440194606781006, R2 -1.0226924419403076\n",
      "Eval loss 3.483524799346924, R2 -0.8846195340156555\n",
      "epoch 78, loss 3.4298202991485596, R2 -1.0166062116622925\n",
      "Eval loss 3.4733636379241943, R2 -0.879116952419281\n",
      "epoch 79, loss 3.4195027351379395, R2 -1.0105470418930054\n",
      "Eval loss 3.463257074356079, R2 -0.8736376166343689\n",
      "epoch 80, loss 3.409241199493408, R2 -1.0045145750045776\n",
      "Eval loss 3.4532032012939453, R2 -0.868182897567749\n",
      "epoch 81, loss 3.399033784866333, R2 -0.9985086917877197\n",
      "Eval loss 3.4432010650634766, R2 -0.8627517223358154\n",
      "epoch 82, loss 3.3888795375823975, R2 -0.9925287365913391\n",
      "Eval loss 3.4332499504089355, R2 -0.8573440313339233\n",
      "epoch 83, loss 3.378777027130127, R2 -0.9865743517875671\n",
      "Eval loss 3.4233477115631104, R2 -0.8519595861434937\n",
      "epoch 84, loss 3.3687257766723633, R2 -0.9806452393531799\n",
      "Eval loss 3.413494348526001, R2 -0.8465971350669861\n",
      "epoch 85, loss 3.358724355697632, R2 -0.9747412204742432\n",
      "Eval loss 3.403688907623291, R2 -0.8412580490112305\n",
      "epoch 86, loss 3.3487720489501953, R2 -0.9688623547554016\n",
      "Eval loss 3.393930435180664, R2 -0.8359401226043701\n",
      "epoch 87, loss 3.3388679027557373, R2 -0.9630081653594971\n",
      "Eval loss 3.3842172622680664, R2 -0.8306451439857483\n",
      "epoch 88, loss 3.3290114402770996, R2 -0.9571783542633057\n",
      "Eval loss 3.3745508193969727, R2 -0.8253721594810486\n",
      "epoch 89, loss 3.319201707839966, R2 -0.9513723850250244\n",
      "Eval loss 3.364928722381592, R2 -0.8201187252998352\n",
      "epoch 90, loss 3.3094382286071777, R2 -0.9455907344818115\n",
      "Eval loss 3.355350971221924, R2 -0.8148897290229797\n",
      "epoch 91, loss 3.29971981048584, R2 -0.939832866191864\n",
      "Eval loss 3.345816135406494, R2 -0.8096809983253479\n",
      "epoch 92, loss 3.2900469303131104, R2 -0.9340982437133789\n",
      "Eval loss 3.336325168609619, R2 -0.8044931292533875\n",
      "epoch 93, loss 3.2804183959960938, R2 -0.9283871054649353\n",
      "Eval loss 3.326876401901245, R2 -0.7993254065513611\n",
      "epoch 94, loss 3.2708334922790527, R2 -0.9226996898651123\n",
      "Eval loss 3.317470073699951, R2 -0.7941802740097046\n",
      "epoch 95, loss 3.261291980743408, R2 -0.9170337915420532\n",
      "Eval loss 3.30810546875, R2 -0.7890543341636658\n",
      "epoch 96, loss 3.251793622970581, R2 -0.9113928079605103\n",
      "Eval loss 3.2987818717956543, R2 -0.7839497923851013\n",
      "epoch 97, loss 3.242338180541992, R2 -0.9057741761207581\n",
      "Eval loss 3.2894999980926514, R2 -0.7788648009300232\n",
      "epoch 98, loss 3.232924461364746, R2 -0.9001778960227966\n",
      "Eval loss 3.2802577018737793, R2 -0.7738003134727478\n",
      "epoch 99, loss 3.2235524654388428, R2 -0.894603967666626\n",
      "Eval loss 3.2710561752319336, R2 -0.7687564492225647\n",
      "epoch 100, loss 3.2142221927642822, R2 -0.8890524506568909\n",
      "Eval loss 3.261894464492798, R2 -0.7637321352958679\n",
      "epoch 101, loss 3.204932451248169, R2 -0.8835233449935913\n",
      "Eval loss 3.252772092819214, R2 -0.7587276697158813\n",
      "epoch 102, loss 3.1956841945648193, R2 -0.8780161142349243\n",
      "Eval loss 3.2436888217926025, R2 -0.7537429928779602\n",
      "epoch 103, loss 3.1864757537841797, R2 -0.8725312948226929\n",
      "Eval loss 3.234644889831543, R2 -0.7487784028053284\n",
      "epoch 104, loss 3.1773083209991455, R2 -0.8670671582221985\n",
      "Eval loss 3.2256391048431396, R2 -0.7438333034515381\n",
      "epoch 105, loss 3.168179988861084, R2 -0.8616265654563904\n",
      "Eval loss 3.216672658920288, R2 -0.7389079928398132\n",
      "epoch 106, loss 3.1590917110443115, R2 -0.8562065362930298\n",
      "Eval loss 3.2077431678771973, R2 -0.7340002655982971\n",
      "epoch 107, loss 3.1500425338745117, R2 -0.8508080840110779\n",
      "Eval loss 3.198852300643921, R2 -0.7291138172149658\n",
      "epoch 108, loss 3.1410326957702637, R2 -0.8454309701919556\n",
      "Eval loss 3.1899988651275635, R2 -0.724245011806488\n",
      "epoch 109, loss 3.132061004638672, R2 -0.8400757312774658\n",
      "Eval loss 3.181183338165283, R2 -0.7193955779075623\n",
      "epoch 110, loss 3.1231281757354736, R2 -0.8347410559654236\n",
      "Eval loss 3.1724045276641846, R2 -0.7145666480064392\n",
      "epoch 111, loss 3.114234447479248, R2 -0.8294273614883423\n",
      "Eval loss 3.1636626720428467, R2 -0.7097546458244324\n",
      "epoch 112, loss 3.1053779125213623, R2 -0.8241351842880249\n",
      "Eval loss 3.1549572944641113, R2 -0.7049627900123596\n",
      "epoch 113, loss 3.096559762954712, R2 -0.8188631534576416\n",
      "Eval loss 3.146289110183716, R2 -0.7001885771751404\n",
      "epoch 114, loss 3.0877790451049805, R2 -0.8136131763458252\n",
      "Eval loss 3.1376569271087646, R2 -0.6954337358474731\n",
      "epoch 115, loss 3.079035520553589, R2 -0.8083832263946533\n",
      "Eval loss 3.129060983657837, R2 -0.6906971335411072\n",
      "epoch 116, loss 3.0703301429748535, R2 -0.8031734824180603\n",
      "Eval loss 3.1205010414123535, R2 -0.6859784722328186\n",
      "epoch 117, loss 3.0616607666015625, R2 -0.7979848980903625\n",
      "Eval loss 3.1119768619537354, R2 -0.6812784671783447\n",
      "epoch 118, loss 3.0530290603637695, R2 -0.7928163409233093\n",
      "Eval loss 3.1034882068634033, R2 -0.6765971183776855\n",
      "epoch 119, loss 3.044433832168579, R2 -0.7876673340797424\n",
      "Eval loss 3.0950348377227783, R2 -0.671933114528656\n",
      "epoch 120, loss 3.035875082015991, R2 -0.7825402617454529\n",
      "Eval loss 3.0866165161132812, R2 -0.6672877669334412\n",
      "epoch 121, loss 3.0273525714874268, R2 -0.777431309223175\n",
      "Eval loss 3.0782337188720703, R2 -0.662660539150238\n",
      "epoch 122, loss 3.0188658237457275, R2 -0.7723443508148193\n",
      "Eval loss 3.069885730743408, R2 -0.6580512523651123\n",
      "epoch 123, loss 3.01041579246521, R2 -0.7672768235206604\n",
      "Eval loss 3.061572551727295, R2 -0.6534590721130371\n",
      "epoch 124, loss 3.0020010471343994, R2 -0.7622289061546326\n",
      "Eval loss 3.053293228149414, R2 -0.6488854289054871\n",
      "epoch 125, loss 2.993621826171875, R2 -0.7572002410888672\n",
      "Eval loss 3.045048713684082, R2 -0.6443291902542114\n",
      "epoch 126, loss 2.9852781295776367, R2 -0.7521914839744568\n",
      "Eval loss 3.0368385314941406, R2 -0.6397907733917236\n",
      "epoch 127, loss 2.9769694805145264, R2 -0.7472034096717834\n",
      "Eval loss 3.0286619663238525, R2 -0.6352692246437073\n",
      "epoch 128, loss 2.968696117401123, R2 -0.7422332167625427\n",
      "Eval loss 3.020519495010376, R2 -0.6307653784751892\n",
      "epoch 129, loss 2.9604573249816895, R2 -0.7372837066650391\n",
      "Eval loss 3.0124104022979736, R2 -0.626279354095459\n",
      "epoch 130, loss 2.952253580093384, R2 -0.7323522567749023\n",
      "Eval loss 3.004335641860962, R2 -0.6218108534812927\n",
      "epoch 131, loss 2.9440841674804688, R2 -0.7274412512779236\n",
      "Eval loss 2.996293783187866, R2 -0.617358922958374\n",
      "epoch 132, loss 2.9359493255615234, R2 -0.7225494980812073\n",
      "Eval loss 2.9882848262786865, R2 -0.6129239797592163\n",
      "epoch 133, loss 2.9278483390808105, R2 -0.7176762819290161\n",
      "Eval loss 2.9803097248077393, R2 -0.6085066199302673\n",
      "epoch 134, loss 2.9197819232940674, R2 -0.7128223180770874\n",
      "Eval loss 2.9723665714263916, R2 -0.6041061282157898\n",
      "epoch 135, loss 2.9117488861083984, R2 -0.7079871296882629\n",
      "Eval loss 2.964456796646118, R2 -0.5997222065925598\n",
      "epoch 136, loss 2.90375018119812, R2 -0.7031707763671875\n",
      "Eval loss 2.9565794467926025, R2 -0.595355749130249\n",
      "epoch 137, loss 2.895784854888916, R2 -0.6983732581138611\n",
      "Eval loss 2.948734760284424, R2 -0.5910056829452515\n",
      "epoch 138, loss 2.887852668762207, R2 -0.6935943961143494\n",
      "Eval loss 2.940922260284424, R2 -0.5866730809211731\n",
      "epoch 139, loss 2.8799538612365723, R2 -0.6888341903686523\n",
      "Eval loss 2.9331419467926025, R2 -0.5823566317558289\n",
      "epoch 140, loss 2.8720884323120117, R2 -0.6840920448303223\n",
      "Eval loss 2.925393581390381, R2 -0.5780560970306396\n",
      "epoch 141, loss 2.864255666732788, R2 -0.6793689131736755\n",
      "Eval loss 2.9176769256591797, R2 -0.5737726092338562\n",
      "epoch 142, loss 2.8564560413360596, R2 -0.6746641397476196\n",
      "Eval loss 2.9099926948547363, R2 -0.5695061087608337\n",
      "epoch 143, loss 2.848688840866089, R2 -0.6699774861335754\n",
      "Eval loss 2.902339458465576, R2 -0.5652551054954529\n",
      "epoch 144, loss 2.840954303741455, R2 -0.6653090119361877\n",
      "Eval loss 2.8947176933288574, R2 -0.5610209107398987\n",
      "epoch 145, loss 2.833251714706421, R2 -0.6606587767601013\n",
      "Eval loss 2.887127637863159, R2 -0.5568026900291443\n",
      "epoch 146, loss 2.8255820274353027, R2 -0.6560266017913818\n",
      "Eval loss 2.879568338394165, R2 -0.5526009202003479\n",
      "epoch 147, loss 2.817944049835205, R2 -0.6514121294021606\n",
      "Eval loss 2.872040033340454, R2 -0.5484148263931274\n",
      "epoch 148, loss 2.810337781906128, R2 -0.646815299987793\n",
      "Eval loss 2.8645429611206055, R2 -0.5442453026771545\n",
      "epoch 149, loss 2.8027634620666504, R2 -0.6422372460365295\n",
      "Eval loss 2.857076644897461, R2 -0.5400913953781128\n",
      "epoch 150, loss 2.7952210903167725, R2 -0.6376757025718689\n",
      "Eval loss 2.8496406078338623, R2 -0.5359534621238708\n",
      "epoch 151, loss 2.7877097129821777, R2 -0.6331333518028259\n",
      "Eval loss 2.8422353267669678, R2 -0.5318306088447571\n",
      "epoch 152, loss 2.7802300453186035, R2 -0.6286075115203857\n",
      "Eval loss 2.83486008644104, R2 -0.5277243256568909\n",
      "epoch 153, loss 2.7727816104888916, R2 -0.6240997910499573\n",
      "Eval loss 2.8275153636932373, R2 -0.5236331224441528\n",
      "epoch 154, loss 2.765363931655884, R2 -0.6196095943450928\n",
      "Eval loss 2.820200204849243, R2 -0.5195584297180176\n",
      "epoch 155, loss 2.7579777240753174, R2 -0.615136444568634\n",
      "Eval loss 2.812915325164795, R2 -0.5154990553855896\n",
      "epoch 156, loss 2.7506215572357178, R2 -0.6106806397438049\n",
      "Eval loss 2.8056602478027344, R2 -0.5114549398422241\n",
      "epoch 157, loss 2.7432963848114014, R2 -0.6062419414520264\n",
      "Eval loss 2.7984349727630615, R2 -0.507425844669342\n",
      "epoch 158, loss 2.736001491546631, R2 -0.6018202900886536\n",
      "Eval loss 2.791239023208618, R2 -0.5034124851226807\n",
      "epoch 159, loss 2.7287375926971436, R2 -0.5974162817001343\n",
      "Eval loss 2.7840726375579834, R2 -0.4994150400161743\n",
      "epoch 160, loss 2.721503257751465, R2 -0.5930286049842834\n",
      "Eval loss 2.776935338973999, R2 -0.4954322874546051\n",
      "epoch 161, loss 2.714299440383911, R2 -0.5886582732200623\n",
      "Eval loss 2.769827127456665, R2 -0.49146461486816406\n",
      "epoch 162, loss 2.707125425338745, R2 -0.5843040347099304\n",
      "Eval loss 2.7627482414245605, R2 -0.4875118136405945\n",
      "epoch 163, loss 2.699981212615967, R2 -0.5799680948257446\n",
      "Eval loss 2.755697727203369, R2 -0.48357391357421875\n",
      "epoch 164, loss 2.692866802215576, R2 -0.57564777135849\n",
      "Eval loss 2.748676300048828, R2 -0.47965195775032043\n",
      "epoch 165, loss 2.685781717300415, R2 -0.5713446140289307\n",
      "Eval loss 2.7416834831237793, R2 -0.4757440388202667\n",
      "epoch 166, loss 2.6787261962890625, R2 -0.5670580267906189\n",
      "Eval loss 2.7347190380096436, R2 -0.4718513488769531\n",
      "epoch 167, loss 2.6717000007629395, R2 -0.5627877116203308\n",
      "Eval loss 2.727783441543579, R2 -0.46797361969947815\n",
      "epoch 168, loss 2.6647026538848877, R2 -0.5585333704948425\n",
      "Eval loss 2.7208752632141113, R2 -0.46411025524139404\n",
      "epoch 169, loss 2.6577348709106445, R2 -0.5542967915534973\n",
      "Eval loss 2.713996171951294, R2 -0.46026164293289185\n",
      "epoch 170, loss 2.6507956981658936, R2 -0.5500747561454773\n",
      "Eval loss 2.707144260406494, R2 -0.4564276933670044\n",
      "epoch 171, loss 2.643885374069214, R2 -0.5458705425262451\n",
      "Eval loss 2.7003204822540283, R2 -0.45260798931121826\n",
      "epoch 172, loss 2.6370036602020264, R2 -0.5416818261146545\n",
      "Eval loss 2.6935245990753174, R2 -0.4488033354282379\n",
      "epoch 173, loss 2.630150318145752, R2 -0.5375092029571533\n",
      "Eval loss 2.686756134033203, R2 -0.44501325488090515\n",
      "epoch 174, loss 2.6233255863189697, R2 -0.5333524346351624\n",
      "Eval loss 2.6800153255462646, R2 -0.44123613834381104\n",
      "epoch 175, loss 2.6165289878845215, R2 -0.5292119979858398\n",
      "Eval loss 2.673301935195923, R2 -0.43747472763061523\n",
      "epoch 176, loss 2.609760284423828, R2 -0.5250873565673828\n",
      "Eval loss 2.6666157245635986, R2 -0.4337274432182312\n",
      "epoch 177, loss 2.603019952774048, R2 -0.5209783911705017\n",
      "Eval loss 2.659956216812134, R2 -0.42999401688575745\n",
      "epoch 178, loss 2.5963072776794434, R2 -0.5168853998184204\n",
      "Eval loss 2.6533243656158447, R2 -0.4262744188308716\n",
      "epoch 179, loss 2.5896224975585938, R2 -0.5128079056739807\n",
      "Eval loss 2.646719455718994, R2 -0.4225689172744751\n",
      "epoch 180, loss 2.582965612411499, R2 -0.5087456703186035\n",
      "Eval loss 2.640141010284424, R2 -0.418878436088562\n",
      "epoch 181, loss 2.5763356685638428, R2 -0.504700243473053\n",
      "Eval loss 2.633589029312134, R2 -0.4152011573314667\n",
      "epoch 182, loss 2.5697333812713623, R2 -0.500669538974762\n",
      "Eval loss 2.6270639896392822, R2 -0.4115374684333801\n",
      "epoch 183, loss 2.5631582736968994, R2 -0.496654212474823\n",
      "Eval loss 2.620565414428711, R2 -0.4078882932662964\n",
      "epoch 184, loss 2.556610345840454, R2 -0.492654412984848\n",
      "Eval loss 2.6140925884246826, R2 -0.4042520523071289\n",
      "epoch 185, loss 2.5500893592834473, R2 -0.4886699616909027\n",
      "Eval loss 2.6076467037200928, R2 -0.40062981843948364\n",
      "epoch 186, loss 2.543595314025879, R2 -0.4847007095813751\n",
      "Eval loss 2.6012260913848877, R2 -0.3970218002796173\n",
      "epoch 187, loss 2.537127733230591, R2 -0.48074638843536377\n",
      "Eval loss 2.594832181930542, R2 -0.3934271037578583\n",
      "epoch 188, loss 2.5306873321533203, R2 -0.476807564496994\n",
      "Eval loss 2.58846378326416, R2 -0.3898458182811737\n",
      "epoch 189, loss 2.524273157119751, R2 -0.472883403301239\n",
      "Eval loss 2.5821213722229004, R2 -0.3862774968147278\n",
      "epoch 190, loss 2.517885684967041, R2 -0.46897467970848083\n",
      "Eval loss 2.5758042335510254, R2 -0.38272368907928467\n",
      "epoch 191, loss 2.511524200439453, R2 -0.4650801718235016\n",
      "Eval loss 2.5695126056671143, R2 -0.37918320298194885\n",
      "epoch 192, loss 2.5051889419555664, R2 -0.46120110154151917\n",
      "Eval loss 2.563246726989746, R2 -0.3756553828716278\n",
      "epoch 193, loss 2.498879909515381, R2 -0.4573371708393097\n",
      "Eval loss 2.5570058822631836, R2 -0.3721407949924469\n",
      "epoch 194, loss 2.4925966262817383, R2 -0.45348694920539856\n",
      "Eval loss 2.5507900714874268, R2 -0.36863991618156433\n",
      "epoch 195, loss 2.4863390922546387, R2 -0.44965216517448425\n",
      "Eval loss 2.544599771499634, R2 -0.36515191197395325\n",
      "epoch 196, loss 2.4801077842712402, R2 -0.44583186507225037\n",
      "Eval loss 2.5384342670440674, R2 -0.3616768419742584\n",
      "epoch 197, loss 2.4739017486572266, R2 -0.4420262277126312\n",
      "Eval loss 2.5322933197021484, R2 -0.35821545124053955\n",
      "epoch 198, loss 2.4677212238311768, R2 -0.43823572993278503\n",
      "Eval loss 2.526177167892456, R2 -0.35476648807525635\n",
      "epoch 199, loss 2.4615659713745117, R2 -0.43445882201194763\n",
      "Eval loss 2.5200860500335693, R2 -0.3513307571411133\n",
      "epoch 200, loss 2.4554362297058105, R2 -0.43069639801979065\n",
      "Eval loss 2.514019250869751, R2 -0.34790748357772827\n",
      "epoch 201, loss 2.449331283569336, R2 -0.42694786190986633\n",
      "Eval loss 2.507976770401001, R2 -0.3444974720478058\n",
      "epoch 202, loss 2.443251848220825, R2 -0.4232146143913269\n",
      "Eval loss 2.5019583702087402, R2 -0.34110045433044434\n",
      "epoch 203, loss 2.437196731567383, R2 -0.4194943308830261\n",
      "Eval loss 2.495964527130127, R2 -0.33771565556526184\n",
      "epoch 204, loss 2.431166887283325, R2 -0.4157896041870117\n",
      "Eval loss 2.489994764328003, R2 -0.33434343338012695\n",
      "epoch 205, loss 2.425161838531494, R2 -0.4120979309082031\n",
      "Eval loss 2.484049081802368, R2 -0.3309839963912964\n",
      "epoch 206, loss 2.4191811084747314, R2 -0.40841996669769287\n",
      "Eval loss 2.4781270027160645, R2 -0.3276378810405731\n",
      "epoch 207, loss 2.413224697113037, R2 -0.40475720167160034\n",
      "Eval loss 2.472228765487671, R2 -0.3243033289909363\n",
      "epoch 208, loss 2.4072930812835693, R2 -0.4011079668998718\n",
      "Eval loss 2.4663541316986084, R2 -0.3209817111492157\n",
      "epoch 209, loss 2.4013853073120117, R2 -0.397472083568573\n",
      "Eval loss 2.460503101348877, R2 -0.3176725506782532\n",
      "epoch 210, loss 2.3955020904541016, R2 -0.3938504159450531\n",
      "Eval loss 2.4546756744384766, R2 -0.3143754005432129\n",
      "epoch 211, loss 2.3896427154541016, R2 -0.39024239778518677\n",
      "Eval loss 2.448871374130249, R2 -0.31109100580215454\n",
      "epoch 212, loss 2.383807420730591, R2 -0.3866473138332367\n",
      "Eval loss 2.4430906772613525, R2 -0.30781808495521545\n",
      "epoch 213, loss 2.377995729446411, R2 -0.38306722044944763\n",
      "Eval loss 2.437333106994629, R2 -0.30455848574638367\n",
      "epoch 214, loss 2.3722081184387207, R2 -0.3794999420642853\n",
      "Eval loss 2.431598424911499, R2 -0.30131006240844727\n",
      "epoch 215, loss 2.3664438724517822, R2 -0.3759462237358093\n",
      "Eval loss 2.425886631011963, R2 -0.2980746328830719\n",
      "epoch 216, loss 2.360703229904175, R2 -0.3724057376384735\n",
      "Eval loss 2.4201977252960205, R2 -0.29485082626342773\n",
      "epoch 217, loss 2.3549861907958984, R2 -0.3688790500164032\n",
      "Eval loss 2.414531707763672, R2 -0.29163897037506104\n",
      "epoch 218, loss 2.349292039871216, R2 -0.3653656840324402\n",
      "Eval loss 2.408888578414917, R2 -0.28843921422958374\n",
      "epoch 219, loss 2.3436214923858643, R2 -0.36186540126800537\n",
      "Eval loss 2.4032673835754395, R2 -0.2852512001991272\n",
      "epoch 220, loss 2.3379738330841064, R2 -0.3583783805370331\n",
      "Eval loss 2.3976690769195557, R2 -0.28207531571388245\n",
      "epoch 221, loss 2.3323495388031006, R2 -0.3549046516418457\n",
      "Eval loss 2.3920929431915283, R2 -0.27891090512275696\n",
      "epoch 222, loss 2.326747417449951, R2 -0.3514431118965149\n",
      "Eval loss 2.3865394592285156, R2 -0.2757582664489746\n",
      "epoch 223, loss 2.321169137954712, R2 -0.3479956090450287\n",
      "Eval loss 2.381007671356201, R2 -0.27261778712272644\n",
      "epoch 224, loss 2.315612554550171, R2 -0.34456154704093933\n",
      "Eval loss 2.3754982948303223, R2 -0.26948854327201843\n",
      "epoch 225, loss 2.3100790977478027, R2 -0.3411397337913513\n",
      "Eval loss 2.3700106143951416, R2 -0.26637086272239685\n",
      "epoch 226, loss 2.304568290710449, R2 -0.33773109316825867\n",
      "Eval loss 2.3645451068878174, R2 -0.26326557993888855\n",
      "epoch 227, loss 2.299079656600952, R2 -0.3343352675437927\n",
      "Eval loss 2.359100818634033, R2 -0.26017114520072937\n",
      "epoch 228, loss 2.2936131954193115, R2 -0.3309520483016968\n",
      "Eval loss 2.3536789417266846, R2 -0.2570883631706238\n",
      "epoch 229, loss 2.2881691455841064, R2 -0.3275817930698395\n",
      "Eval loss 2.348278522491455, R2 -0.2540169656276703\n",
      "epoch 230, loss 2.282747268676758, R2 -0.3242240846157074\n",
      "Eval loss 2.3428990840911865, R2 -0.250956654548645\n",
      "epoch 231, loss 2.2773470878601074, R2 -0.32087910175323486\n",
      "Eval loss 2.3375415802001953, R2 -0.2479081004858017\n",
      "epoch 232, loss 2.2719690799713135, R2 -0.31754666566848755\n",
      "Eval loss 2.332205295562744, R2 -0.24487091600894928\n",
      "epoch 233, loss 2.266613006591797, R2 -0.31422674655914307\n",
      "Eval loss 2.326889991760254, R2 -0.24184447526931763\n",
      "epoch 234, loss 2.2612783908843994, R2 -0.3109191656112671\n",
      "Eval loss 2.3215959072113037, R2 -0.2388298362493515\n",
      "epoch 235, loss 2.2559654712677, R2 -0.30762413144111633\n",
      "Eval loss 2.3163230419158936, R2 -0.23582595586776733\n",
      "epoch 236, loss 2.25067400932312, R2 -0.30434131622314453\n",
      "Eval loss 2.3110709190368652, R2 -0.23283329606056213\n",
      "epoch 237, loss 2.245404005050659, R2 -0.30107128620147705\n",
      "Eval loss 2.305839776992798, R2 -0.22985148429870605\n",
      "epoch 238, loss 2.2401554584503174, R2 -0.2978134751319885\n",
      "Eval loss 2.3006293773651123, R2 -0.22688092291355133\n",
      "epoch 239, loss 2.2349283695220947, R2 -0.29456764459609985\n",
      "Eval loss 2.2954397201538086, R2 -0.2239217758178711\n",
      "epoch 240, loss 2.229722023010254, R2 -0.29133427143096924\n",
      "Eval loss 2.2902705669403076, R2 -0.22097249329090118\n",
      "epoch 241, loss 2.2245371341705322, R2 -0.2881126403808594\n",
      "Eval loss 2.2851219177246094, R2 -0.21803539991378784\n",
      "epoch 242, loss 2.2193729877471924, R2 -0.2849034070968628\n",
      "Eval loss 2.279993772506714, R2 -0.21510787308216095\n",
      "epoch 243, loss 2.2142293453216553, R2 -0.2817060649394989\n",
      "Eval loss 2.274885892868042, R2 -0.21219204366207123\n",
      "epoch 244, loss 2.2091071605682373, R2 -0.27852076292037964\n",
      "Eval loss 2.2697980403900146, R2 -0.2092863917350769\n",
      "epoch 245, loss 2.204005241394043, R2 -0.27534738183021545\n",
      "Eval loss 2.264730453491211, R2 -0.20639203488826752\n",
      "epoch 246, loss 2.1989240646362305, R2 -0.27218589186668396\n",
      "Eval loss 2.259683132171631, R2 -0.20350822806358337\n",
      "epoch 247, loss 2.1938636302948, R2 -0.26903560757637024\n",
      "Eval loss 2.254655599594116, R2 -0.20063456892967224\n",
      "epoch 248, loss 2.1888232231140137, R2 -0.2658982276916504\n",
      "Eval loss 2.249647855758667, R2 -0.1977718323469162\n",
      "epoch 249, loss 2.1838033199310303, R2 -0.26277148723602295\n",
      "Eval loss 2.2446601390838623, R2 -0.19491976499557495\n",
      "epoch 250, loss 2.1788036823272705, R2 -0.259657621383667\n",
      "Eval loss 2.239691734313965, R2 -0.1920778602361679\n",
      "epoch 251, loss 2.1738240718841553, R2 -0.25655481219291687\n",
      "Eval loss 2.234743356704712, R2 -0.18924695253372192\n",
      "epoch 252, loss 2.1688647270202637, R2 -0.2534635663032532\n",
      "Eval loss 2.229814291000366, R2 -0.18642593920230865\n",
      "epoch 253, loss 2.1639251708984375, R2 -0.25038382411003113\n",
      "Eval loss 2.224905252456665, R2 -0.18361511826515198\n",
      "epoch 254, loss 2.159005641937256, R2 -0.24731555581092834\n",
      "Eval loss 2.220014810562134, R2 -0.1808154135942459\n",
      "epoch 255, loss 2.1541059017181396, R2 -0.24425888061523438\n",
      "Eval loss 2.215144157409668, R2 -0.1780254989862442\n",
      "epoch 256, loss 2.149225950241089, R2 -0.24121354520320892\n",
      "Eval loss 2.2102925777435303, R2 -0.17524553835391998\n",
      "epoch 257, loss 2.1443653106689453, R2 -0.2381790280342102\n",
      "Eval loss 2.2054598331451416, R2 -0.17247571051120758\n",
      "epoch 258, loss 2.139524459838867, R2 -0.23515662550926208\n",
      "Eval loss 2.20064640045166, R2 -0.1697169840335846\n",
      "epoch 259, loss 2.1347031593322754, R2 -0.23214522004127502\n",
      "Eval loss 2.1958518028259277, R2 -0.1669677346944809\n",
      "epoch 260, loss 2.129901170730591, R2 -0.22914518415927887\n",
      "Eval loss 2.1910762786865234, R2 -0.164228618144989\n",
      "epoch 261, loss 2.1251184940338135, R2 -0.22615623474121094\n",
      "Eval loss 2.186319589614868, R2 -0.16149944067001343\n",
      "epoch 262, loss 2.1203551292419434, R2 -0.22317829728126526\n",
      "Eval loss 2.1815812587738037, R2 -0.15878047049045563\n",
      "epoch 263, loss 2.115610361099243, R2 -0.22021156549453735\n",
      "Eval loss 2.1768617630004883, R2 -0.15607129037380219\n",
      "epoch 264, loss 2.1108851432800293, R2 -0.21725569665431976\n",
      "Eval loss 2.172161102294922, R2 -0.1533723920583725\n",
      "epoch 265, loss 2.1061785221099854, R2 -0.2143104523420334\n",
      "Eval loss 2.167478561401367, R2 -0.15068231523036957\n",
      "epoch 266, loss 2.1014912128448486, R2 -0.21137714385986328\n",
      "Eval loss 2.1628146171569824, R2 -0.1480032503604889\n",
      "epoch 267, loss 2.096822738647461, R2 -0.2084542214870453\n",
      "Eval loss 2.1581690311431885, R2 -0.14533354341983795\n",
      "epoch 268, loss 2.092172145843506, R2 -0.20554165542125702\n",
      "Eval loss 2.153541326522827, R2 -0.1426735818386078\n",
      "epoch 269, loss 2.087541103363037, R2 -0.20264071226119995\n",
      "Eval loss 2.1489319801330566, R2 -0.14002355933189392\n",
      "epoch 270, loss 2.08292818069458, R2 -0.1997501254081726\n",
      "Eval loss 2.1443405151367188, R2 -0.13738282024860382\n",
      "epoch 271, loss 2.078333854675293, R2 -0.19687007367610931\n",
      "Eval loss 2.139767646789551, R2 -0.1347517967224121\n",
      "epoch 272, loss 2.0737578868865967, R2 -0.19400149583816528\n",
      "Eval loss 2.135211944580078, R2 -0.13213051855564117\n",
      "epoch 273, loss 2.069200038909912, R2 -0.19114303588867188\n",
      "Eval loss 2.1306746006011963, R2 -0.1295187622308731\n",
      "epoch 274, loss 2.0646607875823975, R2 -0.1882951259613037\n",
      "Eval loss 2.126154899597168, R2 -0.12691673636436462\n",
      "epoch 275, loss 2.0601391792297363, R2 -0.18545733392238617\n",
      "Eval loss 2.1216530799865723, R2 -0.12432365864515305\n",
      "epoch 276, loss 2.055635929107666, R2 -0.18263106048107147\n",
      "Eval loss 2.117168664932251, R2 -0.1217404305934906\n",
      "epoch 277, loss 2.0511507987976074, R2 -0.1798142045736313\n",
      "Eval loss 2.112701654434204, R2 -0.11916651576757431\n",
      "epoch 278, loss 2.0466833114624023, R2 -0.17700868844985962\n",
      "Eval loss 2.1082520484924316, R2 -0.11660189926624298\n",
      "epoch 279, loss 2.042233943939209, R2 -0.17421279847621918\n",
      "Eval loss 2.103820323944092, R2 -0.11404702812433243\n",
      "epoch 280, loss 2.03780198097229, R2 -0.17142802476882935\n",
      "Eval loss 2.0994057655334473, R2 -0.11150093376636505\n",
      "epoch 281, loss 2.0333878993988037, R2 -0.16865308582782745\n",
      "Eval loss 2.095008134841919, R2 -0.10896429419517517\n",
      "epoch 282, loss 2.028991460800171, R2 -0.16588836908340454\n",
      "Eval loss 2.090627908706665, R2 -0.10643672943115234\n",
      "epoch 283, loss 2.0246124267578125, R2 -0.16313371062278748\n",
      "Eval loss 2.0862648487091064, R2 -0.10391844063997269\n",
      "epoch 284, loss 2.0202507972717285, R2 -0.1603897213935852\n",
      "Eval loss 2.081918716430664, R2 -0.1014091968536377\n",
      "epoch 285, loss 2.015906572341919, R2 -0.15765567123889923\n",
      "Eval loss 2.077589750289917, R2 -0.09890922904014587\n",
      "epoch 286, loss 2.011579751968384, R2 -0.15493172407150269\n",
      "Eval loss 2.073277235031128, R2 -0.09641816467046738\n",
      "epoch 287, loss 2.007270336151123, R2 -0.15221768617630005\n",
      "Eval loss 2.068981885910034, R2 -0.09393671154975891\n",
      "epoch 288, loss 2.0029776096343994, R2 -0.14951346814632416\n",
      "Eval loss 2.0647032260894775, R2 -0.09146330505609512\n",
      "epoch 289, loss 1.9987024068832397, R2 -0.14681969583034515\n",
      "Eval loss 2.060441017150879, R2 -0.08899971842765808\n",
      "epoch 290, loss 1.9944435358047485, R2 -0.14413557946681976\n",
      "Eval loss 2.0561954975128174, R2 -0.08654467016458511\n",
      "epoch 291, loss 1.9902024269104004, R2 -0.14146126806735992\n",
      "Eval loss 2.051966667175293, R2 -0.08409824967384338\n",
      "epoch 292, loss 1.9859776496887207, R2 -0.13879656791687012\n",
      "Eval loss 2.0477538108825684, R2 -0.08166131377220154\n",
      "epoch 293, loss 1.9817699193954468, R2 -0.13614247739315033\n",
      "Eval loss 2.04355788230896, R2 -0.07923246175050735\n",
      "epoch 294, loss 1.9775785207748413, R2 -0.13349780440330505\n",
      "Eval loss 2.0393779277801514, R2 -0.07681316882371902\n",
      "epoch 295, loss 1.9734039306640625, R2 -0.13086247444152832\n",
      "Eval loss 2.035214424133301, R2 -0.07440190017223358\n",
      "epoch 296, loss 1.9692461490631104, R2 -0.1282372623682022\n",
      "Eval loss 2.031067132949829, R2 -0.07200000435113907\n",
      "epoch 297, loss 1.9651044607162476, R2 -0.12562145292758942\n",
      "Eval loss 2.0269358158111572, R2 -0.06960641592741013\n",
      "epoch 298, loss 1.9609794616699219, R2 -0.12301525473594666\n",
      "Eval loss 2.0228207111358643, R2 -0.06722152978181839\n",
      "epoch 299, loss 1.9568709135055542, R2 -0.12041879445314407\n",
      "Eval loss 2.018721342086792, R2 -0.0648449957370758\n",
      "epoch 300, loss 1.9527782201766968, R2 -0.1178317591547966\n",
      "Eval loss 2.0146377086639404, R2 -0.062477413564920425\n",
      "epoch 301, loss 1.948702096939087, R2 -0.11525425314903259\n",
      "Eval loss 2.0105702877044678, R2 -0.060118306428194046\n",
      "epoch 302, loss 1.9446418285369873, R2 -0.11268579959869385\n",
      "Eval loss 2.0065183639526367, R2 -0.057767268270254135\n",
      "epoch 303, loss 1.9405980110168457, R2 -0.11012756824493408\n",
      "Eval loss 2.0024826526641846, R2 -0.05542539060115814\n",
      "epoch 304, loss 1.9365699291229248, R2 -0.10757815092802048\n",
      "Eval loss 1.9984619617462158, R2 -0.05309135839343071\n",
      "epoch 305, loss 1.9325579404830933, R2 -0.10503792762756348\n",
      "Eval loss 1.9944570064544678, R2 -0.05076625198125839\n",
      "epoch 306, loss 1.9285615682601929, R2 -0.10250776261091232\n",
      "Eval loss 1.9904676675796509, R2 -0.04844878986477852\n",
      "epoch 307, loss 1.9245809316635132, R2 -0.09998635947704315\n",
      "Eval loss 1.9864935874938965, R2 -0.0461406484246254\n",
      "epoch 308, loss 1.9206165075302124, R2 -0.09747421741485596\n",
      "Eval loss 1.9825350046157837, R2 -0.04383997991681099\n",
      "epoch 309, loss 1.9166672229766846, R2 -0.0949711799621582\n",
      "Eval loss 1.9785915613174438, R2 -0.041548121720552444\n",
      "epoch 310, loss 1.9127336740493774, R2 -0.09247712790966034\n",
      "Eval loss 1.9746637344360352, R2 -0.03926390781998634\n",
      "epoch 311, loss 1.908815860748291, R2 -0.08999248594045639\n",
      "Eval loss 1.9707505702972412, R2 -0.03698847442865372\n",
      "epoch 312, loss 1.904913306236267, R2 -0.08751726895570755\n",
      "Eval loss 1.9668530225753784, R2 -0.0347210131585598\n",
      "epoch 313, loss 1.9010261297225952, R2 -0.08505057543516159\n",
      "Eval loss 1.9629701375961304, R2 -0.032461658120155334\n",
      "epoch 314, loss 1.8971543312072754, R2 -0.08259298652410507\n",
      "Eval loss 1.9591022729873657, R2 -0.030210403725504875\n",
      "epoch 315, loss 1.893297791481018, R2 -0.08014459162950516\n",
      "Eval loss 1.9552494287490845, R2 -0.02796686813235283\n",
      "epoch 316, loss 1.8894562721252441, R2 -0.07770490646362305\n",
      "Eval loss 1.9514113664627075, R2 -0.025732306763529778\n",
      "epoch 317, loss 1.8856300115585327, R2 -0.07527417689561844\n",
      "Eval loss 1.9475884437561035, R2 -0.0235051941126585\n",
      "epoch 318, loss 1.8818188905715942, R2 -0.07285232096910477\n",
      "Eval loss 1.943779468536377, R2 -0.02128591388463974\n",
      "epoch 319, loss 1.8780226707458496, R2 -0.07043925672769547\n",
      "Eval loss 1.939985752105713, R2 -0.019074641168117523\n",
      "epoch 320, loss 1.8742414712905884, R2 -0.06803500652313232\n",
      "Eval loss 1.9362066984176636, R2 -0.016871310770511627\n",
      "epoch 321, loss 1.870475172996521, R2 -0.06563908606767654\n",
      "Eval loss 1.9324419498443604, R2 -0.01467602327466011\n",
      "epoch 322, loss 1.8667234182357788, R2 -0.0632527768611908\n",
      "Eval loss 1.9286917448043823, R2 -0.012488478794693947\n",
      "epoch 323, loss 1.86298668384552, R2 -0.060874536633491516\n",
      "Eval loss 1.92495596408844, R2 -0.010308829136192799\n",
      "epoch 324, loss 1.859264612197876, R2 -0.05850529670715332\n",
      "Eval loss 1.921234369277954, R2 -0.008136868476867676\n",
      "epoch 325, loss 1.855556845664978, R2 -0.05614442005753517\n",
      "Eval loss 1.9175273180007935, R2 -0.005972716026008129\n",
      "epoch 326, loss 1.851863980293274, R2 -0.05379220470786095\n",
      "Eval loss 1.913834571838379, R2 -0.00381624698638916\n",
      "epoch 327, loss 1.8481855392456055, R2 -0.051448117941617966\n",
      "Eval loss 1.9101557731628418, R2 -0.001667315256781876\n",
      "epoch 328, loss 1.8445212841033936, R2 -0.04911333695054054\n",
      "Eval loss 1.9064909219741821, R2 0.0004737377166748047\n",
      "epoch 329, loss 1.8408714532852173, R2 -0.046786464750766754\n",
      "Eval loss 1.902840495109558, R2 0.0026069609448313713\n",
      "epoch 330, loss 1.8372361660003662, R2 -0.04446801170706749\n",
      "Eval loss 1.8992037773132324, R2 0.004732614383101463\n",
      "epoch 331, loss 1.8336150646209717, R2 -0.04215854033827782\n",
      "Eval loss 1.8955813646316528, R2 0.006851120386272669\n",
      "epoch 332, loss 1.8300080299377441, R2 -0.03985672816634178\n",
      "Eval loss 1.8919726610183716, R2 0.008961688727140427\n",
      "epoch 333, loss 1.826414942741394, R2 -0.03756401315331459\n",
      "Eval loss 1.8883776664733887, R2 0.011064935475587845\n",
      "epoch 334, loss 1.82283616065979, R2 -0.03527919948101044\n",
      "Eval loss 1.8847965002059937, R2 0.013160391710698605\n",
      "epoch 335, loss 1.8192713260650635, R2 -0.03300265967845917\n",
      "Eval loss 1.881229043006897, R2 0.015248623676598072\n",
      "epoch 336, loss 1.815720558166504, R2 -0.030734408646821976\n",
      "Eval loss 1.8776754140853882, R2 0.01732964999973774\n",
      "epoch 337, loss 1.8121833801269531, R2 -0.02847437933087349\n",
      "Eval loss 1.874135136604309, R2 0.0194028839468956\n",
      "epoch 338, loss 1.8086601495742798, R2 -0.02622244507074356\n",
      "Eval loss 1.8706083297729492, R2 0.021469229832291603\n",
      "epoch 339, loss 1.8051507472991943, R2 -0.02397874742746353\n",
      "Eval loss 1.8670951128005981, R2 0.023528147488832474\n",
      "epoch 340, loss 1.8016548156738281, R2 -0.021743150427937508\n",
      "Eval loss 1.8635954856872559, R2 0.02557903528213501\n",
      "epoch 341, loss 1.798172950744629, R2 -0.019515616819262505\n",
      "Eval loss 1.8601089715957642, R2 0.02762344665825367\n",
      "epoch 342, loss 1.7947043180465698, R2 -0.01729610748589039\n",
      "Eval loss 1.8566358089447021, R2 0.02966023050248623\n",
      "epoch 343, loss 1.7912492752075195, R2 -0.015084153041243553\n",
      "Eval loss 1.8531759977340698, R2 0.03168964013457298\n",
      "epoch 344, loss 1.7878079414367676, R2 -0.012881078757345676\n",
      "Eval loss 1.8497294187545776, R2 0.03371232748031616\n",
      "epoch 345, loss 1.7843796014785767, R2 -0.010685666464269161\n",
      "Eval loss 1.8462958335876465, R2 0.03572756052017212\n",
      "epoch 346, loss 1.780964970588684, R2 -0.008498051203787327\n",
      "Eval loss 1.842875361442566, R2 0.037735484540462494\n",
      "epoch 347, loss 1.7775635719299316, R2 -0.006318352650851011\n",
      "Eval loss 1.8394678831100464, R2 0.03973671421408653\n",
      "epoch 348, loss 1.7741754055023193, R2 -0.00414610467851162\n",
      "Eval loss 1.8360735177993774, R2 0.041730817407369614\n",
      "epoch 349, loss 1.7708004713058472, R2 -0.0019823096226900816\n",
      "Eval loss 1.8326921463012695, R2 0.043717335909605026\n",
      "epoch 350, loss 1.7674384117126465, R2 0.0001737692073220387\n",
      "Eval loss 1.8293232917785645, R2 0.04569728299975395\n",
      "epoch 351, loss 1.7640894651412964, R2 0.002322370419278741\n",
      "Eval loss 1.8259673118591309, R2 0.04767006263136864\n",
      "epoch 352, loss 1.7607537508010864, R2 0.004463011398911476\n",
      "Eval loss 1.8226243257522583, R2 0.049636054784059525\n",
      "epoch 353, loss 1.757430911064148, R2 0.006596050690859556\n",
      "Eval loss 1.8192939758300781, R2 0.05159512534737587\n",
      "epoch 354, loss 1.7541210651397705, R2 0.008721297606825829\n",
      "Eval loss 1.8159762620925903, R2 0.053546927869319916\n",
      "epoch 355, loss 1.7508240938186646, R2 0.010839148424565792\n",
      "Eval loss 1.812671184539795, R2 0.05549245700240135\n",
      "epoch 356, loss 1.7475396394729614, R2 0.012949120253324509\n",
      "Eval loss 1.8093785047531128, R2 0.05743028223514557\n",
      "epoch 357, loss 1.7442680597305298, R2 0.015051716938614845\n",
      "Eval loss 1.8060983419418335, R2 0.059361934661865234\n",
      "epoch 358, loss 1.74100923538208, R2 0.01714659295976162\n",
      "Eval loss 1.8028309345245361, R2 0.06128675118088722\n",
      "epoch 359, loss 1.7377631664276123, R2 0.01923411525785923\n",
      "Eval loss 1.7995754480361938, R2 0.06320449709892273\n",
      "epoch 360, loss 1.7345291376113892, R2 0.02131393365561962\n",
      "Eval loss 1.7963327169418335, R2 0.06511560082435608\n",
      "epoch 361, loss 1.7313082218170166, R2 0.02338653802871704\n",
      "Eval loss 1.7931021451950073, R2 0.06701980531215668\n",
      "epoch 362, loss 1.7280995845794678, R2 0.025451498106122017\n",
      "Eval loss 1.7898834943771362, R2 0.06891757994890213\n",
      "epoch 363, loss 1.7249034643173218, R2 0.027509110048413277\n",
      "Eval loss 1.7866772413253784, R2 0.07080831378698349\n",
      "epoch 364, loss 1.72171950340271, R2 0.029559362679719925\n",
      "Eval loss 1.7834832668304443, R2 0.07269281893968582\n",
      "epoch 365, loss 1.7185479402542114, R2 0.03160225972533226\n",
      "Eval loss 1.7803010940551758, R2 0.07457053661346436\n",
      "epoch 366, loss 1.7153886556625366, R2 0.03363785892724991\n",
      "Eval loss 1.77713143825531, R2 0.07644135504961014\n",
      "epoch 367, loss 1.7122418880462646, R2 0.0356663316488266\n",
      "Eval loss 1.7739733457565308, R2 0.07830602675676346\n",
      "epoch 368, loss 1.7091069221496582, R2 0.03768739476799965\n",
      "Eval loss 1.770827054977417, R2 0.0801641196012497\n",
      "epoch 369, loss 1.7059839963912964, R2 0.03970104828476906\n",
      "Eval loss 1.7676931619644165, R2 0.0820155069231987\n",
      "epoch 370, loss 1.7028732299804688, R2 0.04170777648687363\n",
      "Eval loss 1.764570713043213, R2 0.0838603675365448\n",
      "epoch 371, loss 1.6997746229171753, R2 0.043707069009542465\n",
      "Eval loss 1.7614601850509644, R2 0.0856989175081253\n",
      "epoch 372, loss 1.6966878175735474, R2 0.04569979012012482\n",
      "Eval loss 1.7583614587783813, R2 0.08753098547458649\n",
      "epoch 373, loss 1.6936129331588745, R2 0.047684572637081146\n",
      "Eval loss 1.7552741765975952, R2 0.08935670554637909\n",
      "epoch 374, loss 1.6905500888824463, R2 0.04966271296143532\n",
      "Eval loss 1.7521986961364746, R2 0.09117568284273148\n",
      "epoch 375, loss 1.687498927116394, R2 0.051633790135383606\n",
      "Eval loss 1.74913489818573, R2 0.09298856556415558\n",
      "epoch 376, loss 1.6844593286514282, R2 0.053597867488861084\n",
      "Eval loss 1.7460824251174927, R2 0.09479519724845886\n",
      "epoch 377, loss 1.6814316511154175, R2 0.055554989725351334\n",
      "Eval loss 1.7430415153503418, R2 0.09659519791603088\n",
      "epoch 378, loss 1.6784155368804932, R2 0.05750516802072525\n",
      "Eval loss 1.7400120496749878, R2 0.09838936477899551\n",
      "epoch 379, loss 1.6754112243652344, R2 0.059447839856147766\n",
      "Eval loss 1.7369939088821411, R2 0.10017705708742142\n",
      "epoch 380, loss 1.6724179983139038, R2 0.06138409674167633\n",
      "Eval loss 1.73398756980896, R2 0.10195804387331009\n",
      "epoch 381, loss 1.6694368124008179, R2 0.06331321597099304\n",
      "Eval loss 1.7309918403625488, R2 0.10373341292142868\n",
      "epoch 382, loss 1.6664669513702393, R2 0.06523562222719193\n",
      "Eval loss 1.7280080318450928, R2 0.10550238937139511\n",
      "epoch 383, loss 1.6635082960128784, R2 0.0671514943242073\n",
      "Eval loss 1.7250351905822754, R2 0.10726528614759445\n",
      "epoch 384, loss 1.6605610847473145, R2 0.06906001269817352\n",
      "Eval loss 1.7220731973648071, R2 0.10902175307273865\n",
      "epoch 385, loss 1.6576253175735474, R2 0.07096207141876221\n",
      "Eval loss 1.7191225290298462, R2 0.11077216267585754\n",
      "epoch 386, loss 1.6547006368637085, R2 0.07285702973604202\n",
      "Eval loss 1.716182827949524, R2 0.11251673102378845\n",
      "epoch 387, loss 1.651787519454956, R2 0.07474544644355774\n",
      "Eval loss 1.7132545709609985, R2 0.11425478756427765\n",
      "epoch 388, loss 1.6488852500915527, R2 0.07662732154130936\n",
      "Eval loss 1.7103368043899536, R2 0.11598721146583557\n",
      "epoch 389, loss 1.6459941864013672, R2 0.07850232720375061\n",
      "Eval loss 1.7074302434921265, R2 0.11771323531866074\n",
      "epoch 390, loss 1.6431143283843994, R2 0.08037075400352478\n",
      "Eval loss 1.7045344114303589, R2 0.11943376809358597\n",
      "epoch 391, loss 1.6402454376220703, R2 0.08223244547843933\n",
      "Eval loss 1.7016496658325195, R2 0.12114784866571426\n",
      "epoch 392, loss 1.6373873949050903, R2 0.08408763259649277\n",
      "Eval loss 1.6987755298614502, R2 0.12285611033439636\n",
      "epoch 393, loss 1.6345404386520386, R2 0.08593621850013733\n",
      "Eval loss 1.69591224193573, R2 0.12455873191356659\n",
      "epoch 394, loss 1.631704330444336, R2 0.0877782553434372\n",
      "Eval loss 1.6930593252182007, R2 0.126255065202713\n",
      "epoch 395, loss 1.6288789510726929, R2 0.08961375057697296\n",
      "Eval loss 1.69021737575531, R2 0.12794548273086548\n",
      "epoch 396, loss 1.626064658164978, R2 0.09144275635480881\n",
      "Eval loss 1.6873860359191895, R2 0.129629984498024\n",
      "epoch 397, loss 1.6232609748840332, R2 0.0932653397321701\n",
      "Eval loss 1.6845653057098389, R2 0.13130877912044525\n",
      "epoch 398, loss 1.6204679012298584, R2 0.09508150070905685\n",
      "Eval loss 1.6817551851272583, R2 0.13298195600509644\n",
      "epoch 399, loss 1.6176855564117432, R2 0.09689111262559891\n",
      "Eval loss 1.6789551973342896, R2 0.13464917242527008\n",
      "epoch 400, loss 1.614914059638977, R2 0.09869434684515\n",
      "Eval loss 1.67616605758667, R2 0.13631033897399902\n",
      "epoch 401, loss 1.6121529340744019, R2 0.10049128532409668\n",
      "Eval loss 1.673387050628662, R2 0.13796626031398773\n",
      "epoch 402, loss 1.6094022989273071, R2 0.10228178650140762\n",
      "Eval loss 1.6706186532974243, R2 0.13961611688137054\n",
      "epoch 403, loss 1.6066622734069824, R2 0.10406601428985596\n",
      "Eval loss 1.6678603887557983, R2 0.14126013219356537\n",
      "epoch 404, loss 1.603932499885559, R2 0.10584407299757004\n",
      "Eval loss 1.6651123762130737, R2 0.14289899170398712\n",
      "epoch 405, loss 1.6012134552001953, R2 0.10761590301990509\n",
      "Eval loss 1.6623746156692505, R2 0.144531711935997\n",
      "epoch 406, loss 1.598504662513733, R2 0.10938100516796112\n",
      "Eval loss 1.6596473455429077, R2 0.14615865051746368\n",
      "epoch 407, loss 1.5958061218261719, R2 0.11114013195037842\n",
      "Eval loss 1.656929850578308, R2 0.1477803736925125\n",
      "epoch 408, loss 1.5931177139282227, R2 0.11289309710264206\n",
      "Eval loss 1.6542226076126099, R2 0.1493964046239853\n",
      "epoch 409, loss 1.5904396772384644, R2 0.1146400049328804\n",
      "Eval loss 1.6515253782272339, R2 0.1510065197944641\n",
      "epoch 410, loss 1.5877718925476074, R2 0.11638086289167404\n",
      "Eval loss 1.6488380432128906, R2 0.15261143445968628\n",
      "epoch 411, loss 1.5851141214370728, R2 0.11811518669128418\n",
      "Eval loss 1.6461608409881592, R2 0.15421076118946075\n",
      "epoch 412, loss 1.5824666023254395, R2 0.11984339356422424\n",
      "Eval loss 1.6434937715530396, R2 0.1558045744895935\n",
      "epoch 413, loss 1.5798289775848389, R2 0.12156586349010468\n",
      "Eval loss 1.640836477279663, R2 0.157392680644989\n",
      "epoch 414, loss 1.5772013664245605, R2 0.12328210473060608\n",
      "Eval loss 1.6381889581680298, R2 0.15897546708583832\n",
      "epoch 415, loss 1.5745841264724731, R2 0.12499230355024338\n",
      "Eval loss 1.63555109500885, R2 0.1605529487133026\n",
      "epoch 416, loss 1.5719764232635498, R2 0.12669652700424194\n",
      "Eval loss 1.6329232454299927, R2 0.16212502121925354\n",
      "epoch 417, loss 1.5693784952163696, R2 0.12839487195014954\n",
      "Eval loss 1.6303054094314575, R2 0.16369138658046722\n",
      "epoch 418, loss 1.5667908191680908, R2 0.1300870180130005\n",
      "Eval loss 1.6276967525482178, R2 0.16525234282016754\n",
      "epoch 419, loss 1.564212679862976, R2 0.13177341222763062\n",
      "Eval loss 1.6250979900360107, R2 0.1668081432580948\n",
      "epoch 420, loss 1.5616445541381836, R2 0.13345378637313843\n",
      "Eval loss 1.6225088834762573, R2 0.1683584302663803\n",
      "epoch 421, loss 1.5590859651565552, R2 0.13512834906578064\n",
      "Eval loss 1.619929313659668, R2 0.1699034720659256\n",
      "epoch 422, loss 1.5565369129180908, R2 0.1367969512939453\n",
      "Eval loss 1.6173591613769531, R2 0.17144320905208588\n",
      "epoch 423, loss 1.5539978742599487, R2 0.13845978677272797\n",
      "Eval loss 1.614798665046692, R2 0.17297767102718353\n",
      "epoch 424, loss 1.5514683723449707, R2 0.14011669158935547\n",
      "Eval loss 1.6122474670410156, R2 0.17450712621212006\n",
      "epoch 425, loss 1.5489482879638672, R2 0.14176791906356812\n",
      "Eval loss 1.609705924987793, R2 0.17603081464767456\n",
      "epoch 426, loss 1.5464378595352173, R2 0.14341330528259277\n",
      "Eval loss 1.6071736812591553, R2 0.1775495409965515\n",
      "epoch 427, loss 1.543936848640442, R2 0.14505290985107422\n",
      "Eval loss 1.6046507358551025, R2 0.17906343936920166\n",
      "epoch 428, loss 1.541445255279541, R2 0.14668680727481842\n",
      "Eval loss 1.6021370887756348, R2 0.18057166039943695\n",
      "epoch 429, loss 1.5389630794525146, R2 0.14831499755382538\n",
      "Eval loss 1.5996328592300415, R2 0.18207480013370514\n",
      "epoch 430, loss 1.5364904403686523, R2 0.1499374806880951\n",
      "Eval loss 1.5971378087997437, R2 0.18357238173484802\n",
      "epoch 431, loss 1.5340269804000854, R2 0.15155433118343353\n",
      "Eval loss 1.5946518182754517, R2 0.18506541848182678\n",
      "epoch 432, loss 1.5315725803375244, R2 0.15316557884216309\n",
      "Eval loss 1.5921751260757446, R2 0.1865531951189041\n",
      "epoch 433, loss 1.529128074645996, R2 0.15477102994918823\n",
      "Eval loss 1.589707374572754, R2 0.18803571164608002\n",
      "epoch 434, loss 1.5266921520233154, R2 0.15637096762657166\n",
      "Eval loss 1.5872489213943481, R2 0.18951350450515747\n",
      "epoch 435, loss 1.5242656469345093, R2 0.1579653024673462\n",
      "Eval loss 1.5847994089126587, R2 0.19098614156246185\n",
      "epoch 436, loss 1.5218483209609985, R2 0.15955409407615662\n",
      "Eval loss 1.5823590755462646, R2 0.19245347380638123\n",
      "epoch 437, loss 1.5194401741027832, R2 0.1611374318599701\n",
      "Eval loss 1.5799275636672974, R2 0.19391614198684692\n",
      "epoch 438, loss 1.5170409679412842, R2 0.16271549463272095\n",
      "Eval loss 1.5775049924850464, R2 0.19537359476089478\n",
      "epoch 439, loss 1.5146509408950806, R2 0.16428746283054352\n",
      "Eval loss 1.5750912427902222, R2 0.1968262791633606\n",
      "epoch 440, loss 1.5122697353363037, R2 0.16585412621498108\n",
      "Eval loss 1.5726864337921143, R2 0.19827385246753693\n",
      "epoch 441, loss 1.5098975896835327, R2 0.16741575300693512\n",
      "Eval loss 1.570290446281433, R2 0.19971638917922974\n",
      "epoch 442, loss 1.5075342655181885, R2 0.16897131502628326\n",
      "Eval loss 1.5679030418395996, R2 0.2011541724205017\n",
      "epoch 443, loss 1.5051798820495605, R2 0.1705217808485031\n",
      "Eval loss 1.5655248165130615, R2 0.20258687436580658\n",
      "epoch 444, loss 1.5028342008590698, R2 0.17206725478172302\n",
      "Eval loss 1.563155174255371, R2 0.20401500165462494\n",
      "epoch 445, loss 1.5004974603652954, R2 0.17360657453536987\n",
      "Eval loss 1.5607939958572388, R2 0.20543794333934784\n",
      "epoch 446, loss 1.4981694221496582, R2 0.17514100670814514\n",
      "Eval loss 1.5584417581558228, R2 0.2068561613559723\n",
      "epoch 447, loss 1.4958503246307373, R2 0.17667002975940704\n",
      "Eval loss 1.5560979843139648, R2 0.2082696557044983\n",
      "epoch 448, loss 1.493539810180664, R2 0.17819365859031677\n",
      "Eval loss 1.5537627935409546, R2 0.20967832207679749\n",
      "epoch 449, loss 1.4912378787994385, R2 0.17971213161945343\n",
      "Eval loss 1.5514360666275024, R2 0.21108223497867584\n",
      "epoch 450, loss 1.48894464969635, R2 0.18122521042823792\n",
      "Eval loss 1.549117922782898, R2 0.21248124539852142\n",
      "epoch 451, loss 1.4866600036621094, R2 0.1827331930398941\n",
      "Eval loss 1.5468083620071411, R2 0.2138756364583969\n",
      "epoch 452, loss 1.4843838214874268, R2 0.18423590064048767\n",
      "Eval loss 1.5445071458816528, R2 0.21526505053043365\n",
      "epoch 453, loss 1.4821163415908813, R2 0.18573333323001862\n",
      "Eval loss 1.5422145128250122, R2 0.21665023267269135\n",
      "epoch 454, loss 1.4798572063446045, R2 0.18722572922706604\n",
      "Eval loss 1.539929986000061, R2 0.21803022921085358\n",
      "epoch 455, loss 1.4776065349578857, R2 0.18871277570724487\n",
      "Eval loss 1.537653923034668, R2 0.21940575540065765\n",
      "epoch 456, loss 1.4753642082214355, R2 0.19019478559494019\n",
      "Eval loss 1.5353862047195435, R2 0.22077642381191254\n",
      "epoch 457, loss 1.4731303453445435, R2 0.19167175889015198\n",
      "Eval loss 1.5331268310546875, R2 0.22214265167713165\n",
      "epoch 458, loss 1.47090482711792, R2 0.1931433230638504\n",
      "Eval loss 1.530875563621521, R2 0.22350430488586426\n",
      "epoch 459, loss 1.468687653541565, R2 0.1946101188659668\n",
      "Eval loss 1.528632402420044, R2 0.22486118972301483\n",
      "epoch 460, loss 1.466478705406189, R2 0.1960715800523758\n",
      "Eval loss 1.5263975858688354, R2 0.22621360421180725\n",
      "epoch 461, loss 1.4642781019210815, R2 0.19752813875675201\n",
      "Eval loss 1.5241707563400269, R2 0.22756129503250122\n",
      "epoch 462, loss 1.4620856046676636, R2 0.19897963106632233\n",
      "Eval loss 1.5219520330429077, R2 0.22890472412109375\n",
      "epoch 463, loss 1.4599013328552246, R2 0.20042644441127777\n",
      "Eval loss 1.5197415351867676, R2 0.2302435040473938\n",
      "epoch 464, loss 1.457725167274475, R2 0.20186762511730194\n",
      "Eval loss 1.5175390243530273, R2 0.23157739639282227\n",
      "epoch 465, loss 1.4555572271347046, R2 0.20330415666103363\n",
      "Eval loss 1.515344500541687, R2 0.23290720582008362\n",
      "epoch 466, loss 1.4533971548080444, R2 0.20473580062389374\n",
      "Eval loss 1.5131580829620361, R2 0.23423218727111816\n",
      "epoch 467, loss 1.4512451887130737, R2 0.20616267621517181\n",
      "Eval loss 1.5109795331954956, R2 0.23555321991443634\n",
      "epoch 468, loss 1.4491013288497925, R2 0.20758409798145294\n",
      "Eval loss 1.5088087320327759, R2 0.23686924576759338\n",
      "epoch 469, loss 1.4469653367996216, R2 0.20900095999240875\n",
      "Eval loss 1.506645917892456, R2 0.2381809800863266\n",
      "epoch 470, loss 1.4448373317718506, R2 0.2104129195213318\n",
      "Eval loss 1.5044910907745361, R2 0.23948851227760315\n",
      "epoch 471, loss 1.44271719455719, R2 0.21181999146938324\n",
      "Eval loss 1.5023438930511475, R2 0.24079160392284393\n",
      "epoch 472, loss 1.4406051635742188, R2 0.21322225034236908\n",
      "Eval loss 1.5002046823501587, R2 0.24208997189998627\n",
      "epoch 473, loss 1.4385007619857788, R2 0.21461963653564453\n",
      "Eval loss 1.4980729818344116, R2 0.24338436126708984\n",
      "epoch 474, loss 1.4364042282104492, R2 0.21601229906082153\n",
      "Eval loss 1.4959492683410645, R2 0.24467429518699646\n",
      "epoch 475, loss 1.4343154430389404, R2 0.21740011870861053\n",
      "Eval loss 1.4938329458236694, R2 0.24595971405506134\n",
      "epoch 476, loss 1.4322344064712524, R2 0.2187834233045578\n",
      "Eval loss 1.4917244911193848, R2 0.24724102020263672\n",
      "epoch 477, loss 1.4301612377166748, R2 0.22016187012195587\n",
      "Eval loss 1.489623785018921, R2 0.24851781129837036\n",
      "epoch 478, loss 1.4280956983566284, R2 0.22153516113758087\n",
      "Eval loss 1.4875303506851196, R2 0.24979038536548615\n",
      "epoch 479, loss 1.4260376691818237, R2 0.2229040116071701\n",
      "Eval loss 1.4854446649551392, R2 0.25105878710746765\n",
      "epoch 480, loss 1.4239873886108398, R2 0.22426819801330566\n",
      "Eval loss 1.4833664894104004, R2 0.25232306122779846\n",
      "epoch 481, loss 1.4219446182250977, R2 0.22562772035598755\n",
      "Eval loss 1.4812959432601929, R2 0.25358283519744873\n",
      "epoch 482, loss 1.4199095964431763, R2 0.22698254883289337\n",
      "Eval loss 1.4792325496673584, R2 0.2548384368419647\n",
      "epoch 483, loss 1.417881965637207, R2 0.22833304107189178\n",
      "Eval loss 1.4771769046783447, R2 0.2560897171497345\n",
      "epoch 484, loss 1.41586172580719, R2 0.22967837750911713\n",
      "Eval loss 1.4751285314559937, R2 0.25733718276023865\n",
      "epoch 485, loss 1.413849115371704, R2 0.23101931810379028\n",
      "Eval loss 1.4730876684188843, R2 0.2585799992084503\n",
      "epoch 486, loss 1.4118438959121704, R2 0.23235569894313812\n",
      "Eval loss 1.471054196357727, R2 0.25981906056404114\n",
      "epoch 487, loss 1.4098460674285889, R2 0.2336876392364502\n",
      "Eval loss 1.4690278768539429, R2 0.2610536515712738\n",
      "epoch 488, loss 1.4078556299209595, R2 0.23501504957675934\n",
      "Eval loss 1.4670088291168213, R2 0.26228439807891846\n",
      "epoch 489, loss 1.4058727025985718, R2 0.2363375574350357\n",
      "Eval loss 1.4649972915649414, R2 0.2635108232498169\n",
      "epoch 490, loss 1.4038968086242676, R2 0.23765581846237183\n",
      "Eval loss 1.4629926681518555, R2 0.26473328471183777\n",
      "epoch 491, loss 1.401928424835205, R2 0.23896946012973785\n",
      "Eval loss 1.4609956741333008, R2 0.26595163345336914\n",
      "epoch 492, loss 1.3999671936035156, R2 0.2402786761522293\n",
      "Eval loss 1.4590054750442505, R2 0.2671659588813782\n",
      "epoch 493, loss 1.3980133533477783, R2 0.2415834367275238\n",
      "Eval loss 1.4570226669311523, R2 0.2683761417865753\n",
      "epoch 494, loss 1.3960665464401245, R2 0.2428840696811676\n",
      "Eval loss 1.4550470113754272, R2 0.26958224177360535\n",
      "epoch 495, loss 1.3941270112991333, R2 0.2441795915365219\n",
      "Eval loss 1.453078269958496, R2 0.27078425884246826\n",
      "epoch 496, loss 1.3921945095062256, R2 0.2454712837934494\n",
      "Eval loss 1.451116681098938, R2 0.2719825208187103\n",
      "epoch 497, loss 1.3902690410614014, R2 0.2467581331729889\n",
      "Eval loss 1.4491621255874634, R2 0.2731766104698181\n",
      "epoch 498, loss 1.3883508443832397, R2 0.24804076552391052\n",
      "Eval loss 1.4472147226333618, R2 0.2743665874004364\n",
      "epoch 499, loss 1.3864398002624512, R2 0.24931907653808594\n",
      "Eval loss 1.4452741146087646, R2 0.27555298805236816\n",
      "epoch 500, loss 1.384535551071167, R2 0.250592976808548\n",
      "Eval loss 1.443340539932251, R2 0.2767350971698761\n",
      "epoch 501, loss 1.3826383352279663, R2 0.25186264514923096\n",
      "Eval loss 1.4414138793945312, R2 0.277913361787796\n",
      "epoch 502, loss 1.3807481527328491, R2 0.25312793254852295\n",
      "Eval loss 1.439494252204895, R2 0.2790878415107727\n",
      "epoch 503, loss 1.3788648843765259, R2 0.2543890178203583\n",
      "Eval loss 1.4375813007354736, R2 0.280258446931839\n",
      "epoch 504, loss 1.3769886493682861, R2 0.2556456923484802\n",
      "Eval loss 1.4356752634048462, R2 0.2814249098300934\n",
      "epoch 505, loss 1.3751190900802612, R2 0.25689807534217834\n",
      "Eval loss 1.4337760210037231, R2 0.2825876772403717\n",
      "epoch 506, loss 1.3732565641403198, R2 0.25814637541770935\n",
      "Eval loss 1.431883692741394, R2 0.2837463319301605\n",
      "epoch 507, loss 1.3714007139205933, R2 0.259390264749527\n",
      "Eval loss 1.4299980401992798, R2 0.28490149974823\n",
      "epoch 508, loss 1.3695518970489502, R2 0.26063022017478943\n",
      "Eval loss 1.4281189441680908, R2 0.28605279326438904\n",
      "epoch 509, loss 1.3677096366882324, R2 0.261865496635437\n",
      "Eval loss 1.426247000694275, R2 0.2871999442577362\n",
      "epoch 510, loss 1.365874171257019, R2 0.2630968391895294\n",
      "Eval loss 1.4243812561035156, R2 0.28834354877471924\n",
      "epoch 511, loss 1.36404550075531, R2 0.26432397961616516\n",
      "Eval loss 1.4225225448608398, R2 0.28948357701301575\n",
      "epoch 512, loss 1.362223505973816, R2 0.2655470371246338\n",
      "Eval loss 1.4206702709197998, R2 0.29061928391456604\n",
      "epoch 513, loss 1.360408067703247, R2 0.26676589250564575\n",
      "Eval loss 1.418824553489685, R2 0.29175159335136414\n",
      "epoch 514, loss 1.3585994243621826, R2 0.2679806649684906\n",
      "Eval loss 1.4169856309890747, R2 0.29288002848625183\n",
      "epoch 515, loss 1.356797456741333, R2 0.2691914737224579\n",
      "Eval loss 1.4151532649993896, R2 0.2940046787261963\n",
      "epoch 516, loss 1.3550018072128296, R2 0.27039802074432373\n",
      "Eval loss 1.4133272171020508, R2 0.2951258718967438\n",
      "epoch 517, loss 1.353212833404541, R2 0.27160027623176575\n",
      "Eval loss 1.4115078449249268, R2 0.29624319076538086\n",
      "epoch 518, loss 1.3514305353164673, R2 0.27279871702194214\n",
      "Eval loss 1.409694790840149, R2 0.29735681414604187\n",
      "epoch 519, loss 1.3496544361114502, R2 0.27399298548698425\n",
      "Eval loss 1.4078882932662964, R2 0.29846689105033875\n",
      "epoch 520, loss 1.3478851318359375, R2 0.2751832902431488\n",
      "Eval loss 1.40608811378479, R2 0.29957297444343567\n",
      "epoch 521, loss 1.3461220264434814, R2 0.2763696014881134\n",
      "Eval loss 1.4042946100234985, R2 0.300675630569458\n",
      "epoch 522, loss 1.3443655967712402, R2 0.27755188941955566\n",
      "Eval loss 1.4025070667266846, R2 0.3017745912075043\n",
      "epoch 523, loss 1.3426153659820557, R2 0.278730183839798\n",
      "Eval loss 1.400726079940796, R2 0.30287012457847595\n",
      "epoch 524, loss 1.3408715724945068, R2 0.2799045741558075\n",
      "Eval loss 1.398951530456543, R2 0.303961843252182\n",
      "epoch 525, loss 1.3391342163085938, R2 0.2810748815536499\n",
      "Eval loss 1.3971831798553467, R2 0.3050500154495239\n",
      "epoch 526, loss 1.3374030590057373, R2 0.2822413742542267\n",
      "Eval loss 1.395421028137207, R2 0.30613449215888977\n",
      "epoch 527, loss 1.335678219795227, R2 0.28340384364128113\n",
      "Eval loss 1.393665075302124, R2 0.30721548199653625\n",
      "epoch 528, loss 1.3339598178863525, R2 0.2845625579357147\n",
      "Eval loss 1.3919154405593872, R2 0.30829304456710815\n",
      "epoch 529, loss 1.3322474956512451, R2 0.2857170104980469\n",
      "Eval loss 1.3901722431182861, R2 0.309366911649704\n",
      "epoch 530, loss 1.3305414915084839, R2 0.2868679463863373\n",
      "Eval loss 1.3884350061416626, R2 0.31043741106987\n",
      "epoch 531, loss 1.3288415670394897, R2 0.28801485896110535\n",
      "Eval loss 1.3867038488388062, R2 0.3115043342113495\n",
      "epoch 532, loss 1.3271480798721313, R2 0.289157897233963\n",
      "Eval loss 1.3849788904190063, R2 0.31256741285324097\n",
      "epoch 533, loss 1.3254605531692505, R2 0.2902975082397461\n",
      "Eval loss 1.3832601308822632, R2 0.3136274516582489\n",
      "epoch 534, loss 1.3237789869308472, R2 0.2914326786994934\n",
      "Eval loss 1.381547212600708, R2 0.3146836459636688\n",
      "epoch 535, loss 1.3221038579940796, R2 0.2925640940666199\n",
      "Eval loss 1.379840612411499, R2 0.31573665142059326\n",
      "epoch 536, loss 1.3204344511032104, R2 0.29369187355041504\n",
      "Eval loss 1.3781399726867676, R2 0.3167860805988312\n",
      "epoch 537, loss 1.318771481513977, R2 0.29481619596481323\n",
      "Eval loss 1.3764451742172241, R2 0.3178321123123169\n",
      "epoch 538, loss 1.3171143531799316, R2 0.29593631625175476\n",
      "Eval loss 1.3747565746307373, R2 0.31887486577033997\n",
      "epoch 539, loss 1.3154630661010742, R2 0.29705265164375305\n",
      "Eval loss 1.3730738162994385, R2 0.3199138343334198\n",
      "epoch 540, loss 1.3138179779052734, R2 0.2981654405593872\n",
      "Eval loss 1.3713970184326172, R2 0.3209496736526489\n",
      "epoch 541, loss 1.3121788501739502, R2 0.2992744743824005\n",
      "Eval loss 1.3697260618209839, R2 0.32198211550712585\n",
      "epoch 542, loss 1.310545563697815, R2 0.30037975311279297\n",
      "Eval loss 1.3680610656738281, R2 0.3230111300945282\n",
      "epoch 543, loss 1.3089183568954468, R2 0.30148133635520935\n",
      "Eval loss 1.36640202999115, R2 0.3240368068218231\n",
      "epoch 544, loss 1.3072967529296875, R2 0.30257949233055115\n",
      "Eval loss 1.3647487163543701, R2 0.3250592350959778\n",
      "epoch 545, loss 1.3056811094284058, R2 0.30367347598075867\n",
      "Eval loss 1.3631012439727783, R2 0.3260779082775116\n",
      "epoch 546, loss 1.3040711879730225, R2 0.3047640025615692\n",
      "Eval loss 1.361459493637085, R2 0.32709363102912903\n",
      "epoch 547, loss 1.3024673461914062, R2 0.30585095286369324\n",
      "Eval loss 1.3598235845565796, R2 0.32810601592063904\n",
      "epoch 548, loss 1.300869107246399, R2 0.30693426728248596\n",
      "Eval loss 1.3581935167312622, R2 0.32911479473114014\n",
      "epoch 549, loss 1.2992768287658691, R2 0.308013916015625\n",
      "Eval loss 1.3565690517425537, R2 0.33012059330940247\n",
      "epoch 550, loss 1.2976900339126587, R2 0.3090900182723999\n",
      "Eval loss 1.3549504280090332, R2 0.33112311363220215\n",
      "epoch 551, loss 1.2961090803146362, R2 0.3101627230644226\n",
      "Eval loss 1.3533374071121216, R2 0.3321223258972168\n",
      "epoch 552, loss 1.2945339679718018, R2 0.311231404542923\n",
      "Eval loss 1.3517299890518188, R2 0.3331182599067688\n",
      "epoch 553, loss 1.2929643392562866, R2 0.3122968375682831\n",
      "Eval loss 1.350128173828125, R2 0.33411094546318054\n",
      "epoch 554, loss 1.29140043258667, R2 0.3133586049079895\n",
      "Eval loss 1.3485320806503296, R2 0.335100382566452\n",
      "epoch 555, loss 1.289842128753662, R2 0.3144170045852661\n",
      "Eval loss 1.3469417095184326, R2 0.3360864222049713\n",
      "epoch 556, loss 1.2882894277572632, R2 0.3154715597629547\n",
      "Eval loss 1.3453567028045654, R2 0.33706942200660706\n",
      "epoch 557, loss 1.2867423295974731, R2 0.3165227472782135\n",
      "Eval loss 1.3437772989273071, R2 0.33804938197135925\n",
      "epoch 558, loss 1.285200834274292, R2 0.3175705671310425\n",
      "Eval loss 1.3422034978866577, R2 0.3390258550643921\n",
      "epoch 559, loss 1.2836648225784302, R2 0.31861475110054016\n",
      "Eval loss 1.340635061264038, R2 0.3399994373321533\n",
      "epoch 560, loss 1.2821344137191772, R2 0.3196555972099304\n",
      "Eval loss 1.3390722274780273, R2 0.34096959233283997\n",
      "epoch 561, loss 1.2806094884872437, R2 0.3206928074359894\n",
      "Eval loss 1.337515115737915, R2 0.3419366180896759\n",
      "epoch 562, loss 1.2790900468826294, R2 0.3217266798019409\n",
      "Eval loss 1.3359630107879639, R2 0.3429006040096283\n",
      "epoch 563, loss 1.277575969696045, R2 0.32275712490081787\n",
      "Eval loss 1.3344165086746216, R2 0.3438616096973419\n",
      "epoch 564, loss 1.2760672569274902, R2 0.3237842321395874\n",
      "Eval loss 1.332875370979309, R2 0.3448193371295929\n",
      "epoch 565, loss 1.2745641469955444, R2 0.324807733297348\n",
      "Eval loss 1.331339716911316, R2 0.3457740545272827\n",
      "epoch 566, loss 1.2730662822723389, R2 0.325827956199646\n",
      "Eval loss 1.3298090696334839, R2 0.34672555327415466\n",
      "epoch 567, loss 1.2715739011764526, R2 0.32684487104415894\n",
      "Eval loss 1.3282841444015503, R2 0.34767410159111023\n",
      "epoch 568, loss 1.2700868844985962, R2 0.32785823941230774\n",
      "Eval loss 1.326764464378357, R2 0.34861937165260315\n",
      "epoch 569, loss 1.26860511302948, R2 0.3288682997226715\n",
      "Eval loss 1.3252500295639038, R2 0.3495617210865021\n",
      "epoch 570, loss 1.267128586769104, R2 0.329875111579895\n",
      "Eval loss 1.3237409591674805, R2 0.3505009114742279\n",
      "epoch 571, loss 1.2656574249267578, R2 0.33087846636772156\n",
      "Eval loss 1.3222370147705078, R2 0.35143736004829407\n",
      "epoch 572, loss 1.2641915082931519, R2 0.33187854290008545\n",
      "Eval loss 1.3207383155822754, R2 0.3523705303668976\n",
      "epoch 573, loss 1.2627308368682861, R2 0.3328753411769867\n",
      "Eval loss 1.3192449808120728, R2 0.35330072045326233\n",
      "epoch 574, loss 1.2612754106521606, R2 0.33386877179145813\n",
      "Eval loss 1.3177567720413208, R2 0.3542279899120331\n",
      "epoch 575, loss 1.2598252296447754, R2 0.3348589539527893\n",
      "Eval loss 1.31627357006073, R2 0.3551521301269531\n",
      "epoch 576, loss 1.2583800554275513, R2 0.3358459770679474\n",
      "Eval loss 1.3147958517074585, R2 0.35607340931892395\n",
      "epoch 577, loss 1.2569401264190674, R2 0.33682963252067566\n",
      "Eval loss 1.3133230209350586, R2 0.35699158906936646\n",
      "epoch 578, loss 1.2555053234100342, R2 0.3378100097179413\n",
      "Eval loss 1.311855435371399, R2 0.35790690779685974\n",
      "epoch 579, loss 1.2540757656097412, R2 0.3387871980667114\n",
      "Eval loss 1.3103927373886108, R2 0.35881927609443665\n",
      "epoch 580, loss 1.2526512145996094, R2 0.3397611379623413\n",
      "Eval loss 1.3089354038238525, R2 0.35972872376441956\n",
      "epoch 581, loss 1.2512316703796387, R2 0.3407319486141205\n",
      "Eval loss 1.3074828386306763, R2 0.3606353998184204\n",
      "epoch 582, loss 1.2498172521591187, R2 0.34169942140579224\n",
      "Eval loss 1.3060355186462402, R2 0.3615388572216034\n",
      "epoch 583, loss 1.2484079599380493, R2 0.3426637649536133\n",
      "Eval loss 1.3045932054519653, R2 0.36243936419487\n",
      "epoch 584, loss 1.2470036745071411, R2 0.34362494945526123\n",
      "Eval loss 1.3031556606292725, R2 0.36333727836608887\n",
      "epoch 585, loss 1.2456040382385254, R2 0.3445829749107361\n",
      "Eval loss 1.3017233610153198, R2 0.3642321527004242\n",
      "epoch 586, loss 1.244209885597229, R2 0.34553802013397217\n",
      "Eval loss 1.3002959489822388, R2 0.36512404680252075\n",
      "epoch 587, loss 1.2428202629089355, R2 0.34648969769477844\n",
      "Eval loss 1.2988734245300293, R2 0.36601316928863525\n",
      "epoch 588, loss 1.2414358854293823, R2 0.34743833541870117\n",
      "Eval loss 1.297455906867981, R2 0.3668995201587677\n",
      "epoch 589, loss 1.2400563955307007, R2 0.3483835458755493\n",
      "Eval loss 1.2960432767868042, R2 0.3677828907966614\n",
      "epoch 590, loss 1.238681674003601, R2 0.34932592511177063\n",
      "Eval loss 1.2946354150772095, R2 0.3686635196208954\n",
      "epoch 591, loss 1.237311840057373, R2 0.35026517510414124\n",
      "Eval loss 1.2932325601577759, R2 0.36954134702682495\n",
      "epoch 592, loss 1.2359468936920166, R2 0.3512013852596283\n",
      "Eval loss 1.2918344736099243, R2 0.37041646242141724\n",
      "epoch 593, loss 1.2345869541168213, R2 0.3521345257759094\n",
      "Eval loss 1.2904412746429443, R2 0.37128856778144836\n",
      "epoch 594, loss 1.233231544494629, R2 0.35306450724601746\n",
      "Eval loss 1.2890527248382568, R2 0.3721579611301422\n",
      "epoch 595, loss 1.2318811416625977, R2 0.35399162769317627\n",
      "Eval loss 1.287669062614441, R2 0.37302443385124207\n",
      "epoch 596, loss 1.2305355072021484, R2 0.354915589094162\n",
      "Eval loss 1.2862900495529175, R2 0.37388819456100464\n",
      "epoch 597, loss 1.2291946411132812, R2 0.3558363616466522\n",
      "Eval loss 1.2849160432815552, R2 0.3747493624687195\n",
      "epoch 598, loss 1.2278584241867065, R2 0.35675424337387085\n",
      "Eval loss 1.2835465669631958, R2 0.3756076991558075\n",
      "epoch 599, loss 1.2265269756317139, R2 0.35766926407814026\n",
      "Eval loss 1.2821818590164185, R2 0.3764632046222687\n",
      "epoch 600, loss 1.2252002954483032, R2 0.3585813045501709\n",
      "Eval loss 1.2808218002319336, R2 0.37731608748435974\n",
      "epoch 601, loss 1.223878264427185, R2 0.35948994755744934\n",
      "Eval loss 1.2794666290283203, R2 0.3781661093235016\n",
      "epoch 602, loss 1.2225608825683594, R2 0.3603958785533905\n",
      "Eval loss 1.27811598777771, R2 0.37901341915130615\n",
      "epoch 603, loss 1.2212482690811157, R2 0.3612990379333496\n",
      "Eval loss 1.2767698764801025, R2 0.3798580467700958\n",
      "epoch 604, loss 1.219940185546875, R2 0.36219891905784607\n",
      "Eval loss 1.2754284143447876, R2 0.3807002007961273\n",
      "epoch 605, loss 1.2186367511749268, R2 0.36309611797332764\n",
      "Eval loss 1.2740917205810547, R2 0.381539523601532\n",
      "epoch 606, loss 1.2173378467559814, R2 0.3639901280403137\n",
      "Eval loss 1.2727595567703247, R2 0.3823760747909546\n",
      "epoch 607, loss 1.2160437107086182, R2 0.36488139629364014\n",
      "Eval loss 1.2714320421218872, R2 0.3832099139690399\n",
      "epoch 608, loss 1.2147541046142578, R2 0.3657698333263397\n",
      "Eval loss 1.270108938217163, R2 0.38404133915901184\n",
      "epoch 609, loss 1.2134689092636108, R2 0.3666551411151886\n",
      "Eval loss 1.268790364265442, R2 0.3848700225353241\n",
      "epoch 610, loss 1.2121882438659668, R2 0.36753758788108826\n",
      "Eval loss 1.2674764394760132, R2 0.3856959640979767\n",
      "epoch 611, loss 1.2109122276306152, R2 0.36841729283332825\n",
      "Eval loss 1.2661669254302979, R2 0.3865194618701935\n",
      "epoch 612, loss 1.2096405029296875, R2 0.36929404735565186\n",
      "Eval loss 1.264861822128296, R2 0.38734009861946106\n",
      "epoch 613, loss 1.2083734273910522, R2 0.370168000459671\n",
      "Eval loss 1.2635612487792969, R2 0.38815829157829285\n",
      "epoch 614, loss 1.2071107625961304, R2 0.3710392713546753\n",
      "Eval loss 1.2622650861740112, R2 0.3889738917350769\n",
      "epoch 615, loss 1.2058526277542114, R2 0.3719073534011841\n",
      "Eval loss 1.260973572731018, R2 0.3897869884967804\n",
      "epoch 616, loss 1.2045987844467163, R2 0.3727727234363556\n",
      "Eval loss 1.2596862316131592, R2 0.3905973434448242\n",
      "epoch 617, loss 1.2033494710922241, R2 0.37363556027412415\n",
      "Eval loss 1.2584035396575928, R2 0.3914051055908203\n",
      "epoch 618, loss 1.2021044492721558, R2 0.37449538707733154\n",
      "Eval loss 1.2571250200271606, R2 0.39221036434173584\n",
      "epoch 619, loss 1.2008638381958008, R2 0.37535229325294495\n",
      "Eval loss 1.255850911140442, R2 0.3930130898952484\n",
      "epoch 620, loss 1.1996275186538696, R2 0.37620648741722107\n",
      "Eval loss 1.254581093788147, R2 0.39381322264671326\n",
      "epoch 621, loss 1.1983956098556519, R2 0.3770579397678375\n",
      "Eval loss 1.2533155679702759, R2 0.3946109116077423\n",
      "epoch 622, loss 1.1971681118011475, R2 0.3779067099094391\n",
      "Eval loss 1.2520545721054077, R2 0.395406037569046\n",
      "epoch 623, loss 1.1959446668624878, R2 0.37875261902809143\n",
      "Eval loss 1.2507977485656738, R2 0.39619866013526917\n",
      "epoch 624, loss 1.1947256326675415, R2 0.3795957565307617\n",
      "Eval loss 1.2495453357696533, R2 0.3969886898994446\n",
      "epoch 625, loss 1.1935110092163086, R2 0.3804362118244171\n",
      "Eval loss 1.248297095298767, R2 0.3977762460708618\n",
      "epoch 626, loss 1.19230055809021, R2 0.38127395510673523\n",
      "Eval loss 1.2470529079437256, R2 0.3985614478588104\n",
      "epoch 627, loss 1.191094160079956, R2 0.3821089565753937\n",
      "Eval loss 1.245813250541687, R2 0.39934399724006653\n",
      "epoch 628, loss 1.1898921728134155, R2 0.38294127583503723\n",
      "Eval loss 1.2445777654647827, R2 0.400124192237854\n",
      "epoch 629, loss 1.1886943578720093, R2 0.3837709426879883\n",
      "Eval loss 1.2433464527130127, R2 0.40090176463127136\n",
      "epoch 630, loss 1.1875007152557373, R2 0.38459786772727966\n",
      "Eval loss 1.2421191930770874, R2 0.40167704224586487\n",
      "epoch 631, loss 1.1863112449645996, R2 0.38542211055755615\n",
      "Eval loss 1.2408963441848755, R2 0.4024497866630554\n",
      "epoch 632, loss 1.1851260662078857, R2 0.3862437307834625\n",
      "Eval loss 1.2396774291992188, R2 0.40322020649909973\n",
      "epoch 633, loss 1.183944821357727, R2 0.38706284761428833\n",
      "Eval loss 1.2384626865386963, R2 0.4039880931377411\n",
      "epoch 634, loss 1.1827679872512817, R2 0.3878788948059082\n",
      "Eval loss 1.237252116203308, R2 0.40475356578826904\n",
      "epoch 635, loss 1.181594967842102, R2 0.38869258761405945\n",
      "Eval loss 1.2360457181930542, R2 0.40551674365997314\n",
      "epoch 636, loss 1.1804261207580566, R2 0.3895036578178406\n",
      "Eval loss 1.2348432540893555, R2 0.40627729892730713\n",
      "epoch 637, loss 1.179261326789856, R2 0.39031219482421875\n",
      "Eval loss 1.2336450815200806, R2 0.4070357382297516\n",
      "epoch 638, loss 1.1781005859375, R2 0.3911178410053253\n",
      "Eval loss 1.2324508428573608, R2 0.4077916443347931\n",
      "epoch 639, loss 1.1769440174102783, R2 0.3919210731983185\n",
      "Eval loss 1.2312604188919067, R2 0.40854504704475403\n",
      "epoch 640, loss 1.1757913827896118, R2 0.39272165298461914\n",
      "Eval loss 1.230074405670166, R2 0.4092962145805359\n",
      "epoch 641, loss 1.17464280128479, R2 0.39351969957351685\n",
      "Eval loss 1.2288923263549805, R2 0.4100451171398163\n",
      "epoch 642, loss 1.173498272895813, R2 0.39431536197662354\n",
      "Eval loss 1.22771418094635, R2 0.41079142689704895\n",
      "epoch 643, loss 1.1723575592041016, R2 0.3951081335544586\n",
      "Eval loss 1.226539969444275, R2 0.4115356504917145\n",
      "epoch 644, loss 1.1712208986282349, R2 0.3958984315395355\n",
      "Eval loss 1.2253696918487549, R2 0.41227734088897705\n",
      "epoch 645, loss 1.170088291168213, R2 0.39668628573417664\n",
      "Eval loss 1.2242034673690796, R2 0.4130169749259949\n",
      "epoch 646, loss 1.1689594984054565, R2 0.39747151732444763\n",
      "Eval loss 1.2230411767959595, R2 0.41375401616096497\n",
      "epoch 647, loss 1.1678346395492554, R2 0.39825427532196045\n",
      "Eval loss 1.2218828201293945, R2 0.4144887626171112\n",
      "epoch 648, loss 1.1667137145996094, R2 0.3990345299243927\n",
      "Eval loss 1.2207283973693848, R2 0.415221244096756\n",
      "epoch 649, loss 1.1655967235565186, R2 0.3998122215270996\n",
      "Eval loss 1.2195777893066406, R2 0.41595155000686646\n",
      "epoch 650, loss 1.1644834280014038, R2 0.40058743953704834\n",
      "Eval loss 1.2184311151504517, R2 0.4166795015335083\n",
      "epoch 651, loss 1.1633740663528442, R2 0.40136024355888367\n",
      "Eval loss 1.2172881364822388, R2 0.4174050986766815\n",
      "epoch 652, loss 1.1622686386108398, R2 0.40213048458099365\n",
      "Eval loss 1.216149091720581, R2 0.4181286096572876\n",
      "epoch 653, loss 1.161167025566101, R2 0.40289828181266785\n",
      "Eval loss 1.2150139808654785, R2 0.4188496470451355\n",
      "epoch 654, loss 1.160069227218628, R2 0.40366360545158386\n",
      "Eval loss 1.2138826847076416, R2 0.41956862807273865\n",
      "epoch 655, loss 1.15897536277771, R2 0.4044264853000641\n",
      "Eval loss 1.2127553224563599, R2 0.4202851355075836\n",
      "epoch 656, loss 1.1578850746154785, R2 0.4051871597766876\n",
      "Eval loss 1.211631417274475, R2 0.4209997057914734\n",
      "epoch 657, loss 1.1567986011505127, R2 0.40594491362571716\n",
      "Eval loss 1.210511565208435, R2 0.421711802482605\n",
      "epoch 658, loss 1.1557159423828125, R2 0.4067005217075348\n",
      "Eval loss 1.209395408630371, R2 0.42242181301116943\n",
      "epoch 659, loss 1.1546369791030884, R2 0.40745365619659424\n",
      "Eval loss 1.2082828283309937, R2 0.42312946915626526\n",
      "epoch 660, loss 1.1535618305206299, R2 0.40820443630218506\n",
      "Eval loss 1.2071741819381714, R2 0.42383506894111633\n",
      "epoch 661, loss 1.152490258216858, R2 0.4089528024196625\n",
      "Eval loss 1.2060691118240356, R2 0.4245382249355316\n",
      "epoch 662, loss 1.1514225006103516, R2 0.4096986949443817\n",
      "Eval loss 1.2049678564071655, R2 0.4252394735813141\n",
      "epoch 663, loss 1.1503583192825317, R2 0.4104423224925995\n",
      "Eval loss 1.203870415687561, R2 0.42593833804130554\n",
      "epoch 664, loss 1.1492979526519775, R2 0.4111834466457367\n",
      "Eval loss 1.2027764320373535, R2 0.42663517594337463\n",
      "epoch 665, loss 1.1482411623001099, R2 0.4119223356246948\n",
      "Eval loss 1.201686143875122, R2 0.4273298382759094\n",
      "epoch 666, loss 1.1471880674362183, R2 0.41265878081321716\n",
      "Eval loss 1.2005995512008667, R2 0.4280223548412323\n",
      "epoch 667, loss 1.1461385488510132, R2 0.4133929908275604\n",
      "Eval loss 1.1995166540145874, R2 0.4287124574184418\n",
      "epoch 668, loss 1.1450927257537842, R2 0.41412466764450073\n",
      "Eval loss 1.1984373331069946, R2 0.42940062284469604\n",
      "epoch 669, loss 1.1440504789352417, R2 0.41485413908958435\n",
      "Eval loss 1.1973615884780884, R2 0.43008658289909363\n",
      "epoch 670, loss 1.1430115699768066, R2 0.41558125615119934\n",
      "Eval loss 1.1962895393371582, R2 0.43077051639556885\n",
      "epoch 671, loss 1.1419765949249268, R2 0.4163060486316681\n",
      "Eval loss 1.195220947265625, R2 0.43145230412483215\n",
      "epoch 672, loss 1.1409449577331543, R2 0.417028546333313\n",
      "Eval loss 1.1941560506820679, R2 0.43213194608688354\n",
      "epoch 673, loss 1.1399171352386475, R2 0.4177488386631012\n",
      "Eval loss 1.1930946111679077, R2 0.4328095316886902\n",
      "epoch 674, loss 1.1388925313949585, R2 0.4184666872024536\n",
      "Eval loss 1.192036747932434, R2 0.4334849417209625\n",
      "epoch 675, loss 1.1378716230392456, R2 0.41918233036994934\n",
      "Eval loss 1.190982460975647, R2 0.4341583251953125\n",
      "epoch 676, loss 1.1368541717529297, R2 0.4198956787586212\n",
      "Eval loss 1.1899316310882568, R2 0.4348295331001282\n",
      "epoch 677, loss 1.1358402967453003, R2 0.420606791973114\n",
      "Eval loss 1.1888843774795532, R2 0.43549880385398865\n",
      "epoch 678, loss 1.1348297595977783, R2 0.4213155508041382\n",
      "Eval loss 1.1878403425216675, R2 0.43616580963134766\n",
      "epoch 679, loss 1.1338227987289429, R2 0.4220222532749176\n",
      "Eval loss 1.186800241470337, R2 0.43683096766471863\n",
      "epoch 680, loss 1.1328192949295044, R2 0.42272645235061646\n",
      "Eval loss 1.1857633590698242, R2 0.43749383091926575\n",
      "epoch 681, loss 1.1318191289901733, R2 0.42342880368232727\n",
      "Eval loss 1.1847299337387085, R2 0.4381548762321472\n",
      "epoch 682, loss 1.1308224201202393, R2 0.42412853240966797\n",
      "Eval loss 1.1836997270584106, R2 0.43881380558013916\n",
      "epoch 683, loss 1.1298291683197021, R2 0.42482617497444153\n",
      "Eval loss 1.1826733350753784, R2 0.43947067856788635\n",
      "epoch 684, loss 1.1288392543792725, R2 0.425521582365036\n",
      "Eval loss 1.181650161743164, R2 0.440125435590744\n",
      "epoch 685, loss 1.1278527975082397, R2 0.4262148141860962\n",
      "Eval loss 1.1806303262710571, R2 0.44077834486961365\n",
      "epoch 686, loss 1.126869797706604, R2 0.4269058406352997\n",
      "Eval loss 1.1796139478683472, R2 0.44142916798591614\n",
      "epoch 687, loss 1.1258901357650757, R2 0.42759469151496887\n",
      "Eval loss 1.1786009073257446, R2 0.4420779347419739\n",
      "epoch 688, loss 1.1249136924743652, R2 0.42828142642974854\n",
      "Eval loss 1.1775912046432495, R2 0.4427247941493988\n",
      "epoch 689, loss 1.1239407062530518, R2 0.4289659559726715\n",
      "Eval loss 1.176585078239441, R2 0.44336947798728943\n",
      "epoch 690, loss 1.1229709386825562, R2 0.4296483099460602\n",
      "Eval loss 1.1755821704864502, R2 0.44401246309280396\n",
      "epoch 691, loss 1.122004508972168, R2 0.43032845854759216\n",
      "Eval loss 1.1745823621749878, R2 0.4446532130241394\n",
      "epoch 692, loss 1.1210414171218872, R2 0.431006520986557\n",
      "Eval loss 1.173586130142212, R2 0.445292204618454\n",
      "epoch 693, loss 1.1200816631317139, R2 0.4316824674606323\n",
      "Eval loss 1.1725932359695435, R2 0.44592902064323425\n",
      "epoch 694, loss 1.1191251277923584, R2 0.432356059551239\n",
      "Eval loss 1.1716035604476929, R2 0.4465639293193817\n",
      "epoch 695, loss 1.1181716918945312, R2 0.43302786350250244\n",
      "Eval loss 1.1706169843673706, R2 0.4471970498561859\n",
      "epoch 696, loss 1.117221713066101, R2 0.43369734287261963\n",
      "Eval loss 1.1696338653564453, R2 0.4478282928466797\n",
      "epoch 697, loss 1.1162750720977783, R2 0.43436476588249207\n",
      "Eval loss 1.168653964996338, R2 0.448457270860672\n",
      "epoch 698, loss 1.1153315305709839, R2 0.435030072927475\n",
      "Eval loss 1.1676771640777588, R2 0.44908440113067627\n",
      "epoch 699, loss 1.1143909692764282, R2 0.43569323420524597\n",
      "Eval loss 1.166703701019287, R2 0.4497097134590149\n",
      "epoch 700, loss 1.113453984260559, R2 0.4363543689250946\n",
      "Eval loss 1.1657334566116333, R2 0.4503330886363983\n",
      "epoch 701, loss 1.1125199794769287, R2 0.4370133578777313\n",
      "Eval loss 1.164766550064087, R2 0.4509544372558594\n",
      "epoch 702, loss 1.1115891933441162, R2 0.43767043948173523\n",
      "Eval loss 1.1638026237487793, R2 0.45157402753829956\n",
      "epoch 703, loss 1.1106616258621216, R2 0.43832531571388245\n",
      "Eval loss 1.1628419160842896, R2 0.45219171047210693\n",
      "epoch 704, loss 1.1097370386123657, R2 0.4389781951904297\n",
      "Eval loss 1.1618844270706177, R2 0.4528074264526367\n",
      "epoch 705, loss 1.1088157892227173, R2 0.4396289587020874\n",
      "Eval loss 1.1609301567077637, R2 0.45342138409614563\n",
      "epoch 706, loss 1.1078975200653076, R2 0.44027769565582275\n",
      "Eval loss 1.1599788665771484, R2 0.45403337478637695\n",
      "epoch 707, loss 1.1069824695587158, R2 0.4409245252609253\n",
      "Eval loss 1.159030795097351, R2 0.45464351773262024\n",
      "epoch 708, loss 1.1060705184936523, R2 0.4415692090988159\n",
      "Eval loss 1.1580857038497925, R2 0.4552517235279083\n",
      "epoch 709, loss 1.1051616668701172, R2 0.44221171736717224\n",
      "Eval loss 1.1571439504623413, R2 0.455858051776886\n",
      "epoch 710, loss 1.1042557954788208, R2 0.44285228848457336\n",
      "Eval loss 1.1562052965164185, R2 0.4564627707004547\n",
      "epoch 711, loss 1.1033530235290527, R2 0.44349098205566406\n",
      "Eval loss 1.1552693843841553, R2 0.4570653736591339\n",
      "epoch 712, loss 1.1024534702301025, R2 0.4441276490688324\n",
      "Eval loss 1.1543370485305786, R2 0.45766618847846985\n",
      "epoch 713, loss 1.1015567779541016, R2 0.44476231932640076\n",
      "Eval loss 1.1534074544906616, R2 0.4582653045654297\n",
      "epoch 714, loss 1.100663185119629, R2 0.44539496302604675\n",
      "Eval loss 1.1524808406829834, R2 0.4588623344898224\n",
      "epoch 715, loss 1.099772572517395, R2 0.4460254907608032\n",
      "Eval loss 1.1515575647354126, R2 0.4594578444957733\n",
      "epoch 716, loss 1.0988850593566895, R2 0.4466542899608612\n",
      "Eval loss 1.1506370306015015, R2 0.46005144715309143\n",
      "epoch 717, loss 1.0980005264282227, R2 0.44728097319602966\n",
      "Eval loss 1.1497195959091187, R2 0.46064311265945435\n",
      "epoch 718, loss 1.0971189737319946, R2 0.4479057192802429\n",
      "Eval loss 1.1488052606582642, R2 0.46123310923576355\n",
      "epoch 719, loss 1.0962404012680054, R2 0.44852858781814575\n",
      "Eval loss 1.1478937864303589, R2 0.4618212580680847\n",
      "epoch 720, loss 1.0953646898269653, R2 0.4491495192050934\n",
      "Eval loss 1.1469855308532715, R2 0.46240776777267456\n",
      "epoch 721, loss 1.094491958618164, R2 0.4497683346271515\n",
      "Eval loss 1.1460801362991333, R2 0.462992399930954\n",
      "epoch 722, loss 1.0936223268508911, R2 0.4503853917121887\n",
      "Eval loss 1.1451776027679443, R2 0.463575154542923\n",
      "epoch 723, loss 1.0927555561065674, R2 0.45100051164627075\n",
      "Eval loss 1.1442780494689941, R2 0.4641561210155487\n",
      "epoch 724, loss 1.0918916463851929, R2 0.45161378383636475\n",
      "Eval loss 1.1433815956115723, R2 0.46473538875579834\n",
      "epoch 725, loss 1.0910308361053467, R2 0.45222485065460205\n",
      "Eval loss 1.14248788356781, R2 0.46531298756599426\n",
      "epoch 726, loss 1.0901726484298706, R2 0.45283421874046326\n",
      "Eval loss 1.1415971517562866, R2 0.46588870882987976\n",
      "epoch 727, loss 1.0893176794052124, R2 0.4534416198730469\n",
      "Eval loss 1.1407092809677124, R2 0.4664628207683563\n",
      "epoch 728, loss 1.0884652137756348, R2 0.45404720306396484\n",
      "Eval loss 1.139824390411377, R2 0.4670350253582001\n",
      "epoch 729, loss 1.087615728378296, R2 0.4546509087085724\n",
      "Eval loss 1.1389423608779907, R2 0.4676056206226349\n",
      "epoch 730, loss 1.0867693424224854, R2 0.45525264739990234\n",
      "Eval loss 1.1380631923675537, R2 0.46817442774772644\n",
      "epoch 731, loss 1.085925579071045, R2 0.4558525085449219\n",
      "Eval loss 1.137186884880066, R2 0.46874159574508667\n",
      "epoch 732, loss 1.0850846767425537, R2 0.45645055174827576\n",
      "Eval loss 1.136313557624817, R2 0.46930694580078125\n",
      "epoch 733, loss 1.0842467546463013, R2 0.457046777009964\n",
      "Eval loss 1.135442852973938, R2 0.4698706865310669\n",
      "epoch 734, loss 1.083411455154419, R2 0.45764103531837463\n",
      "Eval loss 1.1345751285552979, R2 0.47043266892433167\n",
      "epoch 735, loss 1.0825790166854858, R2 0.4582335352897644\n",
      "Eval loss 1.133710265159607, R2 0.47099295258522034\n",
      "epoch 736, loss 1.0817493200302124, R2 0.45882418751716614\n",
      "Eval loss 1.1328481435775757, R2 0.4715515375137329\n",
      "epoch 737, loss 1.0809226036071777, R2 0.4594130218029022\n",
      "Eval loss 1.131988763809204, R2 0.472108393907547\n",
      "epoch 738, loss 1.0800985097885132, R2 0.46000000834465027\n",
      "Eval loss 1.1311322450637817, R2 0.4726637005805969\n",
      "epoch 739, loss 1.0792771577835083, R2 0.4605851471424103\n",
      "Eval loss 1.130278468132019, R2 0.4732172191143036\n",
      "epoch 740, loss 1.078458547592163, R2 0.46116864681243896\n",
      "Eval loss 1.129427433013916, R2 0.47376900911331177\n",
      "epoch 741, loss 1.077642798423767, R2 0.4617500305175781\n",
      "Eval loss 1.1285793781280518, R2 0.474319189786911\n",
      "epoch 742, loss 1.0768296718597412, R2 0.46232983469963074\n",
      "Eval loss 1.127733826637268, R2 0.4748677909374237\n",
      "epoch 743, loss 1.0760194063186646, R2 0.4629077911376953\n",
      "Eval loss 1.1268911361694336, R2 0.4754146635532379\n",
      "epoch 744, loss 1.075211763381958, R2 0.46348392963409424\n",
      "Eval loss 1.1260511875152588, R2 0.47595998644828796\n",
      "epoch 745, loss 1.0744067430496216, R2 0.4640582203865051\n",
      "Eval loss 1.1252139806747437, R2 0.4765036404132843\n",
      "epoch 746, loss 1.0736043453216553, R2 0.4646309018135071\n",
      "Eval loss 1.1243793964385986, R2 0.47704553604125977\n",
      "epoch 747, loss 1.0728048086166382, R2 0.4652017652988434\n",
      "Eval loss 1.1235475540161133, R2 0.47758597135543823\n",
      "epoch 748, loss 1.0720080137252808, R2 0.4657707214355469\n",
      "Eval loss 1.1227184534072876, R2 0.47812479734420776\n",
      "epoch 749, loss 1.071213722229004, R2 0.4663381278514862\n",
      "Eval loss 1.121891975402832, R2 0.4786616265773773\n",
      "epoch 750, loss 1.0704220533370972, R2 0.4669037163257599\n",
      "Eval loss 1.1210682392120361, R2 0.479197233915329\n",
      "epoch 751, loss 1.06963312625885, R2 0.46746739745140076\n",
      "Eval loss 1.1202470064163208, R2 0.47973111271858215\n",
      "epoch 752, loss 1.0688469409942627, R2 0.4680294990539551\n",
      "Eval loss 1.1194283962249756, R2 0.48026344180107117\n",
      "epoch 753, loss 1.0680631399154663, R2 0.46858981251716614\n",
      "Eval loss 1.1186127662658691, R2 0.480794221162796\n",
      "epoch 754, loss 1.06728196144104, R2 0.4691484570503235\n",
      "Eval loss 1.1177995204925537, R2 0.4813232123851776\n",
      "epoch 755, loss 1.0665035247802734, R2 0.4697053134441376\n",
      "Eval loss 1.1169888973236084, R2 0.481850802898407\n",
      "epoch 756, loss 1.0657275915145874, R2 0.47026050090789795\n",
      "Eval loss 1.1161808967590332, R2 0.48237669467926025\n",
      "epoch 757, loss 1.064954400062561, R2 0.4708141088485718\n",
      "Eval loss 1.1153755187988281, R2 0.48290112614631653\n",
      "epoch 758, loss 1.0641835927963257, R2 0.47136572003364563\n",
      "Eval loss 1.1145727634429932, R2 0.4834238290786743\n",
      "epoch 759, loss 1.063415288925171, R2 0.4719158411026001\n",
      "Eval loss 1.1137727499008179, R2 0.4839450716972351\n",
      "epoch 760, loss 1.0626498460769653, R2 0.47246411442756653\n",
      "Eval loss 1.112975001335144, R2 0.48446476459503174\n",
      "epoch 761, loss 1.0618866682052612, R2 0.4730108380317688\n",
      "Eval loss 1.1121799945831299, R2 0.48498275876045227\n",
      "epoch 762, loss 1.0611262321472168, R2 0.4735558331012726\n",
      "Eval loss 1.1113874912261963, R2 0.4854993522167206\n",
      "epoch 763, loss 1.060368299484253, R2 0.47409915924072266\n",
      "Eval loss 1.1105976104736328, R2 0.4860144555568695\n",
      "epoch 764, loss 1.05961275100708, R2 0.4746408462524414\n",
      "Eval loss 1.10981023311615, R2 0.48652780055999756\n",
      "epoch 765, loss 1.0588597059249878, R2 0.47518080472946167\n",
      "Eval loss 1.1090253591537476, R2 0.48703983426094055\n",
      "epoch 766, loss 1.058109164237976, R2 0.4757190942764282\n",
      "Eval loss 1.1082431077957153, R2 0.48755016922950745\n",
      "epoch 767, loss 1.057361125946045, R2 0.476255863904953\n",
      "Eval loss 1.1074632406234741, R2 0.48805907368659973\n",
      "epoch 768, loss 1.0566157102584839, R2 0.4767909348011017\n",
      "Eval loss 1.106685996055603, R2 0.48856648802757263\n",
      "epoch 769, loss 1.0558726787567139, R2 0.4773242771625519\n",
      "Eval loss 1.1059112548828125, R2 0.48907235264778137\n",
      "epoch 770, loss 1.0551321506500244, R2 0.4778560400009155\n",
      "Eval loss 1.1051387786865234, R2 0.4895767867565155\n",
      "epoch 771, loss 1.054394006729126, R2 0.4783862233161926\n",
      "Eval loss 1.1043689250946045, R2 0.4900796711444855\n",
      "epoch 772, loss 1.0536582469940186, R2 0.478914737701416\n",
      "Eval loss 1.1036014556884766, R2 0.4905810058116913\n",
      "epoch 773, loss 1.0529251098632812, R2 0.4794416129589081\n",
      "Eval loss 1.1028366088867188, R2 0.4910809397697449\n",
      "epoch 774, loss 1.0521941184997559, R2 0.4799668490886688\n",
      "Eval loss 1.1020740270614624, R2 0.49157923460006714\n",
      "epoch 775, loss 1.0514657497406006, R2 0.480490505695343\n",
      "Eval loss 1.1013140678405762, R2 0.49207618832588196\n",
      "epoch 776, loss 1.0507396459579468, R2 0.48101261258125305\n",
      "Eval loss 1.100556492805481, R2 0.49257156252861023\n",
      "epoch 777, loss 1.050016164779663, R2 0.4815332293510437\n",
      "Eval loss 1.0998013019561768, R2 0.4930654466152191\n",
      "epoch 778, loss 1.0492949485778809, R2 0.4820520579814911\n",
      "Eval loss 1.0990484952926636, R2 0.49355795979499817\n",
      "epoch 779, loss 1.0485761165618896, R2 0.4825694262981415\n",
      "Eval loss 1.0982980728149414, R2 0.4940491020679474\n",
      "epoch 780, loss 1.0478595495224, R2 0.4830850660800934\n",
      "Eval loss 1.0975500345230103, R2 0.4945385754108429\n",
      "epoch 781, loss 1.0471453666687012, R2 0.48359930515289307\n",
      "Eval loss 1.0968044996261597, R2 0.4950267970561981\n",
      "epoch 782, loss 1.0464338064193726, R2 0.4841117560863495\n",
      "Eval loss 1.0960612297058105, R2 0.49551334977149963\n",
      "epoch 783, loss 1.0457242727279663, R2 0.4846228361129761\n",
      "Eval loss 1.095320463180542, R2 0.4959985911846161\n",
      "epoch 784, loss 1.0450172424316406, R2 0.4851323068141937\n",
      "Eval loss 1.094581961631775, R2 0.49648240208625793\n",
      "epoch 785, loss 1.0443124771118164, R2 0.4856402277946472\n",
      "Eval loss 1.0938458442687988, R2 0.49696481227874756\n",
      "epoch 786, loss 1.0436099767684937, R2 0.48614662885665894\n",
      "Eval loss 1.0931119918823242, R2 0.4974457621574402\n",
      "epoch 787, loss 1.0429099798202515, R2 0.48665139079093933\n",
      "Eval loss 1.0923806428909302, R2 0.4979252815246582\n",
      "epoch 788, loss 1.0422121286392212, R2 0.48715466260910034\n",
      "Eval loss 1.091651439666748, R2 0.4984033405780792\n",
      "epoch 789, loss 1.0415165424346924, R2 0.4876563847064972\n",
      "Eval loss 1.0909247398376465, R2 0.49887993931770325\n",
      "epoch 790, loss 1.0408233404159546, R2 0.4881567656993866\n",
      "Eval loss 1.0902003049850464, R2 0.4993553161621094\n",
      "epoch 791, loss 1.0401324033737183, R2 0.4886553883552551\n",
      "Eval loss 1.0894781351089478, R2 0.4998292922973633\n",
      "epoch 792, loss 1.039443850517273, R2 0.489152729511261\n",
      "Eval loss 1.088758111000061, R2 0.500301718711853\n",
      "epoch 793, loss 1.03875732421875, R2 0.48964831233024597\n",
      "Eval loss 1.0880407094955444, R2 0.5007727146148682\n",
      "epoch 794, loss 1.038073182106018, R2 0.4901425242424011\n",
      "Eval loss 1.0873253345489502, R2 0.5012423992156982\n",
      "epoch 795, loss 1.037391185760498, R2 0.4906352758407593\n",
      "Eval loss 1.0866122245788574, R2 0.5017106533050537\n",
      "epoch 796, loss 1.036711573600769, R2 0.4911264181137085\n",
      "Eval loss 1.0859014987945557, R2 0.5021776556968689\n",
      "epoch 797, loss 1.036034107208252, R2 0.49161607027053833\n",
      "Eval loss 1.0851930379867554, R2 0.5026432871818542\n",
      "epoch 798, loss 1.0353587865829468, R2 0.49210435152053833\n",
      "Eval loss 1.0844866037368774, R2 0.5031073689460754\n",
      "epoch 799, loss 1.0346858501434326, R2 0.4925912022590637\n",
      "Eval loss 1.083782434463501, R2 0.5035701394081116\n",
      "epoch 800, loss 1.0340149402618408, R2 0.4930763244628906\n",
      "Eval loss 1.0830806493759155, R2 0.5040315985679626\n",
      "epoch 801, loss 1.0333462953567505, R2 0.4935601055622101\n",
      "Eval loss 1.0823811292648315, R2 0.5044917464256287\n",
      "epoch 802, loss 1.0326799154281616, R2 0.4940424859523773\n",
      "Eval loss 1.0816837549209595, R2 0.5049504637718201\n",
      "epoch 803, loss 1.0320156812667847, R2 0.4945233464241028\n",
      "Eval loss 1.0809886455535889, R2 0.5054077506065369\n",
      "epoch 804, loss 1.03135347366333, R2 0.49500277638435364\n",
      "Eval loss 1.0802956819534302, R2 0.5058638453483582\n",
      "epoch 805, loss 1.0306936502456665, R2 0.49548083543777466\n",
      "Eval loss 1.0796047449111938, R2 0.5063185691833496\n",
      "epoch 806, loss 1.0300358533859253, R2 0.4959573447704315\n",
      "Eval loss 1.0789161920547485, R2 0.506771981716156\n",
      "epoch 807, loss 1.029380202293396, R2 0.4964323341846466\n",
      "Eval loss 1.0782297849655151, R2 0.5072240233421326\n",
      "epoch 808, loss 1.028726577758789, R2 0.49690601229667664\n",
      "Eval loss 1.077545404434204, R2 0.5076747536659241\n",
      "epoch 809, loss 1.0280753374099731, R2 0.49737823009490967\n",
      "Eval loss 1.0768632888793945, R2 0.5081241726875305\n",
      "epoch 810, loss 1.0274261236190796, R2 0.4978489875793457\n",
      "Eval loss 1.0761834383010864, R2 0.5085721611976624\n",
      "epoch 811, loss 1.026779055595398, R2 0.4983184039592743\n",
      "Eval loss 1.0755057334899902, R2 0.5090189576148987\n",
      "epoch 812, loss 1.0261340141296387, R2 0.49878644943237305\n",
      "Eval loss 1.0748299360275269, R2 0.5094643235206604\n",
      "epoch 813, loss 1.0254911184310913, R2 0.4992530047893524\n",
      "Eval loss 1.0741562843322754, R2 0.5099086165428162\n",
      "epoch 814, loss 1.0248503684997559, R2 0.49971815943717957\n",
      "Eval loss 1.073485016822815, R2 0.5103514790534973\n",
      "epoch 815, loss 1.0242116451263428, R2 0.5001817941665649\n",
      "Eval loss 1.0728156566619873, R2 0.5107929706573486\n",
      "epoch 816, loss 1.023574948310852, R2 0.5006442070007324\n",
      "Eval loss 1.072148323059082, R2 0.5112332701683044\n",
      "epoch 817, loss 1.0229403972625732, R2 0.5011053085327148\n",
      "Eval loss 1.0714832544326782, R2 0.5116722583770752\n",
      "epoch 818, loss 1.0223078727722168, R2 0.5015647411346436\n",
      "Eval loss 1.0708203315734863, R2 0.5121098756790161\n",
      "epoch 819, loss 1.0216773748397827, R2 0.5020229816436768\n",
      "Eval loss 1.0701593160629272, R2 0.5125463008880615\n",
      "epoch 820, loss 1.0210490226745605, R2 0.5024799108505249\n",
      "Eval loss 1.06950044631958, R2 0.5129814743995667\n",
      "epoch 821, loss 1.0204225778579712, R2 0.5029352307319641\n",
      "Eval loss 1.0688436031341553, R2 0.5134154558181763\n",
      "epoch 822, loss 1.0197982788085938, R2 0.5033893585205078\n",
      "Eval loss 1.0681889057159424, R2 0.5138479471206665\n",
      "epoch 823, loss 1.0191760063171387, R2 0.5038421154022217\n",
      "Eval loss 1.0675363540649414, R2 0.5142794847488403\n",
      "epoch 824, loss 1.018555760383606, R2 0.5042934417724609\n",
      "Eval loss 1.0668857097625732, R2 0.5147094130516052\n",
      "epoch 825, loss 1.0179373025894165, R2 0.5047435164451599\n",
      "Eval loss 1.0662370920181274, R2 0.5151383876800537\n",
      "epoch 826, loss 1.017321228981018, R2 0.5051921606063843\n",
      "Eval loss 1.065590500831604, R2 0.5155658721923828\n",
      "epoch 827, loss 1.0167068243026733, R2 0.5056394338607788\n",
      "Eval loss 1.0649458169937134, R2 0.5159921646118164\n",
      "epoch 828, loss 1.0160945653915405, R2 0.5060853958129883\n",
      "Eval loss 1.0643033981323242, R2 0.5164174437522888\n",
      "epoch 829, loss 1.0154842138290405, R2 0.5065300464630127\n",
      "Eval loss 1.0636628866195679, R2 0.5168411731719971\n",
      "epoch 830, loss 1.0148760080337524, R2 0.5069733262062073\n",
      "Eval loss 1.0630242824554443, R2 0.5172638893127441\n",
      "epoch 831, loss 1.0142695903778076, R2 0.5074154138565063\n",
      "Eval loss 1.0623879432678223, R2 0.5176853537559509\n",
      "epoch 832, loss 1.0136651992797852, R2 0.5078561305999756\n",
      "Eval loss 1.0617533922195435, R2 0.5181055068969727\n",
      "epoch 833, loss 1.0130627155303955, R2 0.5082952976226807\n",
      "Eval loss 1.061120867729187, R2 0.5185245275497437\n",
      "epoch 834, loss 1.0124621391296387, R2 0.5087333917617798\n",
      "Eval loss 1.0604902505874634, R2 0.5189422965049744\n",
      "epoch 835, loss 1.0118635892868042, R2 0.5091699957847595\n",
      "Eval loss 1.059861660003662, R2 0.5193588137626648\n",
      "epoch 836, loss 1.011267066001892, R2 0.5096054673194885\n",
      "Eval loss 1.0592350959777832, R2 0.5197741389274597\n",
      "epoch 837, loss 1.0106722116470337, R2 0.5100396871566772\n",
      "Eval loss 1.0586103200912476, R2 0.5201882719993591\n",
      "epoch 838, loss 1.0100795030593872, R2 0.5104724764823914\n",
      "Eval loss 1.0579875707626343, R2 0.520601212978363\n",
      "epoch 839, loss 1.009488582611084, R2 0.51090407371521\n",
      "Eval loss 1.0573668479919434, R2 0.5210128426551819\n",
      "epoch 840, loss 1.0088998079299927, R2 0.5113341808319092\n",
      "Eval loss 1.0567479133605957, R2 0.5214235186576843\n",
      "epoch 841, loss 1.0083125829696655, R2 0.5117631554603577\n",
      "Eval loss 1.0561310052871704, R2 0.5218328237533569\n",
      "epoch 842, loss 1.0077275037765503, R2 0.5121909379959106\n",
      "Eval loss 1.055516004562378, R2 0.5222409963607788\n",
      "epoch 843, loss 1.0071442127227783, R2 0.512617290019989\n",
      "Eval loss 1.0549029111862183, R2 0.5226480960845947\n",
      "epoch 844, loss 1.0065627098083496, R2 0.5130423903465271\n",
      "Eval loss 1.0542916059494019, R2 0.5230538845062256\n",
      "epoch 845, loss 1.0059831142425537, R2 0.5134662985801697\n",
      "Eval loss 1.0536823272705078, R2 0.5234584212303162\n",
      "epoch 846, loss 1.005405306816101, R2 0.5138888955116272\n",
      "Eval loss 1.0530749559402466, R2 0.5238619446754456\n",
      "epoch 847, loss 1.0048296451568604, R2 0.514310359954834\n",
      "Eval loss 1.0524694919586182, R2 0.5242642760276794\n",
      "epoch 848, loss 1.0042556524276733, R2 0.5147305130958557\n",
      "Eval loss 1.051865816116333, R2 0.5246655344963074\n",
      "epoch 849, loss 1.0036835670471191, R2 0.5151492953300476\n",
      "Eval loss 1.0512640476226807, R2 0.5250654220581055\n",
      "epoch 850, loss 1.0031131505966187, R2 0.515566885471344\n",
      "Eval loss 1.0506641864776611, R2 0.5254642963409424\n",
      "epoch 851, loss 1.0025447607040405, R2 0.5159833431243896\n",
      "Eval loss 1.0500659942626953, R2 0.5258619785308838\n",
      "epoch 852, loss 1.0019781589508057, R2 0.5163984298706055\n",
      "Eval loss 1.0494699478149414, R2 0.5262585282325745\n",
      "epoch 853, loss 1.001413345336914, R2 0.516812264919281\n",
      "Eval loss 1.0488755702972412, R2 0.5266538858413696\n",
      "epoch 854, loss 1.0008503198623657, R2 0.5172250270843506\n",
      "Eval loss 1.0482829809188843, R2 0.5270482897758484\n",
      "epoch 855, loss 1.0002890825271606, R2 0.5176364779472351\n",
      "Eval loss 1.0476922988891602, R2 0.5274413824081421\n",
      "epoch 856, loss 0.999729573726654, R2 0.5180468559265137\n",
      "Eval loss 1.0471035242080688, R2 0.5278334021568298\n",
      "epoch 857, loss 0.999172031879425, R2 0.5184557437896729\n",
      "Eval loss 1.0465165376663208, R2 0.5282242894172668\n",
      "epoch 858, loss 0.9986162781715393, R2 0.5188636183738708\n",
      "Eval loss 1.045931339263916, R2 0.5286140441894531\n",
      "epoch 859, loss 0.9980621337890625, R2 0.5192701816558838\n",
      "Eval loss 1.0453479290008545, R2 0.5290026664733887\n",
      "epoch 860, loss 0.9975098371505737, R2 0.519675612449646\n",
      "Eval loss 1.0447663068771362, R2 0.5293902158737183\n",
      "epoch 861, loss 0.9969593286514282, R2 0.5200798511505127\n",
      "Eval loss 1.0441864728927612, R2 0.5297766327857971\n",
      "epoch 862, loss 0.9964104890823364, R2 0.5204828381538391\n",
      "Eval loss 1.043608546257019, R2 0.53016197681427\n",
      "epoch 863, loss 0.9958636164665222, R2 0.52088463306427\n",
      "Eval loss 1.0430322885513306, R2 0.5305461287498474\n",
      "epoch 864, loss 0.9953183531761169, R2 0.521285355091095\n",
      "Eval loss 1.0424578189849854, R2 0.5309292078018188\n",
      "epoch 865, loss 0.9947748780250549, R2 0.5216847062110901\n",
      "Eval loss 1.0418851375579834, R2 0.5313112735748291\n",
      "epoch 866, loss 0.9942330718040466, R2 0.5220831036567688\n",
      "Eval loss 1.0413142442703247, R2 0.5316920876502991\n",
      "epoch 867, loss 0.9936929941177368, R2 0.5224801898002625\n",
      "Eval loss 1.0407450199127197, R2 0.5320719480514526\n",
      "epoch 868, loss 0.9931546449661255, R2 0.5228761434555054\n",
      "Eval loss 1.0401777029037476, R2 0.5324506759643555\n",
      "epoch 869, loss 0.9926179647445679, R2 0.523270845413208\n",
      "Eval loss 1.039612054824829, R2 0.5328283905982971\n",
      "epoch 870, loss 0.9920830726623535, R2 0.5236644148826599\n",
      "Eval loss 1.0390480756759644, R2 0.533204972743988\n",
      "epoch 871, loss 0.9915499091148376, R2 0.5240569114685059\n",
      "Eval loss 1.0384857654571533, R2 0.5335805416107178\n",
      "epoch 872, loss 0.9910183548927307, R2 0.5244481563568115\n",
      "Eval loss 1.0379254817962646, R2 0.5339547991752625\n",
      "epoch 873, loss 0.9904885292053223, R2 0.5248382687568665\n",
      "Eval loss 1.0373666286468506, R2 0.5343281626701355\n",
      "epoch 874, loss 0.9899604916572571, R2 0.5252273678779602\n",
      "Eval loss 1.0368096828460693, R2 0.5347005128860474\n",
      "epoch 875, loss 0.9894341230392456, R2 0.5256151556968689\n",
      "Eval loss 1.0362542867660522, R2 0.5350716710090637\n",
      "epoch 876, loss 0.9889092445373535, R2 0.5260018706321716\n",
      "Eval loss 1.0357006788253784, R2 0.5354418754577637\n",
      "epoch 877, loss 0.9883860945701599, R2 0.5263873934745789\n",
      "Eval loss 1.0351487398147583, R2 0.5358110070228577\n",
      "epoch 878, loss 0.9878646731376648, R2 0.5267717838287354\n",
      "Eval loss 1.0345985889434814, R2 0.5361790060997009\n",
      "epoch 879, loss 0.9873449802398682, R2 0.5271551609039307\n",
      "Eval loss 1.0340501070022583, R2 0.536545991897583\n",
      "epoch 880, loss 0.9868267774581909, R2 0.5275372862815857\n",
      "Eval loss 1.0335031747817993, R2 0.5369119644165039\n",
      "epoch 881, loss 0.9863101840019226, R2 0.5279183983802795\n",
      "Eval loss 1.032957911491394, R2 0.5372769236564636\n",
      "epoch 882, loss 0.9857953786849976, R2 0.5282983779907227\n",
      "Eval loss 1.0324145555496216, R2 0.5376408100128174\n",
      "epoch 883, loss 0.9852821230888367, R2 0.5286771059036255\n",
      "Eval loss 1.0318726301193237, R2 0.53800368309021\n",
      "epoch 884, loss 0.984770655632019, R2 0.5290549397468567\n",
      "Eval loss 1.0313323736190796, R2 0.5383654236793518\n",
      "epoch 885, loss 0.9842606782913208, R2 0.5294315814971924\n",
      "Eval loss 1.0307939052581787, R2 0.5387262105941772\n",
      "epoch 886, loss 0.9837523102760315, R2 0.5298070311546326\n",
      "Eval loss 1.030256986618042, R2 0.5390859842300415\n",
      "epoch 887, loss 0.9832455515861511, R2 0.5301814079284668\n",
      "Eval loss 1.029721736907959, R2 0.5394448637962341\n",
      "epoch 888, loss 0.9827404022216797, R2 0.5305548310279846\n",
      "Eval loss 1.0291880369186401, R2 0.539802610874176\n",
      "epoch 889, loss 0.9822367429733276, R2 0.5309270620346069\n",
      "Eval loss 1.0286561250686646, R2 0.5401592254638672\n",
      "epoch 890, loss 0.9817347526550293, R2 0.5312982201576233\n",
      "Eval loss 1.0281257629394531, R2 0.5405148863792419\n",
      "epoch 891, loss 0.9812344908714294, R2 0.5316681265830994\n",
      "Eval loss 1.0275969505310059, R2 0.5408696532249451\n",
      "epoch 892, loss 0.9807357788085938, R2 0.5320371389389038\n",
      "Eval loss 1.0270698070526123, R2 0.5412233471870422\n",
      "epoch 893, loss 0.9802385568618774, R2 0.5324050188064575\n",
      "Eval loss 1.026544213294983, R2 0.5415759682655334\n",
      "epoch 894, loss 0.9797428250312805, R2 0.53277188539505\n",
      "Eval loss 1.0260202884674072, R2 0.5419275760650635\n",
      "epoch 895, loss 0.9792486429214478, R2 0.5331375598907471\n",
      "Eval loss 1.0254980325698853, R2 0.5422783493995667\n",
      "epoch 896, loss 0.9787561893463135, R2 0.5335023403167725\n",
      "Eval loss 1.0249773263931274, R2 0.5426280498504639\n",
      "epoch 897, loss 0.9782652854919434, R2 0.5338658690452576\n",
      "Eval loss 1.0244580507278442, R2 0.5429767370223999\n",
      "epoch 898, loss 0.9777758121490479, R2 0.5342283844947815\n",
      "Eval loss 1.0239405632019043, R2 0.5433245301246643\n",
      "epoch 899, loss 0.9772878885269165, R2 0.5345900058746338\n",
      "Eval loss 1.023424506187439, R2 0.5436712503433228\n",
      "epoch 900, loss 0.9768016934394836, R2 0.5349503755569458\n",
      "Eval loss 1.0229099988937378, R2 0.5440170764923096\n",
      "epoch 901, loss 0.9763168692588806, R2 0.5353097915649414\n",
      "Eval loss 1.0223970413208008, R2 0.5443618893623352\n",
      "epoch 902, loss 0.9758334755897522, R2 0.535668134689331\n",
      "Eval loss 1.021885871887207, R2 0.5447055697441101\n",
      "epoch 903, loss 0.9753517508506775, R2 0.5360254645347595\n",
      "Eval loss 1.0213760137557983, R2 0.5450485348701477\n",
      "epoch 904, loss 0.9748714566230774, R2 0.536381721496582\n",
      "Eval loss 1.0208677053451538, R2 0.5453903675079346\n",
      "epoch 905, loss 0.9743926525115967, R2 0.5367369651794434\n",
      "Eval loss 1.0203609466552734, R2 0.545731246471405\n",
      "epoch 906, loss 0.9739154577255249, R2 0.5370911955833435\n",
      "Eval loss 1.0198558568954468, R2 0.5460712313652039\n",
      "epoch 907, loss 0.9734398126602173, R2 0.5374443531036377\n",
      "Eval loss 1.0193521976470947, R2 0.5464102625846863\n",
      "epoch 908, loss 0.9729655385017395, R2 0.5377965569496155\n",
      "Eval loss 1.0188500881195068, R2 0.5467482805252075\n",
      "epoch 909, loss 0.9724928140640259, R2 0.5381476283073425\n",
      "Eval loss 1.0183494091033936, R2 0.5470854640007019\n",
      "epoch 910, loss 0.9720214605331421, R2 0.5384977459907532\n",
      "Eval loss 1.0178505182266235, R2 0.5474215149879456\n",
      "epoch 911, loss 0.971551775932312, R2 0.5388468503952026\n",
      "Eval loss 1.017352819442749, R2 0.5477567315101624\n",
      "epoch 912, loss 0.9710834622383118, R2 0.5391950607299805\n",
      "Eval loss 1.0168566703796387, R2 0.548090934753418\n",
      "epoch 913, loss 0.9706164598464966, R2 0.539542019367218\n",
      "Eval loss 1.0163620710372925, R2 0.5484243631362915\n",
      "epoch 914, loss 0.9701511263847351, R2 0.5398880839347839\n",
      "Eval loss 1.015869140625, R2 0.5487566590309143\n",
      "epoch 915, loss 0.9696872234344482, R2 0.5402331352233887\n",
      "Eval loss 1.0153775215148926, R2 0.549088180065155\n",
      "epoch 916, loss 0.9692246913909912, R2 0.540577232837677\n",
      "Eval loss 1.0148875713348389, R2 0.5494186282157898\n",
      "epoch 917, loss 0.9687636494636536, R2 0.5409202575683594\n",
      "Eval loss 1.0143988132476807, R2 0.5497482419013977\n",
      "epoch 918, loss 0.9683040380477905, R2 0.5412623882293701\n",
      "Eval loss 1.0139116048812866, R2 0.550076961517334\n",
      "epoch 919, loss 0.9678459763526917, R2 0.5416035056114197\n",
      "Eval loss 1.0134259462356567, R2 0.5504046678543091\n",
      "epoch 920, loss 0.9673892259597778, R2 0.5419435501098633\n",
      "Eval loss 1.0129417181015015, R2 0.5507315397262573\n",
      "epoch 921, loss 0.9669339656829834, R2 0.5422827005386353\n",
      "Eval loss 1.0124590396881104, R2 0.5510573983192444\n",
      "epoch 922, loss 0.9664801359176636, R2 0.542620837688446\n",
      "Eval loss 1.0119776725769043, R2 0.5513824820518494\n",
      "epoch 923, loss 0.9660276770591736, R2 0.5429580211639404\n",
      "Eval loss 1.0114977359771729, R2 0.5517065525054932\n",
      "epoch 924, loss 0.9655765295028687, R2 0.5432942509651184\n",
      "Eval loss 1.0110193490982056, R2 0.5520296692848206\n",
      "epoch 925, loss 0.9651269316673279, R2 0.5436295866966248\n",
      "Eval loss 1.0105422735214233, R2 0.5523518919944763\n",
      "epoch 926, loss 0.9646786451339722, R2 0.5439638495445251\n",
      "Eval loss 1.0100668668746948, R2 0.55267333984375\n",
      "epoch 927, loss 0.9642319083213806, R2 0.5442970991134644\n",
      "Eval loss 1.0095926523208618, R2 0.552993893623352\n",
      "epoch 928, loss 0.9637864232063293, R2 0.5446293354034424\n",
      "Eval loss 1.009119987487793, R2 0.5533134937286377\n",
      "epoch 929, loss 0.9633424282073975, R2 0.5449607372283936\n",
      "Eval loss 1.0086486339569092, R2 0.5536320805549622\n",
      "epoch 930, loss 0.9628996849060059, R2 0.5452911853790283\n",
      "Eval loss 1.0081788301467896, R2 0.5539500117301941\n",
      "epoch 931, loss 0.9624583721160889, R2 0.5456206202507019\n",
      "Eval loss 1.0077104568481445, R2 0.5542668700218201\n",
      "epoch 932, loss 0.9620184898376465, R2 0.5459492206573486\n",
      "Eval loss 1.007243275642395, R2 0.554582953453064\n",
      "epoch 933, loss 0.9615799188613892, R2 0.5462768077850342\n",
      "Eval loss 1.0067775249481201, R2 0.5548981428146362\n",
      "epoch 934, loss 0.9611427783966064, R2 0.5466034412384033\n",
      "Eval loss 1.006313443183899, R2 0.5552123785018921\n",
      "epoch 935, loss 0.9607068300247192, R2 0.546929121017456\n",
      "Eval loss 1.0058505535125732, R2 0.5555258393287659\n",
      "epoch 936, loss 0.9602723717689514, R2 0.5472539663314819\n",
      "Eval loss 1.0053889751434326, R2 0.555838406085968\n",
      "epoch 937, loss 0.9598392844200134, R2 0.5475777983665466\n",
      "Eval loss 1.0049288272857666, R2 0.5561500787734985\n",
      "epoch 938, loss 0.9594075083732605, R2 0.5479006767272949\n",
      "Eval loss 1.0044699907302856, R2 0.5564608573913574\n",
      "epoch 939, loss 0.9589769244194031, R2 0.5482227802276611\n",
      "Eval loss 1.0040128231048584, R2 0.5567707419395447\n",
      "epoch 940, loss 0.9585479497909546, R2 0.5485437512397766\n",
      "Eval loss 1.0035568475723267, R2 0.5570798516273499\n",
      "epoch 941, loss 0.9581201076507568, R2 0.54886394739151\n",
      "Eval loss 1.0031020641326904, R2 0.5573880672454834\n",
      "epoch 942, loss 0.9576936960220337, R2 0.5491831302642822\n",
      "Eval loss 1.0026488304138184, R2 0.5576953887939453\n",
      "epoch 943, loss 0.957268476486206, R2 0.5495014190673828\n",
      "Eval loss 1.0021967887878418, R2 0.5580019354820251\n",
      "epoch 944, loss 0.9568447470664978, R2 0.5498188138008118\n",
      "Eval loss 1.0017461776733398, R2 0.5583075881004333\n",
      "epoch 945, loss 0.9564222097396851, R2 0.5501353740692139\n",
      "Eval loss 1.0012969970703125, R2 0.5586124062538147\n",
      "epoch 946, loss 0.9560010433197021, R2 0.5504509210586548\n",
      "Eval loss 1.0008491277694702, R2 0.5589163303375244\n",
      "epoch 947, loss 0.9555811285972595, R2 0.5507656335830688\n",
      "Eval loss 1.0004026889801025, R2 0.559219479560852\n",
      "epoch 948, loss 0.9551626443862915, R2 0.5510793924331665\n",
      "Eval loss 0.9999573230743408, R2 0.5595217347145081\n",
      "epoch 949, loss 0.9547452330589294, R2 0.5513923168182373\n",
      "Eval loss 0.9995133876800537, R2 0.5598232746124268\n",
      "epoch 950, loss 0.9543291926383972, R2 0.5517042875289917\n",
      "Eval loss 0.9990708231925964, R2 0.560123860836029\n",
      "epoch 951, loss 0.9539144039154053, R2 0.552015483379364\n",
      "Eval loss 0.998629629611969, R2 0.5604236125946045\n",
      "epoch 952, loss 0.9535010457038879, R2 0.5523255467414856\n",
      "Eval loss 0.9981896281242371, R2 0.5607225298881531\n",
      "epoch 953, loss 0.9530889391899109, R2 0.5526348352432251\n",
      "Eval loss 0.9977508783340454, R2 0.5610206723213196\n",
      "epoch 954, loss 0.9526779055595398, R2 0.5529433488845825\n",
      "Eval loss 0.9973135590553284, R2 0.5613179802894592\n",
      "epoch 955, loss 0.9522683024406433, R2 0.5532509684562683\n",
      "Eval loss 0.9968774914741516, R2 0.5616145133972168\n",
      "epoch 956, loss 0.9518598914146423, R2 0.5535576343536377\n",
      "Eval loss 0.9964427351951599, R2 0.5619102120399475\n",
      "epoch 957, loss 0.9514529705047607, R2 0.5538634657859802\n",
      "Eval loss 0.9960092306137085, R2 0.5622050762176514\n",
      "epoch 958, loss 0.9510470628738403, R2 0.5541684627532959\n",
      "Eval loss 0.9955770373344421, R2 0.5624991059303284\n",
      "epoch 959, loss 0.9506424069404602, R2 0.5544726252555847\n",
      "Eval loss 0.9951461553573608, R2 0.5627923607826233\n",
      "epoch 960, loss 0.9502390623092651, R2 0.5547757148742676\n",
      "Eval loss 0.994716465473175, R2 0.5630848407745361\n",
      "epoch 961, loss 0.9498368501663208, R2 0.5550780892372131\n",
      "Eval loss 0.9942879676818848, R2 0.5633763074874878\n",
      "epoch 962, loss 0.9494360089302063, R2 0.5553795695304871\n",
      "Eval loss 0.9938610792160034, R2 0.5636671185493469\n",
      "epoch 963, loss 0.9490364193916321, R2 0.5556802749633789\n",
      "Eval loss 0.9934352040290833, R2 0.563957154750824\n",
      "epoch 964, loss 0.9486379623413086, R2 0.5559800267219543\n",
      "Eval loss 0.9930104613304138, R2 0.564246416091919\n",
      "epoch 965, loss 0.9482407569885254, R2 0.5562789440155029\n",
      "Eval loss 0.9925872087478638, R2 0.5645348429679871\n",
      "epoch 966, loss 0.9478448033332825, R2 0.5565770268440247\n",
      "Eval loss 0.9921653270721436, R2 0.5648224353790283\n",
      "epoch 967, loss 0.9474501013755798, R2 0.5568742752075195\n",
      "Eval loss 0.991744339466095, R2 0.5651092529296875\n",
      "epoch 968, loss 0.9470564723014832, R2 0.5571706295013428\n",
      "Eval loss 0.9913248419761658, R2 0.5653953552246094\n",
      "epoch 969, loss 0.9466641545295715, R2 0.5574662685394287\n",
      "Eval loss 0.9909064173698425, R2 0.5656804442405701\n",
      "epoch 970, loss 0.9462730884552002, R2 0.5577608942985535\n",
      "Eval loss 0.9904893636703491, R2 0.5659651160240173\n",
      "epoch 971, loss 0.9458830952644348, R2 0.5580548048019409\n",
      "Eval loss 0.9900735020637512, R2 0.5662487149238586\n",
      "epoch 972, loss 0.9454942941665649, R2 0.5583478808403015\n",
      "Eval loss 0.9896588921546936, R2 0.5665315985679626\n",
      "epoch 973, loss 0.9451066851615906, R2 0.5586400628089905\n",
      "Eval loss 0.9892453551292419, R2 0.5668137073516846\n",
      "epoch 974, loss 0.9447203278541565, R2 0.5589314699172974\n",
      "Eval loss 0.9888330698013306, R2 0.5670952200889587\n",
      "epoch 975, loss 0.9443352222442627, R2 0.5592221617698669\n",
      "Eval loss 0.9884220361709595, R2 0.5673757791519165\n",
      "epoch 976, loss 0.9439511895179749, R2 0.5595118403434753\n",
      "Eval loss 0.9880122542381287, R2 0.567655622959137\n",
      "epoch 977, loss 0.9435683488845825, R2 0.5598007440567017\n",
      "Eval loss 0.9876036643981934, R2 0.5679346919059753\n",
      "epoch 978, loss 0.9431866407394409, R2 0.5600888729095459\n",
      "Eval loss 0.9871963858604431, R2 0.5682129859924316\n",
      "epoch 979, loss 0.9428061246871948, R2 0.5603761672973633\n",
      "Eval loss 0.986790120601654, R2 0.5684905648231506\n",
      "epoch 980, loss 0.9424269199371338, R2 0.5606626868247986\n",
      "Eval loss 0.9863851070404053, R2 0.5687673091888428\n",
      "epoch 981, loss 0.9420486688613892, R2 0.560948371887207\n",
      "Eval loss 0.9859813451766968, R2 0.5690433382987976\n",
      "epoch 982, loss 0.9416716694831848, R2 0.5612332820892334\n",
      "Eval loss 0.9855786561965942, R2 0.5693185925483704\n",
      "epoch 983, loss 0.9412958025932312, R2 0.5615172982215881\n",
      "Eval loss 0.985177218914032, R2 0.569593071937561\n",
      "epoch 984, loss 0.9409210681915283, R2 0.5618005394935608\n",
      "Eval loss 0.9847769737243652, R2 0.5698668956756592\n",
      "epoch 985, loss 0.9405474066734314, R2 0.5620831847190857\n",
      "Eval loss 0.9843778610229492, R2 0.57014000415802\n",
      "epoch 986, loss 0.9401749968528748, R2 0.5623647570610046\n",
      "Eval loss 0.9839799404144287, R2 0.5704122185707092\n",
      "epoch 987, loss 0.9398036003112793, R2 0.5626456141471863\n",
      "Eval loss 0.9835830926895142, R2 0.5706837773323059\n",
      "epoch 988, loss 0.9394335746765137, R2 0.5629258155822754\n",
      "Eval loss 0.9831876158714294, R2 0.5709545612335205\n",
      "epoch 989, loss 0.9390644431114197, R2 0.5632050633430481\n",
      "Eval loss 0.9827931523323059, R2 0.5712246298789978\n",
      "epoch 990, loss 0.938696563243866, R2 0.563483476638794\n",
      "Eval loss 0.9823998808860779, R2 0.5714939832687378\n",
      "epoch 991, loss 0.9383296966552734, R2 0.563761293888092\n",
      "Eval loss 0.982007622718811, R2 0.5717625617980957\n",
      "epoch 992, loss 0.937964141368866, R2 0.5640382766723633\n",
      "Eval loss 0.9816166758537292, R2 0.5720304846763611\n",
      "epoch 993, loss 0.9375995397567749, R2 0.5643144249916077\n",
      "Eval loss 0.981226921081543, R2 0.5722975134849548\n",
      "epoch 994, loss 0.937235951423645, R2 0.5645898580551147\n",
      "Eval loss 0.9808380603790283, R2 0.5725640058517456\n",
      "epoch 995, loss 0.9368736743927002, R2 0.5648645162582397\n",
      "Eval loss 0.9804506301879883, R2 0.5728296637535095\n",
      "epoch 996, loss 0.9365124106407166, R2 0.5651383996009827\n",
      "Eval loss 0.9800640940666199, R2 0.5730947256088257\n",
      "epoch 997, loss 0.9361522793769836, R2 0.5654114484786987\n",
      "Eval loss 0.9796788096427917, R2 0.5733588933944702\n",
      "epoch 998, loss 0.9357930421829224, R2 0.5656837821006775\n",
      "Eval loss 0.9792945384979248, R2 0.5736223459243774\n",
      "epoch 999, loss 0.9354351162910461, R2 0.565955400466919\n",
      "Eval loss 0.9789115786552429, R2 0.5738852024078369\n",
      "epoch 1000, loss 0.9350782036781311, R2 0.5662262439727783\n",
      "Eval loss 0.9785296320915222, R2 0.5741472840309143\n",
      "epoch 1001, loss 0.9347223043441772, R2 0.5664963126182556\n",
      "Eval loss 0.9781486392021179, R2 0.574408769607544\n",
      "epoch 1002, loss 0.9343676567077637, R2 0.566765546798706\n",
      "Eval loss 0.9777690172195435, R2 0.5746694207191467\n",
      "epoch 1003, loss 0.9340140223503113, R2 0.5670340657234192\n",
      "Eval loss 0.9773902893066406, R2 0.574929416179657\n",
      "epoch 1004, loss 0.9336614012718201, R2 0.567301869392395\n",
      "Eval loss 0.9770128130912781, R2 0.5751886963844299\n",
      "epoch 1005, loss 0.9333098530769348, R2 0.5675688982009888\n",
      "Eval loss 0.9766363501548767, R2 0.5754472017288208\n",
      "epoch 1006, loss 0.9329593181610107, R2 0.5678351521492004\n",
      "Eval loss 0.9762610197067261, R2 0.5757051110267639\n",
      "epoch 1007, loss 0.9326099157333374, R2 0.5681007504463196\n",
      "Eval loss 0.9758867025375366, R2 0.5759623050689697\n",
      "epoch 1008, loss 0.9322615265846252, R2 0.5683655142784119\n",
      "Eval loss 0.9755135774612427, R2 0.5762187838554382\n",
      "epoch 1009, loss 0.931914210319519, R2 0.5686296820640564\n",
      "Eval loss 0.9751413464546204, R2 0.5764746069908142\n",
      "epoch 1010, loss 0.9315680265426636, R2 0.5688929557800293\n",
      "Eval loss 0.9747704267501831, R2 0.5767296552658081\n",
      "epoch 1011, loss 0.9312228560447693, R2 0.5691555142402649\n",
      "Eval loss 0.9744005799293518, R2 0.5769841074943542\n",
      "epoch 1012, loss 0.930878758430481, R2 0.5694174766540527\n",
      "Eval loss 0.9740316271781921, R2 0.5772377848625183\n",
      "epoch 1013, loss 0.9305354952812195, R2 0.5696786046028137\n",
      "Eval loss 0.9736638069152832, R2 0.5774908661842346\n",
      "epoch 1014, loss 0.9301933646202087, R2 0.5699390172958374\n",
      "Eval loss 0.9732970595359802, R2 0.5777431726455688\n",
      "epoch 1015, loss 0.9298522472381592, R2 0.5701987743377686\n",
      "Eval loss 0.972931444644928, R2 0.5779948830604553\n",
      "epoch 1016, loss 0.9295122623443604, R2 0.5704576373100281\n",
      "Eval loss 0.9725667834281921, R2 0.5782458782196045\n",
      "epoch 1017, loss 0.9291732907295227, R2 0.5707159042358398\n",
      "Eval loss 0.9722033143043518, R2 0.5784961581230164\n",
      "epoch 1018, loss 0.928835391998291, R2 0.5709733963012695\n",
      "Eval loss 0.9718407392501831, R2 0.5787458419799805\n",
      "epoch 1019, loss 0.9284983277320862, R2 0.5712301731109619\n",
      "Eval loss 0.9714794158935547, R2 0.5789948105812073\n",
      "epoch 1020, loss 0.9281623959541321, R2 0.5714863538742065\n",
      "Eval loss 0.9711188673973083, R2 0.5792431235313416\n",
      "epoch 1021, loss 0.9278274178504944, R2 0.5717417597770691\n",
      "Eval loss 0.9707595705986023, R2 0.5794907808303833\n",
      "epoch 1022, loss 0.9274934530258179, R2 0.5719964504241943\n",
      "Eval loss 0.9704011082649231, R2 0.5797377228736877\n",
      "epoch 1023, loss 0.9271606206893921, R2 0.5722503662109375\n",
      "Eval loss 0.9700438380241394, R2 0.5799840688705444\n",
      "epoch 1024, loss 0.9268286824226379, R2 0.5725035667419434\n",
      "Eval loss 0.9696875810623169, R2 0.5802296996116638\n",
      "epoch 1025, loss 0.9264976978302002, R2 0.5727561712265015\n",
      "Eval loss 0.9693323969841003, R2 0.5804747343063354\n",
      "epoch 1026, loss 0.9261677861213684, R2 0.573008120059967\n",
      "Eval loss 0.9689781665802002, R2 0.580718994140625\n",
      "epoch 1027, loss 0.9258387684822083, R2 0.573259174823761\n",
      "Eval loss 0.9686248898506165, R2 0.5809627175331116\n",
      "epoch 1028, loss 0.9255107641220093, R2 0.5735097527503967\n",
      "Eval loss 0.9682727456092834, R2 0.5812057256698608\n",
      "epoch 1029, loss 0.9251838326454163, R2 0.5737594962120056\n",
      "Eval loss 0.9679214954376221, R2 0.5814481377601624\n",
      "epoch 1030, loss 0.9248578548431396, R2 0.5740085244178772\n",
      "Eval loss 0.9675714373588562, R2 0.5816898941993713\n",
      "epoch 1031, loss 0.9245328307151794, R2 0.5742570161819458\n",
      "Eval loss 0.967222273349762, R2 0.5819308757781982\n",
      "epoch 1032, loss 0.9242087006568909, R2 0.5745046138763428\n",
      "Eval loss 0.9668741226196289, R2 0.5821713209152222\n",
      "epoch 1033, loss 0.9238856434822083, R2 0.574751615524292\n",
      "Eval loss 0.966526985168457, R2 0.5824111700057983\n",
      "epoch 1034, loss 0.923563539981842, R2 0.5749979615211487\n",
      "Eval loss 0.9661807417869568, R2 0.5826503038406372\n",
      "epoch 1035, loss 0.9232423901557922, R2 0.5752436518669128\n",
      "Eval loss 0.9658355712890625, R2 0.5828887820243835\n",
      "epoch 1036, loss 0.9229221343994141, R2 0.5754886269569397\n",
      "Eval loss 0.9654914736747742, R2 0.5831266641616821\n",
      "epoch 1037, loss 0.9226028323173523, R2 0.575732946395874\n",
      "Eval loss 0.9651482701301575, R2 0.5833640098571777\n",
      "epoch 1038, loss 0.9222846031188965, R2 0.5759766101837158\n",
      "Eval loss 0.964806079864502, R2 0.5836005806922913\n",
      "epoch 1039, loss 0.9219673275947571, R2 0.5762194991111755\n",
      "Eval loss 0.9644648432731628, R2 0.5838364362716675\n",
      "epoch 1040, loss 0.9216509461402893, R2 0.5764618515968323\n",
      "Eval loss 0.9641246199607849, R2 0.5840718150138855\n",
      "epoch 1041, loss 0.9213355183601379, R2 0.5767034888267517\n",
      "Eval loss 0.9637853503227234, R2 0.5843064785003662\n",
      "epoch 1042, loss 0.9210209250450134, R2 0.5769443511962891\n",
      "Eval loss 0.963447093963623, R2 0.584540605545044\n",
      "epoch 1043, loss 0.9207074046134949, R2 0.5771846771240234\n",
      "Eval loss 0.9631097316741943, R2 0.5847740173339844\n",
      "epoch 1044, loss 0.9203948974609375, R2 0.5774242877960205\n",
      "Eval loss 0.9627732634544373, R2 0.5850069522857666\n",
      "epoch 1045, loss 0.9200831055641174, R2 0.577663242816925\n",
      "Eval loss 0.9624378681182861, R2 0.5852392315864563\n",
      "epoch 1046, loss 0.9197723865509033, R2 0.5779016017913818\n",
      "Eval loss 0.9621034860610962, R2 0.5854707360267639\n",
      "epoch 1047, loss 0.9194625616073608, R2 0.5781392455101013\n",
      "Eval loss 0.9617700576782227, R2 0.5857018232345581\n",
      "epoch 1048, loss 0.9191535711288452, R2 0.578376293182373\n",
      "Eval loss 0.9614375233650208, R2 0.585932195186615\n",
      "epoch 1049, loss 0.918845534324646, R2 0.5786125659942627\n",
      "Eval loss 0.9611058831214905, R2 0.5861618518829346\n",
      "epoch 1050, loss 0.918538510799408, R2 0.5788482427597046\n",
      "Eval loss 0.9607751965522766, R2 0.586391031742096\n",
      "epoch 1051, loss 0.9182324409484863, R2 0.5790833234786987\n",
      "Eval loss 0.9604454636573792, R2 0.5866196155548096\n",
      "epoch 1052, loss 0.917927086353302, R2 0.5793176889419556\n",
      "Eval loss 0.9601167440414429, R2 0.5868475437164307\n",
      "epoch 1053, loss 0.9176228046417236, R2 0.5795514583587646\n",
      "Eval loss 0.959788978099823, R2 0.587074875831604\n",
      "epoch 1054, loss 0.9173193573951721, R2 0.5797845721244812\n",
      "Eval loss 0.9594619870185852, R2 0.5873015522956848\n",
      "epoch 1055, loss 0.9170168042182922, R2 0.5800171494483948\n",
      "Eval loss 0.9591360092163086, R2 0.5875277519226074\n",
      "epoch 1056, loss 0.9167150855064392, R2 0.5802489519119263\n",
      "Eval loss 0.9588111042976379, R2 0.5877532362937927\n",
      "epoch 1057, loss 0.9164143204689026, R2 0.58048015832901\n",
      "Eval loss 0.9584869742393494, R2 0.587978184223175\n",
      "epoch 1058, loss 0.9161145091056824, R2 0.580710768699646\n",
      "Eval loss 0.9581636786460876, R2 0.5882025361061096\n",
      "epoch 1059, loss 0.915815532207489, R2 0.5809407234191895\n",
      "Eval loss 0.9578414559364319, R2 0.5884262323379517\n",
      "epoch 1060, loss 0.9155173897743225, R2 0.5811700224876404\n",
      "Eval loss 0.9575201272964478, R2 0.5886495113372803\n",
      "epoch 1061, loss 0.9152202606201172, R2 0.5813987255096436\n",
      "Eval loss 0.9571996331214905, R2 0.5888720154762268\n",
      "epoch 1062, loss 0.914923906326294, R2 0.581626832485199\n",
      "Eval loss 0.9568800330162048, R2 0.5890939831733704\n",
      "epoch 1063, loss 0.9146284461021423, R2 0.5818542838096619\n",
      "Eval loss 0.9565615057945251, R2 0.5893154144287109\n",
      "epoch 1064, loss 0.9143339991569519, R2 0.582081139087677\n",
      "Eval loss 0.9562436938285828, R2 0.5895361304283142\n",
      "epoch 1065, loss 0.9140401482582092, R2 0.5823072791099548\n",
      "Eval loss 0.9559269547462463, R2 0.5897563695907593\n",
      "epoch 1066, loss 0.9137473106384277, R2 0.5825328230857849\n",
      "Eval loss 0.955610990524292, R2 0.5899760127067566\n",
      "epoch 1067, loss 0.9134554266929626, R2 0.5827577710151672\n",
      "Eval loss 0.955295979976654, R2 0.5901951193809509\n",
      "epoch 1068, loss 0.9131642580032349, R2 0.5829821228981018\n",
      "Eval loss 0.9549818634986877, R2 0.5904135704040527\n",
      "epoch 1069, loss 0.9128741025924683, R2 0.5832059383392334\n",
      "Eval loss 0.9546685814857483, R2 0.5906314849853516\n",
      "epoch 1070, loss 0.9125846028327942, R2 0.5834290981292725\n",
      "Eval loss 0.9543561935424805, R2 0.5908486843109131\n",
      "epoch 1071, loss 0.9122961759567261, R2 0.5836515426635742\n",
      "Eval loss 0.954044759273529, R2 0.5910655856132507\n",
      "epoch 1072, loss 0.9120084047317505, R2 0.5838735103607178\n",
      "Eval loss 0.9537342190742493, R2 0.5912817716598511\n",
      "epoch 1073, loss 0.9117215871810913, R2 0.5840948224067688\n",
      "Eval loss 0.953424334526062, R2 0.5914973020553589\n",
      "epoch 1074, loss 0.9114357233047485, R2 0.5843155384063721\n",
      "Eval loss 0.953115701675415, R2 0.5917124152183533\n",
      "epoch 1075, loss 0.9111505746841431, R2 0.5845356583595276\n",
      "Eval loss 0.9528075456619263, R2 0.5919268131256104\n",
      "epoch 1076, loss 0.9108663201332092, R2 0.5847551226615906\n",
      "Eval loss 0.9525005221366882, R2 0.592140793800354\n",
      "epoch 1077, loss 0.9105827212333679, R2 0.5849740505218506\n",
      "Eval loss 0.9521942734718323, R2 0.5923541784286499\n",
      "epoch 1078, loss 0.9103002548217773, R2 0.5851923823356628\n",
      "Eval loss 0.9518888592720032, R2 0.5925669074058533\n",
      "epoch 1079, loss 0.910018265247345, R2 0.5854101181030273\n",
      "Eval loss 0.9515843391418457, R2 0.5927791595458984\n",
      "epoch 1080, loss 0.9097374081611633, R2 0.5856271982192993\n",
      "Eval loss 0.9512807130813599, R2 0.5929908156394958\n",
      "epoch 1081, loss 0.9094572067260742, R2 0.5858438611030579\n",
      "Eval loss 0.9509779810905457, R2 0.5932019948959351\n",
      "epoch 1082, loss 0.9091778993606567, R2 0.5860597491264343\n",
      "Eval loss 0.9506759643554688, R2 0.5934125185012817\n",
      "epoch 1083, loss 0.9088994264602661, R2 0.5862751007080078\n",
      "Eval loss 0.9503748416900635, R2 0.593622624874115\n",
      "epoch 1084, loss 0.9086217880249023, R2 0.5864899754524231\n",
      "Eval loss 0.9500745534896851, R2 0.5938320159912109\n",
      "epoch 1085, loss 0.9083448052406311, R2 0.5867041945457458\n",
      "Eval loss 0.949775218963623, R2 0.5940409302711487\n",
      "epoch 1086, loss 0.9080688953399658, R2 0.5869177579879761\n",
      "Eval loss 0.9494765400886536, R2 0.5942493081092834\n",
      "epoch 1087, loss 0.9077935814857483, R2 0.5871307849884033\n",
      "Eval loss 0.9491788148880005, R2 0.59445720911026\n",
      "epoch 1088, loss 0.9075191020965576, R2 0.5873432159423828\n",
      "Eval loss 0.948881983757019, R2 0.594664454460144\n",
      "epoch 1089, loss 0.9072454571723938, R2 0.5875552296638489\n",
      "Eval loss 0.9485859274864197, R2 0.5948711633682251\n",
      "epoch 1090, loss 0.9069725871086121, R2 0.5877665281295776\n",
      "Eval loss 0.9482906460762024, R2 0.5950773358345032\n",
      "epoch 1091, loss 0.906700611114502, R2 0.5879772901535034\n",
      "Eval loss 0.9479962587356567, R2 0.5952830910682678\n",
      "epoch 1092, loss 0.9064294099807739, R2 0.5881874561309814\n",
      "Eval loss 0.9477025866508484, R2 0.5954881310462952\n",
      "epoch 1093, loss 0.9061589241027832, R2 0.5883970856666565\n",
      "Eval loss 0.947409987449646, R2 0.5956927537918091\n",
      "epoch 1094, loss 0.9058893918991089, R2 0.5886062383651733\n",
      "Eval loss 0.9471179842948914, R2 0.59589684009552\n",
      "epoch 1095, loss 0.9056204557418823, R2 0.5888146162033081\n",
      "Eval loss 0.9468268752098083, R2 0.5961003303527832\n",
      "epoch 1096, loss 0.9053524732589722, R2 0.5890225768089294\n",
      "Eval loss 0.9465366005897522, R2 0.5963032841682434\n",
      "epoch 1097, loss 0.9050851464271545, R2 0.5892300009727478\n",
      "Eval loss 0.9462469816207886, R2 0.5965057611465454\n",
      "epoch 1098, loss 0.9048185348510742, R2 0.5894367694854736\n",
      "Eval loss 0.9459582567214966, R2 0.5967077612876892\n",
      "epoch 1099, loss 0.9045528173446655, R2 0.5896430611610413\n",
      "Eval loss 0.9456704258918762, R2 0.5969091057777405\n",
      "epoch 1100, loss 0.9042878150939941, R2 0.5898486971855164\n",
      "Eval loss 0.9453835487365723, R2 0.5971100330352783\n",
      "epoch 1101, loss 0.9040237665176392, R2 0.5900537967681885\n",
      "Eval loss 0.9450971484184265, R2 0.597310483455658\n",
      "epoch 1102, loss 0.9037603139877319, R2 0.5902584195137024\n",
      "Eval loss 0.9448115825653076, R2 0.5975102782249451\n",
      "epoch 1103, loss 0.9034976959228516, R2 0.5904623866081238\n",
      "Eval loss 0.9445269703865051, R2 0.5977095365524292\n",
      "epoch 1104, loss 0.9032357931137085, R2 0.590665876865387\n",
      "Eval loss 0.9442430734634399, R2 0.5979083180427551\n",
      "epoch 1105, loss 0.9029747247695923, R2 0.5908689498901367\n",
      "Eval loss 0.9439600706100464, R2 0.5981066226959229\n",
      "epoch 1106, loss 0.9027143120765686, R2 0.5910713076591492\n",
      "Eval loss 0.9436777234077454, R2 0.5983044505119324\n",
      "epoch 1107, loss 0.9024547338485718, R2 0.5912730693817139\n",
      "Eval loss 0.9433961510658264, R2 0.5985016822814941\n",
      "epoch 1108, loss 0.9021958708763123, R2 0.5914744138717651\n",
      "Eval loss 0.9431154727935791, R2 0.5986984372138977\n",
      "epoch 1109, loss 0.9019378423690796, R2 0.5916751623153687\n",
      "Eval loss 0.9428353905677795, R2 0.5988946557044983\n",
      "epoch 1110, loss 0.9016804099082947, R2 0.591875433921814\n",
      "Eval loss 0.9425563812255859, R2 0.5990903377532959\n",
      "epoch 1111, loss 0.901423990726471, R2 0.5920751094818115\n",
      "Eval loss 0.9422778487205505, R2 0.5992855429649353\n",
      "epoch 1112, loss 0.9011680483818054, R2 0.5922741889953613\n",
      "Eval loss 0.9420003294944763, R2 0.5994802713394165\n",
      "epoch 1113, loss 0.9009130001068115, R2 0.5924729108810425\n",
      "Eval loss 0.9417234659194946, R2 0.5996744632720947\n",
      "epoch 1114, loss 0.9006585478782654, R2 0.5926710367202759\n",
      "Eval loss 0.9414474368095398, R2 0.5998682379722595\n",
      "epoch 1115, loss 0.9004049897193909, R2 0.592868447303772\n",
      "Eval loss 0.9411720633506775, R2 0.6000614166259766\n",
      "epoch 1116, loss 0.9001520872116089, R2 0.5930655002593994\n",
      "Eval loss 0.940897524356842, R2 0.6002541780471802\n",
      "epoch 1117, loss 0.8999000191688538, R2 0.5932620167732239\n",
      "Eval loss 0.9406237602233887, R2 0.6004462838172913\n",
      "epoch 1118, loss 0.8996486067771912, R2 0.5934579968452454\n",
      "Eval loss 0.9403507709503174, R2 0.6006380319595337\n",
      "epoch 1119, loss 0.8993979096412659, R2 0.5936534404754639\n",
      "Eval loss 0.9400783777236938, R2 0.6008293032646179\n",
      "epoch 1120, loss 0.8991479277610779, R2 0.593848466873169\n",
      "Eval loss 0.9398068785667419, R2 0.6010199189186096\n",
      "epoch 1121, loss 0.8988986015319824, R2 0.5940428376197815\n",
      "Eval loss 0.9395362138748169, R2 0.6012101769447327\n",
      "epoch 1122, loss 0.8986501097679138, R2 0.5942366719245911\n",
      "Eval loss 0.9392661452293396, R2 0.6013998985290527\n",
      "epoch 1123, loss 0.8984023332595825, R2 0.5944300293922424\n",
      "Eval loss 0.9389967918395996, R2 0.6015892028808594\n",
      "epoch 1124, loss 0.8981553912162781, R2 0.5946228504180908\n",
      "Eval loss 0.9387283325195312, R2 0.601777970790863\n",
      "epoch 1125, loss 0.8979089260101318, R2 0.5948152542114258\n",
      "Eval loss 0.9384605884552002, R2 0.6019662618637085\n",
      "epoch 1126, loss 0.8976633548736572, R2 0.595007061958313\n",
      "Eval loss 0.9381935596466064, R2 0.602154016494751\n",
      "epoch 1127, loss 0.8974183201789856, R2 0.595198392868042\n",
      "Eval loss 0.9379271864891052, R2 0.6023412942886353\n",
      "epoch 1128, loss 0.8971741199493408, R2 0.595389187335968\n",
      "Eval loss 0.9376616477966309, R2 0.6025280952453613\n",
      "epoch 1129, loss 0.8969306349754333, R2 0.5955795049667358\n",
      "Eval loss 0.9373968243598938, R2 0.602714478969574\n",
      "epoch 1130, loss 0.8966877460479736, R2 0.595769464969635\n",
      "Eval loss 0.9371326565742493, R2 0.6029002666473389\n",
      "epoch 1131, loss 0.8964456915855408, R2 0.5959586501121521\n",
      "Eval loss 0.936869204044342, R2 0.6030856966972351\n",
      "epoch 1132, loss 0.8962041735649109, R2 0.5961474776268005\n",
      "Eval loss 0.9366066455841064, R2 0.6032706499099731\n",
      "epoch 1133, loss 0.8959634900093079, R2 0.5963356494903564\n",
      "Eval loss 0.9363447427749634, R2 0.603455126285553\n",
      "epoch 1134, loss 0.8957234621047974, R2 0.5965234041213989\n",
      "Eval loss 0.9360834360122681, R2 0.6036389470100403\n",
      "epoch 1135, loss 0.895484209060669, R2 0.5967106819152832\n",
      "Eval loss 0.9358229637145996, R2 0.6038224697113037\n",
      "epoch 1136, loss 0.8952455520629883, R2 0.596897542476654\n",
      "Eval loss 0.935563325881958, R2 0.6040054559707642\n",
      "epoch 1137, loss 0.8950074315071106, R2 0.5970838069915771\n",
      "Eval loss 0.9353041648864746, R2 0.6041880249977112\n",
      "epoch 1138, loss 0.8947702646255493, R2 0.597269594669342\n",
      "Eval loss 0.9350458383560181, R2 0.6043701171875\n",
      "epoch 1139, loss 0.8945335745811462, R2 0.597454845905304\n",
      "Eval loss 0.934788167476654, R2 0.6045517921447754\n",
      "epoch 1140, loss 0.8942977786064148, R2 0.5976396799087524\n",
      "Eval loss 0.9345312714576721, R2 0.6047329306602478\n",
      "epoch 1141, loss 0.8940625190734863, R2 0.5978240370750427\n",
      "Eval loss 0.9342750310897827, R2 0.6049135327339172\n",
      "epoch 1142, loss 0.8938278555870056, R2 0.5980077981948853\n",
      "Eval loss 0.9340195059776306, R2 0.605093777179718\n",
      "epoch 1143, loss 0.8935940861701965, R2 0.5981912016868591\n",
      "Eval loss 0.9337647557258606, R2 0.6052735447883606\n",
      "epoch 1144, loss 0.8933609127998352, R2 0.5983740091323853\n",
      "Eval loss 0.9335106611251831, R2 0.605452835559845\n",
      "epoch 1145, loss 0.8931282758712769, R2 0.5985564589500427\n",
      "Eval loss 0.9332571029663086, R2 0.6056316494941711\n",
      "epoch 1146, loss 0.8928964734077454, R2 0.5987382531166077\n",
      "Eval loss 0.9330044388771057, R2 0.6058099865913391\n",
      "epoch 1147, loss 0.8926652073860168, R2 0.598919689655304\n",
      "Eval loss 0.9327523112297058, R2 0.6059879660606384\n",
      "epoch 1148, loss 0.8924346566200256, R2 0.5991005301475525\n",
      "Eval loss 0.9325010776519775, R2 0.6061654686927795\n",
      "epoch 1149, loss 0.8922047019004822, R2 0.5992810726165771\n",
      "Eval loss 0.9322503805160522, R2 0.6063424944877625\n",
      "epoch 1150, loss 0.8919755220413208, R2 0.5994610786437988\n",
      "Eval loss 0.932000458240509, R2 0.6065189838409424\n",
      "epoch 1151, loss 0.891746997833252, R2 0.5996405482292175\n",
      "Eval loss 0.9317511320114136, R2 0.6066951155662537\n",
      "epoch 1152, loss 0.8915190100669861, R2 0.599819540977478\n",
      "Eval loss 0.9315024614334106, R2 0.6068708300590515\n",
      "epoch 1153, loss 0.8912917375564575, R2 0.5999981164932251\n",
      "Eval loss 0.9312546253204346, R2 0.6070460677146912\n",
      "epoch 1154, loss 0.8910651803016663, R2 0.6001760959625244\n",
      "Eval loss 0.9310072660446167, R2 0.6072207689285278\n",
      "epoch 1155, loss 0.890839159488678, R2 0.6003537178039551\n",
      "Eval loss 0.9307607412338257, R2 0.6073951125144958\n",
      "epoch 1156, loss 0.8906139731407166, R2 0.6005309224128723\n",
      "Eval loss 0.930514931678772, R2 0.6075689792633057\n",
      "epoch 1157, loss 0.8903892040252686, R2 0.6007075905799866\n",
      "Eval loss 0.930269718170166, R2 0.607742428779602\n",
      "epoch 1158, loss 0.8901651501655579, R2 0.6008838415145874\n",
      "Eval loss 0.930025041103363, R2 0.6079154014587402\n",
      "epoch 1159, loss 0.8899417519569397, R2 0.6010594964027405\n",
      "Eval loss 0.9297811388969421, R2 0.6080880165100098\n",
      "epoch 1160, loss 0.8897190093994141, R2 0.6012348532676697\n",
      "Eval loss 0.9295378923416138, R2 0.6082600951194763\n",
      "epoch 1161, loss 0.8894969820976257, R2 0.6014096140861511\n",
      "Eval loss 0.9292954802513123, R2 0.6084317564964294\n",
      "epoch 1162, loss 0.8892754912376404, R2 0.6015840172767639\n",
      "Eval loss 0.9290534257888794, R2 0.6086030006408691\n",
      "epoch 1163, loss 0.8890545964241028, R2 0.6017579436302185\n",
      "Eval loss 0.9288123250007629, R2 0.6087737679481506\n",
      "epoch 1164, loss 0.8888343572616577, R2 0.6019313335418701\n",
      "Eval loss 0.9285715818405151, R2 0.6089441776275635\n",
      "epoch 1165, loss 0.8886147141456604, R2 0.6021043062210083\n",
      "Eval loss 0.9283316731452942, R2 0.6091141104698181\n",
      "epoch 1166, loss 0.8883957862854004, R2 0.6022768020629883\n",
      "Eval loss 0.9280923008918762, R2 0.6092836856842041\n",
      "epoch 1167, loss 0.8881774544715881, R2 0.6024489402770996\n",
      "Eval loss 0.9278537034988403, R2 0.6094527840614319\n",
      "epoch 1168, loss 0.8879597187042236, R2 0.602620542049408\n",
      "Eval loss 0.927615761756897, R2 0.6096214652061462\n",
      "epoch 1169, loss 0.8877426385879517, R2 0.6027917265892029\n",
      "Eval loss 0.9273783564567566, R2 0.6097896099090576\n",
      "epoch 1170, loss 0.8875260949134827, R2 0.6029624342918396\n",
      "Eval loss 0.9271417856216431, R2 0.6099575161933899\n",
      "epoch 1171, loss 0.8873102068901062, R2 0.6031327247619629\n",
      "Eval loss 0.9269056916236877, R2 0.6101248264312744\n",
      "epoch 1172, loss 0.8870949745178223, R2 0.6033025979995728\n",
      "Eval loss 0.926670253276825, R2 0.6102918386459351\n",
      "epoch 1173, loss 0.8868803381919861, R2 0.6034719944000244\n",
      "Eval loss 0.9264354705810547, R2 0.6104583144187927\n",
      "epoch 1174, loss 0.8866663575172424, R2 0.6036408543586731\n",
      "Eval loss 0.926201343536377, R2 0.6106244325637817\n",
      "epoch 1175, loss 0.8864529728889465, R2 0.6038093566894531\n",
      "Eval loss 0.9259678721427917, R2 0.6107901334762573\n",
      "epoch 1176, loss 0.8862400650978088, R2 0.6039774417877197\n",
      "Eval loss 0.9257350564002991, R2 0.6109554171562195\n",
      "epoch 1177, loss 0.8860278725624084, R2 0.6041451096534729\n",
      "Eval loss 0.9255026578903198, R2 0.6111202239990234\n",
      "epoch 1178, loss 0.885816216468811, R2 0.6043123006820679\n",
      "Eval loss 0.9252711534500122, R2 0.6112846732139587\n",
      "epoch 1179, loss 0.8856052160263062, R2 0.6044790744781494\n",
      "Eval loss 0.9250401258468628, R2 0.6114487051963806\n",
      "epoch 1180, loss 0.8853948712348938, R2 0.6046454310417175\n",
      "Eval loss 0.9248097538948059, R2 0.6116123199462891\n",
      "epoch 1181, loss 0.8851850032806396, R2 0.6048112511634827\n",
      "Eval loss 0.9245800971984863, R2 0.6117754578590393\n",
      "epoch 1182, loss 0.884975790977478, R2 0.6049765944480896\n",
      "Eval loss 0.9243508577346802, R2 0.6119382977485657\n",
      "epoch 1183, loss 0.8847670555114746, R2 0.6051416993141174\n",
      "Eval loss 0.9241224527359009, R2 0.6121007204055786\n",
      "epoch 1184, loss 0.884559154510498, R2 0.6053063273429871\n",
      "Eval loss 0.9238945245742798, R2 0.6122626662254333\n",
      "epoch 1185, loss 0.8843515515327454, R2 0.6054704189300537\n",
      "Eval loss 0.9236672520637512, R2 0.6124242544174194\n",
      "epoch 1186, loss 0.8841447234153748, R2 0.6056341528892517\n",
      "Eval loss 0.9234405159950256, R2 0.6125853657722473\n",
      "epoch 1187, loss 0.8839384913444519, R2 0.6057974100112915\n",
      "Eval loss 0.9232144951820374, R2 0.6127461194992065\n",
      "epoch 1188, loss 0.883732795715332, R2 0.6059603095054626\n",
      "Eval loss 0.9229890704154968, R2 0.6129063963890076\n",
      "epoch 1189, loss 0.8835276961326599, R2 0.6061227321624756\n",
      "Eval loss 0.9227641820907593, R2 0.6130664348602295\n",
      "epoch 1190, loss 0.883323073387146, R2 0.6062847375869751\n",
      "Eval loss 0.9225399494171143, R2 0.6132259368896484\n",
      "epoch 1191, loss 0.8831191658973694, R2 0.606446385383606\n",
      "Eval loss 0.9223164319992065, R2 0.6133851408958435\n",
      "epoch 1192, loss 0.8829158544540405, R2 0.6066076159477234\n",
      "Eval loss 0.922093391418457, R2 0.6135438680648804\n",
      "epoch 1193, loss 0.8827129006385803, R2 0.6067683100700378\n",
      "Eval loss 0.9218708872795105, R2 0.6137022376060486\n",
      "epoch 1194, loss 0.8825106620788574, R2 0.6069287061691284\n",
      "Eval loss 0.921649158000946, R2 0.6138600707054138\n",
      "epoch 1195, loss 0.8823090195655823, R2 0.6070886254310608\n",
      "Eval loss 0.9214279055595398, R2 0.6140176653862\n",
      "epoch 1196, loss 0.8821079730987549, R2 0.607248067855835\n",
      "Eval loss 0.9212071895599365, R2 0.6141749024391174\n",
      "epoch 1197, loss 0.8819074034690857, R2 0.6074072122573853\n",
      "Eval loss 0.9209871888160706, R2 0.6143316030502319\n",
      "epoch 1198, loss 0.8817073702812195, R2 0.6075659394264221\n",
      "Eval loss 0.9207677841186523, R2 0.6144879460334778\n",
      "epoch 1199, loss 0.8815079927444458, R2 0.607724130153656\n",
      "Eval loss 0.9205488562583923, R2 0.6146438717842102\n",
      "epoch 1200, loss 0.8813091516494751, R2 0.6078820824623108\n",
      "Eval loss 0.9203305840492249, R2 0.614799439907074\n",
      "epoch 1201, loss 0.8811109066009521, R2 0.6080395579338074\n",
      "Eval loss 0.9201129078865051, R2 0.6149546504020691\n",
      "epoch 1202, loss 0.8809131979942322, R2 0.6081966161727905\n",
      "Eval loss 0.9198957681655884, R2 0.6151096224784851\n",
      "epoch 1203, loss 0.8807159066200256, R2 0.6083531975746155\n",
      "Eval loss 0.9196792244911194, R2 0.6152638792991638\n",
      "epoch 1204, loss 0.8805193305015564, R2 0.6085094809532166\n",
      "Eval loss 0.9194632172584534, R2 0.6154179573059082\n",
      "epoch 1205, loss 0.8803233504295349, R2 0.6086652874946594\n",
      "Eval loss 0.9192478656768799, R2 0.6155716180801392\n",
      "epoch 1206, loss 0.8801277279853821, R2 0.6088207364082336\n",
      "Eval loss 0.9190332293510437, R2 0.6157249212265015\n",
      "epoch 1207, loss 0.8799328804016113, R2 0.6089757680892944\n",
      "Eval loss 0.9188188910484314, R2 0.6158777475357056\n",
      "epoch 1208, loss 0.879738450050354, R2 0.6091303825378418\n",
      "Eval loss 0.9186052083969116, R2 0.6160302758216858\n",
      "epoch 1209, loss 0.8795445561408997, R2 0.6092845797538757\n",
      "Eval loss 0.9183922410011292, R2 0.6161823868751526\n",
      "epoch 1210, loss 0.8793511986732483, R2 0.609438419342041\n",
      "Eval loss 0.9181796908378601, R2 0.616334080696106\n",
      "epoch 1211, loss 0.8791583776473999, R2 0.6095919013023376\n",
      "Eval loss 0.9179677963256836, R2 0.6164855360984802\n",
      "epoch 1212, loss 0.8789661526679993, R2 0.6097449064254761\n",
      "Eval loss 0.9177563786506653, R2 0.6166365146636963\n",
      "epoch 1213, loss 0.8787745237350464, R2 0.6098975539207458\n",
      "Eval loss 0.9175454378128052, R2 0.6167870163917542\n",
      "epoch 1214, loss 0.8785832524299622, R2 0.6100497841835022\n",
      "Eval loss 0.9173353314399719, R2 0.6169373393058777\n",
      "epoch 1215, loss 0.8783925771713257, R2 0.6102015972137451\n",
      "Eval loss 0.9171255230903625, R2 0.617087185382843\n",
      "epoch 1216, loss 0.8782026171684265, R2 0.6103531122207642\n",
      "Eval loss 0.9169163703918457, R2 0.6172367334365845\n",
      "epoch 1217, loss 0.8780129551887512, R2 0.610504150390625\n",
      "Eval loss 0.9167079329490662, R2 0.6173858046531677\n",
      "epoch 1218, loss 0.8778238892555237, R2 0.6106547713279724\n",
      "Eval loss 0.9164997935295105, R2 0.6175345182418823\n",
      "epoch 1219, loss 0.8776353001594543, R2 0.6108051538467407\n",
      "Eval loss 0.9162922501564026, R2 0.6176828742027283\n",
      "epoch 1220, loss 0.8774474263191223, R2 0.6109550595283508\n",
      "Eval loss 0.9160854816436768, R2 0.6178309917449951\n",
      "epoch 1221, loss 0.8772600293159485, R2 0.6111045479774475\n",
      "Eval loss 0.9158790707588196, R2 0.617978572845459\n",
      "epoch 1222, loss 0.8770730495452881, R2 0.6112537384033203\n",
      "Eval loss 0.9156733155250549, R2 0.618125855922699\n",
      "epoch 1223, loss 0.8768866062164307, R2 0.6114024519920349\n",
      "Eval loss 0.9154680371284485, R2 0.6182727813720703\n",
      "epoch 1224, loss 0.8767006993293762, R2 0.6115508675575256\n",
      "Eval loss 0.9152632355690002, R2 0.6184192299842834\n",
      "epoch 1225, loss 0.87651526927948, R2 0.6116988062858582\n",
      "Eval loss 0.9150591492652893, R2 0.6185654997825623\n",
      "epoch 1226, loss 0.8763304352760315, R2 0.6118464469909668\n",
      "Eval loss 0.9148554801940918, R2 0.6187114119529724\n",
      "epoch 1227, loss 0.876146137714386, R2 0.611993670463562\n",
      "Eval loss 0.9146523475646973, R2 0.6188567876815796\n",
      "epoch 1228, loss 0.8759622573852539, R2 0.6121404767036438\n",
      "Eval loss 0.9144497513771057, R2 0.6190019249916077\n",
      "epoch 1229, loss 0.8757789134979248, R2 0.6122869849205017\n",
      "Eval loss 0.9142477512359619, R2 0.6191466450691223\n",
      "epoch 1230, loss 0.8755962252616882, R2 0.6124330759048462\n",
      "Eval loss 0.9140461683273315, R2 0.6192910671234131\n",
      "epoch 1231, loss 0.8754138946533203, R2 0.612578809261322\n",
      "Eval loss 0.9138453602790833, R2 0.6194350719451904\n",
      "epoch 1232, loss 0.8752320408821106, R2 0.612724244594574\n",
      "Eval loss 0.9136449098587036, R2 0.6195787787437439\n",
      "epoch 1233, loss 0.8750507831573486, R2 0.6128690838813782\n",
      "Eval loss 0.9134449362754822, R2 0.6197220683097839\n",
      "epoch 1234, loss 0.8748700022697449, R2 0.6130138039588928\n",
      "Eval loss 0.9132455587387085, R2 0.6198650002479553\n",
      "epoch 1235, loss 0.8746896386146545, R2 0.6131580471992493\n",
      "Eval loss 0.9130467176437378, R2 0.6200075745582581\n",
      "epoch 1236, loss 0.8745099306106567, R2 0.6133018732070923\n",
      "Eval loss 0.9128483533859253, R2 0.6201499700546265\n",
      "epoch 1237, loss 0.8743306398391724, R2 0.6134454607963562\n",
      "Eval loss 0.9126505851745605, R2 0.6202917695045471\n",
      "epoch 1238, loss 0.8741518259048462, R2 0.6135885715484619\n",
      "Eval loss 0.912453293800354, R2 0.6204333901405334\n",
      "epoch 1239, loss 0.873973548412323, R2 0.613731324672699\n",
      "Eval loss 0.9122565984725952, R2 0.6205745339393616\n",
      "epoch 1240, loss 0.8737958073616028, R2 0.6138737201690674\n",
      "Eval loss 0.9120603799819946, R2 0.6207154393196106\n",
      "epoch 1241, loss 0.8736184239387512, R2 0.6140157580375671\n",
      "Eval loss 0.9118645191192627, R2 0.6208558678627014\n",
      "epoch 1242, loss 0.8734415769577026, R2 0.6141574382781982\n",
      "Eval loss 0.9116693735122681, R2 0.6209961771965027\n",
      "epoch 1243, loss 0.8732653856277466, R2 0.6142988204956055\n",
      "Eval loss 0.9114747643470764, R2 0.621135950088501\n",
      "epoch 1244, loss 0.8730894923210144, R2 0.6144397258758545\n",
      "Eval loss 0.9112805128097534, R2 0.6212754249572754\n",
      "epoch 1245, loss 0.8729141354560852, R2 0.6145803332328796\n",
      "Eval loss 0.9110867977142334, R2 0.6214145421981812\n",
      "epoch 1246, loss 0.8727393746376038, R2 0.6147205233573914\n",
      "Eval loss 0.9108937382698059, R2 0.621553361415863\n",
      "epoch 1247, loss 0.8725649118423462, R2 0.614860475063324\n",
      "Eval loss 0.9107011556625366, R2 0.6216918230056763\n",
      "epoch 1248, loss 0.8723909854888916, R2 0.6150000095367432\n",
      "Eval loss 0.9105088710784912, R2 0.6218299865722656\n",
      "epoch 1249, loss 0.87221759557724, R2 0.6151391863822937\n",
      "Eval loss 0.9103173017501831, R2 0.6219677329063416\n",
      "epoch 1250, loss 0.8720447421073914, R2 0.6152780055999756\n",
      "Eval loss 0.910126268863678, R2 0.6221051812171936\n",
      "epoch 1251, loss 0.8718722462654114, R2 0.6154164671897888\n",
      "Eval loss 0.9099356532096863, R2 0.6222422122955322\n",
      "epoch 1252, loss 0.8717002868652344, R2 0.6155546307563782\n",
      "Eval loss 0.9097453951835632, R2 0.622378945350647\n",
      "epoch 1253, loss 0.8715287446975708, R2 0.6156923770904541\n",
      "Eval loss 0.9095557332038879, R2 0.6225154399871826\n",
      "epoch 1254, loss 0.871357798576355, R2 0.6158297061920166\n",
      "Eval loss 0.9093665480613708, R2 0.6226515173912048\n",
      "epoch 1255, loss 0.871187150478363, R2 0.6159668564796448\n",
      "Eval loss 0.9091780185699463, R2 0.6227872967720032\n",
      "epoch 1256, loss 0.8710170388221741, R2 0.61610347032547\n",
      "Eval loss 0.9089899063110352, R2 0.6229227185249329\n",
      "epoch 1257, loss 0.8708474636077881, R2 0.6162399053573608\n",
      "Eval loss 0.9088022112846375, R2 0.6230579018592834\n",
      "epoch 1258, loss 0.8706782460212708, R2 0.6163759231567383\n",
      "Eval loss 0.9086150527000427, R2 0.623192548751831\n",
      "epoch 1259, loss 0.8705095648765564, R2 0.6165115833282471\n",
      "Eval loss 0.908428430557251, R2 0.6233270168304443\n",
      "epoch 1260, loss 0.870341420173645, R2 0.616646945476532\n",
      "Eval loss 0.9082421660423279, R2 0.6234611868858337\n",
      "epoch 1261, loss 0.8701736330986023, R2 0.6167818903923035\n",
      "Eval loss 0.9080565571784973, R2 0.6235948801040649\n",
      "epoch 1262, loss 0.8700063824653625, R2 0.6169165372848511\n",
      "Eval loss 0.9078712463378906, R2 0.623728334903717\n",
      "epoch 1263, loss 0.8698394894599915, R2 0.6170509457588196\n",
      "Eval loss 0.9076865315437317, R2 0.6238614916801453\n",
      "epoch 1264, loss 0.8696731328964233, R2 0.6171848773956299\n",
      "Eval loss 0.907502293586731, R2 0.6239942908287048\n",
      "epoch 1265, loss 0.8695072531700134, R2 0.6173185110092163\n",
      "Eval loss 0.9073185324668884, R2 0.6241268515586853\n",
      "epoch 1266, loss 0.8693417310714722, R2 0.6174517869949341\n",
      "Eval loss 0.9071353077888489, R2 0.6242589354515076\n",
      "epoch 1267, loss 0.8691767454147339, R2 0.6175847053527832\n",
      "Eval loss 0.906952440738678, R2 0.6243908405303955\n",
      "epoch 1268, loss 0.869012176990509, R2 0.6177173852920532\n",
      "Eval loss 0.9067700505256653, R2 0.6245223879814148\n",
      "epoch 1269, loss 0.8688480257987976, R2 0.6178496479988098\n",
      "Eval loss 0.9065882563591003, R2 0.6246535181999207\n",
      "epoch 1270, loss 0.8686842918395996, R2 0.6179816126823425\n",
      "Eval loss 0.9064068794250488, R2 0.6247844099998474\n",
      "epoch 1271, loss 0.8685212135314941, R2 0.6181132197380066\n",
      "Eval loss 0.9062260389328003, R2 0.6249149441719055\n",
      "epoch 1272, loss 0.8683584928512573, R2 0.618244469165802\n",
      "Eval loss 0.9060454368591309, R2 0.6250452399253845\n",
      "epoch 1273, loss 0.8681961894035339, R2 0.6183753609657288\n",
      "Eval loss 0.9058656096458435, R2 0.6251750588417053\n",
      "epoch 1274, loss 0.8680341839790344, R2 0.618506133556366\n",
      "Eval loss 0.90568608045578, R2 0.6253047585487366\n",
      "epoch 1275, loss 0.8678728938102722, R2 0.6186363697052002\n",
      "Eval loss 0.9055070281028748, R2 0.6254340410232544\n",
      "epoch 1276, loss 0.8677118420600891, R2 0.6187663674354553\n",
      "Eval loss 0.9053284525871277, R2 0.6255629658699036\n",
      "epoch 1277, loss 0.867551326751709, R2 0.618895947933197\n",
      "Eval loss 0.905150294303894, R2 0.6256916522979736\n",
      "epoch 1278, loss 0.8673913478851318, R2 0.6190252900123596\n",
      "Eval loss 0.9049728512763977, R2 0.6258201003074646\n",
      "epoch 1279, loss 0.8672314286231995, R2 0.6191542744636536\n",
      "Eval loss 0.9047955274581909, R2 0.6259480714797974\n",
      "epoch 1280, loss 0.8670722842216492, R2 0.6192829012870789\n",
      "Eval loss 0.9046187996864319, R2 0.626075804233551\n",
      "epoch 1281, loss 0.8669135570526123, R2 0.619411289691925\n",
      "Eval loss 0.904442548751831, R2 0.6262032389640808\n",
      "epoch 1282, loss 0.8667551279067993, R2 0.6195392608642578\n",
      "Eval loss 0.9042667150497437, R2 0.6263303160667419\n",
      "epoch 1283, loss 0.8665972948074341, R2 0.6196669936180115\n",
      "Eval loss 0.9040912389755249, R2 0.626457154750824\n",
      "epoch 1284, loss 0.866439700126648, R2 0.6197943091392517\n",
      "Eval loss 0.9039164781570435, R2 0.6265836358070374\n",
      "epoch 1285, loss 0.8662826418876648, R2 0.6199213862419128\n",
      "Eval loss 0.9037420153617859, R2 0.6267098188400269\n",
      "epoch 1286, loss 0.8661260604858398, R2 0.6200481653213501\n",
      "Eval loss 0.9035680294036865, R2 0.6268357634544373\n",
      "epoch 1287, loss 0.8659698367118835, R2 0.6201745867729187\n",
      "Eval loss 0.9033945202827454, R2 0.6269612908363342\n",
      "epoch 1288, loss 0.8658140897750854, R2 0.6203007102012634\n",
      "Eval loss 0.9032214283943176, R2 0.6270865797996521\n",
      "epoch 1289, loss 0.8656587600708008, R2 0.6204264163970947\n",
      "Eval loss 0.9030487537384033, R2 0.6272115707397461\n",
      "epoch 1290, loss 0.86550372838974, R2 0.6205518841743469\n",
      "Eval loss 0.902876615524292, R2 0.6273362636566162\n",
      "epoch 1291, loss 0.8653492331504822, R2 0.6206769943237305\n",
      "Eval loss 0.9027048945426941, R2 0.6274605989456177\n",
      "epoch 1292, loss 0.8651951551437378, R2 0.6208019256591797\n",
      "Eval loss 0.9025334715843201, R2 0.62758469581604\n",
      "epoch 1293, loss 0.8650414347648621, R2 0.6209264397621155\n",
      "Eval loss 0.902362585067749, R2 0.6277084350585938\n",
      "epoch 1294, loss 0.8648883104324341, R2 0.6210507154464722\n",
      "Eval loss 0.9021921157836914, R2 0.6278318762779236\n",
      "epoch 1295, loss 0.8647354245185852, R2 0.6211745738983154\n",
      "Eval loss 0.9020221829414368, R2 0.6279550194740295\n",
      "epoch 1296, loss 0.864582896232605, R2 0.6212981343269348\n",
      "Eval loss 0.9018526077270508, R2 0.6280779838562012\n",
      "epoch 1297, loss 0.8644309043884277, R2 0.6214213967323303\n",
      "Eval loss 0.9016833901405334, R2 0.6282005906105042\n",
      "epoch 1298, loss 0.8642792701721191, R2 0.6215444207191467\n",
      "Eval loss 0.9015148282051086, R2 0.6283228397369385\n",
      "epoch 1299, loss 0.8641281723976135, R2 0.6216671466827393\n",
      "Eval loss 0.9013465642929077, R2 0.6284449100494385\n",
      "epoch 1300, loss 0.8639774322509766, R2 0.6217895150184631\n",
      "Eval loss 0.901178777217865, R2 0.628566563129425\n",
      "epoch 1301, loss 0.8638270497322083, R2 0.6219115257263184\n",
      "Eval loss 0.9010113477706909, R2 0.6286879181861877\n",
      "epoch 1302, loss 0.8636770844459534, R2 0.6220332980155945\n",
      "Eval loss 0.9008444547653198, R2 0.6288090348243713\n",
      "epoch 1303, loss 0.8635275363922119, R2 0.6221547722816467\n",
      "Eval loss 0.9006779193878174, R2 0.6289299726486206\n",
      "epoch 1304, loss 0.8633782863616943, R2 0.6222759485244751\n",
      "Eval loss 0.9005118012428284, R2 0.6290504932403564\n",
      "epoch 1305, loss 0.8632296919822693, R2 0.6223967671394348\n",
      "Eval loss 0.9003462195396423, R2 0.6291706562042236\n",
      "epoch 1306, loss 0.8630813360214233, R2 0.6225173473358154\n",
      "Eval loss 0.9001809358596802, R2 0.6292906403541565\n",
      "epoch 1307, loss 0.8629333972930908, R2 0.6226375102996826\n",
      "Eval loss 0.9000161290168762, R2 0.6294102668762207\n",
      "epoch 1308, loss 0.8627857565879822, R2 0.6227574944496155\n",
      "Eval loss 0.8998517394065857, R2 0.6295297145843506\n",
      "epoch 1309, loss 0.8626386523246765, R2 0.6228771805763245\n",
      "Eval loss 0.899687647819519, R2 0.6296488046646118\n",
      "epoch 1310, loss 0.8624919056892395, R2 0.6229965090751648\n",
      "Eval loss 0.8995241522789001, R2 0.6297675967216492\n",
      "epoch 1311, loss 0.8623456358909607, R2 0.6231155395507812\n",
      "Eval loss 0.8993609547615051, R2 0.6298861503601074\n",
      "epoch 1312, loss 0.8621997237205505, R2 0.6232343316078186\n",
      "Eval loss 0.8991983532905579, R2 0.6300044655799866\n",
      "epoch 1313, loss 0.862054169178009, R2 0.6233527660369873\n",
      "Eval loss 0.8990360498428345, R2 0.6301224231719971\n",
      "epoch 1314, loss 0.861909031867981, R2 0.6234709024429321\n",
      "Eval loss 0.8988741040229797, R2 0.6302400231361389\n",
      "epoch 1315, loss 0.8617641925811768, R2 0.6235888004302979\n",
      "Eval loss 0.898712694644928, R2 0.6303575038909912\n",
      "epoch 1316, loss 0.8616198301315308, R2 0.6237064003944397\n",
      "Eval loss 0.8985514640808105, R2 0.6304745078086853\n",
      "epoch 1317, loss 0.8614758253097534, R2 0.6238237023353577\n",
      "Eval loss 0.8983910083770752, R2 0.6305913925170898\n",
      "epoch 1318, loss 0.8613322377204895, R2 0.6239407062530518\n",
      "Eval loss 0.8982307314872742, R2 0.6307079792022705\n",
      "epoch 1319, loss 0.8611890077590942, R2 0.624057412147522\n",
      "Eval loss 0.8980709314346313, R2 0.6308242082595825\n",
      "epoch 1320, loss 0.8610461354255676, R2 0.6241738200187683\n",
      "Eval loss 0.8979113698005676, R2 0.6309401988983154\n",
      "epoch 1321, loss 0.8609037399291992, R2 0.6242899298667908\n",
      "Eval loss 0.8977524638175964, R2 0.6310558915138245\n",
      "epoch 1322, loss 0.8607617020606995, R2 0.6244058012962341\n",
      "Eval loss 0.8975939154624939, R2 0.6311714053153992\n",
      "epoch 1323, loss 0.8606200218200684, R2 0.6245213747024536\n",
      "Eval loss 0.8974356651306152, R2 0.6312865614891052\n",
      "epoch 1324, loss 0.8604787588119507, R2 0.6246366500854492\n",
      "Eval loss 0.8972778916358948, R2 0.631401538848877\n",
      "epoch 1325, loss 0.8603377938270569, R2 0.6247516870498657\n",
      "Eval loss 0.8971203565597534, R2 0.6315160393714905\n",
      "epoch 1326, loss 0.8601972460746765, R2 0.6248663663864136\n",
      "Eval loss 0.8969634175300598, R2 0.6316303610801697\n",
      "epoch 1327, loss 0.8600571751594543, R2 0.6249808073043823\n",
      "Eval loss 0.8968068957328796, R2 0.6317445635795593\n",
      "epoch 1328, loss 0.8599174618721008, R2 0.6250948309898376\n",
      "Eval loss 0.8966506719589233, R2 0.6318584084510803\n",
      "epoch 1329, loss 0.8597778677940369, R2 0.6252087950706482\n",
      "Eval loss 0.8964948654174805, R2 0.6319718956947327\n",
      "epoch 1330, loss 0.8596388697624207, R2 0.6253223419189453\n",
      "Eval loss 0.8963394165039062, R2 0.6320851445198059\n",
      "epoch 1331, loss 0.8595003485679626, R2 0.6254356503486633\n",
      "Eval loss 0.8961843848228455, R2 0.6321981549263\n",
      "epoch 1332, loss 0.8593620657920837, R2 0.6255486011505127\n",
      "Eval loss 0.8960298299789429, R2 0.6323108673095703\n",
      "epoch 1333, loss 0.8592240810394287, R2 0.6256613731384277\n",
      "Eval loss 0.8958755135536194, R2 0.6324233412742615\n",
      "epoch 1334, loss 0.8590865731239319, R2 0.6257738471031189\n",
      "Eval loss 0.8957217931747437, R2 0.6325355172157288\n",
      "epoch 1335, loss 0.8589494228363037, R2 0.6258860230445862\n",
      "Eval loss 0.8955683708190918, R2 0.6326474547386169\n",
      "epoch 1336, loss 0.8588125705718994, R2 0.6259979605674744\n",
      "Eval loss 0.895415186882019, R2 0.632759153842926\n",
      "epoch 1337, loss 0.8586761355400085, R2 0.6261095404624939\n",
      "Eval loss 0.895262598991394, R2 0.6328704953193665\n",
      "epoch 1338, loss 0.8585401177406311, R2 0.6262209415435791\n",
      "Eval loss 0.8951102495193481, R2 0.6329817175865173\n",
      "epoch 1339, loss 0.858404278755188, R2 0.6263320446014404\n",
      "Eval loss 0.8949584364891052, R2 0.63309246301651\n",
      "epoch 1340, loss 0.8582690954208374, R2 0.6264428496360779\n",
      "Eval loss 0.8948069214820862, R2 0.6332031488418579\n",
      "epoch 1341, loss 0.8581340312957764, R2 0.6265532970428467\n",
      "Eval loss 0.8946558237075806, R2 0.6333134174346924\n",
      "epoch 1342, loss 0.8579994440078735, R2 0.6266636252403259\n",
      "Eval loss 0.894504964351654, R2 0.6334235668182373\n",
      "epoch 1343, loss 0.8578651547431946, R2 0.6267735958099365\n",
      "Eval loss 0.8943545818328857, R2 0.6335334181785583\n",
      "epoch 1344, loss 0.857731282711029, R2 0.626883327960968\n",
      "Eval loss 0.8942046165466309, R2 0.6336429715156555\n",
      "epoch 1345, loss 0.8575976490974426, R2 0.6269927620887756\n",
      "Eval loss 0.8940550684928894, R2 0.6337522864341736\n",
      "epoch 1346, loss 0.8574644923210144, R2 0.6271019577980042\n",
      "Eval loss 0.8939058780670166, R2 0.6338613033294678\n",
      "epoch 1347, loss 0.8573316931724548, R2 0.6272108554840088\n",
      "Eval loss 0.8937569260597229, R2 0.6339700818061829\n",
      "epoch 1348, loss 0.8571993112564087, R2 0.6273195147514343\n",
      "Eval loss 0.8936086893081665, R2 0.6340786814689636\n",
      "epoch 1349, loss 0.8570671081542969, R2 0.6274279952049255\n",
      "Eval loss 0.8934603929519653, R2 0.6341869235038757\n",
      "epoch 1350, loss 0.8569352626800537, R2 0.6275360584259033\n",
      "Eval loss 0.8933126926422119, R2 0.6342949271202087\n",
      "epoch 1351, loss 0.8568038940429688, R2 0.6276438236236572\n",
      "Eval loss 0.8931652903556824, R2 0.6344026923179626\n",
      "epoch 1352, loss 0.8566728234291077, R2 0.6277514100074768\n",
      "Eval loss 0.8930183053016663, R2 0.6345102787017822\n",
      "epoch 1353, loss 0.8565421104431152, R2 0.6278588175773621\n",
      "Eval loss 0.8928717970848083, R2 0.6346175074577332\n",
      "epoch 1354, loss 0.8564117550849915, R2 0.6279658675193787\n",
      "Eval loss 0.8927254676818848, R2 0.634724497795105\n",
      "epoch 1355, loss 0.8562817573547363, R2 0.6280727386474609\n",
      "Eval loss 0.8925797343254089, R2 0.6348313093185425\n",
      "epoch 1356, loss 0.8561520576477051, R2 0.6281792521476746\n",
      "Eval loss 0.8924341201782227, R2 0.6349378228187561\n",
      "epoch 1357, loss 0.8560227155685425, R2 0.6282854676246643\n",
      "Eval loss 0.8922889828681946, R2 0.6350440382957458\n",
      "epoch 1358, loss 0.8558937311172485, R2 0.6283915042877197\n",
      "Eval loss 0.8921442031860352, R2 0.6351500153541565\n",
      "epoch 1359, loss 0.8557651042938232, R2 0.6284972429275513\n",
      "Eval loss 0.8919998407363892, R2 0.635255753993988\n",
      "epoch 1360, loss 0.8556368350982666, R2 0.6286028027534485\n",
      "Eval loss 0.8918558359146118, R2 0.6353613138198853\n",
      "epoch 1361, loss 0.8555088043212891, R2 0.6287079453468323\n",
      "Eval loss 0.8917120099067688, R2 0.6354665160179138\n",
      "epoch 1362, loss 0.855381190776825, R2 0.6288130283355713\n",
      "Eval loss 0.8915687203407288, R2 0.6355714797973633\n",
      "epoch 1363, loss 0.8552539348602295, R2 0.6289177536964417\n",
      "Eval loss 0.8914257884025574, R2 0.6356763243675232\n",
      "epoch 1364, loss 0.8551269769668579, R2 0.6290221810340881\n",
      "Eval loss 0.8912832140922546, R2 0.6357808113098145\n",
      "epoch 1365, loss 0.855000376701355, R2 0.6291264891624451\n",
      "Eval loss 0.8911409974098206, R2 0.6358851790428162\n",
      "epoch 1366, loss 0.8548741340637207, R2 0.6292304396629333\n",
      "Eval loss 0.8909989595413208, R2 0.6359891295433044\n",
      "epoch 1367, loss 0.8547480702400208, R2 0.6293340921401978\n",
      "Eval loss 0.8908573985099792, R2 0.6360929012298584\n",
      "epoch 1368, loss 0.8546226024627686, R2 0.6294375658035278\n",
      "Eval loss 0.8907162547111511, R2 0.636196494102478\n",
      "epoch 1369, loss 0.8544971346855164, R2 0.6295408010482788\n",
      "Eval loss 0.8905754685401917, R2 0.636299729347229\n",
      "epoch 1370, loss 0.8543723225593567, R2 0.6296437382698059\n",
      "Eval loss 0.8904350399971008, R2 0.6364028453826904\n",
      "epoch 1371, loss 0.8542476892471313, R2 0.6297464966773987\n",
      "Eval loss 0.8902948498725891, R2 0.6365056037902832\n",
      "epoch 1372, loss 0.8541234135627747, R2 0.6298488974571228\n",
      "Eval loss 0.890155017375946, R2 0.6366081833839417\n",
      "epoch 1373, loss 0.8539993762969971, R2 0.6299511194229126\n",
      "Eval loss 0.8900156021118164, R2 0.6367105841636658\n",
      "epoch 1374, loss 0.8538758754730225, R2 0.6300531029701233\n",
      "Eval loss 0.8898766040802002, R2 0.6368126273155212\n",
      "epoch 1375, loss 0.8537524938583374, R2 0.6301548480987549\n",
      "Eval loss 0.8897377252578735, R2 0.6369145512580872\n",
      "epoch 1376, loss 0.8536294102668762, R2 0.6302562952041626\n",
      "Eval loss 0.8895994424819946, R2 0.6370161175727844\n",
      "epoch 1377, loss 0.8535069227218628, R2 0.630357563495636\n",
      "Eval loss 0.8894613981246948, R2 0.6371174454689026\n",
      "epoch 1378, loss 0.8533846139907837, R2 0.6304584741592407\n",
      "Eval loss 0.8893237113952637, R2 0.6372186541557312\n",
      "epoch 1379, loss 0.8532626032829285, R2 0.6305592060089111\n",
      "Eval loss 0.8891862630844116, R2 0.6373195648193359\n",
      "epoch 1380, loss 0.8531408905982971, R2 0.6306596994400024\n",
      "Eval loss 0.8890492916107178, R2 0.6374202370643616\n",
      "epoch 1381, loss 0.8530195355415344, R2 0.6307600140571594\n",
      "Eval loss 0.8889126181602478, R2 0.6375207304954529\n",
      "epoch 1382, loss 0.8528984189033508, R2 0.6308599710464478\n",
      "Eval loss 0.8887763023376465, R2 0.6376208662986755\n",
      "epoch 1383, loss 0.8527777791023254, R2 0.6309597492218018\n",
      "Eval loss 0.888640284538269, R2 0.6377208828926086\n",
      "epoch 1384, loss 0.8526573777198792, R2 0.6310592889785767\n",
      "Eval loss 0.8885046243667603, R2 0.6378205418586731\n",
      "epoch 1385, loss 0.8525372743606567, R2 0.6311585903167725\n",
      "Eval loss 0.8883693218231201, R2 0.6379201412200928\n",
      "epoch 1386, loss 0.8524174094200134, R2 0.6312575936317444\n",
      "Eval loss 0.8882343769073486, R2 0.638019323348999\n",
      "epoch 1387, loss 0.8522981405258179, R2 0.6313562989234924\n",
      "Eval loss 0.8880996108055115, R2 0.6381183862686157\n",
      "epoch 1388, loss 0.8521788716316223, R2 0.6314550042152405\n",
      "Eval loss 0.8879653811454773, R2 0.6382172107696533\n",
      "epoch 1389, loss 0.852060079574585, R2 0.6315532326698303\n",
      "Eval loss 0.8878315091133118, R2 0.638315737247467\n",
      "epoch 1390, loss 0.8519416451454163, R2 0.6316514015197754\n",
      "Eval loss 0.8876978158950806, R2 0.6384141445159912\n",
      "epoch 1391, loss 0.8518235087394714, R2 0.6317492127418518\n",
      "Eval loss 0.887564480304718, R2 0.6385122537612915\n",
      "epoch 1392, loss 0.8517056107521057, R2 0.6318468451499939\n",
      "Eval loss 0.8874314427375793, R2 0.6386101841926575\n",
      "epoch 1393, loss 0.8515879511833191, R2 0.6319441795349121\n",
      "Eval loss 0.8872988224029541, R2 0.6387078762054443\n",
      "epoch 1394, loss 0.8514707088470459, R2 0.632041335105896\n",
      "Eval loss 0.8871665596961975, R2 0.6388053297996521\n",
      "epoch 1395, loss 0.8513537049293518, R2 0.632138192653656\n",
      "Eval loss 0.8870344758033752, R2 0.6389026045799255\n",
      "epoch 1396, loss 0.8512370586395264, R2 0.6322349309921265\n",
      "Eval loss 0.8869028091430664, R2 0.6389995217323303\n",
      "epoch 1397, loss 0.8511207103729248, R2 0.6323313117027283\n",
      "Eval loss 0.8867715001106262, R2 0.6390962600708008\n",
      "epoch 1398, loss 0.8510046005249023, R2 0.6324276328086853\n",
      "Eval loss 0.8866404294967651, R2 0.6391928195953369\n",
      "epoch 1399, loss 0.8508890271186829, R2 0.6325235962867737\n",
      "Eval loss 0.8865097761154175, R2 0.639289140701294\n",
      "epoch 1400, loss 0.8507734537124634, R2 0.6326193809509277\n",
      "Eval loss 0.8863794207572937, R2 0.6393852829933167\n",
      "epoch 1401, loss 0.8506583571434021, R2 0.6327148079872131\n",
      "Eval loss 0.886249303817749, R2 0.6394811272621155\n",
      "epoch 1402, loss 0.8505434989929199, R2 0.6328100562095642\n",
      "Eval loss 0.8861194849014282, R2 0.6395768523216248\n",
      "epoch 1403, loss 0.8504289984703064, R2 0.6329051852226257\n",
      "Eval loss 0.8859902620315552, R2 0.6396722197532654\n",
      "epoch 1404, loss 0.8503147959709167, R2 0.6330000162124634\n",
      "Eval loss 0.8858611583709717, R2 0.6397674083709717\n",
      "epoch 1405, loss 0.8502008318901062, R2 0.6330946087837219\n",
      "Eval loss 0.8857324123382568, R2 0.6398624181747437\n",
      "epoch 1406, loss 0.8500872850418091, R2 0.6331889629364014\n",
      "Eval loss 0.8856038451194763, R2 0.6399572491645813\n",
      "epoch 1407, loss 0.8499739170074463, R2 0.6332831382751465\n",
      "Eval loss 0.885475754737854, R2 0.6400517821311951\n",
      "epoch 1408, loss 0.8498607873916626, R2 0.6333770751953125\n",
      "Eval loss 0.885347843170166, R2 0.6401461958885193\n",
      "epoch 1409, loss 0.8497480750083923, R2 0.6334707736968994\n",
      "Eval loss 0.8852203488349915, R2 0.6402402520179749\n",
      "epoch 1410, loss 0.849635660648346, R2 0.6335642337799072\n",
      "Eval loss 0.8850932121276855, R2 0.6403341293334961\n",
      "epoch 1411, loss 0.8495234251022339, R2 0.6336575150489807\n",
      "Eval loss 0.884966254234314, R2 0.6404277682304382\n",
      "epoch 1412, loss 0.8494115471839905, R2 0.6337505578994751\n",
      "Eval loss 0.8848398327827454, R2 0.6405212879180908\n",
      "epoch 1413, loss 0.8493000864982605, R2 0.6338433027267456\n",
      "Eval loss 0.8847135305404663, R2 0.6406145691871643\n",
      "epoch 1414, loss 0.8491887450218201, R2 0.6339359283447266\n",
      "Eval loss 0.8845875263214111, R2 0.6407075524330139\n",
      "epoch 1415, loss 0.8490777611732483, R2 0.6340282559394836\n",
      "Eval loss 0.8844619393348694, R2 0.640800416469574\n",
      "epoch 1416, loss 0.8489671349525452, R2 0.6341204643249512\n",
      "Eval loss 0.8843366503715515, R2 0.6408930420875549\n",
      "epoch 1417, loss 0.8488566875457764, R2 0.6342123746871948\n",
      "Eval loss 0.8842116594314575, R2 0.6409854292869568\n",
      "epoch 1418, loss 0.8487464785575867, R2 0.6343040466308594\n",
      "Eval loss 0.8840868473052979, R2 0.6410776972770691\n",
      "epoch 1419, loss 0.8486367464065552, R2 0.6343955397605896\n",
      "Eval loss 0.8839623332023621, R2 0.641169548034668\n",
      "epoch 1420, loss 0.848527193069458, R2 0.6344868540763855\n",
      "Eval loss 0.8838382363319397, R2 0.6412612795829773\n",
      "epoch 1421, loss 0.8484178781509399, R2 0.6345779299736023\n",
      "Eval loss 0.883714497089386, R2 0.6413528919219971\n",
      "epoch 1422, loss 0.8483089208602905, R2 0.6346687078475952\n",
      "Eval loss 0.8835909962654114, R2 0.6414442658424377\n",
      "epoch 1423, loss 0.8482002019882202, R2 0.6347593665122986\n",
      "Eval loss 0.8834678530693054, R2 0.6415354013442993\n",
      "epoch 1424, loss 0.8480917811393738, R2 0.6348497271537781\n",
      "Eval loss 0.8833450078964233, R2 0.6416263580322266\n",
      "epoch 1425, loss 0.8479836583137512, R2 0.6349399089813232\n",
      "Eval loss 0.8832222819328308, R2 0.6417170166969299\n",
      "epoch 1426, loss 0.8478757739067078, R2 0.6350298523902893\n",
      "Eval loss 0.883100152015686, R2 0.6418075561523438\n",
      "epoch 1427, loss 0.8477683067321777, R2 0.635119616985321\n",
      "Eval loss 0.8829781413078308, R2 0.6418979167938232\n",
      "epoch 1428, loss 0.8476608395576477, R2 0.6352091431617737\n",
      "Eval loss 0.8828563690185547, R2 0.6419879198074341\n",
      "epoch 1429, loss 0.8475539684295654, R2 0.6352983713150024\n",
      "Eval loss 0.8827348947525024, R2 0.6420778036117554\n",
      "epoch 1430, loss 0.8474471569061279, R2 0.6353874802589417\n",
      "Eval loss 0.8826138973236084, R2 0.6421675682067871\n",
      "epoch 1431, loss 0.8473407030105591, R2 0.6354764699935913\n",
      "Eval loss 0.8824930787086487, R2 0.6422569751739502\n",
      "epoch 1432, loss 0.8472345471382141, R2 0.6355652213096619\n",
      "Eval loss 0.8823724985122681, R2 0.6423462629318237\n",
      "epoch 1433, loss 0.8471285700798035, R2 0.6356536746025085\n",
      "Eval loss 0.8822523355484009, R2 0.6424353122711182\n",
      "epoch 1434, loss 0.8470228910446167, R2 0.6357418298721313\n",
      "Eval loss 0.882132351398468, R2 0.642524242401123\n",
      "epoch 1435, loss 0.8469176292419434, R2 0.6358299851417542\n",
      "Eval loss 0.8820127844810486, R2 0.642612874507904\n",
      "epoch 1436, loss 0.8468124866485596, R2 0.6359178423881531\n",
      "Eval loss 0.8818934559822083, R2 0.6427013278007507\n",
      "epoch 1437, loss 0.8467076420783997, R2 0.6360054016113281\n",
      "Eval loss 0.8817743062973022, R2 0.6427896022796631\n",
      "epoch 1438, loss 0.846603274345398, R2 0.6360928416252136\n",
      "Eval loss 0.8816556334495544, R2 0.6428776383399963\n",
      "epoch 1439, loss 0.8464989066123962, R2 0.6361801028251648\n",
      "Eval loss 0.8815370798110962, R2 0.64296555519104\n",
      "epoch 1440, loss 0.8463948965072632, R2 0.6362671256065369\n",
      "Eval loss 0.8814189434051514, R2 0.6430531144142151\n",
      "epoch 1441, loss 0.8462911248207092, R2 0.6363539695739746\n",
      "Eval loss 0.8813010454177856, R2 0.6431406140327454\n",
      "epoch 1442, loss 0.8461876511573792, R2 0.6364405751228333\n",
      "Eval loss 0.8811834454536438, R2 0.6432279348373413\n",
      "epoch 1443, loss 0.8460844159126282, R2 0.6365270018577576\n",
      "Eval loss 0.8810661435127258, R2 0.6433148980140686\n",
      "epoch 1444, loss 0.8459815382957458, R2 0.6366132497787476\n",
      "Eval loss 0.8809490203857422, R2 0.6434018015861511\n",
      "epoch 1445, loss 0.8458788394927979, R2 0.6366991400718689\n",
      "Eval loss 0.8808323740959167, R2 0.6434884667396545\n",
      "epoch 1446, loss 0.845776379108429, R2 0.6367849707603455\n",
      "Eval loss 0.8807159066200256, R2 0.6435749530792236\n",
      "epoch 1447, loss 0.8456742763519287, R2 0.6368705630302429\n",
      "Eval loss 0.8805996179580688, R2 0.6436612010002136\n",
      "epoch 1448, loss 0.8455724716186523, R2 0.636955976486206\n",
      "Eval loss 0.8804837465286255, R2 0.6437472701072693\n",
      "epoch 1449, loss 0.8454707860946655, R2 0.6370411515235901\n",
      "Eval loss 0.880368173122406, R2 0.6438331604003906\n",
      "epoch 1450, loss 0.8453693389892578, R2 0.6371261477470398\n",
      "Eval loss 0.8802527785301208, R2 0.6439188718795776\n",
      "epoch 1451, loss 0.8452683687210083, R2 0.6372109055519104\n",
      "Eval loss 0.8801378011703491, R2 0.6440043449401855\n",
      "epoch 1452, loss 0.8451676368713379, R2 0.6372954845428467\n",
      "Eval loss 0.8800228834152222, R2 0.6440896391868591\n",
      "epoch 1453, loss 0.8450669646263123, R2 0.6373798847198486\n",
      "Eval loss 0.8799083828926086, R2 0.6441747546195984\n",
      "epoch 1454, loss 0.8449665904045105, R2 0.6374640464782715\n",
      "Eval loss 0.8797942399978638, R2 0.6442596912384033\n",
      "epoch 1455, loss 0.8448666334152222, R2 0.6375480890274048\n",
      "Eval loss 0.8796802163124084, R2 0.6443443894386292\n",
      "epoch 1456, loss 0.8447666764259338, R2 0.6376318335533142\n",
      "Eval loss 0.8795666098594666, R2 0.6444289684295654\n",
      "epoch 1457, loss 0.8446671962738037, R2 0.6377153992652893\n",
      "Eval loss 0.8794532418251038, R2 0.6445131897926331\n",
      "epoch 1458, loss 0.8445678949356079, R2 0.6377987861633301\n",
      "Eval loss 0.8793399930000305, R2 0.6445974111557007\n",
      "epoch 1459, loss 0.8444688320159912, R2 0.6378819942474365\n",
      "Eval loss 0.8792271018028259, R2 0.6446813941001892\n",
      "epoch 1460, loss 0.8443701267242432, R2 0.6379649639129639\n",
      "Eval loss 0.8791145086288452, R2 0.6447650790214539\n",
      "epoch 1461, loss 0.8442714810371399, R2 0.6380478739738464\n",
      "Eval loss 0.8790023326873779, R2 0.644848644733429\n",
      "epoch 1462, loss 0.8441733121871948, R2 0.6381304264068604\n",
      "Eval loss 0.8788902759552002, R2 0.6449320316314697\n",
      "epoch 1463, loss 0.8440752625465393, R2 0.6382129192352295\n",
      "Eval loss 0.8787783980369568, R2 0.6450152397155762\n",
      "epoch 1464, loss 0.8439774513244629, R2 0.6382951736450195\n",
      "Eval loss 0.878666877746582, R2 0.6450982689857483\n",
      "epoch 1465, loss 0.8438799977302551, R2 0.6383771300315857\n",
      "Eval loss 0.8785555958747864, R2 0.6451810598373413\n",
      "epoch 1466, loss 0.8437826633453369, R2 0.6384589672088623\n",
      "Eval loss 0.8784447908401489, R2 0.645263671875\n",
      "epoch 1467, loss 0.8436857461929321, R2 0.6385406255722046\n",
      "Eval loss 0.8783339858055115, R2 0.6453461647033691\n",
      "epoch 1468, loss 0.8435889482498169, R2 0.6386221051216125\n",
      "Eval loss 0.8782235383987427, R2 0.6454284191131592\n",
      "epoch 1469, loss 0.8434923887252808, R2 0.638703465461731\n",
      "Eval loss 0.8781134486198425, R2 0.6455104947090149\n",
      "epoch 1470, loss 0.8433961272239685, R2 0.6387844085693359\n",
      "Eval loss 0.8780034780502319, R2 0.6455923914909363\n",
      "epoch 1471, loss 0.8433000445365906, R2 0.6388653516769409\n",
      "Eval loss 0.8778938055038452, R2 0.6456741094589233\n",
      "epoch 1472, loss 0.8432043790817261, R2 0.638945996761322\n",
      "Eval loss 0.8777844309806824, R2 0.6457557082176208\n",
      "epoch 1473, loss 0.8431087732315063, R2 0.6390265822410583\n",
      "Eval loss 0.8776752352714539, R2 0.6458369493484497\n",
      "epoch 1474, loss 0.8430134654045105, R2 0.639106810092926\n",
      "Eval loss 0.877566397190094, R2 0.6459181308746338\n",
      "epoch 1475, loss 0.8429185152053833, R2 0.6391869187355042\n",
      "Eval loss 0.8774579167366028, R2 0.6459991335868835\n",
      "epoch 1476, loss 0.8428236246109009, R2 0.6392669677734375\n",
      "Eval loss 0.8773496150970459, R2 0.646079957485199\n",
      "epoch 1477, loss 0.8427290320396423, R2 0.639346718788147\n",
      "Eval loss 0.8772414922714233, R2 0.6461605429649353\n",
      "epoch 1478, loss 0.8426347374916077, R2 0.6394262909889221\n",
      "Eval loss 0.8771337270736694, R2 0.6462409496307373\n",
      "epoch 1479, loss 0.8425406813621521, R2 0.6395056843757629\n",
      "Eval loss 0.8770260810852051, R2 0.6463212370872498\n",
      "epoch 1480, loss 0.8424468040466309, R2 0.6395848989486694\n",
      "Eval loss 0.8769188523292542, R2 0.6464012265205383\n",
      "epoch 1481, loss 0.8423531651496887, R2 0.6396638751029968\n",
      "Eval loss 0.876811683177948, R2 0.6464810967445374\n",
      "epoch 1482, loss 0.8422597646713257, R2 0.6397426724433899\n",
      "Eval loss 0.8767048716545105, R2 0.6465608477592468\n",
      "epoch 1483, loss 0.8421667218208313, R2 0.6398213505744934\n",
      "Eval loss 0.8765983581542969, R2 0.6466403603553772\n",
      "epoch 1484, loss 0.8420737981796265, R2 0.6398998498916626\n",
      "Eval loss 0.8764922022819519, R2 0.646719753742218\n",
      "epoch 1485, loss 0.8419811129570007, R2 0.6399779915809631\n",
      "Eval loss 0.8763861656188965, R2 0.646798849105835\n",
      "epoch 1486, loss 0.8418886661529541, R2 0.6400561928749084\n",
      "Eval loss 0.8762803673744202, R2 0.6468778848648071\n",
      "epoch 1487, loss 0.8417965173721313, R2 0.6401341557502747\n",
      "Eval loss 0.8761748671531677, R2 0.6469566822052002\n",
      "epoch 1488, loss 0.8417046070098877, R2 0.640211820602417\n",
      "Eval loss 0.8760696053504944, R2 0.6470353603363037\n",
      "epoch 1489, loss 0.8416128158569336, R2 0.6402893662452698\n",
      "Eval loss 0.8759644627571106, R2 0.6471138000488281\n",
      "epoch 1490, loss 0.8415213227272034, R2 0.640366792678833\n",
      "Eval loss 0.875859797000885, R2 0.647192120552063\n",
      "epoch 1491, loss 0.8414300680160522, R2 0.6404439210891724\n",
      "Eval loss 0.8757553100585938, R2 0.6472702026367188\n",
      "epoch 1492, loss 0.841339111328125, R2 0.6405209898948669\n",
      "Eval loss 0.8756511211395264, R2 0.6473482251167297\n",
      "epoch 1493, loss 0.8412482142448425, R2 0.6405978798866272\n",
      "Eval loss 0.8755470514297485, R2 0.6474259495735168\n",
      "epoch 1494, loss 0.8411577343940735, R2 0.6406745314598083\n",
      "Eval loss 0.875443160533905, R2 0.6475036144256592\n",
      "epoch 1495, loss 0.8410674929618835, R2 0.6407509446144104\n",
      "Eval loss 0.8753397464752197, R2 0.6475809812545776\n",
      "epoch 1496, loss 0.8409772515296936, R2 0.6408272981643677\n",
      "Eval loss 0.8752365112304688, R2 0.6476582288742065\n",
      "epoch 1497, loss 0.8408875465393066, R2 0.6409034132957458\n",
      "Eval loss 0.8751334547996521, R2 0.6477352380752563\n",
      "epoch 1498, loss 0.8407977819442749, R2 0.6409792900085449\n",
      "Eval loss 0.8750306367874146, R2 0.6478121280670166\n",
      "epoch 1499, loss 0.8407084345817566, R2 0.6410550475120544\n",
      "Eval loss 0.8749281167984009, R2 0.6478888988494873\n",
      "epoch 1500, loss 0.8406192064285278, R2 0.6411306858062744\n",
      "Eval loss 0.8748258352279663, R2 0.6479654908180237\n",
      "epoch 1501, loss 0.8405301570892334, R2 0.6412061452865601\n",
      "Eval loss 0.8747237324714661, R2 0.6480419039726257\n",
      "epoch 1502, loss 0.8404414057731628, R2 0.6412813663482666\n",
      "Eval loss 0.8746219873428345, R2 0.6481180787086487\n",
      "epoch 1503, loss 0.8403530120849609, R2 0.6413564682006836\n",
      "Eval loss 0.8745204210281372, R2 0.6481941342353821\n",
      "epoch 1504, loss 0.8402647376060486, R2 0.6414313912391663\n",
      "Eval loss 0.8744190335273743, R2 0.6482700705528259\n",
      "epoch 1505, loss 0.8401765823364258, R2 0.6415061354637146\n",
      "Eval loss 0.8743179440498352, R2 0.6483457088470459\n",
      "epoch 1506, loss 0.8400887846946716, R2 0.6415806412696838\n",
      "Eval loss 0.87421715259552, R2 0.6484213471412659\n",
      "epoch 1507, loss 0.8400011658668518, R2 0.6416550278663635\n",
      "Eval loss 0.8741164803504944, R2 0.648496687412262\n",
      "epoch 1508, loss 0.8399137854576111, R2 0.6417292952537537\n",
      "Eval loss 0.8740160465240479, R2 0.6485719084739685\n",
      "epoch 1509, loss 0.8398265242576599, R2 0.6418032050132751\n",
      "Eval loss 0.8739160299301147, R2 0.6486469507217407\n",
      "epoch 1510, loss 0.8397396206855774, R2 0.6418771743774414\n",
      "Eval loss 0.8738160729408264, R2 0.6487218737602234\n",
      "epoch 1511, loss 0.8396528959274292, R2 0.6419508457183838\n",
      "Eval loss 0.8737163543701172, R2 0.6487966179847717\n",
      "epoch 1512, loss 0.8395662903785706, R2 0.6420243978500366\n",
      "Eval loss 0.8736169934272766, R2 0.6488710641860962\n",
      "epoch 1513, loss 0.8394800424575806, R2 0.6420976519584656\n",
      "Eval loss 0.8735177516937256, R2 0.6489455699920654\n",
      "epoch 1514, loss 0.8393939733505249, R2 0.6421709060668945\n",
      "Eval loss 0.8734187483787537, R2 0.6490196585655212\n",
      "epoch 1515, loss 0.8393080830574036, R2 0.6422439217567444\n",
      "Eval loss 0.8733199834823608, R2 0.6490938663482666\n",
      "epoch 1516, loss 0.8392223715782166, R2 0.6423167586326599\n",
      "Eval loss 0.8732215762138367, R2 0.6491675972938538\n",
      "epoch 1517, loss 0.8391368985176086, R2 0.6423894762992859\n",
      "Eval loss 0.8731232285499573, R2 0.6492414474487305\n",
      "epoch 1518, loss 0.8390518426895142, R2 0.6424619555473328\n",
      "Eval loss 0.8730252385139465, R2 0.6493149399757385\n",
      "epoch 1519, loss 0.8389667868614197, R2 0.6425343155860901\n",
      "Eval loss 0.8729275465011597, R2 0.649388313293457\n",
      "epoch 1520, loss 0.8388819098472595, R2 0.6426064968109131\n",
      "Eval loss 0.8728299140930176, R2 0.6494616270065308\n",
      "epoch 1521, loss 0.838797390460968, R2 0.6426784992218018\n",
      "Eval loss 0.8727325797080994, R2 0.6495346426963806\n",
      "epoch 1522, loss 0.8387130498886108, R2 0.6427503228187561\n",
      "Eval loss 0.8726354837417603, R2 0.6496075987815857\n",
      "epoch 1523, loss 0.8386287093162537, R2 0.6428220868110657\n",
      "Eval loss 0.8725386261940002, R2 0.6496803760528564\n",
      "epoch 1524, loss 0.8385449051856995, R2 0.6428936123847961\n",
      "Eval loss 0.8724419474601746, R2 0.6497529149055481\n",
      "epoch 1525, loss 0.8384611010551453, R2 0.6429648995399475\n",
      "Eval loss 0.872345507144928, R2 0.6498253345489502\n",
      "epoch 1526, loss 0.8383776545524597, R2 0.6430361866950989\n",
      "Eval loss 0.8722493052482605, R2 0.6498976349830627\n",
      "epoch 1527, loss 0.8382943868637085, R2 0.6431071758270264\n",
      "Eval loss 0.8721534013748169, R2 0.6499696969985962\n",
      "epoch 1528, loss 0.8382112979888916, R2 0.6431781053543091\n",
      "Eval loss 0.8720576167106628, R2 0.6500416398048401\n",
      "epoch 1529, loss 0.838128387928009, R2 0.6432487368583679\n",
      "Eval loss 0.8719620704650879, R2 0.6501135230064392\n",
      "epoch 1530, loss 0.8380456566810608, R2 0.6433192491531372\n",
      "Eval loss 0.8718668222427368, R2 0.6501851677894592\n",
      "epoch 1531, loss 0.8379632234573364, R2 0.6433896422386169\n",
      "Eval loss 0.8717716336250305, R2 0.6502565741539001\n",
      "epoch 1532, loss 0.8378808498382568, R2 0.6434599161148071\n",
      "Eval loss 0.8716768622398376, R2 0.6503279209136963\n",
      "epoch 1533, loss 0.8377987146377563, R2 0.6435298919677734\n",
      "Eval loss 0.8715822100639343, R2 0.6503991484642029\n",
      "epoch 1534, loss 0.8377169966697693, R2 0.643599808216095\n",
      "Eval loss 0.8714879155158997, R2 0.6504701375961304\n",
      "epoch 1535, loss 0.8376352787017822, R2 0.643669605255127\n",
      "Eval loss 0.871393620967865, R2 0.6505410075187683\n",
      "epoch 1536, loss 0.8375537991523743, R2 0.6437391638755798\n",
      "Eval loss 0.8712996244430542, R2 0.6506116390228271\n",
      "epoch 1537, loss 0.8374724984169006, R2 0.6438086032867432\n",
      "Eval loss 0.8712059855461121, R2 0.6506822109222412\n",
      "epoch 1538, loss 0.8373914957046509, R2 0.6438778638839722\n",
      "Eval loss 0.8711124062538147, R2 0.650752604007721\n",
      "epoch 1539, loss 0.8373107314109802, R2 0.6439470052719116\n",
      "Eval loss 0.8710190653800964, R2 0.6508228182792664\n",
      "epoch 1540, loss 0.8372300267219543, R2 0.644015908241272\n",
      "Eval loss 0.8709259629249573, R2 0.650892972946167\n",
      "epoch 1541, loss 0.8371496200561523, R2 0.6440846920013428\n",
      "Eval loss 0.8708330988883972, R2 0.6509628295898438\n",
      "epoch 1542, loss 0.8370693325996399, R2 0.6441532969474792\n",
      "Eval loss 0.8707404136657715, R2 0.6510326266288757\n",
      "epoch 1543, loss 0.8369893431663513, R2 0.6442217826843262\n",
      "Eval loss 0.8706479668617249, R2 0.6511022448539734\n",
      "epoch 1544, loss 0.8369094729423523, R2 0.6442901492118835\n",
      "Eval loss 0.8705557584762573, R2 0.6511717438697815\n",
      "epoch 1545, loss 0.8368298411369324, R2 0.6443582773208618\n",
      "Eval loss 0.8704637885093689, R2 0.6512410640716553\n",
      "epoch 1546, loss 0.836750328540802, R2 0.6444262862205505\n",
      "Eval loss 0.8703719973564148, R2 0.6513102054595947\n",
      "epoch 1547, loss 0.8366711735725403, R2 0.6444941163063049\n",
      "Eval loss 0.870280385017395, R2 0.6513791680335999\n",
      "epoch 1548, loss 0.8365919589996338, R2 0.6445618271827698\n",
      "Eval loss 0.8701890110969543, R2 0.6514480710029602\n",
      "epoch 1549, loss 0.8365132212638855, R2 0.6446294784545898\n",
      "Eval loss 0.8700979351997375, R2 0.6515167355537415\n",
      "epoch 1550, loss 0.836434543132782, R2 0.6446967720985413\n",
      "Eval loss 0.8700069189071655, R2 0.6515853404998779\n",
      "epoch 1551, loss 0.8363561034202576, R2 0.6447640657424927\n",
      "Eval loss 0.8699162006378174, R2 0.6516537666320801\n",
      "epoch 1552, loss 0.8362779021263123, R2 0.6448310613632202\n",
      "Eval loss 0.8698257207870483, R2 0.6517219543457031\n",
      "epoch 1553, loss 0.8361997604370117, R2 0.644897997379303\n",
      "Eval loss 0.8697354197502136, R2 0.6517900824546814\n",
      "epoch 1554, loss 0.8361219167709351, R2 0.6449647545814514\n",
      "Eval loss 0.8696452379226685, R2 0.6518580913543701\n",
      "epoch 1555, loss 0.836044192314148, R2 0.6450315117835999\n",
      "Eval loss 0.8695554137229919, R2 0.6519258618354797\n",
      "epoch 1556, loss 0.8359666466712952, R2 0.6450979113578796\n",
      "Eval loss 0.8694658279418945, R2 0.6519935131072998\n",
      "epoch 1557, loss 0.8358893990516663, R2 0.6451641917228699\n",
      "Eval loss 0.8693762421607971, R2 0.6520611047744751\n",
      "epoch 1558, loss 0.8358123302459717, R2 0.6452304720878601\n",
      "Eval loss 0.8692870140075684, R2 0.6521284580230713\n",
      "epoch 1559, loss 0.8357354402542114, R2 0.6452964544296265\n",
      "Eval loss 0.8691979646682739, R2 0.6521956920623779\n",
      "epoch 1560, loss 0.8356587290763855, R2 0.6453623175621033\n",
      "Eval loss 0.869109034538269, R2 0.6522627472877502\n",
      "epoch 1561, loss 0.8355821967124939, R2 0.6454280614852905\n",
      "Eval loss 0.869020402431488, R2 0.6523297429084778\n",
      "epoch 1562, loss 0.8355058431625366, R2 0.6454936265945435\n",
      "Eval loss 0.8689320087432861, R2 0.6523965001106262\n",
      "epoch 1563, loss 0.8354296684265137, R2 0.6455590724945068\n",
      "Eval loss 0.8688437342643738, R2 0.6524631977081299\n",
      "epoch 1564, loss 0.835353672504425, R2 0.6456243395805359\n",
      "Eval loss 0.8687557578086853, R2 0.6525296568870544\n",
      "epoch 1565, loss 0.8352778553962708, R2 0.6456894874572754\n",
      "Eval loss 0.8686679601669312, R2 0.6525959372520447\n",
      "epoch 1566, loss 0.8352023959159851, R2 0.6457543969154358\n",
      "Eval loss 0.8685802817344666, R2 0.6526622176170349\n",
      "epoch 1567, loss 0.8351269364356995, R2 0.6458193063735962\n",
      "Eval loss 0.8684929013252258, R2 0.652728259563446\n",
      "epoch 1568, loss 0.8350517749786377, R2 0.6458839774131775\n",
      "Eval loss 0.8684056401252747, R2 0.6527941226959229\n",
      "epoch 1569, loss 0.8349767327308655, R2 0.6459484696388245\n",
      "Eval loss 0.8683187365531921, R2 0.6528599262237549\n",
      "epoch 1570, loss 0.8349019289016724, R2 0.6460129618644714\n",
      "Eval loss 0.8682318925857544, R2 0.6529255509376526\n",
      "epoch 1571, loss 0.8348272442817688, R2 0.6460772156715393\n",
      "Eval loss 0.868145227432251, R2 0.6529910564422607\n",
      "epoch 1572, loss 0.8347527384757996, R2 0.6461412906646729\n",
      "Eval loss 0.8680588006973267, R2 0.6530564427375793\n",
      "epoch 1573, loss 0.8346784710884094, R2 0.6462052464485168\n",
      "Eval loss 0.8679726719856262, R2 0.6531215906143188\n",
      "epoch 1574, loss 0.8346043825149536, R2 0.6462691426277161\n",
      "Eval loss 0.8678865432739258, R2 0.6531867384910583\n",
      "epoch 1575, loss 0.8345304727554321, R2 0.6463328003883362\n",
      "Eval loss 0.867800772190094, R2 0.6532516479492188\n",
      "epoch 1576, loss 0.8344566822052002, R2 0.646396279335022\n",
      "Eval loss 0.8677151203155518, R2 0.6533164381980896\n",
      "epoch 1577, loss 0.8343830704689026, R2 0.646459698677063\n",
      "Eval loss 0.8676295876502991, R2 0.6533811092376709\n",
      "epoch 1578, loss 0.8343098163604736, R2 0.6465229392051697\n",
      "Eval loss 0.8675444722175598, R2 0.6534456014633179\n",
      "epoch 1579, loss 0.8342365026473999, R2 0.646586000919342\n",
      "Eval loss 0.8674595355987549, R2 0.6535099744796753\n",
      "epoch 1580, loss 0.8341635465621948, R2 0.6466490030288696\n",
      "Eval loss 0.8673746585845947, R2 0.6535742282867432\n",
      "epoch 1581, loss 0.8340907096862793, R2 0.6467118859291077\n",
      "Eval loss 0.8672899603843689, R2 0.6536383032798767\n",
      "epoch 1582, loss 0.8340180516242981, R2 0.6467745304107666\n",
      "Eval loss 0.8672054409980774, R2 0.6537022590637207\n",
      "epoch 1583, loss 0.8339455723762512, R2 0.646837055683136\n",
      "Eval loss 0.8671212792396545, R2 0.6537660360336304\n",
      "epoch 1584, loss 0.8338733315467834, R2 0.6468994617462158\n",
      "Eval loss 0.867037296295166, R2 0.6538297533988953\n",
      "epoch 1585, loss 0.8338011503219604, R2 0.6469616889953613\n",
      "Eval loss 0.8669533133506775, R2 0.6538932919502258\n",
      "epoch 1586, loss 0.8337292075157166, R2 0.6470238566398621\n",
      "Eval loss 0.8668696880340576, R2 0.6539566516876221\n",
      "epoch 1587, loss 0.8336575031280518, R2 0.6470857858657837\n",
      "Eval loss 0.8667862415313721, R2 0.6540199518203735\n",
      "epoch 1588, loss 0.8335858583450317, R2 0.6471476554870605\n",
      "Eval loss 0.8667029738426208, R2 0.6540830731391907\n",
      "epoch 1589, loss 0.833514392375946, R2 0.6472093462944031\n",
      "Eval loss 0.8666198253631592, R2 0.654146134853363\n",
      "epoch 1590, loss 0.8334431648254395, R2 0.6472709774971008\n",
      "Eval loss 0.8665369153022766, R2 0.6542089581489563\n",
      "epoch 1591, loss 0.833372175693512, R2 0.6473323702812195\n",
      "Eval loss 0.8664543032646179, R2 0.65427166223526\n",
      "epoch 1592, loss 0.8333012461662292, R2 0.6473936438560486\n",
      "Eval loss 0.8663716316223145, R2 0.654334306716919\n",
      "epoch 1593, loss 0.8332306742668152, R2 0.6474547982215881\n",
      "Eval loss 0.8662894368171692, R2 0.6543967723846436\n",
      "epoch 1594, loss 0.8331599831581116, R2 0.6475158333778381\n",
      "Eval loss 0.8662071824073792, R2 0.6544589996337891\n",
      "epoch 1595, loss 0.8330896496772766, R2 0.6475766897201538\n",
      "Eval loss 0.8661251664161682, R2 0.6545212268829346\n",
      "epoch 1596, loss 0.8330193758010864, R2 0.6476374268531799\n",
      "Eval loss 0.8660435080528259, R2 0.654583215713501\n",
      "epoch 1597, loss 0.8329493999481201, R2 0.6476979851722717\n",
      "Eval loss 0.8659618496894836, R2 0.6546452045440674\n",
      "epoch 1598, loss 0.8328796029090881, R2 0.6477585434913635\n",
      "Eval loss 0.8658804893493652, R2 0.6547068953514099\n",
      "epoch 1599, loss 0.8328098654747009, R2 0.647818922996521\n",
      "Eval loss 0.8657993674278259, R2 0.6547685861587524\n",
      "epoch 1600, loss 0.8327404260635376, R2 0.6478790640830994\n",
      "Eval loss 0.8657182455062866, R2 0.6548301577568054\n",
      "epoch 1601, loss 0.8326709866523743, R2 0.647939145565033\n",
      "Eval loss 0.8656373620033264, R2 0.6548915505409241\n",
      "epoch 1602, loss 0.8326018452644348, R2 0.6479990482330322\n",
      "Eval loss 0.8655567169189453, R2 0.6549528241157532\n",
      "epoch 1603, loss 0.8325328230857849, R2 0.6480588316917419\n",
      "Eval loss 0.8654761910438538, R2 0.655013918876648\n",
      "epoch 1604, loss 0.8324639797210693, R2 0.6481184959411621\n",
      "Eval loss 0.8653959631919861, R2 0.655074954032898\n",
      "epoch 1605, loss 0.8323954343795776, R2 0.6481780409812927\n",
      "Eval loss 0.865315854549408, R2 0.6551357507705688\n",
      "epoch 1606, loss 0.8323267698287964, R2 0.648237407207489\n",
      "Eval loss 0.8652358651161194, R2 0.6551965475082397\n",
      "epoch 1607, loss 0.8322585821151733, R2 0.6482966542243958\n",
      "Eval loss 0.8651560544967651, R2 0.6552570462226868\n",
      "epoch 1608, loss 0.8321903347969055, R2 0.6483559012413025\n",
      "Eval loss 0.8650765419006348, R2 0.6553176045417786\n",
      "epoch 1609, loss 0.8321223258972168, R2 0.6484148502349854\n",
      "Eval loss 0.864997148513794, R2 0.6553779244422913\n",
      "epoch 1610, loss 0.8320544958114624, R2 0.6484737992286682\n",
      "Eval loss 0.8649179935455322, R2 0.6554380655288696\n",
      "epoch 1611, loss 0.8319868445396423, R2 0.6485324501991272\n",
      "Eval loss 0.8648390173912048, R2 0.6554982662200928\n",
      "epoch 1612, loss 0.8319194912910461, R2 0.6485911011695862\n",
      "Eval loss 0.8647601008415222, R2 0.655558168888092\n",
      "epoch 1613, loss 0.8318520784378052, R2 0.6486495733261108\n",
      "Eval loss 0.8646814823150635, R2 0.6556180119514465\n",
      "epoch 1614, loss 0.8317849040031433, R2 0.648707926273346\n",
      "Eval loss 0.8646029829978943, R2 0.6556777358055115\n",
      "epoch 1615, loss 0.831717848777771, R2 0.6487661004066467\n",
      "Eval loss 0.8645246624946594, R2 0.6557373404502869\n",
      "epoch 1616, loss 0.8316510915756226, R2 0.6488242149353027\n",
      "Eval loss 0.8644465804100037, R2 0.6557967662811279\n",
      "epoch 1617, loss 0.8315844535827637, R2 0.6488822102546692\n",
      "Eval loss 0.8643686175346375, R2 0.6558561325073242\n",
      "epoch 1618, loss 0.83151775598526, R2 0.6489400267601013\n",
      "Eval loss 0.8642908334732056, R2 0.6559153199195862\n",
      "epoch 1619, loss 0.8314515352249146, R2 0.6489977240562439\n",
      "Eval loss 0.864213228225708, R2 0.6559744477272034\n",
      "epoch 1620, loss 0.8313853144645691, R2 0.6490553617477417\n",
      "Eval loss 0.8641357421875, R2 0.6560332775115967\n",
      "epoch 1621, loss 0.831319272518158, R2 0.6491127610206604\n",
      "Eval loss 0.8640586137771606, R2 0.6560922265052795\n",
      "epoch 1622, loss 0.8312533497810364, R2 0.6491700410842896\n",
      "Eval loss 0.8639814257621765, R2 0.6561508178710938\n",
      "epoch 1623, loss 0.8311876654624939, R2 0.6492272019386292\n",
      "Eval loss 0.8639046549797058, R2 0.656209409236908\n",
      "epoch 1624, loss 0.8311221599578857, R2 0.649284303188324\n",
      "Eval loss 0.8638279438018799, R2 0.6562678813934326\n",
      "epoch 1625, loss 0.8310567140579224, R2 0.6493412256240845\n",
      "Eval loss 0.8637513518333435, R2 0.6563262343406677\n",
      "epoch 1626, loss 0.8309915065765381, R2 0.6493980884552002\n",
      "Eval loss 0.8636748790740967, R2 0.6563843488693237\n",
      "epoch 1627, loss 0.8309264779090881, R2 0.6494548320770264\n",
      "Eval loss 0.8635987043380737, R2 0.6564424633979797\n",
      "epoch 1628, loss 0.830861508846283, R2 0.6495113372802734\n",
      "Eval loss 0.8635227084159851, R2 0.6565003991127014\n",
      "epoch 1629, loss 0.8307967185974121, R2 0.6495678424835205\n",
      "Eval loss 0.863446831703186, R2 0.6565582752227783\n",
      "epoch 1630, loss 0.8307320475578308, R2 0.6496241092681885\n",
      "Eval loss 0.8633710741996765, R2 0.6566159129142761\n",
      "epoch 1631, loss 0.8306676745414734, R2 0.6496803760528564\n",
      "Eval loss 0.8632956147193909, R2 0.6566734910011292\n",
      "epoch 1632, loss 0.8306033611297607, R2 0.6497364640235901\n",
      "Eval loss 0.8632203340530396, R2 0.6567310094833374\n",
      "epoch 1633, loss 0.8305392265319824, R2 0.6497923135757446\n",
      "Eval loss 0.8631450533866882, R2 0.6567882895469666\n",
      "epoch 1634, loss 0.8304753303527832, R2 0.6498481631278992\n",
      "Eval loss 0.8630700707435608, R2 0.6568455100059509\n",
      "epoch 1635, loss 0.8304113745689392, R2 0.6499038338661194\n",
      "Eval loss 0.8629952073097229, R2 0.6569026708602905\n",
      "epoch 1636, loss 0.8303478360176086, R2 0.6499594449996948\n",
      "Eval loss 0.8629205226898193, R2 0.6569595336914062\n",
      "epoch 1637, loss 0.8302842974662781, R2 0.6500148773193359\n",
      "Eval loss 0.8628460764884949, R2 0.6570164561271667\n",
      "epoch 1638, loss 0.8302208781242371, R2 0.6500701904296875\n",
      "Eval loss 0.8627716898918152, R2 0.6570731997489929\n",
      "epoch 1639, loss 0.8301576972007751, R2 0.6501255035400391\n",
      "Eval loss 0.862697422504425, R2 0.6571298241615295\n",
      "epoch 1640, loss 0.8300946354866028, R2 0.650180459022522\n",
      "Eval loss 0.8626235127449036, R2 0.6571863889694214\n",
      "epoch 1641, loss 0.83003169298172, R2 0.6502354741096497\n",
      "Eval loss 0.8625497221946716, R2 0.6572427749633789\n",
      "epoch 1642, loss 0.8299689292907715, R2 0.6502902507781982\n",
      "Eval loss 0.8624760508537292, R2 0.6572989821434021\n",
      "epoch 1643, loss 0.8299064040184021, R2 0.6503450870513916\n",
      "Eval loss 0.8624024987220764, R2 0.6573550701141357\n",
      "epoch 1644, loss 0.8298439383506775, R2 0.6503996253013611\n",
      "Eval loss 0.8623291850090027, R2 0.6574111580848694\n",
      "epoch 1645, loss 0.8297815918922424, R2 0.6504541039466858\n",
      "Eval loss 0.8622560501098633, R2 0.6574670076370239\n",
      "epoch 1646, loss 0.8297194838523865, R2 0.650508463382721\n",
      "Eval loss 0.8621829748153687, R2 0.6575227975845337\n",
      "epoch 1647, loss 0.8296574950218201, R2 0.6505627036094666\n",
      "Eval loss 0.8621100783348083, R2 0.6575784683227539\n",
      "epoch 1648, loss 0.8295956254005432, R2 0.6506168246269226\n",
      "Eval loss 0.8620373606681824, R2 0.6576340198516846\n",
      "epoch 1649, loss 0.8295339345932007, R2 0.6506709456443787\n",
      "Eval loss 0.8619648218154907, R2 0.6576894521713257\n",
      "epoch 1650, loss 0.8294723629951477, R2 0.6507247686386108\n",
      "Eval loss 0.861892580986023, R2 0.657744824886322\n",
      "epoch 1651, loss 0.829410970211029, R2 0.6507785320281982\n",
      "Eval loss 0.8618203401565552, R2 0.6577999591827393\n",
      "epoch 1652, loss 0.8293498158454895, R2 0.6508321166038513\n",
      "Eval loss 0.8617483377456665, R2 0.6578550934791565\n",
      "epoch 1653, loss 0.8292885422706604, R2 0.6508857607841492\n",
      "Eval loss 0.8616763949394226, R2 0.6579100489616394\n",
      "epoch 1654, loss 0.8292276263237, R2 0.6509391665458679\n",
      "Eval loss 0.8616047501564026, R2 0.6579649448394775\n",
      "epoch 1655, loss 0.8291668891906738, R2 0.6509923934936523\n",
      "Eval loss 0.8615332245826721, R2 0.6580196022987366\n",
      "epoch 1656, loss 0.8291060924530029, R2 0.651045560836792\n",
      "Eval loss 0.8614618182182312, R2 0.6580742001533508\n",
      "epoch 1657, loss 0.8290456533432007, R2 0.6510986685752869\n",
      "Eval loss 0.8613905310630798, R2 0.6581287384033203\n",
      "epoch 1658, loss 0.8289852738380432, R2 0.651151716709137\n",
      "Eval loss 0.8613194227218628, R2 0.6581830978393555\n",
      "epoch 1659, loss 0.8289249539375305, R2 0.651204526424408\n",
      "Eval loss 0.8612485527992249, R2 0.6582374572753906\n",
      "epoch 1660, loss 0.8288648724555969, R2 0.6512571573257446\n",
      "Eval loss 0.8611777424812317, R2 0.6582916378974915\n",
      "epoch 1661, loss 0.8288049697875977, R2 0.6513099074363708\n",
      "Eval loss 0.8611070513725281, R2 0.658345639705658\n",
      "epoch 1662, loss 0.8287451267242432, R2 0.6513622403144836\n",
      "Eval loss 0.8610367774963379, R2 0.6583995223045349\n",
      "epoch 1663, loss 0.8286855220794678, R2 0.6514146327972412\n",
      "Eval loss 0.8609664440155029, R2 0.6584534049034119\n",
      "epoch 1664, loss 0.8286260366439819, R2 0.6514669060707092\n",
      "Eval loss 0.8608962893486023, R2 0.6585071086883545\n",
      "epoch 1665, loss 0.8285666108131409, R2 0.6515191197395325\n",
      "Eval loss 0.8608263731002808, R2 0.6585606932640076\n",
      "epoch 1666, loss 0.8285074830055237, R2 0.6515710949897766\n",
      "Eval loss 0.8607564568519592, R2 0.6586140990257263\n",
      "epoch 1667, loss 0.8284483551979065, R2 0.651623010635376\n",
      "Eval loss 0.8606867790222168, R2 0.6586675643920898\n",
      "epoch 1668, loss 0.8283894062042236, R2 0.6516748666763306\n",
      "Eval loss 0.8606173396110535, R2 0.6587207913398743\n",
      "epoch 1669, loss 0.8283306360244751, R2 0.651726484298706\n",
      "Eval loss 0.8605479598045349, R2 0.6587740182876587\n",
      "epoch 1670, loss 0.8282718658447266, R2 0.6517781615257263\n",
      "Eval loss 0.8604787588119507, R2 0.6588270664215088\n",
      "epoch 1671, loss 0.8282133340835571, R2 0.6518296599388123\n",
      "Eval loss 0.8604097962379456, R2 0.6588799953460693\n",
      "epoch 1672, loss 0.828154981136322, R2 0.6518809199333191\n",
      "Eval loss 0.8603408336639404, R2 0.6589327454566956\n",
      "epoch 1673, loss 0.8280967473983765, R2 0.6519321799278259\n",
      "Eval loss 0.8602719902992249, R2 0.6589855551719666\n",
      "epoch 1674, loss 0.8280386328697205, R2 0.6519833207130432\n",
      "Eval loss 0.8602035045623779, R2 0.6590381264686584\n",
      "epoch 1675, loss 0.827980637550354, R2 0.6520344018936157\n",
      "Eval loss 0.860135018825531, R2 0.6590906381607056\n",
      "epoch 1676, loss 0.8279228806495667, R2 0.6520853042602539\n",
      "Eval loss 0.8600667715072632, R2 0.6591429710388184\n",
      "epoch 1677, loss 0.8278651237487793, R2 0.6521360278129578\n",
      "Eval loss 0.8599985837936401, R2 0.6591953039169312\n",
      "epoch 1678, loss 0.8278075456619263, R2 0.6521867513656616\n",
      "Eval loss 0.8599306344985962, R2 0.6592474579811096\n",
      "epoch 1679, loss 0.8277502059936523, R2 0.6522373557090759\n",
      "Eval loss 0.8598628640174866, R2 0.6592994332313538\n",
      "epoch 1680, loss 0.8276928067207336, R2 0.6522879004478455\n",
      "Eval loss 0.859795093536377, R2 0.6593514084815979\n",
      "epoch 1681, loss 0.8276357650756836, R2 0.6523381471633911\n",
      "Eval loss 0.8597276210784912, R2 0.6594032645225525\n",
      "epoch 1682, loss 0.8275787234306335, R2 0.6523883938789368\n",
      "Eval loss 0.8596601486206055, R2 0.6594550013542175\n",
      "epoch 1683, loss 0.8275218605995178, R2 0.6524385809898376\n",
      "Eval loss 0.8595929741859436, R2 0.659506618976593\n",
      "epoch 1684, loss 0.8274651765823364, R2 0.652488648891449\n",
      "Eval loss 0.8595258593559265, R2 0.6595582365989685\n",
      "epoch 1685, loss 0.8274085521697998, R2 0.6525384783744812\n",
      "Eval loss 0.8594589829444885, R2 0.6596095561981201\n",
      "epoch 1686, loss 0.8273521065711975, R2 0.652588427066803\n",
      "Eval loss 0.8593921065330505, R2 0.6596608757972717\n",
      "epoch 1687, loss 0.82729572057724, R2 0.6526381373405457\n",
      "Eval loss 0.8593254685401917, R2 0.6597120761871338\n",
      "epoch 1688, loss 0.8272395133972168, R2 0.652687668800354\n",
      "Eval loss 0.8592589497566223, R2 0.6597631573677063\n",
      "epoch 1689, loss 0.8271834850311279, R2 0.6527371406555176\n",
      "Eval loss 0.8591926693916321, R2 0.6598141193389893\n",
      "epoch 1690, loss 0.8271274566650391, R2 0.6527865529060364\n",
      "Eval loss 0.8591265082359314, R2 0.6598649621009827\n",
      "epoch 1691, loss 0.8270717263221741, R2 0.6528357863426208\n",
      "Eval loss 0.8590603470802307, R2 0.6599158048629761\n",
      "epoch 1692, loss 0.8270161747932434, R2 0.6528850197792053\n",
      "Eval loss 0.8589944839477539, R2 0.6599664688110352\n",
      "epoch 1693, loss 0.826960563659668, R2 0.6529340744018555\n",
      "Eval loss 0.8589287400245667, R2 0.6600170135498047\n",
      "epoch 1694, loss 0.8269050717353821, R2 0.6529830694198608\n",
      "Eval loss 0.858863115310669, R2 0.6600674986839294\n",
      "epoch 1695, loss 0.8268499374389648, R2 0.6530318856239319\n",
      "Eval loss 0.8587976098060608, R2 0.6601178646087646\n",
      "epoch 1696, loss 0.8267947435379028, R2 0.6530807018280029\n",
      "Eval loss 0.8587323427200317, R2 0.6601681113243103\n",
      "epoch 1697, loss 0.8267397284507751, R2 0.6531293392181396\n",
      "Eval loss 0.8586670756340027, R2 0.6602182984352112\n",
      "epoch 1698, loss 0.826684832572937, R2 0.6531778573989868\n",
      "Eval loss 0.8586021065711975, R2 0.6602683067321777\n",
      "epoch 1699, loss 0.826630175113678, R2 0.6532263159751892\n",
      "Eval loss 0.8585371375083923, R2 0.6603183150291443\n",
      "epoch 1700, loss 0.8265755772590637, R2 0.653274655342102\n",
      "Eval loss 0.8584724068641663, R2 0.660368025302887\n",
      "epoch 1701, loss 0.8265209794044495, R2 0.6533228754997253\n",
      "Eval loss 0.8584078550338745, R2 0.6604177951812744\n",
      "epoch 1702, loss 0.8264666199684143, R2 0.6533710956573486\n",
      "Eval loss 0.8583433032035828, R2 0.6604673862457275\n",
      "epoch 1703, loss 0.8264124393463135, R2 0.6534190773963928\n",
      "Eval loss 0.8582789301872253, R2 0.6605169177055359\n",
      "epoch 1704, loss 0.8263583183288574, R2 0.6534669995307922\n",
      "Eval loss 0.8582146763801575, R2 0.6605663895606995\n",
      "epoch 1705, loss 0.8263043165206909, R2 0.6535148024559021\n",
      "Eval loss 0.8581507205963135, R2 0.6606157422065735\n",
      "epoch 1706, loss 0.8262505531311035, R2 0.6535624861717224\n",
      "Eval loss 0.8580868244171143, R2 0.660664975643158\n",
      "epoch 1707, loss 0.8261968493461609, R2 0.6536101698875427\n",
      "Eval loss 0.8580231070518494, R2 0.6607140898704529\n",
      "epoch 1708, loss 0.8261430859565735, R2 0.6536576747894287\n",
      "Eval loss 0.8579593300819397, R2 0.6607630848884583\n",
      "epoch 1709, loss 0.8260896801948547, R2 0.6537050604820251\n",
      "Eval loss 0.8578959703445435, R2 0.6608120203018188\n",
      "epoch 1710, loss 0.8260363936424255, R2 0.653752326965332\n",
      "Eval loss 0.8578325510025024, R2 0.6608608365058899\n",
      "epoch 1711, loss 0.8259832262992859, R2 0.6537994742393494\n",
      "Eval loss 0.8577693700790405, R2 0.6609095931053162\n",
      "epoch 1712, loss 0.825930118560791, R2 0.6538466811180115\n",
      "Eval loss 0.8577062487602234, R2 0.6609582304954529\n",
      "epoch 1713, loss 0.8258771300315857, R2 0.6538936495780945\n",
      "Eval loss 0.8576433062553406, R2 0.6610066890716553\n",
      "epoch 1714, loss 0.8258243799209595, R2 0.6539405584335327\n",
      "Eval loss 0.8575805425643921, R2 0.6610550284385681\n",
      "epoch 1715, loss 0.825771689414978, R2 0.6539873480796814\n",
      "Eval loss 0.8575177788734436, R2 0.6611034274101257\n",
      "epoch 1716, loss 0.8257189989089966, R2 0.6540340781211853\n",
      "Eval loss 0.8574553728103638, R2 0.661151647567749\n",
      "epoch 1717, loss 0.8256665468215942, R2 0.6540806293487549\n",
      "Eval loss 0.8573929071426392, R2 0.661199688911438\n",
      "epoch 1718, loss 0.8256141543388367, R2 0.6541271209716797\n",
      "Eval loss 0.8573306202888489, R2 0.6612478494644165\n",
      "epoch 1719, loss 0.8255619406700134, R2 0.6541734933853149\n",
      "Eval loss 0.8572685718536377, R2 0.6612957119941711\n",
      "epoch 1720, loss 0.8255099654197693, R2 0.6542198657989502\n",
      "Eval loss 0.8572065234184265, R2 0.661343514919281\n",
      "epoch 1721, loss 0.8254579305648804, R2 0.6542661190032959\n",
      "Eval loss 0.8571447134017944, R2 0.6613912582397461\n",
      "epoch 1722, loss 0.8254060745239258, R2 0.6543121933937073\n",
      "Eval loss 0.8570830225944519, R2 0.6614388823509216\n",
      "epoch 1723, loss 0.8253543972969055, R2 0.6543581485748291\n",
      "Eval loss 0.8570215106010437, R2 0.6614864468574524\n",
      "epoch 1724, loss 0.8253027200698853, R2 0.6544041037559509\n",
      "Eval loss 0.8569599390029907, R2 0.6615338325500488\n",
      "epoch 1725, loss 0.8252512216567993, R2 0.6544499397277832\n",
      "Eval loss 0.8568986058235168, R2 0.6615810990333557\n",
      "epoch 1726, loss 0.8251999020576477, R2 0.6544955968856812\n",
      "Eval loss 0.8568373322486877, R2 0.6616283655166626\n",
      "epoch 1727, loss 0.8251486420631409, R2 0.6545412540435791\n",
      "Eval loss 0.8567763566970825, R2 0.6616756916046143\n",
      "epoch 1728, loss 0.8250975012779236, R2 0.6545867323875427\n",
      "Eval loss 0.8567154407501221, R2 0.6617226004600525\n",
      "epoch 1729, loss 0.8250465393066406, R2 0.6546321511268616\n",
      "Eval loss 0.8566547632217407, R2 0.6617696285247803\n",
      "epoch 1730, loss 0.824995756149292, R2 0.6546774506568909\n",
      "Eval loss 0.8565940856933594, R2 0.661816418170929\n",
      "epoch 1731, loss 0.8249448537826538, R2 0.6547226905822754\n",
      "Eval loss 0.8565335869789124, R2 0.6618632674217224\n",
      "epoch 1732, loss 0.8248942494392395, R2 0.6547678709030151\n",
      "Eval loss 0.8564732074737549, R2 0.661909818649292\n",
      "epoch 1733, loss 0.8248437643051147, R2 0.6548128724098206\n",
      "Eval loss 0.8564128875732422, R2 0.6619564294815063\n",
      "epoch 1734, loss 0.8247933387756348, R2 0.6548578143119812\n",
      "Eval loss 0.8563528060913086, R2 0.6620029211044312\n",
      "epoch 1735, loss 0.8247429728507996, R2 0.6549026966094971\n",
      "Eval loss 0.8562927842140198, R2 0.6620492935180664\n",
      "epoch 1736, loss 0.8246928453445435, R2 0.6549474000930786\n",
      "Eval loss 0.8562329411506653, R2 0.6620956063270569\n",
      "epoch 1737, loss 0.8246427774429321, R2 0.6549921035766602\n",
      "Eval loss 0.8561730980873108, R2 0.662141740322113\n",
      "epoch 1738, loss 0.8245928883552551, R2 0.6550366282463074\n",
      "Eval loss 0.8561135530471802, R2 0.662187933921814\n",
      "epoch 1739, loss 0.8245430588722229, R2 0.6550810933113098\n",
      "Eval loss 0.8560540676116943, R2 0.6622338891029358\n",
      "epoch 1740, loss 0.824493408203125, R2 0.6551254987716675\n",
      "Eval loss 0.855994701385498, R2 0.6622798442840576\n",
      "epoch 1741, loss 0.8244436979293823, R2 0.6551697254180908\n",
      "Eval loss 0.8559354543685913, R2 0.6623256206512451\n",
      "epoch 1742, loss 0.8243942856788635, R2 0.6552139520645142\n",
      "Eval loss 0.8558764457702637, R2 0.6623713970184326\n",
      "epoch 1743, loss 0.8243449330329895, R2 0.6552581191062927\n",
      "Eval loss 0.8558174967765808, R2 0.6624169945716858\n",
      "epoch 1744, loss 0.8242956399917603, R2 0.6553019881248474\n",
      "Eval loss 0.855758547782898, R2 0.6624624729156494\n",
      "epoch 1745, loss 0.8242465257644653, R2 0.6553458571434021\n",
      "Eval loss 0.8556999564170837, R2 0.662507951259613\n",
      "epoch 1746, loss 0.82419753074646, R2 0.6553897261619568\n",
      "Eval loss 0.8556413054466248, R2 0.6625533103942871\n",
      "epoch 1747, loss 0.8241485953330994, R2 0.6554334759712219\n",
      "Eval loss 0.8555827140808105, R2 0.6625986099243164\n",
      "epoch 1748, loss 0.8240998983383179, R2 0.6554771065711975\n",
      "Eval loss 0.855524480342865, R2 0.6626437902450562\n",
      "epoch 1749, loss 0.8240512013435364, R2 0.6555206179618835\n",
      "Eval loss 0.8554664254188538, R2 0.6626887917518616\n",
      "epoch 1750, loss 0.8240026831626892, R2 0.6555640697479248\n",
      "Eval loss 0.8554081916809082, R2 0.662733793258667\n",
      "epoch 1751, loss 0.823954164981842, R2 0.6556074023246765\n",
      "Eval loss 0.8553502559661865, R2 0.6627786755561829\n",
      "epoch 1752, loss 0.823905885219574, R2 0.6556506752967834\n",
      "Eval loss 0.8552924394607544, R2 0.662823498249054\n",
      "epoch 1753, loss 0.8238576054573059, R2 0.655693769454956\n",
      "Eval loss 0.8552347421646118, R2 0.6628682613372803\n",
      "epoch 1754, loss 0.8238095641136169, R2 0.6557368636131287\n",
      "Eval loss 0.855177104473114, R2 0.662912905216217\n",
      "epoch 1755, loss 0.8237616419792175, R2 0.6557798981666565\n",
      "Eval loss 0.8551196455955505, R2 0.6629574298858643\n",
      "epoch 1756, loss 0.8237136602401733, R2 0.65582275390625\n",
      "Eval loss 0.8550623059272766, R2 0.6630018949508667\n",
      "epoch 1757, loss 0.8236658573150635, R2 0.6558656096458435\n",
      "Eval loss 0.855005145072937, R2 0.66304612159729\n",
      "epoch 1758, loss 0.8236182332038879, R2 0.6559082865715027\n",
      "Eval loss 0.8549479842185974, R2 0.6630904674530029\n",
      "epoch 1759, loss 0.8235706686973572, R2 0.6559509634971619\n",
      "Eval loss 0.8548910021781921, R2 0.6631346344947815\n",
      "epoch 1760, loss 0.8235232830047607, R2 0.6559934020042419\n",
      "Eval loss 0.8548341393470764, R2 0.6631788015365601\n",
      "epoch 1761, loss 0.8234758377075195, R2 0.656035840511322\n",
      "Eval loss 0.854777455329895, R2 0.6632226705551147\n",
      "epoch 1762, loss 0.8234288096427917, R2 0.6560782790184021\n",
      "Eval loss 0.8547208905220032, R2 0.6632665991783142\n",
      "epoch 1763, loss 0.8233815431594849, R2 0.6561205387115479\n",
      "Eval loss 0.8546643257141113, R2 0.6633104681968689\n",
      "epoch 1764, loss 0.8233346343040466, R2 0.656162679195404\n",
      "Eval loss 0.8546079397201538, R2 0.6633541584014893\n",
      "epoch 1765, loss 0.823287844657898, R2 0.6562047600746155\n",
      "Eval loss 0.8545518517494202, R2 0.6633977890014648\n",
      "epoch 1766, loss 0.8232409358024597, R2 0.6562467813491821\n",
      "Eval loss 0.854495644569397, R2 0.6634413003921509\n",
      "epoch 1767, loss 0.8231942653656006, R2 0.6562886834144592\n",
      "Eval loss 0.8544396162033081, R2 0.6634848117828369\n",
      "epoch 1768, loss 0.823147714138031, R2 0.6563305258750916\n",
      "Eval loss 0.8543837070465088, R2 0.6635282039642334\n",
      "epoch 1769, loss 0.8231012225151062, R2 0.6563722491264343\n",
      "Eval loss 0.8543279767036438, R2 0.6635714173316956\n",
      "epoch 1770, loss 0.8230549693107605, R2 0.6564139723777771\n",
      "Eval loss 0.8542723655700684, R2 0.6636146903038025\n",
      "epoch 1771, loss 0.8230087161064148, R2 0.6564554572105408\n",
      "Eval loss 0.8542168140411377, R2 0.6636579036712646\n",
      "epoch 1772, loss 0.8229625225067139, R2 0.6564969420433044\n",
      "Eval loss 0.8541613817214966, R2 0.6637008190155029\n",
      "epoch 1773, loss 0.822916567325592, R2 0.6565382480621338\n",
      "Eval loss 0.8541061878204346, R2 0.663743793964386\n",
      "epoch 1774, loss 0.8228705525398254, R2 0.6565795540809631\n",
      "Eval loss 0.8540509343147278, R2 0.6637866497039795\n",
      "epoch 1775, loss 0.8228247761726379, R2 0.6566207408905029\n",
      "Eval loss 0.8539959192276001, R2 0.6638294458389282\n",
      "epoch 1776, loss 0.8227790594100952, R2 0.6566619277000427\n",
      "Eval loss 0.853941023349762, R2 0.6638721823692322\n",
      "epoch 1777, loss 0.822733461856842, R2 0.6567029356956482\n",
      "Eval loss 0.8538861870765686, R2 0.663914680480957\n",
      "epoch 1778, loss 0.8226879239082336, R2 0.6567439436912537\n",
      "Eval loss 0.8538314700126648, R2 0.6639571785926819\n",
      "epoch 1779, loss 0.8226425647735596, R2 0.6567847728729248\n",
      "Eval loss 0.8537768721580505, R2 0.6639996767044067\n",
      "epoch 1780, loss 0.822597324848175, R2 0.656825602054596\n",
      "Eval loss 0.8537223935127258, R2 0.6640419363975525\n",
      "epoch 1781, loss 0.8225520253181458, R2 0.6568663120269775\n",
      "Eval loss 0.8536680340766907, R2 0.664084255695343\n",
      "epoch 1782, loss 0.8225070238113403, R2 0.6569069027900696\n",
      "Eval loss 0.8536139130592346, R2 0.6641263961791992\n",
      "epoch 1783, loss 0.8224620223045349, R2 0.6569473743438721\n",
      "Eval loss 0.853559672832489, R2 0.6641685366630554\n",
      "epoch 1784, loss 0.822417140007019, R2 0.6569877862930298\n",
      "Eval loss 0.8535057902336121, R2 0.6642105579376221\n",
      "epoch 1785, loss 0.822372317314148, R2 0.6570281982421875\n",
      "Eval loss 0.8534519076347351, R2 0.664252519607544\n",
      "epoch 1786, loss 0.8223277926445007, R2 0.6570684909820557\n",
      "Eval loss 0.8533980846405029, R2 0.6642943024635315\n",
      "epoch 1787, loss 0.8222830891609192, R2 0.6571086645126343\n",
      "Eval loss 0.8533444404602051, R2 0.664336085319519\n",
      "epoch 1788, loss 0.8222387433052063, R2 0.6571487784385681\n",
      "Eval loss 0.853290855884552, R2 0.664377748966217\n",
      "epoch 1789, loss 0.8221943378448486, R2 0.6571887731552124\n",
      "Eval loss 0.8532373905181885, R2 0.6644193530082703\n",
      "epoch 1790, loss 0.8221500515937805, R2 0.6572287082672119\n",
      "Eval loss 0.8531841039657593, R2 0.6644608378410339\n",
      "epoch 1791, loss 0.8221060037612915, R2 0.6572685241699219\n",
      "Eval loss 0.8531309366226196, R2 0.6645022034645081\n",
      "epoch 1792, loss 0.8220618963241577, R2 0.6573083400726318\n",
      "Eval loss 0.85307776927948, R2 0.6645435690879822\n",
      "epoch 1793, loss 0.8220179080963135, R2 0.6573480367660522\n",
      "Eval loss 0.8530248403549194, R2 0.6645848751068115\n",
      "epoch 1794, loss 0.8219740986824036, R2 0.6573876738548279\n",
      "Eval loss 0.8529719710350037, R2 0.6646260619163513\n",
      "epoch 1795, loss 0.8219304084777832, R2 0.657427191734314\n",
      "Eval loss 0.8529192209243774, R2 0.6646671295166016\n",
      "epoch 1796, loss 0.8218867182731628, R2 0.6574666500091553\n",
      "Eval loss 0.8528664708137512, R2 0.664708137512207\n",
      "epoch 1797, loss 0.8218432068824768, R2 0.6575059294700623\n",
      "Eval loss 0.8528140783309937, R2 0.6647490859031677\n",
      "epoch 1798, loss 0.8217996954917908, R2 0.657545268535614\n",
      "Eval loss 0.8527615666389465, R2 0.6647899150848389\n",
      "epoch 1799, loss 0.8217563629150391, R2 0.6575844287872314\n",
      "Eval loss 0.8527093529701233, R2 0.6648306250572205\n",
      "epoch 1800, loss 0.8217130899429321, R2 0.6576235294342041\n",
      "Eval loss 0.8526571393013, R2 0.6648713946342468\n",
      "epoch 1801, loss 0.8216699361801147, R2 0.6576626300811768\n",
      "Eval loss 0.8526050448417664, R2 0.6649121046066284\n",
      "epoch 1802, loss 0.8216268420219421, R2 0.6577015519142151\n",
      "Eval loss 0.8525530695915222, R2 0.6649525165557861\n",
      "epoch 1803, loss 0.8215839862823486, R2 0.6577405333518982\n",
      "Eval loss 0.8525010943412781, R2 0.6649928689002991\n",
      "epoch 1804, loss 0.8215410113334656, R2 0.6577791571617126\n",
      "Eval loss 0.8524494171142578, R2 0.6650333404541016\n",
      "epoch 1805, loss 0.8214983940124512, R2 0.6578179001808167\n",
      "Eval loss 0.8523977398872375, R2 0.6650736331939697\n",
      "epoch 1806, loss 0.8214555978775024, R2 0.6578565835952759\n",
      "Eval loss 0.8523460626602173, R2 0.6651138067245483\n",
      "epoch 1807, loss 0.8214131593704224, R2 0.6578950881958008\n",
      "Eval loss 0.8522947430610657, R2 0.6651539206504822\n",
      "epoch 1808, loss 0.8213707208633423, R2 0.6579335927963257\n",
      "Eval loss 0.8522434234619141, R2 0.665194034576416\n",
      "epoch 1809, loss 0.8213282823562622, R2 0.657971978187561\n",
      "Eval loss 0.852192223072052, R2 0.6652340292930603\n",
      "epoch 1810, loss 0.8212860226631165, R2 0.6580102443695068\n",
      "Eval loss 0.8521411418914795, R2 0.6652738451957703\n",
      "epoch 1811, loss 0.8212438821792603, R2 0.6580485105514526\n",
      "Eval loss 0.8520901203155518, R2 0.6653136610984802\n",
      "epoch 1812, loss 0.8212018013000488, R2 0.6580867171287537\n",
      "Eval loss 0.8520392179489136, R2 0.6653533577919006\n",
      "epoch 1813, loss 0.8211597800254822, R2 0.6581248044967651\n",
      "Eval loss 0.8519883751869202, R2 0.665393054485321\n",
      "epoch 1814, loss 0.8211179971694946, R2 0.6581628322601318\n",
      "Eval loss 0.8519376516342163, R2 0.6654326915740967\n",
      "epoch 1815, loss 0.8210761547088623, R2 0.6582006812095642\n",
      "Eval loss 0.851887047290802, R2 0.6654720306396484\n",
      "epoch 1816, loss 0.8210344314575195, R2 0.6582384705543518\n",
      "Eval loss 0.851836621761322, R2 0.6655115485191345\n",
      "epoch 1817, loss 0.8209928870201111, R2 0.6582762598991394\n",
      "Eval loss 0.851786196231842, R2 0.6655508875846863\n",
      "epoch 1818, loss 0.8209513425827026, R2 0.6583139896392822\n",
      "Eval loss 0.8517360687255859, R2 0.6655901074409485\n",
      "epoch 1819, loss 0.8209099173545837, R2 0.6583516597747803\n",
      "Eval loss 0.8516858816146851, R2 0.6656293272972107\n",
      "epoch 1820, loss 0.8208686113357544, R2 0.658389151096344\n",
      "Eval loss 0.851635754108429, R2 0.6656684279441833\n",
      "epoch 1821, loss 0.8208274245262146, R2 0.6584265828132629\n",
      "Eval loss 0.851585865020752, R2 0.6657073497772217\n",
      "epoch 1822, loss 0.8207862377166748, R2 0.6584638953208923\n",
      "Eval loss 0.8515361547470093, R2 0.66574627161026\n",
      "epoch 1823, loss 0.8207452297210693, R2 0.6585012078285217\n",
      "Eval loss 0.851486325263977, R2 0.6657852530479431\n",
      "epoch 1824, loss 0.8207043409347534, R2 0.6585384607315063\n",
      "Eval loss 0.8514366745948792, R2 0.6658239960670471\n",
      "epoch 1825, loss 0.8206635117530823, R2 0.6585755944252014\n",
      "Eval loss 0.8513871431350708, R2 0.6658627390861511\n",
      "epoch 1826, loss 0.8206226825714111, R2 0.6586126685142517\n",
      "Eval loss 0.851337730884552, R2 0.6659014225006104\n",
      "epoch 1827, loss 0.8205820322036743, R2 0.6586496829986572\n",
      "Eval loss 0.8512885570526123, R2 0.66593998670578\n",
      "epoch 1828, loss 0.8205415606498718, R2 0.6586865782737732\n",
      "Eval loss 0.8512391448020935, R2 0.6659784913063049\n",
      "epoch 1829, loss 0.8205009698867798, R2 0.6587234735488892\n",
      "Eval loss 0.8511900901794434, R2 0.6660169363021851\n",
      "epoch 1830, loss 0.8204605579376221, R2 0.658760130405426\n",
      "Eval loss 0.8511411547660828, R2 0.6660552620887756\n",
      "epoch 1831, loss 0.8204202651977539, R2 0.6587968468666077\n",
      "Eval loss 0.8510921597480774, R2 0.6660935282707214\n",
      "epoch 1832, loss 0.8203801512718201, R2 0.6588335037231445\n",
      "Eval loss 0.8510433435440063, R2 0.6661316156387329\n",
      "epoch 1833, loss 0.8203400373458862, R2 0.6588699817657471\n",
      "Eval loss 0.8509946465492249, R2 0.6661698222160339\n",
      "epoch 1834, loss 0.8202999830245972, R2 0.6589065194129944\n",
      "Eval loss 0.8509460091590881, R2 0.6662078499794006\n",
      "epoch 1835, loss 0.8202601075172424, R2 0.6589429378509521\n",
      "Eval loss 0.8508975505828857, R2 0.6662457585334778\n",
      "epoch 1836, loss 0.8202201724052429, R2 0.6589792370796204\n",
      "Eval loss 0.8508491516113281, R2 0.6662837266921997\n",
      "epoch 1837, loss 0.8201804161071777, R2 0.6590154767036438\n",
      "Eval loss 0.8508008718490601, R2 0.6663215160369873\n",
      "epoch 1838, loss 0.8201407790184021, R2 0.6590516567230225\n",
      "Eval loss 0.8507526516914368, R2 0.6663592457771301\n",
      "epoch 1839, loss 0.8201012015342712, R2 0.6590876579284668\n",
      "Eval loss 0.850704550743103, R2 0.6663969159126282\n",
      "epoch 1840, loss 0.8200618028640747, R2 0.6591236591339111\n",
      "Eval loss 0.8506565093994141, R2 0.6664345264434814\n",
      "epoch 1841, loss 0.8200223445892334, R2 0.6591597199440002\n",
      "Eval loss 0.8506086468696594, R2 0.6664720773696899\n",
      "epoch 1842, loss 0.8199830651283264, R2 0.6591955423355103\n",
      "Eval loss 0.8505607843399048, R2 0.6665095090866089\n",
      "epoch 1843, loss 0.8199437856674194, R2 0.6592313647270203\n",
      "Eval loss 0.8505131602287292, R2 0.6665468811988831\n",
      "epoch 1844, loss 0.8199047446250916, R2 0.6592670679092407\n",
      "Eval loss 0.8504655361175537, R2 0.6665840744972229\n",
      "epoch 1845, loss 0.8198657631874084, R2 0.6593026518821716\n",
      "Eval loss 0.8504180312156677, R2 0.6666213870048523\n",
      "epoch 1846, loss 0.8198267221450806, R2 0.6593382954597473\n",
      "Eval loss 0.8503707647323608, R2 0.6666585803031921\n",
      "epoch 1847, loss 0.8197879195213318, R2 0.6593737006187439\n",
      "Eval loss 0.8503233194351196, R2 0.6666956543922424\n",
      "epoch 1848, loss 0.8197491765022278, R2 0.6594091653823853\n",
      "Eval loss 0.8502761125564575, R2 0.6667327284812927\n",
      "epoch 1849, loss 0.8197104334831238, R2 0.6594446301460266\n",
      "Eval loss 0.850229024887085, R2 0.6667695641517639\n",
      "epoch 1850, loss 0.8196718692779541, R2 0.6594798564910889\n",
      "Eval loss 0.8501819968223572, R2 0.6668064594268799\n",
      "epoch 1851, loss 0.8196333646774292, R2 0.6595151424407959\n",
      "Eval loss 0.8501351475715637, R2 0.6668432950973511\n",
      "epoch 1852, loss 0.8195949196815491, R2 0.6595501899719238\n",
      "Eval loss 0.8500882387161255, R2 0.6668799519538879\n",
      "epoch 1853, loss 0.8195566534996033, R2 0.6595852375030518\n",
      "Eval loss 0.8500415682792664, R2 0.6669166088104248\n",
      "epoch 1854, loss 0.8195183873176575, R2 0.6596203446388245\n",
      "Eval loss 0.8499950170516968, R2 0.6669531464576721\n",
      "epoch 1855, loss 0.8194801807403564, R2 0.6596552133560181\n",
      "Eval loss 0.849948525428772, R2 0.6669896841049194\n",
      "epoch 1856, loss 0.819442093372345, R2 0.6596900820732117\n",
      "Eval loss 0.8499019742012024, R2 0.6670261025428772\n",
      "epoch 1857, loss 0.819404125213623, R2 0.6597248315811157\n",
      "Eval loss 0.8498556613922119, R2 0.667062520980835\n",
      "epoch 1858, loss 0.8193662762641907, R2 0.6597595810890198\n",
      "Eval loss 0.849809467792511, R2 0.6670988202095032\n",
      "epoch 1859, loss 0.8193284869194031, R2 0.659794270992279\n",
      "Eval loss 0.8497633337974548, R2 0.6671350002288818\n",
      "epoch 1860, loss 0.8192906379699707, R2 0.6598289012908936\n",
      "Eval loss 0.8497172594070435, R2 0.6671712398529053\n",
      "epoch 1861, loss 0.8192530274391174, R2 0.659863293170929\n",
      "Eval loss 0.8496712446212769, R2 0.6672071814537048\n",
      "epoch 1862, loss 0.8192155361175537, R2 0.6598977446556091\n",
      "Eval loss 0.8496253490447998, R2 0.6672433018684387\n",
      "epoch 1863, loss 0.8191779255867004, R2 0.6599321961402893\n",
      "Eval loss 0.8495797514915466, R2 0.6672792434692383\n",
      "epoch 1864, loss 0.819140613079071, R2 0.6599664092063904\n",
      "Eval loss 0.8495339751243591, R2 0.6673150658607483\n",
      "epoch 1865, loss 0.8191032409667969, R2 0.6600006818771362\n",
      "Eval loss 0.8494884371757507, R2 0.6673508882522583\n",
      "epoch 1866, loss 0.819066047668457, R2 0.6600348353385925\n",
      "Eval loss 0.8494430184364319, R2 0.6673866510391235\n",
      "epoch 1867, loss 0.819028913974762, R2 0.660068929195404\n",
      "Eval loss 0.8493975400924683, R2 0.6674222946166992\n",
      "epoch 1868, loss 0.8189918398857117, R2 0.660102903842926\n",
      "Eval loss 0.8493523001670837, R2 0.6674578189849854\n",
      "epoch 1869, loss 0.8189548850059509, R2 0.6601369380950928\n",
      "Eval loss 0.8493070006370544, R2 0.6674934029579163\n",
      "epoch 1870, loss 0.818917989730835, R2 0.6601707935333252\n",
      "Eval loss 0.8492618799209595, R2 0.6675288677215576\n",
      "epoch 1871, loss 0.8188812136650085, R2 0.6602045893669128\n",
      "Eval loss 0.849216878414154, R2 0.667564332485199\n",
      "epoch 1872, loss 0.8188444375991821, R2 0.6602383852005005\n",
      "Eval loss 0.849172055721283, R2 0.6675996780395508\n",
      "epoch 1873, loss 0.8188077807426453, R2 0.6602720022201538\n",
      "Eval loss 0.8491271138191223, R2 0.6676348447799683\n",
      "epoch 1874, loss 0.818771243095398, R2 0.6603056192398071\n",
      "Eval loss 0.8490824699401855, R2 0.6676700711250305\n",
      "epoch 1875, loss 0.8187347054481506, R2 0.6603391766548157\n",
      "Eval loss 0.8490378260612488, R2 0.6677051186561584\n",
      "epoch 1876, loss 0.8186983466148376, R2 0.6603726744651794\n",
      "Eval loss 0.8489932417869568, R2 0.6677402257919312\n",
      "epoch 1877, loss 0.8186621069908142, R2 0.6604060530662537\n",
      "Eval loss 0.8489487767219543, R2 0.6677752137184143\n",
      "epoch 1878, loss 0.818625807762146, R2 0.6604394912719727\n",
      "Eval loss 0.8489044904708862, R2 0.6678100824356079\n",
      "epoch 1879, loss 0.8185896873474121, R2 0.6604726314544678\n",
      "Eval loss 0.8488600850105286, R2 0.6678449511528015\n",
      "epoch 1880, loss 0.818553626537323, R2 0.6605058312416077\n",
      "Eval loss 0.8488159775733948, R2 0.6678797006607056\n",
      "epoch 1881, loss 0.8185175657272339, R2 0.6605390906333923\n",
      "Eval loss 0.8487718105316162, R2 0.6679143905639648\n",
      "epoch 1882, loss 0.8184815049171448, R2 0.6605720520019531\n",
      "Eval loss 0.848727822303772, R2 0.6679490804672241\n",
      "epoch 1883, loss 0.8184458613395691, R2 0.6606050729751587\n",
      "Eval loss 0.8486839532852173, R2 0.6679835915565491\n",
      "epoch 1884, loss 0.8184101581573486, R2 0.6606380939483643\n",
      "Eval loss 0.8486400246620178, R2 0.6680181622505188\n",
      "epoch 1885, loss 0.8183742761611938, R2 0.6606709361076355\n",
      "Eval loss 0.8485963940620422, R2 0.6680526733398438\n",
      "epoch 1886, loss 0.8183386921882629, R2 0.660703718662262\n",
      "Eval loss 0.8485527634620667, R2 0.6680870056152344\n",
      "epoch 1887, loss 0.8183032870292664, R2 0.6607364416122437\n",
      "Eval loss 0.8485092520713806, R2 0.6681212782859802\n",
      "epoch 1888, loss 0.8182677626609802, R2 0.6607691049575806\n",
      "Eval loss 0.848465621471405, R2 0.6681556105613708\n",
      "epoch 1889, loss 0.8182324171066284, R2 0.6608017683029175\n",
      "Eval loss 0.8484222292900085, R2 0.6681897640228271\n",
      "epoch 1890, loss 0.8181972503662109, R2 0.6608342528343201\n",
      "Eval loss 0.8483790159225464, R2 0.6682238578796387\n",
      "epoch 1891, loss 0.8181620240211487, R2 0.6608667373657227\n",
      "Eval loss 0.8483357429504395, R2 0.6682578325271606\n",
      "epoch 1892, loss 0.8181268572807312, R2 0.6608991622924805\n",
      "Eval loss 0.8482925891876221, R2 0.6682918667793274\n",
      "epoch 1893, loss 0.8180918097496033, R2 0.6609315276145935\n",
      "Eval loss 0.8482496738433838, R2 0.6683258414268494\n",
      "epoch 1894, loss 0.8180568218231201, R2 0.6609638333320618\n",
      "Eval loss 0.8482066988945007, R2 0.6683596968650818\n",
      "epoch 1895, loss 0.8180219531059265, R2 0.6609960794448853\n",
      "Eval loss 0.848163902759552, R2 0.6683934926986694\n",
      "epoch 1896, loss 0.8179872035980225, R2 0.6610281467437744\n",
      "Eval loss 0.8481209874153137, R2 0.6684272289276123\n",
      "epoch 1897, loss 0.8179523944854736, R2 0.6610602736473083\n",
      "Eval loss 0.8480783700942993, R2 0.6684608459472656\n",
      "epoch 1898, loss 0.8179177641868591, R2 0.661092221736908\n",
      "Eval loss 0.8480357527732849, R2 0.668494462966919\n",
      "epoch 1899, loss 0.8178831934928894, R2 0.6611242294311523\n",
      "Eval loss 0.8479932546615601, R2 0.6685279607772827\n",
      "epoch 1900, loss 0.8178486227989197, R2 0.6611561179161072\n",
      "Eval loss 0.84795081615448, R2 0.6685614585876465\n",
      "epoch 1901, loss 0.817814290523529, R2 0.661188006401062\n",
      "Eval loss 0.8479085564613342, R2 0.6685947775840759\n",
      "epoch 1902, loss 0.8177798390388489, R2 0.6612196564674377\n",
      "Eval loss 0.8478662967681885, R2 0.6686281561851501\n",
      "epoch 1903, loss 0.817745566368103, R2 0.6612514853477478\n",
      "Eval loss 0.8478241562843323, R2 0.6686614751815796\n",
      "epoch 1904, loss 0.8177114725112915, R2 0.6612830758094788\n",
      "Eval loss 0.8477819561958313, R2 0.6686946749687195\n",
      "epoch 1905, loss 0.8176773190498352, R2 0.6613146066665649\n",
      "Eval loss 0.8477399945259094, R2 0.6687278151512146\n",
      "epoch 1906, loss 0.8176432847976685, R2 0.6613462567329407\n",
      "Eval loss 0.8476981520652771, R2 0.6687608957290649\n",
      "epoch 1907, loss 0.8176093101501465, R2 0.6613776683807373\n",
      "Eval loss 0.84765625, R2 0.6687939167022705\n",
      "epoch 1908, loss 0.8175754547119141, R2 0.6614090204238892\n",
      "Eval loss 0.8476145267486572, R2 0.6688268780708313\n",
      "epoch 1909, loss 0.8175415396690369, R2 0.6614404320716858\n",
      "Eval loss 0.8475729823112488, R2 0.6688597798347473\n",
      "epoch 1910, loss 0.817507803440094, R2 0.6614716053009033\n",
      "Eval loss 0.8475313186645508, R2 0.668892502784729\n",
      "epoch 1911, loss 0.8174741864204407, R2 0.6615027785301208\n",
      "Eval loss 0.8474898934364319, R2 0.6689253449440002\n",
      "epoch 1912, loss 0.8174405097961426, R2 0.6615340113639832\n",
      "Eval loss 0.8474484086036682, R2 0.6689581274986267\n",
      "epoch 1913, loss 0.8174070715904236, R2 0.6615650653839111\n",
      "Eval loss 0.8474071621894836, R2 0.6689906716346741\n",
      "epoch 1914, loss 0.8173735737800598, R2 0.6615960597991943\n",
      "Eval loss 0.8473659157752991, R2 0.6690232753753662\n",
      "epoch 1915, loss 0.8173401951789856, R2 0.6616270542144775\n",
      "Eval loss 0.8473247289657593, R2 0.6690558195114136\n",
      "epoch 1916, loss 0.8173069357872009, R2 0.6616579294204712\n",
      "Eval loss 0.8472837209701538, R2 0.6690882444381714\n",
      "epoch 1917, loss 0.8172736763954163, R2 0.6616888046264648\n",
      "Eval loss 0.8472427129745483, R2 0.6691206097602844\n",
      "epoch 1918, loss 0.8172405958175659, R2 0.661719560623169\n",
      "Eval loss 0.8472017645835876, R2 0.6691529750823975\n",
      "epoch 1919, loss 0.817207396030426, R2 0.6617502570152283\n",
      "Eval loss 0.847161054611206, R2 0.669185221195221\n",
      "epoch 1920, loss 0.8171744346618652, R2 0.6617808938026428\n",
      "Eval loss 0.8471202850341797, R2 0.6692175269126892\n",
      "epoch 1921, loss 0.8171415328979492, R2 0.6618114113807678\n",
      "Eval loss 0.8470796346664429, R2 0.6692495942115784\n",
      "epoch 1922, loss 0.817108690738678, R2 0.6618419885635376\n",
      "Eval loss 0.8470390439033508, R2 0.6692817211151123\n",
      "epoch 1923, loss 0.8170759081840515, R2 0.6618725061416626\n",
      "Eval loss 0.8469985723495483, R2 0.6693137884140015\n",
      "epoch 1924, loss 0.817043125629425, R2 0.6619028449058533\n",
      "Eval loss 0.8469581007957458, R2 0.6693457961082458\n",
      "epoch 1925, loss 0.8170104622840881, R2 0.661933183670044\n",
      "Eval loss 0.8469178080558777, R2 0.6693776845932007\n",
      "epoch 1926, loss 0.8169779777526855, R2 0.6619634628295898\n",
      "Eval loss 0.8468775153160095, R2 0.669409453868866\n",
      "epoch 1927, loss 0.8169453740119934, R2 0.6619937419891357\n",
      "Eval loss 0.8468374609947205, R2 0.6694412231445312\n",
      "epoch 1928, loss 0.8169130086898804, R2 0.6620239019393921\n",
      "Eval loss 0.8467973470687866, R2 0.6694729924201965\n",
      "epoch 1929, loss 0.8168807029724121, R2 0.6620540022850037\n",
      "Eval loss 0.8467573523521423, R2 0.6695046424865723\n",
      "epoch 1930, loss 0.8168483376502991, R2 0.6620840430259705\n",
      "Eval loss 0.846717357635498, R2 0.6695362329483032\n",
      "epoch 1931, loss 0.8168162107467651, R2 0.6621140837669373\n",
      "Eval loss 0.8466775417327881, R2 0.6695678234100342\n",
      "epoch 1932, loss 0.8167839050292969, R2 0.6621439456939697\n",
      "Eval loss 0.8466377854347229, R2 0.6695992946624756\n",
      "epoch 1933, loss 0.8167518377304077, R2 0.6621738076210022\n",
      "Eval loss 0.846598207950592, R2 0.6696308255195618\n",
      "epoch 1934, loss 0.8167198300361633, R2 0.6622037291526794\n",
      "Eval loss 0.8465585112571716, R2 0.6696620583534241\n",
      "epoch 1935, loss 0.816688060760498, R2 0.6622334122657776\n",
      "Eval loss 0.8465189933776855, R2 0.6696934103965759\n",
      "epoch 1936, loss 0.8166560530662537, R2 0.6622630953788757\n",
      "Eval loss 0.846479594707489, R2 0.6697246432304382\n",
      "epoch 1937, loss 0.8166242837905884, R2 0.6622926592826843\n",
      "Eval loss 0.8464401364326477, R2 0.6697558760643005\n",
      "epoch 1938, loss 0.8165925145149231, R2 0.6623223423957825\n",
      "Eval loss 0.8464009165763855, R2 0.6697869896888733\n",
      "epoch 1939, loss 0.8165609836578369, R2 0.6623518466949463\n",
      "Eval loss 0.8463616967201233, R2 0.6698180437088013\n",
      "epoch 1940, loss 0.816529393196106, R2 0.6623812317848206\n",
      "Eval loss 0.8463225960731506, R2 0.6698490381240845\n",
      "epoch 1941, loss 0.8164978623390198, R2 0.6624106764793396\n",
      "Eval loss 0.8462835550308228, R2 0.6698799729347229\n",
      "epoch 1942, loss 0.8164664506912231, R2 0.6624400019645691\n",
      "Eval loss 0.8462446928024292, R2 0.6699109077453613\n",
      "epoch 1943, loss 0.8164350390434265, R2 0.6624692678451538\n",
      "Eval loss 0.8462055921554565, R2 0.6699418425559998\n",
      "epoch 1944, loss 0.8164037466049194, R2 0.6624985337257385\n",
      "Eval loss 0.8461667895317078, R2 0.6699724793434143\n",
      "epoch 1945, loss 0.8163725137710571, R2 0.6625276803970337\n",
      "Eval loss 0.8461280465126038, R2 0.6700032353401184\n",
      "epoch 1946, loss 0.8163412809371948, R2 0.6625568270683289\n",
      "Eval loss 0.8460894227027893, R2 0.6700339317321777\n",
      "epoch 1947, loss 0.8163101673126221, R2 0.6625858545303345\n",
      "Eval loss 0.8460508584976196, R2 0.6700645685195923\n",
      "epoch 1948, loss 0.8162791132926941, R2 0.6626148819923401\n",
      "Eval loss 0.8460123538970947, R2 0.6700950264930725\n",
      "epoch 1949, loss 0.8162482380867004, R2 0.6626438498497009\n",
      "Eval loss 0.8459739089012146, R2 0.6701255440711975\n",
      "epoch 1950, loss 0.816217303276062, R2 0.6626726984977722\n",
      "Eval loss 0.8459355235099792, R2 0.6701560020446777\n",
      "epoch 1951, loss 0.8161864280700684, R2 0.6627015471458435\n",
      "Eval loss 0.8458972573280334, R2 0.6701862812042236\n",
      "epoch 1952, loss 0.816155731678009, R2 0.6627302765846252\n",
      "Eval loss 0.8458589911460876, R2 0.6702166199684143\n",
      "epoch 1953, loss 0.8161249756813049, R2 0.662759006023407\n",
      "Eval loss 0.845820963382721, R2 0.670246958732605\n",
      "epoch 1954, loss 0.8160942792892456, R2 0.6627876162528992\n",
      "Eval loss 0.8457829356193542, R2 0.6702771782875061\n",
      "epoch 1955, loss 0.8160637617111206, R2 0.6628162860870361\n",
      "Eval loss 0.8457449674606323, R2 0.6703072786331177\n",
      "epoch 1956, loss 0.8160333037376404, R2 0.6628448367118835\n",
      "Eval loss 0.8457070589065552, R2 0.670337438583374\n",
      "epoch 1957, loss 0.8160028457641602, R2 0.6628733277320862\n",
      "Eval loss 0.8456692099571228, R2 0.6703674793243408\n",
      "epoch 1958, loss 0.8159725069999695, R2 0.6629016995429993\n",
      "Eval loss 0.8456314206123352, R2 0.6703974604606628\n",
      "epoch 1959, loss 0.8159422278404236, R2 0.6629301309585571\n",
      "Eval loss 0.8455937504768372, R2 0.6704273223876953\n",
      "epoch 1960, loss 0.8159119486808777, R2 0.6629583835601807\n",
      "Eval loss 0.8455561995506287, R2 0.6704572439193726\n",
      "epoch 1961, loss 0.8158817291259766, R2 0.662986695766449\n",
      "Eval loss 0.8455186486244202, R2 0.6704869866371155\n",
      "epoch 1962, loss 0.8158517479896545, R2 0.6630148887634277\n",
      "Eval loss 0.8454811573028564, R2 0.6705167889595032\n",
      "epoch 1963, loss 0.815821647644043, R2 0.6630430817604065\n",
      "Eval loss 0.8454439043998718, R2 0.6705464720726013\n",
      "epoch 1964, loss 0.8157917261123657, R2 0.6630710959434509\n",
      "Eval loss 0.8454065918922424, R2 0.6705760955810547\n",
      "epoch 1965, loss 0.8157618045806885, R2 0.6630992293357849\n",
      "Eval loss 0.8453693389892578, R2 0.6706056594848633\n",
      "epoch 1966, loss 0.8157320022583008, R2 0.6631271839141846\n",
      "Eval loss 0.8453322052955627, R2 0.6706352829933167\n",
      "epoch 1967, loss 0.8157022595405579, R2 0.6631551384925842\n",
      "Eval loss 0.8452950716018677, R2 0.6706646680831909\n",
      "epoch 1968, loss 0.8156723976135254, R2 0.6631830334663391\n",
      "Eval loss 0.8452581763267517, R2 0.6706941723823547\n",
      "epoch 1969, loss 0.815642774105072, R2 0.6632108688354492\n",
      "Eval loss 0.845221221446991, R2 0.6707234382629395\n",
      "epoch 1970, loss 0.8156132698059082, R2 0.6632385849952698\n",
      "Eval loss 0.845184326171875, R2 0.6707528829574585\n",
      "epoch 1971, loss 0.8155837059020996, R2 0.6632663607597351\n",
      "Eval loss 0.8451474905014038, R2 0.6707820892333984\n",
      "epoch 1972, loss 0.8155542612075806, R2 0.6632940173149109\n",
      "Eval loss 0.8451108932495117, R2 0.6708112359046936\n",
      "epoch 1973, loss 0.8155248761177063, R2 0.6633215546607971\n",
      "Eval loss 0.8450742363929749, R2 0.6708404421806335\n",
      "epoch 1974, loss 0.8154955506324768, R2 0.6633491516113281\n",
      "Eval loss 0.8450376391410828, R2 0.6708695888519287\n",
      "epoch 1975, loss 0.8154662847518921, R2 0.6633767485618591\n",
      "Eval loss 0.8450011610984802, R2 0.6708984971046448\n",
      "epoch 1976, loss 0.8154371380805969, R2 0.663404107093811\n",
      "Eval loss 0.8449646830558777, R2 0.6709275841712952\n",
      "epoch 1977, loss 0.8154080510139465, R2 0.6634315848350525\n",
      "Eval loss 0.8449284434318542, R2 0.670956552028656\n",
      "epoch 1978, loss 0.8153789043426514, R2 0.6634589433670044\n",
      "Eval loss 0.8448920249938965, R2 0.6709854602813721\n",
      "epoch 1979, loss 0.8153497576713562, R2 0.6634861826896667\n",
      "Eval loss 0.8448558449745178, R2 0.6710142493247986\n",
      "epoch 1980, loss 0.8153209090232849, R2 0.6635133624076843\n",
      "Eval loss 0.8448197841644287, R2 0.6710430383682251\n",
      "epoch 1981, loss 0.8152920603752136, R2 0.6635406017303467\n",
      "Eval loss 0.8447837233543396, R2 0.6710717678070068\n",
      "epoch 1982, loss 0.8152632117271423, R2 0.6635677814483643\n",
      "Eval loss 0.8447476625442505, R2 0.671100378036499\n",
      "epoch 1983, loss 0.8152344822883606, R2 0.6635947823524475\n",
      "Eval loss 0.8447118401527405, R2 0.6711289882659912\n",
      "epoch 1984, loss 0.8152056932449341, R2 0.6636218428611755\n",
      "Eval loss 0.8446758985519409, R2 0.6711574792861938\n",
      "epoch 1985, loss 0.8151770234107971, R2 0.6636488437652588\n",
      "Eval loss 0.8446402549743652, R2 0.6711860299110413\n",
      "epoch 1986, loss 0.8151484131813049, R2 0.6636757254600525\n",
      "Eval loss 0.8446044325828552, R2 0.6712145209312439\n",
      "epoch 1987, loss 0.8151200413703918, R2 0.6637026071548462\n",
      "Eval loss 0.8445687890052795, R2 0.671242892742157\n",
      "epoch 1988, loss 0.815091609954834, R2 0.6637293696403503\n",
      "Eval loss 0.8445332050323486, R2 0.6712712645530701\n",
      "epoch 1989, loss 0.8150632381439209, R2 0.6637561917304993\n",
      "Eval loss 0.8444976806640625, R2 0.6712995171546936\n",
      "epoch 1990, loss 0.815034806728363, R2 0.6637828946113586\n",
      "Eval loss 0.8444622755050659, R2 0.6713278293609619\n",
      "epoch 1991, loss 0.8150065541267395, R2 0.6638095378875732\n",
      "Eval loss 0.8444268703460693, R2 0.6713559627532959\n",
      "epoch 1992, loss 0.8149785399436951, R2 0.6638361215591431\n",
      "Eval loss 0.8443917036056519, R2 0.6713840961456299\n",
      "epoch 1993, loss 0.8149502873420715, R2 0.6638627052307129\n",
      "Eval loss 0.8443565368652344, R2 0.6714122295379639\n",
      "epoch 1994, loss 0.8149222135543823, R2 0.6638892889022827\n",
      "Eval loss 0.8443213105201721, R2 0.6714401841163635\n",
      "epoch 1995, loss 0.8148941993713379, R2 0.6639156937599182\n",
      "Eval loss 0.8442862033843994, R2 0.6714682579040527\n",
      "epoch 1996, loss 0.8148661255836487, R2 0.6639420986175537\n",
      "Eval loss 0.8442511558532715, R2 0.6714961528778076\n",
      "epoch 1997, loss 0.8148382306098938, R2 0.6639685034751892\n",
      "Eval loss 0.8442162871360779, R2 0.6715240478515625\n",
      "epoch 1998, loss 0.814810574054718, R2 0.6639948487281799\n",
      "Eval loss 0.8441813588142395, R2 0.6715519428253174\n",
      "epoch 1999, loss 0.8147826790809631, R2 0.6640210747718811\n",
      "Eval loss 0.8441464900970459, R2 0.6715797185897827\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[idx, :]\n",
    "    train_y = train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    test_x = test_x[idx, :]\n",
    "    test_y = test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = lr_model(train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = lr_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = lr_model(train_x)\n",
    "    test_preds = lr_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch duration: 22.04s\n",
      "Epoch: 0 Train loss 2.778, Test loss 4.743 \n",
      "Training epoch duration: 20.56s\n",
      "Epoch: 1 Train loss 2.455, Test loss 4.393 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dynamics_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_normalizer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    115\u001b[0m     dynamics_model\u001b[38;5;241m.\u001b[39mupdate_normalizer(replay_buffer\u001b[38;5;241m.\u001b[39mget_all())\n\u001b[0;32m--> 116\u001b[0m train_losses, test_losses, train_metrics, test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/src/util/model_trainer.py:152\u001b[0m, in \u001b[0;36mModelTrainerOverriden.train\u001b[0;34m(self, dataset_train, dataset_val, num_epochs, patience, improvement_threshold, callback, batch_callback, evaluate, silent, debug)\u001b[0m\n\u001b[1;32m    150\u001b[0m batch_r2_scores: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataset_train, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm):\n\u001b[0;32m--> 152\u001b[0m     loss, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meta:\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/mbrl/models/one_dim_tr_model.py:202\u001b[0m, in \u001b[0;36mOneDTransitionRewardModel.update\u001b[0;34m(self, batch, optimizer, target)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    201\u001b[0m model_in, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_batch(batch)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/mbrl/models/model.py:159\u001b[0m, in \u001b[0;36mModel.update\u001b[0;34m(self, model_in, optimizer, target)\u001b[0m\n\u001b[1;32m    157\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    158\u001b[0m loss, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(model_in, target)\n\u001b[0;32m--> 159\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TEST with model trainer\n",
    "import omegaconf\n",
    "\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.util.model_trainer import ModelTrainerOverriden\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "import mbrl.util.common\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 1\n",
    "device = \"cpu\"\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "optim_lr=0.01 #learningRate\n",
    "model_wd=0.\n",
    "model_batch_size=256 #dataset_size\n",
    "validation_ratio=test_split_ratio\n",
    "num_epochs= 200 #epochs\n",
    "\n",
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "#Seed\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# torch_generator = torch.Generator(device=device)\n",
    "# if seed is not None:\n",
    "#     torch_generator.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#Dynamics model\n",
    "from src.model.simple import Simple\n",
    "from src.model.gaussian_process import MultiOutputGP\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    MultiOutputGP(in_size, out_size, device),\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "#Load replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    dataset_size,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=dataset_size)\n",
    "\n",
    "dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    model_batch_size,\n",
    "    validation_ratio,\n",
    "    ensemble_size=len(dynamics_model),\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,\n",
    ")\n",
    "\n",
    "if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "    dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "train_losses, test_losses, train_metrics, test_metrics = model_trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=num_epochs,\n",
    "    evaluate=True,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_model.model(train_x)\n",
    "    test_preds = dynamics_model.model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.simple import Simple\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.001\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "NN_model = Simple(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                NN_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 5.323896408081055, R2 -0.4018963575363159\n",
      "Eval loss 5.323896408081055, R2 -0.40800949931144714\n",
      "epoch 1, loss 4.939913272857666, R2 -0.25594255328178406\n",
      "Eval loss 4.939913272857666, R2 -0.18786725401878357\n",
      "epoch 2, loss 3.9092791080474854, R2 -0.16295231878757477\n",
      "Eval loss 3.9092791080474854, R2 -0.604037344455719\n",
      "epoch 3, loss 3.834420680999756, R2 -0.7803119421005249\n",
      "Eval loss 3.834420680999756, R2 -0.4473435580730438\n",
      "epoch 4, loss 3.371290445327759, R2 0.05224437639117241\n",
      "Eval loss 3.371290445327759, R2 0.07332278043031693\n",
      "epoch 5, loss 3.4254508018493652, R2 0.06424520909786224\n",
      "Eval loss 3.4254508018493652, R2 0.05064111575484276\n",
      "epoch 6, loss 3.348079204559326, R2 0.045901037752628326\n",
      "Eval loss 3.348079204559326, R2 0.02455156482756138\n",
      "epoch 7, loss 3.3848602771759033, R2 0.025363124907016754\n",
      "Eval loss 3.3848602771759033, R2 0.016881713643670082\n",
      "epoch 8, loss 3.3062312602996826, R2 0.046125736087560654\n",
      "Eval loss 3.3062312602996826, R2 0.04103560000658035\n",
      "epoch 9, loss 3.3096539974212646, R2 0.06484391540288925\n",
      "Eval loss 3.3096539974212646, R2 0.06700757145881653\n",
      "epoch 10, loss 3.2726452350616455, R2 0.09313616156578064\n",
      "Eval loss 3.2726452350616455, R2 0.08706977963447571\n",
      "epoch 11, loss 3.274146318435669, R2 0.09592190384864807\n",
      "Eval loss 3.274146318435669, R2 0.0883624330163002\n",
      "epoch 12, loss 3.2493398189544678, R2 0.10129399597644806\n",
      "Eval loss 3.2493398189544678, R2 0.09411159157752991\n",
      "epoch 13, loss 3.244776725769043, R2 0.10128099471330643\n",
      "Eval loss 3.244776725769043, R2 0.09449050575494766\n",
      "epoch 14, loss 3.229379415512085, R2 0.0999612808227539\n",
      "Eval loss 3.229379415512085, R2 0.09051227569580078\n",
      "epoch 15, loss 3.217719316482544, R2 0.10154340416193008\n",
      "Eval loss 3.217719316482544, R2 0.09678056836128235\n",
      "epoch 16, loss 3.203242301940918, R2 0.10683426260948181\n",
      "Eval loss 3.203242301940918, R2 0.10137978196144104\n",
      "epoch 17, loss 3.179955244064331, R2 0.11137198656797409\n",
      "Eval loss 3.179955244064331, R2 0.10453293472528458\n",
      "epoch 18, loss 3.1541330814361572, R2 0.1158190667629242\n",
      "Eval loss 3.1541330814361572, R2 0.10933423042297363\n",
      "epoch 19, loss 3.118581533432007, R2 0.1223062202334404\n",
      "Eval loss 3.118581533432007, R2 0.11584433913230896\n",
      "epoch 20, loss 3.073164939880371, R2 0.1296682357788086\n",
      "Eval loss 3.073164939880371, R2 0.12321216613054276\n",
      "epoch 21, loss 3.019923686981201, R2 0.13729651272296906\n",
      "Eval loss 3.019923686981201, R2 0.13068436086177826\n",
      "epoch 22, loss 2.9619507789611816, R2 0.1448778659105301\n",
      "Eval loss 2.9619507789611816, R2 0.13588067889213562\n",
      "epoch 23, loss 2.901885986328125, R2 0.1522626429796219\n",
      "Eval loss 2.901885986328125, R2 0.14075520634651184\n",
      "epoch 24, loss 2.8387866020202637, R2 0.1590794175863266\n",
      "Eval loss 2.8387866020202637, R2 0.1432419866323471\n",
      "epoch 25, loss 2.7747511863708496, R2 0.16255496442317963\n",
      "Eval loss 2.7747511863708496, R2 0.14380034804344177\n",
      "epoch 26, loss 2.718090772628784, R2 0.16843855381011963\n",
      "Eval loss 2.718090772628784, R2 0.14861327409744263\n",
      "epoch 27, loss 2.663081169128418, R2 0.17842388153076172\n",
      "Eval loss 2.663081169128418, R2 0.16062414646148682\n",
      "epoch 28, loss 2.5945379734039307, R2 0.18962378799915314\n",
      "Eval loss 2.5945379734039307, R2 0.1713990420103073\n",
      "epoch 29, loss 2.5146071910858154, R2 0.19438162446022034\n",
      "Eval loss 2.5146071910858154, R2 0.17525352537631989\n",
      "epoch 30, loss 2.4349544048309326, R2 0.1865132749080658\n",
      "Eval loss 2.4349544048309326, R2 0.1679902970790863\n",
      "epoch 31, loss 2.3619654178619385, R2 0.1735285520553589\n",
      "Eval loss 2.3619654178619385, R2 0.15650413930416107\n",
      "epoch 32, loss 2.2857470512390137, R2 0.17348119616508484\n",
      "Eval loss 2.2857470512390137, R2 0.161806121468544\n",
      "epoch 33, loss 2.203887701034546, R2 0.20126689970493317\n",
      "Eval loss 2.203887701034546, R2 0.18508559465408325\n",
      "epoch 34, loss 2.128127336502075, R2 0.23836053907871246\n",
      "Eval loss 2.128127336502075, R2 0.21246324479579926\n",
      "epoch 35, loss 2.080873727798462, R2 0.26577746868133545\n",
      "Eval loss 2.080873727798462, R2 0.2374952882528305\n",
      "epoch 36, loss 2.0152153968811035, R2 0.2957301139831543\n",
      "Eval loss 2.0152153968811035, R2 0.2600712180137634\n",
      "epoch 37, loss 1.9534615278244019, R2 0.3193061351776123\n",
      "Eval loss 1.9534615278244019, R2 0.2811076045036316\n",
      "epoch 38, loss 1.8927006721496582, R2 0.3386382758617401\n",
      "Eval loss 1.8927006721496582, R2 0.297244131565094\n",
      "epoch 39, loss 1.8345057964324951, R2 0.35320404171943665\n",
      "Eval loss 1.8345057964324951, R2 0.30929529666900635\n",
      "epoch 40, loss 1.7798044681549072, R2 0.3646615147590637\n",
      "Eval loss 1.7798044681549072, R2 0.31807568669319153\n",
      "epoch 41, loss 1.7277518510818481, R2 0.37430083751678467\n",
      "Eval loss 1.7277518510818481, R2 0.32576608657836914\n",
      "epoch 42, loss 1.6763761043548584, R2 0.38417014479637146\n",
      "Eval loss 1.6763761043548584, R2 0.3332109749317169\n",
      "epoch 43, loss 1.6294513940811157, R2 0.39333537220954895\n",
      "Eval loss 1.6294513940811157, R2 0.34179359674453735\n",
      "epoch 44, loss 1.5898447036743164, R2 0.40245047211647034\n",
      "Eval loss 1.5898447036743164, R2 0.34894871711730957\n",
      "epoch 45, loss 1.5673490762710571, R2 0.4086456000804901\n",
      "Eval loss 1.5673490762710571, R2 0.35792800784111023\n",
      "epoch 46, loss 1.5222132205963135, R2 0.42431479692459106\n",
      "Eval loss 1.5222132205963135, R2 0.36976879835128784\n",
      "epoch 47, loss 1.4904708862304688, R2 0.43697643280029297\n",
      "Eval loss 1.4904708862304688, R2 0.3806411623954773\n",
      "epoch 48, loss 1.4608508348464966, R2 0.4470244348049164\n",
      "Eval loss 1.4608508348464966, R2 0.38920536637306213\n",
      "epoch 49, loss 1.4321855306625366, R2 0.45608389377593994\n",
      "Eval loss 1.4321855306625366, R2 0.39561206102371216\n",
      "epoch 50, loss 1.4046671390533447, R2 0.4640016555786133\n",
      "Eval loss 1.4046671390533447, R2 0.4021168649196625\n",
      "epoch 51, loss 1.3781150579452515, R2 0.4720873534679413\n",
      "Eval loss 1.3781150579452515, R2 0.40889090299606323\n",
      "epoch 52, loss 1.3529021739959717, R2 0.4791528880596161\n",
      "Eval loss 1.3529021739959717, R2 0.41484129428863525\n",
      "epoch 53, loss 1.3292691707611084, R2 0.485740065574646\n",
      "Eval loss 1.3292691707611084, R2 0.4205494225025177\n",
      "epoch 54, loss 1.3146171569824219, R2 0.4905272424221039\n",
      "Eval loss 1.3146171569824219, R2 0.42228251695632935\n",
      "epoch 55, loss 1.3063105344772339, R2 0.49425724148750305\n",
      "Eval loss 1.3063105344772339, R2 0.43045738339424133\n",
      "epoch 56, loss 1.2769826650619507, R2 0.5019212961196899\n",
      "Eval loss 1.2769826650619507, R2 0.43437862396240234\n",
      "epoch 57, loss 1.2493157386779785, R2 0.5094982981681824\n",
      "Eval loss 1.2493157386779785, R2 0.43972378969192505\n",
      "epoch 58, loss 1.2260041236877441, R2 0.51665860414505\n",
      "Eval loss 1.2260041236877441, R2 0.444802463054657\n",
      "epoch 59, loss 1.2041126489639282, R2 0.5230696797370911\n",
      "Eval loss 1.2041126489639282, R2 0.45031702518463135\n",
      "epoch 60, loss 1.183255910873413, R2 0.5289703607559204\n",
      "Eval loss 1.183255910873413, R2 0.45689570903778076\n",
      "epoch 61, loss 1.1633212566375732, R2 0.53413325548172\n",
      "Eval loss 1.1633212566375732, R2 0.4638504683971405\n",
      "epoch 62, loss 1.1434763669967651, R2 0.5392552614212036\n",
      "Eval loss 1.1434763669967651, R2 0.470482736825943\n",
      "epoch 63, loss 1.1232273578643799, R2 0.5444135069847107\n",
      "Eval loss 1.1232273578643799, R2 0.4752771556377411\n",
      "epoch 64, loss 1.103455662727356, R2 0.5493125915527344\n",
      "Eval loss 1.103455662727356, R2 0.479763001203537\n",
      "epoch 65, loss 1.0856794118881226, R2 0.5536301136016846\n",
      "Eval loss 1.0856794118881226, R2 0.4829864501953125\n",
      "epoch 66, loss 1.0682677030563354, R2 0.5581094026565552\n",
      "Eval loss 1.0682677030563354, R2 0.4865805506706238\n",
      "epoch 67, loss 1.0530729293823242, R2 0.5621321797370911\n",
      "Eval loss 1.0530729293823242, R2 0.4915057420730591\n",
      "epoch 68, loss 1.057271957397461, R2 0.5610286593437195\n",
      "Eval loss 1.057271957397461, R2 0.47411927580833435\n",
      "epoch 69, loss 1.0513882637023926, R2 0.5636657476425171\n",
      "Eval loss 1.0513882637023926, R2 0.4887337386608124\n",
      "epoch 70, loss 1.026208519935608, R2 0.5701600909233093\n",
      "Eval loss 1.026208519935608, R2 0.4944770038127899\n",
      "epoch 71, loss 1.0088527202606201, R2 0.574737548828125\n",
      "Eval loss 1.0088527202606201, R2 0.49353474378585815\n",
      "epoch 72, loss 1.0017268657684326, R2 0.5757549405097961\n",
      "Eval loss 1.0017268657684326, R2 0.498392790555954\n",
      "epoch 73, loss 0.9897353053092957, R2 0.578517735004425\n",
      "Eval loss 0.9897353053092957, R2 0.5042902827262878\n",
      "epoch 74, loss 0.9691761136054993, R2 0.5838080048561096\n",
      "Eval loss 0.9691761136054993, R2 0.5062525272369385\n",
      "epoch 75, loss 0.9575741291046143, R2 0.5867404937744141\n",
      "Eval loss 0.9575741291046143, R2 0.507879376411438\n",
      "epoch 76, loss 0.9496657252311707, R2 0.5880671143531799\n",
      "Eval loss 0.9496657252311707, R2 0.5100837349891663\n",
      "epoch 77, loss 0.937531054019928, R2 0.5907403826713562\n",
      "Eval loss 0.937531054019928, R2 0.511113703250885\n",
      "epoch 78, loss 0.9254652857780457, R2 0.5937778949737549\n",
      "Eval loss 0.9254652857780457, R2 0.5121618509292603\n",
      "epoch 79, loss 0.915941596031189, R2 0.5966508388519287\n",
      "Eval loss 0.915941596031189, R2 0.5139744877815247\n",
      "epoch 80, loss 0.9070864915847778, R2 0.6000364422798157\n",
      "Eval loss 0.9070864915847778, R2 0.5171797275543213\n",
      "epoch 81, loss 0.8979930281639099, R2 0.6040488481521606\n",
      "Eval loss 0.8979930281639099, R2 0.520541787147522\n",
      "epoch 82, loss 0.8898810148239136, R2 0.6078987717628479\n",
      "Eval loss 0.8898810148239136, R2 0.5219663381576538\n",
      "epoch 83, loss 0.8906905651092529, R2 0.609902560710907\n",
      "Eval loss 0.8906905651092529, R2 0.5237615704536438\n",
      "epoch 84, loss 0.8909624814987183, R2 0.6126012206077576\n",
      "Eval loss 0.8909624814987183, R2 0.5273167490959167\n",
      "epoch 85, loss 0.8700068593025208, R2 0.6190387010574341\n",
      "Eval loss 0.8700068593025208, R2 0.528329074382782\n",
      "epoch 86, loss 0.8594485521316528, R2 0.622977614402771\n",
      "Eval loss 0.8594485521316528, R2 0.5326259136199951\n",
      "epoch 87, loss 0.8545351028442383, R2 0.625688374042511\n",
      "Eval loss 0.8545351028442383, R2 0.5358755588531494\n",
      "epoch 88, loss 0.8460226058959961, R2 0.6289631724357605\n",
      "Eval loss 0.8460226058959961, R2 0.5385809540748596\n",
      "epoch 89, loss 0.8372164368629456, R2 0.6322245001792908\n",
      "Eval loss 0.8372164368629456, R2 0.5412968993186951\n",
      "epoch 90, loss 0.8296207189559937, R2 0.6352847814559937\n",
      "Eval loss 0.8296207189559937, R2 0.5434436798095703\n",
      "epoch 91, loss 0.8221973776817322, R2 0.6381932497024536\n",
      "Eval loss 0.8221973776817322, R2 0.5451253056526184\n",
      "epoch 92, loss 0.815433144569397, R2 0.6407566666603088\n",
      "Eval loss 0.815433144569397, R2 0.5468294024467468\n",
      "epoch 93, loss 0.808121919631958, R2 0.6433449983596802\n",
      "Eval loss 0.808121919631958, R2 0.5492523908615112\n",
      "epoch 94, loss 0.8009641170501709, R2 0.645818293094635\n",
      "Eval loss 0.8009641170501709, R2 0.5524076223373413\n",
      "epoch 95, loss 0.7945981621742249, R2 0.6481678485870361\n",
      "Eval loss 0.7945981621742249, R2 0.5542228817939758\n",
      "epoch 96, loss 0.7877450585365295, R2 0.6507458090782166\n",
      "Eval loss 0.7877450585365295, R2 0.5559375286102295\n",
      "epoch 97, loss 0.7826557159423828, R2 0.6527777314186096\n",
      "Eval loss 0.7826557159423828, R2 0.5566662549972534\n",
      "epoch 98, loss 0.7839885950088501, R2 0.652704656124115\n",
      "Eval loss 0.7839885950088501, R2 0.5549930930137634\n",
      "epoch 99, loss 0.7871474623680115, R2 0.6531407833099365\n",
      "Eval loss 0.7871474623680115, R2 0.5580767393112183\n",
      "epoch 100, loss 0.764458954334259, R2 0.6592950820922852\n",
      "Eval loss 0.764458954334259, R2 0.5611475706100464\n",
      "epoch 101, loss 0.764141857624054, R2 0.6602085828781128\n",
      "Eval loss 0.764141857624054, R2 0.5634884834289551\n",
      "epoch 102, loss 0.7570518851280212, R2 0.6626288890838623\n",
      "Eval loss 0.7570518851280212, R2 0.5631715655326843\n",
      "epoch 103, loss 0.7475873827934265, R2 0.665862500667572\n",
      "Eval loss 0.7475873827934265, R2 0.565822422504425\n",
      "epoch 104, loss 0.7456557750701904, R2 0.6669067144393921\n",
      "Eval loss 0.7456557750701904, R2 0.5683081150054932\n",
      "epoch 105, loss 0.7383595108985901, R2 0.6694031357765198\n",
      "Eval loss 0.7383595108985901, R2 0.5697497725486755\n",
      "epoch 106, loss 0.7330517172813416, R2 0.6714367270469666\n",
      "Eval loss 0.7330517172813416, R2 0.5705475211143494\n",
      "epoch 107, loss 0.7291679382324219, R2 0.673268735408783\n",
      "Eval loss 0.7291679382324219, R2 0.5718381404876709\n",
      "epoch 108, loss 0.7237625122070312, R2 0.6753512024879456\n",
      "Eval loss 0.7237625122070312, R2 0.5719278454780579\n",
      "epoch 109, loss 0.7258654832839966, R2 0.6751456260681152\n",
      "Eval loss 0.7258654832839966, R2 0.5730535984039307\n",
      "epoch 110, loss 0.7362929582595825, R2 0.673335611820221\n",
      "Eval loss 0.7362929582595825, R2 0.5704785585403442\n",
      "epoch 111, loss 0.7120051980018616, R2 0.6803301572799683\n",
      "Eval loss 0.7120051980018616, R2 0.5758534073829651\n",
      "epoch 112, loss 0.7165605425834656, R2 0.6796295642852783\n",
      "Eval loss 0.7165605425834656, R2 0.5779114961624146\n",
      "epoch 113, loss 0.7023780941963196, R2 0.6840437054634094\n",
      "Eval loss 0.7023780941963196, R2 0.5788543224334717\n",
      "epoch 114, loss 0.6981338858604431, R2 0.6859337687492371\n",
      "Eval loss 0.6981338858604431, R2 0.5806100368499756\n",
      "epoch 115, loss 0.6971319317817688, R2 0.6868206858634949\n",
      "Eval loss 0.6971319317817688, R2 0.5817395448684692\n",
      "epoch 116, loss 0.6898270845413208, R2 0.6894563436508179\n",
      "Eval loss 0.6898270845413208, R2 0.582738995552063\n",
      "epoch 117, loss 0.685862123966217, R2 0.690971314907074\n",
      "Eval loss 0.685862123966217, R2 0.5857323408126831\n",
      "epoch 118, loss 0.6851764917373657, R2 0.6920230388641357\n",
      "Eval loss 0.6851764917373657, R2 0.5835645794868469\n",
      "epoch 119, loss 0.6847144961357117, R2 0.6928414106369019\n",
      "Eval loss 0.6847144961357117, R2 0.586185872554779\n",
      "epoch 120, loss 0.6822620630264282, R2 0.6944797039031982\n",
      "Eval loss 0.6822620630264282, R2 0.5857219099998474\n",
      "epoch 121, loss 0.6700987815856934, R2 0.6975351572036743\n",
      "Eval loss 0.6700987815856934, R2 0.588833749294281\n",
      "epoch 122, loss 0.6692997217178345, R2 0.6984370946884155\n",
      "Eval loss 0.6692997217178345, R2 0.5905352234840393\n",
      "epoch 123, loss 0.6648091077804565, R2 0.7003785967826843\n",
      "Eval loss 0.6648091077804565, R2 0.5908278822898865\n",
      "epoch 124, loss 0.6580916047096252, R2 0.7025064826011658\n",
      "Eval loss 0.6580916047096252, R2 0.5915960073471069\n",
      "epoch 125, loss 0.6565075516700745, R2 0.7034061551094055\n",
      "Eval loss 0.6565075516700745, R2 0.5932669043540955\n",
      "epoch 126, loss 0.6527806520462036, R2 0.70510333776474\n",
      "Eval loss 0.6527806520462036, R2 0.5932124853134155\n",
      "epoch 127, loss 0.6474490165710449, R2 0.7069182395935059\n",
      "Eval loss 0.6474490165710449, R2 0.5954118967056274\n",
      "epoch 128, loss 0.6442403793334961, R2 0.7082472443580627\n",
      "Eval loss 0.6442403793334961, R2 0.5967265367507935\n",
      "epoch 129, loss 0.6465064287185669, R2 0.708191454410553\n",
      "Eval loss 0.6465064287185669, R2 0.590429961681366\n",
      "epoch 130, loss 0.6565355062484741, R2 0.7048096060752869\n",
      "Eval loss 0.6565355062484741, R2 0.5940365195274353\n",
      "epoch 131, loss 0.6373541951179504, R2 0.7116726636886597\n",
      "Eval loss 0.6373541951179504, R2 0.5995654463768005\n",
      "epoch 132, loss 0.6402451992034912, R2 0.7115144729614258\n",
      "Eval loss 0.6402451992034912, R2 0.593711793422699\n",
      "epoch 133, loss 0.6280579566955566, R2 0.7151334285736084\n",
      "Eval loss 0.6280579566955566, R2 0.598821222782135\n",
      "epoch 134, loss 0.6294451951980591, R2 0.7151148915290833\n",
      "Eval loss 0.6294451951980591, R2 0.6009334325790405\n",
      "epoch 135, loss 0.6230270862579346, R2 0.717326819896698\n",
      "Eval loss 0.6230270862579346, R2 0.6019616723060608\n",
      "epoch 136, loss 0.6218708753585815, R2 0.7187031507492065\n",
      "Eval loss 0.6218708753585815, R2 0.6005563139915466\n",
      "epoch 137, loss 0.6226549744606018, R2 0.7188059687614441\n",
      "Eval loss 0.6226549744606018, R2 0.6010363698005676\n",
      "epoch 138, loss 0.6204931139945984, R2 0.7200940847396851\n",
      "Eval loss 0.6204931139945984, R2 0.6004945635795593\n",
      "epoch 139, loss 0.6130757331848145, R2 0.7220559120178223\n",
      "Eval loss 0.6130757331848145, R2 0.6047835946083069\n",
      "epoch 140, loss 0.6071860790252686, R2 0.7240672707557678\n",
      "Eval loss 0.6071860790252686, R2 0.605248212814331\n",
      "epoch 141, loss 0.6067325472831726, R2 0.7249943614006042\n",
      "Eval loss 0.6067325472831726, R2 0.6038822531700134\n",
      "epoch 142, loss 0.6049824953079224, R2 0.7259334325790405\n",
      "Eval loss 0.6049824953079224, R2 0.6056109666824341\n",
      "epoch 143, loss 0.5995714664459229, R2 0.727827250957489\n",
      "Eval loss 0.5995714664459229, R2 0.605579137802124\n",
      "epoch 144, loss 0.5963729619979858, R2 0.7289729714393616\n",
      "Eval loss 0.5963729619979858, R2 0.6070935130119324\n",
      "epoch 145, loss 0.5935474038124084, R2 0.7302812337875366\n",
      "Eval loss 0.5935474038124084, R2 0.6080451607704163\n",
      "epoch 146, loss 0.5931792855262756, R2 0.7309443950653076\n",
      "Eval loss 0.5931792855262756, R2 0.6052700877189636\n",
      "epoch 147, loss 0.5941428542137146, R2 0.731141984462738\n",
      "Eval loss 0.5941428542137146, R2 0.6082279682159424\n",
      "epoch 148, loss 0.6001178622245789, R2 0.7306573390960693\n",
      "Eval loss 0.6001178622245789, R2 0.6040398478507996\n",
      "epoch 149, loss 0.5950829982757568, R2 0.7317952513694763\n",
      "Eval loss 0.5950829982757568, R2 0.6078828573226929\n",
      "epoch 150, loss 0.5834910869598389, R2 0.7352152466773987\n",
      "Eval loss 0.5834910869598389, R2 0.6093364357948303\n",
      "epoch 151, loss 0.5859310626983643, R2 0.7353220582008362\n",
      "Eval loss 0.5859310626983643, R2 0.6082839369773865\n",
      "epoch 152, loss 0.5786685943603516, R2 0.7373227477073669\n",
      "Eval loss 0.5786685943603516, R2 0.6086265444755554\n",
      "epoch 153, loss 0.5797899961471558, R2 0.7375152707099915\n",
      "Eval loss 0.5797899961471558, R2 0.6109704375267029\n",
      "epoch 154, loss 0.5741211175918579, R2 0.7393850684165955\n",
      "Eval loss 0.5741211175918579, R2 0.6099424362182617\n",
      "epoch 155, loss 0.5737914443016052, R2 0.7400661706924438\n",
      "Eval loss 0.5737914443016052, R2 0.6086459755897522\n",
      "epoch 156, loss 0.5733327865600586, R2 0.7402286529541016\n",
      "Eval loss 0.5733327865600586, R2 0.6106548309326172\n",
      "epoch 157, loss 0.5720753073692322, R2 0.7409208416938782\n",
      "Eval loss 0.5720753073692322, R2 0.6099362969398499\n",
      "epoch 158, loss 0.5695419311523438, R2 0.7423359155654907\n",
      "Eval loss 0.5695419311523438, R2 0.6094371676445007\n",
      "epoch 159, loss 0.5632818341255188, R2 0.7441337704658508\n",
      "Eval loss 0.5632818341255188, R2 0.611998975276947\n",
      "epoch 160, loss 0.5616804957389832, R2 0.7450006008148193\n",
      "Eval loss 0.5616804957389832, R2 0.6115602850914001\n",
      "epoch 161, loss 0.5616790652275085, R2 0.7453765869140625\n",
      "Eval loss 0.5616790652275085, R2 0.6113184094429016\n",
      "epoch 162, loss 0.5585418343544006, R2 0.7465200424194336\n",
      "Eval loss 0.5585418343544006, R2 0.6116296052932739\n",
      "epoch 163, loss 0.5568497776985168, R2 0.7473488450050354\n",
      "Eval loss 0.5568497776985168, R2 0.6123142838478088\n",
      "epoch 164, loss 0.5633541345596313, R2 0.7462732791900635\n",
      "Eval loss 0.5633541345596313, R2 0.606842041015625\n",
      "epoch 165, loss 0.5663480162620544, R2 0.7457435727119446\n",
      "Eval loss 0.5663480162620544, R2 0.6121374368667603\n",
      "epoch 166, loss 0.5502400398254395, R2 0.7499896883964539\n",
      "Eval loss 0.5502400398254395, R2 0.6120137572288513\n",
      "epoch 167, loss 0.5553320050239563, R2 0.7489804625511169\n",
      "Eval loss 0.5553320050239563, R2 0.6118378043174744\n",
      "epoch 168, loss 0.5460079312324524, R2 0.7517673969268799\n",
      "Eval loss 0.5460079312324524, R2 0.6118049621582031\n",
      "epoch 169, loss 0.5475510954856873, R2 0.7517048120498657\n",
      "Eval loss 0.5475510954856873, R2 0.6130427718162537\n",
      "epoch 170, loss 0.542102038860321, R2 0.7533596158027649\n",
      "Eval loss 0.542102038860321, R2 0.6133285164833069\n",
      "epoch 171, loss 0.5422391295433044, R2 0.7537450790405273\n",
      "Eval loss 0.5422391295433044, R2 0.6123206615447998\n",
      "epoch 172, loss 0.5391162037849426, R2 0.7547205090522766\n",
      "Eval loss 0.5391162037849426, R2 0.6137222051620483\n",
      "epoch 173, loss 0.5426000356674194, R2 0.7543861865997314\n",
      "Eval loss 0.5426000356674194, R2 0.6084580421447754\n",
      "epoch 174, loss 0.5566002130508423, R2 0.7506089806556702\n",
      "Eval loss 0.5566002130508423, R2 0.6110672950744629\n",
      "epoch 175, loss 0.5431624054908752, R2 0.7548589110374451\n",
      "Eval loss 0.5431624054908752, R2 0.6127561926841736\n",
      "epoch 176, loss 0.5416568517684937, R2 0.755576491355896\n",
      "Eval loss 0.5416568517684937, R2 0.6085864305496216\n",
      "epoch 177, loss 0.532210111618042, R2 0.7580487728118896\n",
      "Eval loss 0.532210111618042, R2 0.6128535866737366\n",
      "epoch 178, loss 0.5362716913223267, R2 0.757358193397522\n",
      "Eval loss 0.5362716913223267, R2 0.6140934228897095\n",
      "epoch 179, loss 0.5272506475448608, R2 0.7599210739135742\n",
      "Eval loss 0.5272506475448608, R2 0.6134873032569885\n",
      "epoch 180, loss 0.5280812382698059, R2 0.7600468993186951\n",
      "Eval loss 0.5280812382698059, R2 0.6127235889434814\n",
      "epoch 181, loss 0.5249626636505127, R2 0.7611211538314819\n",
      "Eval loss 0.5249626636505127, R2 0.6128045916557312\n",
      "epoch 182, loss 0.522704005241394, R2 0.7618763446807861\n",
      "Eval loss 0.522704005241394, R2 0.6144971251487732\n",
      "epoch 183, loss 0.5241965651512146, R2 0.7621479630470276\n",
      "Eval loss 0.5241965651512146, R2 0.6132158637046814\n",
      "epoch 184, loss 0.5328370928764343, R2 0.7605343461036682\n",
      "Eval loss 0.5328370928764343, R2 0.6093154549598694\n",
      "epoch 185, loss 0.5381304621696472, R2 0.759105920791626\n",
      "Eval loss 0.5381304621696472, R2 0.611727774143219\n",
      "epoch 186, loss 0.5174232721328735, R2 0.764578104019165\n",
      "Eval loss 0.5174232721328735, R2 0.6114003658294678\n",
      "epoch 187, loss 0.522409200668335, R2 0.7639207243919373\n",
      "Eval loss 0.522409200668335, R2 0.6136678457260132\n",
      "epoch 188, loss 0.5163769721984863, R2 0.7655301690101624\n",
      "Eval loss 0.5163769721984863, R2 0.6136270761489868\n",
      "epoch 189, loss 0.5129250288009644, R2 0.7666326761245728\n",
      "Eval loss 0.5129250288009644, R2 0.6143323183059692\n",
      "epoch 190, loss 0.5117366313934326, R2 0.7674190402030945\n",
      "Eval loss 0.5117366313934326, R2 0.6133215427398682\n",
      "epoch 191, loss 0.5096736550331116, R2 0.76809161901474\n",
      "Eval loss 0.5096736550331116, R2 0.6139703392982483\n",
      "epoch 192, loss 0.506104588508606, R2 0.7693831920623779\n",
      "Eval loss 0.506104588508606, R2 0.6141618490219116\n",
      "epoch 193, loss 0.5057572722434998, R2 0.7697622179985046\n",
      "Eval loss 0.5057572722434998, R2 0.6150574088096619\n",
      "epoch 194, loss 0.5023719668388367, R2 0.7710546851158142\n",
      "Eval loss 0.5023719668388367, R2 0.614185094833374\n",
      "epoch 195, loss 0.5030317306518555, R2 0.7709773778915405\n",
      "Eval loss 0.5030317306518555, R2 0.6157346367835999\n",
      "epoch 196, loss 0.5031473636627197, R2 0.771536648273468\n",
      "Eval loss 0.5031473636627197, R2 0.6119280457496643\n",
      "epoch 197, loss 0.5048761367797852, R2 0.7711465954780579\n",
      "Eval loss 0.5048761367797852, R2 0.6147124767303467\n",
      "epoch 198, loss 0.5012946128845215, R2 0.7727788686752319\n",
      "Eval loss 0.5012946128845215, R2 0.6141765117645264\n",
      "epoch 199, loss 0.4963555335998535, R2 0.7741556763648987\n",
      "Eval loss 0.4963555335998535, R2 0.613586962223053\n",
      "epoch 200, loss 0.5017609000205994, R2 0.7725799679756165\n",
      "Eval loss 0.5017609000205994, R2 0.6142296195030212\n",
      "epoch 201, loss 0.5003282427787781, R2 0.7730388641357422\n",
      "Eval loss 0.5003282427787781, R2 0.6130625605583191\n",
      "epoch 202, loss 0.4906190037727356, R2 0.7766116857528687\n",
      "Eval loss 0.4906190037727356, R2 0.6142947673797607\n",
      "epoch 203, loss 0.4924263656139374, R2 0.7763965129852295\n",
      "Eval loss 0.4924263656139374, R2 0.6158540844917297\n",
      "epoch 204, loss 0.4881938695907593, R2 0.7777227759361267\n",
      "Eval loss 0.4881938695907593, R2 0.6143835783004761\n",
      "epoch 205, loss 0.48580431938171387, R2 0.7787010669708252\n",
      "Eval loss 0.48580431938171387, R2 0.6156507134437561\n",
      "epoch 206, loss 0.48498791456222534, R2 0.7792825102806091\n",
      "Eval loss 0.48498791456222534, R2 0.6163796186447144\n",
      "epoch 207, loss 0.482687771320343, R2 0.7799810767173767\n",
      "Eval loss 0.482687771320343, R2 0.6139929890632629\n",
      "epoch 208, loss 0.4818885326385498, R2 0.7803863883018494\n",
      "Eval loss 0.4818885326385498, R2 0.6168259978294373\n",
      "epoch 209, loss 0.4829045534133911, R2 0.7806764245033264\n",
      "Eval loss 0.4829045534133911, R2 0.6130691170692444\n",
      "epoch 210, loss 0.4833565354347229, R2 0.7808569073677063\n",
      "Eval loss 0.4833565354347229, R2 0.6149429678916931\n",
      "epoch 211, loss 0.48227599263191223, R2 0.7815501689910889\n",
      "Eval loss 0.48227599263191223, R2 0.6148975491523743\n",
      "epoch 212, loss 0.4783133268356323, R2 0.782740592956543\n",
      "Eval loss 0.4783133268356323, R2 0.6147423982620239\n",
      "epoch 213, loss 0.4751035273075104, R2 0.7834722399711609\n",
      "Eval loss 0.4751035273075104, R2 0.6147179007530212\n",
      "epoch 214, loss 0.4744203984737396, R2 0.7838819622993469\n",
      "Eval loss 0.4744203984737396, R2 0.6153340935707092\n",
      "epoch 215, loss 0.47173771262168884, R2 0.7850738167762756\n",
      "Eval loss 0.47173771262168884, R2 0.6161622405052185\n",
      "epoch 216, loss 0.4733773469924927, R2 0.7842181921005249\n",
      "Eval loss 0.4733773469924927, R2 0.6099509596824646\n",
      "epoch 217, loss 0.4751660227775574, R2 0.7841621041297913\n",
      "Eval loss 0.4751660227775574, R2 0.6167038083076477\n",
      "epoch 218, loss 0.46984636783599854, R2 0.7864070534706116\n",
      "Eval loss 0.46984636783599854, R2 0.6143772602081299\n",
      "epoch 219, loss 0.4681015908718109, R2 0.7868850231170654\n",
      "Eval loss 0.4681015908718109, R2 0.6136981844902039\n",
      "epoch 220, loss 0.46526435017585754, R2 0.7880221009254456\n",
      "Eval loss 0.46526435017585754, R2 0.6159533858299255\n",
      "epoch 221, loss 0.4656476378440857, R2 0.7879704833030701\n",
      "Eval loss 0.4656476378440857, R2 0.6137171387672424\n",
      "epoch 222, loss 0.46518218517303467, R2 0.7882909178733826\n",
      "Eval loss 0.46518218517303467, R2 0.6157739162445068\n",
      "epoch 223, loss 0.4602530300617218, R2 0.7903931140899658\n",
      "Eval loss 0.4602530300617218, R2 0.6141651272773743\n",
      "epoch 224, loss 0.46284806728363037, R2 0.7896131873130798\n",
      "Eval loss 0.46284806728363037, R2 0.6146784424781799\n",
      "epoch 225, loss 0.4649321436882019, R2 0.7894787192344666\n",
      "Eval loss 0.4649321436882019, R2 0.612896740436554\n",
      "epoch 226, loss 0.4647602438926697, R2 0.7891084551811218\n",
      "Eval loss 0.4647602438926697, R2 0.6141257882118225\n",
      "epoch 227, loss 0.4635889232158661, R2 0.7904603481292725\n",
      "Eval loss 0.4635889232158661, R2 0.6096329092979431\n",
      "epoch 228, loss 0.45671898126602173, R2 0.7923049926757812\n",
      "Eval loss 0.45671898126602173, R2 0.6160881519317627\n",
      "epoch 229, loss 0.4538480341434479, R2 0.7932042479515076\n",
      "Eval loss 0.4538480341434479, R2 0.6166670322418213\n",
      "epoch 230, loss 0.4560944139957428, R2 0.7929084300994873\n",
      "Eval loss 0.4560944139957428, R2 0.6132773756980896\n",
      "epoch 231, loss 0.44871053099632263, R2 0.7950830459594727\n",
      "Eval loss 0.44871053099632263, R2 0.6136593222618103\n",
      "epoch 232, loss 0.4504335820674896, R2 0.7949421405792236\n",
      "Eval loss 0.4504335820674896, R2 0.6165236830711365\n",
      "epoch 233, loss 0.4471411108970642, R2 0.7961919903755188\n",
      "Eval loss 0.4471411108970642, R2 0.6155123710632324\n",
      "epoch 234, loss 0.44733232259750366, R2 0.7964505553245544\n",
      "Eval loss 0.44733232259750366, R2 0.6149525046348572\n",
      "epoch 235, loss 0.4460309147834778, R2 0.7968513369560242\n",
      "Eval loss 0.4460309147834778, R2 0.612976610660553\n",
      "epoch 236, loss 0.4532242715358734, R2 0.7949001789093018\n",
      "Eval loss 0.4532242715358734, R2 0.6143975853919983\n",
      "epoch 237, loss 0.4523448646068573, R2 0.7956748604774475\n",
      "Eval loss 0.4523448646068573, R2 0.6122779250144958\n",
      "epoch 238, loss 0.4440183937549591, R2 0.7978654503822327\n",
      "Eval loss 0.4440183937549591, R2 0.6145371794700623\n",
      "epoch 239, loss 0.4424695372581482, R2 0.7986316680908203\n",
      "Eval loss 0.4424695372581482, R2 0.614269495010376\n",
      "epoch 240, loss 0.4390389621257782, R2 0.7999426126480103\n",
      "Eval loss 0.4390389621257782, R2 0.6143484711647034\n",
      "epoch 241, loss 0.44025468826293945, R2 0.7994714975357056\n",
      "Eval loss 0.44025468826293945, R2 0.6161251664161682\n",
      "epoch 242, loss 0.43789878487586975, R2 0.8003892302513123\n",
      "Eval loss 0.43789878487586975, R2 0.6131023168563843\n",
      "epoch 243, loss 0.43429797887802124, R2 0.8016868829727173\n",
      "Eval loss 0.43429797887802124, R2 0.6150925159454346\n",
      "epoch 244, loss 0.4333416819572449, R2 0.8021553158760071\n",
      "Eval loss 0.4333416819572449, R2 0.6153056621551514\n",
      "epoch 245, loss 0.43129339814186096, R2 0.8029083013534546\n",
      "Eval loss 0.43129339814186096, R2 0.6147217154502869\n",
      "epoch 246, loss 0.43055325746536255, R2 0.8031215667724609\n",
      "Eval loss 0.43055325746536255, R2 0.6152158379554749\n",
      "epoch 247, loss 0.4309309124946594, R2 0.8035886883735657\n",
      "Eval loss 0.4309309124946594, R2 0.6128814816474915\n",
      "epoch 248, loss 0.43686234951019287, R2 0.8021745681762695\n",
      "Eval loss 0.43686234951019287, R2 0.6146934032440186\n",
      "epoch 249, loss 0.43715858459472656, R2 0.8025487661361694\n",
      "Eval loss 0.43715858459472656, R2 0.610281229019165\n",
      "epoch 250, loss 0.4297459125518799, R2 0.8039995431900024\n",
      "Eval loss 0.4297459125518799, R2 0.6149831414222717\n",
      "epoch 251, loss 0.42948609590530396, R2 0.8047447204589844\n",
      "Eval loss 0.42948609590530396, R2 0.6131044626235962\n",
      "epoch 252, loss 0.42707541584968567, R2 0.8052818775177002\n",
      "Eval loss 0.42707541584968567, R2 0.6129316687583923\n",
      "epoch 253, loss 0.422548770904541, R2 0.8067781329154968\n",
      "Eval loss 0.422548770904541, R2 0.615500271320343\n",
      "epoch 254, loss 0.4240456819534302, R2 0.806763231754303\n",
      "Eval loss 0.4240456819534302, R2 0.6131751537322998\n",
      "epoch 255, loss 0.42059123516082764, R2 0.807802140712738\n",
      "Eval loss 0.42059123516082764, R2 0.614223301410675\n",
      "epoch 256, loss 0.42108696699142456, R2 0.8078494668006897\n",
      "Eval loss 0.42108696699142456, R2 0.6137908697128296\n",
      "epoch 257, loss 0.4209705591201782, R2 0.8080850839614868\n",
      "Eval loss 0.4209705591201782, R2 0.6141448616981506\n",
      "epoch 258, loss 0.4212082624435425, R2 0.8080589175224304\n",
      "Eval loss 0.4212082624435425, R2 0.6097463965415955\n",
      "epoch 259, loss 0.4237018823623657, R2 0.8068069219589233\n",
      "Eval loss 0.4237018823623657, R2 0.6143762469291687\n",
      "epoch 260, loss 0.4166521728038788, R2 0.8099862933158875\n",
      "Eval loss 0.4166521728038788, R2 0.6130910515785217\n",
      "epoch 261, loss 0.41639336943626404, R2 0.8098533749580383\n",
      "Eval loss 0.41639336943626404, R2 0.6116191148757935\n",
      "epoch 262, loss 0.41372764110565186, R2 0.8107547163963318\n",
      "Eval loss 0.41372764110565186, R2 0.6136197447776794\n",
      "epoch 263, loss 0.4120606482028961, R2 0.8115711808204651\n",
      "Eval loss 0.4120606482028961, R2 0.6139740347862244\n",
      "epoch 264, loss 0.4131239354610443, R2 0.8111211657524109\n",
      "Eval loss 0.4131239354610443, R2 0.6116422414779663\n",
      "epoch 265, loss 0.4130130112171173, R2 0.8115754723548889\n",
      "Eval loss 0.4130130112171173, R2 0.6129289269447327\n",
      "epoch 266, loss 0.4253462553024292, R2 0.8091415762901306\n",
      "Eval loss 0.4253462553024292, R2 0.6071022152900696\n",
      "epoch 267, loss 0.4208693206310272, R2 0.8101425170898438\n",
      "Eval loss 0.4208693206310272, R2 0.6139687299728394\n",
      "epoch 268, loss 0.4131411015987396, R2 0.8122244477272034\n",
      "Eval loss 0.4131411015987396, R2 0.6091676354408264\n",
      "epoch 269, loss 0.4100251793861389, R2 0.8131017088890076\n",
      "Eval loss 0.4100251793861389, R2 0.6111574769020081\n",
      "epoch 270, loss 0.4079286754131317, R2 0.8137136101722717\n",
      "Eval loss 0.4079286754131317, R2 0.6128096580505371\n",
      "epoch 271, loss 0.40632110834121704, R2 0.8145315051078796\n",
      "Eval loss 0.40632110834121704, R2 0.6124500036239624\n",
      "epoch 272, loss 0.4047337472438812, R2 0.8152924180030823\n",
      "Eval loss 0.4047337472438812, R2 0.6135403513908386\n",
      "epoch 273, loss 0.40123251080513, R2 0.8160817623138428\n",
      "Eval loss 0.40123251080513, R2 0.6108062267303467\n",
      "epoch 274, loss 0.4007345736026764, R2 0.8164200186729431\n",
      "Eval loss 0.4007345736026764, R2 0.6129242181777954\n",
      "epoch 275, loss 0.39852872490882874, R2 0.8172245025634766\n",
      "Eval loss 0.39852872490882874, R2 0.6132442951202393\n",
      "epoch 276, loss 0.39982104301452637, R2 0.8171088695526123\n",
      "Eval loss 0.39982104301452637, R2 0.6097476482391357\n",
      "epoch 277, loss 0.4008907377719879, R2 0.8167116045951843\n",
      "Eval loss 0.4008907377719879, R2 0.6107081770896912\n",
      "epoch 278, loss 0.40336233377456665, R2 0.8159698247909546\n",
      "Eval loss 0.40336233377456665, R2 0.6100766658782959\n",
      "epoch 279, loss 0.3977293372154236, R2 0.8181772828102112\n",
      "Eval loss 0.3977293372154236, R2 0.6106333136558533\n",
      "epoch 280, loss 0.40135854482650757, R2 0.8175024390220642\n",
      "Eval loss 0.40135854482650757, R2 0.6125274300575256\n",
      "epoch 281, loss 0.40859293937683105, R2 0.8149703741073608\n",
      "Eval loss 0.40859293937683105, R2 0.6015436053276062\n",
      "epoch 282, loss 0.39692816138267517, R2 0.8183273673057556\n",
      "Eval loss 0.39692816138267517, R2 0.6123371124267578\n",
      "epoch 283, loss 0.3941083252429962, R2 0.8193935751914978\n",
      "Eval loss 0.3941083252429962, R2 0.6130594611167908\n",
      "epoch 284, loss 0.393084317445755, R2 0.8202056288719177\n",
      "Eval loss 0.393084317445755, R2 0.610198974609375\n",
      "epoch 285, loss 0.3916929066181183, R2 0.8205680847167969\n",
      "Eval loss 0.3916929066181183, R2 0.6079428791999817\n",
      "epoch 286, loss 0.39100751280784607, R2 0.8210189938545227\n",
      "Eval loss 0.39100751280784607, R2 0.6116192936897278\n",
      "epoch 287, loss 0.3868381381034851, R2 0.8223115801811218\n",
      "Eval loss 0.3868381381034851, R2 0.6123085618019104\n",
      "epoch 288, loss 0.38709765672683716, R2 0.822344183921814\n",
      "Eval loss 0.38709765672683716, R2 0.6091232895851135\n",
      "epoch 289, loss 0.38430070877075195, R2 0.8233595490455627\n",
      "Eval loss 0.38430070877075195, R2 0.6101725101470947\n",
      "epoch 290, loss 0.38447919487953186, R2 0.8232085704803467\n",
      "Eval loss 0.38447919487953186, R2 0.612172544002533\n",
      "epoch 291, loss 0.38403066992759705, R2 0.8236707448959351\n",
      "Eval loss 0.38403066992759705, R2 0.6079168319702148\n",
      "epoch 292, loss 0.3917669653892517, R2 0.8215075135231018\n",
      "Eval loss 0.3917669653892517, R2 0.6074028611183167\n",
      "epoch 293, loss 0.40418604016304016, R2 0.8185091018676758\n",
      "Eval loss 0.40418604016304016, R2 0.6033605337142944\n",
      "epoch 294, loss 0.3824883699417114, R2 0.8244447112083435\n",
      "Eval loss 0.3824883699417114, R2 0.6097699999809265\n",
      "epoch 295, loss 0.39291125535964966, R2 0.8219018578529358\n",
      "Eval loss 0.39291125535964966, R2 0.6087134480476379\n",
      "epoch 296, loss 0.38084477186203003, R2 0.8251922130584717\n",
      "Eval loss 0.38084477186203003, R2 0.6083212494850159\n",
      "epoch 297, loss 0.3830035924911499, R2 0.8248926401138306\n",
      "Eval loss 0.3830035924911499, R2 0.60672527551651\n",
      "epoch 298, loss 0.37949901819229126, R2 0.8256375789642334\n",
      "Eval loss 0.37949901819229126, R2 0.6088544726371765\n",
      "epoch 299, loss 0.37822017073631287, R2 0.826388418674469\n",
      "Eval loss 0.37822017073631287, R2 0.609224796295166\n",
      "epoch 300, loss 0.37954777479171753, R2 0.8263025879859924\n",
      "Eval loss 0.37954777479171753, R2 0.6087421774864197\n",
      "epoch 301, loss 0.38093680143356323, R2 0.8259090185165405\n",
      "Eval loss 0.38093680143356323, R2 0.6030359864234924\n",
      "epoch 302, loss 0.38552403450012207, R2 0.8240102529525757\n",
      "Eval loss 0.38552403450012207, R2 0.6083455085754395\n",
      "epoch 303, loss 0.3763127624988556, R2 0.8276855945587158\n",
      "Eval loss 0.3763127624988556, R2 0.6081076264381409\n",
      "epoch 304, loss 0.372560977935791, R2 0.8285856246948242\n",
      "Eval loss 0.372560977935791, R2 0.6079269647598267\n",
      "epoch 305, loss 0.3749220669269562, R2 0.8282194137573242\n",
      "Eval loss 0.3749220669269562, R2 0.608206033706665\n",
      "epoch 306, loss 0.37222957611083984, R2 0.8291286826133728\n",
      "Eval loss 0.37222957611083984, R2 0.6059059500694275\n",
      "epoch 307, loss 0.3695601522922516, R2 0.8297393918037415\n",
      "Eval loss 0.3695601522922516, R2 0.6084954738616943\n",
      "epoch 308, loss 0.36853909492492676, R2 0.8302678465843201\n",
      "Eval loss 0.36853909492492676, R2 0.6077752113342285\n",
      "epoch 309, loss 0.36821249127388, R2 0.8305564522743225\n",
      "Eval loss 0.36821249127388, R2 0.6062370538711548\n",
      "epoch 310, loss 0.36842551827430725, R2 0.8306195735931396\n",
      "Eval loss 0.36842551827430725, R2 0.6081460118293762\n",
      "epoch 311, loss 0.3690752387046814, R2 0.8305507898330688\n",
      "Eval loss 0.3690752387046814, R2 0.6051108241081238\n",
      "epoch 312, loss 0.37631651759147644, R2 0.8285270929336548\n",
      "Eval loss 0.37631651759147644, R2 0.6027357578277588\n",
      "epoch 313, loss 0.383405476808548, R2 0.8275152444839478\n",
      "Eval loss 0.383405476808548, R2 0.6033128499984741\n",
      "epoch 314, loss 0.3708934783935547, R2 0.8308050036430359\n",
      "Eval loss 0.3708934783935547, R2 0.6041370034217834\n",
      "epoch 315, loss 0.36996638774871826, R2 0.830812394618988\n",
      "Eval loss 0.36996638774871826, R2 0.6066130995750427\n",
      "epoch 316, loss 0.36459413170814514, R2 0.8326385021209717\n",
      "Eval loss 0.36459413170814514, R2 0.6049743294715881\n",
      "epoch 317, loss 0.36631444096565247, R2 0.8317222595214844\n",
      "Eval loss 0.36631444096565247, R2 0.6052488684654236\n",
      "epoch 318, loss 0.35988426208496094, R2 0.8338005542755127\n",
      "Eval loss 0.35988426208496094, R2 0.6059399247169495\n",
      "epoch 319, loss 0.3624696433544159, R2 0.8333473205566406\n",
      "Eval loss 0.3624696433544159, R2 0.6043229699134827\n",
      "epoch 320, loss 0.35823512077331543, R2 0.8346545696258545\n",
      "Eval loss 0.35823512077331543, R2 0.6056072115898132\n",
      "epoch 321, loss 0.35942548513412476, R2 0.8344472646713257\n",
      "Eval loss 0.35942548513412476, R2 0.6038368344306946\n",
      "epoch 322, loss 0.36068493127822876, R2 0.8336643576622009\n",
      "Eval loss 0.36068493127822876, R2 0.6055076122283936\n",
      "epoch 323, loss 0.36968177556991577, R2 0.8314392566680908\n",
      "Eval loss 0.36968177556991577, R2 0.5952160358428955\n",
      "epoch 324, loss 0.36444026231765747, R2 0.8329854607582092\n",
      "Eval loss 0.36444026231765747, R2 0.6056517362594604\n",
      "epoch 325, loss 0.35459837317466736, R2 0.8362576961517334\n",
      "Eval loss 0.35459837317466736, R2 0.6061102747917175\n",
      "epoch 326, loss 0.35967838764190674, R2 0.8348373770713806\n",
      "Eval loss 0.35967838764190674, R2 0.5997236967086792\n",
      "epoch 327, loss 0.35457658767700195, R2 0.836199164390564\n",
      "Eval loss 0.35457658767700195, R2 0.6051538586616516\n",
      "epoch 328, loss 0.3523276448249817, R2 0.837327778339386\n",
      "Eval loss 0.3523276448249817, R2 0.6046674251556396\n",
      "epoch 329, loss 0.3531774580478668, R2 0.8372660279273987\n",
      "Eval loss 0.3531774580478668, R2 0.6015856862068176\n",
      "epoch 330, loss 0.3512532114982605, R2 0.8378036022186279\n",
      "Eval loss 0.3512532114982605, R2 0.6037313938140869\n",
      "epoch 331, loss 0.35188043117523193, R2 0.8375797271728516\n",
      "Eval loss 0.35188043117523193, R2 0.6035993695259094\n",
      "epoch 332, loss 0.36028677225112915, R2 0.8346097469329834\n",
      "Eval loss 0.36028677225112915, R2 0.5959954857826233\n",
      "epoch 333, loss 0.36309099197387695, R2 0.8337755799293518\n",
      "Eval loss 0.36309099197387695, R2 0.5999669432640076\n",
      "epoch 334, loss 0.3528377413749695, R2 0.8372913599014282\n",
      "Eval loss 0.3528377413749695, R2 0.603127658367157\n",
      "epoch 335, loss 0.3543117046356201, R2 0.8370898962020874\n",
      "Eval loss 0.3543117046356201, R2 0.6002871990203857\n",
      "epoch 336, loss 0.3484456539154053, R2 0.8392820954322815\n",
      "Eval loss 0.3484456539154053, R2 0.599452793598175\n",
      "epoch 337, loss 0.34871819615364075, R2 0.8389658331871033\n",
      "Eval loss 0.34871819615364075, R2 0.6028348207473755\n",
      "epoch 338, loss 0.34583598375320435, R2 0.8402580618858337\n",
      "Eval loss 0.34583598375320435, R2 0.6024402379989624\n",
      "epoch 339, loss 0.3448871970176697, R2 0.8405618667602539\n",
      "Eval loss 0.3448871970176697, R2 0.5995800495147705\n",
      "epoch 340, loss 0.3459327220916748, R2 0.8402105569839478\n",
      "Eval loss 0.3459327220916748, R2 0.6019778847694397\n",
      "epoch 341, loss 0.34459221363067627, R2 0.8409349918365479\n",
      "Eval loss 0.34459221363067627, R2 0.6017534136772156\n",
      "epoch 342, loss 0.347381591796875, R2 0.8403909802436829\n",
      "Eval loss 0.347381591796875, R2 0.5977389812469482\n",
      "epoch 343, loss 0.3484155237674713, R2 0.8403348922729492\n",
      "Eval loss 0.3484155237674713, R2 0.6006212830543518\n",
      "epoch 344, loss 0.3427481949329376, R2 0.8417579531669617\n",
      "Eval loss 0.3427481949329376, R2 0.5998654961585999\n",
      "epoch 345, loss 0.34147369861602783, R2 0.842129647731781\n",
      "Eval loss 0.34147369861602783, R2 0.5993199348449707\n",
      "epoch 346, loss 0.3420354723930359, R2 0.8417865633964539\n",
      "Eval loss 0.3420354723930359, R2 0.598946750164032\n",
      "epoch 347, loss 0.3442701995372772, R2 0.8413285613059998\n",
      "Eval loss 0.3442701995372772, R2 0.5997910499572754\n",
      "epoch 348, loss 0.3433187007904053, R2 0.8417714834213257\n",
      "Eval loss 0.3433187007904053, R2 0.5959822535514832\n",
      "epoch 349, loss 0.3405711054801941, R2 0.8426992297172546\n",
      "Eval loss 0.3405711054801941, R2 0.6010572910308838\n",
      "epoch 350, loss 0.3391895592212677, R2 0.843295156955719\n",
      "Eval loss 0.3391895592212677, R2 0.5969429016113281\n",
      "epoch 351, loss 0.3353605568408966, R2 0.8443485498428345\n",
      "Eval loss 0.3353605568408966, R2 0.5995686054229736\n",
      "epoch 352, loss 0.3359414339065552, R2 0.8442651629447937\n",
      "Eval loss 0.3359414339065552, R2 0.5996509790420532\n",
      "epoch 353, loss 0.3368476927280426, R2 0.8441381454467773\n",
      "Eval loss 0.3368476927280426, R2 0.5963186621665955\n",
      "epoch 354, loss 0.3365677297115326, R2 0.8446339964866638\n",
      "Eval loss 0.3365677297115326, R2 0.598639190196991\n",
      "epoch 355, loss 0.3365001380443573, R2 0.8449447751045227\n",
      "Eval loss 0.3365001380443573, R2 0.5995197296142578\n",
      "epoch 356, loss 0.3391445577144623, R2 0.8443086743354797\n",
      "Eval loss 0.3391445577144623, R2 0.5926515460014343\n",
      "epoch 357, loss 0.34423181414604187, R2 0.8426503539085388\n",
      "Eval loss 0.34423181414604187, R2 0.5991290807723999\n",
      "epoch 358, loss 0.33723515272140503, R2 0.8450075387954712\n",
      "Eval loss 0.33723515272140503, R2 0.5969472527503967\n",
      "epoch 359, loss 0.33107978105545044, R2 0.8464184403419495\n",
      "Eval loss 0.33107978105545044, R2 0.5956364870071411\n",
      "epoch 360, loss 0.33210572600364685, R2 0.8464704155921936\n",
      "Eval loss 0.33210572600364685, R2 0.5984528064727783\n",
      "epoch 361, loss 0.33142411708831787, R2 0.8468528985977173\n",
      "Eval loss 0.33142411708831787, R2 0.5970820188522339\n",
      "epoch 362, loss 0.3281552791595459, R2 0.8476648926734924\n",
      "Eval loss 0.3281552791595459, R2 0.597573459148407\n",
      "epoch 363, loss 0.3277462422847748, R2 0.8480491042137146\n",
      "Eval loss 0.3277462422847748, R2 0.596425473690033\n",
      "epoch 364, loss 0.32914096117019653, R2 0.8476871252059937\n",
      "Eval loss 0.32914096117019653, R2 0.5977503061294556\n",
      "epoch 365, loss 0.329125314950943, R2 0.8475765585899353\n",
      "Eval loss 0.329125314950943, R2 0.5927678942680359\n",
      "epoch 366, loss 0.33470621705055237, R2 0.8458490371704102\n",
      "Eval loss 0.33470621705055237, R2 0.5957610607147217\n",
      "epoch 367, loss 0.33398258686065674, R2 0.8460569977760315\n",
      "Eval loss 0.33398258686065674, R2 0.5919426083564758\n",
      "epoch 368, loss 0.3253422975540161, R2 0.8490143418312073\n",
      "Eval loss 0.3253422975540161, R2 0.5953701734542847\n",
      "epoch 369, loss 0.3297679126262665, R2 0.8477321267127991\n",
      "Eval loss 0.3297679126262665, R2 0.596272885799408\n",
      "epoch 370, loss 0.32592862844467163, R2 0.8486640453338623\n",
      "Eval loss 0.32592862844467163, R2 0.5928483605384827\n",
      "epoch 371, loss 0.3262145519256592, R2 0.8492286205291748\n",
      "Eval loss 0.3262145519256592, R2 0.5947316288948059\n",
      "epoch 372, loss 0.32611867785453796, R2 0.8493177890777588\n",
      "Eval loss 0.32611867785453796, R2 0.5952466130256653\n",
      "epoch 373, loss 0.32609307765960693, R2 0.8490102291107178\n",
      "Eval loss 0.32609307765960693, R2 0.5921751856803894\n",
      "epoch 374, loss 0.3218182921409607, R2 0.8504697680473328\n",
      "Eval loss 0.3218182921409607, R2 0.5939933657646179\n",
      "epoch 375, loss 0.32341867685317993, R2 0.8502817153930664\n",
      "Eval loss 0.32341867685317993, R2 0.5943728089332581\n",
      "epoch 376, loss 0.3204512298107147, R2 0.8513275384902954\n",
      "Eval loss 0.3204512298107147, R2 0.5954418778419495\n",
      "epoch 377, loss 0.3186738193035126, R2 0.8517196178436279\n",
      "Eval loss 0.3186738193035126, R2 0.5909044742584229\n",
      "epoch 378, loss 0.31930649280548096, R2 0.8514148592948914\n",
      "Eval loss 0.31930649280548096, R2 0.5963221192359924\n",
      "epoch 379, loss 0.3195137083530426, R2 0.8518516421318054\n",
      "Eval loss 0.3195137083530426, R2 0.5900309681892395\n",
      "epoch 380, loss 0.32567232847213745, R2 0.8504854440689087\n",
      "Eval loss 0.32567232847213745, R2 0.594018280506134\n",
      "epoch 381, loss 0.3239946663379669, R2 0.8512140512466431\n",
      "Eval loss 0.3239946663379669, R2 0.5907840132713318\n",
      "epoch 382, loss 0.3159993588924408, R2 0.853022575378418\n",
      "Eval loss 0.3159993588924408, R2 0.5924515724182129\n",
      "epoch 383, loss 0.318437397480011, R2 0.8522275686264038\n",
      "Eval loss 0.318437397480011, R2 0.593213677406311\n",
      "epoch 384, loss 0.3224441707134247, R2 0.8511024117469788\n",
      "Eval loss 0.3224441707134247, R2 0.5890169143676758\n",
      "epoch 385, loss 0.3186001777648926, R2 0.8524990677833557\n",
      "Eval loss 0.3186001777648926, R2 0.5885554552078247\n",
      "epoch 386, loss 0.3217141032218933, R2 0.8518466949462891\n",
      "Eval loss 0.3217141032218933, R2 0.5942957401275635\n",
      "epoch 387, loss 0.3108556568622589, R2 0.8550897240638733\n",
      "Eval loss 0.3108556568622589, R2 0.5925886034965515\n",
      "epoch 388, loss 0.3134407103061676, R2 0.8546462655067444\n",
      "Eval loss 0.3134407103061676, R2 0.5903663635253906\n",
      "epoch 389, loss 0.3122662901878357, R2 0.8550524115562439\n",
      "Eval loss 0.3122662901878357, R2 0.591876745223999\n",
      "epoch 390, loss 0.30992254614830017, R2 0.855445921421051\n",
      "Eval loss 0.30992254614830017, R2 0.5928972363471985\n",
      "epoch 391, loss 0.30996859073638916, R2 0.8556379079818726\n",
      "Eval loss 0.30996859073638916, R2 0.5897836089134216\n",
      "epoch 392, loss 0.30809491872787476, R2 0.8562032580375671\n",
      "Eval loss 0.30809491872787476, R2 0.5912451148033142\n",
      "epoch 393, loss 0.30820679664611816, R2 0.8562677502632141\n",
      "Eval loss 0.30820679664611816, R2 0.5924685001373291\n",
      "epoch 394, loss 0.30958014726638794, R2 0.8557633757591248\n",
      "Eval loss 0.30958014726638794, R2 0.5873713493347168\n",
      "epoch 395, loss 0.32240036129951477, R2 0.8513174057006836\n",
      "Eval loss 0.32240036129951477, R2 0.5852175354957581\n",
      "epoch 396, loss 0.3332551419734955, R2 0.8476213216781616\n",
      "Eval loss 0.3332551419734955, R2 0.5840585231781006\n",
      "epoch 397, loss 0.3064142167568207, R2 0.8571957945823669\n",
      "Eval loss 0.3064142167568207, R2 0.587303876876831\n",
      "epoch 398, loss 0.3149050772190094, R2 0.8540826439857483\n",
      "Eval loss 0.3149050772190094, R2 0.5905404686927795\n",
      "epoch 399, loss 0.30732789635658264, R2 0.8569186925888062\n",
      "Eval loss 0.30732789635658264, R2 0.5878300070762634\n",
      "epoch 400, loss 0.3046291172504425, R2 0.8578710556030273\n",
      "Eval loss 0.3046291172504425, R2 0.58957439661026\n",
      "epoch 401, loss 0.3071286678314209, R2 0.8574228882789612\n",
      "Eval loss 0.3071286678314209, R2 0.5885188579559326\n",
      "epoch 402, loss 0.3024815320968628, R2 0.8585630059242249\n",
      "Eval loss 0.3024815320968628, R2 0.5909265875816345\n",
      "epoch 403, loss 0.3073537349700928, R2 0.8566129803657532\n",
      "Eval loss 0.3073537349700928, R2 0.5833050608634949\n",
      "epoch 404, loss 0.3086485266685486, R2 0.8569116592407227\n",
      "Eval loss 0.3086485266685486, R2 0.5889661908149719\n",
      "epoch 405, loss 0.3081154227256775, R2 0.8574013113975525\n",
      "Eval loss 0.3081154227256775, R2 0.5869307518005371\n",
      "epoch 406, loss 0.3020133674144745, R2 0.8588451743125916\n",
      "Eval loss 0.3020133674144745, R2 0.5875992178916931\n",
      "epoch 407, loss 0.30210795998573303, R2 0.8590675592422485\n",
      "Eval loss 0.30210795998573303, R2 0.589910089969635\n",
      "epoch 408, loss 0.3044872581958771, R2 0.8581342697143555\n",
      "Eval loss 0.3044872581958771, R2 0.5832744240760803\n",
      "epoch 409, loss 0.30081623792648315, R2 0.8597202301025391\n",
      "Eval loss 0.30081623792648315, R2 0.5882413983345032\n",
      "epoch 410, loss 0.3011462986469269, R2 0.8596862554550171\n",
      "Eval loss 0.3011462986469269, R2 0.5872332453727722\n",
      "epoch 411, loss 0.3043600022792816, R2 0.8589203357696533\n",
      "Eval loss 0.3043600022792816, R2 0.5836721062660217\n",
      "epoch 412, loss 0.3008844554424286, R2 0.8592396378517151\n",
      "Eval loss 0.3008844554424286, R2 0.5877235531806946\n",
      "epoch 413, loss 0.2966625392436981, R2 0.861461877822876\n",
      "Eval loss 0.2966625392436981, R2 0.5885199904441833\n",
      "epoch 414, loss 0.2982105612754822, R2 0.8606952428817749\n",
      "Eval loss 0.2982105612754822, R2 0.5833438634872437\n",
      "epoch 415, loss 0.29865261912345886, R2 0.8609705567359924\n",
      "Eval loss 0.29865261912345886, R2 0.5870985388755798\n",
      "epoch 416, loss 0.2984108626842499, R2 0.8611621260643005\n",
      "Eval loss 0.2984108626842499, R2 0.586610734462738\n",
      "epoch 417, loss 0.2985551953315735, R2 0.8610317707061768\n",
      "Eval loss 0.2985551953315735, R2 0.5842326879501343\n",
      "epoch 418, loss 0.2947465479373932, R2 0.8620336055755615\n",
      "Eval loss 0.2947465479373932, R2 0.5860222578048706\n",
      "epoch 419, loss 0.2921468913555145, R2 0.8630741238594055\n",
      "Eval loss 0.2921468913555145, R2 0.5876688957214355\n",
      "epoch 420, loss 0.29377785325050354, R2 0.8627397418022156\n",
      "Eval loss 0.29377785325050354, R2 0.5827620625495911\n",
      "epoch 421, loss 0.2947767674922943, R2 0.8620809316635132\n",
      "Eval loss 0.2947767674922943, R2 0.5866744518280029\n",
      "epoch 422, loss 0.29652801156044006, R2 0.8612716197967529\n",
      "Eval loss 0.29652801156044006, R2 0.5804913640022278\n",
      "epoch 423, loss 0.29991254210472107, R2 0.8604559302330017\n",
      "Eval loss 0.29991254210472107, R2 0.5837830305099487\n",
      "epoch 424, loss 0.29687339067459106, R2 0.8621921539306641\n",
      "Eval loss 0.29687339067459106, R2 0.5826337933540344\n",
      "epoch 425, loss 0.29541054368019104, R2 0.8626627326011658\n",
      "Eval loss 0.29541054368019104, R2 0.5850947499275208\n",
      "epoch 426, loss 0.29772621393203735, R2 0.8616623282432556\n",
      "Eval loss 0.29772621393203735, R2 0.5803282260894775\n",
      "epoch 427, loss 0.28935179114341736, R2 0.8644599914550781\n",
      "Eval loss 0.28935179114341736, R2 0.5839954018592834\n",
      "epoch 428, loss 0.2898695766925812, R2 0.86441570520401\n",
      "Eval loss 0.2898695766925812, R2 0.585509717464447\n",
      "epoch 429, loss 0.2873570919036865, R2 0.8652384281158447\n",
      "Eval loss 0.2873570919036865, R2 0.583568274974823\n",
      "epoch 430, loss 0.28790727257728577, R2 0.8652178049087524\n",
      "Eval loss 0.28790727257728577, R2 0.5838484168052673\n",
      "epoch 431, loss 0.29067182540893555, R2 0.8646303415298462\n",
      "Eval loss 0.29067182540893555, R2 0.5819420218467712\n",
      "epoch 432, loss 0.28657957911491394, R2 0.8658004999160767\n",
      "Eval loss 0.28657957911491394, R2 0.5846554040908813\n",
      "epoch 433, loss 0.2886761426925659, R2 0.865644633769989\n",
      "Eval loss 0.2886761426925659, R2 0.5802218914031982\n",
      "epoch 434, loss 0.28919506072998047, R2 0.8653507232666016\n",
      "Eval loss 0.28919506072998047, R2 0.5851238369941711\n",
      "epoch 435, loss 0.28834953904151917, R2 0.8654471039772034\n",
      "Eval loss 0.28834953904151917, R2 0.5799499154090881\n",
      "epoch 436, loss 0.288280189037323, R2 0.8656795620918274\n",
      "Eval loss 0.288280189037323, R2 0.5803234577178955\n",
      "epoch 437, loss 0.28638291358947754, R2 0.8656631708145142\n",
      "Eval loss 0.28638291358947754, R2 0.5842371582984924\n",
      "epoch 438, loss 0.2885269522666931, R2 0.8650203943252563\n",
      "Eval loss 0.2885269522666931, R2 0.5765828490257263\n",
      "epoch 439, loss 0.2847847044467926, R2 0.866450309753418\n",
      "Eval loss 0.2847847044467926, R2 0.5824127793312073\n",
      "epoch 440, loss 0.28168419003486633, R2 0.8677015900611877\n",
      "Eval loss 0.28168419003486633, R2 0.5831208825111389\n",
      "epoch 441, loss 0.28132346272468567, R2 0.8680707812309265\n",
      "Eval loss 0.28132346272468567, R2 0.5807502269744873\n",
      "epoch 442, loss 0.2803282141685486, R2 0.8680749535560608\n",
      "Eval loss 0.2803282141685486, R2 0.5809540152549744\n",
      "epoch 443, loss 0.2823497951030731, R2 0.8673418164253235\n",
      "Eval loss 0.2823497951030731, R2 0.580070436000824\n",
      "epoch 444, loss 0.28162750601768494, R2 0.8675426244735718\n",
      "Eval loss 0.28162750601768494, R2 0.5804566740989685\n",
      "epoch 445, loss 0.277483731508255, R2 0.8693906664848328\n",
      "Eval loss 0.277483731508255, R2 0.5804582238197327\n",
      "epoch 446, loss 0.28004103899002075, R2 0.8688320517539978\n",
      "Eval loss 0.28004103899002075, R2 0.5806863903999329\n",
      "epoch 447, loss 0.2827569842338562, R2 0.8682901263237\n",
      "Eval loss 0.2827569842338562, R2 0.5787664651870728\n",
      "epoch 448, loss 0.290313720703125, R2 0.8668540716171265\n",
      "Eval loss 0.290313720703125, R2 0.5753711462020874\n",
      "epoch 449, loss 0.2915283441543579, R2 0.8663661479949951\n",
      "Eval loss 0.2915283441543579, R2 0.5793401598930359\n",
      "epoch 450, loss 0.28535082936286926, R2 0.8670293688774109\n",
      "Eval loss 0.28535082936286926, R2 0.5756996273994446\n",
      "epoch 451, loss 0.28197675943374634, R2 0.8682588338851929\n",
      "Eval loss 0.28197675943374634, R2 0.5791545510292053\n",
      "epoch 452, loss 0.27956321835517883, R2 0.8693093657493591\n",
      "Eval loss 0.27956321835517883, R2 0.574539065361023\n",
      "epoch 453, loss 0.27999672293663025, R2 0.8690519332885742\n",
      "Eval loss 0.27999672293663025, R2 0.5807231068611145\n",
      "epoch 454, loss 0.27341362833976746, R2 0.8711209893226624\n",
      "Eval loss 0.27341362833976746, R2 0.5772202014923096\n",
      "epoch 455, loss 0.2781410813331604, R2 0.8695447444915771\n",
      "Eval loss 0.2781410813331604, R2 0.5784485936164856\n",
      "epoch 456, loss 0.27475258708000183, R2 0.8710705041885376\n",
      "Eval loss 0.27475258708000183, R2 0.5747732520103455\n",
      "epoch 457, loss 0.27774637937545776, R2 0.8700532913208008\n",
      "Eval loss 0.27774637937545776, R2 0.5798487663269043\n",
      "epoch 458, loss 0.2765231430530548, R2 0.8703022599220276\n",
      "Eval loss 0.2765231430530548, R2 0.5755230784416199\n",
      "epoch 459, loss 0.2746080756187439, R2 0.8710489869117737\n",
      "Eval loss 0.2746080756187439, R2 0.5774055123329163\n",
      "epoch 460, loss 0.26999422907829285, R2 0.8726492524147034\n",
      "Eval loss 0.26999422907829285, R2 0.5781117081642151\n",
      "epoch 461, loss 0.2723940908908844, R2 0.8720506429672241\n",
      "Eval loss 0.2723940908908844, R2 0.5750941634178162\n",
      "epoch 462, loss 0.2734560966491699, R2 0.871573269367218\n",
      "Eval loss 0.2734560966491699, R2 0.5794748663902283\n",
      "epoch 463, loss 0.27384448051452637, R2 0.871573269367218\n",
      "Eval loss 0.27384448051452637, R2 0.5713695883750916\n",
      "epoch 464, loss 0.2692579925060272, R2 0.8729331493377686\n",
      "Eval loss 0.2692579925060272, R2 0.5782213807106018\n",
      "epoch 465, loss 0.2686716616153717, R2 0.8733789920806885\n",
      "Eval loss 0.2686716616153717, R2 0.5782397985458374\n",
      "epoch 466, loss 0.26933953166007996, R2 0.8728773593902588\n",
      "Eval loss 0.26933953166007996, R2 0.5734871625900269\n",
      "epoch 467, loss 0.26746225357055664, R2 0.8739070296287537\n",
      "Eval loss 0.26746225357055664, R2 0.5768736600875854\n",
      "epoch 468, loss 0.26635265350341797, R2 0.8744430541992188\n",
      "Eval loss 0.26635265350341797, R2 0.5759674310684204\n",
      "epoch 469, loss 0.2701526880264282, R2 0.8731855154037476\n",
      "Eval loss 0.2701526880264282, R2 0.5729246139526367\n",
      "epoch 470, loss 0.28286802768707275, R2 0.868793249130249\n",
      "Eval loss 0.28286802768707275, R2 0.5693833231925964\n",
      "epoch 471, loss 0.2759706676006317, R2 0.8714771270751953\n",
      "Eval loss 0.2759706676006317, R2 0.5747883319854736\n",
      "epoch 472, loss 0.268239289522171, R2 0.8739246129989624\n",
      "Eval loss 0.268239289522171, R2 0.5739715099334717\n",
      "epoch 473, loss 0.26977968215942383, R2 0.873417317867279\n",
      "Eval loss 0.26977968215942383, R2 0.5717931389808655\n",
      "epoch 474, loss 0.2646922767162323, R2 0.8747907876968384\n",
      "Eval loss 0.2646922767162323, R2 0.5747823119163513\n",
      "epoch 475, loss 0.26692721247673035, R2 0.8743070363998413\n",
      "Eval loss 0.26692721247673035, R2 0.5734118223190308\n",
      "epoch 476, loss 0.2640331983566284, R2 0.8750202655792236\n",
      "Eval loss 0.2640331983566284, R2 0.5721889734268188\n",
      "epoch 477, loss 0.2814929783344269, R2 0.8689731359481812\n",
      "Eval loss 0.2814929783344269, R2 0.5644885897636414\n",
      "epoch 478, loss 0.2973586916923523, R2 0.8631324172019958\n",
      "Eval loss 0.2973586916923523, R2 0.5651257038116455\n",
      "epoch 479, loss 0.2618838846683502, R2 0.8762624263763428\n",
      "Eval loss 0.2618838846683502, R2 0.5705356001853943\n",
      "epoch 480, loss 0.27337515354156494, R2 0.871980607509613\n",
      "Eval loss 0.27337515354156494, R2 0.5722541213035583\n",
      "epoch 481, loss 0.26381775736808777, R2 0.8753446340560913\n",
      "Eval loss 0.26381775736808777, R2 0.5722533464431763\n",
      "epoch 482, loss 0.2599722146987915, R2 0.8769792318344116\n",
      "Eval loss 0.2599722146987915, R2 0.5744588375091553\n",
      "epoch 483, loss 0.26353880763053894, R2 0.8755627274513245\n",
      "Eval loss 0.26353880763053894, R2 0.5697715282440186\n",
      "epoch 484, loss 0.2571694254875183, R2 0.8780595660209656\n",
      "Eval loss 0.2571694254875183, R2 0.5713022947311401\n",
      "epoch 485, loss 0.25882846117019653, R2 0.8774848580360413\n",
      "Eval loss 0.25882846117019653, R2 0.5718584060668945\n",
      "epoch 486, loss 0.2599336504936218, R2 0.8773589730262756\n",
      "Eval loss 0.2599336504936218, R2 0.5737994313240051\n",
      "epoch 487, loss 0.26500046253204346, R2 0.8765390515327454\n",
      "Eval loss 0.26500046253204346, R2 0.5669022798538208\n",
      "epoch 488, loss 0.27349114418029785, R2 0.8738677501678467\n",
      "Eval loss 0.27349114418029785, R2 0.5726200342178345\n",
      "epoch 489, loss 0.2585671842098236, R2 0.8779733180999756\n",
      "Eval loss 0.2585671842098236, R2 0.5705835819244385\n",
      "epoch 490, loss 0.2575836181640625, R2 0.8786085247993469\n",
      "Eval loss 0.2575836181640625, R2 0.5678864121437073\n",
      "epoch 491, loss 0.2607463002204895, R2 0.8776048421859741\n",
      "Eval loss 0.2607463002204895, R2 0.5726813673973083\n",
      "epoch 492, loss 0.25306037068367004, R2 0.8798086047172546\n",
      "Eval loss 0.25306037068367004, R2 0.5716935396194458\n",
      "epoch 493, loss 0.2558947205543518, R2 0.8793872594833374\n",
      "Eval loss 0.2558947205543518, R2 0.5696380138397217\n",
      "epoch 494, loss 0.25545886158943176, R2 0.8793938159942627\n",
      "Eval loss 0.25545886158943176, R2 0.5702784061431885\n",
      "epoch 495, loss 0.2529517710208893, R2 0.8800098299980164\n",
      "Eval loss 0.2529517710208893, R2 0.5708997845649719\n",
      "epoch 496, loss 0.2566189467906952, R2 0.878993570804596\n",
      "Eval loss 0.2566189467906952, R2 0.5663219094276428\n",
      "epoch 497, loss 0.2604648172855377, R2 0.8778652548789978\n",
      "Eval loss 0.2604648172855377, R2 0.570378303527832\n",
      "epoch 498, loss 0.259682834148407, R2 0.8782339096069336\n",
      "Eval loss 0.259682834148407, R2 0.5660665035247803\n",
      "epoch 499, loss 0.25358593463897705, R2 0.880344808101654\n",
      "Eval loss 0.25358593463897705, R2 0.5686771869659424\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    k = 0\n",
    "    l = batch_size\n",
    "    batch_loss = []\n",
    "    while l < train_x.shape[0]:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = NN_model(train_x)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, train_y)\n",
    "        batch_loss.append(loss.item())\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update k,l\n",
    "        k = l\n",
    "        l = min(l+batch_size, train_x.shape[0])\n",
    "\n",
    "    #Append train loss\n",
    "    train_losses.append(np.mean(batch_loss))\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = NN_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = NN_model(train_x)\n",
    "    test_preds = NN_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 44]) torch.Size([800, 44]) torch.Size([200, 44]) torch.Size([200, 44])\n"
     ]
    }
   ],
   "source": [
    "from src.model.gaussian_process import MultiOutputGP\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.1\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "mean = \"Linear\"\n",
    "kernel = \"Linear\"\n",
    "\n",
    "epochs = 50\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "GP_model = MultiOutputGP(in_size, out_size, device, mean, kernel)\n",
    "mll = gpytorch.mlls.SumMarginalLogLikelihood(GP_model.likelihood, GP_model.gp)\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                GP_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "dataset_size_gp = 1000\n",
    "gp_idx_split = int(test_split_ratio*dataset_size_gp)\n",
    "\n",
    "train_x_gp = train_x[gp_idx_split:dataset_size_gp, ...]\n",
    "train_y_gp = train_y[gp_idx_split:dataset_size_gp, ...]\n",
    "test_x_gp = test_x[:gp_idx_split, ...]\n",
    "test_y_gp = test_y[:gp_idx_split, ...]\n",
    "\n",
    "print(train_x_gp.shape, train_y_gp.shape, test_x_gp.shape, test_y_gp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.6078085899353027, R2 -229.49391174316406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m GP_model\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     41\u001b[0m GP_model\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 42\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mGP_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x_gp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(preds, [test_y_gp[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(GP_model\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mmodels))])\n\u001b[1;32m     44\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/mnt/c/users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/src/model/gaussian_process.py:119\u001b[0m, in \u001b[0;36mMultiOutputGP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mtrain_inputs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [model(x) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mmodels]\n",
      "File \u001b[0;32m/mnt/c/users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/src/model/gaussian_process.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mtrain_inputs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mmodels]\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_prediction_strategies.py:290\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    286\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_predictive_covar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_test_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_train_covar\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_prediction_strategies.py:356\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# In other cases - we'll use the standard infrastructure\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m test_test_covar \u001b[38;5;241m+\u001b[39m MatmulLinearOperator(test_train_covar, covar_correction_rhs\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 356\u001b[0m precomputed_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_cache\u001b[49m\n\u001b[1;32m    357\u001b[0m covar_inv_quad_form_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_prediction_strategies.py:246\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.covar_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;129m@cached\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovar_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcovar_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    245\u001b[0m     train_train_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlik_train_train_covar\n\u001b[0;32m--> 246\u001b[0m     train_train_covar_inv_root \u001b[38;5;241m=\u001b[39m to_dense(\u001b[43mtrain_train_covar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_inv_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_test_train_covar)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2221\u001b[0m, in \u001b[0;36mLinearOperator.root_inv_decomposition\u001b[0;34m(self, initial_vectors, test_vectors, method)\u001b[0m\n\u001b[1;32m   2217\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_root_method()\n\u001b[1;32m   2219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcholesky\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;66;03m# self.cholesky will hit cache if available\u001b[39;00m\n\u001b[0;32m-> 2221\u001b[0m     L \u001b[38;5;241m=\u001b[39m to_dense(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# we know L is triangular, so inverting is a simple triangular solve agaist the identity\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# we don't need the batch shape here, thanks to broadcasting\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     Eye \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(L\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], device\u001b[38;5;241m=\u001b[39mL\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mL\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:515\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub_mat, KeOpsLinearOperator) \u001b[38;5;28;01mfor\u001b[39;00m sub_mat \u001b[38;5;129;01min\u001b[39;00m evaluated_kern_mat\u001b[38;5;241m.\u001b[39m_args):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run Cholesky with KeOps: it will either be really slow or not work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 515\u001b[0m evaluated_mat \u001b[38;5;241m=\u001b[39m \u001b[43mevaluated_kern_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# if the tensor is a scalar, we can just take the square root\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluated_mat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36mSumLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_ops\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/sum_linear_operator.py:80\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/root_linear_operator.py:119\u001b[0m, in \u001b[0;36mRootLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 119\u001b[0m     eval_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(eval_root, eval_root\u001b[38;5;241m.\u001b[39mmT)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/constant_mul_linear_operator.py:175\u001b[0m, in \u001b[0;36mConstantMulLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    174\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_linear_op\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanded_constant\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/linear_operator/operators/constant_mul_linear_operator.py:162\u001b[0m, in \u001b[0;36mConstantMulLinearOperator.expanded_constant\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpanded_constant\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Make sure that the constant can be expanded to the appropriate size\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m         constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstantMulLinearOperator of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m received an invalid constant of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    166\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_linear_op\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constant\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    167\u001b[0m             )\n\u001b[1;32m    168\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    #Set training mode REQUIRED FOR GP\n",
    "    GP_model.gp.train()\n",
    "\n",
    "    #Set the training data\n",
    "    if epoch==0:\n",
    "        GP_model.set_train_data(train_x_gp, train_y_gp)\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = GP_model.forward()\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = -mll(outputs, GP_model.gp.train_targets)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_pred_output = GP_model.likelihood(*outputs)\n",
    "    train_pred_mean = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_pred_output], axis=-1\n",
    "    )\n",
    "    train_metric = metric(train_pred_mean, train_y_gp)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            GP_model.gp.eval()\n",
    "            GP_model.likelihood.eval()\n",
    "            preds = GP_model.forward(test_x_gp)\n",
    "            test_loss = -mll(preds, [test_y_gp[..., i] for i in range(len(GP_model.gp.models))])\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_pred_output = GP_model.likelihood(*preds)\n",
    "            test_pred_mean = torch.cat(\n",
    "                [pred.mean.unsqueeze(-1) for pred in test_pred_output], axis=-1\n",
    "            )\n",
    "            test_metric = metric(test_pred_mean, test_y_gp)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = GP_model.likelihood(*GP_model(train_x_gp))\n",
    "    test_preds = GP_model.likelihood(*GP_model(test_x_gp))\n",
    "    train_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_preds], axis=-1\n",
    "    )\n",
    "    test_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in test_preds], axis=-1\n",
    "    )\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y_gp, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y_gp, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y_gp, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y_gp, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600,) (200,)\n",
      "Lower train: -1.8210667371749878+-0.01887916587293148\n",
      "Upper train: 1.8210667371749878+-0.01887916401028633\n",
      "Lower train: -1.8232992887496948+-0.018743911758065224\n",
      "Lower train: 1.8232992887496948+-0.01874392479658127\n"
     ]
    }
   ],
   "source": [
    "gp_id = 0\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](train_x_gp))\n",
    "    test_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](test_x_gp))\n",
    "\n",
    "    train_lower, train_upper = train_pred.confidence_region()\n",
    "    mean_lower_train = np.mean(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    std_lower_train = np.std(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    mean_upper_train = np.mean(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    std_upper_train = np.std(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    test_lower, test_upper = test_pred.confidence_region()\n",
    "    mean_lower_test = np.mean(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    std_lower_test = np.std(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    mean_upper_test = np.mean(test_upper.numpy()-test_pred.mean.numpy())\n",
    "    std_upper_test = np.std(test_upper.numpy()-test_pred.mean.numpy())\n",
    "\n",
    "    print(train_pred.mean.numpy().shape, test_pred.mean.numpy().shape)\n",
    "    print(f\"Lower train: {mean_lower_train}+-{std_lower_train}\")\n",
    "    print(f\"Upper train: {mean_upper_train}+-{std_upper_train}\")\n",
    "    print(f\"Lower train: {mean_lower_test}+-{std_lower_test}\")\n",
    "    print(f\"Lower train: {mean_upper_test}+-{std_upper_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MBRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 10\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = lr_model\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL loop (with pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1, reward: 2.573672088377971.\n",
      "Trial: 2, reward: 2.705402098585997.\n",
      "Trial: 3, reward: 2.322563968184047.\n",
      "Trial: 4, reward: 2.677491420881591.\n",
      "Trial: 5, reward: 2.8242376956986925.\n",
      "Trial: 6, reward: 3.101734248162826.\n",
      "Trial: 7, reward: 1.843696885892893.\n",
      "Trial: 8, reward: 1.903616935018447.\n",
      "Trial: 9, reward: 2.8298462815498127.\n",
      "Trial: 10, reward: 2.7504850048228.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "\n",
    "while (\n",
    "    current_trial < num_episodes\n",
    "):\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "\n",
    "        # --- Doing env step using the agent ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "        \n",
    "        #print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative Reward')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sum(rewards, axis=-1))\n",
    "plt.xlabel('Day'); plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training MBRL agent with a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from mbrl.models.model import Model\n",
    "from mbrl.util.logger import Logger\n",
    "import mbrl.util.common\n",
    "import mbrl.models\n",
    "\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "from src.util.model_trainer import ModelTrainer, ModelTrainerOverriden\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.model.simple import Simple\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 20\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "optim_lr = 0.1\n",
    "model_wd = 0\n",
    "freq_train_model = 10\n",
    "model_batch_size = num_steps+initial_exploration_steps #Make sense for LR or GP\n",
    "validation_ratio = 0\n",
    "num_epochs = 25\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = Simple(in_size, out_size, device)\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "\n",
    "#Replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    num_steps+initial_exploration_steps,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=initial_exploration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training MBRL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500\n",
      "Epoch: 0 Train loss 5.370, Test loss 864915072.000\n",
      "1 500\n",
      "Epoch: 1 Train loss 864915200.000, Test loss 281.795\n",
      "1 500\n",
      "Epoch: 2 Train loss 281.795, Test loss 5.353\n",
      "1 500\n",
      "Epoch: 3 Train loss 5.353, Test loss 5.373\n",
      "1 500\n",
      "Epoch: 4 Train loss 5.373, Test loss 5.311\n",
      "1 500\n",
      "Epoch: 5 Train loss 5.311, Test loss 5.156\n",
      "1 500\n",
      "Epoch: 6 Train loss 5.156, Test loss 4.905\n",
      "1 500\n",
      "Epoch: 7 Train loss 4.905, Test loss 4.582\n",
      "1 500\n",
      "Epoch: 8 Train loss 4.582, Test loss 4.256\n",
      "1 500\n",
      "Epoch: 9 Train loss 4.256, Test loss 4.038\n",
      "1 500\n",
      "Epoch: 10 Train loss 4.038, Test loss 4.029\n",
      "1 500\n",
      "Epoch: 11 Train loss 4.029, Test loss 4.146\n",
      "1 500\n",
      "Epoch: 12 Train loss 4.146, Test loss 4.144\n",
      "1 500\n",
      "Epoch: 13 Train loss 4.144, Test loss 4.023\n",
      "1 500\n",
      "Epoch: 14 Train loss 4.023, Test loss 3.897\n",
      "1 500\n",
      "Epoch: 15 Train loss 3.897, Test loss 3.841\n",
      "1 500\n",
      "Epoch: 16 Train loss 3.841, Test loss 3.855\n",
      "1 500\n",
      "Epoch: 17 Train loss 3.855, Test loss 3.906\n",
      "1 500\n",
      "Epoch: 18 Train loss 3.906, Test loss 3.956\n",
      "1 500\n",
      "Epoch: 19 Train loss 3.956, Test loss 3.985\n",
      "1 500\n",
      "Epoch: 20 Train loss 3.985, Test loss 3.986\n",
      "1 500\n",
      "Epoch: 21 Train loss 3.986, Test loss 3.962\n",
      "1 500\n",
      "Epoch: 22 Train loss 3.962, Test loss 3.923\n",
      "1 500\n",
      "Epoch: 23 Train loss 3.923, Test loss 3.881\n",
      "1 500\n",
      "Epoch: 24 Train loss 3.881, Test loss 3.849\n",
      "Mean train loss: 34596623.267 Mean val score: 34596618.086\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  0.\n",
      "  -2. -2.  1.  2.  0. -1. -1.  1. -2.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.01303253]] False\n",
      "Step 1: Reward 0.000.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  1.\n",
      "  -2.  4.  0.  2. -1. -1. -1.  9. -2.  0. -2. -1. -1. -2. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.34636587]] False\n",
      "Step 2: Reward 0.333.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   1.  4. -1.  2. -2. -1.  2. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.23525475]] False\n",
      "Step 3: Reward 0.222.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  1.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   6.  4.  2.  5. -2.  1. -1. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.16520643]] False\n",
      "Step 4: Reward 0.152.\n",
      "[[ 0.  0.  0. -1.  0.  0. -1.  0.  0. -1.  1. -2.  0.  1. -1.  1. -2.  1.\n",
      "   6.  4.  5.  5. -2.  1. -1. 17. -2.  3. -2. -1.  2. -2. -2. -1.  0.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3071502]] False\n",
      "Step 5: Reward 0.294.\n",
      "[[ 0.  0.  0. -1.  0.  0.  0.  0.  0. -1.  3. -1.  1.  1. -1.  1. -2.  0.\n",
      "   5.  3.  5.  6. -2. -1. -1. 22. -2.  3. -2. -1.  2. -2. -2. -1.  1. -1.\n",
      "  -1. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.42212343]] False\n",
      "Step 6: Reward 0.409.\n",
      "[[ 1.  0.  0. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  1. -2.  0.\n",
      "   5.  3.  4.  4.  0.  0. -1. 24. -2.  1. -2.  2.  1. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.42969918]] False\n",
      "Step 7: Reward 0.417.\n",
      "[[ 1.  0.  2. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  4. -2.  3.\n",
      "   5.  3.  4.  8.  2. -1. -1. 24. -2.  1. -2.  2.  4. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.26303253]] True\n",
      "Step 8: Reward 0.250.\n",
      "Trial: 1, reward: 2.0776046914154334.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -2. -2.  1.\n",
      "  -2. -2.  1. -2.  7. -1. -1. -2. -2.  0. -2.  0. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.20350872]] False\n",
      "Step 9: Reward 0.190.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  1.  0. -1. -2. -2.  0.\n",
      "  -2.  0.  4.  3.  7. -1. -1. -2. -1.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.07824992]] False\n",
      "Step 10: Reward 0.065.\n",
      "1 510\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 1 Train loss 3.943, Test loss 3.948\n",
      "1 510\n",
      "Epoch: 2 Train loss 3.948, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 3 Train loss 3.943, Test loss 3.930\n",
      "1 510\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.918\n",
      "1 510\n",
      "Epoch: 5 Train loss 3.918, Test loss 3.914\n",
      "1 510\n",
      "Epoch: 6 Train loss 3.914, Test loss 3.919\n",
      "1 510\n",
      "Epoch: 7 Train loss 3.919, Test loss 3.928\n",
      "1 510\n",
      "Epoch: 8 Train loss 3.928, Test loss 3.935\n",
      "1 510\n",
      "Epoch: 9 Train loss 3.935, Test loss 3.936\n",
      "1 510\n",
      "Epoch: 10 Train loss 3.936, Test loss 3.931\n",
      "1 510\n",
      "Epoch: 11 Train loss 3.931, Test loss 3.921\n",
      "1 510\n",
      "Epoch: 12 Train loss 3.921, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 13 Train loss 3.911, Test loss 3.903\n",
      "1 510\n",
      "Epoch: 14 Train loss 3.903, Test loss 3.902\n",
      "1 510\n",
      "Epoch: 15 Train loss 3.902, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 16 Train loss 3.905, Test loss 3.909\n",
      "1 510\n",
      "Epoch: 17 Train loss 3.909, Test loss 3.912\n",
      "1 510\n",
      "Epoch: 18 Train loss 3.912, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 19 Train loss 3.911, Test loss 3.908\n",
      "1 510\n",
      "Epoch: 20 Train loss 3.908, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 21 Train loss 3.906, Test loss 3.904\n",
      "1 510\n",
      "Epoch: 22 Train loss 3.904, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 23 Train loss 3.905, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 24 Train loss 3.906, Test loss 3.906\n",
      "Mean train loss: 3.919 Mean val score: 3.918\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1.  0.  0. -1.  0. -1. -1.  0.\n",
      "  -1. -1.  7.  6. 10. -1. -1.  0. -1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20161985]] False\n",
      "Step 11: Reward 0.053.\n",
      "[[ 0.  0.  0.  1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -1. -1.  8.  8. 12. -1. -1.  0. -1.  0. -2. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.1531674]] False\n",
      "Step 12: Reward 0.102.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  0.\n",
      "  -1. -1.  8.  8. 11.  1. -1.  3.  1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12490711]] False\n",
      "Step 13: Reward 0.130.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  0.\n",
      "   3.  0.  8.  8.  9.  1. -1.  3.  2.  0. -2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -1. -1.  0.  0. -2. -1. -1.  0.  0.  0.]] [[-0.04456946]] False\n",
      "Step 14: Reward 0.211.\n",
      "[[ 0.  0.  2. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  2. -1. -1.  0.  0.\n",
      "   4.  0.  9.  9. 10. -1. -1.  3.  3.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1.  1.  6. -1. -2.  2. -1.  0.  0.  0.]] [[0.02421457]] False\n",
      "Step 15: Reward 0.279.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0. -1.  0. -1.  2.  0.  0. -1.  1. -1.  0.  0.\n",
      "   4. -1.  9.  8. 11.  0. -1.  3.  6.  0. -2. -1. -1. -1. -2.  0. -1.  0.\n",
      "  -2. -1.  6. -1. -1.  3. -1.  0.  0.  0.]] [[-0.01402435]] True\n",
      "Step 16: Reward 0.241.\n",
      "Trial: 2, reward: 1.2721946606988341.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   1.  1. -2. -1. -2.  1.  0. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13744873]] False\n",
      "Step 17: Reward 0.118.\n",
      "[[ 1.  0.  1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  1. -1. -1. -1.  4.\n",
      "   1.  1.  2.  1. -2.  1. -1. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.10204709]] False\n",
      "Step 18: Reward 0.357.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  3.\n",
      "   5.  1.  2. -1. -2.  5.  0.  0. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.06490421]] False\n",
      "Step 19: Reward 0.320.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0.\n",
      "   7.  1.  0.  2. -2.  6.  0.  1. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[0.00653213]] False\n",
      "Step 20: Reward 0.262.\n",
      "1 520\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.935\n",
      "1 520\n",
      "Epoch: 1 Train loss 3.935, Test loss 3.933\n",
      "1 520\n",
      "Epoch: 2 Train loss 3.933, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 3 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 5 Train loss 3.930, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 6 Train loss 3.931, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 7 Train loss 3.932, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 8 Train loss 3.932, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 9 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 10 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 11 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 12 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 13 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 14 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 15 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 16 Train loss 3.930, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 17 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 18 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 19 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 20 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 21 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 22 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 23 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 24 Train loss 3.929, Test loss 3.929\n",
      "Mean train loss: 3.931 Mean val score: 3.930\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1. -1. -1.  3. -1.\n",
      "   7.  4.  0.  5. -2.  4. -1.  1.  0.  1.  2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.0207608]] False\n",
      "Step 21: Reward 0.322.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  1.  3. -1.\n",
      "   7.  4.  0.  5.  0.  4. -1.  1.  6. -1.  6. -1. -1. -1. -2. -1. -1.  0.\n",
      "  -2.  2. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.04603767]] False\n",
      "Step 22: Reward 0.296.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1.  3. -1.  1. -1. -1.  0. -1.  2.  1.  3.  1.\n",
      "   7.  4. -2.  5.  2.  4. -1.  1.  9. -1.  4. -1. -1. -1. -2. -1. -1.  0.\n",
      "   0.  4.  0. -1. -1. -2. -1.  0.  0.  0.]] [[-0.00511685]] False\n",
      "Step 23: Reward 0.337.\n",
      "[[ 0.  1. -1. -1.  0. -1.  2.  3.  0. -1. -1. -1.  0.  2.  1.  0.  3.  0.\n",
      "   7.  5.  3.  3.  3.  4. -1.  1. 16.  0.  3. -1.  1.  0. -2. -1. -1. -1.\n",
      "   0.  4.  2. -1.  1. -1. -1.  0.  0.  0.]] [[-0.02454716]] True\n",
      "Step 24: Reward 0.318.\n",
      "Trial: 3, reward: 2.3298202934631616.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   5. -2.  3.  0. -2. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.34246624]] False\n",
      "Step 25: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   4. -2.  3.  6. -2. -1. -1. -2.  0. -1.  1.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3154392]] False\n",
      "Step 26: Reward 0.027.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "   3. -2.  3.  8.  4. -1. -1.  0.  0. -1.  1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.20785084]] False\n",
      "Step 27: Reward 0.135.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.\n",
      "   1.  1.  1. 11.  2. -1.  0.  0.  0.  0. -1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.03691068]] False\n",
      "Step 28: Reward 0.306.\n",
      "[[-1.  0. -1.  0.  0. -1. -1.  2.  0. -1.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   1.  1.  0. 12.  2. -1.  0.  0. -1.  3. -1. -1. -1. -1. -2.  0. -1.  1.\n",
      "  -2. -1.  2. -1. -2. -1.  1.  0.  0.  0.]] [[-0.00592777]] False\n",
      "Step 29: Reward 0.337.\n",
      "[[-1.  1. -1. -1.  1. -1. -1.  1. -1.  0.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   0.  0.  0. 12. -1. -1. -1.  3.  3.  2.  2. -1. -1. -1. -2. -1. -1.  3.\n",
      "   1.  0.  3. -1. -2. -1.  1.  0.  0.  0.]] [[0.0934312]] False\n",
      "Step 30: Reward 0.436.\n",
      "1 530\n",
      "Epoch: 0 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 1 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 2 Train loss 3.945, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 3 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 4 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 5 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 6 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 7 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 8 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 9 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 10 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 11 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 12 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 13 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 14 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 15 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 16 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 17 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 18 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 19 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 20 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 21 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 22 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 23 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 24 Train loss 3.944, Test loss 3.944\n",
      "Mean train loss: 3.944 Mean val score: 3.944\n",
      "[[-1.  0. -1.  0.  2. -1. -1.  0. -1. -1.  1. -1.  1.  1. -1.  1.  3. -1.\n",
      "  -1.  0.  1. 13. -2. -1.  0.  3.  5.  3.  2. -1. -1. -1. -2.  2. -1.  0.\n",
      "   0.  3.  4. -1. -2. -1.  1.  0.  0.  0.]] [[0.12568054]] False\n",
      "Step 31: Reward 0.442.\n",
      "[[-1.  0.  2.  0.  0. -1.  0.  0. -1. -1.  2. -1.  1.  2. -1.  3.  5.  0.\n",
      "  -1.  3.  1. 15. -2.  0.  0.  3.  6.  3.  4. -1. -1. -1. -2.  2.  1.  1.\n",
      "   1.  3.  4. -1. -2.  1.  1.  0.  0.  0.]] [[0.12454933]] True\n",
      "Step 32: Reward 0.441.\n",
      "Trial: 4, reward: 2.1231180275297925.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "  -2.  3. -2. -2.  3. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31662714]] False\n",
      "Step 33: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1.  0.\n",
      "  -2.  5. -2. -2.  7. -1.  2. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.2990833]] False\n",
      "Step 34: Reward 0.018.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1. -1.\n",
      "   1.  5. -2. -1.  7. -1. -1.  0.  1. -1. -2.  0.  2. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12615095]] False\n",
      "Step 35: Reward 0.190.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2.  1. -1. -1.\n",
      "   4.  5. -2. -1.  5. -1. -1. -1.  3. -1.  0. -1.  0. -1. -2. -1. -1. -1.\n",
      "  -2.  0. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.11095339]] False\n",
      "Step 36: Reward 0.206.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0.  3. -1.  4.\n",
      "   8.  5. -2. -2.  8. -1. -1.  0. -1. -1.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1.  0.  0.  0.  0.  0.]] [[-0.05114043]] False\n",
      "Step 37: Reward 0.265.\n",
      "[[-1.  0.  3. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -1.  1.  3.  0.  3.\n",
      "   8.  5. -2. -2. 10.  0. -1.  1. -1. -1. -2.  1. -1. -1. -2. -1.  1. -1.\n",
      "   0.  1. -1. -1. -1. -1.  0.  0.  0.  0.]] [[-0.04143333]] False\n",
      "Step 38: Reward 0.275.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  0.  3.  3.  6.\n",
      "   8.  5. -2. -1. 14. -1. -1.  1. -1. -1.  4.  5. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[0.11563092]] False\n",
      "Step 39: Reward 0.432.\n",
      "[[ 2.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  1.  3.  6.  6.\n",
      "   8.  7.  4. -1. 14. -1. -1.  1. -1. -1.  4.  4. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[-0.06662714]] True\n",
      "Step 40: Reward 0.250.\n",
      "Trial: 5, reward: 1.6366323976200197.\n",
      "1 540\n",
      "Epoch: 0 Train loss 3.981, Test loss 3.981\n",
      "1 540\n",
      "Epoch: 1 Train loss 3.981, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 2 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 3 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 4 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 5 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 6 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 7 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 8 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 9 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 10 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 11 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 12 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 13 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 14 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 15 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 16 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 17 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 18 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 19 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 20 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 21 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 22 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 23 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 24 Train loss 3.980, Test loss 3.980\n",
      "Mean train loss: 3.980 Mean val score: 3.980\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2. -1. -1.  1.\n",
      "  -2. -2.  0. -2.  0.  2. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3091395]] False\n",
      "Step 41: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -1.  3.\n",
      "   0.  0.  0.  0. -1.  2. -1.  1. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19086051]] False\n",
      "Step 42: Reward 0.500.\n",
      "[[-1.  0.  1. -1.  2. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0. -1. -1. -1.\n",
      "   0.  3.  1.  6. -2.  3. -1.  1. -2. -1. -2.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.10858202]] False\n",
      "Step 43: Reward 0.418.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1.  0.  0.  2.  0.\n",
      "   1.  2.  3.  5. -2.  2. -1.  3. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.00556806]] False\n",
      "Step 44: Reward 0.304.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1.  0. -1.  2.  3.\n",
      "   4.  4.  5.  7. -1. -1. -1.  3. -2.  0. -1. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.05224666]] False\n",
      "Step 45: Reward 0.361.\n",
      "[[-1.  0.  0.  1.  1.  0.  0. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1.  4.\n",
      "   4.  5.  5.  9.  2. -1. -1.  3. -2. -1.  3.  0. -1.  0. -2.  2. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.09974939]] False\n",
      "Step 46: Reward 0.409.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  0.  0.  1. -1.  0.  1.\n",
      "   7. 10.  7.  9.  1.  0. -1.  3. -1. -1.  3. -1. -1.  0. -1. -1. -1. -1.\n",
      "  -2.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.03917512]] False\n",
      "Step 47: Reward 0.348.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  3.  0.  1. -1.  3.  1.\n",
      "   7. 11.  7.  9.  2.  1. -1.  3. -1. -1.  3. -1. -1.  0. -1.  1. -1. -1.\n",
      "   0.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10913949]] True\n",
      "Step 48: Reward 0.200.\n",
      "Trial: 6, reward: 2.539882581803094.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0. -1.  1. -1. -1.\n",
      "  -2.  1. -2. -1.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.04247281]] False\n",
      "Step 49: Reward 0.267.\n",
      "[[-1.  1. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "  -2.  3.  0.  2. -2. -1.  1. -2. -2.  1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.0438017]] False\n",
      "Step 50: Reward 0.353.\n",
      "1 550\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  6. -1. -1.\n",
      "  -2.  8.  1.  2. -2. -1.  3. -2. -2.  1. -1. -1. -1. -1. -2.  1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03686938]] False\n",
      "Step 51: Reward 0.277.\n",
      "[[-1.  0. -1. -1.  0. -1.  1. -1. -1. -1. -1. -1.  0.  0. -1.  7. -1. -1.\n",
      "  -2.  8.  1.  2.  0.  2.  0. -2.  1.  0. -2.  0. -1. -1. -2.  0.  0. -1.\n",
      "  -2.  0. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02903968]] False\n",
      "Step 52: Reward 0.285.\n",
      "[[ 0.  0.  0. -1.  0. -1.  2. -1. -1. -1. -1.  0.  0. -1.  1.  7. -1. -1.\n",
      "   2. 10.  0.  1. -1. -1. -1.  2.  0. -1. -2. -1.  0. -1. -2.  0. -1.  3.\n",
      "  -1. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[0.01174176]] False\n",
      "Step 53: Reward 0.325.\n",
      "[[-1.  1.  0. -1.  0. -1.  1.  0. -1.  2. -1.  0.  0. -1.  6.  7.  1. -1.\n",
      "   4.  7.  0.  0.  2. -1. -1.  2.  2. -1. -1.  1.  0.  0. -2. -1. -1.  3.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.17787033]] False\n",
      "Step 54: Reward 0.492.\n",
      "[[ 1.  1. -1. -1.  0. -1. -1. -1. -1.  4.  0.  2.  0.  0.  5.  7.  1.  1.\n",
      "   4.  8.  0.  0.  2. -1. -1.  3.  4. -1.  1.  1.  1. -1. -2. -1. -1.  2.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.10942185]] False\n",
      "Step 55: Reward 0.423.\n",
      "[[ 0.  1. -1. -1.  1.  0. -1. -1. -1.  4.  2.  2.  0. -1.  7. 10.  1. -1.\n",
      "   4.  8.  0.  3.  2. -1. -1.  6.  4. -1.  1.  1.  2. -1. -2. -1. -1.  1.\n",
      "   2.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.18634492]] True\n",
      "Step 56: Reward 0.500.\n",
      "Trial: 7, reward: 2.921008114240916.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  4.\n",
      "   0. -2. -2. -2. -1. -1.  1. -2. -2. -1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.16365507]] False\n",
      "Step 57: Reward 0.150.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      "   3. -2. -2. -2. -1.  1.  0. -2. -1. -1. -1.  0. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.01540947]] False\n",
      "Step 58: Reward 0.298.\n",
      "[[-1.  0.  0. -1.  0. -1. -1.  0.  1.  0. -1. -1.  2. -1. -1. -1.  4. -1.\n",
      "   8. -1. -2. -2. -1.  1. -1.  1. -2.  0.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.00258425]] False\n",
      "Step 59: Reward 0.316.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0.  0.  0. -1. -1. -1.  7. -1.\n",
      "   7. -1. -1. -2. -2.  1. -1.  0. -2. -1. -2. -1.  1. -1. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -1. -1.  0.  0.  0.]] [[0.04913563]] False\n",
      "Step 60: Reward 0.363.\n",
      "1 560\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0.  1. -1.  1.  1. -1. -1. -1.  9.  0.\n",
      "   8. -2. -1. -1. -2.  2. -1.  0.  2. -1.  1.  1.  0. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -1.  1. -1.  0.  0.  0.]] [[-0.05258617]] False\n",
      "Step 61: Reward 0.261.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1. -1. -1. 14. -1.\n",
      "   9. -1.  1. -1. -2.  2. -1.  0.  6. -1.  0. -1. -1.  0. -1. -1. -1.  1.\n",
      "  -1.  0.  4. -1.  0.  1. -1.  0.  0.  0.]] [[-0.0411129]] False\n",
      "Step 62: Reward 0.272.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1.  1.  0.  1.  0.  1.  1.  0. 18. -1.\n",
      "   7.  0.  1.  1.  0.  0. -1.  0.  8. -1.  0. -1. -1.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  6. -1.  0.  2.  0.  0.  0.  0.]] [[-0.03090695]] False\n",
      "Step 63: Reward 0.283.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1.  0.  0.  0.  1.  0. 18. -1.\n",
      "   7.  1.  1.  2.  0.  0.  1.  3.  9. -1.  4. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1.  6. -1.  0.  4.  0.  0.  0.  0.]] [[-0.07549722]] True\n",
      "Step 64: Reward 0.238.\n",
      "Trial: 8, reward: 2.1815422317524256.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  -2.  4.  1. -2. -2. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31359246]] False\n",
      "Step 65: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   0.  4.  4.  1. -1. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.18640754]] False\n",
      "Step 66: Reward 0.500.\n",
      "[[-1.  0.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   4.  4.  6.  5. -2. -1. -1.  1. -2.  1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13637727]] False\n",
      "Step 67: Reward 0.177.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1. -1.  2.  0.  0.\n",
      "   2.  5.  5.  4. -2.  0.  0.  1. -2. -1. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.07549722]] False\n",
      "Step 68: Reward 0.238.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1. -1.  4.  4.  0.\n",
      "   2.  4.  5.  9. -2. -1. -1.  1. -2.  0. -1. -1.  1.  0. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.00323921]] False\n",
      "Step 69: Reward 0.317.\n",
      "[[ 0.  0. -1.  1.  1.  0.  1. -1. -1. -1. -1. -1.  0. -1.  0.  1.  3. -1.\n",
      "   3.  5.  5. 17. -1. -1. -1.  1. -1. -1.  1.  0.  1.  0. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.02862975]] False\n",
      "Step 70: Reward 0.342.\n",
      "1 570\n",
      "Epoch: 0 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 1 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 2 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 3 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 4 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 5 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 6 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 7 Train loss 4.014, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 8 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 9 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 10 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 11 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 12 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 13 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 14 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 15 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 16 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 17 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 18 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 19 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 20 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 21 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 22 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 23 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 24 Train loss 4.014, Test loss 4.014\n",
      "Mean train loss: 4.014 Mean val score: 4.014\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  0.  0.  0.  0. -1.  5.  1.  0.\n",
      "   3.  4.  6. 17.  3.  1. -1.  3.  2. -1.  1. -1.  1.  0. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.03747821]] False\n",
      "Step 71: Reward 0.275.\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  2.  0.  0.  3. -1.  5.  6.  0.\n",
      "   3.  4.  6. 17.  1.  2. -1.  3.  2. -1.  1. -1.  2.  0. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.01275909]] True\n",
      "Step 72: Reward 0.300.\n",
      "Trial: 9, reward: 2.1496452322355992.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2. -2. -2. -2.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09053688]] False\n",
      "Step 73: Reward 0.222.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "  -2.  1. -2. -2.  4.  2. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19609243]] False\n",
      "Step 74: Reward 0.117.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "  -2.  1. -2.  1.  3.  1. -1.  1.  1. -1.  0. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.0705716]] False\n",
      "Step 75: Reward 0.242.\n",
      "[[-1.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.  5.  0.\n",
      "  -2.  1.  3.  3.  4.  1. -1.  1.  3. -1. -1. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.13094091]] False\n",
      "Step 76: Reward 0.182.\n",
      "[[-1.  0. -1. -1.  1. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0.  1.  5. -1.\n",
      "   0.  1.  3.  4.  5. -1. -1.  1.  8. -1. -2. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.15935001]] False\n",
      "Step 77: Reward 0.153.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.  5.  5.  0.\n",
      "   0.  0.  2.  5.  1.  0. -1.  1. 13. -1. -1. -1.  0.  2.  4. -1. -1.  2.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.12814371]] False\n",
      "Step 78: Reward 0.185.\n",
      "[[-1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1.  2.  6.  5. -1.\n",
      "   0.  2.  0.  8.  3.  2. -1.  1. 13. -1. -2. -1.  0.  0.  4. -1. -1.  2.\n",
      "  -1.  0. -2. -1. -2. -2.  1.  0.  0.  0.]] [[-0.04515347]] False\n",
      "Step 79: Reward 0.268.\n",
      "[[-1.  0. -1. -1.  0.  0. -1.  0. -1.  1.  3.  0.  0. -1.  2.  6.  5.  2.\n",
      "  -1.  2.  0.  9.  5.  1. -1.  3. 14. -1. -2.  3.  0. -1.  4. -1. -1.  2.\n",
      "   0.  0. -1. -1. -2. -2.  2.  0.  0.  0.]] [[-0.07201836]] True\n",
      "Step 80: Reward 0.241.\n",
      "Trial: 10, reward: 1.609265420775104.\n",
      "1 580\n",
      "Epoch: 0 Train loss 4.045, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 1 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 2 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 3 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 4 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 5 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 6 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 7 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 8 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 9 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 10 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 11 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 12 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 13 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 14 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 15 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 16 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 17 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 18 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 19 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 20 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 21 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 22 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 23 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 24 Train loss 4.044, Test loss 4.044\n",
      "Mean train loss: 4.044 Mean val score: 4.044\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0. -1. -2. -1.  5. -2. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.23142757]] False\n",
      "Step 81: Reward 0.080.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1. -1. -1. -2.  4.\n",
      "   0. -2.  0. -1. -2. -1.  7. -2. -2.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.07458545]] False\n",
      "Step 82: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1. -1. -1.  0. -1. -1.  0. -1.  1. -1. -1. -1.  1.  3.\n",
      "   9. -2.  0. -2. -2. -1.  5.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.01615864]] False\n",
      "Step 83: Reward 0.328.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1.  2. -1.  0. -1. -1. -1.  1.  3.\n",
      "  11.  4.  0. -2. -2. -1. -1.  8. -2. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -1. -1. -2. -1. -2.  0.  0.  0.]] [[0.05220881]] False\n",
      "Step 84: Reward 0.364.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1.  0. -1.  1.  1.\n",
      "   9.  4.  2. -2. -2.  0. -1. 13. -2. -1. -2.  4. -1.  1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -1.  0. -2.  0.  0.  0.]] [[-0.08013505]] False\n",
      "Step 85: Reward 0.231.\n",
      "[[-1.  0.  0.  0.  0. -1.  1. -1. -1.  0. -1. -1.  2.  1.  3. -1.  2. -1.\n",
      "   8.  5.  2.  1. -1. -1.  0. 13. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2.  0. -2. -1. -2.  0. -2.  0.  0.  0.]] [[0.09568706]] False\n",
      "Step 86: Reward 0.407.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  6.  0.  0. -1.  2.  1.  4.  0.\n",
      "  10.  5.  1.  3.  0. -1. -1. 12. -2. -1.  4. -1.  0.  1. -1. -1. -1. -1.\n",
      "  -1. -1.  2. -1.  0.  0. -1.  0.  0.  0.]] [[0.07197165]] False\n",
      "Step 87: Reward 0.383.\n",
      "[[-1.  0.  0. -1.  0.  1. -1. -1.  0.  1.  3. -1.  0.  0.  3.  1.  4.  1.\n",
      "  10.  5.  1.  4. -2. -1.  0. 12. -2. -1.  7. -1. -1. -1.  0. -1. -1. -1.\n",
      "   3. -1.  2. -1.  4.  0. -1.  0.  0.  0.]] [[-0.0816139]] True\n",
      "Step 88: Reward 0.230.\n",
      "Trial: 11, reward: 2.2596846913912443.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  2. -1. -2.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19604295]] False\n",
      "Step 89: Reward 0.115.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  3.\n",
      "  -1.  2.  0. -2.  0.  6. -1. -2. -2.  1. -2. -1. -1. -1. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19142756]] False\n",
      "Step 90: Reward 0.120.\n",
      "1 590\n",
      "Epoch: 0 Train loss 4.066, Test loss 4.066\n",
      "1 590\n",
      "Epoch: 1 Train loss 4.066, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 2 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 3 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 4 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 5 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 6 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 7 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 8 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 9 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 10 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 11 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 12 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 13 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 14 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 15 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 16 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 17 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 18 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 19 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 20 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 21 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 22 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 23 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 24 Train loss 4.065, Test loss 4.065\n",
      "Mean train loss: 4.065 Mean val score: 4.065\n",
      "[[-1.  2. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  3.  3.  0.  0.  5. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.22695008]] False\n",
      "Step 91: Reward 0.083.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1. -1.\n",
      "   0.  3.  3.  3. -1.  9. -1.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2.  0. -2. -1. -1. -2. -2.  0.  0.  0.]] [[-0.13638127]] False\n",
      "Step 92: Reward 0.173.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1.  1. -1.\n",
      "  -1.  4.  7. -1.  0.  7. -1.  5. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.\n",
      "   2.  0. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.12800482]] False\n",
      "Step 93: Reward 0.182.\n",
      "[[-1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0.  0. -1. -1.  4. -1.\n",
      "  -1.  5.  9.  0. -2.  7. -1.  5. -2. -1.  6. -1. -1. -1. -1. -1. -1.  1.\n",
      "   5. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.07412507]] False\n",
      "Step 94: Reward 0.236.\n",
      "[[-1.  0. -1.  0.  0.  1.  0. -1. -1. -1.  0.  0.  1. -1.  4. -1.  5. -1.\n",
      "  -1.  2.  9. -2.  0.  8.  0.  7. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.\n",
      "   9. -1. -1. -1. -1. -2.  2.  0.  0.  0.]] [[-0.06379126]] False\n",
      "Step 95: Reward 0.246.\n",
      "[[-1.  0.  1.  1.  0. -1. -1.  0. -1. -1.  1. -1.  1. -1.  4.  1.  5. -1.\n",
      "   0.  2. 11. -2. -1.  8. -1.  8. -1. -1.  1. -1. -1. -1.  0. -1. -1.  1.\n",
      "  11. -1. -1. -1. -1. -2.  4.  0.  0.  0.]] [[0.02351034]] True\n",
      "Step 96: Reward 0.333.\n",
      "Trial: 12, reward: 1.4885804796654487.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  0.  3. -1.  0. -1. -1. -2. -2. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19217595]] False\n",
      "Step 97: Reward 0.118.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2.  0.\n",
      "  -2.  0.  3. -1. -1.  2. -1. -2.  1. -1.  3. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09243171]] False\n",
      "Step 98: Reward 0.217.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2. -1.\n",
      "  -2.  0.  3.  4. -2.  3.  0. -2.  1. -1.  3.  5. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.159823]] False\n",
      "Step 99: Reward 0.150.\n",
      "[[-1.  2.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  3. -2. -1.\n",
      "   1. -2.  4. 11. -1.  6. -1. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.15090828]] False\n",
      "Step 100: Reward 0.159.\n",
      "1 600\n",
      "Epoch: 0 Train loss 4.076, Test loss 4.076\n",
      "1 600\n",
      "Epoch: 1 Train loss 4.076, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 2 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 3 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 4 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 5 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 6 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 7 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 8 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 9 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 10 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 11 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 12 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 13 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 14 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 15 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 16 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 17 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 18 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 19 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 20 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 21 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 22 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 23 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 24 Train loss 4.075, Test loss 4.075\n",
      "Mean train loss: 4.075 Mean val score: 4.075\n",
      "[[ 0.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.  4. -2. -1.\n",
      "   2. -2.  5. 15. -1.  3. -1.  1. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2.  0. -2. -1. -2.  0.  0.  0.]] [[-0.07353824]] False\n",
      "Step 101: Reward 0.235.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  6. -2. -1.\n",
      "   3. -2.  5. 16. -2.  1.  0.  4.  2.  0. -2. -1. -1.  0. -1.  1.  1.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.05061775]] False\n",
      "Step 102: Reward 0.258.\n",
      "[[ 0.  0.  0. -1.  1. -1.  0. -1.  0.  2.  2. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  5. 18.  0.  5.  0.  5.  2. -1. -2.  1. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.03281066]] False\n",
      "Step 103: Reward 0.276.\n",
      "[[ 0.  0. -1. -1.  1. -1.  0. -1.  0.  2.  4. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  7. 18.  0.  5.  0.  5.  2.  4. -2.  5. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.01420319]] True\n",
      "Step 104: Reward 0.294.\n",
      "Trial: 13, reward: 1.7060666329221275.\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 4. 0. 0.\n",
      "  0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0.6412164]] False\n",
      "Step 105: Reward 0.250.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   0.  0.  0.  2.  1. -1.  0.  3. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19167915]] False\n",
      "Step 106: Reward 0.500.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1. -1.  1.  1.\n",
      "   0.  3. -2.  3.  0.  0. -1.  2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03309149]] False\n",
      "Step 107: Reward 0.275.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  1. -1. -1.  1.  2.\n",
      "   0.  5.  1.  5.  2. -1.  0.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02783304]] False\n",
      "Step 108: Reward 0.280.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0.  0. -1. -1.  7.  1.\n",
      "   2.  4.  0.  6.  0. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  0. -2.  0.  0.  0.  0.]] [[-0.03641751]] False\n",
      "Step 109: Reward 0.272.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  0.  3.  5. -1.\n",
      "   1.  4.  1.  8. -1. -1. -1.  0. -2. -1. -2. -1. -1. -1.  0. -1. -1.  0.\n",
      "  -2.  1.  1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.04998752]] False\n",
      "Step 110: Reward 0.258.\n",
      "1 610\n",
      "Epoch: 0 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 1 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 2 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 3 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 4 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 5 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 6 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 7 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 8 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 9 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 10 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 11 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 12 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 13 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 14 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 15 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 16 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 17 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 18 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 19 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 20 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 21 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 22 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 23 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 24 Train loss 4.097, Test loss 4.097\n",
      "Mean train loss: 4.097 Mean val score: 4.097\n",
      "[[ 0.  0.  0. -1.  0.  1.  0. -1.  2.  1. -1.  2.  0.  3. -1.  3.  7.  3.\n",
      "   1.  6.  1.  9. -2. -1. -1.  1. -1.  0. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[-0.07657747]] False\n",
      "Step 111: Reward 0.232.\n",
      "[[ 0.  0. -1. -1.  0.  1. -1. -1.  4.  1. -1.  2.  0.  3. -1.  6.  9.  6.\n",
      "   1.  6.  1.  9.  0. -1. -1.  1.  2.  0.  1. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[0.01433161]] True\n",
      "Step 112: Reward 0.323.\n",
      "Trial: 14, reward: 2.390206018685877.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0.  1.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20824903]] False\n",
      "Step 113: Reward 0.100.\n",
      "[[ 0.  1. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1. -1.  0. -2. -1.\n",
      "   1.  0.  1.  0.  3.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03165329]] False\n",
      "Step 114: Reward 0.277.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -2. -1.\n",
      "   2.  1.  1. -2.  4.  1.  1. -2. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -2.  0. -2. -1. -2.  0.  0.  0.]] [[0.0080775]] False\n",
      "Step 115: Reward 0.316.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0. -1.  0.  0. -1. -1. -2.  0.\n",
      "   8.  2.  2. -1.  5. -1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2.  0.  0. -1. -2.  0.  0.  0.]] [[0.07865575]] False\n",
      "Step 116: Reward 0.387.\n",
      "[[ 0.  0. -1. -1.  0. -1.  0. -1. -1.  0.  2.  1.  0.  0. -1. -1.  1.  1.\n",
      "   9.  1.  2. -1.  6.  0. -1. -2. -2. -1. -2. -1.  0. -1. -1.  2.  0. -1.\n",
      "  -2. -1. -2.  0.  0.  0. -2.  0.  0.  0.]] [[0.05909792]] False\n",
      "Step 117: Reward 0.367.\n",
      "[[ 0.  2. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0.  2.  1. -1.\n",
      "   9.  2.  5.  6.  5.  0.  0. -2. -1. -1. -2.  0.  0. -1. -1. -1. -1. -1.\n",
      "   1.  1. -2.  0.  2.  1. -2.  0.  0.  0.]] [[0.04360282]] False\n",
      "Step 118: Reward 0.352.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  1.  1.  6. -1.\n",
      "   9.  3.  5.  6.  4.  5. -1. -2.  1. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "   1.  1.  2.  1.  2.  1. -1.  0.  0.  0.]] [[-0.09203281]] False\n",
      "Step 119: Reward 0.216.\n",
      "[[ 0.  4. -1.  0.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  4.  0.  5.  0.\n",
      "   9.  2.  8.  7.  2.  9. -1. -2.  1. -1. -2. -1. -1. -1. -1. -1.  2. -1.\n",
      "   1.  2.  1.  1.  2.  1. -1.  0.  0.  0.]] [[-0.14450634]] True\n",
      "Step 120: Reward 0.164.\n",
      "Trial: 15, reward: 2.1789847340999158.\n",
      "1 620\n",
      "Epoch: 0 Train loss 4.102, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 1 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 2 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 3 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 4 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 5 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 6 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 7 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 8 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 9 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 10 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 11 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 12 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 13 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 14 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 15 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 16 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 17 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 18 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 19 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 20 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 21 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 22 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 23 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 24 Train loss 4.101, Test loss 4.101\n",
      "Mean train loss: 4.101 Mean val score: 4.101\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   1. -2.  2. -2.  0.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 121: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  2. -1.\n",
      "   0.  0.  5. -2.  2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19229937]] False\n",
      "Step 122: Reward 0.500.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  4.  1.\n",
      "  -1. -1.  7. -2.  2.  1. -1. -2.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.13378759]] False\n",
      "Step 123: Reward 0.174.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1.  4. -1.\n",
      "   1.  1.  7. -2. -2.  1. -1. -2.  4. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10890545]] False\n",
      "Step 124: Reward 0.199.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0.  1. -1. -1.  0.  0.  0. -1.  2.  7. -1.\n",
      "   0.  2.  6.  0. -2.  0.  0. -2.  3. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  0. -1.  0. -1. -2.  0.  0.  0.  0.]] [[-0.07061857]] False\n",
      "Step 125: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1.  3. -1.  0. -1. -1. -1.  1.  0.  3. -1.  2.  7. -1.\n",
      "   0.  1.  6.  0. -1. -1. -1. -2.  7.  2.  0. -1. -1. -1. -1. -1.  0. -1.\n",
      "   1. -1. -2. -1.  0. -2.  0.  0.  0.  0.]] [[-0.01636204]] False\n",
      "Step 126: Reward 0.291.\n",
      "[[ 1.  1. -1. -1.  1.  0.  0. -1. -1.  3. -1.  2.  0.  3. -1.  4.  7.  0.\n",
      "   2.  3.  6.  2. -1.  0.  2.  0.  8.  2. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.01676744]] False\n",
      "Step 127: Reward 0.324.\n",
      "[[ 1.  2. -1. -1.  2.  0. -1. -1. -1.  3. -1.  2.  1.  3. -1.  4. 10.  0.\n",
      "   2.  3.  6.  5. -1.  0.  2.  3. 12.  1. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.02563271]] True\n",
      "Step 128: Reward 0.333.\n",
      "Trial: 16, reward: 2.058930292187335.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   2.  3. -2. -2. -2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 129: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   3.  2.  3. -2. -1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.11722444]] False\n",
      "Step 130: Reward 0.190.\n",
      "1 630\n",
      "Epoch: 0 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 1 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 2 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 3 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 4 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 5 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 6 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 7 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 8 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 9 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 10 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 11 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 12 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 13 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 14 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 15 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 16 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 17 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 18 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 19 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 20 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 21 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 22 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 23 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 24 Train loss 4.092, Test loss 4.092\n",
      "Mean train loss: 4.092 Mean val score: 4.092\n",
      "[[ 1.  0.  0. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "   7.  1.  5. -2. -1.  1. -1. -2. -1. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.05655059]] False\n",
      "Step 131: Reward 0.250.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1.  2. -1.\n",
      "   6.  3.  6. -1. -1.  0.  0. -2.  2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.06126757]] False\n",
      "Step 132: Reward 0.245.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   5.  5.  6.  4.  0. -1.  0. -2.  4. -1. -2.  0. -1. -1. -1.  1. -1.  0.\n",
      "   0. -1. -2. -1. -1. -1. -2.  0.  0.  0.]] [[-0.0413332]] False\n",
      "Step 133: Reward 0.265.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "   7.  6.  5. 10. -1. -1. -1. -2.  4. -1. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "   0.  1. -2. -1.  1.  0. -1.  0.  0.  0.]] [[-0.11754715]] False\n",
      "Step 134: Reward 0.189.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1.  0.  5. -1.\n",
      "   7.  9.  3. 10. -2. -1. -1.  2.  6. -1. -2. -1. -1.  0. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.18106039]] False\n",
      "Step 135: Reward 0.125.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1.  0. -1.  0. -1. -1.  4.  5. -1.\n",
      "   7. 10.  6.  7. -1. -1. -1.  2.  7.  0. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.08027323]] True\n",
      "Step 136: Reward 0.226.\n",
      "Trial: 17, reward: 1.4917476054157848.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  1.  1.  1.  1. -1.  2. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3065506]] False\n",
      "Step 137: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -2. -1.\n",
      "  -2.  4.  1.  1.  3. -1.  3. -2. -2. -1. -2.  2. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3601161]] False\n",
      "Step 138: Reward 0.667.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  2. -1. -1. -1. -1.  0. -1.  0. -1. -2. -1.\n",
      "  -2.  6.  3.  5. -2.  0.  0.  3. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.04639059]] False\n",
      "Step 139: Reward 0.353.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  1. -1.  3. -1.  1. -1.\n",
      "  -2.  9.  2.  7. -2. -1. -1.  3. -2. -1. -1. -1.  2. -1. -1. -1.  0.  0.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[-0.00655058]] False\n",
      "Step 140: Reward 0.300.\n",
      "1 640\n",
      "Epoch: 0 Train loss 4.094, Test loss 4.094\n",
      "1 640\n",
      "Epoch: 1 Train loss 4.094, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 2 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 3 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 4 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 5 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 6 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 7 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 8 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 9 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 10 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 11 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 12 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 13 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 14 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 15 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 16 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 17 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 18 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 19 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 20 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 21 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 22 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 23 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 24 Train loss 4.093, Test loss 4.093\n",
      "Mean train loss: 4.093 Mean val score: 4.093\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0. -1.  4.  0.  2. -1.\n",
      "  -1. 11.  2.  7. -2. -1. -1.  2.  0.  0.  0. -1.  5. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1.  1. -1.  0.  0.  0.  0.]] [[-0.04637557]] False\n",
      "Step 141: Reward 0.259.\n",
      "[[ 0.  1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  5.  0.  3.  0.\n",
      "  -1.  9.  4.  7.  3. -1. -1.  2.  1.  0.  0. -1.  3. -1. -1.  1. -1. -1.\n",
      "  -2.  0. -2. -1.  1.  0.  0.  0.  0.  0.]] [[-0.06239158]] False\n",
      "Step 142: Reward 0.243.\n",
      "[[ 1.  0. -1.  1.  0. -1. -1. -1. -1.  0. -1.  1.  0.  0.  4.  0.  4.  0.\n",
      "   1. 14.  4.  7.  8.  1. -1.  3.  3. -1. -1. -1.  3. -1. -1. -1.  0.  0.\n",
      "  -1.  0. -2. -1.  1.  0.  2.  0.  0.  0.]] [[0.00300714]] False\n",
      "Step 143: Reward 0.309.\n",
      "[[ 1.  0. -1.  2.  0. -1. -1. -1. -1.  0. -1.  1.  0.  3.  4.  0.  4.  0.\n",
      "   1. 14.  5.  7. 11.  1. -1.  8.  3. -1. -2. -1.  3.  1. -1. -1. -1. -1.\n",
      "  -1.  0. -2. -1.  1.  0.  3.  0.  0.  0.]] [[0.08325407]] True\n",
      "Step 144: Reward 0.389.\n",
      "Trial: 18, reward: 2.519641209837288.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  4. -2. -1.  1. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.2671733]] False\n",
      "Step 145: Reward 0.038.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   2.  6. -1. -1.  0.  0. -1.  1. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.18563482]] False\n",
      "Step 146: Reward 0.120.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  7. -1.  2.  2. -1.  0.  4.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -1.  0.  0.  0.]] [[-0.21723703]] False\n",
      "Step 147: Reward 0.088.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1. -1.\n",
      "   4.  8. -1.  2.  4.  0. -1.  6. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.14845325]] False\n",
      "Step 148: Reward 0.157.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1. -1. -1.\n",
      "   6. 11.  4. -3.  3. -1. -1.  6. -1. -1.  1. -1. -1.  0. -1. -1. -1. -1.\n",
      "   2. -1. -2. -1. -2.  0.  0.  0.  0.  0.]] [[-0.11958832]] False\n",
      "Step 149: Reward 0.186.\n",
      "[[ 0.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   6. 11.  5. -1.  1.  0. -1.  7. -2. -1.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "   4. -1.  1. -1. -2.  0.  0.  0.  0.  0.]] [[-0.13629845]] False\n",
      "Step 150: Reward 0.169.\n",
      "1 650\n",
      "Epoch: 0 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 1 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 2 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 3 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 4 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 5 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 6 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 7 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 8 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 9 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 10 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 11 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 12 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 13 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 14 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 15 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 16 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 17 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 18 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 19 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 20 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 21 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 22 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 23 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 24 Train loss 4.115, Test loss 4.115\n",
      "Mean train loss: 4.115 Mean val score: 4.115\n",
      "[[ 0.  0. -1.  0.  0.  1.  0. -1. -1. -1. -1.  0.  0. -1.  1. -1.  0. -1.\n",
      "   6.  8.  6. -3.  0. -1. -1. 10. -1.  0.  0. -1. -1.  0.  0. -1. -1. -1.\n",
      "   8. -1.  1. -1. -1. -1.  3.  0.  0.  0.]] [[-0.00797501]] False\n",
      "Step 151: Reward 0.296.\n",
      "[[ 0.  0.  1.  1.  1. -1. -1.  0.  0. -1.  1. -1.  0. -1.  1. -1.  3. -1.\n",
      "   8.  8.  6. -3. -2.  3. -1. 14. -1.  2.  0. -1. -1. -1.  0. -1. -1. -1.\n",
      "  10. -1.  0. -1. -1. -2.  5.  0.  0.  0.]] [[0.00049061]] True\n",
      "Step 152: Reward 0.305.\n",
      "Trial: 19, reward: 1.3604819974579725.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2.  1. -2.  4. -2. -1. -1. -2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.3042713]] False\n",
      "Step 153: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  1.\n",
      "  -2.  3. -2.  4.  0.  2.  2.  1. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 154: Reward 0.500.\n",
      "[[ 0.  1.  2. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1.  2. -1. -2. -1.\n",
      "   1.  3. -2.  4.  2.  2.  0.  1. -2.  2. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 155: Reward 0.500.\n",
      "[[ 0.  0.  1. -1.  1. -1. -1.  0. -1.  1.  1. -1.  0. -1.  2. -1. -2. -1.\n",
      "   1.  3.  3.  6.  2.  2.  0.  1. -2.  0. -2.  0.  1. -1.  1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.09046555]] False\n",
      "Step 156: Reward 0.395.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  1. -1. -1. -1.  2. -1.  2. -1. -2. -1.\n",
      "   0.  3.  3.  7.  2.  6.  1.  1. -2.  0.  1. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.03641418]] False\n",
      "Step 157: Reward 0.268.\n",
      "[[ 0.  0. -1.  2.  0. -1.  1. -1. -1.  0. -1.  0.  2.  0.  2. -1. -2. -1.\n",
      "   0.  3.  3.  6.  2. 11.  0.  1.  0.  0.  2. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.02685452]] False\n",
      "Step 158: Reward 0.331.\n",
      "[[ 0.  0. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  0.  2.  0.  2. -1.\n",
      "   0.  3.  4.  6.  3. 15. -1.  1.  2.  2.  1.  0.  3. -1.  4. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[-0.12780072]] False\n",
      "Step 159: Reward 0.176.\n",
      "[[ 0.  1. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  2.  5.  2.  2. -1.\n",
      "   3.  3.  4.  6.  3. 16. -1.  1.  3.  1.  1.  0. -1. -1.  2. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.2671573]] True\n",
      "Step 160: Reward 0.571.\n",
      "Trial: 20, reward: 2.7416189724408406.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "all_training_losses = []\n",
    "all_val_scores = []\n",
    "\n",
    "while current_trial < num_episodes:\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "        # --------------- Model Training -----------------\n",
    "        if env_steps % freq_train_model == 0:\n",
    "            dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                model_batch_size,\n",
    "                validation_ratio,\n",
    "                ensemble_size=len(dynamics_model),\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,\n",
    "            )\n",
    "            if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "            training_losses, val_scores = model_trainer.train(\n",
    "                dataset_train,\n",
    "                dataset_val=dataset_val,\n",
    "                num_epochs=num_epochs,\n",
    "                patience=num_epochs,\n",
    "                improvement_threshold=0.01,\n",
    "            )\n",
    "            all_training_losses += training_losses\n",
    "            all_val_scores += val_scores\n",
    "            print(f\"Mean train loss: {np.mean(training_losses):.3f} Mean val score: {np.mean(val_scores):.3f}\")\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        replay_buffer.add(obs, action, next_obs, reward, terminated, truncated)\n",
    "\n",
    "        model_action = action[None, ...]\n",
    "        model_observation = {\n",
    "            \"obs\": obs[None, ...],\n",
    "            \"propagation_indices\": None,\n",
    "        }\n",
    "        model_next_obs, model_reward, model_dones, next_model_state = model_env.step(model_action, model_observation)\n",
    "        #TODO: compare actual next_obs and rewards with model env\n",
    "        print(next_obs-torch.round(model_next_obs).numpy(), reward-model_reward.numpy(), terminated)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "\n",
    "        print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(all_training_losses, 'b', label='Training loss')\n",
    "ax[0].plot(all_val_scores, 'g', label='Eval score')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].plot(np.mean(rewards, axis=-1), 'b', label='Cumulative reward')\n",
    "ax[1].set_xlabel('Episode')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
