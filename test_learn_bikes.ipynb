{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "from mbrl.models import Model\n",
    "from src.env.bikes import Bikes\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load env dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 46]) torch.Size([10000, 10])\n",
      "{'bikes_distr': slice(0, 43, None), 'day': slice(43, 44, None), 'month': slice(44, 45, None), 'time_counter': slice(45, 46, None)} {'truck_centroid': slice(0, 5, None), 'truck_num_bikes': slice(5, 10, None)}\n"
     ]
    }
   ],
   "source": [
    "load_dir = \"datasets/Bikes/None\"\n",
    "path = pathlib.Path(load_dir) / \"replay_buffer.npz\"\n",
    "buffer=np.load(path)\n",
    "next_obs = torch.tensor(buffer[\"next_obs\"], dtype=torch.float32)\n",
    "obs = torch.tensor(buffer[\"obs\"], dtype=torch.float32)\n",
    "act = torch.round(torch.tensor(buffer[\"action\"], dtype=torch.float32))\n",
    "reward = torch.tensor(buffer[\"reward\"], dtype=torch.float32)\n",
    "print(obs.shape, act.shape)\n",
    "\n",
    "num_centroids = 43\n",
    "map_obs = {\n",
    "    \"bikes_distr\": slice(0, num_centroids),\n",
    "    \"day\": slice(num_centroids, 44),\n",
    "    \"month\": slice(44, 45),\n",
    "    \"time_counter\": slice(45, 46),\n",
    "}\n",
    "num_trucks = act.shape[-1]//2\n",
    "map_act={\n",
    "    \"truck_centroid\": slice(0, num_trucks),\n",
    "    \"truck_num_bikes\": slice(num_trucks, 2*num_trucks),\n",
    "}\n",
    "\n",
    "print(map_obs, map_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute delta bikes and obs += delta_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n",
      "tensor([13., 10., 18.,  ..., 11., 13., 10.])\n"
     ]
    }
   ],
   "source": [
    "obs_before_action = obs.clone()\n",
    "\n",
    "resize = False\n",
    "while obs.ndim < 3:\n",
    "    assert act.ndim == obs.ndim\n",
    "    obs = obs[None, ...]\n",
    "    act = act[None, ...]\n",
    "    resize = True\n",
    "\n",
    "ensemble_size = obs.shape[0]\n",
    "batch_size = obs.shape[1]\n",
    "distr_size = len(obs[0, 0, map_obs[\"bikes_distr\"]]) #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "# Compute delta_bikes in a parallel way\n",
    "delta_bikes = np.zeros((ensemble_size, batch_size, distr_size), dtype=int)\n",
    "truck_centroids = act[..., map_act[\"truck_centroid\"]] #self.map_act[\"truck_centroid\"]\n",
    "truck_bikes = act[..., map_act[\"truck_num_bikes\"]] #self.map_act[\"truck_num_bikes\"]\n",
    "n = distr_size\n",
    "truck_centroids = np.reshape(\n",
    "    truck_centroids, (truck_centroids.shape[0] * truck_centroids.shape[1], -1)\n",
    ")\n",
    "offset = np.arange(truck_centroids.shape[0])[..., None]\n",
    "truck_centroids_offset = truck_centroids + offset * n\n",
    "unq, inv = np.unique(truck_centroids_offset.ravel(), return_inverse=True)\n",
    "unq = unq.astype(int)\n",
    "sol = np.bincount(inv, truck_bikes.ravel())\n",
    "delta_bikes[\n",
    "    unq // (batch_size * n),\n",
    "    (unq % (batch_size * n)) // n,\n",
    "    (unq % (batch_size * n)) % n,\n",
    "] = sol\n",
    "\n",
    "if resize:\n",
    "    delta_bikes = delta_bikes.reshape((batch_size, -1))\n",
    "    act = act.reshape((batch_size, -1))\n",
    "    obs = obs.reshape((batch_size, -1))\n",
    "\n",
    "# Update obs\n",
    "obs[..., map_obs[\"bikes_distr\"]] += delta_bikes #self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "print(torch.sum(obs-obs_before_action, axis=-1))\n",
    "print(torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "assert torch.all(torch.sum(obs-obs_before_action, axis=-1) == torch.sum(act[...,map_act[\"truck_num_bikes\"]], axis=-1))\n",
    "\n",
    "# Super long check to see if preprocess is good, and it is so far\n",
    "# for obs_, previous_obs_, truck_centroids, truck_num_bikes in zip(obs, obs_before_action, act[...,truck_centroid_idx], act[...,truck_num_bikes_idx]):\n",
    "#     bikes_idx = torch.nonzero(truck_num_bikes, as_tuple=True)[0]\n",
    "#     truck_centroids = truck_centroids[bikes_idx]\n",
    "#     centroids_new_bikes = torch.nonzero(obs_-previous_obs_, as_tuple=True)[0]\n",
    "#     print(torch.sort(centroids_new_bikes).values)\n",
    "#     print(torch.unique(truck_centroids))\n",
    "#     print(torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y from obs and next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 44]) torch.Size([10000, 43])\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 10000\n",
    "input_obs_keys = [\"bikes_distr\", \"time_counter\"]\n",
    "input_act_keys = [] #not implemented\n",
    "output_keys = [\"bikes_distr\"]\n",
    "\n",
    "input_mask = np.zeros(obs.shape[-1])\n",
    "for key in input_obs_keys:\n",
    "    input_mask[map_obs[key]] = 1\n",
    "input_mask = np.ma.make_mask(input_mask)\n",
    "\n",
    "output_mask = np.zeros(obs.shape[-1])\n",
    "for key in output_keys:\n",
    "    output_mask[map_obs[key]] = 1\n",
    "output_mask = np.ma.make_mask(output_mask)\n",
    "\n",
    "assert obs.ndim == 2\n",
    "assert next_obs.ndim == 2\n",
    "x = obs[:dataset_size, input_mask]\n",
    "y = next_obs[:dataset_size, output_mask]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe add a date proxy, weather ? holiday, week-end ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional output preds (e.g. reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 43]) torch.Size([10000, 1])\n",
      "torch.Size([10000, 44])\n"
     ]
    }
   ],
   "source": [
    "learned_rewards = True\n",
    "if learned_rewards:\n",
    "    reward_ = reward[:dataset_size, ...].unsqueeze(-1)\n",
    "    print(y.shape, reward_.shape)\n",
    "    y = torch.cat([y, reward_], dim=-1)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_x, test_x and train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n"
     ]
    }
   ],
   "source": [
    "test_split_ratio = 0.2\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# idx = torch.randperm(x.shape[0])\n",
    "# x = x[idx, :]\n",
    "# y = y[idx, :]\n",
    "\n",
    "train_x = x[int(test_split_ratio*dataset_size):, ...]\n",
    "train_y = y[int(test_split_ratio*dataset_size):, ...]\n",
    "test_x = x[:int(test_split_ratio*dataset_size), ...]\n",
    "test_y = y[:int(test_split_ratio*dataset_size), ...]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Dict, Any, Tuple\n",
    "from torch.functional import F\n",
    "import pathlib\n",
    "\n",
    "class linearRegression(Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size: int,\n",
    "            out_size: int,\n",
    "            device: Union[str, torch.device],   \n",
    "        ):\n",
    "        super().__init__(device)\n",
    "        self.linear = torch.nn.Linear(in_size, out_size)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    def loss(self, model_in: torch.Tensor, target: torch.Tensor = None) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        assert model_in.ndim == 2 and target.ndim == 2\n",
    "        pred_out = self.forward(model_in)\n",
    "        return self.criterion(pred_out, target), {}\n",
    "\n",
    "    def eval_score(\n",
    "        self, model_in: torch.Tensor, target: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        assert model_in.ndim == 2 and target.ndim == 2\n",
    "        with torch.no_grad():\n",
    "            pred_output = self.forward(model_in)\n",
    "            return F.mse_loss(pred_output, target, reduction=\"none\").unsqueeze(0), {}\n",
    "\n",
    "    def save(self, save_dir: Union[str, pathlib.Path]):\n",
    "        \"\"\"Saves the model to the given directory.\"\"\"\n",
    "        model_dict = {\"state_dict\": self.state_dict()}\n",
    "        torch.save(model_dict, pathlib.Path(save_dir) / self._MODEL_FNAME)\n",
    "\n",
    "    def load(self, load_dir: Union[str, pathlib.Path]):\n",
    "        \"\"\"Loads the model from the given path.\"\"\"\n",
    "        model_dict = torch.load(pathlib.Path(load_dir) / self._MODEL_FNAME)\n",
    "        self.load_state_dict(model_dict[\"state_dict\"])\n",
    "\n",
    "    def reset_1d(\n",
    "        self, obs: torch.Tensor, rng: Optional[torch.Generator] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        assert rng is not None\n",
    "        propagation_indices = None\n",
    "        return {\"obs\": obs, \"propagation_indices\": propagation_indices}\n",
    "\n",
    "    def sample_1d(\n",
    "        self,\n",
    "        model_input: torch.Tensor,\n",
    "        model_state: Dict[str, torch.Tensor],\n",
    "        deterministic: bool = False,\n",
    "        rng: Optional[torch.Generator] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
    "        return (self.forward(model_input), model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01 \n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "lr_model = linearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 0, loss 7.737802505493164, R2 -3.513991594314575\n",
      "Eval loss 7.462893486022949, R2 -3.1201021671295166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1, loss 7.4055705070495605, R2 -3.321925401687622\n",
      "Eval loss 7.160141944885254, R2 -2.9471359252929688\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 2, loss 7.102957248687744, R2 -3.146972417831421\n",
      "Eval loss 6.8842315673828125, R2 -2.7892630100250244\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 3, loss 6.8272199630737305, R2 -2.987549066543579\n",
      "Eval loss 6.6326775550842285, R2 -2.6450929641723633\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 4, loss 6.575865745544434, R2 -2.842212438583374\n",
      "Eval loss 6.403227806091309, R2 -2.5133824348449707\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 5, loss 6.3466362953186035, R2 -2.7096593379974365\n",
      "Eval loss 6.193836688995361, R2 -2.3929853439331055\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 6, loss 6.1374831199646, R2 -2.5887038707733154\n",
      "Eval loss 6.002649307250977, R2 -2.282869815826416\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 7, loss 5.946545600891113, R2 -2.478274345397949\n",
      "Eval loss 5.827983856201172, R2 -2.1820998191833496\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 8, loss 5.772138595581055, R2 -2.377392053604126\n",
      "Eval loss 5.668313026428223, R2 -2.08982253074646\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 9, loss 5.612731456756592, R2 -2.28518009185791\n",
      "Eval loss 5.522252082824707, R2 -2.005265951156616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 10, loss 5.466937065124512, R2 -2.2008304595947266\n",
      "Eval loss 5.38854455947876, R2 -1.92772376537323\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 11, loss 5.333496570587158, R2 -2.123617172241211\n",
      "Eval loss 5.266051769256592, R2 -1.8565623760223389\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 12, loss 5.211266994476318, R2 -2.0528817176818848\n",
      "Eval loss 5.1537370681762695, R2 -1.791202425956726\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 13, loss 5.099213600158691, R2 -1.9880234003067017\n",
      "Eval loss 5.050663948059082, R2 -1.731117606163025\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 14, loss 4.996395111083984, R2 -1.928501009941101\n",
      "Eval loss 4.955981254577637, R2 -1.6758294105529785\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 15, loss 4.901960372924805, R2 -1.8738216161727905\n",
      "Eval loss 4.868915557861328, R2 -1.6249045133590698\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 16, loss 4.8151350021362305, R2 -1.8235347270965576\n",
      "Eval loss 4.7887654304504395, R2 -1.577949047088623\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 17, loss 4.735217094421387, R2 -1.777241826057434\n",
      "Eval loss 4.714897155761719, R2 -1.5346062183380127\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 18, loss 4.661571025848389, R2 -1.7345679998397827\n",
      "Eval loss 4.646732807159424, R2 -1.4945505857467651\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 19, loss 4.593618869781494, R2 -1.6951866149902344\n",
      "Eval loss 4.583749294281006, R2 -1.4574874639511108\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 20, loss 4.530837535858154, R2 -1.6587891578674316\n",
      "Eval loss 4.525470733642578, R2 -1.4231466054916382\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 21, loss 4.472750186920166, R2 -1.6251037120819092\n",
      "Eval loss 4.471467971801758, R2 -1.391286849975586\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 22, loss 4.41892671585083, R2 -1.5938804149627686\n",
      "Eval loss 4.421348571777344, R2 -1.3616851568222046\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 23, loss 4.368976593017578, R2 -1.5648937225341797\n",
      "Eval loss 4.374758243560791, R2 -1.3341411352157593\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 24, loss 4.322544097900391, R2 -1.5379382371902466\n",
      "Eval loss 4.3313751220703125, R2 -1.3084715604782104\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 25, loss 4.279307842254639, R2 -1.5128283500671387\n",
      "Eval loss 4.290907859802246, R2 -1.2845101356506348\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 26, loss 4.238976001739502, R2 -1.4893959760665894\n",
      "Eval loss 4.253089427947998, R2 -1.2621057033538818\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 27, loss 4.201282501220703, R2 -1.4674855470657349\n",
      "Eval loss 4.217681884765625, R2 -1.2411222457885742\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 28, loss 4.165988922119141, R2 -1.4469610452651978\n",
      "Eval loss 4.184465408325195, R2 -1.2214339971542358\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 29, loss 4.132876396179199, R2 -1.4276952743530273\n",
      "Eval loss 4.153243064880371, R2 -1.2029284238815308\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 30, loss 4.10174560546875, R2 -1.4095736742019653\n",
      "Eval loss 4.123833656311035, R2 -1.1855019330978394\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 31, loss 4.0724196434021, R2 -1.3924938440322876\n",
      "Eval loss 4.096075057983398, R2 -1.1690596342086792\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 32, loss 4.04473352432251, R2 -1.3763598203659058\n",
      "Eval loss 4.069819450378418, R2 -1.153516173362732\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 33, loss 4.01854133605957, R2 -1.3610883951187134\n",
      "Eval loss 4.044931888580322, R2 -1.1387958526611328\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 34, loss 3.9937076568603516, R2 -1.3466006517410278\n",
      "Eval loss 4.021291255950928, R2 -1.1248278617858887\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 35, loss 3.970111608505249, R2 -1.3328261375427246\n",
      "Eval loss 3.998785972595215, R2 -1.1115460395812988\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 36, loss 3.9476423263549805, R2 -1.319701910018921\n",
      "Eval loss 3.97731614112854, R2 -1.0988932847976685\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 37, loss 3.9262006282806396, R2 -1.3071688413619995\n",
      "Eval loss 3.9567911624908447, R2 -1.0868167877197266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 38, loss 3.9056944847106934, R2 -1.295177936553955\n",
      "Eval loss 3.937127113342285, R2 -1.07526695728302\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 39, loss 3.886042594909668, R2 -1.2836769819259644\n",
      "Eval loss 3.9182496070861816, R2 -1.0642000436782837\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 40, loss 3.8671698570251465, R2 -1.2726268768310547\n",
      "Eval loss 3.900090217590332, R2 -1.0535773038864136\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 41, loss 3.8490078449249268, R2 -1.2619853019714355\n",
      "Eval loss 3.8825876712799072, R2 -1.043359637260437\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 42, loss 3.831495761871338, R2 -1.251717209815979\n",
      "Eval loss 3.86568546295166, R2 -1.0335164070129395\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 43, loss 3.8145766258239746, R2 -1.2417932748794556\n",
      "Eval loss 3.849332094192505, R2 -1.0240161418914795\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 44, loss 3.798201084136963, R2 -1.2321803569793701\n",
      "Eval loss 3.833481788635254, R2 -1.014831304550171\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 45, loss 3.782322406768799, R2 -1.2228541374206543\n",
      "Eval loss 3.8180923461914062, R2 -1.0059362649917603\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 46, loss 3.7668991088867188, R2 -1.2137891054153442\n",
      "Eval loss 3.8031256198883057, R2 -0.9973090887069702\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 47, loss 3.7518928050994873, R2 -1.2049648761749268\n",
      "Eval loss 3.7885468006134033, R2 -0.9889277815818787\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 48, loss 3.737269163131714, R2 -1.1963592767715454\n",
      "Eval loss 3.774324655532837, R2 -0.9807737469673157\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 49, loss 3.7229971885681152, R2 -1.1879572868347168\n",
      "Eval loss 3.760430335998535, R2 -0.9728282690048218\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 50, loss 3.709048271179199, R2 -1.1797409057617188\n",
      "Eval loss 3.746837615966797, R2 -0.9650789499282837\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 51, loss 3.6953976154327393, R2 -1.1716951131820679\n",
      "Eval loss 3.733523368835449, R2 -0.9575082659721375\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 52, loss 3.682020664215088, R2 -1.1638062000274658\n",
      "Eval loss 3.720466375350952, R2 -0.9501041173934937\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 53, loss 3.6688969135284424, R2 -1.1560622453689575\n",
      "Eval loss 3.707646369934082, R2 -0.9428544640541077\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 54, loss 3.656006336212158, R2 -1.1484538316726685\n",
      "Eval loss 3.6950457096099854, R2 -0.9357476830482483\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 55, loss 3.6433324813842773, R2 -1.1409683227539062\n",
      "Eval loss 3.6826491355895996, R2 -0.9287732839584351\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 56, loss 3.6308586597442627, R2 -1.1335972547531128\n",
      "Eval loss 3.6704413890838623, R2 -0.9219236373901367\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 57, loss 3.6185710430145264, R2 -1.126332402229309\n",
      "Eval loss 3.6584091186523438, R2 -0.9151896834373474\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 58, loss 3.6064553260803223, R2 -1.1191678047180176\n",
      "Eval loss 3.646540403366089, R2 -0.9085627198219299\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 59, loss 3.59450101852417, R2 -1.112094521522522\n",
      "Eval loss 3.634823799133301, R2 -0.9020360708236694\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 60, loss 3.5826966762542725, R2 -1.1051061153411865\n",
      "Eval loss 3.6232500076293945, R2 -0.8956052660942078\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 61, loss 3.571032762527466, R2 -1.0981974601745605\n",
      "Eval loss 3.611809730529785, R2 -0.8892619609832764\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 62, loss 3.5594992637634277, R2 -1.0913655757904053\n",
      "Eval loss 3.600494384765625, R2 -0.8830015063285828\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 63, loss 3.5480895042419434, R2 -1.0846028327941895\n",
      "Eval loss 3.589296817779541, R2 -0.8768187165260315\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 64, loss 3.5367953777313232, R2 -1.0779061317443848\n",
      "Eval loss 3.5782101154327393, R2 -0.8707094788551331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 65, loss 3.5256097316741943, R2 -1.0712710618972778\n",
      "Eval loss 3.567227363586426, R2 -0.8646697998046875\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 66, loss 3.5145277976989746, R2 -1.0646947622299194\n",
      "Eval loss 3.5563433170318604, R2 -0.8586958050727844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 67, loss 3.503542184829712, R2 -1.0581738948822021\n",
      "Eval loss 3.545552968978882, R2 -0.8527826070785522\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 68, loss 3.4926483631134033, R2 -1.051705241203308\n",
      "Eval loss 3.534851551055908, R2 -0.8469297885894775\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 69, loss 3.4818429946899414, R2 -1.0452865362167358\n",
      "Eval loss 3.5242342948913574, R2 -0.8411316275596619\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 70, loss 3.471121072769165, R2 -1.038914680480957\n",
      "Eval loss 3.513697862625122, R2 -0.835386335849762\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 71, loss 3.4604783058166504, R2 -1.0325887203216553\n",
      "Eval loss 3.5032379627227783, R2 -0.8296919465065002\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 72, loss 3.4499118328094482, R2 -1.0263047218322754\n",
      "Eval loss 3.492852210998535, R2 -0.824045717716217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 73, loss 3.439418077468872, R2 -1.020063042640686\n",
      "Eval loss 3.482537031173706, R2 -0.8184450268745422\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 74, loss 3.4289944171905518, R2 -1.0138605833053589\n",
      "Eval loss 3.4722890853881836, R2 -0.8128877878189087\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 75, loss 3.4186370372772217, R2 -1.0076956748962402\n",
      "Eval loss 3.4621071815490723, R2 -0.807373583316803\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 76, loss 3.4083454608917236, R2 -1.00156831741333\n",
      "Eval loss 3.4519872665405273, R2 -0.8019005060195923\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 77, loss 3.3981165885925293, R2 -0.9954758286476135\n",
      "Eval loss 3.441929578781128, R2 -0.7964655160903931\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 78, loss 3.3879475593566895, R2 -0.9894176125526428\n",
      "Eval loss 3.4319303035736084, R2 -0.7910688519477844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 79, loss 3.3778374195098877, R2 -0.9833921790122986\n",
      "Eval loss 3.4219887256622314, R2 -0.7857075333595276\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 80, loss 3.3677847385406494, R2 -0.9773992896080017\n",
      "Eval loss 3.4121029376983643, R2 -0.7803817391395569\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 81, loss 3.3577873706817627, R2 -0.9714379906654358\n",
      "Eval loss 3.402270555496216, R2 -0.7750896215438843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 82, loss 3.347843647003174, R2 -0.9655067324638367\n",
      "Eval loss 3.392490863800049, R2 -0.7698303461074829\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 83, loss 3.3379528522491455, R2 -0.9596052765846252\n",
      "Eval loss 3.382763147354126, R2 -0.7646035552024841\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 84, loss 3.328113555908203, R2 -0.9537330269813538\n",
      "Eval loss 3.3730854988098145, R2 -0.7594066858291626\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 85, loss 3.318324565887451, R2 -0.9478885531425476\n",
      "Eval loss 3.363457441329956, R2 -0.7542406320571899\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 86, loss 3.3085849285125732, R2 -0.942072331905365\n",
      "Eval loss 3.353877544403076, R2 -0.7491028308868408\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 87, loss 3.298894166946411, R2 -0.9362836480140686\n",
      "Eval loss 3.3443443775177, R2 -0.7439943552017212\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 88, loss 3.289249897003174, R2 -0.9305211901664734\n",
      "Eval loss 3.334857940673828, R2 -0.7389141321182251\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 89, loss 3.279653310775757, R2 -0.9247850179672241\n",
      "Eval loss 3.3254175186157227, R2 -0.7338594198226929\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 90, loss 3.270102024078369, R2 -0.9190753698348999\n",
      "Eval loss 3.316021203994751, R2 -0.7288332581520081\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 91, loss 3.26059627532959, R2 -0.9133906960487366\n",
      "Eval loss 3.3066701889038086, R2 -0.7238327860832214\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 92, loss 3.2511348724365234, R2 -0.9077311158180237\n",
      "Eval loss 3.2973623275756836, R2 -0.7188572287559509\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 93, loss 3.2417173385620117, R2 -0.9020959734916687\n",
      "Eval loss 3.2880969047546387, R2 -0.713906466960907\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 94, loss 3.2323434352874756, R2 -0.8964859247207642\n",
      "Eval loss 3.2788751125335693, R2 -0.7089812755584717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 95, loss 3.2230124473571777, R2 -0.8908984661102295\n",
      "Eval loss 3.2696938514709473, R2 -0.7040792107582092\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 96, loss 3.213723659515381, R2 -0.8853369355201721\n",
      "Eval loss 3.260554790496826, R2 -0.699201762676239\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 97, loss 3.204476833343506, R2 -0.8797983527183533\n",
      "Eval loss 3.2514567375183105, R2 -0.6943466663360596\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 98, loss 3.1952712535858154, R2 -0.8742828965187073\n",
      "Eval loss 3.242398500442505, R2 -0.6895147562026978\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 99, loss 3.1861071586608887, R2 -0.868790864944458\n",
      "Eval loss 3.2333807945251465, R2 -0.6847060322761536\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 100, loss 3.17698335647583, R2 -0.8633208870887756\n",
      "Eval loss 3.224402666091919, R2 -0.6799191832542419\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 101, loss 3.167900323867798, R2 -0.8578745126724243\n",
      "Eval loss 3.215463876724243, R2 -0.6751542091369629\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 102, loss 3.1588566303253174, R2 -0.8524499535560608\n",
      "Eval loss 3.206563949584961, R2 -0.6704110503196716\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 103, loss 3.149852991104126, R2 -0.8470483422279358\n",
      "Eval loss 3.1977031230926514, R2 -0.6656894087791443\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 104, loss 3.1408884525299072, R2 -0.8416677117347717\n",
      "Eval loss 3.188880681991577, R2 -0.6609891653060913\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 105, loss 3.131963014602661, R2 -0.8363105654716492\n",
      "Eval loss 3.180095911026001, R2 -0.6563097238540649\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 106, loss 3.1230764389038086, R2 -0.8309745788574219\n",
      "Eval loss 3.171348810195923, R2 -0.6516498923301697\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 107, loss 3.114227533340454, R2 -0.825659990310669\n",
      "Eval loss 3.1626391410827637, R2 -0.6470118761062622\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 108, loss 3.1054177284240723, R2 -0.8203672766685486\n",
      "Eval loss 3.1539666652679443, R2 -0.6423928737640381\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 109, loss 3.096644878387451, R2 -0.8150959610939026\n",
      "Eval loss 3.1453309059143066, R2 -0.6377939581871033\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 110, loss 3.0879104137420654, R2 -0.8098456859588623\n",
      "Eval loss 3.136732339859009, R2 -0.6332163214683533\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 111, loss 3.0792131423950195, R2 -0.8046163320541382\n",
      "Eval loss 3.1281697750091553, R2 -0.6286565065383911\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 112, loss 3.0705528259277344, R2 -0.799407958984375\n",
      "Eval loss 3.119643449783325, R2 -0.6241174936294556\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 113, loss 3.061929941177368, R2 -0.7942205667495728\n",
      "Eval loss 3.1111533641815186, R2 -0.6195967197418213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 114, loss 3.0533432960510254, R2 -0.789054274559021\n",
      "Eval loss 3.102698802947998, R2 -0.6150956749916077\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 115, loss 3.0447933673858643, R2 -0.7839083075523376\n",
      "Eval loss 3.0942797660827637, R2 -0.6106131672859192\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 116, loss 3.0362799167633057, R2 -0.7787824273109436\n",
      "Eval loss 3.0858962535858154, R2 -0.6061489582061768\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 117, loss 3.027801752090454, R2 -0.7736775875091553\n",
      "Eval loss 3.077547550201416, R2 -0.6017038226127625\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 118, loss 3.019360065460205, R2 -0.7685926556587219\n",
      "Eval loss 3.0692336559295654, R2 -0.5972775220870972\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 119, loss 3.0109541416168213, R2 -0.7635272741317749\n",
      "Eval loss 3.0609548091888428, R2 -0.5928685665130615\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 120, loss 3.0025837421417236, R2 -0.7584834694862366\n",
      "Eval loss 3.0527102947235107, R2 -0.5884783864021301\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 121, loss 2.994248628616333, R2 -0.75345778465271\n",
      "Eval loss 3.0445003509521484, R2 -0.5841062664985657\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 122, loss 2.9859485626220703, R2 -0.748453676700592\n",
      "Eval loss 3.0363242626190186, R2 -0.5797520279884338\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 123, loss 2.9776833057403564, R2 -0.7434688806533813\n",
      "Eval loss 3.0281825065612793, R2 -0.5754150152206421\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 124, loss 2.9694530963897705, R2 -0.7385035157203674\n",
      "Eval loss 3.020073890686035, R2 -0.5710963010787964\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 125, loss 2.961257219314575, R2 -0.7335574626922607\n",
      "Eval loss 3.0119993686676025, R2 -0.5667948722839355\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 126, loss 2.9530961513519287, R2 -0.7286310195922852\n",
      "Eval loss 3.003957986831665, R2 -0.5625109076499939\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 127, loss 2.9449691772460938, R2 -0.723724901676178\n",
      "Eval loss 2.995950698852539, R2 -0.5582438707351685\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 128, loss 2.9368765354156494, R2 -0.7188361883163452\n",
      "Eval loss 2.987975835800171, R2 -0.5539939999580383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 129, loss 2.9288175106048584, R2 -0.7139685153961182\n",
      "Eval loss 2.980034112930298, R2 -0.5497617125511169\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 130, loss 2.9207923412323, R2 -0.7091186046600342\n",
      "Eval loss 2.972125291824341, R2 -0.5455465316772461\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 131, loss 2.9128007888793945, R2 -0.704288899898529\n",
      "Eval loss 2.9642486572265625, R2 -0.5413477420806885\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 132, loss 2.9048426151275635, R2 -0.6994778513908386\n",
      "Eval loss 2.956404685974121, R2 -0.5371655225753784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 133, loss 2.8969178199768066, R2 -0.6946853399276733\n",
      "Eval loss 2.9485933780670166, R2 -0.5330003499984741\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 134, loss 2.889025926589966, R2 -0.6899117231369019\n",
      "Eval loss 2.9408140182495117, R2 -0.5288516283035278\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 135, loss 2.8811676502227783, R2 -0.6851567625999451\n",
      "Eval loss 2.9330663681030273, R2 -0.5247191786766052\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 136, loss 2.8733415603637695, R2 -0.680420458316803\n",
      "Eval loss 2.9253509044647217, R2 -0.5206034183502197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 137, loss 2.8655483722686768, R2 -0.6757026314735413\n",
      "Eval loss 2.9176669120788574, R2 -0.5165038704872131\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 138, loss 2.8577873706817627, R2 -0.6710032224655151\n",
      "Eval loss 2.9100146293640137, R2 -0.5124208331108093\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 139, loss 2.8500590324401855, R2 -0.6663222312927246\n",
      "Eval loss 2.9023938179016113, R2 -0.5083538889884949\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 140, loss 2.842362880706787, R2 -0.6616594791412354\n",
      "Eval loss 2.8948042392730713, R2 -0.5043021440505981\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 141, loss 2.8346989154815674, R2 -0.6570149064064026\n",
      "Eval loss 2.8872456550598145, R2 -0.5002668499946594\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 142, loss 2.827066659927368, R2 -0.6523888111114502\n",
      "Eval loss 2.87971830368042, R2 -0.4962477385997772\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 143, loss 2.8194663524627686, R2 -0.6477803587913513\n",
      "Eval loss 2.8722217082977295, R2 -0.4922439455986023\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 144, loss 2.8118975162506104, R2 -0.6431899666786194\n",
      "Eval loss 2.864755630493164, R2 -0.4882558584213257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 145, loss 2.8043601512908936, R2 -0.6386175751686096\n",
      "Eval loss 2.8573200702667236, R2 -0.48428356647491455\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 146, loss 2.7968544960021973, R2 -0.6340630054473877\n",
      "Eval loss 2.849915027618408, R2 -0.48032695055007935\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 147, loss 2.789379358291626, R2 -0.6295258402824402\n",
      "Eval loss 2.8425400257110596, R2 -0.4763853847980499\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 148, loss 2.781935691833496, R2 -0.6250061392784119\n",
      "Eval loss 2.835195779800415, R2 -0.472459614276886\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 149, loss 2.7745230197906494, R2 -0.6205049157142639\n",
      "Eval loss 2.827880620956421, R2 -0.46854907274246216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 150, loss 2.7671408653259277, R2 -0.6160200834274292\n",
      "Eval loss 2.8205959796905518, R2 -0.46465352177619934\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 151, loss 2.7597897052764893, R2 -0.6115540862083435\n",
      "Eval loss 2.813340663909912, R2 -0.46077272295951843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 152, loss 2.7524688243865967, R2 -0.6071045398712158\n",
      "Eval loss 2.8061153888702393, R2 -0.45690757036209106\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 153, loss 2.745177984237671, R2 -0.6026725172996521\n",
      "Eval loss 2.798919439315796, R2 -0.4530569911003113\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 154, loss 2.737917900085449, R2 -0.5982580184936523\n",
      "Eval loss 2.791752576828003, R2 -0.44922196865081787\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 155, loss 2.7306878566741943, R2 -0.5938602089881897\n",
      "Eval loss 2.7846150398254395, R2 -0.44540175795555115\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 156, loss 2.723487138748169, R2 -0.5894795060157776\n",
      "Eval loss 2.7775063514709473, R2 -0.44159603118896484\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 157, loss 2.7163166999816895, R2 -0.5851158499717712\n",
      "Eval loss 2.7704267501831055, R2 -0.4378047287464142\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 158, loss 2.7091760635375977, R2 -0.5807689428329468\n",
      "Eval loss 2.763376235961914, R2 -0.4340285062789917\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 159, loss 2.7020645141601562, R2 -0.5764389634132385\n",
      "Eval loss 2.7563540935516357, R2 -0.4302671253681183\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 160, loss 2.6949830055236816, R2 -0.5721256732940674\n",
      "Eval loss 2.7493603229522705, R2 -0.4265199601650238\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 161, loss 2.6879303455352783, R2 -0.5678291916847229\n",
      "Eval loss 2.7423951625823975, R2 -0.422787070274353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 162, loss 2.6809067726135254, R2 -0.5635488033294678\n",
      "Eval loss 2.7354583740234375, R2 -0.41906842589378357\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 163, loss 2.673912525177002, R2 -0.55928635597229\n",
      "Eval loss 2.7285497188568115, R2 -0.4153639078140259\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 164, loss 2.66694712638855, R2 -0.5550393462181091\n",
      "Eval loss 2.7216689586639404, R2 -0.4116746187210083\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 165, loss 2.66001033782959, R2 -0.5508091449737549\n",
      "Eval loss 2.714816093444824, R2 -0.4079987704753876\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 166, loss 2.653102159500122, R2 -0.5465953350067139\n",
      "Eval loss 2.707991123199463, R2 -0.40433722734451294\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 167, loss 2.6462225914001465, R2 -0.5423975586891174\n",
      "Eval loss 2.7011938095092773, R2 -0.400689959526062\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 168, loss 2.639371395111084, R2 -0.5382153987884521\n",
      "Eval loss 2.6944241523742676, R2 -0.39705637097358704\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 169, loss 2.6325483322143555, R2 -0.5340508222579956\n",
      "Eval loss 2.6876819133758545, R2 -0.3934367895126343\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 170, loss 2.62575364112854, R2 -0.5299007296562195\n",
      "Eval loss 2.68096661567688, R2 -0.3898310661315918\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 171, loss 2.6189868450164795, R2 -0.5257680416107178\n",
      "Eval loss 2.674278974533081, R2 -0.38623887300491333\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 172, loss 2.6122477054595947, R2 -0.5216507911682129\n",
      "Eval loss 2.6676177978515625, R2 -0.38266098499298096\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 173, loss 2.6055362224578857, R2 -0.5175492167472839\n",
      "Eval loss 2.6609840393066406, R2 -0.3790968656539917\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 174, loss 2.5988526344299316, R2 -0.5134633779525757\n",
      "Eval loss 2.654376983642578, R2 -0.37554529309272766\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 175, loss 2.5921967029571533, R2 -0.5093936324119568\n",
      "Eval loss 2.647796392440796, R2 -0.3720083236694336\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 176, loss 2.5855677127838135, R2 -0.5053394436836243\n",
      "Eval loss 2.641242504119873, R2 -0.3684849441051483\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 177, loss 2.5789663791656494, R2 -0.5013004541397095\n",
      "Eval loss 2.6347150802612305, R2 -0.3649745285511017\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 178, loss 2.572391986846924, R2 -0.49727773666381836\n",
      "Eval loss 2.6282145977020264, R2 -0.361477255821228\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 179, loss 2.565844774246216, R2 -0.49326983094215393\n",
      "Eval loss 2.621739387512207, R2 -0.357993483543396\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 180, loss 2.559324264526367, R2 -0.489277184009552\n",
      "Eval loss 2.615290880203247, R2 -0.3545236587524414\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 181, loss 2.552830457687378, R2 -0.48530083894729614\n",
      "Eval loss 2.60886812210083, R2 -0.35106661915779114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 182, loss 2.546363592147827, R2 -0.4813392460346222\n",
      "Eval loss 2.6024715900421143, R2 -0.347622275352478\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 183, loss 2.5399231910705566, R2 -0.47739285230636597\n",
      "Eval loss 2.596100330352783, R2 -0.34419190883636475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 184, loss 2.533508777618408, R2 -0.47346165776252747\n",
      "Eval loss 2.589755058288574, R2 -0.3407736122608185\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 185, loss 2.5271213054656982, R2 -0.4695453941822052\n",
      "Eval loss 2.58343505859375, R2 -0.3373686373233795\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 186, loss 2.5207595825195312, R2 -0.46564462780952454\n",
      "Eval loss 2.5771408081054688, R2 -0.33397701382637024\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 187, loss 2.5144240856170654, R2 -0.4617581367492676\n",
      "Eval loss 2.570871591567993, R2 -0.3305979073047638\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 188, loss 2.5081143379211426, R2 -0.4578869044780731\n",
      "Eval loss 2.5646276473999023, R2 -0.327231764793396\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 189, loss 2.5018310546875, R2 -0.45403024554252625\n",
      "Eval loss 2.5584089756011963, R2 -0.32387760281562805\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 190, loss 2.495572805404663, R2 -0.45018884539604187\n",
      "Eval loss 2.552215099334717, R2 -0.3205375373363495\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 191, loss 2.489340305328369, R2 -0.4463614523410797\n",
      "Eval loss 2.546046495437622, R2 -0.3172096610069275\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 192, loss 2.483133554458618, R2 -0.4425491392612457\n",
      "Eval loss 2.539902448654175, R2 -0.31389400362968445\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 193, loss 2.476952075958252, R2 -0.4387517273426056\n",
      "Eval loss 2.533782958984375, R2 -0.3105907738208771\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 194, loss 2.4707958698272705, R2 -0.43496790528297424\n",
      "Eval loss 2.5276877880096436, R2 -0.3073005676269531\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 195, loss 2.4646646976470947, R2 -0.43119925260543823\n",
      "Eval loss 2.5216176509857178, R2 -0.30402249097824097\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 196, loss 2.4585585594177246, R2 -0.42744484543800354\n",
      "Eval loss 2.515571355819702, R2 -0.30075666308403015\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 197, loss 2.452477216720581, R2 -0.42370492219924927\n",
      "Eval loss 2.509549617767334, R2 -0.29750391840934753\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 198, loss 2.4464211463928223, R2 -0.4199800193309784\n",
      "Eval loss 2.503551959991455, R2 -0.29426252841949463\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 199, loss 2.440389633178711, R2 -0.41626831889152527\n",
      "Eval loss 2.4975781440734863, R2 -0.29103386402130127\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 200, loss 2.434382438659668, R2 -0.4125709533691406\n",
      "Eval loss 2.4916281700134277, R2 -0.28781697154045105\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 201, loss 2.4284002780914307, R2 -0.40888726711273193\n",
      "Eval loss 2.4857020378112793, R2 -0.28461265563964844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 202, loss 2.4224419593811035, R2 -0.40521860122680664\n",
      "Eval loss 2.47979998588562, R2 -0.28142058849334717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 203, loss 2.416508197784424, R2 -0.4015628695487976\n",
      "Eval loss 2.473921060562134, R2 -0.27823999524116516\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 204, loss 2.4105985164642334, R2 -0.39792221784591675\n",
      "Eval loss 2.4680657386779785, R2 -0.2750713527202606\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 205, loss 2.4047131538391113, R2 -0.3942946493625641\n",
      "Eval loss 2.4622340202331543, R2 -0.2719147205352783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 206, loss 2.398851156234741, R2 -0.3906804323196411\n",
      "Eval loss 2.456425666809082, R2 -0.2687707245349884\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 207, loss 2.3930134773254395, R2 -0.3870810568332672\n",
      "Eval loss 2.4506402015686035, R2 -0.26563751697540283\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 208, loss 2.387199640274048, R2 -0.3834952116012573\n",
      "Eval loss 2.444878101348877, R2 -0.26251673698425293\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 209, loss 2.381409168243408, R2 -0.37992268800735474\n",
      "Eval loss 2.4391391277313232, R2 -0.25940755009651184\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 210, loss 2.3756422996520996, R2 -0.37636393308639526\n",
      "Eval loss 2.433422565460205, R2 -0.2563096284866333\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 211, loss 2.369898796081543, R2 -0.3728184401988983\n",
      "Eval loss 2.4277291297912598, R2 -0.25322404503822327\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 212, loss 2.3641788959503174, R2 -0.36928603053092957\n",
      "Eval loss 2.422058343887329, R2 -0.25014907121658325\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 213, loss 2.3584821224212646, R2 -0.36576807498931885\n",
      "Eval loss 2.416410207748413, R2 -0.247086763381958\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 214, loss 2.3528084754943848, R2 -0.36226290464401245\n",
      "Eval loss 2.4107844829559326, R2 -0.24403499066829681\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 215, loss 2.3471577167510986, R2 -0.3587711453437805\n",
      "Eval loss 2.4051809310913086, R2 -0.2409954071044922\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 216, loss 2.3415298461914062, R2 -0.35529229044914246\n",
      "Eval loss 2.399600028991699, R2 -0.2379668802022934\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 217, loss 2.3359251022338867, R2 -0.3518270254135132\n",
      "Eval loss 2.3940412998199463, R2 -0.23494940996170044\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 218, loss 2.3303427696228027, R2 -0.3483748733997345\n",
      "Eval loss 2.3885045051574707, R2 -0.23194365203380585\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 219, loss 2.3247830867767334, R2 -0.3449355959892273\n",
      "Eval loss 2.3829898834228516, R2 -0.22894884645938873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 220, loss 2.3192460536956787, R2 -0.34150946140289307\n",
      "Eval loss 2.3774971961975098, R2 -0.2259654551744461\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 221, loss 2.3137314319610596, R2 -0.3380962610244751\n",
      "Eval loss 2.372026205062866, R2 -0.22299295663833618\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 222, loss 2.308239221572876, R2 -0.33469533920288086\n",
      "Eval loss 2.366576910018921, R2 -0.22003164887428284\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 223, loss 2.302769184112549, R2 -0.3313080370426178\n",
      "Eval loss 2.361149311065674, R2 -0.217081680893898\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 224, loss 2.297321081161499, R2 -0.32793399691581726\n",
      "Eval loss 2.355743169784546, R2 -0.21414226293563843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 225, loss 2.2918951511383057, R2 -0.32457202672958374\n",
      "Eval loss 2.350358724594116, R2 -0.2112138867378235\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 226, loss 2.2864913940429688, R2 -0.32122305035591125\n",
      "Eval loss 2.3449954986572266, R2 -0.20829705893993378\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 227, loss 2.281109094619751, R2 -0.3178865909576416\n",
      "Eval loss 2.339653730392456, R2 -0.20539064705371857\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 228, loss 2.2757487297058105, R2 -0.3145627975463867\n",
      "Eval loss 2.3343327045440674, R2 -0.2024950534105301\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 229, loss 2.2704100608825684, R2 -0.3112514019012451\n",
      "Eval loss 2.329033136367798, R2 -0.19961024820804596\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 230, loss 2.265092611312866, R2 -0.30795249342918396\n",
      "Eval loss 2.3237545490264893, R2 -0.19673587381839752\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 231, loss 2.2597973346710205, R2 -0.304666131734848\n",
      "Eval loss 2.3184967041015625, R2 -0.19387270510196686\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 232, loss 2.2545228004455566, R2 -0.301392138004303\n",
      "Eval loss 2.3132596015930176, R2 -0.19102011620998383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 233, loss 2.249269485473633, R2 -0.29813045263290405\n",
      "Eval loss 2.3080432415008545, R2 -0.18817777931690216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 234, loss 2.2440378665924072, R2 -0.29488086700439453\n",
      "Eval loss 2.3028476238250732, R2 -0.18534646928310394\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 235, loss 2.2388269901275635, R2 -0.29164376854896545\n",
      "Eval loss 2.2976725101470947, R2 -0.18252533674240112\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 236, loss 2.2336370944976807, R2 -0.28841838240623474\n",
      "Eval loss 2.29251766204834, R2 -0.1797148436307907\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 237, loss 2.228468179702759, R2 -0.2852058708667755\n",
      "Eval loss 2.287383556365967, R2 -0.17691445350646973\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 238, loss 2.2233200073242188, R2 -0.2820053696632385\n",
      "Eval loss 2.2822694778442383, R2 -0.17412471771240234\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 239, loss 2.2181925773620605, R2 -0.27881643176078796\n",
      "Eval loss 2.2771756649017334, R2 -0.1713457703590393\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 240, loss 2.213085889816284, R2 -0.27563998103141785\n",
      "Eval loss 2.272101879119873, R2 -0.16857613623142242\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 241, loss 2.2079997062683105, R2 -0.2724750339984894\n",
      "Eval loss 2.2670481204986572, R2 -0.16581790149211884\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 242, loss 2.2029335498809814, R2 -0.26932230591773987\n",
      "Eval loss 2.262014389038086, R2 -0.1630687713623047\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 243, loss 2.1978883743286133, R2 -0.26618120074272156\n",
      "Eval loss 2.25700044631958, R2 -0.160330668091774\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 244, loss 2.1928629875183105, R2 -0.26305195689201355\n",
      "Eval loss 2.2520060539245605, R2 -0.15760208666324615\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 245, loss 2.1878578662872314, R2 -0.2599343955516815\n",
      "Eval loss 2.2470316886901855, R2 -0.15488417446613312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 246, loss 2.182873249053955, R2 -0.25682857632637024\n",
      "Eval loss 2.2420766353607178, R2 -0.15217621624469757\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 247, loss 2.177908182144165, R2 -0.2537338435649872\n",
      "Eval loss 2.2371413707733154, R2 -0.14947785437107086\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 248, loss 2.1729633808135986, R2 -0.2506517469882965\n",
      "Eval loss 2.2322254180908203, R2 -0.14678965508937836\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 249, loss 2.1680383682250977, R2 -0.24758018553256989\n",
      "Eval loss 2.2273285388946533, R2 -0.14411158859729767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 250, loss 2.163132905960083, R2 -0.2445211410522461\n",
      "Eval loss 2.2224509716033936, R2 -0.1414431929588318\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 251, loss 2.158247232437134, R2 -0.2414730042219162\n",
      "Eval loss 2.21759295463562, R2 -0.13878503441810608\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 252, loss 2.153381109237671, R2 -0.23843635618686676\n",
      "Eval loss 2.2127537727355957, R2 -0.1361362487077713\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 253, loss 2.148534059524536, R2 -0.23541103303432465\n",
      "Eval loss 2.2079334259033203, R2 -0.1334972083568573\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 254, loss 2.143707036972046, R2 -0.23239703476428986\n",
      "Eval loss 2.203132390975952, R2 -0.13086830079555511\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 255, loss 2.138899087905884, R2 -0.22939413785934448\n",
      "Eval loss 2.198349714279175, R2 -0.12824885547161102\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 256, loss 2.134110450744629, R2 -0.22640271484851837\n",
      "Eval loss 2.1935863494873047, R2 -0.1256387084722519\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 257, loss 2.129340648651123, R2 -0.22342181205749512\n",
      "Eval loss 2.1888413429260254, R2 -0.12303821742534637\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 258, loss 2.1245903968811035, R2 -0.22045283019542694\n",
      "Eval loss 2.184114933013916, R2 -0.12044798582792282\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 259, loss 2.119858980178833, R2 -0.21749471127986908\n",
      "Eval loss 2.1794071197509766, R2 -0.11786684393882751\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 260, loss 2.1151463985443115, R2 -0.21454781293869019\n",
      "Eval loss 2.174717426300049, R2 -0.11529511958360672\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 261, loss 2.11045241355896, R2 -0.2116115540266037\n",
      "Eval loss 2.17004656791687, R2 -0.11273295432329178\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 262, loss 2.1057772636413574, R2 -0.20868639647960663\n",
      "Eval loss 2.165393829345703, R2 -0.11018023639917374\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 263, loss 2.101121187210083, R2 -0.20577210187911987\n",
      "Eval loss 2.160759210586548, R2 -0.10763678699731827\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 264, loss 2.0964832305908203, R2 -0.20286865532398224\n",
      "Eval loss 2.1561427116394043, R2 -0.10510297119617462\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 265, loss 2.0918638706207275, R2 -0.19997553527355194\n",
      "Eval loss 2.1515443325042725, R2 -0.1025775671005249\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 266, loss 2.0872628688812256, R2 -0.1970941573381424\n",
      "Eval loss 2.1469640731811523, R2 -0.10006241500377655\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 267, loss 2.0826802253723145, R2 -0.19422294199466705\n",
      "Eval loss 2.142401695251465, R2 -0.09755609184503555\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 268, loss 2.0781161785125732, R2 -0.19136203825473785\n",
      "Eval loss 2.137856960296631, R2 -0.09505902975797653\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 269, loss 2.0735700130462646, R2 -0.18851254880428314\n",
      "Eval loss 2.1333298683166504, R2 -0.09257133305072784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 270, loss 2.0690419673919678, R2 -0.1856732815504074\n",
      "Eval loss 2.1288206577301025, R2 -0.090092234313488\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 271, loss 2.0645320415496826, R2 -0.18284422159194946\n",
      "Eval loss 2.124329090118408, R2 -0.0876224935054779\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 272, loss 2.060039758682251, R2 -0.18002654612064362\n",
      "Eval loss 2.119854688644409, R2 -0.08516175299882889\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 273, loss 2.055565595626831, R2 -0.1772187203168869\n",
      "Eval loss 2.1153981685638428, R2 -0.08271002024412155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 274, loss 2.0511090755462646, R2 -0.17442132532596588\n",
      "Eval loss 2.1109588146209717, R2 -0.08026747405529022\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 275, loss 2.0466701984405518, R2 -0.1716339886188507\n",
      "Eval loss 2.106536865234375, R2 -0.07783323526382446\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 276, loss 2.0422492027282715, R2 -0.16885782778263092\n",
      "Eval loss 2.1021318435668945, R2 -0.07540839165449142\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 277, loss 2.0378456115722656, R2 -0.16609105467796326\n",
      "Eval loss 2.0977439880371094, R2 -0.07299231737852097\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 278, loss 2.033459424972534, R2 -0.1633351743221283\n",
      "Eval loss 2.0933737754821777, R2 -0.07058493047952652\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 279, loss 2.0290908813476562, R2 -0.1605890840291977\n",
      "Eval loss 2.089020013809204, R2 -0.06818670779466629\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 280, loss 2.0247397422790527, R2 -0.15785373747348785\n",
      "Eval loss 2.0846831798553467, R2 -0.06579692661762238\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 281, loss 2.0204055309295654, R2 -0.15512815117835999\n",
      "Eval loss 2.0803632736206055, R2 -0.06341585516929626\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 282, loss 2.0160884857177734, R2 -0.1524125039577484\n",
      "Eval loss 2.0760602951049805, R2 -0.06104335933923721\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 283, loss 2.0117886066436768, R2 -0.14970670640468597\n",
      "Eval loss 2.0717742443084717, R2 -0.058679644018411636\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 284, loss 2.0075061321258545, R2 -0.14701156318187714\n",
      "Eval loss 2.0675041675567627, R2 -0.05632438883185387\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 285, loss 2.0032403469085693, R2 -0.14432616531848907\n",
      "Eval loss 2.063251495361328, R2 -0.05397786945104599\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 286, loss 1.9989912509918213, R2 -0.14165057241916656\n",
      "Eval loss 2.0590147972106934, R2 -0.051639772951602936\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 287, loss 1.994759202003479, R2 -0.138984814286232\n",
      "Eval loss 2.0547945499420166, R2 -0.04931057617068291\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 288, loss 1.9905437231063843, R2 -0.13632874190807343\n",
      "Eval loss 2.0505905151367188, R2 -0.046989038586616516\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 289, loss 1.9863451719284058, R2 -0.1336827427148819\n",
      "Eval loss 2.046403169631958, R2 -0.04467674344778061\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 290, loss 1.9821629524230957, R2 -0.13104645907878876\n",
      "Eval loss 2.042231798171997, R2 -0.04237249121069908\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 291, loss 1.9779975414276123, R2 -0.12841960787773132\n",
      "Eval loss 2.038076639175415, R2 -0.040076371282339096\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 292, loss 1.9738483428955078, R2 -0.1258024126291275\n",
      "Eval loss 2.033937692642212, R2 -0.03778918460011482\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 293, loss 1.9697155952453613, R2 -0.12319549918174744\n",
      "Eval loss 2.0298147201538086, R2 -0.0355096310377121\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 294, loss 1.9655990600585938, R2 -0.1205979660153389\n",
      "Eval loss 2.025707483291626, R2 -0.0332389697432518\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 295, loss 1.961498737335205, R2 -0.11800945550203323\n",
      "Eval loss 2.021616220474243, R2 -0.03097589686512947\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 296, loss 1.9574147462844849, R2 -0.11543098092079163\n",
      "Eval loss 2.01754093170166, R2 -0.02872164361178875\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 297, loss 1.9533469676971436, R2 -0.11286187916994095\n",
      "Eval loss 2.0134811401367188, R2 -0.026475222781300545\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 298, loss 1.9492950439453125, R2 -0.1103019043803215\n",
      "Eval loss 2.009437322616577, R2 -0.024236947298049927\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 299, loss 1.9452590942382812, R2 -0.1077517569065094\n",
      "Eval loss 2.005409002304077, R2 -0.02200656570494175\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 300, loss 1.9412389993667603, R2 -0.10521065443754196\n",
      "Eval loss 2.0013959407806396, R2 -0.019784526899456978\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 301, loss 1.9372347593307495, R2 -0.10267920792102814\n",
      "Eval loss 1.9973987340927124, R2 -0.01757057197391987\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 302, loss 1.9332464933395386, R2 -0.10015647858381271\n",
      "Eval loss 1.9934169054031372, R2 -0.015364235267043114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 303, loss 1.9292736053466797, R2 -0.09764378517866135\n",
      "Eval loss 1.9894503355026245, R2 -0.013166368007659912\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 304, loss 1.9253164529800415, R2 -0.09513981640338898\n",
      "Eval loss 1.9854991436004639, R2 -0.010975975543260574\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 305, loss 1.921375036239624, R2 -0.09264478832483292\n",
      "Eval loss 1.981562852859497, R2 -0.008793934248387814\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 306, loss 1.917448878288269, R2 -0.09015963971614838\n",
      "Eval loss 1.9776420593261719, R2 -0.006619131192564964\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 307, loss 1.9135379791259766, R2 -0.08768320083618164\n",
      "Eval loss 1.9737361669540405, R2 -0.004452911205589771\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 308, loss 1.9096429347991943, R2 -0.08521579205989838\n",
      "Eval loss 1.9698452949523926, R2 -0.0022939660120755434\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 309, loss 1.905762791633606, R2 -0.08275735378265381\n",
      "Eval loss 1.9659696817398071, R2 -0.00014315951557364315\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 310, loss 1.9018980264663696, R2 -0.08030766993761063\n",
      "Eval loss 1.9621086120605469, R2 0.002000380540266633\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 311, loss 1.8980482816696167, R2 -0.07786727696657181\n",
      "Eval loss 1.9582628011703491, R2 0.004135820083320141\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 312, loss 1.8942137956619263, R2 -0.07543618977069855\n",
      "Eval loss 1.954431414604187, R2 0.00626352708786726\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 313, loss 1.8903943300247192, R2 -0.07301351428031921\n",
      "Eval loss 1.9506149291992188, R2 0.00838381052017212\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 314, loss 1.8865898847579956, R2 -0.07059972733259201\n",
      "Eval loss 1.9468128681182861, R2 0.010496372357010841\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 315, loss 1.8828001022338867, R2 -0.0681949108839035\n",
      "Eval loss 1.9430255889892578, R2 0.01260171178728342\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 316, loss 1.8790253400802612, R2 -0.06579876691102982\n",
      "Eval loss 1.9392528533935547, R2 0.014698624610900879\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 317, loss 1.8752652406692505, R2 -0.06341130286455154\n",
      "Eval loss 1.935494303703308, R2 0.01678849384188652\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 318, loss 1.871519923210144, R2 -0.06103258952498436\n",
      "Eval loss 1.9317505359649658, R2 0.018871009349822998\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 319, loss 1.8677892684936523, R2 -0.05866248905658722\n",
      "Eval loss 1.92802095413208, R2 0.02094603143632412\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 320, loss 1.8640731573104858, R2 -0.05630112811923027\n",
      "Eval loss 1.92430579662323, R2 0.023013608530163765\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 321, loss 1.8603718280792236, R2 -0.05394789204001427\n",
      "Eval loss 1.9206043481826782, R2 0.025073610246181488\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 322, loss 1.856684684753418, R2 -0.05160404369235039\n",
      "Eval loss 1.9169178009033203, R2 0.027126312255859375\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 323, loss 1.8530117273330688, R2 -0.0492682009935379\n",
      "Eval loss 1.9132448434829712, R2 0.029171667993068695\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 324, loss 1.849353313446045, R2 -0.046941064298152924\n",
      "Eval loss 1.9095863103866577, R2 0.03120972402393818\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 325, loss 1.8457095623016357, R2 -0.044622357934713364\n",
      "Eval loss 1.905941367149353, R2 0.03324040398001671\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 326, loss 1.842079520225525, R2 -0.04231201112270355\n",
      "Eval loss 1.9023107290267944, R2 0.03526391088962555\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 327, loss 1.8384637832641602, R2 -0.04000967741012573\n",
      "Eval loss 1.898693561553955, R2 0.037280336022377014\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 328, loss 1.834862232208252, R2 -0.03771653398871422\n",
      "Eval loss 1.8950905799865723, R2 0.03928940370678902\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 329, loss 1.8312748670578003, R2 -0.03543110936880112\n",
      "Eval loss 1.8915010690689087, R2 0.041291046887636185\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 330, loss 1.8277009725570679, R2 -0.0331539511680603\n",
      "Eval loss 1.887925624847412, R2 0.04328561946749687\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 331, loss 1.8241413831710815, R2 -0.03088562563061714\n",
      "Eval loss 1.8843632936477661, R2 0.04527348652482033\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 332, loss 1.8205957412719727, R2 -0.028624804690480232\n",
      "Eval loss 1.880815029144287, R2 0.04725390300154686\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 333, loss 1.817063570022583, R2 -0.026372872292995453\n",
      "Eval loss 1.8772799968719482, R2 0.049227356910705566\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 334, loss 1.8135452270507812, R2 -0.024128686636686325\n",
      "Eval loss 1.8737585544586182, R2 0.051193591207265854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 335, loss 1.8100404739379883, R2 -0.021892748773097992\n",
      "Eval loss 1.8702505826950073, R2 0.05315301567316055\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 336, loss 1.8065496683120728, R2 -0.01966489478945732\n",
      "Eval loss 1.8667558431625366, R2 0.05510566011071205\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 337, loss 1.803072452545166, R2 -0.017444979399442673\n",
      "Eval loss 1.8632744550704956, R2 0.057050928473472595\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 338, loss 1.7996083498001099, R2 -0.015233224257826805\n",
      "Eval loss 1.8598064184188843, R2 0.058989811688661575\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 339, loss 1.7961578369140625, R2 -0.01302945613861084\n",
      "Eval loss 1.856351375579834, R2 0.060921646654605865\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 340, loss 1.792720913887024, R2 -0.010833594016730785\n",
      "Eval loss 1.8529094457626343, R2 0.06284603476524353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 341, loss 1.789297103881836, R2 -0.008645588532090187\n",
      "Eval loss 1.8494808673858643, R2 0.06476423144340515\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 342, loss 1.7858867645263672, R2 -0.00646564643830061\n",
      "Eval loss 1.8460651636123657, R2 0.0666753277182579\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 343, loss 1.78248929977417, R2 -0.004293067846447229\n",
      "Eval loss 1.8426623344421387, R2 0.06857947260141373\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 344, loss 1.7791054248809814, R2 -0.0021291484590619802\n",
      "Eval loss 1.8392724990844727, R2 0.07047728449106216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 345, loss 1.7757344245910645, R2 2.7255578970653005e-05\n",
      "Eval loss 1.8358955383300781, R2 0.07236817479133606\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 346, loss 1.7723764181137085, R2 0.0021760084200650454\n",
      "Eval loss 1.8325315713882446, R2 0.07425219565629959\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 347, loss 1.7690316438674927, R2 0.004316909704357386\n",
      "Eval loss 1.8291800022125244, R2 0.07612985372543335\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 348, loss 1.7656997442245483, R2 0.006450544577091932\n",
      "Eval loss 1.8258413076400757, R2 0.07800091058015823\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 349, loss 1.7623804807662964, R2 0.00857585109770298\n",
      "Eval loss 1.8225152492523193, R2 0.07986477017402649\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 350, loss 1.7590744495391846, R2 0.010693680495023727\n",
      "Eval loss 1.8192020654678345, R2 0.08172247558832169\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 351, loss 1.7557809352874756, R2 0.012804079800844193\n",
      "Eval loss 1.8159009218215942, R2 0.08357345312833786\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 352, loss 1.7525001764297485, R2 0.014906683005392551\n",
      "Eval loss 1.812612533569336, R2 0.08541807532310486\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 353, loss 1.749232292175293, R2 0.017001958563923836\n",
      "Eval loss 1.8093366622924805, R2 0.08725616335868835\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 354, loss 1.7459766864776611, R2 0.019089477136731148\n",
      "Eval loss 1.8060729503631592, R2 0.08908750861883163\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 355, loss 1.7427340745925903, R2 0.021169651299715042\n",
      "Eval loss 1.8028217554092407, R2 0.09091288596391678\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 356, loss 1.7395035028457642, R2 0.023242207244038582\n",
      "Eval loss 1.7995827198028564, R2 0.09273114055395126\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 357, loss 1.73628568649292, R2 0.025307541713118553\n",
      "Eval loss 1.796356201171875, R2 0.09454345703125\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 358, loss 1.733080267906189, R2 0.027365267276763916\n",
      "Eval loss 1.7931413650512695, R2 0.09634941071271896\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 359, loss 1.7298871278762817, R2 0.029415780678391457\n",
      "Eval loss 1.789939284324646, R2 0.09814873337745667\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 360, loss 1.7267062664031982, R2 0.03145867586135864\n",
      "Eval loss 1.7867488861083984, R2 0.09994180500507355\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 361, loss 1.7235376834869385, R2 0.033494506031274796\n",
      "Eval loss 1.7835705280303955, R2 0.10172843933105469\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 362, loss 1.720381259918213, R2 0.03552284464240074\n",
      "Eval loss 1.7804043292999268, R2 0.10350903868675232\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 363, loss 1.7172369956970215, R2 0.03754400834441185\n",
      "Eval loss 1.777250051498413, R2 0.10528306663036346\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 364, loss 1.7141048908233643, R2 0.03955799713730812\n",
      "Eval loss 1.7741074562072754, R2 0.10705122351646423\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 365, loss 1.7109849452972412, R2 0.041564688086509705\n",
      "Eval loss 1.7709767818450928, R2 0.10881292074918747\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 366, loss 1.7078768014907837, R2 0.043564263731241226\n",
      "Eval loss 1.7678579092025757, R2 0.11056819558143616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 367, loss 1.7047805786132812, R2 0.045556724071502686\n",
      "Eval loss 1.7647507190704346, R2 0.1123177781701088\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 368, loss 1.7016963958740234, R2 0.04754209518432617\n",
      "Eval loss 1.7616552114486694, R2 0.1140611469745636\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 369, loss 1.6986238956451416, R2 0.04952019453048706\n",
      "Eval loss 1.7585713863372803, R2 0.11579819023609161\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 370, loss 1.6955634355545044, R2 0.05149141699075699\n",
      "Eval loss 1.755499243736267, R2 0.11752908676862717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 371, loss 1.6925145387649536, R2 0.053455375134944916\n",
      "Eval loss 1.7524385452270508, R2 0.11925413459539413\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 372, loss 1.6894773244857788, R2 0.05541276931762695\n",
      "Eval loss 1.7493891716003418, R2 0.12097302079200745\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 373, loss 1.6864516735076904, R2 0.05736251175403595\n",
      "Eval loss 1.7463515996932983, R2 0.12268602102994919\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 374, loss 1.6834378242492676, R2 0.05930573120713234\n",
      "Eval loss 1.7433252334594727, R2 0.12439263612031937\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 375, loss 1.6804357767105103, R2 0.06124194711446762\n",
      "Eval loss 1.7403101921081543, R2 0.12609362602233887\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 376, loss 1.677444577217102, R2 0.0631714016199112\n",
      "Eval loss 1.7373064756393433, R2 0.12778867781162262\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 377, loss 1.6744651794433594, R2 0.06509394198656082\n",
      "Eval loss 1.7343140840530396, R2 0.12947753071784973\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 378, loss 1.671497106552124, R2 0.06700977683067322\n",
      "Eval loss 1.731332778930664, R2 0.1311608999967575\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 379, loss 1.6685404777526855, R2 0.06891817599534988\n",
      "Eval loss 1.7283629179000854, R2 0.13283823430538177\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 380, loss 1.6655951738357544, R2 0.07082026451826096\n",
      "Eval loss 1.7254037857055664, R2 0.13450926542282104\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 381, loss 1.6626611948013306, R2 0.07271547615528107\n",
      "Eval loss 1.7224559783935547, R2 0.1361749768257141\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 382, loss 1.659738302230835, R2 0.07460394501686096\n",
      "Eval loss 1.7195191383361816, R2 0.13783466815948486\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 383, loss 1.6568264961242676, R2 0.07648604363203049\n",
      "Eval loss 1.7165933847427368, R2 0.1394888311624527\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 384, loss 1.6539257764816284, R2 0.07836093753576279\n",
      "Eval loss 1.713678240776062, R2 0.1411367952823639\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 385, loss 1.6510363817214966, R2 0.08022956550121307\n",
      "Eval loss 1.710774540901184, R2 0.1427791714668274\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 386, loss 1.648158073425293, R2 0.08209116756916046\n",
      "Eval loss 1.7078810930252075, R2 0.1444159746170044\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 387, loss 1.645290493965149, R2 0.0839463621377945\n",
      "Eval loss 1.7049986124038696, R2 0.14604675769805908\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 388, loss 1.642434000968933, R2 0.08579515665769577\n",
      "Eval loss 1.7021270990371704, R2 0.14767226576805115\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 389, loss 1.6395882368087769, R2 0.08763726055622101\n",
      "Eval loss 1.6992663145065308, R2 0.1492917686700821\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 390, loss 1.6367533206939697, R2 0.08947287499904633\n",
      "Eval loss 1.6964160203933716, R2 0.1509060263633728\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 391, loss 1.6339293718338013, R2 0.09130185097455978\n",
      "Eval loss 1.6935763359069824, R2 0.15251432359218597\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 392, loss 1.6311161518096924, R2 0.09312445670366287\n",
      "Eval loss 1.6907474994659424, R2 0.1541171371936798\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 393, loss 1.6283131837844849, R2 0.09494059532880783\n",
      "Eval loss 1.6879287958145142, R2 0.1557147055864334\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 394, loss 1.6255215406417847, R2 0.09675032645463943\n",
      "Eval loss 1.685120701789856, R2 0.15730629861354828\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 395, loss 1.6227400302886963, R2 0.09855366498231888\n",
      "Eval loss 1.6823234558105469, R2 0.15889239311218262\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 396, loss 1.6199694871902466, R2 0.10035061836242676\n",
      "Eval loss 1.67953622341156, R2 0.16047297418117523\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 397, loss 1.6172093152999878, R2 0.10214123874902725\n",
      "Eval loss 1.6767593622207642, R2 0.16204822063446045\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 398, loss 1.6144593954086304, R2 0.1039256602525711\n",
      "Eval loss 1.6739928722381592, R2 0.16361810266971588\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 399, loss 1.6117202043533325, R2 0.10570359230041504\n",
      "Eval loss 1.6712368726730347, R2 0.1651824414730072\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 400, loss 1.608991265296936, R2 0.10747527331113815\n",
      "Eval loss 1.6684907674789429, R2 0.16674113273620605\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 401, loss 1.6062726974487305, R2 0.10924076288938522\n",
      "Eval loss 1.6657549142837524, R2 0.16829492151737213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 402, loss 1.6035646200180054, R2 0.11100006103515625\n",
      "Eval loss 1.663029432296753, R2 0.16984298825263977\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 403, loss 1.600866675376892, R2 0.11275310069322586\n",
      "Eval loss 1.6603140830993652, R2 0.17138558626174927\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 404, loss 1.5981789827346802, R2 0.11450009047985077\n",
      "Eval loss 1.6576086282730103, R2 0.17292334139347076\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 405, loss 1.59550142288208, R2 0.11624100804328918\n",
      "Eval loss 1.654913306236267, R2 0.17445534467697144\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 406, loss 1.5928341150283813, R2 0.11797534674406052\n",
      "Eval loss 1.6522279977798462, R2 0.17598199844360352\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 407, loss 1.5901764631271362, R2 0.11970385909080505\n",
      "Eval loss 1.649552583694458, R2 0.177503764629364\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 408, loss 1.5875294208526611, R2 0.12142626941204071\n",
      "Eval loss 1.6468870639801025, R2 0.17902010679244995\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 409, loss 1.5848923921585083, R2 0.12314271926879883\n",
      "Eval loss 1.6442313194274902, R2 0.18053089082241058\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 410, loss 1.5822651386260986, R2 0.12485327571630478\n",
      "Eval loss 1.6415855884552002, R2 0.18203692138195038\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 411, loss 1.5796477794647217, R2 0.12655740976333618\n",
      "Eval loss 1.6389496326446533, R2 0.1835375726222992\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 412, loss 1.5770403146743774, R2 0.12825556099414825\n",
      "Eval loss 1.63632333278656, R2 0.1850331723690033\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 413, loss 1.5744428634643555, R2 0.12994807958602905\n",
      "Eval loss 1.6337066888809204, R2 0.18652339279651642\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 414, loss 1.571855068206787, R2 0.13163451850414276\n",
      "Eval loss 1.631099820137024, R2 0.188008651137352\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 415, loss 1.569277048110962, R2 0.1333150714635849\n",
      "Eval loss 1.6285024881362915, R2 0.18948893249034882\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 416, loss 1.5667088031768799, R2 0.1349896788597107\n",
      "Eval loss 1.6259148120880127, R2 0.19096413254737854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 417, loss 1.5641502141952515, R2 0.1366586536169052\n",
      "Eval loss 1.623336672782898, R2 0.1924339234828949\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 418, loss 1.561601161956787, R2 0.13832145929336548\n",
      "Eval loss 1.6207680702209473, R2 0.1938987374305725\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 419, loss 1.559062123298645, R2 0.1399785429239273\n",
      "Eval loss 1.6182087659835815, R2 0.19535866379737854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 420, loss 1.5565322637557983, R2 0.1416298896074295\n",
      "Eval loss 1.6156591176986694, R2 0.1968134492635727\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 421, loss 1.5540120601654053, R2 0.1432754248380661\n",
      "Eval loss 1.6131190061569214, R2 0.19826339185237885\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 422, loss 1.5515012741088867, R2 0.14491522312164307\n",
      "Eval loss 1.6105878353118896, R2 0.1997082382440567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 423, loss 1.5490000247955322, R2 0.1465492844581604\n",
      "Eval loss 1.6080660820007324, R2 0.20114822685718536\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 424, loss 1.5465081930160522, R2 0.1481776386499405\n",
      "Eval loss 1.6055537462234497, R2 0.20258349180221558\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 425, loss 1.5440255403518677, R2 0.14980031549930573\n",
      "Eval loss 1.6030505895614624, R2 0.204013392329216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 426, loss 1.5415525436401367, R2 0.15141735970973969\n",
      "Eval loss 1.6005566120147705, R2 0.20543861389160156\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 427, loss 1.5390886068344116, R2 0.15302874147891998\n",
      "Eval loss 1.5980719327926636, R2 0.20685933530330658\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 428, loss 1.536634087562561, R2 0.154634490609169\n",
      "Eval loss 1.595596194267273, R2 0.208274707198143\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 429, loss 1.5341885089874268, R2 0.15623463690280914\n",
      "Eval loss 1.5931296348571777, R2 0.20968537032604218\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 430, loss 1.5317524671554565, R2 0.15782921016216278\n",
      "Eval loss 1.590672254562378, R2 0.2110908478498459\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 431, loss 1.529325246810913, R2 0.1594182401895523\n",
      "Eval loss 1.5882236957550049, R2 0.21249203383922577\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 432, loss 1.5269073247909546, R2 0.1610018014907837\n",
      "Eval loss 1.5857843160629272, R2 0.2138882577419281\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 433, loss 1.5244982242584229, R2 0.16257968544960022\n",
      "Eval loss 1.5833536386489868, R2 0.21527957916259766\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 434, loss 1.522098422050476, R2 0.1641521453857422\n",
      "Eval loss 1.5809320211410522, R2 0.21666644513607025\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 435, loss 1.5197075605392456, R2 0.1657191962003708\n",
      "Eval loss 1.5785192251205444, R2 0.21804851293563843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 436, loss 1.5173256397247314, R2 0.16728079319000244\n",
      "Eval loss 1.576115369796753, R2 0.21942567825317383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 437, loss 1.5149524211883545, R2 0.16883689165115356\n",
      "Eval loss 1.5737202167510986, R2 0.22079844772815704\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 438, loss 1.5125882625579834, R2 0.17038799822330475\n",
      "Eval loss 1.5713337659835815, R2 0.2221662849187851\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 439, loss 1.5102331638336182, R2 0.1719331294298172\n",
      "Eval loss 1.5689561367034912, R2 0.22352968156337738\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 440, loss 1.5078867673873901, R2 0.1734730452299118\n",
      "Eval loss 1.5665873289108276, R2 0.22488833963871002\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 441, loss 1.5055488348007202, R2 0.17500802874565125\n",
      "Eval loss 1.5642268657684326, R2 0.22624219954013824\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 442, loss 1.5032198429107666, R2 0.17653705179691315\n",
      "Eval loss 1.561875343322754, R2 0.22759170830249786\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 443, loss 1.5008996725082397, R2 0.17806105315685272\n",
      "Eval loss 1.5595321655273438, R2 0.22893637418746948\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 444, loss 1.4985878467559814, R2 0.1795801818370819\n",
      "Eval loss 1.5571978092193604, R2 0.23027676343917847\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 445, loss 1.4962849617004395, R2 0.18109330534934998\n",
      "Eval loss 1.5548717975616455, R2 0.23161238431930542\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 446, loss 1.493990421295166, R2 0.1826016753911972\n",
      "Eval loss 1.5525543689727783, R2 0.23294353485107422\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 447, loss 1.4917045831680298, R2 0.18410469591617584\n",
      "Eval loss 1.5502450466156006, R2 0.23427027463912964\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 448, loss 1.4894273281097412, R2 0.18560242652893066\n",
      "Eval loss 1.5479445457458496, R2 0.23559240996837616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 449, loss 1.4871585369110107, R2 0.18709507584571838\n",
      "Eval loss 1.545652151107788, R2 0.23691019415855408\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 450, loss 1.4848982095718384, R2 0.18858259916305542\n",
      "Eval loss 1.5433682203292847, R2 0.23822329938411713\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 451, loss 1.482646107673645, R2 0.19006499648094177\n",
      "Eval loss 1.5410926342010498, R2 0.23953214287757874\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 452, loss 1.4804027080535889, R2 0.19154219329357147\n",
      "Eval loss 1.5388256311416626, R2 0.24083638191223145\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 453, loss 1.4781672954559326, R2 0.19301436841487885\n",
      "Eval loss 1.5365663766860962, R2 0.24213658273220062\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 454, loss 1.475940465927124, R2 0.1944815069437027\n",
      "Eval loss 1.5343157052993774, R2 0.24343200027942657\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 455, loss 1.4737217426300049, R2 0.1959434449672699\n",
      "Eval loss 1.5320730209350586, R2 0.24472318589687347\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 456, loss 1.4715113639831543, R2 0.19740037620067596\n",
      "Eval loss 1.5298384428024292, R2 0.24600987136363983\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 457, loss 1.4693090915679932, R2 0.1988525241613388\n",
      "Eval loss 1.5276120901107788, R2 0.2472923845052719\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 458, loss 1.4671152830123901, R2 0.20029927790164948\n",
      "Eval loss 1.5253938436508179, R2 0.24857057631015778\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 459, loss 1.4649293422698975, R2 0.20174135267734528\n",
      "Eval loss 1.5231834650039673, R2 0.2498442530632019\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 460, loss 1.4627515077590942, R2 0.20317822694778442\n",
      "Eval loss 1.5209811925888062, R2 0.2511138916015625\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 461, loss 1.46058189868927, R2 0.20461034774780273\n",
      "Eval loss 1.518786907196045, R2 0.2523789703845978\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 462, loss 1.4584203958511353, R2 0.20603743195533752\n",
      "Eval loss 1.5166008472442627, R2 0.25364017486572266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 463, loss 1.4562666416168213, R2 0.20746003091335297\n",
      "Eval loss 1.5144221782684326, R2 0.25489696860313416\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 464, loss 1.4541211128234863, R2 0.20887699723243713\n",
      "Eval loss 1.5122517347335815, R2 0.2561492621898651\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 465, loss 1.4519833326339722, R2 0.21028946340084076\n",
      "Eval loss 1.5100891590118408, R2 0.2573976516723633\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 466, loss 1.449853539466858, R2 0.21169723570346832\n",
      "Eval loss 1.5079344511032104, R2 0.25864163041114807\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 467, loss 1.4477317333221436, R2 0.21310019493103027\n",
      "Eval loss 1.5057872533798218, R2 0.259881854057312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 468, loss 1.44561767578125, R2 0.214497908949852\n",
      "Eval loss 1.503648042678833, R2 0.2611173689365387\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 469, loss 1.4435113668441772, R2 0.2158910632133484\n",
      "Eval loss 1.5015166997909546, R2 0.26234886050224304\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 470, loss 1.4414130449295044, R2 0.21727949380874634\n",
      "Eval loss 1.4993927478790283, R2 0.263576477766037\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 471, loss 1.4393222332000732, R2 0.21866317093372345\n",
      "Eval loss 1.4972766637802124, R2 0.26479989290237427\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 472, loss 1.4372392892837524, R2 0.22004207968711853\n",
      "Eval loss 1.4951680898666382, R2 0.26601895689964294\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 473, loss 1.4351640939712524, R2 0.22141627967357635\n",
      "Eval loss 1.4930675029754639, R2 0.2672342360019684\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 474, loss 1.4330966472625732, R2 0.2227858006954193\n",
      "Eval loss 1.4909740686416626, R2 0.26844537258148193\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 475, loss 1.4310367107391357, R2 0.224150612950325\n",
      "Eval loss 1.4888882637023926, R2 0.2696523368358612\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 476, loss 1.4289844036102295, R2 0.22551098465919495\n",
      "Eval loss 1.4868099689483643, R2 0.2708553969860077\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 477, loss 1.4269392490386963, R2 0.22686654329299927\n",
      "Eval loss 1.4847391843795776, R2 0.2720543146133423\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 478, loss 1.4249022006988525, R2 0.22821711003780365\n",
      "Eval loss 1.4826759099960327, R2 0.27324917912483215\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 479, loss 1.4228723049163818, R2 0.2295633852481842\n",
      "Eval loss 1.48062002658844, R2 0.274440199136734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 480, loss 1.4208499193191528, R2 0.23090501129627228\n",
      "Eval loss 1.4785716533660889, R2 0.2756273150444031\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 481, loss 1.418835163116455, R2 0.23224219679832458\n",
      "Eval loss 1.4765305519104004, R2 0.2768102288246155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 482, loss 1.4168277978897095, R2 0.23357467353343964\n",
      "Eval loss 1.474496841430664, R2 0.2779892385005951\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 483, loss 1.4148277044296265, R2 0.23490290343761444\n",
      "Eval loss 1.4724704027175903, R2 0.2791643440723419\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 484, loss 1.4128350019454956, R2 0.23622609674930573\n",
      "Eval loss 1.4704511165618896, R2 0.2803357243537903\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 485, loss 1.4108495712280273, R2 0.23754505813121796\n",
      "Eval loss 1.4684392213821411, R2 0.2815028429031372\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 486, loss 1.4088716506958008, R2 0.23885956406593323\n",
      "Eval loss 1.4664345979690552, R2 0.28266641497612\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 487, loss 1.4069007635116577, R2 0.24016956984996796\n",
      "Eval loss 1.4644370079040527, R2 0.2838257849216461\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 488, loss 1.4049371480941772, R2 0.24147529900074005\n",
      "Eval loss 1.462446689605713, R2 0.2849816083908081\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 489, loss 1.4029806852340698, R2 0.24277615547180176\n",
      "Eval loss 1.4604636430740356, R2 0.28613340854644775\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 490, loss 1.4010316133499146, R2 0.24407295882701874\n",
      "Eval loss 1.4584875106811523, R2 0.28728148341178894\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 491, loss 1.3990895748138428, R2 0.24536509811878204\n",
      "Eval loss 1.4565184116363525, R2 0.28842565417289734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 492, loss 1.397154688835144, R2 0.2466530203819275\n",
      "Eval loss 1.4545564651489258, R2 0.28956612944602966\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 493, loss 1.3952269554138184, R2 0.24793650209903717\n",
      "Eval loss 1.452601671218872, R2 0.2907027304172516\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 494, loss 1.393306016921997, R2 0.2492159903049469\n",
      "Eval loss 1.4506535530090332, R2 0.2918354570865631\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 495, loss 1.3913923501968384, R2 0.2504904568195343\n",
      "Eval loss 1.448712706565857, R2 0.29296448826789856\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 496, loss 1.3894855976104736, R2 0.25176122784614563\n",
      "Eval loss 1.446778655052185, R2 0.2940899431705475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 497, loss 1.3875858783721924, R2 0.25302717089653015\n",
      "Eval loss 1.4448515176773071, R2 0.29521143436431885\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 498, loss 1.3856931924819946, R2 0.25428909063339233\n",
      "Eval loss 1.4429312944412231, R2 0.29632920026779175\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 499, loss 1.3838071823120117, R2 0.2555466890335083\n",
      "Eval loss 1.441017985343933, R2 0.2974435091018677\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 500, loss 1.3819283246994019, R2 0.2568000853061676\n",
      "Eval loss 1.4391114711761475, R2 0.2985539138317108\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 501, loss 1.3800560235977173, R2 0.25804921984672546\n",
      "Eval loss 1.4372116327285767, R2 0.29966068267822266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 502, loss 1.3781907558441162, R2 0.2592941224575043\n",
      "Eval loss 1.435318946838379, R2 0.300763875246048\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 503, loss 1.3763322830200195, R2 0.2605349123477936\n",
      "Eval loss 1.4334325790405273, R2 0.30186349153518677\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 504, loss 1.3744806051254272, R2 0.2617713212966919\n",
      "Eval loss 1.4315531253814697, R2 0.3029593229293823\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 505, loss 1.3726356029510498, R2 0.2630036771297455\n",
      "Eval loss 1.429680347442627, R2 0.30405160784721375\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 506, loss 1.3707973957061768, R2 0.26423192024230957\n",
      "Eval loss 1.427814245223999, R2 0.30514004826545715\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 507, loss 1.3689658641815186, R2 0.2654559016227722\n",
      "Eval loss 1.4259546995162964, R2 0.3062252104282379\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 508, loss 1.3671410083770752, R2 0.2666759788990021\n",
      "Eval loss 1.4241019487380981, R2 0.30730679631233215\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 509, loss 1.3653228282928467, R2 0.2678915560245514\n",
      "Eval loss 1.4222556352615356, R2 0.3083844482898712\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 510, loss 1.363511323928833, R2 0.2691032588481903\n",
      "Eval loss 1.420415997505188, R2 0.3094589114189148\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 511, loss 1.3617063760757446, R2 0.2703108787536621\n",
      "Eval loss 1.418582797050476, R2 0.3105298578739166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 512, loss 1.359907865524292, R2 0.2715144157409668\n",
      "Eval loss 1.4167560338974, R2 0.31159690022468567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 513, loss 1.3581159114837646, R2 0.27271386981010437\n",
      "Eval loss 1.4149359464645386, R2 0.31266072392463684\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 514, loss 1.3563306331634521, R2 0.27390941977500916\n",
      "Eval loss 1.413122296333313, R2 0.3137210011482239\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 515, loss 1.3545516729354858, R2 0.2751010060310364\n",
      "Eval loss 1.4113150835037231, R2 0.314777672290802\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 516, loss 1.3527792692184448, R2 0.2762884199619293\n",
      "Eval loss 1.4095141887664795, R2 0.31583115458488464\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 517, loss 1.35101318359375, R2 0.277471661567688\n",
      "Eval loss 1.407719612121582, R2 0.3168809711933136\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 518, loss 1.34925377368927, R2 0.2786512076854706\n",
      "Eval loss 1.4059315919876099, R2 0.3179273307323456\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 519, loss 1.3475005626678467, R2 0.27982664108276367\n",
      "Eval loss 1.4041498899459839, R2 0.3189704120159149\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 520, loss 1.3457536697387695, R2 0.280998170375824\n",
      "Eval loss 1.402374267578125, R2 0.3200097680091858\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 521, loss 1.3440130949020386, R2 0.28216585516929626\n",
      "Eval loss 1.4006052017211914, R2 0.3210458755493164\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 522, loss 1.3422789573669434, R2 0.2833295166492462\n",
      "Eval loss 1.3988423347473145, R2 0.32207855582237244\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 523, loss 1.3405511379241943, R2 0.28448939323425293\n",
      "Eval loss 1.3970855474472046, R2 0.32310807704925537\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 524, loss 1.3388293981552124, R2 0.28564536571502686\n",
      "Eval loss 1.3953351974487305, R2 0.3241339921951294\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 525, loss 1.3371139764785767, R2 0.2867973744869232\n",
      "Eval loss 1.3935908079147339, R2 0.3251565992832184\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 526, loss 1.3354047536849976, R2 0.28794562816619873\n",
      "Eval loss 1.3918527364730835, R2 0.326175719499588\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 527, loss 1.333701729774475, R2 0.2890898883342743\n",
      "Eval loss 1.3901207447052002, R2 0.32719171047210693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 528, loss 1.3320049047470093, R2 0.29023054242134094\n",
      "Eval loss 1.388394832611084, R2 0.3282044231891632\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 529, loss 1.330314040184021, R2 0.29136714339256287\n",
      "Eval loss 1.3866751194000244, R2 0.3292137086391449\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 530, loss 1.328629493713379, R2 0.29250019788742065\n",
      "Eval loss 1.384961485862732, R2 0.3302197754383087\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 531, loss 1.326951026916504, R2 0.2936292886734009\n",
      "Eval loss 1.3832536935806274, R2 0.3312225639820099\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 532, loss 1.325278401374817, R2 0.29475462436676025\n",
      "Eval loss 1.38155198097229, R2 0.3322218358516693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 533, loss 1.323611855506897, R2 0.29587656259536743\n",
      "Eval loss 1.3798563480377197, R2 0.3332180976867676\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 534, loss 1.3219515085220337, R2 0.29699429869651794\n",
      "Eval loss 1.378166675567627, R2 0.33421099185943604\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 535, loss 1.3202970027923584, R2 0.2981082499027252\n",
      "Eval loss 1.3764829635620117, R2 0.33520063757896423\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 536, loss 1.3186485767364502, R2 0.29921865463256836\n",
      "Eval loss 1.3748050928115845, R2 0.33618712425231934\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 537, loss 1.31700599193573, R2 0.3003256618976593\n",
      "Eval loss 1.3731333017349243, R2 0.33717042207717896\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 538, loss 1.3153693675994873, R2 0.3014286756515503\n",
      "Eval loss 1.371467113494873, R2 0.33815068006515503\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 539, loss 1.3137385845184326, R2 0.3025277853012085\n",
      "Eval loss 1.3698070049285889, R2 0.3391273617744446\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 540, loss 1.312113642692566, R2 0.30362358689308167\n",
      "Eval loss 1.3681527376174927, R2 0.34010106325149536\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 541, loss 1.3104946613311768, R2 0.30471566319465637\n",
      "Eval loss 1.366504192352295, R2 0.34107163548469543\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 542, loss 1.308881402015686, R2 0.3058040738105774\n",
      "Eval loss 1.3648613691329956, R2 0.3420391082763672\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 543, loss 1.3072739839553833, R2 0.3068888783454895\n",
      "Eval loss 1.3632243871688843, R2 0.34300339221954346\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 544, loss 1.305672287940979, R2 0.3079703152179718\n",
      "Eval loss 1.3615930080413818, R2 0.3439646065235138\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 545, loss 1.3040763139724731, R2 0.309047669172287\n",
      "Eval loss 1.3599674701690674, R2 0.34492236375808716\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 546, loss 1.3024861812591553, R2 0.3101217448711395\n",
      "Eval loss 1.3583476543426514, R2 0.34587734937667847\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 547, loss 1.3009017705917358, R2 0.31119221448898315\n",
      "Eval loss 1.3567334413528442, R2 0.34682923555374146\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 548, loss 1.2993229627609253, R2 0.31225916743278503\n",
      "Eval loss 1.3551249504089355, R2 0.3477778136730194\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 549, loss 1.2977498769760132, R2 0.313322514295578\n",
      "Eval loss 1.3535221815109253, R2 0.34872353076934814\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 550, loss 1.29618239402771, R2 0.3143824338912964\n",
      "Eval loss 1.3519247770309448, R2 0.34966614842414856\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 551, loss 1.2946205139160156, R2 0.31543898582458496\n",
      "Eval loss 1.3503330945968628, R2 0.35060572624206543\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 552, loss 1.2930642366409302, R2 0.31649163365364075\n",
      "Eval loss 1.3487470149993896, R2 0.35154229402542114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 553, loss 1.2915136814117432, R2 0.31754112243652344\n",
      "Eval loss 1.3471664190292358, R2 0.35247573256492615\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 554, loss 1.2899683713912964, R2 0.31858691573143005\n",
      "Eval loss 1.3455913066864014, R2 0.3534061908721924\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 555, loss 1.288428783416748, R2 0.31962957978248596\n",
      "Eval loss 1.3440217971801758, R2 0.3543335199356079\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 556, loss 1.286894679069519, R2 0.32066839933395386\n",
      "Eval loss 1.3424577713012695, R2 0.3552579879760742\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 557, loss 1.2853660583496094, R2 0.3217039108276367\n",
      "Eval loss 1.3408989906311035, R2 0.3561795949935913\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 558, loss 1.283842921257019, R2 0.3227361738681793\n",
      "Eval loss 1.339345932006836, R2 0.3570979833602905\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 559, loss 1.2823251485824585, R2 0.323764830827713\n",
      "Eval loss 1.337797999382019, R2 0.35801365971565247\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 560, loss 1.2808128595352173, R2 0.32479026913642883\n",
      "Eval loss 1.336255669593811, R2 0.35892611742019653\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 561, loss 1.2793059349060059, R2 0.3258121907711029\n",
      "Eval loss 1.3347187042236328, R2 0.3598356544971466\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 562, loss 1.2778043746948242, R2 0.32683080434799194\n",
      "Eval loss 1.3331869840621948, R2 0.3607424199581146\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 563, loss 1.276308298110962, R2 0.3278460204601288\n",
      "Eval loss 1.3316606283187866, R2 0.3616463840007782\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 564, loss 1.2748174667358398, R2 0.32885801792144775\n",
      "Eval loss 1.3301395177841187, R2 0.36254721879959106\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 565, loss 1.2733319997787476, R2 0.3298664391040802\n",
      "Eval loss 1.3286237716674805, R2 0.3634452819824219\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 566, loss 1.271851658821106, R2 0.3308717608451843\n",
      "Eval loss 1.3271132707595825, R2 0.36434033513069153\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 567, loss 1.2703768014907837, R2 0.33187374472618103\n",
      "Eval loss 1.3256080150604248, R2 0.3652327358722687\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 568, loss 1.2689069509506226, R2 0.3328723609447479\n",
      "Eval loss 1.3241080045700073, R2 0.36612194776535034\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 569, loss 1.2674424648284912, R2 0.3338676989078522\n",
      "Eval loss 1.32261323928833, R2 0.3670085668563843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 570, loss 1.2659831047058105, R2 0.3348598778247833\n",
      "Eval loss 1.3211236000061035, R2 0.36789214611053467\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 571, loss 1.2645291090011597, R2 0.33584868907928467\n",
      "Eval loss 1.3196392059326172, R2 0.36877310276031494\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 572, loss 1.26308012008667, R2 0.33683425188064575\n",
      "Eval loss 1.318159818649292, R2 0.36965110898017883\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 573, loss 1.2616363763809204, R2 0.33781659603118896\n",
      "Eval loss 1.316685676574707, R2 0.3705263137817383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 574, loss 1.2601977586746216, R2 0.3387957215309143\n",
      "Eval loss 1.3152166604995728, R2 0.3713987469673157\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 575, loss 1.2587641477584839, R2 0.33977165818214417\n",
      "Eval loss 1.3137526512145996, R2 0.3722683787345886\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 576, loss 1.2573357820510864, R2 0.3407444655895233\n",
      "Eval loss 1.3122937679290771, R2 0.37313520908355713\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 577, loss 1.25591242313385, R2 0.34171390533447266\n",
      "Eval loss 1.3108398914337158, R2 0.3739992082118988\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 578, loss 1.254494071006775, R2 0.34268027544021606\n",
      "Eval loss 1.3093910217285156, R2 0.374860554933548\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 579, loss 1.2530808448791504, R2 0.34364357590675354\n",
      "Eval loss 1.307947039604187, R2 0.3757191002368927\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 580, loss 1.2516722679138184, R2 0.3446035385131836\n",
      "Eval loss 1.3065083026885986, R2 0.3765749931335449\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 581, loss 1.2502690553665161, R2 0.3455605208873749\n",
      "Eval loss 1.3050744533538818, R2 0.37742817401885986\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 582, loss 1.2488707304000854, R2 0.3465142250061035\n",
      "Eval loss 1.303645372390747, R2 0.3782784640789032\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 583, loss 1.2474772930145264, R2 0.3474648892879486\n",
      "Eval loss 1.302221417427063, R2 0.37912604212760925\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 584, loss 1.2460888624191284, R2 0.34841248393058777\n",
      "Eval loss 1.3008023500442505, R2 0.37997111678123474\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 585, loss 1.2447052001953125, R2 0.34935691952705383\n",
      "Eval loss 1.2993881702423096, R2 0.3808133900165558\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 586, loss 1.2433265447616577, R2 0.35029852390289307\n",
      "Eval loss 1.2979788780212402, R2 0.38165292143821716\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 587, loss 1.2419527769088745, R2 0.35123682022094727\n",
      "Eval loss 1.296574354171753, R2 0.38248980045318604\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 588, loss 1.2405837774276733, R2 0.3521721065044403\n",
      "Eval loss 1.2951748371124268, R2 0.38332411646842957\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 589, loss 1.2392196655273438, R2 0.3531041145324707\n",
      "Eval loss 1.293779969215393, R2 0.38415566086769104\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 590, loss 1.2378603219985962, R2 0.3540332615375519\n",
      "Eval loss 1.292389988899231, R2 0.3849846124649048\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 591, loss 1.2365058660507202, R2 0.3549594581127167\n",
      "Eval loss 1.2910048961639404, R2 0.3858109712600708\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 592, loss 1.2351561784744263, R2 0.35588252544403076\n",
      "Eval loss 1.2896242141723633, R2 0.38663482666015625\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 593, loss 1.2338111400604248, R2 0.35680273175239563\n",
      "Eval loss 1.2882485389709473, R2 0.38745585083961487\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 594, loss 1.2324708700180054, R2 0.3577198088169098\n",
      "Eval loss 1.2868776321411133, R2 0.3882744014263153\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 595, loss 1.231135368347168, R2 0.35863417387008667\n",
      "Eval loss 1.2855113744735718, R2 0.3890901803970337\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 596, loss 1.2298046350479126, R2 0.35954535007476807\n",
      "Eval loss 1.2841496467590332, R2 0.3899034559726715\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 597, loss 1.2284785509109497, R2 0.36045339703559875\n",
      "Eval loss 1.2827928066253662, R2 0.39071428775787354\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 598, loss 1.2271569967269897, R2 0.3613587021827698\n",
      "Eval loss 1.2814404964447021, R2 0.39152249693870544\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 599, loss 1.2258399724960327, R2 0.36226120591163635\n",
      "Eval loss 1.2800928354263306, R2 0.39232805371284485\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 600, loss 1.2245279550552368, R2 0.3631606698036194\n",
      "Eval loss 1.2787498235702515, R2 0.39313116669654846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 601, loss 1.2232202291488647, R2 0.36405694484710693\n",
      "Eval loss 1.2774114608764648, R2 0.3939315974712372\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 602, loss 1.2219173908233643, R2 0.3649505376815796\n",
      "Eval loss 1.2760776281356812, R2 0.39472952485084534\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 603, loss 1.2206189632415771, R2 0.3658413887023926\n",
      "Eval loss 1.27474844455719, R2 0.3955250084400177\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 604, loss 1.219325065612793, R2 0.36672908067703247\n",
      "Eval loss 1.273423671722412, R2 0.39631813764572144\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 605, loss 1.2180358171463013, R2 0.36761415004730225\n",
      "Eval loss 1.2721035480499268, R2 0.3971085846424103\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 606, loss 1.2167508602142334, R2 0.36849603056907654\n",
      "Eval loss 1.2707877159118652, R2 0.3978964686393738\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 607, loss 1.2154706716537476, R2 0.36937522888183594\n",
      "Eval loss 1.2694765329360962, R2 0.3986818492412567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 608, loss 1.2141950130462646, R2 0.37025177478790283\n",
      "Eval loss 1.26816987991333, R2 0.3994649648666382\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 609, loss 1.212923526763916, R2 0.37112513184547424\n",
      "Eval loss 1.2668675184249878, R2 0.4002454876899719\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 610, loss 1.2116565704345703, R2 0.3719959259033203\n",
      "Eval loss 1.265569806098938, R2 0.4010235071182251\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 611, loss 1.2103941440582275, R2 0.3728637993335724\n",
      "Eval loss 1.2642765045166016, R2 0.40179920196533203\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 612, loss 1.2091361284255981, R2 0.37372887134552\n",
      "Eval loss 1.2629873752593994, R2 0.402572363615036\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 613, loss 1.2078824043273926, R2 0.37459126114845276\n",
      "Eval loss 1.2617028951644897, R2 0.4033431112766266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 614, loss 1.20663321018219, R2 0.375450998544693\n",
      "Eval loss 1.2604225873947144, R2 0.40411150455474854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 615, loss 1.2053883075714111, R2 0.37630757689476013\n",
      "Eval loss 1.259146809577942, R2 0.4048774838447571\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 616, loss 1.2041476964950562, R2 0.37716153264045715\n",
      "Eval loss 1.2578753232955933, R2 0.40564098954200745\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 617, loss 1.202911615371704, R2 0.378013014793396\n",
      "Eval loss 1.2566081285476685, R2 0.4064020812511444\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 618, loss 1.2016797065734863, R2 0.37886154651641846\n",
      "Eval loss 1.2553452253341675, R2 0.40716075897216797\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 619, loss 1.2004520893096924, R2 0.3797072470188141\n",
      "Eval loss 1.2540868520736694, R2 0.4079171419143677\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 620, loss 1.1992287635803223, R2 0.38055020570755005\n",
      "Eval loss 1.2528325319290161, R2 0.408671110868454\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 621, loss 1.198009729385376, R2 0.3813905715942383\n",
      "Eval loss 1.251582384109497, R2 0.4094228446483612\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 622, loss 1.196794867515564, R2 0.38222846388816833\n",
      "Eval loss 1.2503366470336914, R2 0.41017207503318787\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 623, loss 1.1955842971801758, R2 0.38306328654289246\n",
      "Eval loss 1.2490952014923096, R2 0.4109190106391907\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 624, loss 1.1943780183792114, R2 0.38389551639556885\n",
      "Eval loss 1.2478578090667725, R2 0.41166365146636963\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 625, loss 1.1931759119033813, R2 0.3847251236438751\n",
      "Eval loss 1.2466245889663696, R2 0.4124058783054352\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 626, loss 1.191977858543396, R2 0.3855520486831665\n",
      "Eval loss 1.245395541191101, R2 0.41314584016799927\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 627, loss 1.1907840967178345, R2 0.38637638092041016\n",
      "Eval loss 1.2441707849502563, R2 0.4138834774494171\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 628, loss 1.1895942687988281, R2 0.3871980607509613\n",
      "Eval loss 1.2429503202438354, R2 0.4146188795566559\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 629, loss 1.1884089708328247, R2 0.3880171477794647\n",
      "Eval loss 1.2417336702346802, R2 0.4153517782688141\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 630, loss 1.187227487564087, R2 0.38883352279663086\n",
      "Eval loss 1.2405213117599487, R2 0.41608256101608276\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 631, loss 1.1860500574111938, R2 0.38964730501174927\n",
      "Eval loss 1.239313006401062, R2 0.4168109893798828\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 632, loss 1.1848769187927246, R2 0.39045849442481995\n",
      "Eval loss 1.23810875415802, R2 0.4175373315811157\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 633, loss 1.1837077140808105, R2 0.391267329454422\n",
      "Eval loss 1.2369085550308228, R2 0.41826125979423523\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 634, loss 1.1825426816940308, R2 0.3920731246471405\n",
      "Eval loss 1.2357125282287598, R2 0.4189828634262085\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 635, loss 1.1813815832138062, R2 0.39287659525871277\n",
      "Eval loss 1.234520435333252, R2 0.41970235109329224\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 636, loss 1.1802246570587158, R2 0.3936774432659149\n",
      "Eval loss 1.2333323955535889, R2 0.42041951417922974\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 637, loss 1.1790715456008911, R2 0.39447590708732605\n",
      "Eval loss 1.2321484088897705, R2 0.4211345911026001\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 638, loss 1.1779224872589111, R2 0.39527150988578796\n",
      "Eval loss 1.2309683561325073, R2 0.4218473732471466\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 639, loss 1.1767774820327759, R2 0.39606472849845886\n",
      "Eval loss 1.2297922372817993, R2 0.4225578010082245\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 640, loss 1.1756364107131958, R2 0.3968553841114044\n",
      "Eval loss 1.2286202907562256, R2 0.42326611280441284\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 641, loss 1.1744991540908813, R2 0.3976435959339142\n",
      "Eval loss 1.227452039718628, R2 0.42397239804267883\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 642, loss 1.1733659505844116, R2 0.3984295129776001\n",
      "Eval loss 1.2262879610061646, R2 0.42467620968818665\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 643, loss 1.172236680984497, R2 0.39921247959136963\n",
      "Eval loss 1.2251276969909668, R2 0.42537805438041687\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 644, loss 1.1711113452911377, R2 0.3999931216239929\n",
      "Eval loss 1.2239712476730347, R2 0.42607757449150085\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 645, loss 1.169989824295044, R2 0.4007713198661804\n",
      "Eval loss 1.2228187322616577, R2 0.426775187253952\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 646, loss 1.1688721179962158, R2 0.40154704451560974\n",
      "Eval loss 1.221670150756836, R2 0.427470326423645\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 647, loss 1.1677583456039429, R2 0.4023202657699585\n",
      "Eval loss 1.2205253839492798, R2 0.42816340923309326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 648, loss 1.1666483879089355, R2 0.4030911326408386\n",
      "Eval loss 1.2193844318389893, R2 0.428854376077652\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 649, loss 1.1655422449111938, R2 0.403859406709671\n",
      "Eval loss 1.2182472944259644, R2 0.42954322695732117\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 650, loss 1.1644400358200073, R2 0.4046252965927124\n",
      "Eval loss 1.2171140909194946, R2 0.43022996187210083\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 651, loss 1.1633415222167969, R2 0.40538883209228516\n",
      "Eval loss 1.2159847021102905, R2 0.43091443181037903\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 652, loss 1.1622467041015625, R2 0.40614983439445496\n",
      "Eval loss 1.2148590087890625, R2 0.4315970540046692\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 653, loss 1.1611558198928833, R2 0.40690848231315613\n",
      "Eval loss 1.2137371301651, R2 0.4322773218154907\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 654, loss 1.1600685119628906, R2 0.4076646864414215\n",
      "Eval loss 1.2126189470291138, R2 0.43295571208000183\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 655, loss 1.1589851379394531, R2 0.40841853618621826\n",
      "Eval loss 1.2115044593811035, R2 0.4336318075656891\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 656, loss 1.1579053401947021, R2 0.40917015075683594\n",
      "Eval loss 1.2103937864303589, R2 0.43430596590042114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 657, loss 1.156829595565796, R2 0.40991905331611633\n",
      "Eval loss 1.2092869281768799, R2 0.4349779188632965\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 658, loss 1.155756950378418, R2 0.4106657803058624\n",
      "Eval loss 1.2081836462020874, R2 0.4356479346752167\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 659, loss 1.1546883583068848, R2 0.4114101231098175\n",
      "Eval loss 1.2070841789245605, R2 0.4363158345222473\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 660, loss 1.153623342514038, R2 0.41215208172798157\n",
      "Eval loss 1.2059882879257202, R2 0.4369817078113556\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 661, loss 1.152562141418457, R2 0.41289177536964417\n",
      "Eval loss 1.2048959732055664, R2 0.4376453161239624\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 662, loss 1.1515045166015625, R2 0.41362902522087097\n",
      "Eval loss 1.2038074731826782, R2 0.43830716609954834\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 663, loss 1.1504504680633545, R2 0.4143640697002411\n",
      "Eval loss 1.202722430229187, R2 0.4389668405056\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 664, loss 1.149399995803833, R2 0.415096640586853\n",
      "Eval loss 1.2016410827636719, R2 0.43962448835372925\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 665, loss 1.148353099822998, R2 0.4158270061016083\n",
      "Eval loss 1.2005634307861328, R2 0.4402802288532257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 666, loss 1.1473097801208496, R2 0.4165550470352173\n",
      "Eval loss 1.1994891166687012, R2 0.44093388319015503\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 667, loss 1.1462701559066772, R2 0.41728079319000244\n",
      "Eval loss 1.1984187364578247, R2 0.4415854215621948\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 668, loss 1.1452339887619019, R2 0.41800424456596375\n",
      "Eval loss 1.1973516941070557, R2 0.4422350823879242\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 669, loss 1.144201397895813, R2 0.4187254011631012\n",
      "Eval loss 1.1962882280349731, R2 0.4428827166557312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 670, loss 1.1431723833084106, R2 0.41944435238838196\n",
      "Eval loss 1.1952284574508667, R2 0.4435284435749054\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 671, loss 1.1421468257904053, R2 0.4201609790325165\n",
      "Eval loss 1.1941719055175781, R2 0.4441721737384796\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 672, loss 1.1411248445510864, R2 0.4208753705024719\n",
      "Eval loss 1.1931190490722656, R2 0.4448138475418091\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 673, loss 1.1401063203811646, R2 0.42158758640289307\n",
      "Eval loss 1.1920697689056396, R2 0.4454536736011505\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 674, loss 1.1390912532806396, R2 0.4222974479198456\n",
      "Eval loss 1.1910239458084106, R2 0.4460914433002472\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 675, loss 1.1380796432495117, R2 0.4230051040649414\n",
      "Eval loss 1.1899815797805786, R2 0.4467273950576782\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 676, loss 1.1370714902877808, R2 0.42371055483818054\n",
      "Eval loss 1.1889426708221436, R2 0.4473612606525421\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 677, loss 1.1360667943954468, R2 0.42441385984420776\n",
      "Eval loss 1.187907099723816, R2 0.44799336791038513\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 678, loss 1.1350655555725098, R2 0.42511484026908875\n",
      "Eval loss 1.1868749856948853, R2 0.44862326979637146\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 679, loss 1.1340676546096802, R2 0.425813764333725\n",
      "Eval loss 1.1858464479446411, R2 0.44925153255462646\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 680, loss 1.1330732107162476, R2 0.4265103042125702\n",
      "Eval loss 1.1848212480545044, R2 0.44987764954566956\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 681, loss 1.1320819854736328, R2 0.42720499634742737\n",
      "Eval loss 1.183799386024475, R2 0.45050203800201416\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 682, loss 1.1310943365097046, R2 0.427897185087204\n",
      "Eval loss 1.1827808618545532, R2 0.45112448930740356\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 683, loss 1.1301101446151733, R2 0.42858731746673584\n",
      "Eval loss 1.1817659139633179, R2 0.45174503326416016\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 684, loss 1.1291289329528809, R2 0.429275244474411\n",
      "Eval loss 1.1807540655136108, R2 0.45236361026763916\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 685, loss 1.128151297569275, R2 0.42996102571487427\n",
      "Eval loss 1.1797457933425903, R2 0.4529803395271301\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 686, loss 1.1271768808364868, R2 0.430644690990448\n",
      "Eval loss 1.1787407398223877, R2 0.45359528064727783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 687, loss 1.1262059211730957, R2 0.4313262701034546\n",
      "Eval loss 1.1777390241622925, R2 0.45420828461647034\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 688, loss 1.125238060951233, R2 0.4320056140422821\n",
      "Eval loss 1.1767406463623047, R2 0.4548195004463196\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 689, loss 1.124273657798767, R2 0.43268296122550964\n",
      "Eval loss 1.1757454872131348, R2 0.45542868971824646\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 690, loss 1.1233124732971191, R2 0.4333581030368805\n",
      "Eval loss 1.1747537851333618, R2 0.4560362994670868\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 691, loss 1.122354507446289, R2 0.4340311288833618\n",
      "Eval loss 1.1737650632858276, R2 0.4566417932510376\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 692, loss 1.1213997602462769, R2 0.4347021281719208\n",
      "Eval loss 1.1727797985076904, R2 0.4572456479072571\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 693, loss 1.120448350906372, R2 0.4353710114955902\n",
      "Eval loss 1.171797752380371, R2 0.4578475058078766\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 694, loss 1.1195001602172852, R2 0.43603771924972534\n",
      "Eval loss 1.1708189249038696, R2 0.4584476351737976\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 695, loss 1.1185550689697266, R2 0.4367026090621948\n",
      "Eval loss 1.169843316078186, R2 0.45904600620269775\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 696, loss 1.1176133155822754, R2 0.43736517429351807\n",
      "Eval loss 1.1688709259033203, R2 0.4596426784992218\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 697, loss 1.1166746616363525, R2 0.43802574276924133\n",
      "Eval loss 1.1679017543792725, R2 0.46023720502853394\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 698, loss 1.1157392263412476, R2 0.43868425488471985\n",
      "Eval loss 1.166935682296753, R2 0.46083009243011475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 699, loss 1.114806890487671, R2 0.4393407702445984\n",
      "Eval loss 1.1659728288650513, R2 0.4614212214946747\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 700, loss 1.1138776540756226, R2 0.4399951696395874\n",
      "Eval loss 1.1650131940841675, R2 0.46201056241989136\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 701, loss 1.1129517555236816, R2 0.44064757227897644\n",
      "Eval loss 1.1640567779541016, R2 0.4625980257987976\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 702, loss 1.11202871799469, R2 0.4412979781627655\n",
      "Eval loss 1.1631033420562744, R2 0.46318385004997253\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 703, loss 1.1111090183258057, R2 0.4419463872909546\n",
      "Eval loss 1.1621530055999756, R2 0.4637678861618042\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 704, loss 1.1101922988891602, R2 0.44259271025657654\n",
      "Eval loss 1.1612060070037842, R2 0.4643501043319702\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 705, loss 1.1092787981033325, R2 0.4432370364665985\n",
      "Eval loss 1.1602619886398315, R2 0.4649306535720825\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 706, loss 1.1083682775497437, R2 0.4438793957233429\n",
      "Eval loss 1.1593210697174072, R2 0.46550941467285156\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 707, loss 1.1074607372283936, R2 0.44451987743377686\n",
      "Eval loss 1.1583833694458008, R2 0.46608638763427734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 708, loss 1.1065564155578613, R2 0.4451582729816437\n",
      "Eval loss 1.1574485301971436, R2 0.466661661863327\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 709, loss 1.1056550741195679, R2 0.4457944631576538\n",
      "Eval loss 1.1565167903900146, R2 0.46723511815071106\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 710, loss 1.1047567129135132, R2 0.44642889499664307\n",
      "Eval loss 1.1555880308151245, R2 0.46780702471733093\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 711, loss 1.1038614511489868, R2 0.4470614492893219\n",
      "Eval loss 1.1546623706817627, R2 0.46837708353996277\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 712, loss 1.1029692888259888, R2 0.4476919174194336\n",
      "Eval loss 1.1537399291992188, R2 0.4689454138278961\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 713, loss 1.1020798683166504, R2 0.44832056760787964\n",
      "Eval loss 1.152820348739624, R2 0.4695121645927429\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 714, loss 1.1011936664581299, R2 0.44894716143608093\n",
      "Eval loss 1.151903748512268, R2 0.4700770676136017\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 715, loss 1.1003103256225586, R2 0.44957178831100464\n",
      "Eval loss 1.1509900093078613, R2 0.4706404507160187\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 716, loss 1.0994300842285156, R2 0.4501946270465851\n",
      "Eval loss 1.1500794887542725, R2 0.47120201587677\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 717, loss 1.0985527038574219, R2 0.4508154094219208\n",
      "Eval loss 1.1491719484329224, R2 0.47176188230514526\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 718, loss 1.0976781845092773, R2 0.45143434405326843\n",
      "Eval loss 1.1482672691345215, R2 0.4723201096057892\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 719, loss 1.0968066453933716, R2 0.45205143094062805\n",
      "Eval loss 1.1473654508590698, R2 0.472876638174057\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 720, loss 1.0959380865097046, R2 0.45266658067703247\n",
      "Eval loss 1.146466612815857, R2 0.47343170642852783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 721, loss 1.0950723886489868, R2 0.4532797634601593\n",
      "Eval loss 1.1455708742141724, R2 0.4739850163459778\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 722, loss 1.0942096710205078, R2 0.4538910686969757\n",
      "Eval loss 1.1446778774261475, R2 0.4745365083217621\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 723, loss 1.093349814414978, R2 0.45450055599212646\n",
      "Eval loss 1.1437878608703613, R2 0.47508639097213745\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 724, loss 1.0924928188323975, R2 0.45510825514793396\n",
      "Eval loss 1.1429007053375244, R2 0.47563469409942627\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 725, loss 1.0916386842727661, R2 0.45571383833885193\n",
      "Eval loss 1.1420164108276367, R2 0.4761814773082733\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 726, loss 1.0907872915267944, R2 0.4563176929950714\n",
      "Eval loss 1.1411350965499878, R2 0.47672635316848755\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 727, loss 1.0899388790130615, R2 0.45691967010498047\n",
      "Eval loss 1.1402565240859985, R2 0.47726985812187195\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 728, loss 1.0890933275222778, R2 0.4575198292732239\n",
      "Eval loss 1.1393808126449585, R2 0.4778115451335907\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 729, loss 1.0882506370544434, R2 0.4581182301044464\n",
      "Eval loss 1.1385079622268677, R2 0.47835177183151245\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 730, loss 1.087410569190979, R2 0.4587145745754242\n",
      "Eval loss 1.1376380920410156, R2 0.4788903295993805\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 731, loss 1.0865733623504639, R2 0.4593091905117035\n",
      "Eval loss 1.1367709636688232, R2 0.47942736744880676\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 732, loss 1.085739016532898, R2 0.4599020183086395\n",
      "Eval loss 1.135906457901001, R2 0.47996270656585693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 733, loss 1.0849075317382812, R2 0.4604930877685547\n",
      "Eval loss 1.1350449323654175, R2 0.4804965853691101\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 734, loss 1.0840786695480347, R2 0.46108219027519226\n",
      "Eval loss 1.1341862678527832, R2 0.4810287356376648\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 735, loss 1.0832524299621582, R2 0.46166956424713135\n",
      "Eval loss 1.133330225944519, R2 0.48155930638313293\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 736, loss 1.08242928981781, R2 0.46225520968437195\n",
      "Eval loss 1.1324769258499146, R2 0.4820883870124817\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 737, loss 1.0816086530685425, R2 0.4628390073776245\n",
      "Eval loss 1.1316266059875488, R2 0.4826159179210663\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 738, loss 1.0807908773422241, R2 0.4634209871292114\n",
      "Eval loss 1.1307787895202637, R2 0.48314177989959717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 739, loss 1.0799757242202759, R2 0.46400126814842224\n",
      "Eval loss 1.1299338340759277, R2 0.4836660623550415\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 740, loss 1.0791633129119873, R2 0.46457985043525696\n",
      "Eval loss 1.129091501235962, R2 0.48418882489204407\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 741, loss 1.0783535242080688, R2 0.4651564657688141\n",
      "Eval loss 1.1282521486282349, R2 0.48471009731292725\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 742, loss 1.0775467157363892, R2 0.4657314121723175\n",
      "Eval loss 1.1274151802062988, R2 0.4852297604084015\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 743, loss 1.0767422914505005, R2 0.46630457043647766\n",
      "Eval loss 1.1265811920166016, R2 0.4857478439807892\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 744, loss 1.075940728187561, R2 0.46687600016593933\n",
      "Eval loss 1.1257495880126953, R2 0.4862646758556366\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 745, loss 1.0751415491104126, R2 0.46744564175605774\n",
      "Eval loss 1.1249209642410278, R2 0.48677971959114075\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 746, loss 1.074345350265503, R2 0.46801358461380005\n",
      "Eval loss 1.124094843864441, R2 0.4872932434082031\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 747, loss 1.0735515356063843, R2 0.46857988834381104\n",
      "Eval loss 1.1232714653015137, R2 0.48780539631843567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 748, loss 1.0727604627609253, R2 0.469144344329834\n",
      "Eval loss 1.122450828552246, R2 0.4883159399032593\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 749, loss 1.071972131729126, R2 0.4697071313858032\n",
      "Eval loss 1.1216325759887695, R2 0.48882484436035156\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 750, loss 1.0711863040924072, R2 0.4702683389186859\n",
      "Eval loss 1.1208170652389526, R2 0.4893324673175812\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 751, loss 1.070402979850769, R2 0.47082754969596863\n",
      "Eval loss 1.1200042963027954, R2 0.48983851075172424\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 752, loss 1.069622278213501, R2 0.47138527035713196\n",
      "Eval loss 1.1191939115524292, R2 0.4903430938720703\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 753, loss 1.068844199180603, R2 0.471941202878952\n",
      "Eval loss 1.1183863878250122, R2 0.490846186876297\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 754, loss 1.0680687427520752, R2 0.47249552607536316\n",
      "Eval loss 1.1175813674926758, R2 0.49134770035743713\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 755, loss 1.067295789718628, R2 0.4730481207370758\n",
      "Eval loss 1.1167787313461304, R2 0.4918479025363922\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 756, loss 1.0665253400802612, R2 0.47359907627105713\n",
      "Eval loss 1.1159788370132446, R2 0.49234646558761597\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 757, loss 1.0657575130462646, R2 0.47414839267730713\n",
      "Eval loss 1.1151814460754395, R2 0.49284371733665466\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 758, loss 1.0649923086166382, R2 0.4746958911418915\n",
      "Eval loss 1.114386796951294, R2 0.49333932995796204\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 759, loss 1.0642294883728027, R2 0.47524183988571167\n",
      "Eval loss 1.11359441280365, R2 0.49383363127708435\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 760, loss 1.0634692907333374, R2 0.47578608989715576\n",
      "Eval loss 1.112804889678955, R2 0.49432647228240967\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 761, loss 1.0627115964889526, R2 0.4763287305831909\n",
      "Eval loss 1.1120176315307617, R2 0.49481773376464844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 762, loss 1.0619562864303589, R2 0.47686967253685\n",
      "Eval loss 1.1112329959869385, R2 0.4953076243400574\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 763, loss 1.0612034797668457, R2 0.4774090051651001\n",
      "Eval loss 1.1104509830474854, R2 0.49579620361328125\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 764, loss 1.060453176498413, R2 0.47794678807258606\n",
      "Eval loss 1.1096713542938232, R2 0.4962831735610962\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 765, loss 1.059705376625061, R2 0.4784829020500183\n",
      "Eval loss 1.1088942289352417, R2 0.49676886200904846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 766, loss 1.0589600801467896, R2 0.47901734709739685\n",
      "Eval loss 1.1081193685531616, R2 0.4972529709339142\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 767, loss 1.058217167854309, R2 0.47955024242401123\n",
      "Eval loss 1.1073472499847412, R2 0.49773576855659485\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 768, loss 1.0574767589569092, R2 0.48008158802986145\n",
      "Eval loss 1.1065776348114014, R2 0.4982171356678009\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 769, loss 1.0567386150360107, R2 0.480611115694046\n",
      "Eval loss 1.105810284614563, R2 0.49869710206985474\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 770, loss 1.0560029745101929, R2 0.4811391830444336\n",
      "Eval loss 1.1050454378128052, R2 0.4991757571697235\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 771, loss 1.0552698373794556, R2 0.4816657304763794\n",
      "Eval loss 1.1042829751968384, R2 0.4996528923511505\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 772, loss 1.0545390844345093, R2 0.4821905791759491\n",
      "Eval loss 1.1035231351852417, R2 0.5001286864280701\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 773, loss 1.053810715675354, R2 0.48271381855010986\n",
      "Eval loss 1.102765679359436, R2 0.5006031394004822\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 774, loss 1.0530847311019897, R2 0.48323556780815125\n",
      "Eval loss 1.1020104885101318, R2 0.5010759830474854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 775, loss 1.0523611307144165, R2 0.4837557077407837\n",
      "Eval loss 1.1012576818466187, R2 0.5015476942062378\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 776, loss 1.0516399145126343, R2 0.4842742681503296\n",
      "Eval loss 1.1005074977874756, R2 0.5020177960395813\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 777, loss 1.050921082496643, R2 0.4847913980484009\n",
      "Eval loss 1.0997594594955444, R2 0.5024867653846741\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 778, loss 1.0502045154571533, R2 0.48530682921409607\n",
      "Eval loss 1.0990140438079834, R2 0.5029542446136475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 779, loss 1.0494903326034546, R2 0.48582080006599426\n",
      "Eval loss 1.0982707738876343, R2 0.5034204721450806\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 780, loss 1.0487785339355469, R2 0.4863331615924835\n",
      "Eval loss 1.0975300073623657, R2 0.5038852095603943\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 781, loss 1.0480691194534302, R2 0.48684415221214294\n",
      "Eval loss 1.096791386604309, R2 0.5043486952781677\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 782, loss 1.0473618507385254, R2 0.48735329508781433\n",
      "Eval loss 1.096055269241333, R2 0.5048107504844666\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 783, loss 1.0466569662094116, R2 0.4878610670566559\n",
      "Eval loss 1.095321536064148, R2 0.5052714943885803\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 784, loss 1.0459543466567993, R2 0.4883672893047333\n",
      "Eval loss 1.0945900678634644, R2 0.5057308673858643\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 785, loss 1.0452542304992676, R2 0.48887205123901367\n",
      "Eval loss 1.0938608646392822, R2 0.5061889290809631\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 786, loss 1.0445561408996582, R2 0.4893752932548523\n",
      "Eval loss 1.0931340456008911, R2 0.5066457390785217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 787, loss 1.0438603162765503, R2 0.48987704515457153\n",
      "Eval loss 1.0924094915390015, R2 0.5071011781692505\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 788, loss 1.0431671142578125, R2 0.4903772175312042\n",
      "Eval loss 1.0916872024536133, R2 0.5075553059577942\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 789, loss 1.0424758195877075, R2 0.49087584018707275\n",
      "Eval loss 1.0909672975540161, R2 0.5080080032348633\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 790, loss 1.041786789894104, R2 0.4913732409477234\n",
      "Eval loss 1.0902496576309204, R2 0.5084595680236816\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 791, loss 1.041100263595581, R2 0.4918689429759979\n",
      "Eval loss 1.0895341634750366, R2 0.5089099407196045\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 792, loss 1.0404157638549805, R2 0.4923633337020874\n",
      "Eval loss 1.0888210535049438, R2 0.5093586444854736\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 793, loss 1.0397335290908813, R2 0.4928560256958008\n",
      "Eval loss 1.088110089302063, R2 0.509806215763092\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 794, loss 1.0390536785125732, R2 0.4933473467826843\n",
      "Eval loss 1.0874015092849731, R2 0.5102525353431702\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 795, loss 1.038375735282898, R2 0.49383723735809326\n",
      "Eval loss 1.0866949558258057, R2 0.5106974244117737\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 796, loss 1.0377001762390137, R2 0.4943256378173828\n",
      "Eval loss 1.0859907865524292, R2 0.5111412405967712\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 797, loss 1.0370267629623413, R2 0.4948125183582306\n",
      "Eval loss 1.0852887630462646, R2 0.511583685874939\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 798, loss 1.0363556146621704, R2 0.4952980875968933\n",
      "Eval loss 1.0845890045166016, R2 0.5120248198509216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 799, loss 1.0356866121292114, R2 0.49578219652175903\n",
      "Eval loss 1.08389151096344, R2 0.512464702129364\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 800, loss 1.0350197553634644, R2 0.49626466631889343\n",
      "Eval loss 1.0831960439682007, R2 0.5129032731056213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 801, loss 1.0343551635742188, R2 0.49674588441848755\n",
      "Eval loss 1.0825029611587524, R2 0.5133407115936279\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 802, loss 1.0336925983428955, R2 0.4972255527973175\n",
      "Eval loss 1.081811785697937, R2 0.5137767195701599\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 803, loss 1.0330321788787842, R2 0.4977039098739624\n",
      "Eval loss 1.0811231136322021, R2 0.5142114758491516\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 804, loss 1.0323739051818848, R2 0.4981808364391327\n",
      "Eval loss 1.0804364681243896, R2 0.5146450400352478\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 805, loss 1.0317178964614868, R2 0.49865636229515076\n",
      "Eval loss 1.079751968383789, R2 0.5150774717330933\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 806, loss 1.0310639142990112, R2 0.49913036823272705\n",
      "Eval loss 1.0790696144104004, R2 0.5155085921287537\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 807, loss 1.0304120779037476, R2 0.49960291385650635\n",
      "Eval loss 1.0783894062042236, R2 0.5159384608268738\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 808, loss 1.0297623872756958, R2 0.5000742077827454\n",
      "Eval loss 1.0777113437652588, R2 0.5163670778274536\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 809, loss 1.0291146039962769, R2 0.500544011592865\n",
      "Eval loss 1.0770353078842163, R2 0.5167945027351379\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 810, loss 1.0284690856933594, R2 0.5010125637054443\n",
      "Eval loss 1.0763616561889648, R2 0.5172206163406372\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 811, loss 1.0278255939483643, R2 0.5014796257019043\n",
      "Eval loss 1.0756899118423462, R2 0.517645537853241\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 812, loss 1.027184247970581, R2 0.5019452571868896\n",
      "Eval loss 1.07502019405365, R2 0.5180692076683044\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 813, loss 1.0265449285507202, R2 0.5024096369743347\n",
      "Eval loss 1.0743526220321655, R2 0.5184918642044067\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 814, loss 1.0259076356887817, R2 0.5028725266456604\n",
      "Eval loss 1.0736873149871826, R2 0.5189130902290344\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 815, loss 1.0252726078033447, R2 0.5033340454101562\n",
      "Eval loss 1.0730239152908325, R2 0.5193331241607666\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 816, loss 1.024639368057251, R2 0.503794252872467\n",
      "Eval loss 1.0723627805709839, R2 0.5197520852088928\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 817, loss 1.0240081548690796, R2 0.5042532086372375\n",
      "Eval loss 1.0717034339904785, R2 0.5201697945594788\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 818, loss 1.0233792066574097, R2 0.5047105550765991\n",
      "Eval loss 1.0710461139678955, R2 0.5205861926078796\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 819, loss 1.0227521657943726, R2 0.5051667094230652\n",
      "Eval loss 1.0703911781311035, R2 0.5210015177726746\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 820, loss 1.0221271514892578, R2 0.5056216716766357\n",
      "Eval loss 1.0697381496429443, R2 0.521415650844574\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 821, loss 1.0215040445327759, R2 0.5060749650001526\n",
      "Eval loss 1.069087028503418, R2 0.5218285918235779\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 822, loss 1.0208832025527954, R2 0.5065270662307739\n",
      "Eval loss 1.068438172340393, R2 0.522240400314331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 823, loss 1.0202641487121582, R2 0.5069778561592102\n",
      "Eval loss 1.067791223526001, R2 0.5226511359214783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 824, loss 1.0196470022201538, R2 0.5074272751808167\n",
      "Eval loss 1.0671461820602417, R2 0.5230603814125061\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 825, loss 1.0190321207046509, R2 0.5078754425048828\n",
      "Eval loss 1.0665034055709839, R2 0.5234687328338623\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 826, loss 1.0184190273284912, R2 0.5083222985267639\n",
      "Eval loss 1.0658624172210693, R2 0.5238757729530334\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 827, loss 1.017807960510254, R2 0.5087676644325256\n",
      "Eval loss 1.0652233362197876, R2 0.5242816805839539\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 828, loss 1.0171988010406494, R2 0.5092118978500366\n",
      "Eval loss 1.0645865201950073, R2 0.5246864557266235\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 829, loss 1.0165916681289673, R2 0.5096547603607178\n",
      "Eval loss 1.0639514923095703, R2 0.5250901579856873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 830, loss 1.0159863233566284, R2 0.5100963115692139\n",
      "Eval loss 1.0633184909820557, R2 0.5254926681518555\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 831, loss 1.0153831243515015, R2 0.5105367302894592\n",
      "Eval loss 1.0626873970031738, R2 0.5258939862251282\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 832, loss 1.0147818326950073, R2 0.51097571849823\n",
      "Eval loss 1.0620583295822144, R2 0.5262941718101501\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 833, loss 1.014182448387146, R2 0.5114132761955261\n",
      "Eval loss 1.0614312887191772, R2 0.5266932845115662\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 834, loss 1.013584852218628, R2 0.5118497014045715\n",
      "Eval loss 1.060806155204773, R2 0.5270911455154419\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 835, loss 1.0129894018173218, R2 0.5122848153114319\n",
      "Eval loss 1.0601829290390015, R2 0.5274879932403564\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 836, loss 1.0123956203460693, R2 0.5127186179161072\n",
      "Eval loss 1.0595614910125732, R2 0.5278835892677307\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 837, loss 1.0118037462234497, R2 0.513151228427887\n",
      "Eval loss 1.058942198753357, R2 0.5282782316207886\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 838, loss 1.0112138986587524, R2 0.5135825276374817\n",
      "Eval loss 1.0583246946334839, R2 0.5286715030670166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 839, loss 1.0106258392333984, R2 0.5140126347541809\n",
      "Eval loss 1.0577092170715332, R2 0.5290639400482178\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 840, loss 1.0100398063659668, R2 0.5144413709640503\n",
      "Eval loss 1.0570956468582153, R2 0.5294551849365234\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 841, loss 1.0094555616378784, R2 0.5148688554763794\n",
      "Eval loss 1.0564838647842407, R2 0.5298452377319336\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 842, loss 1.0088732242584229, R2 0.5152952671051025\n",
      "Eval loss 1.055873990058899, R2 0.5302342772483826\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 843, loss 1.008292555809021, R2 0.5157202482223511\n",
      "Eval loss 1.05526602268219, R2 0.5306221842765808\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 844, loss 1.0077139139175415, R2 0.5161439776420593\n",
      "Eval loss 1.0546598434448242, R2 0.5310090184211731\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 845, loss 1.0071372985839844, R2 0.5165665149688721\n",
      "Eval loss 1.0540558099746704, R2 0.5313947200775146\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 846, loss 1.0065621137619019, R2 0.5169878602027893\n",
      "Eval loss 1.0534533262252808, R2 0.5317793488502502\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 847, loss 1.0059889554977417, R2 0.5174080729484558\n",
      "Eval loss 1.0528528690338135, R2 0.5321628451347351\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 848, loss 1.0054177045822144, R2 0.5178268551826477\n",
      "Eval loss 1.052254319190979, R2 0.5325454473495483\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 849, loss 1.0048482418060303, R2 0.5182445049285889\n",
      "Eval loss 1.0516574382781982, R2 0.5329267978668213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 850, loss 1.0042804479599, R2 0.5186609625816345\n",
      "Eval loss 1.0510624647140503, R2 0.5333070755004883\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 851, loss 1.003714680671692, R2 0.5190761685371399\n",
      "Eval loss 1.0504692792892456, R2 0.5336863398551941\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 852, loss 1.0031505823135376, R2 0.519490122795105\n",
      "Eval loss 1.0498781204223633, R2 0.534064531326294\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 853, loss 1.0025882720947266, R2 0.5199029445648193\n",
      "Eval loss 1.0492885112762451, R2 0.5344415903091431\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 854, loss 1.0020277500152588, R2 0.5203145146369934\n",
      "Eval loss 1.0487008094787598, R2 0.5348177552223206\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 855, loss 1.0014688968658447, R2 0.5207250118255615\n",
      "Eval loss 1.0481150150299072, R2 0.5351926684379578\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 856, loss 1.0009119510650635, R2 0.5211343169212341\n",
      "Eval loss 1.0475311279296875, R2 0.5355666279792786\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 857, loss 1.000356912612915, R2 0.5215421915054321\n",
      "Eval loss 1.046948790550232, R2 0.5359394550323486\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 858, loss 0.9998034238815308, R2 0.521949052810669\n",
      "Eval loss 1.0463683605194092, R2 0.5363113880157471\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 859, loss 0.9992517828941345, R2 0.5223546624183655\n",
      "Eval loss 1.0457895994186401, R2 0.5366821885108948\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 860, loss 0.998701810836792, R2 0.522759199142456\n",
      "Eval loss 1.0452126264572144, R2 0.5370519161224365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 861, loss 0.998153567314148, R2 0.5231624841690063\n",
      "Eval loss 1.0446375608444214, R2 0.5374206900596619\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 862, loss 0.9976071715354919, R2 0.5235645771026611\n",
      "Eval loss 1.0440641641616821, R2 0.5377883315086365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 863, loss 0.9970625042915344, R2 0.52396559715271\n",
      "Eval loss 1.0434925556182861, R2 0.5381550192832947\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 864, loss 0.9965195059776306, R2 0.5243654251098633\n",
      "Eval loss 1.0429226160049438, R2 0.5385206937789917\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 865, loss 0.9959782361984253, R2 0.5247641205787659\n",
      "Eval loss 1.0423544645309448, R2 0.5388853549957275\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 866, loss 0.9954385757446289, R2 0.525161623954773\n",
      "Eval loss 1.0417882204055786, R2 0.5392488837242126\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 867, loss 0.9949008226394653, R2 0.5255579352378845\n",
      "Eval loss 1.041223406791687, R2 0.5396114587783813\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 868, loss 0.9943646788597107, R2 0.5259531736373901\n",
      "Eval loss 1.0406605005264282, R2 0.5399731397628784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 869, loss 0.9938302636146545, R2 0.5263471603393555\n",
      "Eval loss 1.0400993824005127, R2 0.5403336882591248\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 870, loss 0.9932973980903625, R2 0.5267400741577148\n",
      "Eval loss 1.0395398139953613, R2 0.5406932234764099\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 871, loss 0.992766261100769, R2 0.5271318554878235\n",
      "Eval loss 1.0389820337295532, R2 0.5410518050193787\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 872, loss 0.992236852645874, R2 0.5275225043296814\n",
      "Eval loss 1.0384259223937988, R2 0.5414093732833862\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 873, loss 0.9917091727256775, R2 0.5279120206832886\n",
      "Eval loss 1.0378715991973877, R2 0.5417658090591431\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 874, loss 0.9911830425262451, R2 0.5283005237579346\n",
      "Eval loss 1.0373189449310303, R2 0.5421214699745178\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 875, loss 0.9906586408615112, R2 0.5286877155303955\n",
      "Eval loss 1.036767840385437, R2 0.5424759984016418\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 876, loss 0.9901358485221863, R2 0.5290738940238953\n",
      "Eval loss 1.036218523979187, R2 0.542829692363739\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 877, loss 0.9896147847175598, R2 0.5294589400291443\n",
      "Eval loss 1.0356707572937012, R2 0.5431822538375854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 878, loss 0.989095151424408, R2 0.5298429131507874\n",
      "Eval loss 1.0351247787475586, R2 0.5435338616371155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 879, loss 0.9885774254798889, R2 0.5302257537841797\n",
      "Eval loss 1.0345805883407593, R2 0.5438845157623291\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 880, loss 0.988061249256134, R2 0.5306074023246765\n",
      "Eval loss 1.0340378284454346, R2 0.5442341566085815\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 881, loss 0.9875466227531433, R2 0.5309880971908569\n",
      "Eval loss 1.0334968566894531, R2 0.5445829033851624\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 882, loss 0.9870335459709167, R2 0.5313676595687866\n",
      "Eval loss 1.0329574346542358, R2 0.544930636882782\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 883, loss 0.9865221977233887, R2 0.5317461490631104\n",
      "Eval loss 1.0324198007583618, R2 0.5452773571014404\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 884, loss 0.9860123991966248, R2 0.5321235656738281\n",
      "Eval loss 1.0318835973739624, R2 0.5456231832504272\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 885, loss 0.9855042695999146, R2 0.5324997305870056\n",
      "Eval loss 1.0313490629196167, R2 0.5459679961204529\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 886, loss 0.9849976897239685, R2 0.5328750014305115\n",
      "Eval loss 1.0308161973953247, R2 0.5463119149208069\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 887, loss 0.9844926595687866, R2 0.5332491397857666\n",
      "Eval loss 1.0302850008010864, R2 0.5466549396514893\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 888, loss 0.9839893579483032, R2 0.5336222052574158\n",
      "Eval loss 1.0297553539276123, R2 0.5469970107078552\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 889, loss 0.9834875464439392, R2 0.533994197845459\n",
      "Eval loss 1.0292272567749023, R2 0.5473379492759705\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 890, loss 0.9829872250556946, R2 0.5343651175498962\n",
      "Eval loss 1.028700828552246, R2 0.5476780533790588\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 891, loss 0.9824886322021484, R2 0.534734845161438\n",
      "Eval loss 1.028176188468933, R2 0.5480172634124756\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 892, loss 0.9819914698600769, R2 0.5351037383079529\n",
      "Eval loss 1.0276528596878052, R2 0.5483555197715759\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 893, loss 0.9814959168434143, R2 0.5354713797569275\n",
      "Eval loss 1.0271310806274414, R2 0.5486927628517151\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 894, loss 0.9810018539428711, R2 0.5358381867408752\n",
      "Eval loss 1.0266109704971313, R2 0.5490291118621826\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 895, loss 0.9805094003677368, R2 0.5362038016319275\n",
      "Eval loss 1.026092529296875, R2 0.5493645668029785\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 896, loss 0.9800184369087219, R2 0.5365684032440186\n",
      "Eval loss 1.0255755186080933, R2 0.5496991276741028\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 897, loss 0.9795289635658264, R2 0.5369319319725037\n",
      "Eval loss 1.0250601768493652, R2 0.5500326752662659\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 898, loss 0.9790412187576294, R2 0.5372944474220276\n",
      "Eval loss 1.0245463848114014, R2 0.5503653883934021\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 899, loss 0.9785547852516174, R2 0.5376558899879456\n",
      "Eval loss 1.024034023284912, R2 0.5506971478462219\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 900, loss 0.9780699610710144, R2 0.5380163192749023\n",
      "Eval loss 1.0235233306884766, R2 0.5510280132293701\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 901, loss 0.9775866270065308, R2 0.5383757948875427\n",
      "Eval loss 1.0230140686035156, R2 0.5513579845428467\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 902, loss 0.9771047830581665, R2 0.5387341976165771\n",
      "Eval loss 1.0225063562393188, R2 0.5516869425773621\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 903, loss 0.9766245484352112, R2 0.5390915274620056\n",
      "Eval loss 1.0220003128051758, R2 0.5520150661468506\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 904, loss 0.9761458039283752, R2 0.5394478440284729\n",
      "Eval loss 1.0214955806732178, R2 0.5523423552513123\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 905, loss 0.9756684899330139, R2 0.5398032069206238\n",
      "Eval loss 1.020992636680603, R2 0.552668571472168\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 906, loss 0.9751925468444824, R2 0.5401574969291687\n",
      "Eval loss 1.0204910039901733, R2 0.5529940128326416\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 907, loss 0.9747182130813599, R2 0.540510892868042\n",
      "Eval loss 1.0199909210205078, R2 0.5533185601234436\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 908, loss 0.9742453694343567, R2 0.5408632755279541\n",
      "Eval loss 1.0194923877716064, R2 0.5536422729492188\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 909, loss 0.9737738966941833, R2 0.5412145256996155\n",
      "Eval loss 1.0189952850341797, R2 0.5539650321006775\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 910, loss 0.9733039736747742, R2 0.5415648221969604\n",
      "Eval loss 1.0184996128082275, R2 0.5542868375778198\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 911, loss 0.9728354215621948, R2 0.5419140458106995\n",
      "Eval loss 1.018005609512329, R2 0.5546078085899353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 912, loss 0.9723684191703796, R2 0.542262613773346\n",
      "Eval loss 1.0175129175186157, R2 0.5549278855323792\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 913, loss 0.9719028472900391, R2 0.5426097512245178\n",
      "Eval loss 1.017021894454956, R2 0.5552471876144409\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 914, loss 0.9714387655258179, R2 0.5429561734199524\n",
      "Eval loss 1.0165321826934814, R2 0.5555654764175415\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 915, loss 0.9709761142730713, R2 0.543301522731781\n",
      "Eval loss 1.016044020652771, R2 0.5558831095695496\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 916, loss 0.9705148339271545, R2 0.543645977973938\n",
      "Eval loss 1.0155574083328247, R2 0.5561996102333069\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 917, loss 0.9700550436973572, R2 0.5439892411231995\n",
      "Eval loss 1.015071988105774, R2 0.5565154552459717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 918, loss 0.9695963859558105, R2 0.5443317890167236\n",
      "Eval loss 1.0145882368087769, R2 0.5568303465843201\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 919, loss 0.969139575958252, R2 0.5446732640266418\n",
      "Eval loss 1.0141059160232544, R2 0.5571443438529968\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 920, loss 0.9686839580535889, R2 0.5450137853622437\n",
      "Eval loss 1.0136250257492065, R2 0.5574576258659363\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 921, loss 0.9682297706604004, R2 0.545353353023529\n",
      "Eval loss 1.0131454467773438, R2 0.5577699542045593\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 922, loss 0.9677770137786865, R2 0.5456919074058533\n",
      "Eval loss 1.0126672983169556, R2 0.5580813884735107\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 923, loss 0.967325747013092, R2 0.5460295081138611\n",
      "Eval loss 1.0121906995773315, R2 0.5583919882774353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 924, loss 0.9668757319450378, R2 0.546366274356842\n",
      "Eval loss 1.0117154121398926, R2 0.5587018132209778\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 925, loss 0.9664270877838135, R2 0.546701967716217\n",
      "Eval loss 1.0112416744232178, R2 0.5590107440948486\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 926, loss 0.9659799337387085, R2 0.5470367670059204\n",
      "Eval loss 1.010769248008728, R2 0.5593188405036926\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 927, loss 0.9655342698097229, R2 0.5473704934120178\n",
      "Eval loss 1.010298252105713, R2 0.5596261620521545\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 928, loss 0.965089738368988, R2 0.5477033853530884\n",
      "Eval loss 1.0098286867141724, R2 0.5599325895309448\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 929, loss 0.9646466374397278, R2 0.5480353832244873\n",
      "Eval loss 1.009360671043396, R2 0.5602380633354187\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 930, loss 0.9642049670219421, R2 0.5483664274215698\n",
      "Eval loss 1.0088938474655151, R2 0.5605428814888\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 931, loss 0.9637646675109863, R2 0.5486965179443359\n",
      "Eval loss 1.0084284543991089, R2 0.5608468055725098\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 932, loss 0.9633256196975708, R2 0.5490255951881409\n",
      "Eval loss 1.0079643726348877, R2 0.5611498951911926\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 933, loss 0.9628881216049194, R2 0.549353837966919\n",
      "Eval loss 1.0075017213821411, R2 0.561452329158783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 934, loss 0.962451696395874, R2 0.5496811866760254\n",
      "Eval loss 1.0070405006408691, R2 0.5617536902427673\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 935, loss 0.9620168805122375, R2 0.5500075817108154\n",
      "Eval loss 1.0065807104110718, R2 0.562054455280304\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 936, loss 0.9615832567214966, R2 0.5503330230712891\n",
      "Eval loss 1.00612211227417, R2 0.562354326248169\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 937, loss 0.9611509442329407, R2 0.5506575703620911\n",
      "Eval loss 1.0056649446487427, R2 0.5626533627510071\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 938, loss 0.9607200026512146, R2 0.5509812831878662\n",
      "Eval loss 1.00520920753479, R2 0.5629515051841736\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 939, loss 0.9602904915809631, R2 0.551304042339325\n",
      "Eval loss 1.004754662513733, R2 0.5632489919662476\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 940, loss 0.9598621129989624, R2 0.5516257882118225\n",
      "Eval loss 1.00430166721344, R2 0.5635455250740051\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 941, loss 0.9594351649284363, R2 0.5519466996192932\n",
      "Eval loss 1.003849744796753, R2 0.5638413429260254\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 942, loss 0.95900958776474, R2 0.5522667169570923\n",
      "Eval loss 1.00339937210083, R2 0.5641363859176636\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 943, loss 0.9585850238800049, R2 0.5525858402252197\n",
      "Eval loss 1.0029504299163818, R2 0.5644306540489197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 944, loss 0.9581621289253235, R2 0.5529040694236755\n",
      "Eval loss 1.00250244140625, R2 0.5647239685058594\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 945, loss 0.9577402472496033, R2 0.5532214641571045\n",
      "Eval loss 1.0020561218261719, R2 0.5650166273117065\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 946, loss 0.9573197960853577, R2 0.553537905216217\n",
      "Eval loss 1.0016109943389893, R2 0.5653085112571716\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 947, loss 0.9569005966186523, R2 0.553853452205658\n",
      "Eval loss 1.0011672973632812, R2 0.5655995607376099\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 948, loss 0.9564827084541321, R2 0.5541682839393616\n",
      "Eval loss 1.0007246732711792, R2 0.5658897757530212\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 949, loss 0.9560661315917969, R2 0.5544819235801697\n",
      "Eval loss 1.0002835988998413, R2 0.5661793351173401\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 950, loss 0.9556507468223572, R2 0.5547948479652405\n",
      "Eval loss 0.9998436570167542, R2 0.5664680004119873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 951, loss 0.9552366733551025, R2 0.555107057094574\n",
      "Eval loss 0.9994052052497864, R2 0.5667559504508972\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 952, loss 0.9548238515853882, R2 0.5554181337356567\n",
      "Eval loss 0.9989678859710693, R2 0.567043125629425\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 953, loss 0.9544122815132141, R2 0.5557284951210022\n",
      "Eval loss 0.9985317587852478, R2 0.5673295259475708\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 954, loss 0.9540019631385803, R2 0.5560379028320312\n",
      "Eval loss 0.9980971217155457, R2 0.5676150918006897\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 955, loss 0.9535930156707764, R2 0.5563465356826782\n",
      "Eval loss 0.9976636171340942, R2 0.5679000616073608\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 956, loss 0.9531852006912231, R2 0.5566542148590088\n",
      "Eval loss 0.9972315430641174, R2 0.5681841373443604\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 957, loss 0.9527787566184998, R2 0.5569611191749573\n",
      "Eval loss 0.9968006014823914, R2 0.5684674978256226\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 958, loss 0.9523733258247375, R2 0.5572671294212341\n",
      "Eval loss 0.9963709115982056, R2 0.5687501430511475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 959, loss 0.9519692659378052, R2 0.5575723052024841\n",
      "Eval loss 0.9959425330162048, R2 0.5690318942070007\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 960, loss 0.9515663981437683, R2 0.5578765273094177\n",
      "Eval loss 0.9955155253410339, R2 0.5693129897117615\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 961, loss 0.9511648416519165, R2 0.5581800937652588\n",
      "Eval loss 0.9950896501541138, R2 0.5695931911468506\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 962, loss 0.9507644772529602, R2 0.5584827065467834\n",
      "Eval loss 0.9946651458740234, R2 0.5698727965354919\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 963, loss 0.9503652453422546, R2 0.5587844848632812\n",
      "Eval loss 0.9942417144775391, R2 0.5701515674591064\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 964, loss 0.9499673247337341, R2 0.559085488319397\n",
      "Eval loss 0.9938195943832397, R2 0.5704296827316284\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 965, loss 0.9495704770088196, R2 0.5593855381011963\n",
      "Eval loss 0.9933987855911255, R2 0.5707069635391235\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 966, loss 0.9491750001907349, R2 0.5596848130226135\n",
      "Eval loss 0.992979109287262, R2 0.5709834694862366\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 967, loss 0.9487807154655457, R2 0.5599831938743591\n",
      "Eval loss 0.9925607442855835, R2 0.5712594389915466\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 968, loss 0.948387622833252, R2 0.5602808594703674\n",
      "Eval loss 0.992143452167511, R2 0.5715346336364746\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 969, loss 0.9479955434799194, R2 0.5605776309967041\n",
      "Eval loss 0.9917274713516235, R2 0.5718088746070862\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 970, loss 0.947604775428772, R2 0.5608735680580139\n",
      "Eval loss 0.9913127422332764, R2 0.57208251953125\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 971, loss 0.9472152590751648, R2 0.5611687302589417\n",
      "Eval loss 0.9908992648124695, R2 0.5723554491996765\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 972, loss 0.9468268752098083, R2 0.5614631175994873\n",
      "Eval loss 0.9904870390892029, R2 0.5726276636123657\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 973, loss 0.9464396238327026, R2 0.5617565512657166\n",
      "Eval loss 0.9900758266448975, R2 0.5728991031646729\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 974, loss 0.9460536241531372, R2 0.5620493292808533\n",
      "Eval loss 0.9896660447120667, R2 0.5731698274612427\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 975, loss 0.9456688761711121, R2 0.5623412728309631\n",
      "Eval loss 0.9892570972442627, R2 0.5734398365020752\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 976, loss 0.9452850818634033, R2 0.5626322627067566\n",
      "Eval loss 0.9888495802879333, R2 0.5737091898918152\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 977, loss 0.9449024200439453, R2 0.5629225373268127\n",
      "Eval loss 0.9884433746337891, R2 0.5739777088165283\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 978, loss 0.9445211291313171, R2 0.563211977481842\n",
      "Eval loss 0.988038182258606, R2 0.5742456316947937\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 979, loss 0.9441407918930054, R2 0.5635006427764893\n",
      "Eval loss 0.9876342415809631, R2 0.5745128393173218\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 980, loss 0.9437618255615234, R2 0.563788652420044\n",
      "Eval loss 0.9872313737869263, R2 0.5747792720794678\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 981, loss 0.9433838129043579, R2 0.5640757083892822\n",
      "Eval loss 0.9868296980857849, R2 0.5750449299812317\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 982, loss 0.9430071115493774, R2 0.564362108707428\n",
      "Eval loss 0.9864293336868286, R2 0.5753099918365479\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 983, loss 0.9426313638687134, R2 0.5646474957466125\n",
      "Eval loss 0.9860299825668335, R2 0.5755743384361267\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 984, loss 0.9422568082809448, R2 0.5649322271347046\n",
      "Eval loss 0.9856319427490234, R2 0.575838029384613\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 985, loss 0.9418835043907166, R2 0.5652163028717041\n",
      "Eval loss 0.9852349162101746, R2 0.5761009454727173\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 986, loss 0.9415112137794495, R2 0.5654993653297424\n",
      "Eval loss 0.984839141368866, R2 0.5763631463050842\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 987, loss 0.9411399364471436, R2 0.5657817721366882\n",
      "Eval loss 0.9844444394111633, R2 0.5766248106956482\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 988, loss 0.9407699704170227, R2 0.5660633444786072\n",
      "Eval loss 0.9840509295463562, R2 0.5768856406211853\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 989, loss 0.9404010772705078, R2 0.5663442015647888\n",
      "Eval loss 0.9836585521697998, R2 0.5771458148956299\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 990, loss 0.9400331974029541, R2 0.5666241645812988\n",
      "Eval loss 0.9832672476768494, R2 0.5774052143096924\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 991, loss 0.9396665692329407, R2 0.5669034719467163\n",
      "Eval loss 0.9828771352767944, R2 0.5776639580726624\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 992, loss 0.939301073551178, R2 0.5671820044517517\n",
      "Eval loss 0.9824882745742798, R2 0.5779221057891846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 993, loss 0.9389365911483765, R2 0.5674598217010498\n",
      "Eval loss 0.9821003079414368, R2 0.5781795382499695\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 994, loss 0.9385730624198914, R2 0.5677366852760315\n",
      "Eval loss 0.981713593006134, R2 0.5784364342689514\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 995, loss 0.9382108449935913, R2 0.5680129528045654\n",
      "Eval loss 0.9813281297683716, R2 0.5786924958229065\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 996, loss 0.9378495812416077, R2 0.5682884454727173\n",
      "Eval loss 0.9809435606002808, R2 0.5789478421211243\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 997, loss 0.9374895095825195, R2 0.5685632228851318\n",
      "Eval loss 0.9805601835250854, R2 0.5792025327682495\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 998, loss 0.9371305108070374, R2 0.5688371062278748\n",
      "Eval loss 0.9801778197288513, R2 0.5794565677642822\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 999, loss 0.9367724657058716, R2 0.5691103339195251\n",
      "Eval loss 0.9797967076301575, R2 0.5797099471092224\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1000, loss 0.9364154934883118, R2 0.569382905960083\n",
      "Eval loss 0.9794166088104248, R2 0.5799627304077148\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1001, loss 0.9360597729682922, R2 0.5696545839309692\n",
      "Eval loss 0.9790376424789429, R2 0.5802147388458252\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1002, loss 0.9357050061225891, R2 0.5699255466461182\n",
      "Eval loss 0.9786598086357117, R2 0.5804661512374878\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1003, loss 0.9353511929512024, R2 0.5701957941055298\n",
      "Eval loss 0.9782829284667969, R2 0.5807169079780579\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1004, loss 0.9349985718727112, R2 0.5704653263092041\n",
      "Eval loss 0.9779072999954224, R2 0.5809669494628906\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1005, loss 0.9346470236778259, R2 0.5707340240478516\n",
      "Eval loss 0.977532684803009, R2 0.5812164545059204\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1006, loss 0.9342965483665466, R2 0.5710020065307617\n",
      "Eval loss 0.9771592020988464, R2 0.5814651846885681\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1007, loss 0.9339470267295837, R2 0.5712693929672241\n",
      "Eval loss 0.976786732673645, R2 0.5817132592201233\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1008, loss 0.933598518371582, R2 0.5715360045433044\n",
      "Eval loss 0.9764152765274048, R2 0.5819607973098755\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1009, loss 0.9332512617111206, R2 0.5718017220497131\n",
      "Eval loss 0.9760450124740601, R2 0.5822076201438904\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1010, loss 0.932904839515686, R2 0.5720668435096741\n",
      "Eval loss 0.9756757020950317, R2 0.5824537873268127\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1011, loss 0.9325594902038574, R2 0.5723312497138977\n",
      "Eval loss 0.9753075242042542, R2 0.5826992988586426\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1012, loss 0.9322152137756348, R2 0.5725950002670288\n",
      "Eval loss 0.9749403595924377, R2 0.5829441547393799\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1013, loss 0.9318720102310181, R2 0.5728579163551331\n",
      "Eval loss 0.9745743870735168, R2 0.5831884145736694\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1014, loss 0.9315296411514282, R2 0.5731201767921448\n",
      "Eval loss 0.9742093682289124, R2 0.5834319591522217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1015, loss 0.9311884641647339, R2 0.5733817219734192\n",
      "Eval loss 0.9738451838493347, R2 0.583674967288971\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1016, loss 0.9308481812477112, R2 0.5736425518989563\n",
      "Eval loss 0.9734822511672974, R2 0.5839173197746277\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1017, loss 0.9305090308189392, R2 0.5739026665687561\n",
      "Eval loss 0.973120391368866, R2 0.5841589570045471\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1018, loss 0.9301708936691284, R2 0.5741620063781738\n",
      "Eval loss 0.9727596044540405, R2 0.5844001173973083\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1019, loss 0.929833710193634, R2 0.574420690536499\n",
      "Eval loss 0.9723996520042419, R2 0.5846405029296875\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1020, loss 0.9294975399971008, R2 0.5746787190437317\n",
      "Eval loss 0.9720408320426941, R2 0.5848802924156189\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1021, loss 0.9291623830795288, R2 0.574936032295227\n",
      "Eval loss 0.9716830849647522, R2 0.5851194858551025\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1022, loss 0.928828239440918, R2 0.5751926302909851\n",
      "Eval loss 0.9713263511657715, R2 0.5853580832481384\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1023, loss 0.9284950494766235, R2 0.5754485726356506\n",
      "Eval loss 0.970970630645752, R2 0.5855960249900818\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1024, loss 0.9281628131866455, R2 0.5757037401199341\n",
      "Eval loss 0.9706159234046936, R2 0.5858333110809326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1025, loss 0.9278314709663391, R2 0.5759581923484802\n",
      "Eval loss 0.9702622294425964, R2 0.5860700011253357\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1026, loss 0.9275013208389282, R2 0.5762121081352234\n",
      "Eval loss 0.9699094295501709, R2 0.5863061547279358\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1027, loss 0.927172064781189, R2 0.5764651894569397\n",
      "Eval loss 0.9695578813552856, R2 0.5865415930747986\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1028, loss 0.9268439412117004, R2 0.5767176151275635\n",
      "Eval loss 0.969207227230072, R2 0.5867764353752136\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1029, loss 0.926516592502594, R2 0.5769693851470947\n",
      "Eval loss 0.9688575267791748, R2 0.5870107412338257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1030, loss 0.9261903166770935, R2 0.5772204995155334\n",
      "Eval loss 0.968508780002594, R2 0.58724445104599\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1031, loss 0.9258648753166199, R2 0.5774710178375244\n",
      "Eval loss 0.9681610465049744, R2 0.587477445602417\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1032, loss 0.925540566444397, R2 0.5777207016944885\n",
      "Eval loss 0.9678144454956055, R2 0.5877098441123962\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1033, loss 0.9252171516418457, R2 0.5779697299003601\n",
      "Eval loss 0.9674687385559082, R2 0.5879417061805725\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1034, loss 0.9248946905136108, R2 0.5782181024551392\n",
      "Eval loss 0.9671241044998169, R2 0.588172972202301\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1035, loss 0.9245731830596924, R2 0.5784658193588257\n",
      "Eval loss 0.9667801856994629, R2 0.588403582572937\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1036, loss 0.9242525696754456, R2 0.5787128210067749\n",
      "Eval loss 0.9664373993873596, R2 0.5886335968971252\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1037, loss 0.9239329099655151, R2 0.5789592862129211\n",
      "Eval loss 0.9660956263542175, R2 0.5888631343841553\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1038, loss 0.9236143231391907, R2 0.5792049765586853\n",
      "Eval loss 0.9657548069953918, R2 0.589091956615448\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1039, loss 0.9232965111732483, R2 0.5794500112533569\n",
      "Eval loss 0.9654149413108826, R2 0.5893202424049377\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1040, loss 0.9229798316955566, R2 0.5796944499015808\n",
      "Eval loss 0.9650760889053345, R2 0.589547872543335\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1041, loss 0.9226638674736023, R2 0.5799382328987122\n",
      "Eval loss 0.964738130569458, R2 0.5897749066352844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1042, loss 0.9223489165306091, R2 0.5801812410354614\n",
      "Eval loss 0.9644011855125427, R2 0.5900014042854309\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1043, loss 0.9220349788665771, R2 0.5804235935211182\n",
      "Eval loss 0.9640651345252991, R2 0.5902273058891296\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1044, loss 0.9217219352722168, R2 0.5806655287742615\n",
      "Eval loss 0.9637300968170166, R2 0.5904526710510254\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1045, loss 0.9214097261428833, R2 0.5809066295623779\n",
      "Eval loss 0.9633959531784058, R2 0.5906774997711182\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1046, loss 0.921098530292511, R2 0.5811470746994019\n",
      "Eval loss 0.9630628824234009, R2 0.5909015536308289\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1047, loss 0.9207881689071655, R2 0.581386923789978\n",
      "Eval loss 0.9627304673194885, R2 0.5911251902580261\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1048, loss 0.920478880405426, R2 0.5816261172294617\n",
      "Eval loss 0.9623993039131165, R2 0.5913482308387756\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1049, loss 0.9201703667640686, R2 0.581864595413208\n",
      "Eval loss 0.9620689749717712, R2 0.5915706157684326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1050, loss 0.9198627471923828, R2 0.5821025371551514\n",
      "Eval loss 0.9617395401000977, R2 0.5917924642562866\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1051, loss 0.9195560812950134, R2 0.5823398232460022\n",
      "Eval loss 0.9614110589027405, R2 0.5920137763023376\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1052, loss 0.9192503690719604, R2 0.5825764536857605\n",
      "Eval loss 0.9610834717750549, R2 0.5922344923019409\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1053, loss 0.9189454913139343, R2 0.5828124284744263\n",
      "Eval loss 0.9607567191123962, R2 0.592454731464386\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1054, loss 0.9186414480209351, R2 0.5830478668212891\n",
      "Eval loss 0.9604310989379883, R2 0.5926742553710938\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1055, loss 0.9183383584022522, R2 0.5832825303077698\n",
      "Eval loss 0.960106372833252, R2 0.5928933024406433\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1056, loss 0.9180362224578857, R2 0.5835166573524475\n",
      "Eval loss 0.9597824215888977, R2 0.5931117534637451\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1057, loss 0.9177348017692566, R2 0.5837501883506775\n",
      "Eval loss 0.9594594240188599, R2 0.593329668045044\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1058, loss 0.9174344539642334, R2 0.5839830040931702\n",
      "Eval loss 0.9591372609138489, R2 0.5935470461845398\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1059, loss 0.9171347618103027, R2 0.5842152833938599\n",
      "Eval loss 0.9588162302970886, R2 0.5937637686729431\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1060, loss 0.916836142539978, R2 0.584446907043457\n",
      "Eval loss 0.9584959149360657, R2 0.5939801335334778\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1061, loss 0.9165383577346802, R2 0.5846779346466064\n",
      "Eval loss 0.9581766724586487, R2 0.5941957831382751\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1062, loss 0.916241466999054, R2 0.5849082469940186\n",
      "Eval loss 0.9578582048416138, R2 0.5944109559059143\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1063, loss 0.9159454107284546, R2 0.5851380825042725\n",
      "Eval loss 0.9575406312942505, R2 0.5946254730224609\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1064, loss 0.9156501889228821, R2 0.5853672623634338\n",
      "Eval loss 0.9572238922119141, R2 0.5948395133018494\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1065, loss 0.915355920791626, R2 0.5855956673622131\n",
      "Eval loss 0.9569082260131836, R2 0.59505295753479\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1066, loss 0.915062427520752, R2 0.5858235955238342\n",
      "Eval loss 0.9565933346748352, R2 0.5952659249305725\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1067, loss 0.9147698879241943, R2 0.5860509276390076\n",
      "Eval loss 0.9562792778015137, R2 0.595478355884552\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1068, loss 0.9144781827926636, R2 0.5862777233123779\n",
      "Eval loss 0.9559662938117981, R2 0.5956903100013733\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1069, loss 0.9141871333122253, R2 0.586503803730011\n",
      "Eval loss 0.955653965473175, R2 0.5959015488624573\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1070, loss 0.9138972163200378, R2 0.5867294073104858\n",
      "Eval loss 0.9553425312042236, R2 0.5961122512817383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1071, loss 0.9136079549789429, R2 0.5869542956352234\n",
      "Eval loss 0.9550319314002991, R2 0.5963225960731506\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1072, loss 0.9133195877075195, R2 0.5871785283088684\n",
      "Eval loss 0.9547224640846252, R2 0.5965322852134705\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1073, loss 0.9130321145057678, R2 0.5874022841453552\n",
      "Eval loss 0.9544137120246887, R2 0.5967414379119873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1074, loss 0.9127453565597534, R2 0.5876253843307495\n",
      "Eval loss 0.9541057348251343, R2 0.596950113773346\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1075, loss 0.9124594926834106, R2 0.5878479480743408\n",
      "Eval loss 0.9537988305091858, R2 0.5971581935882568\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1076, loss 0.9121745228767395, R2 0.5880699157714844\n",
      "Eval loss 0.9534925222396851, R2 0.5973657369613647\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1077, loss 0.9118902683258057, R2 0.5882912278175354\n",
      "Eval loss 0.9531872272491455, R2 0.5975728631019592\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1078, loss 0.9116070866584778, R2 0.5885120034217834\n",
      "Eval loss 0.952882707118988, R2 0.5977792739868164\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1079, loss 0.9113243818283081, R2 0.5887323021888733\n",
      "Eval loss 0.9525790810585022, R2 0.5979853272438049\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1080, loss 0.9110426902770996, R2 0.588951826095581\n",
      "Eval loss 0.9522764682769775, R2 0.5981908440589905\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1081, loss 0.910761833190918, R2 0.5891708731651306\n",
      "Eval loss 0.9519744515419006, R2 0.5983957648277283\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1082, loss 0.9104816913604736, R2 0.5893892645835876\n",
      "Eval loss 0.9516732692718506, R2 0.5986001491546631\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1083, loss 0.9102025628089905, R2 0.5896071791648865\n",
      "Eval loss 0.9513730406761169, R2 0.5988041162490845\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1084, loss 0.9099240303039551, R2 0.5898244976997375\n",
      "Eval loss 0.9510737061500549, R2 0.5990074872970581\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1085, loss 0.9096463918685913, R2 0.5900411605834961\n",
      "Eval loss 0.9507750272750854, R2 0.5992103815078735\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1086, loss 0.9093694686889648, R2 0.5902573466300964\n",
      "Eval loss 0.9504773020744324, R2 0.5994127988815308\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1087, loss 0.9090935587882996, R2 0.590472936630249\n",
      "Eval loss 0.9501804113388062, R2 0.599614679813385\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1088, loss 0.908818244934082, R2 0.5906878113746643\n",
      "Eval loss 0.9498842358589172, R2 0.5998159646987915\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1089, loss 0.9085438847541809, R2 0.5909023880958557\n",
      "Eval loss 0.9495889544487, R2 0.6000168323516846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1090, loss 0.9082701802253723, R2 0.5911161303520203\n",
      "Eval loss 0.9492945671081543, R2 0.6002171039581299\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1091, loss 0.9079973101615906, R2 0.5913294553756714\n",
      "Eval loss 0.949000895023346, R2 0.6004169583320618\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1092, loss 0.9077253341674805, R2 0.5915421843528748\n",
      "Eval loss 0.9487081170082092, R2 0.6006162166595459\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1093, loss 0.9074540138244629, R2 0.5917543768882751\n",
      "Eval loss 0.9484161138534546, R2 0.6008150577545166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1094, loss 0.9071834087371826, R2 0.5919660925865173\n",
      "Eval loss 0.9481248259544373, R2 0.6010133624076843\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1095, loss 0.9069138169288635, R2 0.5921770334243774\n",
      "Eval loss 0.9478344321250916, R2 0.6012111306190491\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1096, loss 0.906644880771637, R2 0.5923875570297241\n",
      "Eval loss 0.9475449323654175, R2 0.6014085412025452\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1097, loss 0.9063766598701477, R2 0.5925975441932678\n",
      "Eval loss 0.9472561478614807, R2 0.6016053557395935\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1098, loss 0.9061092734336853, R2 0.5928069353103638\n",
      "Eval loss 0.9469680190086365, R2 0.6018016934394836\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1099, loss 0.905842661857605, R2 0.593015730381012\n",
      "Eval loss 0.9466809034347534, R2 0.6019974946975708\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1100, loss 0.9055768847465515, R2 0.5932239890098572\n",
      "Eval loss 0.9463945031166077, R2 0.6021928191184998\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1101, loss 0.9053117632865906, R2 0.5934317708015442\n",
      "Eval loss 0.9461089372634888, R2 0.6023877859115601\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1102, loss 0.9050474762916565, R2 0.5936389565467834\n",
      "Eval loss 0.9458240270614624, R2 0.6025820374488831\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1103, loss 0.9047839045524597, R2 0.5938456654548645\n",
      "Eval loss 0.9455401301383972, R2 0.6027758717536926\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1104, loss 0.904521107673645, R2 0.5940517783164978\n",
      "Eval loss 0.9452568292617798, R2 0.6029692888259888\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1105, loss 0.9042590260505676, R2 0.5942574143409729\n",
      "Eval loss 0.944974422454834, R2 0.6031621694564819\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1106, loss 0.9039978981018066, R2 0.5944624543190002\n",
      "Eval loss 0.9446926712989807, R2 0.6033545732498169\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1107, loss 0.9037373065948486, R2 0.5946668982505798\n",
      "Eval loss 0.9444117546081543, R2 0.6035465598106384\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1108, loss 0.9034776091575623, R2 0.5948708653450012\n",
      "Eval loss 0.9441316723823547, R2 0.6037379503250122\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1109, loss 0.9032185673713684, R2 0.5950742363929749\n",
      "Eval loss 0.9438522458076477, R2 0.6039289236068726\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1110, loss 0.9029602408409119, R2 0.5952771902084351\n",
      "Eval loss 0.9435737133026123, R2 0.6041194200515747\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1111, loss 0.9027027487754822, R2 0.5954795479774475\n",
      "Eval loss 0.9432958960533142, R2 0.6043094396591187\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1112, loss 0.902445912361145, R2 0.595681369304657\n",
      "Eval loss 0.9430187940597534, R2 0.6044989228248596\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1113, loss 0.9021897912025452, R2 0.5958826541900635\n",
      "Eval loss 0.9427424669265747, R2 0.6046880483627319\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1114, loss 0.9019345045089722, R2 0.5960835814476013\n",
      "Eval loss 0.9424669742584229, R2 0.6048765778541565\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1115, loss 0.9016799330711365, R2 0.5962837338447571\n",
      "Eval loss 0.9421921372413635, R2 0.6050646305084229\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1116, loss 0.9014260768890381, R2 0.5964834690093994\n",
      "Eval loss 0.941918134689331, R2 0.6052523255348206\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1117, loss 0.901172935962677, R2 0.5966826677322388\n",
      "Eval loss 0.9416449069976807, R2 0.6054395437240601\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1118, loss 0.900920569896698, R2 0.5968814492225647\n",
      "Eval loss 0.9413723349571228, R2 0.6056261658668518\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1119, loss 0.9006688594818115, R2 0.5970795154571533\n",
      "Eval loss 0.9411006569862366, R2 0.6058124899864197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1120, loss 0.9004178047180176, R2 0.5972772836685181\n",
      "Eval loss 0.9408295750617981, R2 0.6059982776641846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1121, loss 0.9001675248146057, R2 0.5974743366241455\n",
      "Eval loss 0.9405592083930969, R2 0.6061835885047913\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1122, loss 0.8999180793762207, R2 0.5976709723472595\n",
      "Eval loss 0.9402897953987122, R2 0.6063684225082397\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1123, loss 0.8996692299842834, R2 0.5978670716285706\n",
      "Eval loss 0.9400208592414856, R2 0.60655277967453\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1124, loss 0.8994211554527283, R2 0.5980627536773682\n",
      "Eval loss 0.9397528171539307, R2 0.6067368388175964\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1125, loss 0.8991737365722656, R2 0.5982577800750732\n",
      "Eval loss 0.9394854307174683, R2 0.6069202423095703\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1126, loss 0.8989270329475403, R2 0.5984523892402649\n",
      "Eval loss 0.9392188191413879, R2 0.6071032881736755\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1127, loss 0.8986810445785522, R2 0.5986465215682983\n",
      "Eval loss 0.9389529228210449, R2 0.6072857975959778\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1128, loss 0.8984357118606567, R2 0.5988399982452393\n",
      "Eval loss 0.9386877417564392, R2 0.6074679493904114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1129, loss 0.8981911540031433, R2 0.5990331768989563\n",
      "Eval loss 0.9384233951568604, R2 0.6076496839523315\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1130, loss 0.8979472517967224, R2 0.5992258191108704\n",
      "Eval loss 0.9381596446037292, R2 0.6078307628631592\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1131, loss 0.8977041244506836, R2 0.5994178652763367\n",
      "Eval loss 0.9378966689109802, R2 0.6080114841461182\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1132, loss 0.8974615335464478, R2 0.599609375\n",
      "Eval loss 0.9376344084739685, R2 0.6081917881965637\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1133, loss 0.897219717502594, R2 0.5998004078865051\n",
      "Eval loss 0.9373728632926941, R2 0.6083717346191406\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1134, loss 0.8969786763191223, R2 0.5999910235404968\n",
      "Eval loss 0.937112033367157, R2 0.6085510849952698\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1135, loss 0.8967381715774536, R2 0.6001811623573303\n",
      "Eval loss 0.9368519186973572, R2 0.6087300777435303\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1136, loss 0.8964983820915222, R2 0.6003707051277161\n",
      "Eval loss 0.9365925192832947, R2 0.6089085936546326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1137, loss 0.8962593078613281, R2 0.6005598306655884\n",
      "Eval loss 0.9363337159156799, R2 0.6090866327285767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1138, loss 0.8960209488868713, R2 0.6007484197616577\n",
      "Eval loss 0.9360756278038025, R2 0.6092643141746521\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1139, loss 0.8957833051681519, R2 0.6009365320205688\n",
      "Eval loss 0.9358183741569519, R2 0.6094415187835693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1140, loss 0.8955461382865906, R2 0.6011242270469666\n",
      "Eval loss 0.9355617761611938, R2 0.6096182465553284\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1141, loss 0.8953098654747009, R2 0.6013113856315613\n",
      "Eval loss 0.9353059530258179, R2 0.6097946166992188\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1142, loss 0.8950742483139038, R2 0.601498007774353\n",
      "Eval loss 0.9350506663322449, R2 0.6099705100059509\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1143, loss 0.8948391079902649, R2 0.6016842126846313\n",
      "Eval loss 0.934796154499054, R2 0.6101459264755249\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1144, loss 0.8946048617362976, R2 0.6018700003623962\n",
      "Eval loss 0.9345422387123108, R2 0.6103209853172302\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1145, loss 0.8943710923194885, R2 0.6020551919937134\n",
      "Eval loss 0.9342891573905945, R2 0.6104955077171326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1146, loss 0.8941380381584167, R2 0.6022398471832275\n",
      "Eval loss 0.9340367317199707, R2 0.6106696724891663\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1147, loss 0.8939056992530823, R2 0.6024242043495178\n",
      "Eval loss 0.9337849020957947, R2 0.6108434796333313\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1148, loss 0.8936740159988403, R2 0.6026079654693604\n",
      "Eval loss 0.9335337281227112, R2 0.6110166907310486\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1149, loss 0.8934429883956909, R2 0.6027913093566895\n",
      "Eval loss 0.9332833886146545, R2 0.611189603805542\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1150, loss 0.8932125568389893, R2 0.6029741764068604\n",
      "Eval loss 0.9330337643623352, R2 0.6113620400428772\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1151, loss 0.8929827809333801, R2 0.6031565070152283\n",
      "Eval loss 0.9327846169471741, R2 0.6115341186523438\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1152, loss 0.8927536606788635, R2 0.6033384799957275\n",
      "Eval loss 0.932536244392395, R2 0.6117057204246521\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1153, loss 0.8925251960754395, R2 0.6035199165344238\n",
      "Eval loss 0.9322884678840637, R2 0.611876904964447\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1154, loss 0.8922973871231079, R2 0.6037008166313171\n",
      "Eval loss 0.932041347026825, R2 0.6120476126670837\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1155, loss 0.8920702338218689, R2 0.6038813591003418\n",
      "Eval loss 0.9317950010299683, R2 0.612217903137207\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1156, loss 0.8918437361717224, R2 0.6040614247322083\n",
      "Eval loss 0.9315493702888489, R2 0.6123878359794617\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1157, loss 0.8916178941726685, R2 0.6042410135269165\n",
      "Eval loss 0.9313041567802429, R2 0.6125574707984924\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1158, loss 0.8913925886154175, R2 0.6044202446937561\n",
      "Eval loss 0.9310598373413086, R2 0.6127264499664307\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1159, loss 0.8911679983139038, R2 0.6045988202095032\n",
      "Eval loss 0.9308160543441772, R2 0.612895131111145\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1160, loss 0.8909440040588379, R2 0.6047769784927368\n",
      "Eval loss 0.9305731654167175, R2 0.6130633354187012\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1161, loss 0.8907206058502197, R2 0.604954719543457\n",
      "Eval loss 0.9303306341171265, R2 0.6132312417030334\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1162, loss 0.8904978632926941, R2 0.6051320433616638\n",
      "Eval loss 0.9300888776779175, R2 0.6133986115455627\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1163, loss 0.8902758955955505, R2 0.6053089499473572\n",
      "Eval loss 0.9298477172851562, R2 0.6135656833648682\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1164, loss 0.8900544047355652, R2 0.6054853200912476\n",
      "Eval loss 0.9296073317527771, R2 0.6137323379516602\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1165, loss 0.8898334503173828, R2 0.6056612133979797\n",
      "Eval loss 0.9293675422668457, R2 0.6138983964920044\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1166, loss 0.8896132707595825, R2 0.6058366298675537\n",
      "Eval loss 0.9291281700134277, R2 0.6140642762184143\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1167, loss 0.8893937468528748, R2 0.606011688709259\n",
      "Eval loss 0.9288897514343262, R2 0.6142296195030212\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1168, loss 0.8891748785972595, R2 0.6061862707138062\n",
      "Eval loss 0.9286518096923828, R2 0.6143946051597595\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1169, loss 0.8889563083648682, R2 0.6063603758811951\n",
      "Eval loss 0.9284145832061768, R2 0.6145592331886292\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1170, loss 0.8887386322021484, R2 0.6065340638160706\n",
      "Eval loss 0.9281778931617737, R2 0.6147234439849854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1171, loss 0.8885214924812317, R2 0.6067073345184326\n",
      "Eval loss 0.9279420375823975, R2 0.6148872375488281\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1172, loss 0.8883049488067627, R2 0.6068801283836365\n",
      "Eval loss 0.9277066588401794, R2 0.6150506138801575\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1173, loss 0.888089120388031, R2 0.6070525050163269\n",
      "Eval loss 0.927471935749054, R2 0.6152135729789734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1174, loss 0.8878737688064575, R2 0.6072243452072144\n",
      "Eval loss 0.9272379279136658, R2 0.6153761148452759\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1175, loss 0.8876588940620422, R2 0.6073958873748779\n",
      "Eval loss 0.927004337310791, R2 0.6155383586883545\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1176, loss 0.8874448537826538, R2 0.6075668931007385\n",
      "Eval loss 0.9267714619636536, R2 0.6157001256942749\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1177, loss 0.8872313499450684, R2 0.6077375411987305\n",
      "Eval loss 0.9265392422676086, R2 0.6158615946769714\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1178, loss 0.8870184421539307, R2 0.607907772064209\n",
      "Eval loss 0.9263076782226562, R2 0.616022527217865\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1179, loss 0.8868061304092407, R2 0.6080775260925293\n",
      "Eval loss 0.9260767102241516, R2 0.6161832213401794\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1180, loss 0.8865944743156433, R2 0.6082468032836914\n",
      "Eval loss 0.9258463978767395, R2 0.6163434386253357\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1181, loss 0.8863832354545593, R2 0.6084156632423401\n",
      "Eval loss 0.9256166815757751, R2 0.6165031790733337\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1182, loss 0.8861727714538574, R2 0.6085840463638306\n",
      "Eval loss 0.925387442111969, R2 0.6166626811027527\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1183, loss 0.8859627842903137, R2 0.6087520718574524\n",
      "Eval loss 0.9251589775085449, R2 0.6168217062950134\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1184, loss 0.8857533931732178, R2 0.6089196801185608\n",
      "Eval loss 0.9249311089515686, R2 0.6169804334640503\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1185, loss 0.8855446577072144, R2 0.6090869307518005\n",
      "Eval loss 0.9247037172317505, R2 0.6171387434005737\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1186, loss 0.8853364586830139, R2 0.6092535853385925\n",
      "Eval loss 0.9244771003723145, R2 0.6172966361045837\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1187, loss 0.885128915309906, R2 0.6094199419021606\n",
      "Eval loss 0.9242509603500366, R2 0.6174541115760803\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1188, loss 0.8849218487739563, R2 0.6095858216285706\n",
      "Eval loss 0.9240254759788513, R2 0.6176112294197083\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1189, loss 0.8847153782844543, R2 0.609751284122467\n",
      "Eval loss 0.9238005876541138, R2 0.6177680492401123\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1190, loss 0.8845093846321106, R2 0.6099163293838501\n",
      "Eval loss 0.9235762357711792, R2 0.6179244518280029\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1191, loss 0.8843041658401489, R2 0.6100809574127197\n",
      "Eval loss 0.9233525991439819, R2 0.6180804371833801\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1192, loss 0.8840994238853455, R2 0.6102451682090759\n",
      "Eval loss 0.9231294393539429, R2 0.6182360053062439\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1193, loss 0.883895218372345, R2 0.6104090213775635\n",
      "Eval loss 0.9229069352149963, R2 0.6183913350105286\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1194, loss 0.8836916089057922, R2 0.610572338104248\n",
      "Eval loss 0.9226850271224976, R2 0.6185461282730103\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1195, loss 0.883488655090332, R2 0.6107353568077087\n",
      "Eval loss 0.9224637746810913, R2 0.6187005639076233\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1196, loss 0.8832861185073853, R2 0.6108978986740112\n",
      "Eval loss 0.922243058681488, R2 0.6188547611236572\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1197, loss 0.883084237575531, R2 0.6110600829124451\n",
      "Eval loss 0.9220228791236877, R2 0.619008481502533\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1198, loss 0.8828828930854797, R2 0.6112217903137207\n",
      "Eval loss 0.9218032956123352, R2 0.6191619634628296\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1199, loss 0.8826821446418762, R2 0.6113830208778381\n",
      "Eval loss 0.9215843677520752, R2 0.6193149089813232\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1200, loss 0.8824818134307861, R2 0.6115440726280212\n",
      "Eval loss 0.9213659167289734, R2 0.6194674968719482\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1201, loss 0.8822821974754333, R2 0.6117045879364014\n",
      "Eval loss 0.9211480617523193, R2 0.6196198463439941\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1202, loss 0.8820831179618835, R2 0.6118646860122681\n",
      "Eval loss 0.9209308624267578, R2 0.6197717785835266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1203, loss 0.8818845748901367, R2 0.6120243072509766\n",
      "Eval loss 0.9207141399383545, R2 0.6199232935905457\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1204, loss 0.8816865086555481, R2 0.6121836304664612\n",
      "Eval loss 0.9204980731010437, R2 0.6200745105743408\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1205, loss 0.8814889788627625, R2 0.6123425960540771\n",
      "Eval loss 0.9202824831008911, R2 0.6202253103256226\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1206, loss 0.8812921047210693, R2 0.6125010848045349\n",
      "Eval loss 0.9200674891471863, R2 0.6203756928443909\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1207, loss 0.8810957074165344, R2 0.6126590371131897\n",
      "Eval loss 0.919853150844574, R2 0.6205257773399353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1208, loss 0.8808998465538025, R2 0.6128168106079102\n",
      "Eval loss 0.9196394085884094, R2 0.6206755042076111\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1209, loss 0.8807046413421631, R2 0.6129741072654724\n",
      "Eval loss 0.9194260239601135, R2 0.6208248734474182\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1210, loss 0.8805098533630371, R2 0.6131309866905212\n",
      "Eval loss 0.9192134141921997, R2 0.6209738850593567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1211, loss 0.8803156018257141, R2 0.6132875084877014\n",
      "Eval loss 0.919001042842865, R2 0.6211225390434265\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1212, loss 0.8801220059394836, R2 0.6134437322616577\n",
      "Eval loss 0.9187895655632019, R2 0.6212708353996277\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1213, loss 0.8799288272857666, R2 0.6135993599891663\n",
      "Eval loss 0.9185785055160522, R2 0.6214187741279602\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1214, loss 0.8797362446784973, R2 0.6137548089027405\n",
      "Eval loss 0.9183681011199951, R2 0.6215663552284241\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1215, loss 0.879544198513031, R2 0.6139097213745117\n",
      "Eval loss 0.9181581735610962, R2 0.6217136383056641\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1216, loss 0.8793526291847229, R2 0.6140642166137695\n",
      "Eval loss 0.917948842048645, R2 0.6218605041503906\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1217, loss 0.8791615962982178, R2 0.6142183542251587\n",
      "Eval loss 0.917739987373352, R2 0.6220069527626038\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1218, loss 0.8789710402488708, R2 0.614372193813324\n",
      "Eval loss 0.9175317883491516, R2 0.6221531629562378\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1219, loss 0.878781259059906, R2 0.6145256161689758\n",
      "Eval loss 0.9173240661621094, R2 0.6222989559173584\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1220, loss 0.878591775894165, R2 0.6146786212921143\n",
      "Eval loss 0.9171168208122253, R2 0.6224445104598999\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1221, loss 0.8784028887748718, R2 0.6148312091827393\n",
      "Eval loss 0.9169102311134338, R2 0.622589647769928\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1222, loss 0.8782144784927368, R2 0.6149834990501404\n",
      "Eval loss 0.9167041182518005, R2 0.6227343082427979\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1223, loss 0.8780264854431152, R2 0.6151352524757385\n",
      "Eval loss 0.916498601436615, R2 0.6228787899017334\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1224, loss 0.8778391480445862, R2 0.6152867674827576\n",
      "Eval loss 0.9162936806678772, R2 0.6230229735374451\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1225, loss 0.8776523470878601, R2 0.615437924861908\n",
      "Eval loss 0.9160891175270081, R2 0.6231666207313538\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1226, loss 0.8774659037590027, R2 0.6155886054039001\n",
      "Eval loss 0.9158851504325867, R2 0.6233100891113281\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1227, loss 0.8772801756858826, R2 0.6157389283180237\n",
      "Eval loss 0.9156818389892578, R2 0.6234530806541443\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1228, loss 0.8770948052406311, R2 0.6158888936042786\n",
      "Eval loss 0.9154789447784424, R2 0.6235957741737366\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1229, loss 0.8769100904464722, R2 0.6160385012626648\n",
      "Eval loss 0.9152766466140747, R2 0.623738169670105\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1230, loss 0.8767257928848267, R2 0.6161876916885376\n",
      "Eval loss 0.9150747656822205, R2 0.6238803267478943\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1231, loss 0.8765419721603394, R2 0.6163365840911865\n",
      "Eval loss 0.9148734211921692, R2 0.6240219473838806\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1232, loss 0.876358687877655, R2 0.616485059261322\n",
      "Eval loss 0.9146727919578552, R2 0.6241632699966431\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1233, loss 0.8761759400367737, R2 0.6166331768035889\n",
      "Eval loss 0.9144724607467651, R2 0.6243043541908264\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1234, loss 0.8759936690330505, R2 0.6167808175086975\n",
      "Eval loss 0.9142727255821228, R2 0.6244450211524963\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1235, loss 0.8758119940757751, R2 0.6169282793998718\n",
      "Eval loss 0.9140735864639282, R2 0.6245854496955872\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1236, loss 0.8756306171417236, R2 0.6170752048492432\n",
      "Eval loss 0.9138749837875366, R2 0.6247254610061646\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1237, loss 0.8754498362541199, R2 0.6172218918800354\n",
      "Eval loss 0.9136767387390137, R2 0.6248651146888733\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1238, loss 0.8752695322036743, R2 0.6173681616783142\n",
      "Eval loss 0.9134791493415833, R2 0.6250044703483582\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1239, loss 0.8750898241996765, R2 0.6175140142440796\n",
      "Eval loss 0.913282036781311, R2 0.6251434683799744\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1240, loss 0.8749105334281921, R2 0.6176595687866211\n",
      "Eval loss 0.913085401058197, R2 0.6252822279930115\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1241, loss 0.8747316002845764, R2 0.617804765701294\n",
      "Eval loss 0.9128893613815308, R2 0.6254205703735352\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1242, loss 0.8745532631874084, R2 0.6179496049880981\n",
      "Eval loss 0.9126937389373779, R2 0.625558614730835\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1243, loss 0.8743754625320435, R2 0.6180940270423889\n",
      "Eval loss 0.9124987721443176, R2 0.6256963610649109\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1244, loss 0.8741981387138367, R2 0.618238091468811\n",
      "Eval loss 0.912304162979126, R2 0.6258337497711182\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1245, loss 0.8740212917327881, R2 0.6183818578720093\n",
      "Eval loss 0.9121100902557373, R2 0.6259708404541016\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1246, loss 0.8738448023796082, R2 0.6185252666473389\n",
      "Eval loss 0.9119165539741516, R2 0.6261075735092163\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1247, loss 0.8736690282821655, R2 0.618668258190155\n",
      "Eval loss 0.9117233753204346, R2 0.6262440085411072\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1248, loss 0.8734936118125916, R2 0.6188110113143921\n",
      "Eval loss 0.9115309119224548, R2 0.6263800263404846\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1249, loss 0.8733187317848206, R2 0.6189532279968262\n",
      "Eval loss 0.9113388061523438, R2 0.626515805721283\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1250, loss 0.8731441497802734, R2 0.6190952062606812\n",
      "Eval loss 0.9111472964286804, R2 0.6266512870788574\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1251, loss 0.8729701638221741, R2 0.6192368865013123\n",
      "Eval loss 0.9109563231468201, R2 0.6267863512039185\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1252, loss 0.8727967143058777, R2 0.6193780899047852\n",
      "Eval loss 0.9107656478881836, R2 0.6269212365150452\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1253, loss 0.8726235628128052, R2 0.6195189952850342\n",
      "Eval loss 0.9105755686759949, R2 0.6270557045936584\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1254, loss 0.8724510073661804, R2 0.6196594834327698\n",
      "Eval loss 0.9103860259056091, R2 0.6271898746490479\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1255, loss 0.8722789287567139, R2 0.6197997331619263\n",
      "Eval loss 0.9101969003677368, R2 0.6273237466812134\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1256, loss 0.8721072673797607, R2 0.6199396252632141\n",
      "Eval loss 0.9100083708763123, R2 0.6274572610855103\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1257, loss 0.8719360828399658, R2 0.6200791597366333\n",
      "Eval loss 0.9098203182220459, R2 0.627590537071228\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1258, loss 0.8717654347419739, R2 0.6202183365821838\n",
      "Eval loss 0.9096325635910034, R2 0.6277234554290771\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1259, loss 0.8715951442718506, R2 0.6203571557998657\n",
      "Eval loss 0.9094454646110535, R2 0.6278561353683472\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1260, loss 0.871425449848175, R2 0.6204955577850342\n",
      "Eval loss 0.9092589020729065, R2 0.627988338470459\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1261, loss 0.8712560534477234, R2 0.6206338405609131\n",
      "Eval loss 0.9090726375579834, R2 0.6281203627586365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1262, loss 0.8710871934890747, R2 0.620771586894989\n",
      "Eval loss 0.9088869094848633, R2 0.6282519698143005\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1263, loss 0.870918869972229, R2 0.6209090948104858\n",
      "Eval loss 0.9087017178535461, R2 0.6283833980560303\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1264, loss 0.870750904083252, R2 0.6210461258888245\n",
      "Eval loss 0.9085169434547424, R2 0.6285144686698914\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1265, loss 0.8705834746360779, R2 0.6211829781532288\n",
      "Eval loss 0.9083327651023865, R2 0.6286451816558838\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1266, loss 0.8704163432121277, R2 0.6213194131851196\n",
      "Eval loss 0.9081488847732544, R2 0.6287755966186523\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1267, loss 0.8702498078346252, R2 0.6214554309844971\n",
      "Eval loss 0.9079657196998596, R2 0.6289057731628418\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1268, loss 0.8700836300849915, R2 0.6215913891792297\n",
      "Eval loss 0.9077827334403992, R2 0.6290356516838074\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1269, loss 0.8699179887771606, R2 0.6217267513275146\n",
      "Eval loss 0.9076004028320312, R2 0.6291651129722595\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1270, loss 0.8697527647018433, R2 0.6218618750572205\n",
      "Eval loss 0.9074184894561768, R2 0.6292943358421326\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1271, loss 0.8695879578590393, R2 0.6219966411590576\n",
      "Eval loss 0.9072370529174805, R2 0.629423201084137\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1272, loss 0.8694235682487488, R2 0.6221311092376709\n",
      "Eval loss 0.9070560932159424, R2 0.6295518279075623\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1273, loss 0.869259774684906, R2 0.6222652196884155\n",
      "Eval loss 0.906875729560852, R2 0.6296802163124084\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1274, loss 0.8690962195396423, R2 0.6223989725112915\n",
      "Eval loss 0.9066956043243408, R2 0.6298081874847412\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1275, loss 0.8689332604408264, R2 0.6225324273109436\n",
      "Eval loss 0.9065159559249878, R2 0.6299359798431396\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1276, loss 0.8687705993652344, R2 0.6226655840873718\n",
      "Eval loss 0.9063369035720825, R2 0.6300632953643799\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1277, loss 0.8686085939407349, R2 0.6227983832359314\n",
      "Eval loss 0.9061582088470459, R2 0.6301904320716858\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1278, loss 0.8684468865394592, R2 0.6229309439659119\n",
      "Eval loss 0.9059799313545227, R2 0.6303172707557678\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1279, loss 0.8682855367660522, R2 0.6230630278587341\n",
      "Eval loss 0.905802309513092, R2 0.630443811416626\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1280, loss 0.8681248426437378, R2 0.6231947541236877\n",
      "Eval loss 0.9056249856948853, R2 0.6305699944496155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1281, loss 0.867964506149292, R2 0.6233263611793518\n",
      "Eval loss 0.9054481387138367, R2 0.6306958794593811\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1282, loss 0.8678044080734253, R2 0.6234574913978577\n",
      "Eval loss 0.9052717685699463, R2 0.6308216452598572\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1283, loss 0.8676449060440063, R2 0.6235883831977844\n",
      "Eval loss 0.9050958752632141, R2 0.630946934223175\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1284, loss 0.8674858212471008, R2 0.6237189769744873\n",
      "Eval loss 0.9049202799797058, R2 0.6310719847679138\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1285, loss 0.8673271536827087, R2 0.623849093914032\n",
      "Eval loss 0.9047452211380005, R2 0.6311967968940735\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1286, loss 0.8671688437461853, R2 0.6239790320396423\n",
      "Eval loss 0.9045706391334534, R2 0.6313211917877197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1287, loss 0.8670110702514648, R2 0.6241086721420288\n",
      "Eval loss 0.904396653175354, R2 0.6314454674720764\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1288, loss 0.8668537735939026, R2 0.6242379546165466\n",
      "Eval loss 0.904222846031189, R2 0.6315693259239197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1289, loss 0.8666967153549194, R2 0.624366819858551\n",
      "Eval loss 0.9040496349334717, R2 0.6316929459571838\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1290, loss 0.8665401339530945, R2 0.6244955062866211\n",
      "Eval loss 0.903876781463623, R2 0.6318163275718689\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1291, loss 0.8663840293884277, R2 0.6246237754821777\n",
      "Eval loss 0.9037044644355774, R2 0.6319392323493958\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1292, loss 0.8662283420562744, R2 0.6247518062591553\n",
      "Eval loss 0.9035325050354004, R2 0.6320620179176331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1293, loss 0.8660730719566345, R2 0.6248794794082642\n",
      "Eval loss 0.9033610820770264, R2 0.6321845054626465\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1294, loss 0.8659181594848633, R2 0.625006914138794\n",
      "Eval loss 0.9031899571418762, R2 0.6323066353797913\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1295, loss 0.8657636642456055, R2 0.6251339912414551\n",
      "Eval loss 0.903019368648529, R2 0.6324285864830017\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1296, loss 0.8656097054481506, R2 0.6252607703208923\n",
      "Eval loss 0.9028492569923401, R2 0.6325501799583435\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1297, loss 0.8654561638832092, R2 0.6253871917724609\n",
      "Eval loss 0.902679443359375, R2 0.632671594619751\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1298, loss 0.8653027415275574, R2 0.6255133152008057\n",
      "Eval loss 0.9025100469589233, R2 0.6327925324440002\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1299, loss 0.8651500344276428, R2 0.6256391406059265\n",
      "Eval loss 0.9023411870002747, R2 0.63291335105896\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1300, loss 0.8649976253509521, R2 0.6257646679878235\n",
      "Eval loss 0.9021726846694946, R2 0.6330339312553406\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1301, loss 0.8648455142974854, R2 0.6258899569511414\n",
      "Eval loss 0.9020047187805176, R2 0.633154034614563\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1302, loss 0.8646940588951111, R2 0.6260148882865906\n",
      "Eval loss 0.9018371105194092, R2 0.6332739591598511\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1303, loss 0.8645427823066711, R2 0.6261394023895264\n",
      "Eval loss 0.9016699194908142, R2 0.6333937048912048\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1304, loss 0.8643920421600342, R2 0.6262637972831726\n",
      "Eval loss 0.9015030860900879, R2 0.6335130333900452\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1305, loss 0.8642416596412659, R2 0.6263878345489502\n",
      "Eval loss 0.9013368487358093, R2 0.6336321234703064\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1306, loss 0.864091694355011, R2 0.6265115141868591\n",
      "Eval loss 0.9011707901954651, R2 0.6337509751319885\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1307, loss 0.8639420866966248, R2 0.626634955406189\n",
      "Eval loss 0.9010053277015686, R2 0.6338695287704468\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1308, loss 0.8637929558753967, R2 0.6267580389976501\n",
      "Eval loss 0.9008402228355408, R2 0.6339878439903259\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1309, loss 0.8636441826820374, R2 0.6268808841705322\n",
      "Eval loss 0.9006755948066711, R2 0.6341058611869812\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1310, loss 0.8634958267211914, R2 0.6270033121109009\n",
      "Eval loss 0.9005113840103149, R2 0.6342235803604126\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1311, loss 0.8633478283882141, R2 0.6271255612373352\n",
      "Eval loss 0.9003474712371826, R2 0.6343410611152649\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1312, loss 0.8632001876831055, R2 0.6272475123405457\n",
      "Eval loss 0.900184154510498, R2 0.6344582438468933\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1313, loss 0.8630530834197998, R2 0.6273691058158875\n",
      "Eval loss 0.9000211358070374, R2 0.6345751285552979\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1314, loss 0.862906277179718, R2 0.6274904012680054\n",
      "Eval loss 0.8998584747314453, R2 0.6346917748451233\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1315, loss 0.8627597689628601, R2 0.6276114583015442\n",
      "Eval loss 0.8996962904930115, R2 0.6348082423210144\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1316, loss 0.8626137375831604, R2 0.6277322173118591\n",
      "Eval loss 0.8995344638824463, R2 0.6349243521690369\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1317, loss 0.8624681234359741, R2 0.6278526782989502\n",
      "Eval loss 0.8993731141090393, R2 0.6350402235984802\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1318, loss 0.8623228073120117, R2 0.6279728412628174\n",
      "Eval loss 0.8992122411727905, R2 0.6351557374000549\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1319, loss 0.8621780872344971, R2 0.6280927062034607\n",
      "Eval loss 0.8990516662597656, R2 0.6352710723876953\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1320, loss 0.862033486366272, R2 0.6282122731208801\n",
      "Eval loss 0.8988915085792542, R2 0.6353861093521118\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1321, loss 0.8618893623352051, R2 0.6283316016197205\n",
      "Eval loss 0.8987318277359009, R2 0.635500967502594\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1322, loss 0.8617457151412964, R2 0.6284505724906921\n",
      "Eval loss 0.8985724449157715, R2 0.6356154680252075\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1323, loss 0.8616023659706116, R2 0.6285693049430847\n",
      "Eval loss 0.8984134197235107, R2 0.6357297301292419\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1324, loss 0.8614594340324402, R2 0.6286876797676086\n",
      "Eval loss 0.8982548713684082, R2 0.635843813419342\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1325, loss 0.8613168597221375, R2 0.628805935382843\n",
      "Eval loss 0.8980967402458191, R2 0.6359574794769287\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1326, loss 0.8611746430397034, R2 0.6289238333702087\n",
      "Eval loss 0.8979390263557434, R2 0.636070966720581\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1327, loss 0.8610328435897827, R2 0.629041314125061\n",
      "Eval loss 0.8977816104888916, R2 0.6361841559410095\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1328, loss 0.8608913421630859, R2 0.629158616065979\n",
      "Eval loss 0.897624671459198, R2 0.6362971067428589\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1329, loss 0.8607503771781921, R2 0.6292756795883179\n",
      "Eval loss 0.8974681496620178, R2 0.6364098191261292\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1330, loss 0.8606096506118774, R2 0.6293923854827881\n",
      "Eval loss 0.8973119854927063, R2 0.6365222930908203\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1331, loss 0.8604692816734314, R2 0.6295087933540344\n",
      "Eval loss 0.8971562385559082, R2 0.6366344690322876\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1332, loss 0.8603293895721436, R2 0.6296249032020569\n",
      "Eval loss 0.897000789642334, R2 0.6367464661598206\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1333, loss 0.8601897954940796, R2 0.6297408938407898\n",
      "Eval loss 0.896845817565918, R2 0.6368581056594849\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1334, loss 0.8600504994392395, R2 0.629856526851654\n",
      "Eval loss 0.8966911435127258, R2 0.6369695067405701\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1335, loss 0.8599117398262024, R2 0.6299717426300049\n",
      "Eval loss 0.8965369462966919, R2 0.637080729007721\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1336, loss 0.8597732782363892, R2 0.6300868391990662\n",
      "Eval loss 0.8963831067085266, R2 0.6371915936470032\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1337, loss 0.8596351146697998, R2 0.6302016377449036\n",
      "Eval loss 0.8962295651435852, R2 0.6373023390769958\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1338, loss 0.8594973087310791, R2 0.6303161382675171\n",
      "Eval loss 0.896076500415802, R2 0.6374126672744751\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1339, loss 0.8593599796295166, R2 0.6304304003715515\n",
      "Eval loss 0.8959238529205322, R2 0.6375228762626648\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1340, loss 0.8592230081558228, R2 0.6305443048477173\n",
      "Eval loss 0.8957715034484863, R2 0.6376327872276306\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1341, loss 0.8590862154960632, R2 0.630657970905304\n",
      "Eval loss 0.8956195712089539, R2 0.6377424597740173\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1342, loss 0.8589498996734619, R2 0.6307713985443115\n",
      "Eval loss 0.8954680562019348, R2 0.6378518342971802\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1343, loss 0.858814001083374, R2 0.63088458776474\n",
      "Eval loss 0.8953168392181396, R2 0.6379609704017639\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1344, loss 0.8586784601211548, R2 0.6309974193572998\n",
      "Eval loss 0.8951660394668579, R2 0.6380698680877686\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1345, loss 0.858543336391449, R2 0.6311100125312805\n",
      "Eval loss 0.8950157165527344, R2 0.6381785869598389\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1346, loss 0.8584083914756775, R2 0.6312223076820374\n",
      "Eval loss 0.8948655724525452, R2 0.6382870078086853\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1347, loss 0.8582738041877747, R2 0.6313343644142151\n",
      "Eval loss 0.8947159051895142, R2 0.6383951306343079\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1348, loss 0.8581397533416748, R2 0.631446123123169\n",
      "Eval loss 0.8945665955543518, R2 0.6385030150413513\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1349, loss 0.8580058813095093, R2 0.6315577030181885\n",
      "Eval loss 0.8944175839424133, R2 0.6386106610298157\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1350, loss 0.8578724265098572, R2 0.6316689848899841\n",
      "Eval loss 0.8942691683769226, R2 0.6387181878089905\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1351, loss 0.8577393293380737, R2 0.6317799091339111\n",
      "Eval loss 0.8941209316253662, R2 0.6388253569602966\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1352, loss 0.8576065301895142, R2 0.6318907141685486\n",
      "Eval loss 0.893973171710968, R2 0.6389322876930237\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1353, loss 0.8574740886688232, R2 0.6320011019706726\n",
      "Eval loss 0.8938256502151489, R2 0.6390389800071716\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1354, loss 0.8573420643806458, R2 0.6321113109588623\n",
      "Eval loss 0.893678605556488, R2 0.6391454935073853\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1355, loss 0.8572103381156921, R2 0.6322212219238281\n",
      "Eval loss 0.893531858921051, R2 0.639251708984375\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1356, loss 0.8570789098739624, R2 0.6323309540748596\n",
      "Eval loss 0.8933854699134827, R2 0.6393576264381409\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1357, loss 0.8569478988647461, R2 0.6324403285980225\n",
      "Eval loss 0.893239438533783, R2 0.6394634246826172\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1358, loss 0.8568173050880432, R2 0.6325494647026062\n",
      "Eval loss 0.8930938243865967, R2 0.6395689249038696\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1359, loss 0.8566869497299194, R2 0.6326583623886108\n",
      "Eval loss 0.8929485082626343, R2 0.639674186706543\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1360, loss 0.8565570116043091, R2 0.6327669620513916\n",
      "Eval loss 0.8928036093711853, R2 0.6397791504859924\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1361, loss 0.8564273715019226, R2 0.632875382900238\n",
      "Eval loss 0.892659068107605, R2 0.6398839950561523\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1362, loss 0.8562979698181152, R2 0.6329834461212158\n",
      "Eval loss 0.8925149440765381, R2 0.6399885416030884\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1363, loss 0.8561689257621765, R2 0.6330912709236145\n",
      "Eval loss 0.8923709988594055, R2 0.6400928497314453\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1364, loss 0.8560402989387512, R2 0.6331989765167236\n",
      "Eval loss 0.8922275304794312, R2 0.6401969194412231\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1365, loss 0.855911910533905, R2 0.6333062648773193\n",
      "Eval loss 0.8920843601226807, R2 0.6403008103370667\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1366, loss 0.8557841181755066, R2 0.6334133148193359\n",
      "Eval loss 0.8919416069984436, R2 0.6404042840003967\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1367, loss 0.8556563258171082, R2 0.6335201859474182\n",
      "Eval loss 0.8917992115020752, R2 0.6405077576637268\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1368, loss 0.8555291295051575, R2 0.6336267590522766\n",
      "Eval loss 0.8916571140289307, R2 0.6406108736991882\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1369, loss 0.8554021716117859, R2 0.6337330341339111\n",
      "Eval loss 0.8915154337882996, R2 0.6407137513160706\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1370, loss 0.8552753925323486, R2 0.6338391900062561\n",
      "Eval loss 0.8913739323616028, R2 0.6408164501190186\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1371, loss 0.8551491498947144, R2 0.6339449882507324\n",
      "Eval loss 0.891232967376709, R2 0.6409189701080322\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1372, loss 0.8550230860710144, R2 0.6340504884719849\n",
      "Eval loss 0.8910923004150391, R2 0.6410211324691772\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1373, loss 0.8548975586891174, R2 0.6341558694839478\n",
      "Eval loss 0.8909520506858826, R2 0.6411231756210327\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1374, loss 0.8547722101211548, R2 0.6342609524726868\n",
      "Eval loss 0.8908119797706604, R2 0.6412249207496643\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1375, loss 0.8546472191810608, R2 0.6343657970428467\n",
      "Eval loss 0.8906723260879517, R2 0.6413264870643616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1376, loss 0.8545225262641907, R2 0.6344703435897827\n",
      "Eval loss 0.8905330300331116, R2 0.6414278149604797\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1377, loss 0.854398250579834, R2 0.6345747113227844\n",
      "Eval loss 0.8903939723968506, R2 0.641528844833374\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1378, loss 0.8542741537094116, R2 0.6346787214279175\n",
      "Eval loss 0.8902555108070374, R2 0.6416297554969788\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1379, loss 0.8541504144668579, R2 0.6347825527191162\n",
      "Eval loss 0.8901171684265137, R2 0.6417303085327148\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1380, loss 0.8540269732475281, R2 0.6348861455917358\n",
      "Eval loss 0.8899792432785034, R2 0.6418306827545166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1381, loss 0.8539039492607117, R2 0.6349895000457764\n",
      "Eval loss 0.889841616153717, R2 0.6419309377670288\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1382, loss 0.8537812232971191, R2 0.635092556476593\n",
      "Eval loss 0.8897043466567993, R2 0.6420308351516724\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1383, loss 0.8536587357521057, R2 0.6351954936981201\n",
      "Eval loss 0.889567494392395, R2 0.6421304941177368\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1384, loss 0.8535367250442505, R2 0.6352980136871338\n",
      "Eval loss 0.8894307613372803, R2 0.6422300338745117\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1385, loss 0.8534148335456848, R2 0.6354004144668579\n",
      "Eval loss 0.8892945647239685, R2 0.6423293352127075\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1386, loss 0.8532934188842773, R2 0.6355025172233582\n",
      "Eval loss 0.8891587257385254, R2 0.6424283385276794\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1387, loss 0.853172242641449, R2 0.6356043219566345\n",
      "Eval loss 0.8890230655670166, R2 0.6425272226333618\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1388, loss 0.853051483631134, R2 0.6357061266899109\n",
      "Eval loss 0.8888877630233765, R2 0.6426258087158203\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1389, loss 0.8529308438301086, R2 0.635807454586029\n",
      "Eval loss 0.888752818107605, R2 0.6427242159843445\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1390, loss 0.8528107404708862, R2 0.6359086632728577\n",
      "Eval loss 0.8886182308197021, R2 0.6428223848342896\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1391, loss 0.8526907563209534, R2 0.6360095739364624\n",
      "Eval loss 0.888484001159668, R2 0.6429203748703003\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1392, loss 0.8525711894035339, R2 0.6361103057861328\n",
      "Eval loss 0.8883501291275024, R2 0.6430181264877319\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1393, loss 0.8524518609046936, R2 0.6362106800079346\n",
      "Eval loss 0.8882164359092712, R2 0.6431156396865845\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1394, loss 0.8523328900337219, R2 0.6363109350204468\n",
      "Eval loss 0.8880831599235535, R2 0.6432129740715027\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1395, loss 0.8522142171859741, R2 0.6364108920097351\n",
      "Eval loss 0.8879503011703491, R2 0.643310010433197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1396, loss 0.852095901966095, R2 0.6365106701850891\n",
      "Eval loss 0.8878175616264343, R2 0.6434068083763123\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1397, loss 0.8519778251647949, R2 0.636610209941864\n",
      "Eval loss 0.8876851797103882, R2 0.6435035467147827\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1398, loss 0.851859986782074, R2 0.636709451675415\n",
      "Eval loss 0.8875532746315002, R2 0.6436000466346741\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1399, loss 0.8517425656318665, R2 0.636808454990387\n",
      "Eval loss 0.8874215483665466, R2 0.6436961889266968\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1400, loss 0.851625382900238, R2 0.6369073390960693\n",
      "Eval loss 0.8872902393341064, R2 0.6437922120094299\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1401, loss 0.8515086770057678, R2 0.6370058655738831\n",
      "Eval loss 0.8871591687202454, R2 0.6438879370689392\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1402, loss 0.8513920307159424, R2 0.6371042728424072\n",
      "Eval loss 0.8870283961296082, R2 0.6439835429191589\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1403, loss 0.8512757420539856, R2 0.6372023224830627\n",
      "Eval loss 0.8868981003761292, R2 0.6440788507461548\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1404, loss 0.8511598110198975, R2 0.6373002529144287\n",
      "Eval loss 0.8867679238319397, R2 0.6441740393638611\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1405, loss 0.851044237613678, R2 0.637397825717926\n",
      "Eval loss 0.8866381645202637, R2 0.6442690491676331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1406, loss 0.850928783416748, R2 0.6374953389167786\n",
      "Eval loss 0.8865087628364563, R2 0.6443637609481812\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1407, loss 0.8508137464523315, R2 0.6375924944877625\n",
      "Eval loss 0.886379599571228, R2 0.6444583535194397\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1408, loss 0.8506990671157837, R2 0.6376895308494568\n",
      "Eval loss 0.886250913143158, R2 0.6445525884628296\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1409, loss 0.8505846261978149, R2 0.6377862691879272\n",
      "Eval loss 0.8861223459243774, R2 0.6446467041969299\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1410, loss 0.8504703640937805, R2 0.6378827095031738\n",
      "Eval loss 0.8859941363334656, R2 0.6447405219078064\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1411, loss 0.8503565192222595, R2 0.6379790902137756\n",
      "Eval loss 0.885866105556488, R2 0.6448342800140381\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1412, loss 0.8502429127693176, R2 0.6380751132965088\n",
      "Eval loss 0.8857386112213135, R2 0.6449277400970459\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1413, loss 0.8501296043395996, R2 0.6381710171699524\n",
      "Eval loss 0.885611355304718, R2 0.6450210213661194\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1414, loss 0.850016713142395, R2 0.6382666230201721\n",
      "Eval loss 0.8854843974113464, R2 0.6451140642166138\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1415, loss 0.84990394115448, R2 0.638361930847168\n",
      "Eval loss 0.8853577971458435, R2 0.6452069878578186\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1416, loss 0.8497915267944336, R2 0.638457179069519\n",
      "Eval loss 0.8852313756942749, R2 0.6452996134757996\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1417, loss 0.8496793508529663, R2 0.6385520696640015\n",
      "Eval loss 0.8851053714752197, R2 0.6453920006752014\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1418, loss 0.8495675325393677, R2 0.6386468410491943\n",
      "Eval loss 0.8849796056747437, R2 0.6454843282699585\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1419, loss 0.8494559526443481, R2 0.6387413740158081\n",
      "Eval loss 0.8848541975021362, R2 0.6455762982368469\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1420, loss 0.849344789981842, R2 0.6388356685638428\n",
      "Eval loss 0.8847290277481079, R2 0.6456681489944458\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1421, loss 0.8492336869239807, R2 0.6389297842979431\n",
      "Eval loss 0.8846042156219482, R2 0.6457597613334656\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1422, loss 0.8491230607032776, R2 0.6390235424041748\n",
      "Eval loss 0.8844797611236572, R2 0.6458513140678406\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1423, loss 0.8490126132965088, R2 0.6391171813011169\n",
      "Eval loss 0.8843554854393005, R2 0.6459424495697021\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1424, loss 0.8489025235176086, R2 0.6392106413841248\n",
      "Eval loss 0.8842315077781677, R2 0.646033525466919\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1425, loss 0.8487926125526428, R2 0.6393038034439087\n",
      "Eval loss 0.8841079473495483, R2 0.6461243629455566\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1426, loss 0.8486830592155457, R2 0.6393967866897583\n",
      "Eval loss 0.8839846253395081, R2 0.64621502161026\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1427, loss 0.8485738039016724, R2 0.6394895911216736\n",
      "Eval loss 0.8838616013526917, R2 0.6463053822517395\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1428, loss 0.8484647274017334, R2 0.639582097530365\n",
      "Eval loss 0.8837388753890991, R2 0.6463956236839294\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1429, loss 0.8483560085296631, R2 0.6396743655204773\n",
      "Eval loss 0.8836163878440857, R2 0.6464856266975403\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1430, loss 0.8482475876808167, R2 0.6397665143013\n",
      "Eval loss 0.8834941387176514, R2 0.6465755105018616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1431, loss 0.8481394052505493, R2 0.6398584842681885\n",
      "Eval loss 0.88337242603302, R2 0.6466651558876038\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1432, loss 0.8480314016342163, R2 0.639950156211853\n",
      "Eval loss 0.8832507133483887, R2 0.6467546224594116\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1433, loss 0.8479238152503967, R2 0.6400415897369385\n",
      "Eval loss 0.8831295371055603, R2 0.6468438506126404\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1434, loss 0.8478164672851562, R2 0.6401329040527344\n",
      "Eval loss 0.8830085396766663, R2 0.64693284034729\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1435, loss 0.8477094173431396, R2 0.6402239203453064\n",
      "Eval loss 0.8828878998756409, R2 0.6470217108726501\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1436, loss 0.8476026058197021, R2 0.6403148174285889\n",
      "Eval loss 0.882767379283905, R2 0.6471104025840759\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1437, loss 0.8474960923194885, R2 0.6404054164886475\n",
      "Eval loss 0.8826473951339722, R2 0.6471989154815674\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1438, loss 0.8473897576332092, R2 0.6404958367347717\n",
      "Eval loss 0.8825275301933289, R2 0.647287130355835\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1439, loss 0.8472837209701538, R2 0.6405860781669617\n",
      "Eval loss 0.8824080228805542, R2 0.6473752856254578\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1440, loss 0.8471781015396118, R2 0.6406760811805725\n",
      "Eval loss 0.8822886943817139, R2 0.6474630832672119\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1441, loss 0.8470726013183594, R2 0.6407657861709595\n",
      "Eval loss 0.8821697235107422, R2 0.6475507020950317\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1442, loss 0.8469675183296204, R2 0.6408554315567017\n",
      "Eval loss 0.8820511102676392, R2 0.6476382613182068\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1443, loss 0.8468624949455261, R2 0.6409447193145752\n",
      "Eval loss 0.8819327354431152, R2 0.6477255821228027\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1444, loss 0.8467578887939453, R2 0.6410340666770935\n",
      "Eval loss 0.8818146586418152, R2 0.6478126645088196\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1445, loss 0.8466535210609436, R2 0.6411228775978088\n",
      "Eval loss 0.8816967606544495, R2 0.6478995680809021\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1446, loss 0.8465493321418762, R2 0.641211748123169\n",
      "Eval loss 0.8815791606903076, R2 0.6479863524436951\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1447, loss 0.8464455008506775, R2 0.6413002610206604\n",
      "Eval loss 0.8814619183540344, R2 0.6480728387832642\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1448, loss 0.8463419079780579, R2 0.6413885354995728\n",
      "Eval loss 0.8813449740409851, R2 0.6481592059135437\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1449, loss 0.8462386131286621, R2 0.6414768099784851\n",
      "Eval loss 0.8812281489372253, R2 0.6482453942298889\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1450, loss 0.8461354970932007, R2 0.6415647268295288\n",
      "Eval loss 0.8811118602752686, R2 0.6483314037322998\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1451, loss 0.8460326790809631, R2 0.641652524471283\n",
      "Eval loss 0.8809956312179565, R2 0.648417055606842\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1452, loss 0.8459302186965942, R2 0.6417399644851685\n",
      "Eval loss 0.880879819393158, R2 0.648502767086029\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1453, loss 0.8458277583122253, R2 0.6418273448944092\n",
      "Eval loss 0.8807641267776489, R2 0.6485881805419922\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1454, loss 0.8457258343696594, R2 0.641914427280426\n",
      "Eval loss 0.8806487917900085, R2 0.6486734747886658\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1455, loss 0.8456239104270935, R2 0.6420013904571533\n",
      "Eval loss 0.880533754825592, R2 0.6487584710121155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1456, loss 0.8455225229263306, R2 0.6420881152153015\n",
      "Eval loss 0.8804190158843994, R2 0.6488433480262756\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1457, loss 0.8454212546348572, R2 0.6421746611595154\n",
      "Eval loss 0.8803045153617859, R2 0.6489279866218567\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1458, loss 0.8453202843666077, R2 0.6422609686851501\n",
      "Eval loss 0.8801901340484619, R2 0.6490125060081482\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1459, loss 0.8452194333076477, R2 0.6423470973968506\n",
      "Eval loss 0.8800761699676514, R2 0.6490968465805054\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1460, loss 0.8451189398765564, R2 0.6424330472946167\n",
      "Eval loss 0.8799625635147095, R2 0.6491809487342834\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1461, loss 0.8450188040733337, R2 0.6425187587738037\n",
      "Eval loss 0.8798491358757019, R2 0.6492648720741272\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1462, loss 0.8449187874794006, R2 0.6426043510437012\n",
      "Eval loss 0.8797359466552734, R2 0.6493486166000366\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1463, loss 0.8448190689086914, R2 0.6426896452903748\n",
      "Eval loss 0.8796230554580688, R2 0.6494321227073669\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1464, loss 0.8447195291519165, R2 0.6427748799324036\n",
      "Eval loss 0.8795104622840881, R2 0.6495156288146973\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1465, loss 0.844620406627655, R2 0.6428598165512085\n",
      "Eval loss 0.8793981075286865, R2 0.6495988368988037\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1466, loss 0.8445212841033936, R2 0.6429445743560791\n",
      "Eval loss 0.8792860507965088, R2 0.649681806564331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1467, loss 0.8444226980209351, R2 0.6430290937423706\n",
      "Eval loss 0.8791741728782654, R2 0.6497647166252136\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1468, loss 0.8443242311477661, R2 0.6431134939193726\n",
      "Eval loss 0.8790626525878906, R2 0.6498473882675171\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1469, loss 0.8442258238792419, R2 0.643197774887085\n",
      "Eval loss 0.878951370716095, R2 0.649929940700531\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1470, loss 0.8441279530525208, R2 0.6432816982269287\n",
      "Eval loss 0.8788403868675232, R2 0.650012195110321\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1471, loss 0.8440302014350891, R2 0.6433654427528381\n",
      "Eval loss 0.8787295818328857, R2 0.6500943303108215\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1472, loss 0.8439326882362366, R2 0.643449068069458\n",
      "Eval loss 0.8786191344261169, R2 0.6501762866973877\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1473, loss 0.8438354134559631, R2 0.6435325145721436\n",
      "Eval loss 0.8785088658332825, R2 0.6502581238746643\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1474, loss 0.8437384366989136, R2 0.6436156630516052\n",
      "Eval loss 0.8783988952636719, R2 0.650339663028717\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1475, loss 0.8436416983604431, R2 0.6436986923217773\n",
      "Eval loss 0.8782890439033508, R2 0.6504210829734802\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1476, loss 0.8435451984405518, R2 0.6437816619873047\n",
      "Eval loss 0.878179669380188, R2 0.6505023837089539\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1477, loss 0.8434488773345947, R2 0.6438642740249634\n",
      "Eval loss 0.8780704736709595, R2 0.6505834460258484\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1478, loss 0.8433527946472168, R2 0.643946647644043\n",
      "Eval loss 0.8779615759849548, R2 0.6506643891334534\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1479, loss 0.8432571291923523, R2 0.6440289616584778\n",
      "Eval loss 0.8778527975082397, R2 0.6507450938224792\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1480, loss 0.8431614637374878, R2 0.6441110968589783\n",
      "Eval loss 0.8777444362640381, R2 0.650825560092926\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1481, loss 0.8430662155151367, R2 0.6441929340362549\n",
      "Eval loss 0.877636194229126, R2 0.6509060263633728\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1482, loss 0.8429710865020752, R2 0.6442747116088867\n",
      "Eval loss 0.8775283098220825, R2 0.6509861350059509\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1483, loss 0.8428762555122375, R2 0.6443561911582947\n",
      "Eval loss 0.8774206042289734, R2 0.651066243648529\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1484, loss 0.8427817225456238, R2 0.6444375514984131\n",
      "Eval loss 0.8773131966590881, R2 0.6511460542678833\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1485, loss 0.8426873087882996, R2 0.6445186138153076\n",
      "Eval loss 0.8772059679031372, R2 0.6512258052825928\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1486, loss 0.8425931930541992, R2 0.6445996761322021\n",
      "Eval loss 0.8770990967750549, R2 0.6513052582740784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1487, loss 0.842499315738678, R2 0.6446804404258728\n",
      "Eval loss 0.8769923448562622, R2 0.6513845920562744\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1488, loss 0.8424056172370911, R2 0.6447610259056091\n",
      "Eval loss 0.8768860101699829, R2 0.6514638066291809\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1489, loss 0.8423123359680176, R2 0.6448414921760559\n",
      "Eval loss 0.8767798542976379, R2 0.6515427827835083\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1490, loss 0.8422191143035889, R2 0.6449217200279236\n",
      "Eval loss 0.8766738176345825, R2 0.6516215801239014\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1491, loss 0.8421260714530945, R2 0.6450017690658569\n",
      "Eval loss 0.8765683770179749, R2 0.6517003178596497\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1492, loss 0.8420333862304688, R2 0.6450815796852112\n",
      "Eval loss 0.8764628767967224, R2 0.6517788171768188\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1493, loss 0.8419408798217773, R2 0.6451612114906311\n",
      "Eval loss 0.8763576149940491, R2 0.6518571376800537\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1494, loss 0.841848611831665, R2 0.645240843296051\n",
      "Eval loss 0.8762526512145996, R2 0.6519352793693542\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1495, loss 0.8417565822601318, R2 0.6453201174736023\n",
      "Eval loss 0.8761481046676636, R2 0.6520132422447205\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1496, loss 0.8416647911071777, R2 0.6453992128372192\n",
      "Eval loss 0.8760436773300171, R2 0.6520911455154419\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1497, loss 0.8415732383728027, R2 0.6454782485961914\n",
      "Eval loss 0.8759394288063049, R2 0.6521687507629395\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1498, loss 0.8414818644523621, R2 0.6455569863319397\n",
      "Eval loss 0.8758354783058167, R2 0.6522462368011475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1499, loss 0.8413907885551453, R2 0.6456355452537537\n",
      "Eval loss 0.8757318258285522, R2 0.6523236036300659\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1500, loss 0.8412998914718628, R2 0.6457140445709229\n",
      "Eval loss 0.8756282925605774, R2 0.6524007320404053\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1501, loss 0.8412091732025146, R2 0.6457922458648682\n",
      "Eval loss 0.8755250573158264, R2 0.6524777412414551\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1502, loss 0.8411188125610352, R2 0.6458703875541687\n",
      "Eval loss 0.8754220604896545, R2 0.6525545120239258\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1503, loss 0.8410285711288452, R2 0.6459482312202454\n",
      "Eval loss 0.8753194212913513, R2 0.6526312232017517\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1504, loss 0.8409385681152344, R2 0.6460260152816772\n",
      "Eval loss 0.8752167820930481, R2 0.6527078151702881\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1505, loss 0.8408489227294922, R2 0.6461035013198853\n",
      "Eval loss 0.8751145005226135, R2 0.6527841091156006\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1506, loss 0.8407592177391052, R2 0.6461808681488037\n",
      "Eval loss 0.8750126361846924, R2 0.6528602242469788\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1507, loss 0.8406699299812317, R2 0.6462581157684326\n",
      "Eval loss 0.8749107718467712, R2 0.6529362201690674\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1508, loss 0.840580940246582, R2 0.6463350653648376\n",
      "Eval loss 0.874809205532074, R2 0.6530120968818665\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1509, loss 0.8404920101165771, R2 0.6464119553565979\n",
      "Eval loss 0.8747079372406006, R2 0.6530877947807312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1510, loss 0.8404034376144409, R2 0.646488606929779\n",
      "Eval loss 0.8746069073677063, R2 0.6531633138656616\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1511, loss 0.8403149843215942, R2 0.6465650796890259\n",
      "Eval loss 0.8745060563087463, R2 0.6532386541366577\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1512, loss 0.8402267694473267, R2 0.6466414928436279\n",
      "Eval loss 0.8744053840637207, R2 0.6533138155937195\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1513, loss 0.8401386737823486, R2 0.6467176079750061\n",
      "Eval loss 0.8743050694465637, R2 0.6533889174461365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1514, loss 0.8400509357452393, R2 0.64679354429245\n",
      "Eval loss 0.8742048740386963, R2 0.6534637808799744\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1515, loss 0.839963436126709, R2 0.646869421005249\n",
      "Eval loss 0.874104917049408, R2 0.6535384654998779\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1516, loss 0.8398760557174683, R2 0.6469449400901794\n",
      "Eval loss 0.8740053176879883, R2 0.6536130309104919\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1517, loss 0.8397889137268066, R2 0.6470205187797546\n",
      "Eval loss 0.8739058971405029, R2 0.6536874771118164\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1518, loss 0.8397020697593689, R2 0.6470957398414612\n",
      "Eval loss 0.8738066554069519, R2 0.6537618041038513\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1519, loss 0.8396152257919312, R2 0.6471709609031677\n",
      "Eval loss 0.8737075924873352, R2 0.6538358330726624\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1520, loss 0.8395288586616516, R2 0.6472458839416504\n",
      "Eval loss 0.8736088275909424, R2 0.6539097428321838\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1521, loss 0.8394426703453064, R2 0.647320568561554\n",
      "Eval loss 0.8735103011131287, R2 0.653983473777771\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1522, loss 0.839356541633606, R2 0.6473953127861023\n",
      "Eval loss 0.8734119534492493, R2 0.6540571451187134\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1523, loss 0.8392705917358398, R2 0.647469699382782\n",
      "Eval loss 0.8733139038085938, R2 0.6541305780410767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1524, loss 0.8391851186752319, R2 0.6475439667701721\n",
      "Eval loss 0.8732160925865173, R2 0.6542038321495056\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1525, loss 0.8390995860099792, R2 0.6476181149482727\n",
      "Eval loss 0.87311851978302, R2 0.6542770862579346\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1526, loss 0.8390144109725952, R2 0.6476921439170837\n",
      "Eval loss 0.873021125793457, R2 0.6543500423431396\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1527, loss 0.8389294147491455, R2 0.6477658748626709\n",
      "Eval loss 0.8729239106178284, R2 0.6544228196144104\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1528, loss 0.8388446569442749, R2 0.6478394865989685\n",
      "Eval loss 0.8728269934654236, R2 0.6544955968856812\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1529, loss 0.8387601375579834, R2 0.6479129195213318\n",
      "Eval loss 0.8727301955223083, R2 0.654568076133728\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1530, loss 0.8386756777763367, R2 0.6479861736297607\n",
      "Eval loss 0.872633695602417, R2 0.6546404361724854\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1531, loss 0.8385916352272034, R2 0.6480593085289001\n",
      "Eval loss 0.8725374937057495, R2 0.6547126770019531\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1532, loss 0.8385075330734253, R2 0.6481322646141052\n",
      "Eval loss 0.8724414110183716, R2 0.6547847390174866\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1533, loss 0.8384238481521606, R2 0.648205041885376\n",
      "Eval loss 0.8723456263542175, R2 0.6548566222190857\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1534, loss 0.8383404016494751, R2 0.648277759552002\n",
      "Eval loss 0.8722499012947083, R2 0.65492844581604\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1535, loss 0.8382568955421448, R2 0.6483502388000488\n",
      "Eval loss 0.8721545338630676, R2 0.6550000309944153\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1536, loss 0.8381738066673279, R2 0.6484224796295166\n",
      "Eval loss 0.8720592856407166, R2 0.655071496963501\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1537, loss 0.8380908966064453, R2 0.6484946608543396\n",
      "Eval loss 0.8719645142555237, R2 0.6551427841186523\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1538, loss 0.8380082845687866, R2 0.6485666632652283\n",
      "Eval loss 0.871869683265686, R2 0.6552139520645142\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1539, loss 0.8379256725311279, R2 0.6486384868621826\n",
      "Eval loss 0.871775209903717, R2 0.6552849411964417\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1540, loss 0.8378432989120483, R2 0.6487100720405579\n",
      "Eval loss 0.8716809153556824, R2 0.6553558707237244\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1541, loss 0.8377611637115479, R2 0.6487815976142883\n",
      "Eval loss 0.871586799621582, R2 0.6554266214370728\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1542, loss 0.8376793265342712, R2 0.6488528847694397\n",
      "Eval loss 0.8714929223060608, R2 0.655497133731842\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1543, loss 0.837597668170929, R2 0.6489241123199463\n",
      "Eval loss 0.8713993430137634, R2 0.6555675268173218\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1544, loss 0.8375161290168762, R2 0.6489951610565186\n",
      "Eval loss 0.8713058233261108, R2 0.6556377410888672\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1545, loss 0.8374348282814026, R2 0.6490660309791565\n",
      "Eval loss 0.8712126016616821, R2 0.6557078957557678\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1546, loss 0.8373537063598633, R2 0.6491366624832153\n",
      "Eval loss 0.8711195588111877, R2 0.6557778716087341\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1547, loss 0.8372728228569031, R2 0.6492072939872742\n",
      "Eval loss 0.8710268139839172, R2 0.6558476090431213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1548, loss 0.8371921181678772, R2 0.6492776274681091\n",
      "Eval loss 0.8709341287612915, R2 0.6559173464775085\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1549, loss 0.8371115922927856, R2 0.6493479013442993\n",
      "Eval loss 0.8708418011665344, R2 0.6559868454933167\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1550, loss 0.8370312452316284, R2 0.6494178771972656\n",
      "Eval loss 0.8707496523857117, R2 0.6560563445091248\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1551, loss 0.8369510769844055, R2 0.6494879126548767\n",
      "Eval loss 0.8706578612327576, R2 0.656125545501709\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1552, loss 0.8368712663650513, R2 0.6495576500892639\n",
      "Eval loss 0.8705659508705139, R2 0.6561946272850037\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1553, loss 0.8367915749549866, R2 0.6496272683143616\n",
      "Eval loss 0.8704744577407837, R2 0.6562634706497192\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1554, loss 0.8367119431495667, R2 0.6496966481208801\n",
      "Eval loss 0.8703831434249878, R2 0.65633225440979\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1555, loss 0.8366326093673706, R2 0.6497659683227539\n",
      "Eval loss 0.870292067527771, R2 0.6564010381698608\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1556, loss 0.8365534543991089, R2 0.6498351693153381\n",
      "Eval loss 0.8702011108398438, R2 0.6564695239067078\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1557, loss 0.8364745378494263, R2 0.649904191493988\n",
      "Eval loss 0.8701104521751404, R2 0.6565378904342651\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1558, loss 0.836395800113678, R2 0.6499729752540588\n",
      "Eval loss 0.8700199127197266, R2 0.6566060185432434\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1559, loss 0.8363173007965088, R2 0.6500416398048401\n",
      "Eval loss 0.8699296116828918, R2 0.6566741466522217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1560, loss 0.8362388014793396, R2 0.6501101851463318\n",
      "Eval loss 0.8698394894599915, R2 0.6567420959472656\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1561, loss 0.8361606001853943, R2 0.6501785516738892\n",
      "Eval loss 0.8697496652603149, R2 0.65680992603302\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1562, loss 0.8360826373100281, R2 0.650246798992157\n",
      "Eval loss 0.869659960269928, R2 0.6568775177001953\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1563, loss 0.8360047936439514, R2 0.6503148674964905\n",
      "Eval loss 0.8695704936981201, R2 0.6569450497627258\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1564, loss 0.8359271883964539, R2 0.6503827571868896\n",
      "Eval loss 0.8694812655448914, R2 0.657012403011322\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1565, loss 0.8358497619628906, R2 0.6504505276679993\n",
      "Eval loss 0.8693922162055969, R2 0.6570796370506287\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1566, loss 0.8357725739479065, R2 0.6505181789398193\n",
      "Eval loss 0.8693033456802368, R2 0.657146692276001\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1567, loss 0.8356955051422119, R2 0.6505856513977051\n",
      "Eval loss 0.869214653968811, R2 0.6572136878967285\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1568, loss 0.8356186151504517, R2 0.6506529450416565\n",
      "Eval loss 0.8691261410713196, R2 0.6572805047035217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1569, loss 0.8355419039726257, R2 0.6507201790809631\n",
      "Eval loss 0.8690379858016968, R2 0.6573471426963806\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1570, loss 0.8354653716087341, R2 0.6507871747016907\n",
      "Eval loss 0.8689499497413635, R2 0.6574137210845947\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1571, loss 0.8353891968727112, R2 0.6508541107177734\n",
      "Eval loss 0.8688621520996094, R2 0.6574800610542297\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1572, loss 0.8353130221366882, R2 0.6509208083152771\n",
      "Eval loss 0.8687744140625, R2 0.6575462818145752\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1573, loss 0.8352370262145996, R2 0.6509873867034912\n",
      "Eval loss 0.8686869740486145, R2 0.6576124429702759\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1574, loss 0.8351613879203796, R2 0.6510538458824158\n",
      "Eval loss 0.8685997128486633, R2 0.6576783657073975\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1575, loss 0.8350858688354492, R2 0.6511200666427612\n",
      "Eval loss 0.8685126900672913, R2 0.657744288444519\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1576, loss 0.8350104689598083, R2 0.6511862277984619\n",
      "Eval loss 0.8684257864952087, R2 0.6578100323677063\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1577, loss 0.834935188293457, R2 0.6512522101402283\n",
      "Eval loss 0.8683390617370605, R2 0.6578754782676697\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1578, loss 0.8348601460456848, R2 0.6513180136680603\n",
      "Eval loss 0.8682526350021362, R2 0.6579409241676331\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1579, loss 0.8347853422164917, R2 0.6513837575912476\n",
      "Eval loss 0.8681663870811462, R2 0.6580062508583069\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1580, loss 0.8347107768058777, R2 0.6514493823051453\n",
      "Eval loss 0.8680802583694458, R2 0.6580714583396912\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1581, loss 0.8346362709999084, R2 0.6515146493911743\n",
      "Eval loss 0.8679944276809692, R2 0.6581364274024963\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1582, loss 0.8345619440078735, R2 0.6515799760818481\n",
      "Eval loss 0.8679087162017822, R2 0.658201277256012\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1583, loss 0.8344879150390625, R2 0.6516450643539429\n",
      "Eval loss 0.8678231239318848, R2 0.6582660675048828\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1584, loss 0.8344138860702515, R2 0.6517100930213928\n",
      "Eval loss 0.8677379488945007, R2 0.6583306789398193\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1585, loss 0.8343402147293091, R2 0.6517749428749084\n",
      "Eval loss 0.8676528930664062, R2 0.6583951115608215\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1586, loss 0.8342666625976562, R2 0.6518396139144897\n",
      "Eval loss 0.8675678372383118, R2 0.658459484577179\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1587, loss 0.8341931700706482, R2 0.6519041061401367\n",
      "Eval loss 0.8674831390380859, R2 0.6585237383842468\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1588, loss 0.8341200351715088, R2 0.6519685983657837\n",
      "Eval loss 0.8673986196517944, R2 0.6585878133773804\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1589, loss 0.8340470790863037, R2 0.6520327925682068\n",
      "Eval loss 0.8673142790794373, R2 0.6586517691612244\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1590, loss 0.8339740633964539, R2 0.6520969867706299\n",
      "Eval loss 0.8672301173210144, R2 0.6587156057357788\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1591, loss 0.8339014649391174, R2 0.6521610021591187\n",
      "Eval loss 0.8671461343765259, R2 0.6587792038917542\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1592, loss 0.8338288068771362, R2 0.6522247791290283\n",
      "Eval loss 0.8670623302459717, R2 0.6588428020477295\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1593, loss 0.8337565660476685, R2 0.6522884964942932\n",
      "Eval loss 0.8669787645339966, R2 0.6589061617851257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1594, loss 0.8336843848228455, R2 0.6523520350456238\n",
      "Eval loss 0.8668954372406006, R2 0.6589694619178772\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1595, loss 0.8336123824119568, R2 0.6524154543876648\n",
      "Eval loss 0.8668121695518494, R2 0.6590325832366943\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1596, loss 0.833540678024292, R2 0.6524787545204163\n",
      "Eval loss 0.8667290210723877, R2 0.6590957045555115\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1597, loss 0.8334689140319824, R2 0.6525418758392334\n",
      "Eval loss 0.8666462898254395, R2 0.6591585874557495\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1598, loss 0.8333974480628967, R2 0.6526049375534058\n",
      "Eval loss 0.8665635585784912, R2 0.6592212915420532\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1599, loss 0.8333261609077454, R2 0.652667760848999\n",
      "Eval loss 0.8664811849594116, R2 0.6592839360237122\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1600, loss 0.8332550525665283, R2 0.6527305245399475\n",
      "Eval loss 0.8663988709449768, R2 0.6593464612960815\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1601, loss 0.8331840634346008, R2 0.6527931690216064\n",
      "Eval loss 0.8663167357444763, R2 0.6594088077545166\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1602, loss 0.8331132531166077, R2 0.6528555750846863\n",
      "Eval loss 0.8662348389625549, R2 0.6594710946083069\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1603, loss 0.8330426216125488, R2 0.6529178619384766\n",
      "Eval loss 0.8661530613899231, R2 0.6595332622528076\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1604, loss 0.8329721093177795, R2 0.6529800295829773\n",
      "Eval loss 0.8660715818405151, R2 0.6595951914787292\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1605, loss 0.8329018950462341, R2 0.6530420780181885\n",
      "Eval loss 0.8659902215003967, R2 0.6596570014953613\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1606, loss 0.832831859588623, R2 0.6531040072441101\n",
      "Eval loss 0.8659089207649231, R2 0.6597188115119934\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1607, loss 0.8327620029449463, R2 0.6531656980514526\n",
      "Eval loss 0.8658279180526733, R2 0.6597804427146912\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1608, loss 0.8326920866966248, R2 0.6532273888587952\n",
      "Eval loss 0.8657471537590027, R2 0.6598418951034546\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1609, loss 0.8326225280761719, R2 0.6532889008522034\n",
      "Eval loss 0.8656665682792664, R2 0.6599032282829285\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1610, loss 0.8325530886650085, R2 0.653350293636322\n",
      "Eval loss 0.8655861020088196, R2 0.6599644422531128\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1611, loss 0.8324838280677795, R2 0.6534114480018616\n",
      "Eval loss 0.8655056953430176, R2 0.6600255370140076\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1612, loss 0.8324147462844849, R2 0.6534726023674011\n",
      "Eval loss 0.8654255867004395, R2 0.660086452960968\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1613, loss 0.8323457837104797, R2 0.6535335183143616\n",
      "Eval loss 0.8653457164764404, R2 0.6601473093032837\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1614, loss 0.8322769999504089, R2 0.6535943746566772\n",
      "Eval loss 0.8652659058570862, R2 0.6602081060409546\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1615, loss 0.8322084546089172, R2 0.6536551117897034\n",
      "Eval loss 0.865186333656311, R2 0.6602687239646912\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1616, loss 0.8321400284767151, R2 0.6537156701087952\n",
      "Eval loss 0.865106999874115, R2 0.6603292226791382\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1617, loss 0.8320717215538025, R2 0.6537760496139526\n",
      "Eval loss 0.8650277853012085, R2 0.6603896021842957\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1618, loss 0.8320035338401794, R2 0.6538364291191101\n",
      "Eval loss 0.8649486899375916, R2 0.660449743270874\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1619, loss 0.8319355249404907, R2 0.6538965702056885\n",
      "Eval loss 0.8648698329925537, R2 0.6605098247528076\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1620, loss 0.8318678736686707, R2 0.6539565920829773\n",
      "Eval loss 0.8647910952568054, R2 0.6605698466300964\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1621, loss 0.8318002223968506, R2 0.6540165543556213\n",
      "Eval loss 0.8647127151489258, R2 0.6606297492980957\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1622, loss 0.8317327499389648, R2 0.654076337814331\n",
      "Eval loss 0.8646342158317566, R2 0.6606894731521606\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1623, loss 0.8316654562950134, R2 0.6541359424591064\n",
      "Eval loss 0.8645561337471008, R2 0.6607490181922913\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1624, loss 0.8315982818603516, R2 0.6541954874992371\n",
      "Eval loss 0.8644781708717346, R2 0.6608085632324219\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1625, loss 0.8315312266349792, R2 0.6542548537254333\n",
      "Eval loss 0.8644003868103027, R2 0.6608679294586182\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1626, loss 0.8314644694328308, R2 0.6543141007423401\n",
      "Eval loss 0.8643227815628052, R2 0.6609270572662354\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1627, loss 0.8313977122306824, R2 0.6543732285499573\n",
      "Eval loss 0.8642451763153076, R2 0.6609863042831421\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1628, loss 0.8313313126564026, R2 0.6544322371482849\n",
      "Eval loss 0.8641678690910339, R2 0.661045253276825\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1629, loss 0.8312649130821228, R2 0.6544910669326782\n",
      "Eval loss 0.8640907406806946, R2 0.661104142665863\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1630, loss 0.8311988711357117, R2 0.6545498371124268\n",
      "Eval loss 0.8640138506889343, R2 0.6611629128456116\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1631, loss 0.8311328291893005, R2 0.6546084880828857\n",
      "Eval loss 0.8639369606971741, R2 0.6612215638160706\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1632, loss 0.8310667872428894, R2 0.6546669602394104\n",
      "Eval loss 0.8638604283332825, R2 0.66128009557724\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1633, loss 0.8310011625289917, R2 0.6547253131866455\n",
      "Eval loss 0.8637838959693909, R2 0.6613385081291199\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1634, loss 0.8309356570243835, R2 0.6547836065292358\n",
      "Eval loss 0.8637077212333679, R2 0.6613968014717102\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1635, loss 0.8308702111244202, R2 0.6548416614532471\n",
      "Eval loss 0.863631546497345, R2 0.661454975605011\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1636, loss 0.8308048844337463, R2 0.6548996567726135\n",
      "Eval loss 0.8635555505752563, R2 0.6615129113197327\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1637, loss 0.8307397961616516, R2 0.6549575328826904\n",
      "Eval loss 0.863479733467102, R2 0.6615709662437439\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1638, loss 0.8306748867034912, R2 0.655015230178833\n",
      "Eval loss 0.8634040951728821, R2 0.661628782749176\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1639, loss 0.8306100964546204, R2 0.6550728678703308\n",
      "Eval loss 0.8633288145065308, R2 0.6616864800453186\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1640, loss 0.8305454254150391, R2 0.6551303863525391\n",
      "Eval loss 0.8632533550262451, R2 0.6617439985275269\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1641, loss 0.8304809927940369, R2 0.6551877856254578\n",
      "Eval loss 0.8631782531738281, R2 0.6618014574050903\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1642, loss 0.830416738986969, R2 0.6552449464797974\n",
      "Eval loss 0.863103449344635, R2 0.661858856678009\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1643, loss 0.8303524255752563, R2 0.655302107334137\n",
      "Eval loss 0.8630287051200867, R2 0.6619160771369934\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1644, loss 0.8302885293960571, R2 0.6553590297698975\n",
      "Eval loss 0.8629540205001831, R2 0.6619731783866882\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1645, loss 0.8302245140075684, R2 0.6554158926010132\n",
      "Eval loss 0.8628795146942139, R2 0.6620302200317383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1646, loss 0.8301608562469482, R2 0.6554725766181946\n",
      "Eval loss 0.8628052473068237, R2 0.6620870232582092\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1647, loss 0.8300973176956177, R2 0.655529260635376\n",
      "Eval loss 0.8627311587333679, R2 0.6621437668800354\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1648, loss 0.8300338983535767, R2 0.6555857062339783\n",
      "Eval loss 0.8626572489738464, R2 0.6622004508972168\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1649, loss 0.8299707174301147, R2 0.6556421518325806\n",
      "Eval loss 0.862583339214325, R2 0.6622568964958191\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1650, loss 0.8299074769020081, R2 0.6556983590126038\n",
      "Eval loss 0.8625096678733826, R2 0.6623134016990662\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1651, loss 0.8298445343971252, R2 0.6557545065879822\n",
      "Eval loss 0.8624362349510193, R2 0.6623696684837341\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1652, loss 0.829781711101532, R2 0.655810534954071\n",
      "Eval loss 0.8623629212379456, R2 0.6624258756637573\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1653, loss 0.8297191262245178, R2 0.6558663845062256\n",
      "Eval loss 0.8622897863388062, R2 0.662481963634491\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1654, loss 0.8296565413475037, R2 0.6559221744537354\n",
      "Eval loss 0.8622167706489563, R2 0.6625378131866455\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1655, loss 0.8295941948890686, R2 0.6559778451919556\n",
      "Eval loss 0.8621439933776855, R2 0.6625937819480896\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1656, loss 0.8295319676399231, R2 0.6560333371162415\n",
      "Eval loss 0.8620712161064148, R2 0.6626495122909546\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1657, loss 0.8294699192047119, R2 0.6560887098312378\n",
      "Eval loss 0.8619987368583679, R2 0.6627050638198853\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1658, loss 0.8294078707695007, R2 0.6561440229415894\n",
      "Eval loss 0.8619263768196106, R2 0.6627605557441711\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1659, loss 0.8293461203575134, R2 0.6561992168426514\n",
      "Eval loss 0.8618542551994324, R2 0.6628159880638123\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1660, loss 0.8292845487594604, R2 0.6562542915344238\n",
      "Eval loss 0.8617821335792542, R2 0.6628713011741638\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1661, loss 0.8292230367660522, R2 0.6563092470169067\n",
      "Eval loss 0.861710250377655, R2 0.662926435470581\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1662, loss 0.8291616439819336, R2 0.6563640236854553\n",
      "Eval loss 0.8616384863853455, R2 0.6629814505577087\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1663, loss 0.829100489616394, R2 0.6564187407493591\n",
      "Eval loss 0.861566960811615, R2 0.6630364060401917\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1664, loss 0.8290393948554993, R2 0.6564732789993286\n",
      "Eval loss 0.8614955544471741, R2 0.663091242313385\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1665, loss 0.8289785385131836, R2 0.6565277576446533\n",
      "Eval loss 0.8614243865013123, R2 0.6631460785865784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1666, loss 0.8289176821708679, R2 0.6565820574760437\n",
      "Eval loss 0.8613532781600952, R2 0.6632006168365479\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1667, loss 0.8288570642471313, R2 0.6566362977027893\n",
      "Eval loss 0.8612822890281677, R2 0.6632550954818726\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1668, loss 0.8287966251373291, R2 0.6566903591156006\n",
      "Eval loss 0.8612114787101746, R2 0.6633095145225525\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1669, loss 0.8287361264228821, R2 0.6567444205284119\n",
      "Eval loss 0.8611409068107605, R2 0.6633638739585876\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1670, loss 0.8286760449409485, R2 0.6567983031272888\n",
      "Eval loss 0.8610703349113464, R2 0.6634179353713989\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1671, loss 0.8286159634590149, R2 0.656852126121521\n",
      "Eval loss 0.8610000610351562, R2 0.6634719967842102\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1672, loss 0.8285560011863708, R2 0.6569057106971741\n",
      "Eval loss 0.8609298467636108, R2 0.6635259985923767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1673, loss 0.8284962773323059, R2 0.6569592356681824\n",
      "Eval loss 0.8608599305152893, R2 0.6635798215866089\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1674, loss 0.8284366130828857, R2 0.6570126414299011\n",
      "Eval loss 0.860789954662323, R2 0.6636335849761963\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1675, loss 0.8283771276473999, R2 0.6570659279823303\n",
      "Eval loss 0.8607202768325806, R2 0.6636872887611389\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1676, loss 0.8283178210258484, R2 0.65711909532547\n",
      "Eval loss 0.8606507182121277, R2 0.6637406945228577\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1677, loss 0.8282585144042969, R2 0.6571722030639648\n",
      "Eval loss 0.8605812191963196, R2 0.6637941598892212\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1678, loss 0.8281993865966797, R2 0.6572251915931702\n",
      "Eval loss 0.8605119585990906, R2 0.6638475060462952\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1679, loss 0.8281404376029968, R2 0.6572780609130859\n",
      "Eval loss 0.8604428172111511, R2 0.66390061378479\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1680, loss 0.8280816674232483, R2 0.6573307514190674\n",
      "Eval loss 0.8603739142417908, R2 0.6639537811279297\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1681, loss 0.8280230760574341, R2 0.6573833227157593\n",
      "Eval loss 0.86030513048172, R2 0.6640067100524902\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1682, loss 0.8279644846916199, R2 0.657435953617096\n",
      "Eval loss 0.8602365255355835, R2 0.664059579372406\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1683, loss 0.82790607213974, R2 0.6574882864952087\n",
      "Eval loss 0.8601679801940918, R2 0.6641123294830322\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1684, loss 0.8278478384017944, R2 0.6575405597686768\n",
      "Eval loss 0.8600994944572449, R2 0.6641650199890137\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1685, loss 0.8277896642684937, R2 0.6575927138328552\n",
      "Eval loss 0.8600313663482666, R2 0.6642176508903503\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1686, loss 0.8277317881584167, R2 0.6576448678970337\n",
      "Eval loss 0.8599632382392883, R2 0.6642699837684631\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1687, loss 0.8276739120483398, R2 0.6576967835426331\n",
      "Eval loss 0.8598953485488892, R2 0.6643224358558655\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1688, loss 0.8276162147521973, R2 0.6577485203742981\n",
      "Eval loss 0.8598275780677795, R2 0.6643747687339783\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1689, loss 0.8275585770606995, R2 0.6578003168106079\n",
      "Eval loss 0.8597599267959595, R2 0.664426863193512\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1690, loss 0.8275012373924255, R2 0.6578519344329834\n",
      "Eval loss 0.8596924543380737, R2 0.6644788980484009\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1691, loss 0.8274438977241516, R2 0.6579034328460693\n",
      "Eval loss 0.8596251010894775, R2 0.6645308136940002\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1692, loss 0.827386736869812, R2 0.657954752445221\n",
      "Eval loss 0.8595578670501709, R2 0.6645826697349548\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1693, loss 0.8273296356201172, R2 0.6580061316490173\n",
      "Eval loss 0.8594907522201538, R2 0.6646344065666199\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1694, loss 0.8272727131843567, R2 0.6580572724342346\n",
      "Eval loss 0.8594239354133606, R2 0.6646860241889954\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1695, loss 0.8272159099578857, R2 0.6581084132194519\n",
      "Eval loss 0.8593572378158569, R2 0.6647375226020813\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1696, loss 0.8271592855453491, R2 0.6581593155860901\n",
      "Eval loss 0.8592904806137085, R2 0.6647889614105225\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1697, loss 0.827102780342102, R2 0.6582101583480835\n",
      "Eval loss 0.8592240810394287, R2 0.6648402214050293\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1698, loss 0.8270464539527893, R2 0.6582610011100769\n",
      "Eval loss 0.8591577410697937, R2 0.6648914813995361\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1699, loss 0.8269902467727661, R2 0.6583116054534912\n",
      "Eval loss 0.8590916395187378, R2 0.6649426817893982\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1700, loss 0.8269340395927429, R2 0.6583621501922607\n",
      "Eval loss 0.8590255975723267, R2 0.6649935245513916\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1701, loss 0.826878011226654, R2 0.658412516117096\n",
      "Eval loss 0.8589596152305603, R2 0.6650445461273193\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1702, loss 0.8268222808837891, R2 0.6584628224372864\n",
      "Eval loss 0.8588938117027283, R2 0.6650952696800232\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1703, loss 0.8267664909362793, R2 0.658513069152832\n",
      "Eval loss 0.8588283061981201, R2 0.665145993232727\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1704, loss 0.8267109394073486, R2 0.6585631966590881\n",
      "Eval loss 0.858762800693512, R2 0.6651966571807861\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1705, loss 0.826655387878418, R2 0.6586131453514099\n",
      "Eval loss 0.8586974143981934, R2 0.6652470231056213\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1706, loss 0.8266000747680664, R2 0.6586629748344421\n",
      "Eval loss 0.8586322665214539, R2 0.6652975082397461\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1707, loss 0.8265449404716492, R2 0.6587128043174744\n",
      "Eval loss 0.8585672974586487, R2 0.6653478145599365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1708, loss 0.8264898657798767, R2 0.658762514591217\n",
      "Eval loss 0.8585023283958435, R2 0.6653979420661926\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1709, loss 0.826434850692749, R2 0.6588121056556702\n",
      "Eval loss 0.8584374785423279, R2 0.6654481291770935\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1710, loss 0.8263800740242004, R2 0.6588615775108337\n",
      "Eval loss 0.8583728671073914, R2 0.6654981374740601\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1711, loss 0.8263252973556519, R2 0.6589108109474182\n",
      "Eval loss 0.8583084344863892, R2 0.6655479669570923\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1712, loss 0.8262707591056824, R2 0.6589601039886475\n",
      "Eval loss 0.8582440614700317, R2 0.6655978560447693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1713, loss 0.8262162804603577, R2 0.6590092778205872\n",
      "Eval loss 0.8581798672676086, R2 0.6656475067138672\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1714, loss 0.8261619210243225, R2 0.6590583324432373\n",
      "Eval loss 0.8581157922744751, R2 0.6656970977783203\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1715, loss 0.8261078000068665, R2 0.6591072678565979\n",
      "Eval loss 0.8580517768859863, R2 0.6657466888427734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1716, loss 0.8260537981987, R2 0.6591561436653137\n",
      "Eval loss 0.8579879403114319, R2 0.6657960414886475\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1717, loss 0.8259997963905334, R2 0.6592048406600952\n",
      "Eval loss 0.8579243421554565, R2 0.6658453345298767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1718, loss 0.825946033000946, R2 0.6592534780502319\n",
      "Eval loss 0.857860803604126, R2 0.6658945679664612\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1719, loss 0.8258923292160034, R2 0.6593019962310791\n",
      "Eval loss 0.8577973246574402, R2 0.6659436821937561\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1720, loss 0.8258386850357056, R2 0.6593504548072815\n",
      "Eval loss 0.8577340245246887, R2 0.6659926772117615\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1721, loss 0.8257851600646973, R2 0.6593987941741943\n",
      "Eval loss 0.857670783996582, R2 0.6660416126251221\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1722, loss 0.8257318735122681, R2 0.6594470739364624\n",
      "Eval loss 0.8576079607009888, R2 0.6660904288291931\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1723, loss 0.8256787061691284, R2 0.6594951152801514\n",
      "Eval loss 0.8575450778007507, R2 0.6661391854286194\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1724, loss 0.8256255984306335, R2 0.6595431566238403\n",
      "Eval loss 0.8574823141098022, R2 0.6661878228187561\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1725, loss 0.8255726099014282, R2 0.6595910787582397\n",
      "Eval loss 0.8574197292327881, R2 0.6662363409996033\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1726, loss 0.825519859790802, R2 0.6596389412879944\n",
      "Eval loss 0.8573573231697083, R2 0.6662847995758057\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1727, loss 0.8254671692848206, R2 0.6596866250038147\n",
      "Eval loss 0.857295036315918, R2 0.6663331389427185\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1728, loss 0.8254145979881287, R2 0.6597341895103455\n",
      "Eval loss 0.8572327494621277, R2 0.666381299495697\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1729, loss 0.8253620266914368, R2 0.6597816944122314\n",
      "Eval loss 0.8571706414222717, R2 0.6664295196533203\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1730, loss 0.8253096342086792, R2 0.6598291993141174\n",
      "Eval loss 0.8571087718009949, R2 0.6664775013923645\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1731, loss 0.8252575397491455, R2 0.6598764657974243\n",
      "Eval loss 0.8570469617843628, R2 0.6665254831314087\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1732, loss 0.8252054452896118, R2 0.6599237322807312\n",
      "Eval loss 0.856985330581665, R2 0.6665734052658081\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1733, loss 0.8251534104347229, R2 0.659970760345459\n",
      "Eval loss 0.8569237589836121, R2 0.6666211485862732\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1734, loss 0.8251015543937683, R2 0.6600179076194763\n",
      "Eval loss 0.8568623661994934, R2 0.6666688919067383\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1735, loss 0.825049877166748, R2 0.660064697265625\n",
      "Eval loss 0.8568011522293091, R2 0.6667163968086243\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1736, loss 0.8249981999397278, R2 0.6601116061210632\n",
      "Eval loss 0.8567399382591248, R2 0.6667639017105103\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1737, loss 0.8249467611312866, R2 0.6601582765579224\n",
      "Eval loss 0.8566789627075195, R2 0.6668112277984619\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1738, loss 0.8248952627182007, R2 0.6602049469947815\n",
      "Eval loss 0.8566179871559143, R2 0.6668586134910583\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1739, loss 0.8248440027236938, R2 0.6602514386177063\n",
      "Eval loss 0.8565572500228882, R2 0.6669057607650757\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1740, loss 0.8247928023338318, R2 0.6602979302406311\n",
      "Eval loss 0.8564966320991516, R2 0.6669529676437378\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1741, loss 0.8247418403625488, R2 0.6603441834449768\n",
      "Eval loss 0.8564360737800598, R2 0.6669999957084656\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1742, loss 0.8246908783912659, R2 0.6603904366493225\n",
      "Eval loss 0.8563756942749023, R2 0.6670469045639038\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1743, loss 0.8246400952339172, R2 0.6604366302490234\n",
      "Eval loss 0.8563154935836792, R2 0.6670936942100525\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1744, loss 0.8245893120765686, R2 0.6604825854301453\n",
      "Eval loss 0.8562555313110352, R2 0.6671404242515564\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1745, loss 0.8245388865470886, R2 0.6605285406112671\n",
      "Eval loss 0.856195330619812, R2 0.6671870946884155\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1746, loss 0.8244884610176086, R2 0.6605743765830994\n",
      "Eval loss 0.8561355471611023, R2 0.6672336459159851\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1747, loss 0.8244380354881287, R2 0.6606201529502869\n",
      "Eval loss 0.8560758233070374, R2 0.6672801971435547\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1748, loss 0.8243878483772278, R2 0.66066575050354\n",
      "Eval loss 0.8560161590576172, R2 0.6673265099525452\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1749, loss 0.8243377208709717, R2 0.6607113480567932\n",
      "Eval loss 0.8559566736221313, R2 0.6673728227615356\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1750, loss 0.8242878317832947, R2 0.6607568264007568\n",
      "Eval loss 0.8558973670005798, R2 0.6674190163612366\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1751, loss 0.8242379426956177, R2 0.6608021855354309\n",
      "Eval loss 0.8558380603790283, R2 0.667465090751648\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1752, loss 0.8241881132125854, R2 0.6608474254608154\n",
      "Eval loss 0.8557790517807007, R2 0.6675111651420593\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1753, loss 0.8241384029388428, R2 0.6608926653862\n",
      "Eval loss 0.8557199835777283, R2 0.6675570011138916\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1754, loss 0.8240889310836792, R2 0.6609377264976501\n",
      "Eval loss 0.8556612133979797, R2 0.6676028966903687\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1755, loss 0.8240393996238708, R2 0.6609827280044556\n",
      "Eval loss 0.8556024432182312, R2 0.6676486134529114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1756, loss 0.8239901661872864, R2 0.6610276103019714\n",
      "Eval loss 0.8555437922477722, R2 0.6676943302154541\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1757, loss 0.8239409327507019, R2 0.6610724925994873\n",
      "Eval loss 0.8554852604866028, R2 0.6677398681640625\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1758, loss 0.8238918781280518, R2 0.6611170768737793\n",
      "Eval loss 0.8554270267486572, R2 0.6677854061126709\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1759, loss 0.8238428831100464, R2 0.6611617803573608\n",
      "Eval loss 0.8553686141967773, R2 0.667830765247345\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1760, loss 0.8237940073013306, R2 0.6612063050270081\n",
      "Eval loss 0.8553105592727661, R2 0.6678760647773743\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1761, loss 0.8237453699111938, R2 0.661250650882721\n",
      "Eval loss 0.8552525639533997, R2 0.667921245098114\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1762, loss 0.8236967325210571, R2 0.6612949967384338\n",
      "Eval loss 0.8551946878433228, R2 0.6679664254188538\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1763, loss 0.8236481547355652, R2 0.661339282989502\n",
      "Eval loss 0.8551368713378906, R2 0.6680114269256592\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1764, loss 0.8235998153686523, R2 0.6613833904266357\n",
      "Eval loss 0.8550792932510376, R2 0.6680563688278198\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1765, loss 0.8235514760017395, R2 0.6614274382591248\n",
      "Eval loss 0.8550218343734741, R2 0.6681012511253357\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1766, loss 0.8235031962394714, R2 0.661471426486969\n",
      "Eval loss 0.8549643158912659, R2 0.6681459546089172\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1767, loss 0.8234551548957825, R2 0.6615152955055237\n",
      "Eval loss 0.8549071550369263, R2 0.6681907176971436\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1768, loss 0.8234071135520935, R2 0.6615591049194336\n",
      "Eval loss 0.8548499941825867, R2 0.6682353019714355\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1769, loss 0.8233593106269836, R2 0.661602795124054\n",
      "Eval loss 0.8547929525375366, R2 0.6682798266410828\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1770, loss 0.8233116269111633, R2 0.6616464257240295\n",
      "Eval loss 0.8547359704971313, R2 0.6683242321014404\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1771, loss 0.8232638239860535, R2 0.6616899371147156\n",
      "Eval loss 0.8546792268753052, R2 0.6683685779571533\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1772, loss 0.8232163786888123, R2 0.6617333889007568\n",
      "Eval loss 0.8546225428581238, R2 0.6684128046035767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1773, loss 0.823168933391571, R2 0.6617766618728638\n",
      "Eval loss 0.8545660376548767, R2 0.6684569716453552\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1774, loss 0.8231216073036194, R2 0.6618199348449707\n",
      "Eval loss 0.8545095920562744, R2 0.668501079082489\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1775, loss 0.8230744004249573, R2 0.6618630886077881\n",
      "Eval loss 0.8544532060623169, R2 0.6685450077056885\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1776, loss 0.8230273723602295, R2 0.6619061231613159\n",
      "Eval loss 0.8543969988822937, R2 0.6685889363288879\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1777, loss 0.8229802846908569, R2 0.6619491577148438\n",
      "Eval loss 0.8543409109115601, R2 0.6686327457427979\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1778, loss 0.8229334354400635, R2 0.6619920134544373\n",
      "Eval loss 0.8542850017547607, R2 0.668676495552063\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1779, loss 0.8228866457939148, R2 0.6620347499847412\n",
      "Eval loss 0.8542291522026062, R2 0.6687201857566833\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1780, loss 0.8228398561477661, R2 0.6620774865150452\n",
      "Eval loss 0.854173481464386, R2 0.6687636971473694\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1781, loss 0.8227933049201965, R2 0.6621200442314148\n",
      "Eval loss 0.8541176915168762, R2 0.6688071489334106\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1782, loss 0.8227468729019165, R2 0.6621626019477844\n",
      "Eval loss 0.8540622591972351, R2 0.6688506603240967\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1783, loss 0.8227004408836365, R2 0.6622051000595093\n",
      "Eval loss 0.854006826877594, R2 0.6688939332962036\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1784, loss 0.8226541876792908, R2 0.6622474193572998\n",
      "Eval loss 0.853951632976532, R2 0.6689371466636658\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1785, loss 0.8226080536842346, R2 0.6622896790504456\n",
      "Eval loss 0.8538964986801147, R2 0.6689803004264832\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1786, loss 0.8225619792938232, R2 0.6623318195343018\n",
      "Eval loss 0.8538414239883423, R2 0.6690234541893005\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1787, loss 0.8225160837173462, R2 0.662373960018158\n",
      "Eval loss 0.8537865877151489, R2 0.6690663695335388\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1788, loss 0.8224701881408691, R2 0.6624159216880798\n",
      "Eval loss 0.8537315130233765, R2 0.6691092848777771\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1789, loss 0.8224245309829712, R2 0.6624578833580017\n",
      "Eval loss 0.8536770343780518, R2 0.6691520810127258\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1790, loss 0.8223788142204285, R2 0.6624996662139893\n",
      "Eval loss 0.853622317314148, R2 0.669194757938385\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1791, loss 0.8223332762718201, R2 0.662541389465332\n",
      "Eval loss 0.853567898273468, R2 0.6692374348640442\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1792, loss 0.8222877979278564, R2 0.6625831127166748\n",
      "Eval loss 0.8535134792327881, R2 0.6692800521850586\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1793, loss 0.8222425580024719, R2 0.6626245975494385\n",
      "Eval loss 0.853459358215332, R2 0.6693225502967834\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1794, loss 0.8221972584724426, R2 0.6626660823822021\n",
      "Eval loss 0.8534051775932312, R2 0.6693649291992188\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1795, loss 0.8221521377563477, R2 0.662707507610321\n",
      "Eval loss 0.8533511161804199, R2 0.6694071292877197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1796, loss 0.8221072554588318, R2 0.6627488732337952\n",
      "Eval loss 0.853297233581543, R2 0.6694493889808655\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1797, loss 0.8220621347427368, R2 0.662790060043335\n",
      "Eval loss 0.8532434105873108, R2 0.6694915890693665\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1798, loss 0.8220173716545105, R2 0.66283118724823\n",
      "Eval loss 0.8531898260116577, R2 0.6695336699485779\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1799, loss 0.821972668170929, R2 0.6628722548484802\n",
      "Eval loss 0.8531363010406494, R2 0.6695756912231445\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1800, loss 0.821928083896637, R2 0.6629132628440857\n",
      "Eval loss 0.8530826568603516, R2 0.6696175932884216\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1801, loss 0.8218836188316345, R2 0.6629540920257568\n",
      "Eval loss 0.8530293703079224, R2 0.6696593761444092\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1802, loss 0.8218391537666321, R2 0.6629948616027832\n",
      "Eval loss 0.8529762029647827, R2 0.6697011590003967\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1803, loss 0.8217948079109192, R2 0.6630356311798096\n",
      "Eval loss 0.8529230356216431, R2 0.6697428226470947\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1804, loss 0.8217506408691406, R2 0.6630762815475464\n",
      "Eval loss 0.8528699278831482, R2 0.669784426689148\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1805, loss 0.8217064738273621, R2 0.6631168127059937\n",
      "Eval loss 0.8528171181678772, R2 0.6698259115219116\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1806, loss 0.8216625452041626, R2 0.6631572246551514\n",
      "Eval loss 0.8527641892433167, R2 0.6698673367500305\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1807, loss 0.8216186165809631, R2 0.6631976366043091\n",
      "Eval loss 0.85271155834198, R2 0.6699087023735046\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1808, loss 0.821574866771698, R2 0.663237988948822\n",
      "Eval loss 0.8526589274406433, R2 0.6699499487876892\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1809, loss 0.8215310573577881, R2 0.6632781624794006\n",
      "Eval loss 0.8526064157485962, R2 0.6699910759925842\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1810, loss 0.8214873671531677, R2 0.6633183360099792\n",
      "Eval loss 0.8525541424751282, R2 0.670032262802124\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1811, loss 0.8214439153671265, R2 0.6633583307266235\n",
      "Eval loss 0.8525018692016602, R2 0.6700733304023743\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1812, loss 0.8214004039764404, R2 0.6633983254432678\n",
      "Eval loss 0.8524497747421265, R2 0.670114278793335\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1813, loss 0.8213570713996887, R2 0.6634382605552673\n",
      "Eval loss 0.852397620677948, R2 0.6701551079750061\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1814, loss 0.8213139176368713, R2 0.6634779572486877\n",
      "Eval loss 0.8523457050323486, R2 0.6701959371566772\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1815, loss 0.821270763874054, R2 0.6635177135467529\n",
      "Eval loss 0.852293848991394, R2 0.6702366471290588\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1816, loss 0.8212277889251709, R2 0.6635574102401733\n",
      "Eval loss 0.8522421717643738, R2 0.6702773571014404\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1817, loss 0.8211847543716431, R2 0.6635969281196594\n",
      "Eval loss 0.8521904945373535, R2 0.6703178286552429\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1818, loss 0.8211419582366943, R2 0.6636364459991455\n",
      "Eval loss 0.8521390557289124, R2 0.6703583002090454\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1819, loss 0.8210991621017456, R2 0.663675844669342\n",
      "Eval loss 0.8520875573158264, R2 0.6703987121582031\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1820, loss 0.8210564851760864, R2 0.663715124130249\n",
      "Eval loss 0.8520362377166748, R2 0.6704390048980713\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1821, loss 0.8210140466690063, R2 0.663754403591156\n",
      "Eval loss 0.8519850969314575, R2 0.6704792380332947\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1822, loss 0.8209714889526367, R2 0.6637935042381287\n",
      "Eval loss 0.8519339561462402, R2 0.6705194115638733\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1823, loss 0.8209291696548462, R2 0.6638326048851013\n",
      "Eval loss 0.8518829941749573, R2 0.6705595254898071\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1824, loss 0.8208869099617004, R2 0.6638716459274292\n",
      "Eval loss 0.8518322110176086, R2 0.6705995798110962\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1825, loss 0.8208446502685547, R2 0.6639105677604675\n",
      "Eval loss 0.8517813682556152, R2 0.6706395745277405\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1826, loss 0.8208025693893433, R2 0.6639494299888611\n",
      "Eval loss 0.8517307639122009, R2 0.6706793904304504\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1827, loss 0.8207606673240662, R2 0.6639881134033203\n",
      "Eval loss 0.8516802191734314, R2 0.6707192063331604\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1828, loss 0.8207187652587891, R2 0.6640267968177795\n",
      "Eval loss 0.8516296148300171, R2 0.6707589030265808\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1829, loss 0.820676863193512, R2 0.6640653610229492\n",
      "Eval loss 0.8515793085098267, R2 0.6707985401153564\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1830, loss 0.8206351399421692, R2 0.6641039252281189\n",
      "Eval loss 0.8515290021896362, R2 0.6708381175994873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1831, loss 0.8205935955047607, R2 0.664142370223999\n",
      "Eval loss 0.8514788746833801, R2 0.6708776354789734\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1832, loss 0.8205520510673523, R2 0.6641807556152344\n",
      "Eval loss 0.8514288067817688, R2 0.6709170341491699\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1833, loss 0.8205106258392334, R2 0.6642189621925354\n",
      "Eval loss 0.8513787984848022, R2 0.6709563732147217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1834, loss 0.8204692602157593, R2 0.6642572283744812\n",
      "Eval loss 0.8513290286064148, R2 0.6709956526756287\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1835, loss 0.8204280138015747, R2 0.6642953157424927\n",
      "Eval loss 0.8512791991233826, R2 0.6710348129272461\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1836, loss 0.8203868865966797, R2 0.6643333435058594\n",
      "Eval loss 0.8512296080589294, R2 0.671073853969574\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1837, loss 0.8203458189964294, R2 0.6643714308738708\n",
      "Eval loss 0.8511801362037659, R2 0.6711129546165466\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1838, loss 0.8203048706054688, R2 0.6644092202186584\n",
      "Eval loss 0.8511306643486023, R2 0.671151876449585\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1839, loss 0.8202640414237976, R2 0.6644470691680908\n",
      "Eval loss 0.8510813117027283, R2 0.6711907386779785\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1840, loss 0.8202232122421265, R2 0.6644847989082336\n",
      "Eval loss 0.8510320782661438, R2 0.6712296009063721\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1841, loss 0.8201825022697449, R2 0.6645224690437317\n",
      "Eval loss 0.8509828448295593, R2 0.6712682843208313\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1842, loss 0.8201418519020081, R2 0.664560079574585\n",
      "Eval loss 0.850933849811554, R2 0.6713069677352905\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1843, loss 0.8201013207435608, R2 0.6645975708961487\n",
      "Eval loss 0.8508849143981934, R2 0.6713456511497498\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1844, loss 0.8200609087944031, R2 0.6646349430084229\n",
      "Eval loss 0.8508360981941223, R2 0.6713840365409851\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1845, loss 0.8200206160545349, R2 0.6646723747253418\n",
      "Eval loss 0.8507872819900513, R2 0.67142254114151\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1846, loss 0.819980263710022, R2 0.6647096276283264\n",
      "Eval loss 0.8507386445999146, R2 0.6714608669281006\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1847, loss 0.8199401497840881, R2 0.6647468209266663\n",
      "Eval loss 0.8506901860237122, R2 0.6714991927146912\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1848, loss 0.8199000954627991, R2 0.6647839546203613\n",
      "Eval loss 0.850641667842865, R2 0.671537458896637\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1849, loss 0.8198601007461548, R2 0.6648210287094116\n",
      "Eval loss 0.8505933284759521, R2 0.6715756058692932\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1850, loss 0.8198202848434448, R2 0.6648579835891724\n",
      "Eval loss 0.8505451083183289, R2 0.6716136336326599\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1851, loss 0.8197803497314453, R2 0.6648949384689331\n",
      "Eval loss 0.8504968881607056, R2 0.6716516613960266\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1852, loss 0.8197405934333801, R2 0.6649317741394043\n",
      "Eval loss 0.8504488468170166, R2 0.6716895699501038\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1853, loss 0.8197010159492493, R2 0.6649684309959412\n",
      "Eval loss 0.8504008054733276, R2 0.6717275381088257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1854, loss 0.819661557674408, R2 0.6650051474571228\n",
      "Eval loss 0.8503530025482178, R2 0.6717652678489685\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1855, loss 0.8196220993995667, R2 0.6650417447090149\n",
      "Eval loss 0.8503051996231079, R2 0.6718030571937561\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1856, loss 0.8195827603340149, R2 0.6650782823562622\n",
      "Eval loss 0.8502575755119324, R2 0.6718406677246094\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1857, loss 0.8195434808731079, R2 0.6651148200035095\n",
      "Eval loss 0.8502100706100464, R2 0.6718782782554626\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1858, loss 0.8195042610168457, R2 0.6651511192321777\n",
      "Eval loss 0.8501625657081604, R2 0.6719157695770264\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1859, loss 0.8194652199745178, R2 0.6651874780654907\n",
      "Eval loss 0.8501152396202087, R2 0.6719532608985901\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1860, loss 0.8194260597229004, R2 0.6652236580848694\n",
      "Eval loss 0.8500679135322571, R2 0.6719905138015747\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1861, loss 0.8193871378898621, R2 0.6652598977088928\n",
      "Eval loss 0.8500207662582397, R2 0.6720278859138489\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1862, loss 0.8193483948707581, R2 0.6652959585189819\n",
      "Eval loss 0.8499735593795776, R2 0.6720650792121887\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1863, loss 0.819309651851654, R2 0.6653319597244263\n",
      "Eval loss 0.8499266505241394, R2 0.6721022129058838\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1864, loss 0.8192709684371948, R2 0.6653679013252258\n",
      "Eval loss 0.8498796224594116, R2 0.6721393465995789\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1865, loss 0.8192322254180908, R2 0.6654037833213806\n",
      "Eval loss 0.8498328924179077, R2 0.6721763014793396\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1866, loss 0.8191937208175659, R2 0.6654395461082458\n",
      "Eval loss 0.8497861623764038, R2 0.6722133159637451\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1867, loss 0.8191553354263306, R2 0.6654753088951111\n",
      "Eval loss 0.8497396111488342, R2 0.6722501516342163\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1868, loss 0.81911700963974, R2 0.6655109524726868\n",
      "Eval loss 0.8496930003166199, R2 0.6722869873046875\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1869, loss 0.8190788626670837, R2 0.6655464768409729\n",
      "Eval loss 0.8496466875076294, R2 0.6723236441612244\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1870, loss 0.8190406560897827, R2 0.6655820608139038\n",
      "Eval loss 0.8496002554893494, R2 0.672360360622406\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1871, loss 0.8190025687217712, R2 0.6656174659729004\n",
      "Eval loss 0.8495540022850037, R2 0.6723968982696533\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1872, loss 0.8189646005630493, R2 0.6656528115272522\n",
      "Eval loss 0.8495078086853027, R2 0.6724334359169006\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1873, loss 0.8189266920089722, R2 0.6656880974769592\n",
      "Eval loss 0.8494618535041809, R2 0.6724699139595032\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1874, loss 0.8188886642456055, R2 0.6657233834266663\n",
      "Eval loss 0.8494158387184143, R2 0.6725063323974609\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1875, loss 0.8188510537147522, R2 0.6657585501670837\n",
      "Eval loss 0.8493699431419373, R2 0.6725426316261292\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1876, loss 0.8188133835792542, R2 0.6657934784889221\n",
      "Eval loss 0.8493242263793945, R2 0.6725788712501526\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1877, loss 0.8187758326530457, R2 0.6658286452293396\n",
      "Eval loss 0.8492783904075623, R2 0.6726150512695312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1878, loss 0.8187382817268372, R2 0.665863573551178\n",
      "Eval loss 0.8492329716682434, R2 0.6726512312889099\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1879, loss 0.818700909614563, R2 0.6658983826637268\n",
      "Eval loss 0.8491873145103455, R2 0.6726871728897095\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1880, loss 0.8186635375022888, R2 0.6659331917762756\n",
      "Eval loss 0.8491419553756714, R2 0.6727231740951538\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1881, loss 0.8186262249946594, R2 0.6659679412841797\n",
      "Eval loss 0.8490966558456421, R2 0.6727591156959534\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1882, loss 0.8185891509056091, R2 0.6660025715827942\n",
      "Eval loss 0.8490514159202576, R2 0.6727949380874634\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1883, loss 0.8185520172119141, R2 0.6660371422767639\n",
      "Eval loss 0.8490062355995178, R2 0.6728307008743286\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1884, loss 0.8185151219367981, R2 0.6660716533660889\n",
      "Eval loss 0.8489612936973572, R2 0.6728664040565491\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1885, loss 0.8184781670570374, R2 0.6661060452461243\n",
      "Eval loss 0.8489161729812622, R2 0.6729021072387695\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1886, loss 0.8184413313865662, R2 0.6661404967308044\n",
      "Eval loss 0.8488714694976807, R2 0.6729376316070557\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1887, loss 0.818404495716095, R2 0.6661748290061951\n",
      "Eval loss 0.8488267064094543, R2 0.6729731559753418\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1888, loss 0.8183677792549133, R2 0.6662090420722961\n",
      "Eval loss 0.848781943321228, R2 0.6730086803436279\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1889, loss 0.818331241607666, R2 0.666243314743042\n",
      "Eval loss 0.8487374186515808, R2 0.673043966293335\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1890, loss 0.8182947635650635, R2 0.6662773489952087\n",
      "Eval loss 0.8486928343772888, R2 0.6730793118476868\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1891, loss 0.8182582855224609, R2 0.6663113236427307\n",
      "Eval loss 0.8486483693122864, R2 0.6731145977973938\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1892, loss 0.818221926689148, R2 0.6663453578948975\n",
      "Eval loss 0.8486039638519287, R2 0.6731497645378113\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1893, loss 0.8181856274604797, R2 0.6663792133331299\n",
      "Eval loss 0.8485598564147949, R2 0.673184871673584\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1894, loss 0.8181493878364563, R2 0.6664130687713623\n",
      "Eval loss 0.8485156297683716, R2 0.6732198596000671\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1895, loss 0.8181132674217224, R2 0.66644686460495\n",
      "Eval loss 0.8484715819358826, R2 0.6732549071311951\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1896, loss 0.8180772662162781, R2 0.6664806008338928\n",
      "Eval loss 0.8484275341033936, R2 0.6732898354530334\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1897, loss 0.8180412650108337, R2 0.6665141582489014\n",
      "Eval loss 0.8483837246894836, R2 0.6733246445655823\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1898, loss 0.8180053234100342, R2 0.6665477156639099\n",
      "Eval loss 0.848339855670929, R2 0.6733593344688416\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1899, loss 0.8179696202278137, R2 0.6665812134742737\n",
      "Eval loss 0.8482961654663086, R2 0.6733941435813904\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1900, loss 0.8179337978363037, R2 0.6666146516799927\n",
      "Eval loss 0.8482524752616882, R2 0.6734287738800049\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1901, loss 0.8178982734680176, R2 0.6666480302810669\n",
      "Eval loss 0.8482089042663574, R2 0.6734634041786194\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1902, loss 0.8178625702857971, R2 0.6666812896728516\n",
      "Eval loss 0.8481655120849609, R2 0.6734979152679443\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1903, loss 0.817827045917511, R2 0.6667145490646362\n",
      "Eval loss 0.8481221795082092, R2 0.6735324263572693\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1904, loss 0.8177917003631592, R2 0.6667476296424866\n",
      "Eval loss 0.8480788469314575, R2 0.6735667586326599\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1905, loss 0.8177564144134521, R2 0.6667807698249817\n",
      "Eval loss 0.8480355739593506, R2 0.6736011505126953\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1906, loss 0.8177210688591003, R2 0.6668137907981873\n",
      "Eval loss 0.8479925394058228, R2 0.6736354231834412\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1907, loss 0.8176859021186829, R2 0.6668468117713928\n",
      "Eval loss 0.8479495644569397, R2 0.6736695766448975\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1908, loss 0.8176507353782654, R2 0.6668795943260193\n",
      "Eval loss 0.8479065895080566, R2 0.6737037301063538\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1909, loss 0.8176157474517822, R2 0.6669124364852905\n",
      "Eval loss 0.8478636145591736, R2 0.6737378835678101\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1910, loss 0.8175807595252991, R2 0.666945219039917\n",
      "Eval loss 0.8478209376335144, R2 0.6737717986106873\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1911, loss 0.8175458908081055, R2 0.6669778823852539\n",
      "Eval loss 0.8477782607078552, R2 0.6738057732582092\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1912, loss 0.8175110816955566, R2 0.6670105457305908\n",
      "Eval loss 0.8477356433868408, R2 0.6738396286964417\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1913, loss 0.8174763917922974, R2 0.667043149471283\n",
      "Eval loss 0.8476930260658264, R2 0.6738734245300293\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1914, loss 0.8174417614936829, R2 0.6670756936073303\n",
      "Eval loss 0.8476506471633911, R2 0.6739072203636169\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1915, loss 0.8174071311950684, R2 0.6671081185340881\n",
      "Eval loss 0.8476083278656006, R2 0.673940896987915\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1916, loss 0.8173726797103882, R2 0.6671404242515564\n",
      "Eval loss 0.8475660681724548, R2 0.6739745736122131\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1917, loss 0.8173381686210632, R2 0.6671727895736694\n",
      "Eval loss 0.8475238084793091, R2 0.6740081310272217\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1918, loss 0.8173037767410278, R2 0.6672049760818481\n",
      "Eval loss 0.8474817276000977, R2 0.6740416884422302\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1919, loss 0.817269504070282, R2 0.6672372221946716\n",
      "Eval loss 0.8474396467208862, R2 0.6740751266479492\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1920, loss 0.8172353506088257, R2 0.667269229888916\n",
      "Eval loss 0.8473977446556091, R2 0.6741084456443787\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1921, loss 0.8172011971473694, R2 0.6673012971878052\n",
      "Eval loss 0.847355842590332, R2 0.6741418838500977\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1922, loss 0.8171671628952026, R2 0.6673333644866943\n",
      "Eval loss 0.8473141193389893, R2 0.6741751432418823\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1923, loss 0.8171331882476807, R2 0.667365312576294\n",
      "Eval loss 0.8472725749015808, R2 0.6742083430290222\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1924, loss 0.8170992732048035, R2 0.6673970818519592\n",
      "Eval loss 0.847230851650238, R2 0.6742414236068726\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1925, loss 0.8170653581619263, R2 0.6674289107322693\n",
      "Eval loss 0.8471892476081848, R2 0.6742745041847229\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1926, loss 0.8170316219329834, R2 0.667460560798645\n",
      "Eval loss 0.8471478819847107, R2 0.6743075251579285\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1927, loss 0.8169978857040405, R2 0.6674923300743103\n",
      "Eval loss 0.8471065163612366, R2 0.6743404865264893\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1928, loss 0.816964328289032, R2 0.6675238609313965\n",
      "Eval loss 0.8470653295516968, R2 0.6743733286857605\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1929, loss 0.8169308304786682, R2 0.6675553917884827\n",
      "Eval loss 0.847024142742157, R2 0.6744061708450317\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1930, loss 0.8168973922729492, R2 0.6675869226455688\n",
      "Eval loss 0.8469829559326172, R2 0.674439013004303\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1931, loss 0.8168638348579407, R2 0.6676183342933655\n",
      "Eval loss 0.8469419479370117, R2 0.6744716763496399\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1932, loss 0.8168306350708008, R2 0.6676496863365173\n",
      "Eval loss 0.8469009399414062, R2 0.6745043396949768\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1933, loss 0.8167973160743713, R2 0.6676809787750244\n",
      "Eval loss 0.8468601107597351, R2 0.6745368838310242\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1934, loss 0.8167641162872314, R2 0.6677122116088867\n",
      "Eval loss 0.8468192219734192, R2 0.6745694875717163\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1935, loss 0.8167309761047363, R2 0.6677433252334595\n",
      "Eval loss 0.8467785120010376, R2 0.6746019721031189\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1936, loss 0.8166979551315308, R2 0.6677745580673218\n",
      "Eval loss 0.8467379212379456, R2 0.6746343970298767\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1937, loss 0.8166650533676147, R2 0.6678056120872498\n",
      "Eval loss 0.8466974496841431, R2 0.6746667623519897\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1938, loss 0.816632091999054, R2 0.6678365468978882\n",
      "Eval loss 0.8466569781303406, R2 0.6746989488601685\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1939, loss 0.8165992498397827, R2 0.6678674817085266\n",
      "Eval loss 0.8466165065765381, R2 0.6747311949729919\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1940, loss 0.8165665864944458, R2 0.6678982377052307\n",
      "Eval loss 0.8465761542320251, R2 0.6747633218765259\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1941, loss 0.8165338039398193, R2 0.6679291129112244\n",
      "Eval loss 0.8465359807014465, R2 0.6747955083847046\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1942, loss 0.816501259803772, R2 0.6679598093032837\n",
      "Eval loss 0.8464957475662231, R2 0.674827516078949\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1943, loss 0.8164686560630798, R2 0.6679905652999878\n",
      "Eval loss 0.8464558124542236, R2 0.6748595833778381\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1944, loss 0.8164360523223877, R2 0.6680211424827576\n",
      "Eval loss 0.846415638923645, R2 0.674891471862793\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1945, loss 0.8164037466049194, R2 0.6680517196655273\n",
      "Eval loss 0.8463758230209351, R2 0.6749233603477478\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1946, loss 0.8163714408874512, R2 0.6680822372436523\n",
      "Eval loss 0.8463359475135803, R2 0.6749551296234131\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1947, loss 0.8163391351699829, R2 0.6681126952171326\n",
      "Eval loss 0.8462961912155151, R2 0.6749868988990784\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1948, loss 0.816307008266449, R2 0.6681430339813232\n",
      "Eval loss 0.8462564945220947, R2 0.6750186085700989\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1949, loss 0.816274881362915, R2 0.6681733727455139\n",
      "Eval loss 0.8462168574333191, R2 0.6750503182411194\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1950, loss 0.8162426948547363, R2 0.668203592300415\n",
      "Eval loss 0.8461773991584778, R2 0.6750818490982056\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1951, loss 0.8162108659744263, R2 0.6682338118553162\n",
      "Eval loss 0.8461378812789917, R2 0.6751134395599365\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1952, loss 0.8161787986755371, R2 0.6682639122009277\n",
      "Eval loss 0.8460985422134399, R2 0.6751448512077332\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1953, loss 0.8161470293998718, R2 0.6682940125465393\n",
      "Eval loss 0.8460591435432434, R2 0.6751762628555298\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1954, loss 0.8161152601242065, R2 0.6683239936828613\n",
      "Eval loss 0.846019983291626, R2 0.6752075552940369\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1955, loss 0.8160834312438965, R2 0.6683539748191833\n",
      "Eval loss 0.8459808230400085, R2 0.6752389669418335\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1956, loss 0.8160518407821655, R2 0.6683838963508606\n",
      "Eval loss 0.8459417819976807, R2 0.675270140171051\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1957, loss 0.8160202503204346, R2 0.6684137582778931\n",
      "Eval loss 0.845902681350708, R2 0.6753013730049133\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1958, loss 0.8159887194633484, R2 0.6684435606002808\n",
      "Eval loss 0.8458638191223145, R2 0.6753324270248413\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1959, loss 0.8159573078155518, R2 0.6684732437133789\n",
      "Eval loss 0.8458250164985657, R2 0.6753635406494141\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1960, loss 0.8159259557723999, R2 0.668502926826477\n",
      "Eval loss 0.8457862138748169, R2 0.6753944754600525\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1961, loss 0.815894603729248, R2 0.6685324311256409\n",
      "Eval loss 0.8457475304603577, R2 0.6754254698753357\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1962, loss 0.8158632516860962, R2 0.6685620546340942\n",
      "Eval loss 0.8457087874412537, R2 0.6754563450813293\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1963, loss 0.8158321976661682, R2 0.6685914993286133\n",
      "Eval loss 0.8456702828407288, R2 0.6754872798919678\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1964, loss 0.8158011436462402, R2 0.6686208844184875\n",
      "Eval loss 0.8456318378448486, R2 0.6755179762840271\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1965, loss 0.8157700896263123, R2 0.6686503291130066\n",
      "Eval loss 0.8455933928489685, R2 0.6755487322807312\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1966, loss 0.8157389760017395, R2 0.6686796545982361\n",
      "Eval loss 0.8455552458763123, R2 0.6755794882774353\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1967, loss 0.8157081007957458, R2 0.668708860874176\n",
      "Eval loss 0.8455168604850769, R2 0.6756100058555603\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1968, loss 0.8156774044036865, R2 0.6687381267547607\n",
      "Eval loss 0.8454787135124207, R2 0.6756405234336853\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1969, loss 0.8156464695930481, R2 0.6687672138214111\n",
      "Eval loss 0.845440685749054, R2 0.6756710410118103\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1970, loss 0.8156159520149231, R2 0.6687963008880615\n",
      "Eval loss 0.845402717590332, R2 0.6757014989852905\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1971, loss 0.8155852556228638, R2 0.6688253283500671\n",
      "Eval loss 0.8453646898269653, R2 0.6757318377494812\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1972, loss 0.8155546188354492, R2 0.668854296207428\n",
      "Eval loss 0.8453267812728882, R2 0.6757622361183167\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1973, loss 0.815524160861969, R2 0.6688832640647888\n",
      "Eval loss 0.845288872718811, R2 0.6757925152778625\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1974, loss 0.8154935836791992, R2 0.6689120531082153\n",
      "Eval loss 0.8452512621879578, R2 0.6758226752281189\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1975, loss 0.8154632449150085, R2 0.6689409613609314\n",
      "Eval loss 0.8452135920524597, R2 0.6758529543876648\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1976, loss 0.8154329061508179, R2 0.6689696907997131\n",
      "Eval loss 0.8451759815216064, R2 0.6758829951286316\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1977, loss 0.8154026865959167, R2 0.6689983010292053\n",
      "Eval loss 0.8451384902000427, R2 0.6759130358695984\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1978, loss 0.8153725266456604, R2 0.6690269708633423\n",
      "Eval loss 0.8451010584831238, R2 0.6759430766105652\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1979, loss 0.8153423070907593, R2 0.6690555214881897\n",
      "Eval loss 0.8450635671615601, R2 0.6759729981422424\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1980, loss 0.8153123259544373, R2 0.6690840721130371\n",
      "Eval loss 0.8450262546539307, R2 0.6760029196739197\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1981, loss 0.8152823448181152, R2 0.669112503528595\n",
      "Eval loss 0.8449891805648804, R2 0.6760327816009521\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1982, loss 0.8152523040771484, R2 0.6691409349441528\n",
      "Eval loss 0.8449520468711853, R2 0.6760625243186951\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1983, loss 0.8152225017547607, R2 0.6691692471504211\n",
      "Eval loss 0.844914972782135, R2 0.6760922074317932\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1984, loss 0.8151926398277283, R2 0.6691975593566895\n",
      "Eval loss 0.8448778390884399, R2 0.6761220693588257\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1985, loss 0.8151628375053406, R2 0.669225811958313\n",
      "Eval loss 0.8448408842086792, R2 0.6761516332626343\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1986, loss 0.8151331543922424, R2 0.6692540645599365\n",
      "Eval loss 0.8448041677474976, R2 0.6761810779571533\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1987, loss 0.8151035308837891, R2 0.6692821979522705\n",
      "Eval loss 0.8447672128677368, R2 0.6762106418609619\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1988, loss 0.8150740265846252, R2 0.6693102121353149\n",
      "Eval loss 0.8447305560112, R2 0.676240086555481\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1989, loss 0.8150445818901062, R2 0.6693382263183594\n",
      "Eval loss 0.8446938991546631, R2 0.67626953125\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1990, loss 0.8150150775909424, R2 0.6693662405014038\n",
      "Eval loss 0.844657301902771, R2 0.6762988567352295\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1991, loss 0.8149856925010681, R2 0.6693940758705139\n",
      "Eval loss 0.8446208238601685, R2 0.6763281226158142\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1992, loss 0.8149564862251282, R2 0.6694219708442688\n",
      "Eval loss 0.8445843458175659, R2 0.6763573884963989\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1993, loss 0.8149272799491882, R2 0.6694498062133789\n",
      "Eval loss 0.8445480465888977, R2 0.6763866543769836\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1994, loss 0.8148980140686035, R2 0.6694775223731995\n",
      "Eval loss 0.8445117473602295, R2 0.6764158010482788\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1995, loss 0.8148688673973083, R2 0.6695051789283752\n",
      "Eval loss 0.844475507736206, R2 0.6764448285102844\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1996, loss 0.8148398399353027, R2 0.6695328950881958\n",
      "Eval loss 0.8444393873214722, R2 0.67647385597229\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1997, loss 0.8148109316825867, R2 0.669560432434082\n",
      "Eval loss 0.8444032073020935, R2 0.6765029430389404\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1998, loss 0.8147819638252258, R2 0.669588029384613\n",
      "Eval loss 0.8443672060966492, R2 0.6765317916870117\n",
      "torch.Size([8000, 44]) torch.Size([8000, 44]) torch.Size([2000, 44]) torch.Size([2000, 44])\n",
      "epoch 1999, loss 0.814752995967865, R2 0.6696155071258545\n",
      "Eval loss 0.8443312048912048, R2 0.676560640335083\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[idx, :]\n",
    "    train_y = train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    test_x = test_x[idx, :]\n",
    "    test_y = test_y[idx, :]\n",
    "    print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = lr_model(train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = lr_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = lr_model(train_x)\n",
    "    test_preds = lr_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "from src.util.model_trainer import ModelTrainerOverriden\n",
    "from mbrl.util.logger import Logger\n",
    "from mbrl.util.replay_buffer import TransitionIterator\n",
    "\n",
    "class SimpleModelTrainer(ModelTrainerOverriden):\n",
    "    def __init__(self, \n",
    "        model: Model,\n",
    "        criterion,\n",
    "        metric,\n",
    "        optim_lr: float = 0.0001, \n",
    "        weight_decay: float = 0., \n",
    "        optim_eps: float = 1e-8, \n",
    "        logger: Optional[Logger] = None):\n",
    "        super().__init__(model, optim_lr, weight_decay, optim_eps, logger)\n",
    "        self.metric = metric\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.model.parameters(),\n",
    "            lr=optim_lr,\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "            self, \n",
    "            train_x,\n",
    "            train_y,\n",
    "            test_x,\n",
    "            test_y,\n",
    "            num_epochs: Optional[int] = None, \n",
    "            ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "            train_metrics = []\n",
    "            test_metrics = []\n",
    "            for epoch in range(num_epochs):\n",
    "                # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # get output from the model, given the inputs\n",
    "                outputs = self.model.model(train_x)\n",
    "\n",
    "                # get loss for the predicted output\n",
    "                loss = self.criterion(outputs, train_y)\n",
    "                train_losses.append(loss.item())\n",
    "                # get gradients w.r.t to parameters\n",
    "                loss.backward()\n",
    "\n",
    "                # update parameters\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Compute metric\n",
    "                train_metric = self.metric(outputs, train_y)\n",
    "                train_metrics.append(train_metric.item())\n",
    "\n",
    "                print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "                if epoch%eval_epoch_freq==0:\n",
    "                    with torch.no_grad():\n",
    "                        preds = self.model.model(test_x)\n",
    "                        test_loss = self.criterion(preds, test_y)\n",
    "                        test_losses.append(test_loss.item())\n",
    "                        test_metric = self.metric(preds, test_y)\n",
    "                        test_metrics.append(test_metric.item())\n",
    "                        print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n",
    "            \n",
    "            return train_losses, test_losses, train_metrics, test_metrics\n",
    "\n",
    "    # def train(\n",
    "    #         self, \n",
    "    #         dataset_train: TransitionIterator, \n",
    "    #         dataset_val: Optional[TransitionIterator] = None, \n",
    "    #         num_epochs: Optional[int] = None, \n",
    "    #     ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "    #     eval_dataset = dataset_train if dataset_val is None else dataset_val\n",
    "    #     train_losses = []\n",
    "    #     test_losses = []\n",
    "    #     train_metrics = []\n",
    "    #     test_metrics = []\n",
    "    #     for epoch in range(num_epochs):\n",
    "    #         batch_losses = []\n",
    "    #         batch_metric = []\n",
    "    #         for batch_train in dataset_train:\n",
    "    #             print(\"Batch train\", epoch, batch_train.obs.shape)\n",
    "    #             train_x, train_y = self.model._process_batch(batch_train)\n",
    "    #             # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    #             self.optimizer.zero_grad()\n",
    "\n",
    "    #             # get output from the model, given the inputs\n",
    "    #             outputs = self.model.model(train_x)\n",
    "\n",
    "    #             # get loss for the predicted output\n",
    "    #             loss = self.criterion(outputs, train_y)\n",
    "    #             batch_losses.append(loss.item())\n",
    "    #             # get gradients w.r.t to parameters\n",
    "    #             loss.backward()\n",
    "\n",
    "    #             # update parameters\n",
    "    #             self.optimizer.step()\n",
    "\n",
    "    #             # Compute metric\n",
    "    #             train_metric = self.metric(outputs, train_y)\n",
    "    #             batch_metric.append(train_metric.item())\n",
    "            \n",
    "    #         #Epoch loss and metric\n",
    "    #         train_losses.append(np.mean(batch_losses))\n",
    "    #         train_metrics.append(np.mean(batch_metric))\n",
    "\n",
    "    #         print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    #         if epoch%eval_epoch_freq==0:\n",
    "    #             with torch.no_grad():\n",
    "    #                 batch_test_loss = []\n",
    "    #                 batch_test_metric = []\n",
    "    #                 for batch in eval_dataset:\n",
    "    #                     print(\"Batch test\", epoch, batch.obs.shape)\n",
    "    #                     test_x, test_y = self.model._process_batch(batch)\n",
    "    #                     preds = self.model.model(test_x)\n",
    "    #                     test_loss = self.criterion(preds, test_y)\n",
    "    #                     batch_test_loss.append(test_loss.item())\n",
    "    #                     test_metric = self.metric(preds, test_y)\n",
    "    #                     batch_test_metric.append(test_metric.item())\n",
    "    #                 test_losses.append(np.mean(batch_test_loss))\n",
    "    #                 test_metrics.append(np.mean(batch_test_metric))\n",
    "    #                 print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))\n",
    "        \n",
    "    #     return train_losses, test_losses, train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 7.345428943634033, R2 -1.9806392192840576\n",
      "Eval loss 7.059577941894531, R2 -1.7640007734298706\n",
      "epoch 1, loss 7.053285598754883, R2 -1.9037600755691528\n",
      "Eval loss 6.797088146209717, R2 -1.6956852674484253\n",
      "epoch 2, loss 6.787016868591309, R2 -1.8333964347839355\n",
      "Eval loss 6.557851791381836, R2 -1.6331729888916016\n",
      "epoch 3, loss 6.5442280769348145, R2 -1.768945336341858\n",
      "Eval loss 6.339709281921387, R2 -1.5759263038635254\n",
      "epoch 4, loss 6.322741985321045, R2 -1.7098610401153564\n",
      "Eval loss 6.140703201293945, R2 -1.5234566926956177\n",
      "epoch 5, loss 6.12058687210083, R2 -1.6556473970413208\n",
      "Eval loss 5.959055423736572, R2 -1.475321888923645\n",
      "epoch 6, loss 5.935973167419434, R2 -1.6058545112609863\n",
      "Eval loss 5.793155670166016, R2 -1.4311199188232422\n",
      "epoch 7, loss 5.767276287078857, R2 -1.5600749254226685\n",
      "Eval loss 5.641541957855225, R2 -1.3904869556427002\n",
      "epoch 8, loss 5.613024711608887, R2 -1.5179381370544434\n",
      "Eval loss 5.502887725830078, R2 -1.3530926704406738\n",
      "epoch 9, loss 5.471881866455078, R2 -1.4791079759597778\n",
      "Eval loss 5.3759918212890625, R2 -1.318637490272522\n",
      "epoch 10, loss 5.342635154724121, R2 -1.4432803392410278\n",
      "Eval loss 5.259762763977051, R2 -1.2868492603302002\n",
      "epoch 11, loss 5.224185466766357, R2 -1.4101790189743042\n",
      "Eval loss 5.153212547302246, R2 -1.2574821710586548\n",
      "epoch 12, loss 5.115535736083984, R2 -1.3795530796051025\n",
      "Eval loss 5.055442810058594, R2 -1.2303122282028198\n",
      "epoch 13, loss 5.015779972076416, R2 -1.3511751890182495\n",
      "Eval loss 4.965640544891357, R2 -1.2051368951797485\n",
      "epoch 14, loss 4.924099445343018, R2 -1.3248387575149536\n",
      "Eval loss 4.883067607879639, R2 -1.1817721128463745\n",
      "epoch 15, loss 4.839747905731201, R2 -1.3003569841384888\n",
      "Eval loss 4.80705451965332, R2 -1.1600514650344849\n",
      "epoch 16, loss 4.762049674987793, R2 -1.2775598764419556\n",
      "Eval loss 4.736995697021484, R2 -1.139823317527771\n",
      "epoch 17, loss 4.690393447875977, R2 -1.256293773651123\n",
      "Eval loss 4.672338962554932, R2 -1.1209508180618286\n",
      "epoch 18, loss 4.624222278594971, R2 -1.23641836643219\n",
      "Eval loss 4.612584590911865, R2 -1.1033093929290771\n",
      "epoch 19, loss 4.563032150268555, R2 -1.2178072929382324\n",
      "Eval loss 4.557281494140625, R2 -1.086786150932312\n",
      "epoch 20, loss 4.506364822387695, R2 -1.200345516204834\n",
      "Eval loss 4.50601863861084, R2 -1.0712789297103882\n",
      "epoch 21, loss 4.4538068771362305, R2 -1.1839288473129272\n",
      "Eval loss 4.458422660827637, R2 -1.0566948652267456\n",
      "epoch 22, loss 4.404979705810547, R2 -1.1684619188308716\n",
      "Eval loss 4.414156436920166, R2 -1.0429497957229614\n",
      "epoch 23, loss 4.359543323516846, R2 -1.1538596153259277\n",
      "Eval loss 4.372913360595703, R2 -1.0299670696258545\n",
      "epoch 24, loss 4.317187309265137, R2 -1.1400434970855713\n",
      "Eval loss 4.334415435791016, R2 -1.017678141593933\n",
      "epoch 25, loss 4.277630805969238, R2 -1.1269431114196777\n",
      "Eval loss 4.298411846160889, R2 -1.0060194730758667\n",
      "epoch 26, loss 4.240618705749512, R2 -1.1144944429397583\n",
      "Eval loss 4.26467227935791, R2 -0.9949347972869873\n",
      "epoch 27, loss 4.20591926574707, R2 -1.102639079093933\n",
      "Eval loss 4.2329912185668945, R2 -0.9843721389770508\n",
      "epoch 28, loss 4.173322677612305, R2 -1.0913243293762207\n",
      "Eval loss 4.203179836273193, R2 -0.9742847084999084\n",
      "epoch 29, loss 4.142638206481934, R2 -1.0805023908615112\n",
      "Eval loss 4.175067901611328, R2 -0.964630126953125\n",
      "epoch 30, loss 4.113693714141846, R2 -1.07012939453125\n",
      "Eval loss 4.148500919342041, R2 -0.955369770526886\n",
      "epoch 31, loss 4.086332321166992, R2 -1.0601664781570435\n",
      "Eval loss 4.12333869934082, R2 -0.9464685916900635\n",
      "epoch 32, loss 4.0604119300842285, R2 -1.0505774021148682\n",
      "Eval loss 4.099454402923584, R2 -0.9378953576087952\n",
      "epoch 33, loss 4.035802841186523, R2 -1.0413298606872559\n",
      "Eval loss 4.076733112335205, R2 -0.9296206831932068\n",
      "epoch 34, loss 4.012388706207275, R2 -1.032394289970398\n",
      "Eval loss 4.055069446563721, R2 -0.921619176864624\n",
      "epoch 35, loss 3.9900624752044678, R2 -1.0237438678741455\n",
      "Eval loss 4.034368991851807, R2 -0.9138669371604919\n",
      "epoch 36, loss 3.9687283039093018, R2 -1.015354871749878\n",
      "Eval loss 4.014545440673828, R2 -0.9063421487808228\n",
      "epoch 37, loss 3.948298931121826, R2 -1.0072046518325806\n",
      "Eval loss 3.9955222606658936, R2 -0.899026095867157\n",
      "epoch 38, loss 3.928694486618042, R2 -0.9992734789848328\n",
      "Eval loss 3.9772279262542725, R2 -0.8919008374214172\n",
      "epoch 39, loss 3.9098434448242188, R2 -0.9915432333946228\n",
      "Eval loss 3.959597587585449, R2 -0.8849502801895142\n",
      "epoch 40, loss 3.8916800022125244, R2 -0.9839971661567688\n",
      "Eval loss 3.9425745010375977, R2 -0.8781602382659912\n",
      "epoch 41, loss 3.8741455078125, R2 -0.9766206741333008\n",
      "Eval loss 3.92610502243042, R2 -0.87151700258255\n",
      "epoch 42, loss 3.857185125350952, R2 -0.9693999886512756\n",
      "Eval loss 3.910141944885254, R2 -0.8650092482566833\n",
      "epoch 43, loss 3.8407506942749023, R2 -0.9623231291770935\n",
      "Eval loss 3.8946421146392822, R2 -0.8586259484291077\n",
      "epoch 44, loss 3.824798345565796, R2 -0.9553783535957336\n",
      "Eval loss 3.8795652389526367, R2 -0.8523569703102112\n",
      "epoch 45, loss 3.8092868328094482, R2 -0.9485559463500977\n",
      "Eval loss 3.8648765087127686, R2 -0.8461940288543701\n",
      "epoch 46, loss 3.794180393218994, R2 -0.9418464303016663\n",
      "Eval loss 3.8505430221557617, R2 -0.8401286602020264\n",
      "epoch 47, loss 3.7794461250305176, R2 -0.9352414608001709\n",
      "Eval loss 3.836536169052124, R2 -0.8341535329818726\n",
      "epoch 48, loss 3.765052556991577, R2 -0.9287336468696594\n",
      "Eval loss 3.822829484939575, R2 -0.8282622694969177\n",
      "epoch 49, loss 3.750973701477051, R2 -0.9223159551620483\n",
      "Eval loss 3.8093984127044678, R2 -0.8224484920501709\n",
      "epoch 50, loss 3.7371840476989746, R2 -0.9159818291664124\n",
      "Eval loss 3.7962207794189453, R2 -0.8167067170143127\n",
      "epoch 51, loss 3.7236618995666504, R2 -0.909726083278656\n",
      "Eval loss 3.783277750015259, R2 -0.8110326528549194\n",
      "epoch 52, loss 3.71038556098938, R2 -0.90354323387146\n",
      "Eval loss 3.770550489425659, R2 -0.8054210543632507\n",
      "epoch 53, loss 3.697338104248047, R2 -0.8974287509918213\n",
      "Eval loss 3.758023500442505, R2 -0.7998682260513306\n",
      "epoch 54, loss 3.6845014095306396, R2 -0.891377866268158\n",
      "Eval loss 3.745680809020996, R2 -0.7943701148033142\n",
      "epoch 55, loss 3.6718599796295166, R2 -0.8853872418403625\n",
      "Eval loss 3.7335100173950195, R2 -0.7889237999916077\n",
      "epoch 56, loss 3.6594011783599854, R2 -0.8794531226158142\n",
      "Eval loss 3.721498489379883, R2 -0.7835260033607483\n",
      "epoch 57, loss 3.6471107006073, R2 -0.8735721707344055\n",
      "Eval loss 3.7096359729766846, R2 -0.7781736850738525\n",
      "epoch 58, loss 3.634979486465454, R2 -0.867741584777832\n",
      "Eval loss 3.697911262512207, R2 -0.7728647589683533\n",
      "epoch 59, loss 3.6229944229125977, R2 -0.8619588613510132\n",
      "Eval loss 3.686316728591919, R2 -0.7675966620445251\n",
      "epoch 60, loss 3.611146926879883, R2 -0.8562214374542236\n",
      "Eval loss 3.674842357635498, R2 -0.7623676061630249\n",
      "epoch 61, loss 3.5994296073913574, R2 -0.850526750087738\n",
      "Eval loss 3.6634814739227295, R2 -0.7571754455566406\n",
      "epoch 62, loss 3.5878331661224365, R2 -0.8448732495307922\n",
      "Eval loss 3.6522278785705566, R2 -0.7520183324813843\n",
      "epoch 63, loss 3.5763509273529053, R2 -0.8392587900161743\n",
      "Eval loss 3.6410746574401855, R2 -0.746894896030426\n",
      "epoch 64, loss 3.564976453781128, R2 -0.8336818218231201\n",
      "Eval loss 3.630017042160034, R2 -0.7418038845062256\n",
      "epoch 65, loss 3.553703784942627, R2 -0.8281412124633789\n",
      "Eval loss 3.619048595428467, R2 -0.7367437481880188\n",
      "epoch 66, loss 3.542527675628662, R2 -0.8226348757743835\n",
      "Eval loss 3.608165740966797, R2 -0.7317134737968445\n",
      "epoch 67, loss 3.5314431190490723, R2 -0.8171619176864624\n",
      "Eval loss 3.5973644256591797, R2 -0.7267117500305176\n",
      "epoch 68, loss 3.5204455852508545, R2 -0.8117209076881409\n",
      "Eval loss 3.586639881134033, R2 -0.7217376828193665\n",
      "epoch 69, loss 3.509531259536743, R2 -0.8063114285469055\n",
      "Eval loss 3.5759897232055664, R2 -0.7167904376983643\n",
      "epoch 70, loss 3.4986960887908936, R2 -0.8009316921234131\n",
      "Eval loss 3.5654098987579346, R2 -0.7118692994117737\n",
      "epoch 71, loss 3.4879367351531982, R2 -0.7955812811851501\n",
      "Eval loss 3.5548977851867676, R2 -0.7069734334945679\n",
      "epoch 72, loss 3.477250099182129, R2 -0.7902591228485107\n",
      "Eval loss 3.5444509983062744, R2 -0.70210200548172\n",
      "epoch 73, loss 3.4666342735290527, R2 -0.7849647998809814\n",
      "Eval loss 3.534066677093506, R2 -0.6972546577453613\n",
      "epoch 74, loss 3.456085205078125, R2 -0.7796974182128906\n",
      "Eval loss 3.5237436294555664, R2 -0.6924305558204651\n",
      "epoch 75, loss 3.445600748062134, R2 -0.774456262588501\n",
      "Eval loss 3.5134787559509277, R2 -0.6876295804977417\n",
      "epoch 76, loss 3.435180425643921, R2 -0.7692409157752991\n",
      "Eval loss 3.5032713413238525, R2 -0.6828505992889404\n",
      "epoch 77, loss 3.4248204231262207, R2 -0.7640508413314819\n",
      "Eval loss 3.4931185245513916, R2 -0.6780938506126404\n",
      "epoch 78, loss 3.414519786834717, R2 -0.7588854432106018\n",
      "Eval loss 3.4830191135406494, R2 -0.6733585000038147\n",
      "epoch 79, loss 3.4042770862579346, R2 -0.7537441253662109\n",
      "Eval loss 3.472973108291626, R2 -0.6686441898345947\n",
      "epoch 80, loss 3.394090175628662, R2 -0.748626708984375\n",
      "Eval loss 3.462977170944214, R2 -0.6639509797096252\n",
      "epoch 81, loss 3.3839573860168457, R2 -0.7435328960418701\n",
      "Eval loss 3.4530320167541504, R2 -0.659278154373169\n",
      "epoch 82, loss 3.3738784790039062, R2 -0.7384619116783142\n",
      "Eval loss 3.4431350231170654, R2 -0.654625415802002\n",
      "epoch 83, loss 3.363851547241211, R2 -0.7334137558937073\n",
      "Eval loss 3.433286190032959, R2 -0.6499927043914795\n",
      "epoch 84, loss 3.3538756370544434, R2 -0.7283881902694702\n",
      "Eval loss 3.4234843254089355, R2 -0.6453794836997986\n",
      "epoch 85, loss 3.343949556350708, R2 -0.723384439945221\n",
      "Eval loss 3.413728713989258, R2 -0.6407859325408936\n",
      "epoch 86, loss 3.334073066711426, R2 -0.7184027433395386\n",
      "Eval loss 3.4040184020996094, R2 -0.6362115144729614\n",
      "epoch 87, loss 3.324244260787964, R2 -0.7134425640106201\n",
      "Eval loss 3.394352912902832, R2 -0.6316562294960022\n",
      "epoch 88, loss 3.3144638538360596, R2 -0.7085038423538208\n",
      "Eval loss 3.3847315311431885, R2 -0.6271194815635681\n",
      "epoch 89, loss 3.304729461669922, R2 -0.7035861015319824\n",
      "Eval loss 3.3751533031463623, R2 -0.6226016879081726\n",
      "epoch 90, loss 3.295041084289551, R2 -0.6986894607543945\n",
      "Eval loss 3.3656182289123535, R2 -0.6181021332740784\n",
      "epoch 91, loss 3.285398483276367, R2 -0.6938133835792542\n",
      "Eval loss 3.356125593185425, R2 -0.6136211156845093\n",
      "epoch 92, loss 3.2758004665374756, R2 -0.688957929611206\n",
      "Eval loss 3.346674680709839, R2 -0.6091580986976624\n",
      "epoch 93, loss 3.266247272491455, R2 -0.6841227412223816\n",
      "Eval loss 3.3372650146484375, R2 -0.6047132015228271\n",
      "epoch 94, loss 3.256737232208252, R2 -0.6793078780174255\n",
      "Eval loss 3.3278965950012207, R2 -0.600286066532135\n",
      "epoch 95, loss 3.247270345687866, R2 -0.6745129227638245\n",
      "Eval loss 3.3185689449310303, R2 -0.5958768129348755\n",
      "epoch 96, loss 3.237846612930298, R2 -0.6697378158569336\n",
      "Eval loss 3.309281349182129, R2 -0.5914851427078247\n",
      "epoch 97, loss 3.2284646034240723, R2 -0.6649825572967529\n",
      "Eval loss 3.3000333309173584, R2 -0.5871109366416931\n",
      "epoch 98, loss 3.219125270843506, R2 -0.6602466106414795\n",
      "Eval loss 3.290825366973877, R2 -0.5827542543411255\n",
      "epoch 99, loss 3.209826946258545, R2 -0.6555303931236267\n",
      "Eval loss 3.2816569805145264, R2 -0.5784146785736084\n",
      "epoch 100, loss 3.2005703449249268, R2 -0.6508333683013916\n",
      "Eval loss 3.272526741027832, R2 -0.5740925073623657\n",
      "epoch 101, loss 3.191354513168335, R2 -0.6461554169654846\n",
      "Eval loss 3.2634353637695312, R2 -0.5697872638702393\n",
      "epoch 102, loss 3.1821787357330322, R2 -0.6414966583251953\n",
      "Eval loss 3.254382848739624, R2 -0.5654991269111633\n",
      "epoch 103, loss 3.1730432510375977, R2 -0.6368567943572998\n",
      "Eval loss 3.245368003845215, R2 -0.5612279176712036\n",
      "epoch 104, loss 3.1639480590820312, R2 -0.6322358250617981\n",
      "Eval loss 3.236391305923462, R2 -0.5569734573364258\n",
      "epoch 105, loss 3.1548919677734375, R2 -0.6276335120201111\n",
      "Eval loss 3.227451801300049, R2 -0.5527357459068298\n",
      "epoch 106, loss 3.1458754539489746, R2 -0.6230499148368835\n",
      "Eval loss 3.218550443649292, R2 -0.5485145449638367\n",
      "epoch 107, loss 3.1368978023529053, R2 -0.6184846758842468\n",
      "Eval loss 3.209685802459717, R2 -0.5443100333213806\n",
      "epoch 108, loss 3.1279592514038086, R2 -0.6139377951622009\n",
      "Eval loss 3.2008578777313232, R2 -0.5401219725608826\n",
      "epoch 109, loss 3.119058609008789, R2 -0.6094093918800354\n",
      "Eval loss 3.1920671463012695, R2 -0.535950243473053\n",
      "epoch 110, loss 3.110196828842163, R2 -0.6048992276191711\n",
      "Eval loss 3.1833131313323975, R2 -0.5317948460578918\n",
      "epoch 111, loss 3.1013729572296143, R2 -0.6004071235656738\n",
      "Eval loss 3.174595594406128, R2 -0.5276557207107544\n",
      "epoch 112, loss 3.0925867557525635, R2 -0.5959329009056091\n",
      "Eval loss 3.1659138202667236, R2 -0.5235328078269958\n",
      "epoch 113, loss 3.0838379859924316, R2 -0.5914768576622009\n",
      "Eval loss 3.1572678089141846, R2 -0.5194258093833923\n",
      "epoch 114, loss 3.075127124786377, R2 -0.5870384573936462\n",
      "Eval loss 3.1486575603485107, R2 -0.5153348445892334\n",
      "epoch 115, loss 3.066452741622925, R2 -0.5826179385185242\n",
      "Eval loss 3.1400837898254395, R2 -0.511259913444519\n",
      "epoch 116, loss 3.0578157901763916, R2 -0.5782150030136108\n",
      "Eval loss 3.131545066833496, R2 -0.5072007775306702\n",
      "epoch 117, loss 3.04921555519104, R2 -0.5738295912742615\n",
      "Eval loss 3.1230411529541016, R2 -0.5031575560569763\n",
      "epoch 118, loss 3.04065203666687, R2 -0.5694617629051208\n",
      "Eval loss 3.1145732402801514, R2 -0.49912986159324646\n",
      "epoch 119, loss 3.0321242809295654, R2 -0.565111517906189\n",
      "Eval loss 3.106139898300171, R2 -0.4951179623603821\n",
      "epoch 120, loss 3.0236334800720215, R2 -0.5607783794403076\n",
      "Eval loss 3.097740650177002, R2 -0.4911215901374817\n",
      "epoch 121, loss 3.0151782035827637, R2 -0.5564624667167664\n",
      "Eval loss 3.08937668800354, R2 -0.4871406853199005\n",
      "epoch 122, loss 3.00675892829895, R2 -0.55216383934021\n",
      "Eval loss 3.081047296524048, R2 -0.48317527770996094\n",
      "epoch 123, loss 2.9983749389648438, R2 -0.5478822588920593\n",
      "Eval loss 3.072751760482788, R2 -0.47922518849372864\n",
      "epoch 124, loss 2.9900267124176025, R2 -0.5436175465583801\n",
      "Eval loss 3.064490795135498, R2 -0.4752904772758484\n",
      "epoch 125, loss 2.9817137718200684, R2 -0.5393700003623962\n",
      "Eval loss 3.0562639236450195, R2 -0.471371054649353\n",
      "epoch 126, loss 2.973435640335083, R2 -0.5351392030715942\n",
      "Eval loss 3.0480704307556152, R2 -0.46746665239334106\n",
      "epoch 127, loss 2.9651925563812256, R2 -0.5309252142906189\n",
      "Eval loss 3.0399105548858643, R2 -0.4635774791240692\n",
      "epoch 128, loss 2.956984281539917, R2 -0.5267278552055359\n",
      "Eval loss 3.0317845344543457, R2 -0.45970332622528076\n",
      "epoch 129, loss 2.9488108158111572, R2 -0.5225471258163452\n",
      "Eval loss 3.023691415786743, R2 -0.45584428310394287\n",
      "epoch 130, loss 2.940671443939209, R2 -0.5183830261230469\n",
      "Eval loss 3.015632152557373, R2 -0.4520000219345093\n",
      "epoch 131, loss 2.9325664043426514, R2 -0.5142354369163513\n",
      "Eval loss 3.007605791091919, R2 -0.44817057251930237\n",
      "epoch 132, loss 2.9244954586029053, R2 -0.5101041197776794\n",
      "Eval loss 2.999612331390381, R2 -0.4443560838699341\n",
      "epoch 133, loss 2.9164583683013916, R2 -0.5059892535209656\n",
      "Eval loss 2.9916512966156006, R2 -0.44055625796318054\n",
      "epoch 134, loss 2.9084553718566895, R2 -0.5018904805183411\n",
      "Eval loss 2.9837229251861572, R2 -0.4367709457874298\n",
      "epoch 135, loss 2.9004857540130615, R2 -0.4978080689907074\n",
      "Eval loss 2.97582745552063, R2 -0.433000385761261\n",
      "epoch 136, loss 2.8925492763519287, R2 -0.4937417209148407\n",
      "Eval loss 2.9679641723632812, R2 -0.42924442887306213\n",
      "epoch 137, loss 2.8846466541290283, R2 -0.4896913766860962\n",
      "Eval loss 2.9601330757141113, R2 -0.42550280690193176\n",
      "epoch 138, loss 2.876776695251465, R2 -0.4856570363044739\n",
      "Eval loss 2.95233416557312, R2 -0.42177581787109375\n",
      "epoch 139, loss 2.8689398765563965, R2 -0.48163846135139465\n",
      "Eval loss 2.9445672035217285, R2 -0.41806304454803467\n",
      "epoch 140, loss 2.8611361980438232, R2 -0.4776358902454376\n",
      "Eval loss 2.9368317127227783, R2 -0.4143645167350769\n",
      "epoch 141, loss 2.853364944458008, R2 -0.4736490547657013\n",
      "Eval loss 2.9291279315948486, R2 -0.41068047285079956\n",
      "epoch 142, loss 2.84562611579895, R2 -0.4696779251098633\n",
      "Eval loss 2.9214558601379395, R2 -0.4070102870464325\n",
      "epoch 143, loss 2.8379199504852295, R2 -0.4657224416732788\n",
      "Eval loss 2.9138150215148926, R2 -0.4033544659614563\n",
      "epoch 144, loss 2.8302457332611084, R2 -0.4617823660373688\n",
      "Eval loss 2.906205415725708, R2 -0.39971256256103516\n",
      "epoch 145, loss 2.822603940963745, R2 -0.45785799622535706\n",
      "Eval loss 2.8986270427703857, R2 -0.3960846960544586\n",
      "epoch 146, loss 2.8149938583374023, R2 -0.4539487957954407\n",
      "Eval loss 2.8910796642303467, R2 -0.3924708366394043\n",
      "epoch 147, loss 2.80741548538208, R2 -0.45005524158477783\n",
      "Eval loss 2.8835628032684326, R2 -0.38887080550193787\n",
      "epoch 148, loss 2.7998690605163574, R2 -0.44617679715156555\n",
      "Eval loss 2.8760766983032227, R2 -0.3852846920490265\n",
      "epoch 149, loss 2.792353868484497, R2 -0.44231346249580383\n",
      "Eval loss 2.868621349334717, R2 -0.3817121684551239\n",
      "epoch 150, loss 2.784869909286499, R2 -0.4384654760360718\n",
      "Eval loss 2.861196279525757, R2 -0.37815341353416443\n",
      "epoch 151, loss 2.7774176597595215, R2 -0.43463248014450073\n",
      "Eval loss 2.8538014888763428, R2 -0.3746083974838257\n",
      "epoch 152, loss 2.769996166229248, R2 -0.4308145344257355\n",
      "Eval loss 2.8464369773864746, R2 -0.37107688188552856\n",
      "epoch 153, loss 2.762605905532837, R2 -0.42701148986816406\n",
      "Eval loss 2.839102268218994, R2 -0.36755895614624023\n",
      "epoch 154, loss 2.755246162414551, R2 -0.42322346568107605\n",
      "Eval loss 2.8317975997924805, R2 -0.3640545606613159\n",
      "epoch 155, loss 2.7479169368743896, R2 -0.4194501042366028\n",
      "Eval loss 2.8245227336883545, R2 -0.3605634868144989\n",
      "epoch 156, loss 2.7406184673309326, R2 -0.41569167375564575\n",
      "Eval loss 2.817277431488037, R2 -0.35708582401275635\n",
      "epoch 157, loss 2.7333505153656006, R2 -0.4119478464126587\n",
      "Eval loss 2.810061454772949, R2 -0.3536214232444763\n",
      "epoch 158, loss 2.7261123657226562, R2 -0.40821874141693115\n",
      "Eval loss 2.802875280380249, R2 -0.35017040371894836\n",
      "epoch 159, loss 2.718904733657837, R2 -0.40450403094291687\n",
      "Eval loss 2.79571795463562, R2 -0.346732497215271\n",
      "epoch 160, loss 2.7117269039154053, R2 -0.4008040726184845\n",
      "Eval loss 2.7885899543762207, R2 -0.343307763338089\n",
      "epoch 161, loss 2.7045791149139404, R2 -0.39711838960647583\n",
      "Eval loss 2.7814905643463135, R2 -0.3398960828781128\n",
      "epoch 162, loss 2.697460651397705, R2 -0.3934471011161804\n",
      "Eval loss 2.7744202613830566, R2 -0.33649736642837524\n",
      "epoch 163, loss 2.6903719902038574, R2 -0.3897901475429535\n",
      "Eval loss 2.767378807067871, R2 -0.33311179280281067\n",
      "epoch 164, loss 2.6833128929138184, R2 -0.3861476182937622\n",
      "Eval loss 2.760366201400757, R2 -0.3297390043735504\n",
      "epoch 165, loss 2.676283121109009, R2 -0.38251903653144836\n",
      "Eval loss 2.7533814907073975, R2 -0.3263792395591736\n",
      "epoch 166, loss 2.6692821979522705, R2 -0.37890470027923584\n",
      "Eval loss 2.7464256286621094, R2 -0.3230322003364563\n",
      "epoch 167, loss 2.6623103618621826, R2 -0.37530460953712463\n",
      "Eval loss 2.739497423171997, R2 -0.31969788670539856\n",
      "epoch 168, loss 2.655367612838745, R2 -0.37171828746795654\n",
      "Eval loss 2.732597827911377, R2 -0.31637629866600037\n",
      "epoch 169, loss 2.6484534740448, R2 -0.3681459426879883\n",
      "Eval loss 2.7257258892059326, R2 -0.31306740641593933\n",
      "epoch 170, loss 2.6415679454803467, R2 -0.36458760499954224\n",
      "Eval loss 2.7188820838928223, R2 -0.3097710609436035\n",
      "epoch 171, loss 2.634711265563965, R2 -0.3610430955886841\n",
      "Eval loss 2.7120659351348877, R2 -0.3064873218536377\n",
      "epoch 172, loss 2.627882719039917, R2 -0.35751235485076904\n",
      "Eval loss 2.705277442932129, R2 -0.30321598052978516\n",
      "epoch 173, loss 2.6210827827453613, R2 -0.35399529337882996\n",
      "Eval loss 2.698516368865967, R2 -0.2999570965766907\n",
      "epoch 174, loss 2.6143105030059814, R2 -0.3504919409751892\n",
      "Eval loss 2.6917827129364014, R2 -0.29671069979667664\n",
      "epoch 175, loss 2.6075663566589355, R2 -0.3470020592212677\n",
      "Eval loss 2.6850762367248535, R2 -0.29347655177116394\n",
      "epoch 176, loss 2.6008505821228027, R2 -0.34352579712867737\n",
      "Eval loss 2.6783969402313232, R2 -0.2902548015117645\n",
      "epoch 177, loss 2.5941619873046875, R2 -0.3400630056858063\n",
      "Eval loss 2.6717445850372314, R2 -0.2870451807975769\n",
      "epoch 178, loss 2.587501287460327, R2 -0.33661383390426636\n",
      "Eval loss 2.6651194095611572, R2 -0.2838478088378906\n",
      "epoch 179, loss 2.5808680057525635, R2 -0.3331778347492218\n",
      "Eval loss 2.6585206985473633, R2 -0.28066250681877136\n",
      "epoch 180, loss 2.5742621421813965, R2 -0.329755038022995\n",
      "Eval loss 2.651948928833008, R2 -0.2774893045425415\n",
      "epoch 181, loss 2.567683696746826, R2 -0.3263458013534546\n",
      "Eval loss 2.6454033851623535, R2 -0.2743281126022339\n",
      "epoch 182, loss 2.5611321926116943, R2 -0.32294961810112\n",
      "Eval loss 2.6388845443725586, R2 -0.2711789309978485\n",
      "epoch 183, loss 2.55460786819458, R2 -0.3195665180683136\n",
      "Eval loss 2.632391929626465, R2 -0.2680416703224182\n",
      "epoch 184, loss 2.5481104850769043, R2 -0.3161965608596802\n",
      "Eval loss 2.6259255409240723, R2 -0.26491624116897583\n",
      "epoch 185, loss 2.541639804840088, R2 -0.3128395080566406\n",
      "Eval loss 2.6194851398468018, R2 -0.2618027329444885\n",
      "epoch 186, loss 2.5351955890655518, R2 -0.30949556827545166\n",
      "Eval loss 2.613070487976074, R2 -0.258700966835022\n",
      "epoch 187, loss 2.528778314590454, R2 -0.30616453289985657\n",
      "Eval loss 2.606682062149048, R2 -0.25561097264289856\n",
      "epoch 188, loss 2.5223870277404785, R2 -0.3028464615345001\n",
      "Eval loss 2.6003196239471436, R2 -0.25253263115882874\n",
      "epoch 189, loss 2.5160224437713623, R2 -0.2995409667491913\n",
      "Eval loss 2.593982219696045, R2 -0.2494659274816513\n",
      "epoch 190, loss 2.509683847427368, R2 -0.29624834656715393\n",
      "Eval loss 2.5876708030700684, R2 -0.2464108020067215\n",
      "epoch 191, loss 2.503371477127075, R2 -0.29296842217445374\n",
      "Eval loss 2.5813844203948975, R2 -0.24336719512939453\n",
      "epoch 192, loss 2.4970853328704834, R2 -0.28970110416412354\n",
      "Eval loss 2.5751235485076904, R2 -0.24033497273921967\n",
      "epoch 193, loss 2.4908244609832764, R2 -0.2864464819431305\n",
      "Eval loss 2.568887710571289, R2 -0.237314373254776\n",
      "epoch 194, loss 2.4845895767211914, R2 -0.2832043468952179\n",
      "Eval loss 2.5626771450042725, R2 -0.23430509865283966\n",
      "epoch 195, loss 2.4783804416656494, R2 -0.2799745500087738\n",
      "Eval loss 2.5564916133880615, R2 -0.23130713403224945\n",
      "epoch 196, loss 2.472196578979492, R2 -0.2767573595046997\n",
      "Eval loss 2.550330638885498, R2 -0.22832053899765015\n",
      "epoch 197, loss 2.466038465499878, R2 -0.2735525369644165\n",
      "Eval loss 2.5441946983337402, R2 -0.22534511983394623\n",
      "epoch 198, loss 2.459904909133911, R2 -0.27035990357398987\n",
      "Eval loss 2.538083076477051, R2 -0.2223808914422989\n",
      "epoch 199, loss 2.4537971019744873, R2 -0.2671796977519989\n",
      "Eval loss 2.531996011734009, R2 -0.2194279134273529\n",
      "epoch 200, loss 2.44771409034729, R2 -0.2640116214752197\n",
      "Eval loss 2.5259335041046143, R2 -0.21648597717285156\n",
      "epoch 201, loss 2.4416563510894775, R2 -0.2608557641506195\n",
      "Eval loss 2.519895553588867, R2 -0.21355509757995605\n",
      "epoch 202, loss 2.4356231689453125, R2 -0.25771209597587585\n",
      "Eval loss 2.5138816833496094, R2 -0.2106352299451828\n",
      "epoch 203, loss 2.429614782333374, R2 -0.2545802891254425\n",
      "Eval loss 2.507891893386841, R2 -0.207726389169693\n",
      "epoch 204, loss 2.423630952835083, R2 -0.25146058201789856\n",
      "Eval loss 2.5019257068634033, R2 -0.2048284113407135\n",
      "epoch 205, loss 2.4176714420318604, R2 -0.24835285544395447\n",
      "Eval loss 2.495983839035034, R2 -0.2019413858652115\n",
      "epoch 206, loss 2.4117369651794434, R2 -0.24525712430477142\n",
      "Eval loss 2.490065813064575, R2 -0.19906511902809143\n",
      "epoch 207, loss 2.4058260917663574, R2 -0.24217315018177032\n",
      "Eval loss 2.4841713905334473, R2 -0.19619964063167572\n",
      "epoch 208, loss 2.39993953704834, R2 -0.23910094797611237\n",
      "Eval loss 2.4783003330230713, R2 -0.193344846367836\n",
      "epoch 209, loss 2.3940773010253906, R2 -0.23604059219360352\n",
      "Eval loss 2.4724531173706055, R2 -0.19050082564353943\n",
      "epoch 210, loss 2.3882389068603516, R2 -0.23299194872379303\n",
      "Eval loss 2.4666287899017334, R2 -0.18766754865646362\n",
      "epoch 211, loss 2.3824245929718018, R2 -0.22995489835739136\n",
      "Eval loss 2.4608283042907715, R2 -0.1848447471857071\n",
      "epoch 212, loss 2.376633405685425, R2 -0.22692957520484924\n",
      "Eval loss 2.455050468444824, R2 -0.18203258514404297\n",
      "epoch 213, loss 2.370866537094116, R2 -0.2239157259464264\n",
      "Eval loss 2.449296236038208, R2 -0.1792309284210205\n",
      "epoch 214, loss 2.3651230335235596, R2 -0.220913365483284\n",
      "Eval loss 2.4435648918151855, R2 -0.17643968760967255\n",
      "epoch 215, loss 2.359402656555176, R2 -0.21792246401309967\n",
      "Eval loss 2.4378559589385986, R2 -0.17365895211696625\n",
      "epoch 216, loss 2.353705883026123, R2 -0.2149430811405182\n",
      "Eval loss 2.4321701526641846, R2 -0.17088861763477325\n",
      "epoch 217, loss 2.348032236099243, R2 -0.21197496354579926\n",
      "Eval loss 2.426506996154785, R2 -0.1681285798549652\n",
      "epoch 218, loss 2.3423819541931152, R2 -0.20901818573474884\n",
      "Eval loss 2.4208664894104004, R2 -0.1653788536787033\n",
      "epoch 219, loss 2.336754322052002, R2 -0.20607267320156097\n",
      "Eval loss 2.415248155593872, R2 -0.16263940930366516\n",
      "epoch 220, loss 2.3311498165130615, R2 -0.20313844084739685\n",
      "Eval loss 2.4096524715423584, R2 -0.1599101424217224\n",
      "epoch 221, loss 2.325568199157715, R2 -0.20021547377109528\n",
      "Eval loss 2.404078960418701, R2 -0.15719108283519745\n",
      "epoch 222, loss 2.320009231567383, R2 -0.19730344414710999\n",
      "Eval loss 2.3985278606414795, R2 -0.1544821709394455\n",
      "epoch 223, loss 2.3144729137420654, R2 -0.1944025456905365\n",
      "Eval loss 2.3929989337921143, R2 -0.151783287525177\n",
      "epoch 224, loss 2.3089592456817627, R2 -0.19151271879673004\n",
      "Eval loss 2.387491464614868, R2 -0.14909453690052032\n",
      "epoch 225, loss 2.3034677505493164, R2 -0.18863382935523987\n",
      "Eval loss 2.3820059299468994, R2 -0.14641572535037994\n",
      "epoch 226, loss 2.2979984283447266, R2 -0.18576598167419434\n",
      "Eval loss 2.376542806625366, R2 -0.1437469869852066\n",
      "epoch 227, loss 2.2925517559051514, R2 -0.18290895223617554\n",
      "Eval loss 2.371100902557373, R2 -0.1410881131887436\n",
      "epoch 228, loss 2.2871270179748535, R2 -0.18006277084350586\n",
      "Eval loss 2.365680694580078, R2 -0.13843902945518494\n",
      "epoch 229, loss 2.281724452972412, R2 -0.17722740769386292\n",
      "Eval loss 2.3602824211120605, R2 -0.13579988479614258\n",
      "epoch 230, loss 2.276343584060669, R2 -0.17440281808376312\n",
      "Eval loss 2.354905128479004, R2 -0.1331705003976822\n",
      "epoch 231, loss 2.270984649658203, R2 -0.1715889722108841\n",
      "Eval loss 2.3495492935180664, R2 -0.13055096566677094\n",
      "epoch 232, loss 2.2656476497650146, R2 -0.1687857210636139\n",
      "Eval loss 2.34421443939209, R2 -0.1279410421848297\n",
      "epoch 233, loss 2.260331869125366, R2 -0.16599315404891968\n",
      "Eval loss 2.3389012813568115, R2 -0.12534090876579285\n",
      "epoch 234, loss 2.255038022994995, R2 -0.16321110725402832\n",
      "Eval loss 2.333608627319336, R2 -0.12275032699108124\n",
      "epoch 235, loss 2.249765396118164, R2 -0.16043968498706818\n",
      "Eval loss 2.3283374309539795, R2 -0.12016940116882324\n",
      "epoch 236, loss 2.244514226913452, R2 -0.15767869353294373\n",
      "Eval loss 2.323086738586426, R2 -0.11759795993566513\n",
      "epoch 237, loss 2.2392842769622803, R2 -0.1549280732870102\n",
      "Eval loss 2.317857027053833, R2 -0.11503617465496063\n",
      "epoch 238, loss 2.2340755462646484, R2 -0.15218795835971832\n",
      "Eval loss 2.312647819519043, R2 -0.11248371750116348\n",
      "epoch 239, loss 2.2288877964019775, R2 -0.14945818483829498\n",
      "Eval loss 2.307459592819214, R2 -0.10994076728820801\n",
      "epoch 240, loss 2.2237212657928467, R2 -0.14673875272274017\n",
      "Eval loss 2.3022916316986084, R2 -0.10740721970796585\n",
      "epoch 241, loss 2.2185752391815186, R2 -0.14402948319911957\n",
      "Eval loss 2.2971439361572266, R2 -0.10488308221101761\n",
      "epoch 242, loss 2.2134501934051514, R2 -0.1413305550813675\n",
      "Eval loss 2.2920169830322266, R2 -0.10236826539039612\n",
      "epoch 243, loss 2.208346128463745, R2 -0.13864174485206604\n",
      "Eval loss 2.286909818649292, R2 -0.09986266493797302\n",
      "epoch 244, loss 2.2032623291015625, R2 -0.13596314191818237\n",
      "Eval loss 2.28182315826416, R2 -0.09736637771129608\n",
      "epoch 245, loss 2.1981992721557617, R2 -0.13329459726810455\n",
      "Eval loss 2.276756525039673, R2 -0.09487931430339813\n",
      "epoch 246, loss 2.1931567192077637, R2 -0.13063612580299377\n",
      "Eval loss 2.27170991897583, R2 -0.09240143746137619\n",
      "epoch 247, loss 2.18813419342041, R2 -0.12798763811588287\n",
      "Eval loss 2.266683340072632, R2 -0.08993261307477951\n",
      "epoch 248, loss 2.1831321716308594, R2 -0.125349223613739\n",
      "Eval loss 2.261676073074341, R2 -0.08747294545173645\n",
      "epoch 249, loss 2.178150177001953, R2 -0.12272060662508011\n",
      "Eval loss 2.2566888332366943, R2 -0.08502227813005447\n",
      "epoch 250, loss 2.1731884479522705, R2 -0.12010195106267929\n",
      "Eval loss 2.251721143722534, R2 -0.08258069306612015\n",
      "epoch 251, loss 2.1682465076446533, R2 -0.11749310791492462\n",
      "Eval loss 2.2467730045318604, R2 -0.08014810085296631\n",
      "epoch 252, loss 2.1633245944976807, R2 -0.11489406228065491\n",
      "Eval loss 2.241844892501831, R2 -0.07772444188594818\n",
      "epoch 253, loss 2.1584224700927734, R2 -0.11230485886335373\n",
      "Eval loss 2.236935615539551, R2 -0.07530970126390457\n",
      "epoch 254, loss 2.1535401344299316, R2 -0.10972530394792557\n",
      "Eval loss 2.232045888900757, R2 -0.07290379703044891\n",
      "epoch 255, loss 2.148677349090576, R2 -0.10715547204017639\n",
      "Eval loss 2.22717547416687, R2 -0.0705067589879036\n",
      "epoch 256, loss 2.143834114074707, R2 -0.10459531098604202\n",
      "Eval loss 2.2223238945007324, R2 -0.06811854243278503\n",
      "epoch 257, loss 2.139010429382324, R2 -0.10204463452100754\n",
      "Eval loss 2.217491388320923, R2 -0.06573905795812607\n",
      "epoch 258, loss 2.1342062950134277, R2 -0.09950363636016846\n",
      "Eval loss 2.2126781940460205, R2 -0.06336824595928192\n",
      "epoch 259, loss 2.1294212341308594, R2 -0.09697213768959045\n",
      "Eval loss 2.207883596420288, R2 -0.06100630387663841\n",
      "epoch 260, loss 2.1246554851531982, R2 -0.09445009380578995\n",
      "Eval loss 2.203108072280884, R2 -0.05865290015935898\n",
      "epoch 261, loss 2.1199088096618652, R2 -0.0919375866651535\n",
      "Eval loss 2.1983509063720703, R2 -0.056308161467313766\n",
      "epoch 262, loss 2.1151812076568604, R2 -0.08943438529968262\n",
      "Eval loss 2.193612813949585, R2 -0.05397205427289009\n",
      "epoch 263, loss 2.1104726791381836, R2 -0.08694057911634445\n",
      "Eval loss 2.1888930797576904, R2 -0.05164436995983124\n",
      "epoch 264, loss 2.105782985687256, R2 -0.08445609360933304\n",
      "Eval loss 2.184192180633545, R2 -0.04932530224323273\n",
      "epoch 265, loss 2.101112127304077, R2 -0.08198097348213196\n",
      "Eval loss 2.179509162902832, R2 -0.047014713287353516\n",
      "epoch 266, loss 2.0964598655700684, R2 -0.0795150026679039\n",
      "Eval loss 2.174844741821289, R2 -0.04471251368522644\n",
      "epoch 267, loss 2.0918259620666504, R2 -0.07705830037593842\n",
      "Eval loss 2.170198678970337, R2 -0.04241883382201195\n",
      "epoch 268, loss 2.0872113704681396, R2 -0.07461082935333252\n",
      "Eval loss 2.1655707359313965, R2 -0.04013344272971153\n",
      "epoch 269, loss 2.0826146602630615, R2 -0.07217233628034592\n",
      "Eval loss 2.1609606742858887, R2 -0.03785640746355057\n",
      "epoch 270, loss 2.0780365467071533, R2 -0.06974302977323532\n",
      "Eval loss 2.1563689708709717, R2 -0.03558769449591637\n",
      "epoch 271, loss 2.073476791381836, R2 -0.0673227310180664\n",
      "Eval loss 2.151794910430908, R2 -0.033327337354421616\n",
      "epoch 272, loss 2.0689351558685303, R2 -0.06491155177354813\n",
      "Eval loss 2.1472389698028564, R2 -0.031075119972229004\n",
      "epoch 273, loss 2.0644118785858154, R2 -0.06250929832458496\n",
      "Eval loss 2.1427009105682373, R2 -0.028831152245402336\n",
      "epoch 274, loss 2.0599067211151123, R2 -0.06011595577001572\n",
      "Eval loss 2.1381804943084717, R2 -0.026595354080200195\n",
      "epoch 275, loss 2.0554189682006836, R2 -0.057731568813323975\n",
      "Eval loss 2.1336772441864014, R2 -0.02436772547662258\n",
      "epoch 276, loss 2.0509495735168457, R2 -0.05535605922341347\n",
      "Eval loss 2.1291921138763428, R2 -0.02214816026389599\n",
      "epoch 277, loss 2.0464982986450195, R2 -0.052989330142736435\n",
      "Eval loss 2.1247241497039795, R2 -0.01993662677705288\n",
      "epoch 278, loss 2.0420641899108887, R2 -0.05063146725296974\n",
      "Eval loss 2.1202738285064697, R2 -0.017733251675963402\n",
      "epoch 279, loss 2.0376482009887695, R2 -0.04828229174017906\n",
      "Eval loss 2.1158409118652344, R2 -0.015537909232079983\n",
      "epoch 280, loss 2.033249616622925, R2 -0.04594194516539574\n",
      "Eval loss 2.1114251613616943, R2 -0.013350400142371655\n",
      "epoch 281, loss 2.0288689136505127, R2 -0.043610215187072754\n",
      "Eval loss 2.1070263385772705, R2 -0.011170950718224049\n",
      "epoch 282, loss 2.024505376815796, R2 -0.04128711298108101\n",
      "Eval loss 2.1026451587677, R2 -0.008999396115541458\n",
      "epoch 283, loss 2.0201590061187744, R2 -0.03897266089916229\n",
      "Eval loss 2.098280906677246, R2 -0.006835747975856066\n",
      "epoch 284, loss 2.0158302783966064, R2 -0.036666832864284515\n",
      "Eval loss 2.093933343887329, R2 -0.004679864272475243\n",
      "epoch 285, loss 2.011518716812134, R2 -0.034369584172964096\n",
      "Eval loss 2.0896027088165283, R2 -0.0025318535044789314\n",
      "epoch 286, loss 2.0072243213653564, R2 -0.0320807546377182\n",
      "Eval loss 2.085289239883423, R2 -0.0003916187852155417\n",
      "epoch 287, loss 2.0029470920562744, R2 -0.029800377786159515\n",
      "Eval loss 2.0809922218322754, R2 0.0017408783314749599\n",
      "epoch 288, loss 1.9986867904663086, R2 -0.02752852998673916\n",
      "Eval loss 2.076711893081665, R2 0.003865632228553295\n",
      "epoch 289, loss 1.9944435358047485, R2 -0.025265086442232132\n",
      "Eval loss 2.072448492050171, R2 0.005982707720249891\n",
      "epoch 290, loss 1.9902169704437256, R2 -0.023009933531284332\n",
      "Eval loss 2.0682010650634766, R2 0.008092072792351246\n",
      "epoch 291, loss 1.9860073328018188, R2 -0.02076316438615322\n",
      "Eval loss 2.0639705657958984, R2 0.010193835943937302\n",
      "epoch 292, loss 1.9818142652511597, R2 -0.018524711951613426\n",
      "Eval loss 2.059756278991699, R2 0.012288066558539867\n",
      "epoch 293, loss 1.977637767791748, R2 -0.016294490545988083\n",
      "Eval loss 2.055558681488037, R2 0.014374646358191967\n",
      "epoch 294, loss 1.9734779596328735, R2 -0.014072570018470287\n",
      "Eval loss 2.051377058029175, R2 0.016453754156827927\n",
      "epoch 295, loss 1.9693348407745361, R2 -0.011858788318932056\n",
      "Eval loss 2.0472116470336914, R2 0.018525313585996628\n",
      "epoch 296, loss 1.9652080535888672, R2 -0.009653172455728054\n",
      "Eval loss 2.043062210083008, R2 0.02058938331902027\n",
      "epoch 297, loss 1.9610975980758667, R2 -0.007455728482455015\n",
      "Eval loss 2.0389292240142822, R2 0.02264602668583393\n",
      "epoch 298, loss 1.9570032358169556, R2 -0.005266368389129639\n",
      "Eval loss 2.0348122119903564, R2 0.024695271626114845\n",
      "epoch 299, loss 1.952925443649292, R2 -0.003085093107074499\n",
      "Eval loss 2.0307106971740723, R2 0.02673710510134697\n",
      "epoch 300, loss 1.9488636255264282, R2 -0.000911810202524066\n",
      "Eval loss 2.026625394821167, R2 0.02877161093056202\n",
      "epoch 301, loss 1.9448177814483643, R2 0.0012534531997516751\n",
      "Eval loss 2.0225555896759033, R2 0.03079877607524395\n",
      "epoch 302, loss 1.9407880306243896, R2 0.0034107675310224295\n",
      "Eval loss 2.0185017585754395, R2 0.03281863033771515\n",
      "epoch 303, loss 1.9367741346359253, R2 0.005560121964663267\n",
      "Eval loss 2.0144636631011963, R2 0.0348312109708786\n",
      "epoch 304, loss 1.9327763319015503, R2 0.00770157016813755\n",
      "Eval loss 2.0104405879974365, R2 0.03683657571673393\n",
      "epoch 305, loss 1.928794026374817, R2 0.009835162200033665\n",
      "Eval loss 2.0064337253570557, R2 0.03883475065231323\n",
      "epoch 306, loss 1.9248278141021729, R2 0.011960972100496292\n",
      "Eval loss 2.002441883087158, R2 0.04082570970058441\n",
      "epoch 307, loss 1.9208770990371704, R2 0.014078909531235695\n",
      "Eval loss 1.9984657764434814, R2 0.04280954226851463\n",
      "epoch 308, loss 1.9169418811798096, R2 0.01618911512196064\n",
      "Eval loss 1.9945048093795776, R2 0.04478627070784569\n",
      "epoch 309, loss 1.9130223989486694, R2 0.01829155534505844\n",
      "Eval loss 1.9905589818954468, R2 0.046755895018577576\n",
      "epoch 310, loss 1.9091182947158813, R2 0.02038624696433544\n",
      "Eval loss 1.9866284132003784, R2 0.04871838167309761\n",
      "epoch 311, loss 1.9052295684814453, R2 0.022473307326436043\n",
      "Eval loss 1.9827131032943726, R2 0.050673965364694595\n",
      "epoch 312, loss 1.9013562202453613, R2 0.024552730843424797\n",
      "Eval loss 1.97881281375885, R2 0.05262242257595062\n",
      "epoch 313, loss 1.8974978923797607, R2 0.02662450075149536\n",
      "Eval loss 1.9749271869659424, R2 0.054563961923122406\n",
      "epoch 314, loss 1.8936548233032227, R2 0.02868872880935669\n",
      "Eval loss 1.9710569381713867, R2 0.05649857223033905\n",
      "epoch 315, loss 1.8898272514343262, R2 0.0307453703135252\n",
      "Eval loss 1.9672014713287354, R2 0.058426205068826675\n",
      "epoch 316, loss 1.8860143423080444, R2 0.0327945314347744\n",
      "Eval loss 1.9633607864379883, R2 0.060346972197294235\n",
      "epoch 317, loss 1.8822165727615356, R2 0.03483610227704048\n",
      "Eval loss 1.9595351219177246, R2 0.06226084381341934\n",
      "epoch 318, loss 1.8784339427947998, R2 0.03687030449509621\n",
      "Eval loss 1.955723762512207, R2 0.06416791677474976\n",
      "epoch 319, loss 1.8746659755706787, R2 0.0388970710337162\n",
      "Eval loss 1.9519271850585938, R2 0.06606818735599518\n",
      "epoch 320, loss 1.870913028717041, R2 0.040916457772254944\n",
      "Eval loss 1.9481449127197266, R2 0.06796162575483322\n",
      "epoch 321, loss 1.867174506187439, R2 0.04292842373251915\n",
      "Eval loss 1.9443775415420532, R2 0.06984829902648926\n",
      "epoch 322, loss 1.8634510040283203, R2 0.04493309184908867\n",
      "Eval loss 1.940624475479126, R2 0.07172829657793045\n",
      "epoch 323, loss 1.8597418069839478, R2 0.046930406242609024\n",
      "Eval loss 1.9368855953216553, R2 0.07360153645277023\n",
      "epoch 324, loss 1.8560476303100586, R2 0.048920515924692154\n",
      "Eval loss 1.9331612586975098, R2 0.07546815276145935\n",
      "epoch 325, loss 1.8523675203323364, R2 0.05090327188372612\n",
      "Eval loss 1.9294509887695312, R2 0.07732809334993362\n",
      "epoch 326, loss 1.84870183467865, R2 0.05287889391183853\n",
      "Eval loss 1.9257553815841675, R2 0.07918141037225723\n",
      "epoch 327, loss 1.8450508117675781, R2 0.05484727770090103\n",
      "Eval loss 1.9220733642578125, R2 0.08102817088365555\n",
      "epoch 328, loss 1.8414140939712524, R2 0.056808508932590485\n",
      "Eval loss 1.918405532836914, R2 0.08286828547716141\n",
      "epoch 329, loss 1.8377913236618042, R2 0.05876268073916435\n",
      "Eval loss 1.9147517681121826, R2 0.08470186591148376\n",
      "epoch 330, loss 1.834182858467102, R2 0.06070966646075249\n",
      "Eval loss 1.9111121892929077, R2 0.0865289643406868\n",
      "epoch 331, loss 1.8305885791778564, R2 0.06264960020780563\n",
      "Eval loss 1.9074862003326416, R2 0.08834958076477051\n",
      "epoch 332, loss 1.8270083665847778, R2 0.06458248943090439\n",
      "Eval loss 1.9038742780685425, R2 0.09016373753547668\n",
      "epoch 333, loss 1.8234421014785767, R2 0.06650836020708084\n",
      "Eval loss 1.9002763032913208, R2 0.09197142720222473\n",
      "epoch 334, loss 1.819889783859253, R2 0.06842729449272156\n",
      "Eval loss 1.8966917991638184, R2 0.09377271682024002\n",
      "epoch 335, loss 1.8163515329360962, R2 0.07033923268318176\n",
      "Eval loss 1.8931207656860352, R2 0.09556763619184494\n",
      "epoch 336, loss 1.8128271102905273, R2 0.07224418222904205\n",
      "Eval loss 1.889563798904419, R2 0.09735613316297531\n",
      "epoch 337, loss 1.8093160390853882, R2 0.07414230704307556\n",
      "Eval loss 1.8860200643539429, R2 0.09913837164640427\n",
      "epoch 338, loss 1.805819034576416, R2 0.07603354007005692\n",
      "Eval loss 1.8824899196624756, R2 0.10091424733400345\n",
      "epoch 339, loss 1.8023356199264526, R2 0.07791789621114731\n",
      "Eval loss 1.8789732456207275, R2 0.10268385708332062\n",
      "epoch 340, loss 1.798865795135498, R2 0.07979544997215271\n",
      "Eval loss 1.8754698038101196, R2 0.10444715619087219\n",
      "epoch 341, loss 1.7954094409942627, R2 0.08166619390249252\n",
      "Eval loss 1.8719797134399414, R2 0.10620424896478653\n",
      "epoch 342, loss 1.7919667959213257, R2 0.08353018015623093\n",
      "Eval loss 1.868503212928772, R2 0.1079551950097084\n",
      "epoch 343, loss 1.7885372638702393, R2 0.08538751304149628\n",
      "Eval loss 1.8650398254394531, R2 0.10969986766576767\n",
      "epoch 344, loss 1.7851213216781616, R2 0.08723801374435425\n",
      "Eval loss 1.8615894317626953, R2 0.11143839359283447\n",
      "epoch 345, loss 1.7817186117172241, R2 0.08908197283744812\n",
      "Eval loss 1.858152151107788, R2 0.1131708174943924\n",
      "epoch 346, loss 1.7783290147781372, R2 0.0909191444516182\n",
      "Eval loss 1.8547279834747314, R2 0.11489710211753845\n",
      "epoch 347, loss 1.77495276927948, R2 0.0927496924996376\n",
      "Eval loss 1.8513168096542358, R2 0.11661729216575623\n",
      "epoch 348, loss 1.7715896368026733, R2 0.09457366168498993\n",
      "Eval loss 1.8479186296463013, R2 0.1183314397931099\n",
      "epoch 349, loss 1.7682397365570068, R2 0.09639113396406174\n",
      "Eval loss 1.8445333242416382, R2 0.12003955245018005\n",
      "epoch 350, loss 1.7649027109146118, R2 0.09820199757814407\n",
      "Eval loss 1.8411608934402466, R2 0.12174160778522491\n",
      "epoch 351, loss 1.7615786790847778, R2 0.1000063493847847\n",
      "Eval loss 1.8378010988235474, R2 0.12343766540288925\n",
      "epoch 352, loss 1.7582674026489258, R2 0.10180414468050003\n",
      "Eval loss 1.8344541788101196, R2 0.12512782216072083\n",
      "epoch 353, loss 1.7549691200256348, R2 0.10359559208154678\n",
      "Eval loss 1.8311198949813843, R2 0.12681198120117188\n",
      "epoch 354, loss 1.7516835927963257, R2 0.10538052767515182\n",
      "Eval loss 1.8277982473373413, R2 0.12849022448062897\n",
      "epoch 355, loss 1.7484108209609985, R2 0.1071590855717659\n",
      "Eval loss 1.8244889974594116, R2 0.1301625818014145\n",
      "epoch 356, loss 1.7451509237289429, R2 0.10893117636442184\n",
      "Eval loss 1.8211925029754639, R2 0.13182903826236725\n",
      "epoch 357, loss 1.7419034242630005, R2 0.110696941614151\n",
      "Eval loss 1.8179084062576294, R2 0.1334896981716156\n",
      "epoch 358, loss 1.73866868019104, R2 0.11245638877153397\n",
      "Eval loss 1.8146367073059082, R2 0.13514448702335358\n",
      "epoch 359, loss 1.7354463338851929, R2 0.11420946568250656\n",
      "Eval loss 1.8113771677017212, R2 0.13679343461990356\n",
      "epoch 360, loss 1.732236623764038, R2 0.11595633625984192\n",
      "Eval loss 1.8081303834915161, R2 0.1384366899728775\n",
      "epoch 361, loss 1.729039192199707, R2 0.11769688129425049\n",
      "Eval loss 1.8048954010009766, R2 0.14007414877414703\n",
      "epoch 362, loss 1.7258543968200684, R2 0.1194312646985054\n",
      "Eval loss 1.8016728162765503, R2 0.14170590043067932\n",
      "epoch 363, loss 1.7226816415786743, R2 0.12115936726331711\n",
      "Eval loss 1.7984623908996582, R2 0.14333190023899078\n",
      "epoch 364, loss 1.7195212841033936, R2 0.12288126349449158\n",
      "Eval loss 1.7952638864517212, R2 0.1449522078037262\n",
      "epoch 365, loss 1.7163732051849365, R2 0.12459710240364075\n",
      "Eval loss 1.7920777797698975, R2 0.14656686782836914\n",
      "epoch 366, loss 1.7132371664047241, R2 0.1263066977262497\n",
      "Eval loss 1.7889035940170288, R2 0.14817580580711365\n",
      "epoch 367, loss 1.710113286972046, R2 0.12801025807857513\n",
      "Eval loss 1.7857410907745361, R2 0.14977918565273285\n",
      "epoch 368, loss 1.7070014476776123, R2 0.1297076791524887\n",
      "Eval loss 1.7825908660888672, R2 0.15137699246406555\n",
      "epoch 369, loss 1.7039016485214233, R2 0.13139905035495758\n",
      "Eval loss 1.7794520854949951, R2 0.15296918153762817\n",
      "epoch 370, loss 1.7008137702941895, R2 0.13308435678482056\n",
      "Eval loss 1.7763254642486572, R2 0.1545558124780655\n",
      "epoch 371, loss 1.6977379322052002, R2 0.1347636580467224\n",
      "Eval loss 1.7732105255126953, R2 0.1561369001865387\n",
      "epoch 372, loss 1.6946736574172974, R2 0.13643695414066315\n",
      "Eval loss 1.7701072692871094, R2 0.15771247446537018\n",
      "epoch 373, loss 1.6916216611862183, R2 0.13810434937477112\n",
      "Eval loss 1.7670155763626099, R2 0.1592825949192047\n",
      "epoch 374, loss 1.6885809898376465, R2 0.13976572453975677\n",
      "Eval loss 1.7639356851577759, R2 0.16084721684455872\n",
      "epoch 375, loss 1.6855522394180298, R2 0.14142125844955444\n",
      "Eval loss 1.7608672380447388, R2 0.16240638494491577\n",
      "epoch 376, loss 1.682535171508789, R2 0.14307080209255219\n",
      "Eval loss 1.757810354232788, R2 0.16396015882492065\n",
      "epoch 377, loss 1.6795297861099243, R2 0.14471451938152313\n",
      "Eval loss 1.7547649145126343, R2 0.16550850868225098\n",
      "epoch 378, loss 1.676535725593567, R2 0.14635242521762848\n",
      "Eval loss 1.7517307996749878, R2 0.16705144941806793\n",
      "epoch 379, loss 1.673553228378296, R2 0.14798450469970703\n",
      "Eval loss 1.7487081289291382, R2 0.1685890555381775\n",
      "epoch 380, loss 1.6705824136734009, R2 0.1496107429265976\n",
      "Eval loss 1.7456969022750854, R2 0.17012132704257965\n",
      "epoch 381, loss 1.6676229238510132, R2 0.15123121440410614\n",
      "Eval loss 1.742696762084961, R2 0.17164824903011322\n",
      "epoch 382, loss 1.6646747589111328, R2 0.15284590423107147\n",
      "Eval loss 1.7397079467773438, R2 0.17316986620426178\n",
      "epoch 383, loss 1.6617379188537598, R2 0.15445493161678314\n",
      "Eval loss 1.7367304563522339, R2 0.1746862232685089\n",
      "epoch 384, loss 1.6588122844696045, R2 0.1560581773519516\n",
      "Eval loss 1.7337636947631836, R2 0.17619730532169342\n",
      "epoch 385, loss 1.655898094177246, R2 0.1576557755470276\n",
      "Eval loss 1.7308082580566406, R2 0.1777031570672989\n",
      "epoch 386, loss 1.6529946327209473, R2 0.15924768149852753\n",
      "Eval loss 1.7278640270233154, R2 0.1792038083076477\n",
      "epoch 387, loss 1.6501026153564453, R2 0.16083398461341858\n",
      "Eval loss 1.7249305248260498, R2 0.18069927394390106\n",
      "epoch 388, loss 1.6472219228744507, R2 0.16241461038589478\n",
      "Eval loss 1.7220081090927124, R2 0.18218955397605896\n",
      "epoch 389, loss 1.6443519592285156, R2 0.16398978233337402\n",
      "Eval loss 1.7190965414047241, R2 0.1836746335029602\n",
      "epoch 390, loss 1.6414932012557983, R2 0.16555920243263245\n",
      "Eval loss 1.716196060180664, R2 0.18515458703041077\n",
      "epoch 391, loss 1.638645052909851, R2 0.1671231985092163\n",
      "Eval loss 1.7133060693740845, R2 0.18662947416305542\n",
      "epoch 392, loss 1.6358078718185425, R2 0.1686815768480301\n",
      "Eval loss 1.710427165031433, R2 0.188099205493927\n",
      "epoch 393, loss 1.6329818964004517, R2 0.17023448646068573\n",
      "Eval loss 1.7075587511062622, R2 0.18956393003463745\n",
      "epoch 394, loss 1.6301665306091309, R2 0.17178191244602203\n",
      "Eval loss 1.70470130443573, R2 0.19102352857589722\n",
      "epoch 395, loss 1.6273618936538696, R2 0.173323854804039\n",
      "Eval loss 1.7018542289733887, R2 0.19247817993164062\n",
      "epoch 396, loss 1.6245678663253784, R2 0.1748604029417038\n",
      "Eval loss 1.699017882347107, R2 0.19392772018909454\n",
      "epoch 397, loss 1.6217848062515259, R2 0.17639145255088806\n",
      "Eval loss 1.6961921453475952, R2 0.19537228345870972\n",
      "epoch 398, loss 1.6190122365951538, R2 0.17791715264320374\n",
      "Eval loss 1.6933767795562744, R2 0.19681189954280853\n",
      "epoch 399, loss 1.6162503957748413, R2 0.17943745851516724\n",
      "Eval loss 1.690571904182434, R2 0.19824649393558502\n",
      "epoch 400, loss 1.6134988069534302, R2 0.18095241487026215\n",
      "Eval loss 1.6877777576446533, R2 0.1996762603521347\n",
      "epoch 401, loss 1.610757827758789, R2 0.18246208131313324\n",
      "Eval loss 1.6849933862686157, R2 0.20110100507736206\n",
      "epoch 402, loss 1.6080273389816284, R2 0.18396638333797455\n",
      "Eval loss 1.6822198629379272, R2 0.20252086222171783\n",
      "epoch 403, loss 1.6053072214126587, R2 0.18546538054943085\n",
      "Eval loss 1.679456353187561, R2 0.2039358764886856\n",
      "epoch 404, loss 1.6025973558425903, R2 0.1869591325521469\n",
      "Eval loss 1.6767030954360962, R2 0.20534604787826538\n",
      "epoch 405, loss 1.599898099899292, R2 0.18844762444496155\n",
      "Eval loss 1.6739602088928223, R2 0.20675134658813477\n",
      "epoch 406, loss 1.597208857536316, R2 0.18993087112903595\n",
      "Eval loss 1.6712274551391602, R2 0.20815181732177734\n",
      "epoch 407, loss 1.5945298671722412, R2 0.1914089322090149\n",
      "Eval loss 1.6685049533843994, R2 0.2095475196838379\n",
      "epoch 408, loss 1.5918611288070679, R2 0.19288183748722076\n",
      "Eval loss 1.6657922267913818, R2 0.21093836426734924\n",
      "epoch 409, loss 1.589202642440796, R2 0.1943495273590088\n",
      "Eval loss 1.6630898714065552, R2 0.21232444047927856\n",
      "epoch 410, loss 1.5865541696548462, R2 0.1958121359348297\n",
      "Eval loss 1.6603974103927612, R2 0.21370582282543182\n",
      "epoch 411, loss 1.5839157104492188, R2 0.1972695291042328\n",
      "Eval loss 1.65771484375, R2 0.21508245170116425\n",
      "epoch 412, loss 1.5812872648239136, R2 0.19872179627418518\n",
      "Eval loss 1.655042290687561, R2 0.2164543718099594\n",
      "epoch 413, loss 1.5786690711975098, R2 0.20016904175281525\n",
      "Eval loss 1.6523792743682861, R2 0.21782159805297852\n",
      "epoch 414, loss 1.57606041431427, R2 0.2016112357378006\n",
      "Eval loss 1.6497265100479126, R2 0.21918414533138275\n",
      "epoch 415, loss 1.5734620094299316, R2 0.20304833352565765\n",
      "Eval loss 1.6470834016799927, R2 0.2205420434474945\n",
      "epoch 416, loss 1.5708732604980469, R2 0.20448042452335358\n",
      "Eval loss 1.644450068473816, R2 0.2218952775001526\n",
      "epoch 417, loss 1.5682944059371948, R2 0.20590753853321075\n",
      "Eval loss 1.6418267488479614, R2 0.22324389219284058\n",
      "epoch 418, loss 1.565725326538086, R2 0.2073296159505844\n",
      "Eval loss 1.639212727546692, R2 0.22458790242671967\n",
      "epoch 419, loss 1.5631657838821411, R2 0.2087467610836029\n",
      "Eval loss 1.636608362197876, R2 0.22592735290527344\n",
      "epoch 420, loss 1.560616135597229, R2 0.21015897393226624\n",
      "Eval loss 1.6340135335922241, R2 0.2272622138261795\n",
      "epoch 421, loss 1.5580761432647705, R2 0.21156620979309082\n",
      "Eval loss 1.631428599357605, R2 0.22859254479408264\n",
      "epoch 422, loss 1.5555458068847656, R2 0.21296857297420502\n",
      "Eval loss 1.6288529634475708, R2 0.22991830110549927\n",
      "epoch 423, loss 1.5530248880386353, R2 0.21436600387096405\n",
      "Eval loss 1.6262867450714111, R2 0.23123957216739655\n",
      "epoch 424, loss 1.550513505935669, R2 0.21575860679149628\n",
      "Eval loss 1.6237300634384155, R2 0.23255635797977448\n",
      "epoch 425, loss 1.5480116605758667, R2 0.2171463519334793\n",
      "Eval loss 1.621182918548584, R2 0.23386864364147186\n",
      "epoch 426, loss 1.5455195903778076, R2 0.21852926909923553\n",
      "Eval loss 1.6186448335647583, R2 0.2351764738559723\n",
      "epoch 427, loss 1.5430365800857544, R2 0.21990734338760376\n",
      "Eval loss 1.6161165237426758, R2 0.23647984862327576\n",
      "epoch 428, loss 1.5405629873275757, R2 0.22128063440322876\n",
      "Eval loss 1.6135969161987305, R2 0.23777879774570465\n",
      "epoch 429, loss 1.538098692893982, R2 0.22264915704727173\n",
      "Eval loss 1.6110869646072388, R2 0.23907333612442017\n",
      "epoch 430, loss 1.5356438159942627, R2 0.22401294112205505\n",
      "Eval loss 1.608586311340332, R2 0.2403634935617447\n",
      "epoch 431, loss 1.5331979990005493, R2 0.22537201642990112\n",
      "Eval loss 1.6060945987701416, R2 0.24164928495883942\n",
      "epoch 432, loss 1.53076171875, R2 0.22672627866268158\n",
      "Eval loss 1.6036121845245361, R2 0.24293065071105957\n",
      "epoch 433, loss 1.528334379196167, R2 0.22807587683200836\n",
      "Eval loss 1.6011388301849365, R2 0.2442077249288559\n",
      "epoch 434, loss 1.5259162187576294, R2 0.22942085564136505\n",
      "Eval loss 1.5986744165420532, R2 0.24548043310642242\n",
      "epoch 435, loss 1.5235071182250977, R2 0.2307610958814621\n",
      "Eval loss 1.5962190628051758, R2 0.24674886465072632\n",
      "epoch 436, loss 1.5211071968078613, R2 0.23209668695926666\n",
      "Eval loss 1.5937728881835938, R2 0.248012974858284\n",
      "epoch 437, loss 1.5187162160873413, R2 0.2334277182817459\n",
      "Eval loss 1.5913355350494385, R2 0.24927286803722382\n",
      "epoch 438, loss 1.5163342952728271, R2 0.23475408554077148\n",
      "Eval loss 1.5889071226119995, R2 0.2505284547805786\n",
      "epoch 439, loss 1.5139613151550293, R2 0.23607589304447174\n",
      "Eval loss 1.5864875316619873, R2 0.25177979469299316\n",
      "epoch 440, loss 1.5115972757339478, R2 0.2373930811882019\n",
      "Eval loss 1.5840768814086914, R2 0.25302693247795105\n",
      "epoch 441, loss 1.5092421770095825, R2 0.2387058138847351\n",
      "Eval loss 1.5816750526428223, R2 0.25426986813545227\n",
      "epoch 442, loss 1.5068955421447754, R2 0.24001389741897583\n",
      "Eval loss 1.5792819261550903, R2 0.25550854206085205\n",
      "epoch 443, loss 1.5045582056045532, R2 0.2413175255060196\n",
      "Eval loss 1.5768977403640747, R2 0.25674307346343994\n",
      "epoch 444, loss 1.5022295713424683, R2 0.24261660873889923\n",
      "Eval loss 1.5745220184326172, R2 0.25797346234321594\n",
      "epoch 445, loss 1.4999094009399414, R2 0.2439112663269043\n",
      "Eval loss 1.5721548795700073, R2 0.2591996490955353\n",
      "epoch 446, loss 1.49759840965271, R2 0.24520140886306763\n",
      "Eval loss 1.5697965621948242, R2 0.2604217529296875\n",
      "epoch 447, loss 1.4952956438064575, R2 0.24648712575435638\n",
      "Eval loss 1.5674467086791992, R2 0.26163971424102783\n",
      "epoch 448, loss 1.4930015802383423, R2 0.24776838719844818\n",
      "Eval loss 1.5651054382324219, R2 0.26285359263420105\n",
      "epoch 449, loss 1.4907160997390747, R2 0.249045267701149\n",
      "Eval loss 1.5627728700637817, R2 0.26406338810920715\n",
      "epoch 450, loss 1.4884393215179443, R2 0.25031769275665283\n",
      "Eval loss 1.5604488849639893, R2 0.26526907086372375\n",
      "epoch 451, loss 1.486170768737793, R2 0.25158578157424927\n",
      "Eval loss 1.5581331253051758, R2 0.266470730304718\n",
      "epoch 452, loss 1.4839110374450684, R2 0.2528494894504547\n",
      "Eval loss 1.5558258295059204, R2 0.26766839623451233\n",
      "epoch 453, loss 1.4816596508026123, R2 0.25410890579223633\n",
      "Eval loss 1.5535269975662231, R2 0.2688620388507843\n",
      "epoch 454, loss 1.4794164896011353, R2 0.25536394119262695\n",
      "Eval loss 1.5512365102767944, R2 0.27005159854888916\n",
      "epoch 455, loss 1.4771820306777954, R2 0.25661468505859375\n",
      "Eval loss 1.5489542484283447, R2 0.27123722434043884\n",
      "epoch 456, loss 1.4749557971954346, R2 0.25786110758781433\n",
      "Eval loss 1.5466804504394531, R2 0.2724188566207886\n",
      "epoch 457, loss 1.4727377891540527, R2 0.25910326838493347\n",
      "Eval loss 1.544414758682251, R2 0.27359649538993835\n",
      "epoch 458, loss 1.4705281257629395, R2 0.26034119725227356\n",
      "Eval loss 1.5421572923660278, R2 0.27477025985717773\n",
      "epoch 459, loss 1.4683266878128052, R2 0.26157477498054504\n",
      "Eval loss 1.5399080514907837, R2 0.27594003081321716\n",
      "epoch 460, loss 1.46613347530365, R2 0.26280421018600464\n",
      "Eval loss 1.537666916847229, R2 0.2771058976650238\n",
      "epoch 461, loss 1.4639484882354736, R2 0.26402944326400757\n",
      "Eval loss 1.5354341268539429, R2 0.27826789021492004\n",
      "epoch 462, loss 1.4617716073989868, R2 0.26525047421455383\n",
      "Eval loss 1.533209204673767, R2 0.2794259786605835\n",
      "epoch 463, loss 1.4596028327941895, R2 0.26646727323532104\n",
      "Eval loss 1.5309923887252808, R2 0.28058019280433655\n",
      "epoch 464, loss 1.4574421644210815, R2 0.26767995953559875\n",
      "Eval loss 1.5287836790084839, R2 0.2817305624485016\n",
      "epoch 465, loss 1.455289363861084, R2 0.2688884437084198\n",
      "Eval loss 1.5265827178955078, R2 0.2828770577907562\n",
      "epoch 466, loss 1.4531446695327759, R2 0.27009284496307373\n",
      "Eval loss 1.5243898630142212, R2 0.2840198278427124\n",
      "epoch 467, loss 1.4510079622268677, R2 0.27129313349723816\n",
      "Eval loss 1.5222052335739136, R2 0.2851586639881134\n",
      "epoch 468, loss 1.4488792419433594, R2 0.2724893391132355\n",
      "Eval loss 1.520027995109558, R2 0.28629371523857117\n",
      "epoch 469, loss 1.446758508682251, R2 0.2736813724040985\n",
      "Eval loss 1.5178589820861816, R2 0.2874250113964081\n",
      "epoch 470, loss 1.444645643234253, R2 0.2748694121837616\n",
      "Eval loss 1.5156975984573364, R2 0.28855258226394653\n",
      "epoch 471, loss 1.4425405263900757, R2 0.27605336904525757\n",
      "Eval loss 1.5135444402694702, R2 0.28967636823654175\n",
      "epoch 472, loss 1.4404431581497192, R2 0.2772333025932312\n",
      "Eval loss 1.511398434638977, R2 0.2907963991165161\n",
      "epoch 473, loss 1.4383536577224731, R2 0.2784092128276825\n",
      "Eval loss 1.5092605352401733, R2 0.291912704706192\n",
      "epoch 474, loss 1.4362717866897583, R2 0.27958109974861145\n",
      "Eval loss 1.5071303844451904, R2 0.29302534461021423\n",
      "epoch 475, loss 1.434198021888733, R2 0.28074902296066284\n",
      "Eval loss 1.5050075054168701, R2 0.2941342294216156\n",
      "epoch 476, loss 1.4321315288543701, R2 0.28191298246383667\n",
      "Eval loss 1.5028927326202393, R2 0.2952394485473633\n",
      "epoch 477, loss 1.4300730228424072, R2 0.28307297825813293\n",
      "Eval loss 1.50078547000885, R2 0.2963409721851349\n",
      "epoch 478, loss 1.428021788597107, R2 0.28422901034355164\n",
      "Eval loss 1.4986857175827026, R2 0.2974388897418976\n",
      "epoch 479, loss 1.4259783029556274, R2 0.28538110852241516\n",
      "Eval loss 1.4965933561325073, R2 0.2985330820083618\n",
      "epoch 480, loss 1.4239424467086792, R2 0.2865292727947235\n",
      "Eval loss 1.4945088624954224, R2 0.2996237576007843\n",
      "epoch 481, loss 1.4219141006469727, R2 0.28767359256744385\n",
      "Eval loss 1.4924317598342896, R2 0.3007107377052307\n",
      "epoch 482, loss 1.4198933839797974, R2 0.2888140380382538\n",
      "Eval loss 1.4903620481491089, R2 0.3017941415309906\n",
      "epoch 483, loss 1.4178799390792847, R2 0.28995057940483093\n",
      "Eval loss 1.48829984664917, R2 0.30287399888038635\n",
      "epoch 484, loss 1.4158738851547241, R2 0.2910832464694977\n",
      "Eval loss 1.4862449169158936, R2 0.3039501905441284\n",
      "epoch 485, loss 1.4138753414154053, R2 0.2922120988368988\n",
      "Eval loss 1.4841974973678589, R2 0.3050228953361511\n",
      "epoch 486, loss 1.4118841886520386, R2 0.2933371365070343\n",
      "Eval loss 1.4821572303771973, R2 0.3060920238494873\n",
      "epoch 487, loss 1.409900426864624, R2 0.2944583594799042\n",
      "Eval loss 1.4801244735717773, R2 0.30715763568878174\n",
      "epoch 488, loss 1.4079240560531616, R2 0.2955757677555084\n",
      "Eval loss 1.4780988693237305, R2 0.3082197606563568\n",
      "epoch 489, loss 1.4059547185897827, R2 0.2966894805431366\n",
      "Eval loss 1.4760806560516357, R2 0.309278279542923\n",
      "epoch 490, loss 1.403992772102356, R2 0.297799289226532\n",
      "Eval loss 1.474069595336914, R2 0.31033337116241455\n",
      "epoch 491, loss 1.4020379781723022, R2 0.29890546202659607\n",
      "Eval loss 1.472065806388855, R2 0.31138500571250916\n",
      "epoch 492, loss 1.4000905752182007, R2 0.3000078499317169\n",
      "Eval loss 1.470069169998169, R2 0.312433123588562\n",
      "epoch 493, loss 1.398150086402893, R2 0.3011065423488617\n",
      "Eval loss 1.4680795669555664, R2 0.3134777843952179\n",
      "epoch 494, loss 1.3962169885635376, R2 0.3022014796733856\n",
      "Eval loss 1.466097116470337, R2 0.3145190477371216\n",
      "epoch 495, loss 1.3942909240722656, R2 0.30329275131225586\n",
      "Eval loss 1.46412193775177, R2 0.3155568540096283\n",
      "epoch 496, loss 1.3923717737197876, R2 0.30438029766082764\n",
      "Eval loss 1.4621535539627075, R2 0.3165912628173828\n",
      "epoch 497, loss 1.3904600143432617, R2 0.3054642677307129\n",
      "Eval loss 1.460192322731018, R2 0.31762227416038513\n",
      "epoch 498, loss 1.3885550498962402, R2 0.3065445125102997\n",
      "Eval loss 1.458238124847412, R2 0.31864988803863525\n",
      "epoch 499, loss 1.3866571187973022, R2 0.3076211214065552\n",
      "Eval loss 1.4562908411026, R2 0.3196740448474884\n",
      "epoch 500, loss 1.3847661018371582, R2 0.30869415402412415\n",
      "Eval loss 1.454350471496582, R2 0.32069501280784607\n",
      "epoch 501, loss 1.3828821182250977, R2 0.3097635507583618\n",
      "Eval loss 1.4524171352386475, R2 0.3217124938964844\n",
      "epoch 502, loss 1.3810051679611206, R2 0.31082937121391296\n",
      "Eval loss 1.4504905939102173, R2 0.32272669672966003\n",
      "epoch 503, loss 1.3791348934173584, R2 0.3118915855884552\n",
      "Eval loss 1.448570966720581, R2 0.3237375020980835\n",
      "epoch 504, loss 1.3772715330123901, R2 0.3129502534866333\n",
      "Eval loss 1.4466582536697388, R2 0.3247450292110443\n",
      "epoch 505, loss 1.3754149675369263, R2 0.3140053451061249\n",
      "Eval loss 1.4447520971298218, R2 0.3257492780685425\n",
      "epoch 506, loss 1.3735651969909668, R2 0.31505686044692993\n",
      "Eval loss 1.4428529739379883, R2 0.326750248670578\n",
      "epoch 507, loss 1.3717223405838013, R2 0.316104918718338\n",
      "Eval loss 1.44096040725708, R2 0.3277478516101837\n",
      "epoch 508, loss 1.369886040687561, R2 0.31714943051338196\n",
      "Eval loss 1.4390747547149658, R2 0.32874229550361633\n",
      "epoch 509, loss 1.3680566549301147, R2 0.31819045543670654\n",
      "Eval loss 1.4371956586837769, R2 0.32973340153694153\n",
      "epoch 510, loss 1.3662338256835938, R2 0.3192279636859894\n",
      "Eval loss 1.4353233575820923, R2 0.330721378326416\n",
      "epoch 511, loss 1.3644176721572876, R2 0.32026204466819763\n",
      "Eval loss 1.433457612991333, R2 0.33170604705810547\n",
      "epoch 512, loss 1.3626081943511963, R2 0.32129260897636414\n",
      "Eval loss 1.4315985441207886, R2 0.33268749713897705\n",
      "epoch 513, loss 1.3608053922653198, R2 0.32231977581977844\n",
      "Eval loss 1.429746150970459, R2 0.33366572856903076\n",
      "epoch 514, loss 1.359009027481079, R2 0.3233434855937958\n",
      "Eval loss 1.4279000759124756, R2 0.33464086055755615\n",
      "epoch 515, loss 1.3572193384170532, R2 0.32436373829841614\n",
      "Eval loss 1.426060676574707, R2 0.3356127440929413\n",
      "epoch 516, loss 1.355436086654663, R2 0.32538068294525146\n",
      "Eval loss 1.4242278337478638, R2 0.3365814983844757\n",
      "epoch 517, loss 1.3536593914031982, R2 0.32639411091804504\n",
      "Eval loss 1.4224014282226562, R2 0.33754703402519226\n",
      "epoch 518, loss 1.3518892526626587, R2 0.3274042308330536\n",
      "Eval loss 1.420581579208374, R2 0.3385095000267029\n",
      "epoch 519, loss 1.3501254320144653, R2 0.32841092348098755\n",
      "Eval loss 1.4187681674957275, R2 0.33946874737739563\n",
      "epoch 520, loss 1.3483680486679077, R2 0.32941436767578125\n",
      "Eval loss 1.4169610738754272, R2 0.34042495489120483\n",
      "epoch 521, loss 1.3466171026229858, R2 0.33041438460350037\n",
      "Eval loss 1.4151604175567627, R2 0.34137800335884094\n",
      "epoch 522, loss 1.3448725938796997, R2 0.33141109347343445\n",
      "Eval loss 1.4133660793304443, R2 0.34232795238494873\n",
      "epoch 523, loss 1.3431344032287598, R2 0.3324044942855835\n",
      "Eval loss 1.4115781784057617, R2 0.3432748317718506\n",
      "epoch 524, loss 1.341402530670166, R2 0.33339452743530273\n",
      "Eval loss 1.4097964763641357, R2 0.3442186117172241\n",
      "epoch 525, loss 1.3396769762039185, R2 0.3343812823295593\n",
      "Eval loss 1.408021092414856, R2 0.3451593220233917\n",
      "epoch 526, loss 1.3379576206207275, R2 0.33536478877067566\n",
      "Eval loss 1.4062519073486328, R2 0.34609705209732056\n",
      "epoch 527, loss 1.3362444639205933, R2 0.33634501695632935\n",
      "Eval loss 1.4044891595840454, R2 0.34703171253204346\n",
      "epoch 528, loss 1.3345376253128052, R2 0.33732202649116516\n",
      "Eval loss 1.402732491493225, R2 0.3479633629322052\n",
      "epoch 529, loss 1.3328371047973633, R2 0.33829575777053833\n",
      "Eval loss 1.4009820222854614, R2 0.348891943693161\n",
      "epoch 530, loss 1.331142544746399, R2 0.33926627039909363\n",
      "Eval loss 1.3992377519607544, R2 0.34981754422187805\n",
      "epoch 531, loss 1.3294541835784912, R2 0.34023353457450867\n",
      "Eval loss 1.397499680519104, R2 0.3507401645183563\n",
      "epoch 532, loss 1.3277720212936401, R2 0.3411976099014282\n",
      "Eval loss 1.3957674503326416, R2 0.35165974497795105\n",
      "epoch 533, loss 1.3260959386825562, R2 0.3421584367752075\n",
      "Eval loss 1.394041657447815, R2 0.3525763750076294\n",
      "epoch 534, loss 1.3244258165359497, R2 0.3431161344051361\n",
      "Eval loss 1.3923217058181763, R2 0.35349008440971375\n",
      "epoch 535, loss 1.3227618932724, R2 0.3440706431865692\n",
      "Eval loss 1.3906078338623047, R2 0.3544008135795593\n",
      "epoch 536, loss 1.321103811264038, R2 0.3450220227241516\n",
      "Eval loss 1.3889001607894897, R2 0.3553086221218109\n",
      "epoch 537, loss 1.319451928138733, R2 0.3459702432155609\n",
      "Eval loss 1.3871983289718628, R2 0.3562134802341461\n",
      "epoch 538, loss 1.3178058862686157, R2 0.3469153642654419\n",
      "Eval loss 1.3855023384094238, R2 0.3571154475212097\n",
      "epoch 539, loss 1.316165804862976, R2 0.347857266664505\n",
      "Eval loss 1.3838125467300415, R2 0.35801446437835693\n",
      "epoch 540, loss 1.3145318031311035, R2 0.3487960994243622\n",
      "Eval loss 1.3821285963058472, R2 0.35891059041023254\n",
      "epoch 541, loss 1.312903642654419, R2 0.3497318625450134\n",
      "Eval loss 1.3804504871368408, R2 0.35980379581451416\n",
      "epoch 542, loss 1.3112812042236328, R2 0.35066449642181396\n",
      "Eval loss 1.3787784576416016, R2 0.3606942296028137\n",
      "epoch 543, loss 1.3096647262573242, R2 0.35159415006637573\n",
      "Eval loss 1.3771120309829712, R2 0.3615817129611969\n",
      "epoch 544, loss 1.3080542087554932, R2 0.35252058506011963\n",
      "Eval loss 1.3754515647888184, R2 0.36246639490127563\n",
      "epoch 545, loss 1.3064494132995605, R2 0.35344403982162476\n",
      "Eval loss 1.373796820640564, R2 0.36334821581840515\n",
      "epoch 546, loss 1.3048503398895264, R2 0.3543644845485687\n",
      "Eval loss 1.372148036956787, R2 0.36422717571258545\n",
      "epoch 547, loss 1.3032571077346802, R2 0.35528185963630676\n",
      "Eval loss 1.3705048561096191, R2 0.3651033639907837\n",
      "epoch 548, loss 1.3016695976257324, R2 0.35619619488716125\n",
      "Eval loss 1.3688675165176392, R2 0.36597663164138794\n",
      "epoch 549, loss 1.300087809562683, R2 0.3571075201034546\n",
      "Eval loss 1.3672358989715576, R2 0.3668472468852997\n",
      "epoch 550, loss 1.2985117435455322, R2 0.35801592469215393\n",
      "Eval loss 1.365609884262085, R2 0.3677149713039398\n",
      "epoch 551, loss 1.2969411611557007, R2 0.35892125964164734\n",
      "Eval loss 1.3639895915985107, R2 0.3685799539089203\n",
      "epoch 552, loss 1.2953764200210571, R2 0.35982370376586914\n",
      "Eval loss 1.362375020980835, R2 0.3694421648979187\n",
      "epoch 553, loss 1.2938172817230225, R2 0.3607231676578522\n",
      "Eval loss 1.3607659339904785, R2 0.37030160427093506\n",
      "epoch 554, loss 1.2922636270523071, R2 0.36161962151527405\n",
      "Eval loss 1.3591625690460205, R2 0.37115830183029175\n",
      "epoch 555, loss 1.2907156944274902, R2 0.3625131845474243\n",
      "Eval loss 1.3575648069381714, R2 0.3720122277736664\n",
      "epoch 556, loss 1.2891733646392822, R2 0.3634037673473358\n",
      "Eval loss 1.3559725284576416, R2 0.3728634715080261\n",
      "epoch 557, loss 1.287636399269104, R2 0.3642914593219757\n",
      "Eval loss 1.3543859720230103, R2 0.3737119734287262\n",
      "epoch 558, loss 1.2861050367355347, R2 0.3651762008666992\n",
      "Eval loss 1.3528046607971191, R2 0.374557763338089\n",
      "epoch 559, loss 1.2845791578292847, R2 0.3660581409931183\n",
      "Eval loss 1.3512290716171265, R2 0.3754008412361145\n",
      "epoch 560, loss 1.283059000968933, R2 0.36693716049194336\n",
      "Eval loss 1.3496588468551636, R2 0.3762412369251251\n",
      "epoch 561, loss 1.2815438508987427, R2 0.36781322956085205\n",
      "Eval loss 1.34809410572052, R2 0.37707892060279846\n",
      "epoch 562, loss 1.2800344228744507, R2 0.3686864972114563\n",
      "Eval loss 1.3465348482131958, R2 0.37791404128074646\n",
      "epoch 563, loss 1.2785303592681885, R2 0.3695569336414337\n",
      "Eval loss 1.3449809551239014, R2 0.3787464201450348\n",
      "epoch 564, loss 1.277031660079956, R2 0.3704244792461395\n",
      "Eval loss 1.3434323072433472, R2 0.3795761168003082\n",
      "epoch 565, loss 1.2755382061004639, R2 0.3712892532348633\n",
      "Eval loss 1.3418893814086914, R2 0.38040322065353394\n",
      "epoch 566, loss 1.2740503549575806, R2 0.37215110659599304\n",
      "Eval loss 1.3403515815734863, R2 0.38122767210006714\n",
      "epoch 567, loss 1.2725675106048584, R2 0.3730102479457855\n",
      "Eval loss 1.3388190269470215, R2 0.38204947113990784\n",
      "epoch 568, loss 1.2710901498794556, R2 0.3738665282726288\n",
      "Eval loss 1.3372920751571655, R2 0.3828687369823456\n",
      "epoch 569, loss 1.269618034362793, R2 0.3747200667858124\n",
      "Eval loss 1.3357702493667603, R2 0.3836853802204132\n",
      "epoch 570, loss 1.2681512832641602, R2 0.37557074427604675\n",
      "Eval loss 1.3342537879943848, R2 0.38449934124946594\n",
      "epoch 571, loss 1.266689658164978, R2 0.37641870975494385\n",
      "Eval loss 1.3327423334121704, R2 0.3853108584880829\n",
      "epoch 572, loss 1.2652332782745361, R2 0.3772638440132141\n",
      "Eval loss 1.3312362432479858, R2 0.38611966371536255\n",
      "epoch 573, loss 1.263782024383545, R2 0.37810632586479187\n",
      "Eval loss 1.3297353982925415, R2 0.38692599534988403\n",
      "epoch 574, loss 1.262336015701294, R2 0.3789460361003876\n",
      "Eval loss 1.3282397985458374, R2 0.3877297639846802\n",
      "epoch 575, loss 1.2608952522277832, R2 0.379783034324646\n",
      "Eval loss 1.326749324798584, R2 0.3885309398174286\n",
      "epoch 576, loss 1.2594594955444336, R2 0.3806172311306\n",
      "Eval loss 1.3252638578414917, R2 0.38932961225509644\n",
      "epoch 577, loss 1.2580288648605347, R2 0.3814488351345062\n",
      "Eval loss 1.32378351688385, R2 0.39012575149536133\n",
      "epoch 578, loss 1.2566033601760864, R2 0.3822776675224304\n",
      "Eval loss 1.3223085403442383, R2 0.39091941714286804\n",
      "epoch 579, loss 1.2551831007003784, R2 0.3831038177013397\n",
      "Eval loss 1.320838451385498, R2 0.391710489988327\n",
      "epoch 580, loss 1.253767490386963, R2 0.3839273154735565\n",
      "Eval loss 1.319373369216919, R2 0.39249908924102783\n",
      "epoch 581, loss 1.2523572444915771, R2 0.38474807143211365\n",
      "Eval loss 1.31791353225708, R2 0.39328524470329285\n",
      "epoch 582, loss 1.2509517669677734, R2 0.38556623458862305\n",
      "Eval loss 1.3164585828781128, R2 0.3940688371658325\n",
      "epoch 583, loss 1.24955153465271, R2 0.38638174533843994\n",
      "Eval loss 1.3150087594985962, R2 0.3948500454425812\n",
      "epoch 584, loss 1.248156189918518, R2 0.3871946334838867\n",
      "Eval loss 1.3135638236999512, R2 0.39562875032424927\n",
      "epoch 585, loss 1.2467657327651978, R2 0.388004869222641\n",
      "Eval loss 1.3121238946914673, R2 0.39640501141548157\n",
      "epoch 586, loss 1.2453802824020386, R2 0.38881245255470276\n",
      "Eval loss 1.3106889724731445, R2 0.3971787393093109\n",
      "epoch 587, loss 1.2439996004104614, R2 0.3896174132823944\n",
      "Eval loss 1.3092588186264038, R2 0.397950142621994\n",
      "epoch 588, loss 1.2426239252090454, R2 0.3904198706150055\n",
      "Eval loss 1.3078337907791138, R2 0.39871910214424133\n",
      "epoch 589, loss 1.241253137588501, R2 0.3912196457386017\n",
      "Eval loss 1.3064135313034058, R2 0.39948558807373047\n",
      "epoch 590, loss 1.2398872375488281, R2 0.3920169174671173\n",
      "Eval loss 1.3049982786178589, R2 0.4002496898174286\n",
      "epoch 591, loss 1.2385261058807373, R2 0.39281150698661804\n",
      "Eval loss 1.3035876750946045, R2 0.4010113775730133\n",
      "epoch 592, loss 1.2371699810028076, R2 0.3936036229133606\n",
      "Eval loss 1.3021820783615112, R2 0.401770681142807\n",
      "epoch 593, loss 1.2358185052871704, R2 0.3943932354450226\n",
      "Eval loss 1.30078125, R2 0.4025276303291321\n",
      "epoch 594, loss 1.2344716787338257, R2 0.39518019556999207\n",
      "Eval loss 1.2993853092193604, R2 0.40328216552734375\n",
      "epoch 595, loss 1.2331297397613525, R2 0.3959646224975586\n",
      "Eval loss 1.2979940176010132, R2 0.4040343463420868\n",
      "epoch 596, loss 1.2317925691604614, R2 0.39674660563468933\n",
      "Eval loss 1.2966073751449585, R2 0.4047841727733612\n",
      "epoch 597, loss 1.2304601669311523, R2 0.39752596616744995\n",
      "Eval loss 1.2952256202697754, R2 0.4055316150188446\n",
      "epoch 598, loss 1.2291324138641357, R2 0.39830291271209717\n",
      "Eval loss 1.2938487529754639, R2 0.40627679228782654\n",
      "epoch 599, loss 1.2278093099594116, R2 0.39907732605934143\n",
      "Eval loss 1.2924764156341553, R2 0.40701958537101746\n",
      "epoch 600, loss 1.2264909744262695, R2 0.3998492956161499\n",
      "Eval loss 1.2911087274551392, R2 0.40776002407073975\n",
      "epoch 601, loss 1.2251771688461304, R2 0.4006187319755554\n",
      "Eval loss 1.2897456884384155, R2 0.40849819779396057\n",
      "epoch 602, loss 1.2238681316375732, R2 0.40138569474220276\n",
      "Eval loss 1.288387417793274, R2 0.40923401713371277\n",
      "epoch 603, loss 1.2225637435913086, R2 0.4021502435207367\n",
      "Eval loss 1.2870339155197144, R2 0.4099675416946411\n",
      "epoch 604, loss 1.2212637662887573, R2 0.40291231870651245\n",
      "Eval loss 1.2856847047805786, R2 0.410698801279068\n",
      "epoch 605, loss 1.2199684381484985, R2 0.4036719501018524\n",
      "Eval loss 1.2843403816223145, R2 0.4114277958869934\n",
      "epoch 606, loss 1.2186777591705322, R2 0.4044291377067566\n",
      "Eval loss 1.2830005884170532, R2 0.4121544659137726\n",
      "epoch 607, loss 1.2173913717269897, R2 0.40518391132354736\n",
      "Eval loss 1.2816652059555054, R2 0.4128789007663727\n",
      "epoch 608, loss 1.2161097526550293, R2 0.40593624114990234\n",
      "Eval loss 1.28033447265625, R2 0.4136011004447937\n",
      "epoch 609, loss 1.2148325443267822, R2 0.4066861867904663\n",
      "Eval loss 1.2790082693099976, R2 0.4143209755420685\n",
      "epoch 610, loss 1.2135599851608276, R2 0.40743377804756165\n",
      "Eval loss 1.277686595916748, R2 0.4150386154651642\n",
      "epoch 611, loss 1.2122917175292969, R2 0.4081788957118988\n",
      "Eval loss 1.2763694524765015, R2 0.4157540500164032\n",
      "epoch 612, loss 1.211027979850769, R2 0.4089216887950897\n",
      "Eval loss 1.2750566005706787, R2 0.4164673089981079\n",
      "epoch 613, loss 1.209768533706665, R2 0.40966206789016724\n",
      "Eval loss 1.2737483978271484, R2 0.41717833280563354\n",
      "epoch 614, loss 1.208513617515564, R2 0.4104001224040985\n",
      "Eval loss 1.2724446058273315, R2 0.4178870916366577\n",
      "epoch 615, loss 1.2072631120681763, R2 0.41113585233688354\n",
      "Eval loss 1.271145224571228, R2 0.41859370470046997\n",
      "epoch 616, loss 1.206017255783081, R2 0.4118691682815552\n",
      "Eval loss 1.2698501348495483, R2 0.41929808259010315\n",
      "epoch 617, loss 1.204775333404541, R2 0.41260024905204773\n",
      "Eval loss 1.2685596942901611, R2 0.420000284910202\n",
      "epoch 618, loss 1.203537940979004, R2 0.4133289158344269\n",
      "Eval loss 1.2672734260559082, R2 0.420700341463089\n",
      "epoch 619, loss 1.2023049592971802, R2 0.4140552580356598\n",
      "Eval loss 1.2659915685653687, R2 0.42139819264411926\n",
      "epoch 620, loss 1.2010763883590698, R2 0.41477930545806885\n",
      "Eval loss 1.2647141218185425, R2 0.4220938980579376\n",
      "epoch 621, loss 1.1998518705368042, R2 0.41550108790397644\n",
      "Eval loss 1.2634408473968506, R2 0.42278745770454407\n",
      "epoch 622, loss 1.1986315250396729, R2 0.4162205755710602\n",
      "Eval loss 1.262171983718872, R2 0.4234788119792938\n",
      "epoch 623, loss 1.1974157094955444, R2 0.4169377088546753\n",
      "Eval loss 1.260907530784607, R2 0.42416802048683167\n",
      "epoch 624, loss 1.1962039470672607, R2 0.41765257716178894\n",
      "Eval loss 1.259647250175476, R2 0.42485520243644714\n",
      "epoch 625, loss 1.1949965953826904, R2 0.4183652102947235\n",
      "Eval loss 1.2583911418914795, R2 0.42554014921188354\n",
      "epoch 626, loss 1.193793535232544, R2 0.41907551884651184\n",
      "Eval loss 1.2571393251419067, R2 0.4262230694293976\n",
      "epoch 627, loss 1.1925944089889526, R2 0.41978368163108826\n",
      "Eval loss 1.2558916807174683, R2 0.4269038140773773\n",
      "epoch 628, loss 1.1913996934890747, R2 0.42048949003219604\n",
      "Eval loss 1.2546484470367432, R2 0.42758244276046753\n",
      "epoch 629, loss 1.1902090311050415, R2 0.4211931526660919\n",
      "Eval loss 1.2534093856811523, R2 0.42825907468795776\n",
      "epoch 630, loss 1.1890225410461426, R2 0.42189452052116394\n",
      "Eval loss 1.2521743774414062, R2 0.42893359065055847\n",
      "epoch 631, loss 1.1878401041030884, R2 0.42259371280670166\n",
      "Eval loss 1.2509435415267944, R2 0.4296059310436249\n",
      "epoch 632, loss 1.186661958694458, R2 0.4232906401157379\n",
      "Eval loss 1.2497167587280273, R2 0.4302762746810913\n",
      "epoch 633, loss 1.1854878664016724, R2 0.42398545145988464\n",
      "Eval loss 1.2484943866729736, R2 0.4309445321559906\n",
      "epoch 634, loss 1.184317708015442, R2 0.4246779680252075\n",
      "Eval loss 1.2472760677337646, R2 0.43161076307296753\n",
      "epoch 635, loss 1.1831518411636353, R2 0.42536839842796326\n",
      "Eval loss 1.2460615634918213, R2 0.4322749078273773\n",
      "epoch 636, loss 1.1819899082183838, R2 0.42605650424957275\n",
      "Eval loss 1.2448513507843018, R2 0.43293702602386475\n",
      "epoch 637, loss 1.180832028388977, R2 0.4267425537109375\n",
      "Eval loss 1.243645191192627, R2 0.4335970878601074\n",
      "epoch 638, loss 1.1796783208847046, R2 0.42742642760276794\n",
      "Eval loss 1.2424432039260864, R2 0.4342551529407501\n",
      "epoch 639, loss 1.1785284280776978, R2 0.4281081259250641\n",
      "Eval loss 1.241245150566101, R2 0.43491122126579285\n",
      "epoch 640, loss 1.1773825883865356, R2 0.4287876784801483\n",
      "Eval loss 1.2400511503219604, R2 0.43556520342826843\n",
      "epoch 641, loss 1.1762408018112183, R2 0.42946508526802063\n",
      "Eval loss 1.2388612031936646, R2 0.4362172782421112\n",
      "epoch 642, loss 1.1751028299331665, R2 0.43014031648635864\n",
      "Eval loss 1.2376751899719238, R2 0.43686723709106445\n",
      "epoch 643, loss 1.1739689111709595, R2 0.4308134913444519\n",
      "Eval loss 1.2364931106567383, R2 0.4375152885913849\n",
      "epoch 644, loss 1.172838807106018, R2 0.43148452043533325\n",
      "Eval loss 1.235315203666687, R2 0.43816134333610535\n",
      "epoch 645, loss 1.1717127561569214, R2 0.4321534335613251\n",
      "Eval loss 1.2341409921646118, R2 0.43880537152290344\n",
      "epoch 646, loss 1.1705906391143799, R2 0.43282029032707214\n",
      "Eval loss 1.2329707145690918, R2 0.4394474923610687\n",
      "epoch 647, loss 1.169472336769104, R2 0.4334849417209625\n",
      "Eval loss 1.231804370880127, R2 0.44008758664131165\n",
      "epoch 648, loss 1.1683577299118042, R2 0.43414756655693054\n",
      "Eval loss 1.2306420803070068, R2 0.440725713968277\n",
      "epoch 649, loss 1.1672472953796387, R2 0.4348081350326538\n",
      "Eval loss 1.2294836044311523, R2 0.4413619339466095\n",
      "epoch 650, loss 1.1661404371261597, R2 0.4354666471481323\n",
      "Eval loss 1.228328824043274, R2 0.4419962763786316\n",
      "epoch 651, loss 1.1650376319885254, R2 0.43612298369407654\n",
      "Eval loss 1.2271780967712402, R2 0.4426285922527313\n",
      "epoch 652, loss 1.1639385223388672, R2 0.43677738308906555\n",
      "Eval loss 1.2260313034057617, R2 0.44325894117355347\n",
      "epoch 653, loss 1.1628432273864746, R2 0.4374296963214874\n",
      "Eval loss 1.2248883247375488, R2 0.4438874423503876\n",
      "epoch 654, loss 1.1617517471313477, R2 0.43807995319366455\n",
      "Eval loss 1.223749041557312, R2 0.44451406598091125\n",
      "epoch 655, loss 1.1606638431549072, R2 0.4387281537055969\n",
      "Eval loss 1.2226135730743408, R2 0.44513872265815735\n",
      "epoch 656, loss 1.1595799922943115, R2 0.4393743574619293\n",
      "Eval loss 1.2214819192886353, R2 0.44576141238212585\n",
      "epoch 657, loss 1.1584997177124023, R2 0.44001856446266174\n",
      "Eval loss 1.2203540802001953, R2 0.4463823139667511\n",
      "epoch 658, loss 1.1574232578277588, R2 0.4406607449054718\n",
      "Eval loss 1.2192299365997314, R2 0.44700127840042114\n",
      "epoch 659, loss 1.1563503742218018, R2 0.4413008987903595\n",
      "Eval loss 1.2181094884872437, R2 0.447618305683136\n",
      "epoch 660, loss 1.1552813053131104, R2 0.44193899631500244\n",
      "Eval loss 1.2169928550720215, R2 0.44823357462882996\n",
      "epoch 661, loss 1.154215693473816, R2 0.4425751864910126\n",
      "Eval loss 1.2158799171447754, R2 0.4488469362258911\n",
      "epoch 662, loss 1.153153896331787, R2 0.44320937991142273\n",
      "Eval loss 1.2147706747055054, R2 0.44945842027664185\n",
      "epoch 663, loss 1.152095913887024, R2 0.4438415765762329\n",
      "Eval loss 1.2136651277542114, R2 0.45006799697875977\n",
      "epoch 664, loss 1.1510415077209473, R2 0.4444717466831207\n",
      "Eval loss 1.2125632762908936, R2 0.4506757855415344\n",
      "epoch 665, loss 1.1499905586242676, R2 0.4451000392436981\n",
      "Eval loss 1.2114650011062622, R2 0.45128172636032104\n",
      "epoch 666, loss 1.148943305015564, R2 0.4457263648509979\n",
      "Eval loss 1.210370421409607, R2 0.45188581943511963\n",
      "epoch 667, loss 1.1478996276855469, R2 0.44635069370269775\n",
      "Eval loss 1.2092796564102173, R2 0.4524880647659302\n",
      "epoch 668, loss 1.1468595266342163, R2 0.44697311520576477\n",
      "Eval loss 1.208192229270935, R2 0.45308855175971985\n",
      "epoch 669, loss 1.1458231210708618, R2 0.44759368896484375\n",
      "Eval loss 1.207108497619629, R2 0.4536871612071991\n",
      "epoch 670, loss 1.1447901725769043, R2 0.4482121765613556\n",
      "Eval loss 1.2060284614562988, R2 0.4542839825153351\n",
      "epoch 671, loss 1.1437608003616333, R2 0.448828786611557\n",
      "Eval loss 1.2049517631530762, R2 0.4548789858818054\n",
      "epoch 672, loss 1.1427350044250488, R2 0.4494435489177704\n",
      "Eval loss 1.2038788795471191, R2 0.4554721713066101\n",
      "epoch 673, loss 1.1417126655578613, R2 0.4500563442707062\n",
      "Eval loss 1.202809453010559, R2 0.4560636579990387\n",
      "epoch 674, loss 1.1406936645507812, R2 0.45066729187965393\n",
      "Eval loss 1.2017436027526855, R2 0.4566532373428345\n",
      "epoch 675, loss 1.1396784782409668, R2 0.4512762725353241\n",
      "Eval loss 1.200681209564209, R2 0.45724114775657654\n",
      "epoch 676, loss 1.1386665105819702, R2 0.4518834054470062\n",
      "Eval loss 1.199622392654419, R2 0.45782721042633057\n",
      "epoch 677, loss 1.1376579999923706, R2 0.45248860120773315\n",
      "Eval loss 1.1985671520233154, R2 0.4584115743637085\n",
      "epoch 678, loss 1.136653184890747, R2 0.45309197902679443\n",
      "Eval loss 1.1975152492523193, R2 0.4589940905570984\n",
      "epoch 679, loss 1.1356515884399414, R2 0.4536934792995453\n",
      "Eval loss 1.1964669227600098, R2 0.4595749080181122\n",
      "epoch 680, loss 1.1346535682678223, R2 0.4542931318283081\n",
      "Eval loss 1.1954219341278076, R2 0.4601540267467499\n",
      "epoch 681, loss 1.133658766746521, R2 0.4548909366130829\n",
      "Eval loss 1.194380521774292, R2 0.46073129773139954\n",
      "epoch 682, loss 1.1326676607131958, R2 0.45548686385154724\n",
      "Eval loss 1.1933424472808838, R2 0.4613068401813507\n",
      "epoch 683, loss 1.131679654121399, R2 0.45608094334602356\n",
      "Eval loss 1.1923078298568726, R2 0.4618806838989258\n",
      "epoch 684, loss 1.1306949853897095, R2 0.4566732347011566\n",
      "Eval loss 1.1912766695022583, R2 0.46245279908180237\n",
      "epoch 685, loss 1.129713773727417, R2 0.45726364850997925\n",
      "Eval loss 1.1902488470077515, R2 0.46302318572998047\n",
      "epoch 686, loss 1.1287360191345215, R2 0.4578522741794586\n",
      "Eval loss 1.1892244815826416, R2 0.46359187364578247\n",
      "epoch 687, loss 1.1277614831924438, R2 0.45843905210494995\n",
      "Eval loss 1.1882033348083496, R2 0.46415889263153076\n",
      "epoch 688, loss 1.1267902851104736, R2 0.45902398228645325\n",
      "Eval loss 1.1871857643127441, R2 0.46472418308258057\n",
      "epoch 689, loss 1.1258224248886108, R2 0.45960715413093567\n",
      "Eval loss 1.186171293258667, R2 0.4652877748012543\n",
      "epoch 690, loss 1.1248579025268555, R2 0.4601885974407196\n",
      "Eval loss 1.1851602792739868, R2 0.46584969758987427\n",
      "epoch 691, loss 1.123896598815918, R2 0.4607681930065155\n",
      "Eval loss 1.1841527223587036, R2 0.46640995144844055\n",
      "epoch 692, loss 1.1229385137557983, R2 0.46134597063064575\n",
      "Eval loss 1.1831482648849487, R2 0.46696844696998596\n",
      "epoch 693, loss 1.1219837665557861, R2 0.4619220793247223\n",
      "Eval loss 1.1821471452713013, R2 0.4675253629684448\n",
      "epoch 694, loss 1.1210322380065918, R2 0.4624963104724884\n",
      "Eval loss 1.1811493635177612, R2 0.4680805206298828\n",
      "epoch 695, loss 1.1200839281082153, R2 0.4630688428878784\n",
      "Eval loss 1.1801549196243286, R2 0.46863409876823425\n",
      "epoch 696, loss 1.1191388368606567, R2 0.4636395573616028\n",
      "Eval loss 1.1791635751724243, R2 0.4691859483718872\n",
      "epoch 697, loss 1.118196964263916, R2 0.4642085134983063\n",
      "Eval loss 1.1781755685806274, R2 0.4697362184524536\n",
      "epoch 698, loss 1.1172583103179932, R2 0.46477583050727844\n",
      "Eval loss 1.177190899848938, R2 0.4702848494052887\n",
      "epoch 699, loss 1.1163227558135986, R2 0.4653412997722626\n",
      "Eval loss 1.1762092113494873, R2 0.47083184123039246\n",
      "epoch 700, loss 1.115390419960022, R2 0.46590515971183777\n",
      "Eval loss 1.175230860710144, R2 0.4713771939277649\n",
      "epoch 701, loss 1.1144613027572632, R2 0.4664671719074249\n",
      "Eval loss 1.1742557287216187, R2 0.4719208776950836\n",
      "epoch 702, loss 1.1135352849960327, R2 0.467027485370636\n",
      "Eval loss 1.1732836961746216, R2 0.4724629521369934\n",
      "epoch 703, loss 1.1126123666763306, R2 0.46758607029914856\n",
      "Eval loss 1.172315001487732, R2 0.47300347685813904\n",
      "epoch 704, loss 1.1116926670074463, R2 0.4681429862976074\n",
      "Eval loss 1.171349287033081, R2 0.47354230284690857\n",
      "epoch 705, loss 1.1107759475708008, R2 0.4686981439590454\n",
      "Eval loss 1.1703869104385376, R2 0.47407960891723633\n",
      "epoch 706, loss 1.1098624467849731, R2 0.46925172209739685\n",
      "Eval loss 1.169427514076233, R2 0.47461527585983276\n",
      "epoch 707, loss 1.1089519262313843, R2 0.46980351209640503\n",
      "Eval loss 1.168471336364746, R2 0.47514933347702026\n",
      "epoch 708, loss 1.1080446243286133, R2 0.4703536033630371\n",
      "Eval loss 1.1675182580947876, R2 0.4756819009780884\n",
      "epoch 709, loss 1.107140302658081, R2 0.47090208530426025\n",
      "Eval loss 1.1665681600570679, R2 0.4762127995491028\n",
      "epoch 710, loss 1.1062389612197876, R2 0.4714488685131073\n",
      "Eval loss 1.1656213998794556, R2 0.476742148399353\n",
      "epoch 711, loss 1.1053407192230225, R2 0.47199395298957825\n",
      "Eval loss 1.1646775007247925, R2 0.4772699177265167\n",
      "epoch 712, loss 1.1044456958770752, R2 0.47253739833831787\n",
      "Eval loss 1.1637368202209473, R2 0.47779616713523865\n",
      "epoch 713, loss 1.103553295135498, R2 0.47307920455932617\n",
      "Eval loss 1.1627992391586304, R2 0.4783208668231964\n",
      "epoch 714, loss 1.1026642322540283, R2 0.47361934185028076\n",
      "Eval loss 1.1618645191192627, R2 0.47884389758110046\n",
      "epoch 715, loss 1.1017780303955078, R2 0.47415781021118164\n",
      "Eval loss 1.1609328985214233, R2 0.47936543822288513\n",
      "epoch 716, loss 1.100894808769226, R2 0.47469469904899597\n",
      "Eval loss 1.1600044965744019, R2 0.4798854887485504\n",
      "epoch 717, loss 1.100014567375183, R2 0.4752299189567566\n",
      "Eval loss 1.1590789556503296, R2 0.48040398955345154\n",
      "epoch 718, loss 1.0991374254226685, R2 0.4757635295391083\n",
      "Eval loss 1.158156394958496, R2 0.4809209108352661\n",
      "epoch 719, loss 1.098263144493103, R2 0.47629547119140625\n",
      "Eval loss 1.1572368144989014, R2 0.4814363420009613\n",
      "epoch 720, loss 1.0973917245864868, R2 0.47682589292526245\n",
      "Eval loss 1.1563202142715454, R2 0.48195019364356995\n",
      "epoch 721, loss 1.0965232849121094, R2 0.47735461592674255\n",
      "Eval loss 1.1554065942764282, R2 0.482462614774704\n",
      "epoch 722, loss 1.0956577062606812, R2 0.4778817296028137\n",
      "Eval loss 1.1544960737228394, R2 0.48297345638275146\n",
      "epoch 723, loss 1.0947951078414917, R2 0.4784072935581207\n",
      "Eval loss 1.1535884141921997, R2 0.48348286747932434\n",
      "epoch 724, loss 1.093935489654541, R2 0.4789312481880188\n",
      "Eval loss 1.1526836156845093, R2 0.48399075865745544\n",
      "epoch 725, loss 1.09307861328125, R2 0.47945359349250793\n",
      "Eval loss 1.1517817974090576, R2 0.4844970703125\n",
      "epoch 726, loss 1.0922247171401978, R2 0.4799743890762329\n",
      "Eval loss 1.1508829593658447, R2 0.48500195145606995\n",
      "epoch 727, loss 1.0913735628128052, R2 0.4804936349391937\n",
      "Eval loss 1.1499868631362915, R2 0.4855053126811981\n",
      "epoch 728, loss 1.0905253887176514, R2 0.481011301279068\n",
      "Eval loss 1.1490938663482666, R2 0.4860072433948517\n",
      "epoch 729, loss 1.0896798372268677, R2 0.48152732849121094\n",
      "Eval loss 1.1482036113739014, R2 0.48650768399238586\n",
      "epoch 730, loss 1.0888373851776123, R2 0.4820418655872345\n",
      "Eval loss 1.147316336631775, R2 0.48700666427612305\n",
      "epoch 731, loss 1.087997555732727, R2 0.4825548827648163\n",
      "Eval loss 1.1464316844940186, R2 0.48750418424606323\n",
      "epoch 732, loss 1.0871607065200806, R2 0.48306626081466675\n",
      "Eval loss 1.1455501317977905, R2 0.4880002737045288\n",
      "epoch 733, loss 1.0863264799118042, R2 0.48357611894607544\n",
      "Eval loss 1.1446713209152222, R2 0.48849478363990784\n",
      "epoch 734, loss 1.0854952335357666, R2 0.48408442735671997\n",
      "Eval loss 1.1437954902648926, R2 0.48898792266845703\n",
      "epoch 735, loss 1.0846666097640991, R2 0.48459121584892273\n",
      "Eval loss 1.1429224014282227, R2 0.489479660987854\n",
      "epoch 736, loss 1.0838408470153809, R2 0.4850965440273285\n",
      "Eval loss 1.1420520544052124, R2 0.48996981978416443\n",
      "epoch 737, loss 1.0830178260803223, R2 0.48560020327568054\n",
      "Eval loss 1.1411845684051514, R2 0.4904586672782898\n",
      "epoch 738, loss 1.0821974277496338, R2 0.48610246181488037\n",
      "Eval loss 1.14031982421875, R2 0.49094608426094055\n",
      "epoch 739, loss 1.081380009651184, R2 0.48660317063331604\n",
      "Eval loss 1.1394578218460083, R2 0.4914320707321167\n",
      "epoch 740, loss 1.0805649757385254, R2 0.4871024787425995\n",
      "Eval loss 1.1385986804962158, R2 0.49191656708717346\n",
      "epoch 741, loss 1.0797529220581055, R2 0.4876001477241516\n",
      "Eval loss 1.137742280960083, R2 0.4923997223377228\n",
      "epoch 742, loss 1.0789434909820557, R2 0.48809635639190674\n",
      "Eval loss 1.1368886232376099, R2 0.4928814172744751\n",
      "epoch 743, loss 1.0781368017196655, R2 0.4885910749435425\n",
      "Eval loss 1.1360375881195068, R2 0.49336177110671997\n",
      "epoch 744, loss 1.0773327350616455, R2 0.48908427357673645\n",
      "Eval loss 1.1351895332336426, R2 0.49384069442749023\n",
      "epoch 745, loss 1.0765314102172852, R2 0.4895760416984558\n",
      "Eval loss 1.1343441009521484, R2 0.4943181872367859\n",
      "epoch 746, loss 1.0757328271865845, R2 0.4900663197040558\n",
      "Eval loss 1.1335012912750244, R2 0.4947942793369293\n",
      "epoch 747, loss 1.0749367475509644, R2 0.49055516719818115\n",
      "Eval loss 1.13266122341156, R2 0.4952690303325653\n",
      "epoch 748, loss 1.0741432905197144, R2 0.49104252457618713\n",
      "Eval loss 1.1318238973617554, R2 0.49574241042137146\n",
      "epoch 749, loss 1.073352575302124, R2 0.49152839183807373\n",
      "Eval loss 1.1309891939163208, R2 0.4962144196033478\n",
      "epoch 750, loss 1.0725644826889038, R2 0.49201279878616333\n",
      "Eval loss 1.1301571130752563, R2 0.4966849982738495\n",
      "epoch 751, loss 1.0717788934707642, R2 0.4924958348274231\n",
      "Eval loss 1.1293278932571411, R2 0.49715423583984375\n",
      "epoch 752, loss 1.0709960460662842, R2 0.4929773211479187\n",
      "Eval loss 1.128501296043396, R2 0.4976221024990082\n",
      "epoch 753, loss 1.0702157020568848, R2 0.4934574365615845\n",
      "Eval loss 1.1276772022247314, R2 0.4980885684490204\n",
      "epoch 754, loss 1.0694379806518555, R2 0.493936151266098\n",
      "Eval loss 1.1268556118011475, R2 0.4985537528991699\n",
      "epoch 755, loss 1.0686630010604858, R2 0.4944133460521698\n",
      "Eval loss 1.1260367631912231, R2 0.49901753664016724\n",
      "epoch 756, loss 1.0678902864456177, R2 0.49488916993141174\n",
      "Eval loss 1.1252206563949585, R2 0.4994800388813019\n",
      "epoch 757, loss 1.0671203136444092, R2 0.4953635334968567\n",
      "Eval loss 1.1244070529937744, R2 0.4999411702156067\n",
      "epoch 758, loss 1.0663528442382812, R2 0.4958365559577942\n",
      "Eval loss 1.1235960721969604, R2 0.500400960445404\n",
      "epoch 759, loss 1.0655879974365234, R2 0.4963081479072571\n",
      "Eval loss 1.1227877140045166, R2 0.5008594393730164\n",
      "epoch 760, loss 1.0648256540298462, R2 0.496778279542923\n",
      "Eval loss 1.1219818592071533, R2 0.5013165473937988\n",
      "epoch 761, loss 1.06406569480896, R2 0.49724704027175903\n",
      "Eval loss 1.1211786270141602, R2 0.5017723441123962\n",
      "epoch 762, loss 1.0633083581924438, R2 0.49771443009376526\n",
      "Eval loss 1.1203778982162476, R2 0.5022268891334534\n",
      "epoch 763, loss 1.0625535249710083, R2 0.4981803894042969\n",
      "Eval loss 1.1195796728134155, R2 0.5026800632476807\n",
      "epoch 764, loss 1.0618011951446533, R2 0.4986449182033539\n",
      "Eval loss 1.1187840700149536, R2 0.5031319260597229\n",
      "epoch 765, loss 1.061051368713379, R2 0.49910813570022583\n",
      "Eval loss 1.1179910898208618, R2 0.5035824179649353\n",
      "epoch 766, loss 1.0603039264678955, R2 0.49956998229026794\n",
      "Eval loss 1.117200493812561, R2 0.5040317177772522\n",
      "epoch 767, loss 1.0595589876174927, R2 0.5000304579734802\n",
      "Eval loss 1.1164125204086304, R2 0.5044796466827393\n",
      "epoch 768, loss 1.0588164329528809, R2 0.5004895329475403\n",
      "Eval loss 1.1156268119812012, R2 0.5049264430999756\n",
      "epoch 769, loss 1.0580763816833496, R2 0.5009472370147705\n",
      "Eval loss 1.114843726158142, R2 0.5053717494010925\n",
      "epoch 770, loss 1.057338833808899, R2 0.5014036893844604\n",
      "Eval loss 1.1140632629394531, R2 0.505815863609314\n",
      "epoch 771, loss 1.0566036701202393, R2 0.501858651638031\n",
      "Eval loss 1.1132850646972656, R2 0.5062587261199951\n",
      "epoch 772, loss 1.055870771408081, R2 0.5023123025894165\n",
      "Eval loss 1.1125093698501587, R2 0.5067002773284912\n",
      "epoch 773, loss 1.0551406145095825, R2 0.5027647018432617\n",
      "Eval loss 1.1117362976074219, R2 0.5071405172348022\n",
      "epoch 774, loss 1.054412603378296, R2 0.5032156109809875\n",
      "Eval loss 1.110965609550476, R2 0.5075795650482178\n",
      "epoch 775, loss 1.0536870956420898, R2 0.5036652684211731\n",
      "Eval loss 1.1101973056793213, R2 0.5080173015594482\n",
      "epoch 776, loss 1.0529639720916748, R2 0.5041136145591736\n",
      "Eval loss 1.109431266784668, R2 0.5084537863731384\n",
      "epoch 777, loss 1.0522431135177612, R2 0.5045605301856995\n",
      "Eval loss 1.1086678504943848, R2 0.5088890790939331\n",
      "epoch 778, loss 1.0515246391296387, R2 0.5050063133239746\n",
      "Eval loss 1.1079068183898926, R2 0.5093230605125427\n",
      "epoch 779, loss 1.0508086681365967, R2 0.5054506063461304\n",
      "Eval loss 1.107148289680481, R2 0.5097557902336121\n",
      "epoch 780, loss 1.0500949621200562, R2 0.5058937072753906\n",
      "Eval loss 1.1063920259475708, R2 0.5101873278617859\n",
      "epoch 781, loss 1.049383521080017, R2 0.5063353776931763\n",
      "Eval loss 1.1056381464004517, R2 0.5106176137924194\n",
      "epoch 782, loss 1.0486745834350586, R2 0.5067757964134216\n",
      "Eval loss 1.104886770248413, R2 0.5110466480255127\n",
      "epoch 783, loss 1.0479679107666016, R2 0.5072149634361267\n",
      "Eval loss 1.104137659072876, R2 0.5114744901657104\n",
      "epoch 784, loss 1.047263503074646, R2 0.5076528191566467\n",
      "Eval loss 1.1033909320831299, R2 0.5119010806083679\n",
      "epoch 785, loss 1.046561360359192, R2 0.5080894231796265\n",
      "Eval loss 1.1026463508605957, R2 0.5123264193534851\n",
      "epoch 786, loss 1.0458614826202393, R2 0.5085246562957764\n",
      "Eval loss 1.1019043922424316, R2 0.5127505660057068\n",
      "epoch 787, loss 1.0451641082763672, R2 0.5089585781097412\n",
      "Eval loss 1.101164698600769, R2 0.5131735801696777\n",
      "epoch 788, loss 1.0444687604904175, R2 0.5093913078308105\n",
      "Eval loss 1.1004273891448975, R2 0.5135953426361084\n",
      "epoch 789, loss 1.0437757968902588, R2 0.5098227858543396\n",
      "Eval loss 1.0996922254562378, R2 0.5140158534049988\n",
      "epoch 790, loss 1.0430850982666016, R2 0.5102528929710388\n",
      "Eval loss 1.0989594459533691, R2 0.5144352316856384\n",
      "epoch 791, loss 1.0423966646194458, R2 0.5106818079948425\n",
      "Eval loss 1.0982288122177124, R2 0.5148533582687378\n",
      "epoch 792, loss 1.041710615158081, R2 0.5111095309257507\n",
      "Eval loss 1.0975009202957153, R2 0.5152702927589417\n",
      "epoch 793, loss 1.0410265922546387, R2 0.5115358829498291\n",
      "Eval loss 1.0967750549316406, R2 0.51568603515625\n",
      "epoch 794, loss 1.0403449535369873, R2 0.511961042881012\n",
      "Eval loss 1.0960514545440674, R2 0.5161006450653076\n",
      "epoch 795, loss 1.0396653413772583, R2 0.5123849511146545\n",
      "Eval loss 1.0953301191329956, R2 0.5165140628814697\n",
      "epoch 796, loss 1.0389882326126099, R2 0.5128075480461121\n",
      "Eval loss 1.0946110486984253, R2 0.5169262290000916\n",
      "epoch 797, loss 1.0383130311965942, R2 0.5132290124893188\n",
      "Eval loss 1.0938942432403564, R2 0.5173372626304626\n",
      "epoch 798, loss 1.0376402139663696, R2 0.5136491060256958\n",
      "Eval loss 1.0931795835494995, R2 0.5177472233772278\n",
      "epoch 799, loss 1.0369694232940674, R2 0.514068067073822\n",
      "Eval loss 1.0924673080444336, R2 0.5181559920310974\n",
      "epoch 800, loss 1.0363010168075562, R2 0.5144858360290527\n",
      "Eval loss 1.0917571783065796, R2 0.5185635089874268\n",
      "epoch 801, loss 1.0356347560882568, R2 0.5149023532867432\n",
      "Eval loss 1.0910494327545166, R2 0.5189698934555054\n",
      "epoch 802, loss 1.0349705219268799, R2 0.5153176784515381\n",
      "Eval loss 1.090343713760376, R2 0.519375205039978\n",
      "epoch 803, loss 1.0343085527420044, R2 0.5157317519187927\n",
      "Eval loss 1.0896402597427368, R2 0.5197792649269104\n",
      "epoch 804, loss 1.0336488485336304, R2 0.5161445736885071\n",
      "Eval loss 1.0889389514923096, R2 0.5201822519302368\n",
      "epoch 805, loss 1.0329910516738892, R2 0.5165562033653259\n",
      "Eval loss 1.0882399082183838, R2 0.5205840468406677\n",
      "epoch 806, loss 1.0323354005813599, R2 0.516966700553894\n",
      "Eval loss 1.0875431299209595, R2 0.5209847092628479\n",
      "epoch 807, loss 1.031682014465332, R2 0.5173758864402771\n",
      "Eval loss 1.0868483781814575, R2 0.5213842988014221\n",
      "epoch 808, loss 1.0310307741165161, R2 0.5177839994430542\n",
      "Eval loss 1.0861557722091675, R2 0.5217826962471008\n",
      "epoch 809, loss 1.030381441116333, R2 0.518190860748291\n",
      "Eval loss 1.085465431213379, R2 0.5221800208091736\n",
      "epoch 810, loss 1.0297343730926514, R2 0.5185965895652771\n",
      "Eval loss 1.0847772359848022, R2 0.5225761532783508\n",
      "epoch 811, loss 1.0290894508361816, R2 0.5190010666847229\n",
      "Eval loss 1.084091067314148, R2 0.5229712128639221\n",
      "epoch 812, loss 1.0284464359283447, R2 0.5194044709205627\n",
      "Eval loss 1.0834071636199951, R2 0.5233651995658875\n",
      "epoch 813, loss 1.0278055667877197, R2 0.5198066234588623\n",
      "Eval loss 1.0827252864837646, R2 0.5237579941749573\n",
      "epoch 814, loss 1.0271668434143066, R2 0.5202075839042664\n",
      "Eval loss 1.082045555114746, R2 0.5241497755050659\n",
      "epoch 815, loss 1.026530146598816, R2 0.5206074714660645\n",
      "Eval loss 1.0813679695129395, R2 0.524540364742279\n",
      "epoch 816, loss 1.0258954763412476, R2 0.5210060477256775\n",
      "Eval loss 1.0806925296783447, R2 0.524929940700531\n",
      "epoch 817, loss 1.0252629518508911, R2 0.5214035511016846\n",
      "Eval loss 1.0800191164016724, R2 0.5253183245658875\n",
      "epoch 818, loss 1.0246323347091675, R2 0.5217999815940857\n",
      "Eval loss 1.079347848892212, R2 0.5257055759429932\n",
      "epoch 819, loss 1.0240038633346558, R2 0.5221952199935913\n",
      "Eval loss 1.0786786079406738, R2 0.5260919332504272\n",
      "epoch 820, loss 1.0233772993087769, R2 0.5225892663002014\n",
      "Eval loss 1.0780115127563477, R2 0.526477038860321\n",
      "epoch 821, loss 1.0227528810501099, R2 0.5229821801185608\n",
      "Eval loss 1.0773464441299438, R2 0.5268611311912537\n",
      "epoch 822, loss 1.0221304893493652, R2 0.5233739614486694\n",
      "Eval loss 1.0766832828521729, R2 0.5272440910339355\n",
      "epoch 823, loss 1.021510124206543, R2 0.5237646102905273\n",
      "Eval loss 1.0760223865509033, R2 0.5276260375976562\n",
      "epoch 824, loss 1.020891547203064, R2 0.5241541266441345\n",
      "Eval loss 1.0753635168075562, R2 0.528006911277771\n",
      "epoch 825, loss 1.0202751159667969, R2 0.524542510509491\n",
      "Eval loss 1.0747065544128418, R2 0.528386652469635\n",
      "epoch 826, loss 1.0196607112884521, R2 0.5249297618865967\n",
      "Eval loss 1.0740517377853394, R2 0.5287653803825378\n",
      "epoch 827, loss 1.0190480947494507, R2 0.5253159403800964\n",
      "Eval loss 1.0733989477157593, R2 0.5291430354118347\n",
      "epoch 828, loss 1.0184375047683716, R2 0.5257009863853455\n",
      "Eval loss 1.072748064994812, R2 0.5295196175575256\n",
      "epoch 829, loss 1.0178290605545044, R2 0.526084840297699\n",
      "Eval loss 1.072099208831787, R2 0.5298951864242554\n",
      "epoch 830, loss 1.01722252368927, R2 0.5264677405357361\n",
      "Eval loss 1.0714526176452637, R2 0.5302696824073792\n",
      "epoch 831, loss 1.016617774963379, R2 0.5268494486808777\n",
      "Eval loss 1.0708078145980835, R2 0.5306431651115417\n",
      "epoch 832, loss 1.0160150527954102, R2 0.5272300839424133\n",
      "Eval loss 1.0701649188995361, R2 0.5310155749320984\n",
      "epoch 833, loss 1.0154143571853638, R2 0.5276095867156982\n",
      "Eval loss 1.0695241689682007, R2 0.5313869118690491\n",
      "epoch 834, loss 1.0148155689239502, R2 0.5279880166053772\n",
      "Eval loss 1.068885326385498, R2 0.5317572951316833\n",
      "epoch 835, loss 1.0142185688018799, R2 0.5283653140068054\n",
      "Eval loss 1.0682483911514282, R2 0.5321266055107117\n",
      "epoch 836, loss 1.013623595237732, R2 0.5287415981292725\n",
      "Eval loss 1.0676136016845703, R2 0.5324947834014893\n",
      "epoch 837, loss 1.0130305290222168, R2 0.5291168093681335\n",
      "Eval loss 1.0669804811477661, R2 0.53286212682724\n",
      "epoch 838, loss 1.012439250946045, R2 0.5294908285140991\n",
      "Eval loss 1.0663495063781738, R2 0.5332282185554504\n",
      "epoch 839, loss 1.0118499994277954, R2 0.5298638939857483\n",
      "Eval loss 1.065720558166504, R2 0.533593475818634\n",
      "epoch 840, loss 1.0112626552581787, R2 0.530235767364502\n",
      "Eval loss 1.0650932788848877, R2 0.5339576005935669\n",
      "epoch 841, loss 1.0106769800186157, R2 0.530606746673584\n",
      "Eval loss 1.0644681453704834, R2 0.5343208312988281\n",
      "epoch 842, loss 1.0100932121276855, R2 0.5309764742851257\n",
      "Eval loss 1.0638447999954224, R2 0.5346829891204834\n",
      "epoch 843, loss 1.0095114707946777, R2 0.5313453078269958\n",
      "Eval loss 1.0632233619689941, R2 0.5350441336631775\n",
      "epoch 844, loss 1.0089315176010132, R2 0.5317129492759705\n",
      "Eval loss 1.0626038312911987, R2 0.5354042649269104\n",
      "epoch 845, loss 1.0083534717559814, R2 0.5320796370506287\n",
      "Eval loss 1.0619863271713257, R2 0.5357635021209717\n",
      "epoch 846, loss 1.007777214050293, R2 0.5324451923370361\n",
      "Eval loss 1.061370611190796, R2 0.5361216068267822\n",
      "epoch 847, loss 1.0072028636932373, R2 0.5328097343444824\n",
      "Eval loss 1.0607569217681885, R2 0.5364787578582764\n",
      "epoch 848, loss 1.0066301822662354, R2 0.5331732034683228\n",
      "Eval loss 1.0601449012756348, R2 0.5368349552154541\n",
      "epoch 849, loss 1.0060595273971558, R2 0.5335357189178467\n",
      "Eval loss 1.059535026550293, R2 0.5371901392936707\n",
      "epoch 850, loss 1.0054905414581299, R2 0.5338971018791199\n",
      "Eval loss 1.0589267015457153, R2 0.537544310092926\n",
      "epoch 851, loss 1.0049234628677368, R2 0.5342574715614319\n",
      "Eval loss 1.0583202838897705, R2 0.537897527217865\n",
      "epoch 852, loss 1.0043580532073975, R2 0.5346168875694275\n",
      "Eval loss 1.057715892791748, R2 0.5382497310638428\n",
      "epoch 853, loss 1.003794550895691, R2 0.5349751710891724\n",
      "Eval loss 1.0571132898330688, R2 0.5386009812355042\n",
      "epoch 854, loss 1.0032329559326172, R2 0.5353325605392456\n",
      "Eval loss 1.056512475013733, R2 0.5389513373374939\n",
      "epoch 855, loss 1.0026729106903076, R2 0.5356888175010681\n",
      "Eval loss 1.0559135675430298, R2 0.5393006205558777\n",
      "epoch 856, loss 1.0021148920059204, R2 0.5360441207885742\n",
      "Eval loss 1.05531644821167, R2 0.5396490693092346\n",
      "epoch 857, loss 1.001558542251587, R2 0.5363983511924744\n",
      "Eval loss 1.0547211170196533, R2 0.5399965047836304\n",
      "epoch 858, loss 1.0010038614273071, R2 0.5367516875267029\n",
      "Eval loss 1.054127812385559, R2 0.5403428673744202\n",
      "epoch 859, loss 1.0004509687423706, R2 0.5371038913726807\n",
      "Eval loss 1.0535361766815186, R2 0.5406883955001831\n",
      "epoch 860, loss 0.9998998641967773, R2 0.537455141544342\n",
      "Eval loss 1.0529463291168213, R2 0.5410329103469849\n",
      "epoch 861, loss 0.9993506073951721, R2 0.537805438041687\n",
      "Eval loss 1.0523581504821777, R2 0.541376531124115\n",
      "epoch 862, loss 0.9988030791282654, R2 0.538154661655426\n",
      "Eval loss 1.051771879196167, R2 0.5417191386222839\n",
      "epoch 863, loss 0.9982571005821228, R2 0.5385029911994934\n",
      "Eval loss 1.05118727684021, R2 0.5420607924461365\n",
      "epoch 864, loss 0.9977129697799683, R2 0.5388502478599548\n",
      "Eval loss 1.0506045818328857, R2 0.5424016118049622\n",
      "epoch 865, loss 0.997170627117157, R2 0.5391965508460999\n",
      "Eval loss 1.0500237941741943, R2 0.5427414178848267\n",
      "epoch 866, loss 0.9966298937797546, R2 0.5395418405532837\n",
      "Eval loss 1.0494444370269775, R2 0.5430802702903748\n",
      "epoch 867, loss 0.9960908889770508, R2 0.5398860573768616\n",
      "Eval loss 1.0488669872283936, R2 0.543418288230896\n",
      "epoch 868, loss 0.9955536127090454, R2 0.5402294397354126\n",
      "Eval loss 1.0482913255691528, R2 0.543755292892456\n",
      "epoch 869, loss 0.9950181245803833, R2 0.5405718684196472\n",
      "Eval loss 1.0477174520492554, R2 0.5440914034843445\n",
      "epoch 870, loss 0.9944841861724854, R2 0.5409132838249207\n",
      "Eval loss 1.0471452474594116, R2 0.5444265604019165\n",
      "epoch 871, loss 0.9939520359039307, R2 0.5412536859512329\n",
      "Eval loss 1.0465748310089111, R2 0.5447608828544617\n",
      "epoch 872, loss 0.9934214353561401, R2 0.5415931940078735\n",
      "Eval loss 1.046006202697754, R2 0.5450942516326904\n",
      "epoch 873, loss 0.9928926825523376, R2 0.5419317483901978\n",
      "Eval loss 1.0454391241073608, R2 0.545426607131958\n",
      "epoch 874, loss 0.9923654794692993, R2 0.5422692894935608\n",
      "Eval loss 1.0448737144470215, R2 0.5457581877708435\n",
      "epoch 875, loss 0.9918399453163147, R2 0.5426059365272522\n",
      "Eval loss 1.044310212135315, R2 0.5460887551307678\n",
      "epoch 876, loss 0.9913161993026733, R2 0.5429415106773376\n",
      "Eval loss 1.0437482595443726, R2 0.5464184284210205\n",
      "epoch 877, loss 0.9907939434051514, R2 0.543276309967041\n",
      "Eval loss 1.043188214302063, R2 0.5467473268508911\n",
      "epoch 878, loss 0.9902733564376831, R2 0.5436100363731384\n",
      "Eval loss 1.0426297187805176, R2 0.5470752716064453\n",
      "epoch 879, loss 0.9897544384002686, R2 0.5439428687095642\n",
      "Eval loss 1.0420730113983154, R2 0.5474022626876831\n",
      "epoch 880, loss 0.9892371892929077, R2 0.5442747473716736\n",
      "Eval loss 1.041517734527588, R2 0.5477283596992493\n",
      "epoch 881, loss 0.9887216091156006, R2 0.5446056723594666\n",
      "Eval loss 1.0409643650054932, R2 0.5480536818504333\n",
      "epoch 882, loss 0.9882075786590576, R2 0.5449357032775879\n",
      "Eval loss 1.0404126644134521, R2 0.5483779907226562\n",
      "epoch 883, loss 0.9876952171325684, R2 0.5452647805213928\n",
      "Eval loss 1.0398625135421753, R2 0.5487014651298523\n",
      "epoch 884, loss 0.9871842861175537, R2 0.5455929040908813\n",
      "Eval loss 1.0393141508102417, R2 0.5490240454673767\n",
      "epoch 885, loss 0.9866751432418823, R2 0.5459201335906982\n",
      "Eval loss 1.0387672185897827, R2 0.5493457913398743\n",
      "epoch 886, loss 0.9861675500869751, R2 0.5462465286254883\n",
      "Eval loss 1.038222074508667, R2 0.5496665835380554\n",
      "epoch 887, loss 0.9856615662574768, R2 0.5465719103813171\n",
      "Eval loss 1.037678837776184, R2 0.5499865412712097\n",
      "epoch 888, loss 0.9851571321487427, R2 0.5468963980674744\n",
      "Eval loss 1.0371367931365967, R2 0.5503056049346924\n",
      "epoch 889, loss 0.9846543073654175, R2 0.54721999168396\n",
      "Eval loss 1.0365965366363525, R2 0.5506238341331482\n",
      "epoch 890, loss 0.9841532111167908, R2 0.5475426316261292\n",
      "Eval loss 1.0360578298568726, R2 0.5509412288665771\n",
      "epoch 891, loss 0.9836534261703491, R2 0.5478644371032715\n",
      "Eval loss 1.0355209112167358, R2 0.5512576699256897\n",
      "epoch 892, loss 0.983155369758606, R2 0.5481852889060974\n",
      "Eval loss 1.0349856615066528, R2 0.5515733361244202\n",
      "epoch 893, loss 0.9826587438583374, R2 0.5485052466392517\n",
      "Eval loss 1.0344517230987549, R2 0.551888108253479\n",
      "epoch 894, loss 0.9821637868881226, R2 0.5488243103027344\n",
      "Eval loss 1.0339196920394897, R2 0.552202045917511\n",
      "epoch 895, loss 0.9816704392433167, R2 0.5491424798965454\n",
      "Eval loss 1.0333889722824097, R2 0.5525150895118713\n",
      "epoch 896, loss 0.9811784625053406, R2 0.5494597554206848\n",
      "Eval loss 1.0328600406646729, R2 0.5528273582458496\n",
      "epoch 897, loss 0.980688214302063, R2 0.5497761964797974\n",
      "Eval loss 1.0323325395584106, R2 0.5531387329101562\n",
      "epoch 898, loss 0.9801992177963257, R2 0.5500916838645935\n",
      "Eval loss 1.0318068265914917, R2 0.5534492135047913\n",
      "epoch 899, loss 0.9797120094299316, R2 0.550406277179718\n",
      "Eval loss 1.0312825441360474, R2 0.553758978843689\n",
      "epoch 900, loss 0.9792262315750122, R2 0.5507200360298157\n",
      "Eval loss 1.0307599306106567, R2 0.554067850112915\n",
      "epoch 901, loss 0.9787419438362122, R2 0.5510329008102417\n",
      "Eval loss 1.0302388668060303, R2 0.5543758869171143\n",
      "epoch 902, loss 0.9782591462135315, R2 0.5513448715209961\n",
      "Eval loss 1.0297192335128784, R2 0.5546830892562866\n",
      "epoch 903, loss 0.977777898311615, R2 0.5516560077667236\n",
      "Eval loss 1.0292011499404907, R2 0.5549893975257874\n",
      "epoch 904, loss 0.9772982001304626, R2 0.5519662499427795\n",
      "Eval loss 1.0286846160888672, R2 0.5552949905395508\n",
      "epoch 905, loss 0.9768199324607849, R2 0.5522756576538086\n",
      "Eval loss 1.0281697511672974, R2 0.5555997490882874\n",
      "epoch 906, loss 0.9763432145118713, R2 0.5525842308998108\n",
      "Eval loss 1.0276564359664917, R2 0.5559035539627075\n",
      "epoch 907, loss 0.9758679270744324, R2 0.5528919696807861\n",
      "Eval loss 1.0271445512771606, R2 0.5562066435813904\n",
      "epoch 908, loss 0.9753940105438232, R2 0.5531986951828003\n",
      "Eval loss 1.0266342163085938, R2 0.5565089583396912\n",
      "epoch 909, loss 0.9749218821525574, R2 0.5535047054290771\n",
      "Eval loss 1.0261253118515015, R2 0.5568104386329651\n",
      "epoch 910, loss 0.9744508862495422, R2 0.5538098216056824\n",
      "Eval loss 1.0256181955337524, R2 0.5571110248565674\n",
      "epoch 911, loss 0.9739815592765808, R2 0.554114043712616\n",
      "Eval loss 1.0251123905181885, R2 0.5574108362197876\n",
      "epoch 912, loss 0.9735136032104492, R2 0.5544175505638123\n",
      "Eval loss 1.0246078968048096, R2 0.5577098727226257\n",
      "epoch 913, loss 0.9730472564697266, R2 0.5547200441360474\n",
      "Eval loss 1.0241053104400635, R2 0.5580081343650818\n",
      "epoch 914, loss 0.9725820422172546, R2 0.5550218224525452\n",
      "Eval loss 1.023603916168213, R2 0.558305561542511\n",
      "epoch 915, loss 0.9721184968948364, R2 0.5553227066993713\n",
      "Eval loss 1.0231040716171265, R2 0.5586021542549133\n",
      "epoch 916, loss 0.971656322479248, R2 0.5556228160858154\n",
      "Eval loss 1.0226057767868042, R2 0.5588980317115784\n",
      "epoch 917, loss 0.9711955785751343, R2 0.5559219717979431\n",
      "Eval loss 1.022109031677246, R2 0.5591930747032166\n",
      "epoch 918, loss 0.9707363247871399, R2 0.5562204718589783\n",
      "Eval loss 1.021613597869873, R2 0.5594872832298279\n",
      "epoch 919, loss 0.9702785611152649, R2 0.556518018245697\n",
      "Eval loss 1.0211198329925537, R2 0.5597807168960571\n",
      "epoch 920, loss 0.9698219895362854, R2 0.5568147301673889\n",
      "Eval loss 1.0206273794174194, R2 0.5600734353065491\n",
      "epoch 921, loss 0.9693670272827148, R2 0.5571107268333435\n",
      "Eval loss 1.0201363563537598, R2 0.5603652596473694\n",
      "epoch 922, loss 0.9689135551452637, R2 0.5574057102203369\n",
      "Eval loss 1.0196470022201538, R2 0.5606563687324524\n",
      "epoch 923, loss 0.968461275100708, R2 0.5577000975608826\n",
      "Eval loss 1.0191588401794434, R2 0.5609467029571533\n",
      "epoch 924, loss 0.9680104851722717, R2 0.5579935908317566\n",
      "Eval loss 1.018672227859497, R2 0.5612362623214722\n",
      "epoch 925, loss 0.9675610065460205, R2 0.5582862496376038\n",
      "Eval loss 1.018187165260315, R2 0.5615250468254089\n",
      "epoch 926, loss 0.9671128988265991, R2 0.5585781335830688\n",
      "Eval loss 1.0177032947540283, R2 0.5618130564689636\n",
      "epoch 927, loss 0.9666662812232971, R2 0.5588691830635071\n",
      "Eval loss 1.0172210931777954, R2 0.5621002316474915\n",
      "epoch 928, loss 0.9662211537361145, R2 0.5591594576835632\n",
      "Eval loss 1.0167402029037476, R2 0.5623867511749268\n",
      "epoch 929, loss 0.9657771587371826, R2 0.5594488978385925\n",
      "Eval loss 1.0162606239318848, R2 0.56267249584198\n",
      "epoch 930, loss 0.9653347134590149, R2 0.5597375631332397\n",
      "Eval loss 1.0157827138900757, R2 0.5629574060440063\n",
      "epoch 931, loss 0.9648934602737427, R2 0.5600254535675049\n",
      "Eval loss 1.0153058767318726, R2 0.5632416009902954\n",
      "epoch 932, loss 0.9644536375999451, R2 0.5603125095367432\n",
      "Eval loss 1.0148308277130127, R2 0.5635250806808472\n",
      "epoch 933, loss 0.9640152454376221, R2 0.5605987906455994\n",
      "Eval loss 1.0143568515777588, R2 0.5638077259063721\n",
      "epoch 934, loss 0.9635781049728394, R2 0.5608842372894287\n",
      "Eval loss 1.0138845443725586, R2 0.5640896558761597\n",
      "epoch 935, loss 0.9631423950195312, R2 0.5611690282821655\n",
      "Eval loss 1.0134133100509644, R2 0.5643708109855652\n",
      "epoch 936, loss 0.9627079367637634, R2 0.5614529252052307\n",
      "Eval loss 1.0129437446594238, R2 0.5646512508392334\n",
      "epoch 937, loss 0.9622748494148254, R2 0.5617360472679138\n",
      "Eval loss 1.0124754905700684, R2 0.5649309754371643\n",
      "epoch 938, loss 0.9618431329727173, R2 0.5620183944702148\n",
      "Eval loss 1.0120086669921875, R2 0.5652099251747131\n",
      "epoch 939, loss 0.9614126682281494, R2 0.5622999668121338\n",
      "Eval loss 1.0115430355072021, R2 0.5654881000518799\n",
      "epoch 940, loss 0.9609836935997009, R2 0.5625808238983154\n",
      "Eval loss 1.0110788345336914, R2 0.5657655596733093\n",
      "epoch 941, loss 0.9605558514595032, R2 0.562860906124115\n",
      "Eval loss 1.0106160640716553, R2 0.5660423040390015\n",
      "epoch 942, loss 0.96012943983078, R2 0.5631401538848877\n",
      "Eval loss 1.0101546049118042, R2 0.5663183927536011\n",
      "epoch 943, loss 0.9597041606903076, R2 0.5634186863899231\n",
      "Eval loss 1.0096945762634277, R2 0.566593587398529\n",
      "epoch 944, loss 0.9592803716659546, R2 0.5636964440345764\n",
      "Eval loss 1.0092357397079468, R2 0.5668681263923645\n",
      "epoch 945, loss 0.9588578939437866, R2 0.5639734268188477\n",
      "Eval loss 1.00877845287323, R2 0.5671419501304626\n",
      "epoch 946, loss 0.9584366083145142, R2 0.5642496943473816\n",
      "Eval loss 1.0083224773406982, R2 0.5674149990081787\n",
      "epoch 947, loss 0.9580166935920715, R2 0.5645251274108887\n",
      "Eval loss 1.007867693901062, R2 0.5676873922348022\n",
      "epoch 948, loss 0.9575979113578796, R2 0.5647999048233032\n",
      "Eval loss 1.00741446018219, R2 0.5679590702056885\n",
      "epoch 949, loss 0.9571805596351624, R2 0.5650737881660461\n",
      "Eval loss 1.0069621801376343, R2 0.5682299733161926\n",
      "epoch 950, loss 0.9567644000053406, R2 0.5653470158576965\n",
      "Eval loss 1.0065115690231323, R2 0.5685002207756042\n",
      "epoch 951, loss 0.9563496112823486, R2 0.5656195878982544\n",
      "Eval loss 1.0060621500015259, R2 0.5687696933746338\n",
      "epoch 952, loss 0.9559360146522522, R2 0.5658912658691406\n",
      "Eval loss 1.0056140422821045, R2 0.5690385103225708\n",
      "epoch 953, loss 0.9555236101150513, R2 0.5661622881889343\n",
      "Eval loss 1.0051672458648682, R2 0.5693066120147705\n",
      "epoch 954, loss 0.955112636089325, R2 0.566432535648346\n",
      "Eval loss 1.004721760749817, R2 0.5695739388465881\n",
      "epoch 955, loss 0.9547028541564941, R2 0.5667020082473755\n",
      "Eval loss 1.0042774677276611, R2 0.569840669631958\n",
      "epoch 956, loss 0.9542942047119141, R2 0.5669708847999573\n",
      "Eval loss 1.0038347244262695, R2 0.570106565952301\n",
      "epoch 957, loss 0.9538869857788086, R2 0.5672389268875122\n",
      "Eval loss 1.0033930540084839, R2 0.5703718662261963\n",
      "epoch 958, loss 0.9534810185432434, R2 0.5675062537193298\n",
      "Eval loss 1.0029528141021729, R2 0.570636510848999\n",
      "epoch 959, loss 0.953076183795929, R2 0.5677728056907654\n",
      "Eval loss 1.0025136470794678, R2 0.5709003806114197\n",
      "epoch 960, loss 0.9526726007461548, R2 0.5680387020111084\n",
      "Eval loss 1.0020760297775269, R2 0.571163535118103\n",
      "epoch 961, loss 0.9522702693939209, R2 0.5683038830757141\n",
      "Eval loss 1.0016393661499023, R2 0.5714260339736938\n",
      "epoch 962, loss 0.9518690705299377, R2 0.5685682892799377\n",
      "Eval loss 1.0012043714523315, R2 0.5716878771781921\n",
      "epoch 963, loss 0.9514691233634949, R2 0.5688319802284241\n",
      "Eval loss 1.0007704496383667, R2 0.5719489455223083\n",
      "epoch 964, loss 0.9510704874992371, R2 0.5690950155258179\n",
      "Eval loss 1.0003376007080078, R2 0.5722094178199768\n",
      "epoch 965, loss 0.9506730437278748, R2 0.5693572759628296\n",
      "Eval loss 0.9999061822891235, R2 0.572469174861908\n",
      "epoch 966, loss 0.9502767324447632, R2 0.5696188807487488\n",
      "Eval loss 0.9994761347770691, R2 0.5727282166481018\n",
      "epoch 967, loss 0.9498817324638367, R2 0.5698797106742859\n",
      "Eval loss 0.9990471601486206, R2 0.5729866027832031\n",
      "epoch 968, loss 0.9494878649711609, R2 0.5701398849487305\n",
      "Eval loss 0.9986194968223572, R2 0.5732443928718567\n",
      "epoch 969, loss 0.9490951895713806, R2 0.570399284362793\n",
      "Eval loss 0.9981930255889893, R2 0.5735013484954834\n",
      "epoch 970, loss 0.9487036466598511, R2 0.5706580877304077\n",
      "Eval loss 0.9977677464485168, R2 0.5737577676773071\n",
      "epoch 971, loss 0.948313295841217, R2 0.5709161162376404\n",
      "Eval loss 0.9973438382148743, R2 0.5740134716033936\n",
      "epoch 972, loss 0.9479242563247681, R2 0.5711734890937805\n",
      "Eval loss 0.996921181678772, R2 0.5742685198783875\n",
      "epoch 973, loss 0.9475364089012146, R2 0.5714301466941833\n",
      "Eval loss 0.9964996576309204, R2 0.5745229125022888\n",
      "epoch 974, loss 0.9471495747566223, R2 0.5716860294342041\n",
      "Eval loss 0.9960792660713196, R2 0.5747767090797424\n",
      "epoch 975, loss 0.9467638731002808, R2 0.5719413757324219\n",
      "Eval loss 0.9956602454185486, R2 0.5750296115875244\n",
      "epoch 976, loss 0.946379542350769, R2 0.5721959471702576\n",
      "Eval loss 0.9952423572540283, R2 0.5752820372581482\n",
      "epoch 977, loss 0.9459963440895081, R2 0.5724498629570007\n",
      "Eval loss 0.9948257207870483, R2 0.5755338072776794\n",
      "epoch 978, loss 0.9456141591072083, R2 0.5727030038833618\n",
      "Eval loss 0.9944102168083191, R2 0.5757848620414734\n",
      "epoch 979, loss 0.9452332854270935, R2 0.5729555487632751\n",
      "Eval loss 0.9939960241317749, R2 0.5760352611541748\n",
      "epoch 980, loss 0.9448535442352295, R2 0.5732073783874512\n",
      "Eval loss 0.9935829043388367, R2 0.5762851238250732\n",
      "epoch 981, loss 0.9444748759269714, R2 0.5734586119651794\n",
      "Eval loss 0.9931710958480835, R2 0.5765342116355896\n",
      "epoch 982, loss 0.9440973997116089, R2 0.5737090706825256\n",
      "Eval loss 0.9927603602409363, R2 0.5767826437950134\n",
      "epoch 983, loss 0.9437210559844971, R2 0.5739588737487793\n",
      "Eval loss 0.9923508763313293, R2 0.5770304799079895\n",
      "epoch 984, loss 0.9433457851409912, R2 0.5742080211639404\n",
      "Eval loss 0.9919424653053284, R2 0.577277660369873\n",
      "epoch 985, loss 0.9429717659950256, R2 0.5744564533233643\n",
      "Eval loss 0.9915354251861572, R2 0.5775242447853088\n",
      "epoch 986, loss 0.9425987005233765, R2 0.5747042298316956\n",
      "Eval loss 0.991129457950592, R2 0.5777701139450073\n",
      "epoch 987, loss 0.9422269463539124, R2 0.5749514102935791\n",
      "Eval loss 0.9907246232032776, R2 0.5780153870582581\n",
      "epoch 988, loss 0.9418562054634094, R2 0.5751978754997253\n",
      "Eval loss 0.9903210401535034, R2 0.5782600045204163\n",
      "epoch 989, loss 0.941486656665802, R2 0.5754437446594238\n",
      "Eval loss 0.9899185299873352, R2 0.5785040259361267\n",
      "epoch 990, loss 0.9411181807518005, R2 0.575688898563385\n",
      "Eval loss 0.9895172119140625, R2 0.5787473917007446\n",
      "epoch 991, loss 0.940750777721405, R2 0.5759333968162537\n",
      "Eval loss 0.9891170263290405, R2 0.5789900422096252\n",
      "epoch 992, loss 0.940384566783905, R2 0.5761772990226746\n",
      "Eval loss 0.9887180328369141, R2 0.5792322158813477\n",
      "epoch 993, loss 0.9400193691253662, R2 0.5764204263687134\n",
      "Eval loss 0.9883201122283936, R2 0.5794737339019775\n",
      "epoch 994, loss 0.9396552443504333, R2 0.5766629576683044\n",
      "Eval loss 0.9879233837127686, R2 0.5797145366668701\n",
      "epoch 995, loss 0.9392922520637512, R2 0.576904833316803\n",
      "Eval loss 0.9875277876853943, R2 0.5799547433853149\n",
      "epoch 996, loss 0.9389303922653198, R2 0.577146053314209\n",
      "Eval loss 0.9871333241462708, R2 0.5801944136619568\n",
      "epoch 997, loss 0.9385696053504944, R2 0.5773866176605225\n",
      "Eval loss 0.9867398738861084, R2 0.5804334282875061\n",
      "epoch 998, loss 0.9382098913192749, R2 0.577626645565033\n",
      "Eval loss 0.9863478541374207, R2 0.5806718468666077\n",
      "epoch 999, loss 0.9378511905670166, R2 0.5778659582138062\n",
      "Eval loss 0.9859566688537598, R2 0.5809095501899719\n",
      "epoch 1000, loss 0.9374936819076538, R2 0.578104555606842\n",
      "Eval loss 0.9855667352676392, R2 0.5811467170715332\n",
      "epoch 1001, loss 0.937137246131897, R2 0.5783426761627197\n",
      "Eval loss 0.9851778149604797, R2 0.5813832879066467\n",
      "epoch 1002, loss 0.9367817640304565, R2 0.5785800814628601\n",
      "Eval loss 0.9847901463508606, R2 0.5816192030906677\n",
      "epoch 1003, loss 0.9364273548126221, R2 0.578816831111908\n",
      "Eval loss 0.9844034314155579, R2 0.5818545818328857\n",
      "epoch 1004, loss 0.9360740184783936, R2 0.5790529251098633\n",
      "Eval loss 0.9840179085731506, R2 0.5820892453193665\n",
      "epoch 1005, loss 0.9357219338417053, R2 0.5792884230613708\n",
      "Eval loss 0.9836335182189941, R2 0.5823233723640442\n",
      "epoch 1006, loss 0.9353707432746887, R2 0.5795233249664307\n",
      "Eval loss 0.9832502007484436, R2 0.5825569033622742\n",
      "epoch 1007, loss 0.9350206255912781, R2 0.579757571220398\n",
      "Eval loss 0.9828678965568542, R2 0.5827898383140564\n",
      "epoch 1008, loss 0.9346715211868286, R2 0.5799911618232727\n",
      "Eval loss 0.9824866652488708, R2 0.5830221772193909\n",
      "epoch 1009, loss 0.9343234896659851, R2 0.5802242159843445\n",
      "Eval loss 0.9821066856384277, R2 0.583253800868988\n",
      "epoch 1010, loss 0.9339764714241028, R2 0.5804566144943237\n",
      "Eval loss 0.9817276000976562, R2 0.5834850072860718\n",
      "epoch 1011, loss 0.9336305260658264, R2 0.5806883573532104\n",
      "Eval loss 0.9813498854637146, R2 0.5837154984474182\n",
      "epoch 1012, loss 0.9332855343818665, R2 0.5809195041656494\n",
      "Eval loss 0.9809730052947998, R2 0.5839455127716064\n",
      "epoch 1013, loss 0.9329415559768677, R2 0.5811499953269958\n",
      "Eval loss 0.9805973172187805, R2 0.5841748714447021\n",
      "epoch 1014, loss 0.9325987100601196, R2 0.5813799500465393\n",
      "Eval loss 0.9802225828170776, R2 0.5844036340713501\n",
      "epoch 1015, loss 0.9322569370269775, R2 0.5816092491149902\n",
      "Eval loss 0.9798489212989807, R2 0.5846317410469055\n",
      "epoch 1016, loss 0.9319159984588623, R2 0.5818379521369934\n",
      "Eval loss 0.9794763922691345, R2 0.5848593711853027\n",
      "epoch 1017, loss 0.9315761923789978, R2 0.5820660591125488\n",
      "Eval loss 0.97910475730896, R2 0.585086464881897\n",
      "epoch 1018, loss 0.9312373995780945, R2 0.5822935700416565\n",
      "Eval loss 0.9787343740463257, R2 0.5853128433227539\n",
      "epoch 1019, loss 0.9308996200561523, R2 0.5825204849243164\n",
      "Eval loss 0.9783650636672974, R2 0.5855386853218079\n",
      "epoch 1020, loss 0.9305628538131714, R2 0.5827467441558838\n",
      "Eval loss 0.9779966473579407, R2 0.5857639908790588\n",
      "epoch 1021, loss 0.9302269220352173, R2 0.5829724669456482\n",
      "Eval loss 0.9776292443275452, R2 0.5859887599945068\n",
      "epoch 1022, loss 0.9298922419548035, R2 0.5831975936889648\n",
      "Eval loss 0.9772629737854004, R2 0.5862128138542175\n",
      "epoch 1023, loss 0.9295583963394165, R2 0.583422064781189\n",
      "Eval loss 0.9768978357315063, R2 0.5864363312721252\n",
      "epoch 1024, loss 0.929225504398346, R2 0.5836459994316101\n",
      "Eval loss 0.9765335321426392, R2 0.5866594314575195\n",
      "epoch 1025, loss 0.9288937449455261, R2 0.5838693380355835\n",
      "Eval loss 0.9761703610420227, R2 0.5868818759918213\n",
      "epoch 1026, loss 0.9285628795623779, R2 0.5840920209884644\n",
      "Eval loss 0.9758082628250122, R2 0.5871037244796753\n",
      "epoch 1027, loss 0.9282330274581909, R2 0.5843141674995422\n",
      "Eval loss 0.9754470586776733, R2 0.5873250365257263\n",
      "epoch 1028, loss 0.9279041290283203, R2 0.5845357179641724\n",
      "Eval loss 0.9750869870185852, R2 0.5875456929206848\n",
      "epoch 1029, loss 0.9275762438774109, R2 0.5847566723823547\n",
      "Eval loss 0.974727988243103, R2 0.5877659320831299\n",
      "epoch 1030, loss 0.9272493124008179, R2 0.5849770307540894\n",
      "Eval loss 0.9743698239326477, R2 0.5879855155944824\n",
      "epoch 1031, loss 0.9269232749938965, R2 0.5851969122886658\n",
      "Eval loss 0.9740127921104431, R2 0.588204562664032\n",
      "epoch 1032, loss 0.9265983700752258, R2 0.5854160785675049\n",
      "Eval loss 0.9736567735671997, R2 0.5884231328964233\n",
      "epoch 1033, loss 0.926274299621582, R2 0.585634708404541\n",
      "Eval loss 0.9733016490936279, R2 0.5886410474777222\n",
      "epoch 1034, loss 0.9259511828422546, R2 0.5858527421951294\n",
      "Eval loss 0.9729475975036621, R2 0.588858425617218\n",
      "epoch 1035, loss 0.9256290197372437, R2 0.5860702991485596\n",
      "Eval loss 0.9725946187973022, R2 0.5890752673149109\n",
      "epoch 1036, loss 0.9253078699111938, R2 0.5862872004508972\n",
      "Eval loss 0.972242534160614, R2 0.5892915725708008\n",
      "epoch 1037, loss 0.9249875545501709, R2 0.5865035653114319\n",
      "Eval loss 0.9718915224075317, R2 0.5895073413848877\n",
      "epoch 1038, loss 0.9246683120727539, R2 0.5867193937301636\n",
      "Eval loss 0.9715413451194763, R2 0.5897225141525269\n",
      "epoch 1039, loss 0.9243499636650085, R2 0.5869345664978027\n",
      "Eval loss 0.9711923003196716, R2 0.589937150478363\n",
      "epoch 1040, loss 0.9240325093269348, R2 0.5871492028236389\n",
      "Eval loss 0.9708440899848938, R2 0.590151309967041\n",
      "epoch 1041, loss 0.9237160682678223, R2 0.5873633623123169\n",
      "Eval loss 0.9704969525337219, R2 0.5903648734092712\n",
      "epoch 1042, loss 0.9234005808830261, R2 0.5875768661499023\n",
      "Eval loss 0.9701507687568665, R2 0.5905778408050537\n",
      "epoch 1043, loss 0.9230859279632568, R2 0.5877898335456848\n",
      "Eval loss 0.9698056578636169, R2 0.590790331363678\n",
      "epoch 1044, loss 0.9227723479270935, R2 0.5880022644996643\n",
      "Eval loss 0.9694613218307495, R2 0.5910024046897888\n",
      "epoch 1045, loss 0.9224595427513123, R2 0.5882140398025513\n",
      "Eval loss 0.969118058681488, R2 0.5912137627601624\n",
      "epoch 1046, loss 0.9221477508544922, R2 0.5884253978729248\n",
      "Eval loss 0.968775749206543, R2 0.5914246439933777\n",
      "epoch 1047, loss 0.9218367338180542, R2 0.5886361002922058\n",
      "Eval loss 0.9684344530105591, R2 0.59163498878479\n",
      "epoch 1048, loss 0.9215267300605774, R2 0.5888463258743286\n",
      "Eval loss 0.9680941104888916, R2 0.5918448567390442\n",
      "epoch 1049, loss 0.921217679977417, R2 0.5890559554100037\n",
      "Eval loss 0.967754602432251, R2 0.5920541286468506\n",
      "epoch 1050, loss 0.9209095239639282, R2 0.5892651081085205\n",
      "Eval loss 0.9674161672592163, R2 0.5922629833221436\n",
      "epoch 1051, loss 0.9206022620201111, R2 0.5894736051559448\n",
      "Eval loss 0.967078685760498, R2 0.592471182346344\n",
      "epoch 1052, loss 0.9202958345413208, R2 0.5896816849708557\n",
      "Eval loss 0.9667420387268066, R2 0.5926788449287415\n",
      "epoch 1053, loss 0.9199904203414917, R2 0.5898891091346741\n",
      "Eval loss 0.9664064049720764, R2 0.5928860902786255\n",
      "epoch 1054, loss 0.9196857213973999, R2 0.5900960564613342\n",
      "Eval loss 0.9660716652870178, R2 0.5930927991867065\n",
      "epoch 1055, loss 0.9193820953369141, R2 0.5903024673461914\n",
      "Eval loss 0.9657379388809204, R2 0.5932989716529846\n",
      "epoch 1056, loss 0.9190793633460999, R2 0.590508222579956\n",
      "Eval loss 0.9654049873352051, R2 0.5935045480728149\n",
      "epoch 1057, loss 0.918777346611023, R2 0.5907135605812073\n",
      "Eval loss 0.9650731682777405, R2 0.5937097072601318\n",
      "epoch 1058, loss 0.918476402759552, R2 0.5909183621406555\n",
      "Eval loss 0.9647420048713684, R2 0.5939143300056458\n",
      "epoch 1059, loss 0.9181762337684631, R2 0.591122567653656\n",
      "Eval loss 0.9644119143486023, R2 0.5941184759140015\n",
      "epoch 1060, loss 0.9178769588470459, R2 0.5913262367248535\n",
      "Eval loss 0.9640828371047974, R2 0.5943220257759094\n",
      "epoch 1061, loss 0.9175785779953003, R2 0.5915294885635376\n",
      "Eval loss 0.9637545347213745, R2 0.5945250988006592\n",
      "epoch 1062, loss 0.9172810912132263, R2 0.5917320847511292\n",
      "Eval loss 0.9634271860122681, R2 0.5947276949882507\n",
      "epoch 1063, loss 0.9169843792915344, R2 0.5919342637062073\n",
      "Eval loss 0.9631008505821228, R2 0.5949297547340393\n",
      "epoch 1064, loss 0.9166885614395142, R2 0.5921358466148376\n",
      "Eval loss 0.9627752304077148, R2 0.5951313376426697\n",
      "epoch 1065, loss 0.9163937568664551, R2 0.5923369526863098\n",
      "Eval loss 0.9624506235122681, R2 0.5953323841094971\n",
      "epoch 1066, loss 0.9160996079444885, R2 0.5925374627113342\n",
      "Eval loss 0.9621269702911377, R2 0.5955329537391663\n",
      "epoch 1067, loss 0.9158064723014832, R2 0.5927374958992004\n",
      "Eval loss 0.9618041515350342, R2 0.5957329869270325\n",
      "epoch 1068, loss 0.9155141115188599, R2 0.5929370522499084\n",
      "Eval loss 0.9614822268486023, R2 0.5959326028823853\n",
      "epoch 1069, loss 0.9152226448059082, R2 0.5931359529495239\n",
      "Eval loss 0.9611613154411316, R2 0.5961316227912903\n",
      "epoch 1070, loss 0.9149320125579834, R2 0.5933345556259155\n",
      "Eval loss 0.9608410596847534, R2 0.5963302254676819\n",
      "epoch 1071, loss 0.9146422147750854, R2 0.5935325026512146\n",
      "Eval loss 0.9605218172073364, R2 0.596528172492981\n",
      "epoch 1072, loss 0.9143532514572144, R2 0.5937299728393555\n",
      "Eval loss 0.9602034687995911, R2 0.5967258214950562\n",
      "epoch 1073, loss 0.9140651822090149, R2 0.5939269065856934\n",
      "Eval loss 0.9598860144615173, R2 0.5969229340553284\n",
      "epoch 1074, loss 0.9137778878211975, R2 0.5941233038902283\n",
      "Eval loss 0.9595694541931152, R2 0.5971194505691528\n",
      "epoch 1075, loss 0.9134914875030518, R2 0.5943192839622498\n",
      "Eval loss 0.95925372838974, R2 0.5973155498504639\n",
      "epoch 1076, loss 0.9132058024406433, R2 0.5945146679878235\n",
      "Eval loss 0.9589388370513916, R2 0.5975112318992615\n",
      "epoch 1077, loss 0.9129210710525513, R2 0.594709575176239\n",
      "Eval loss 0.9586248397827148, R2 0.5977063775062561\n",
      "epoch 1078, loss 0.9126372337341309, R2 0.5949040055274963\n",
      "Eval loss 0.9583117961883545, R2 0.597900927066803\n",
      "epoch 1079, loss 0.912354052066803, R2 0.5950979590415955\n",
      "Eval loss 0.9579994678497314, R2 0.5980951189994812\n",
      "epoch 1080, loss 0.9120716452598572, R2 0.5952913165092468\n",
      "Eval loss 0.95768803358078, R2 0.5982887744903564\n",
      "epoch 1081, loss 0.911790132522583, R2 0.59548419713974\n",
      "Eval loss 0.9573774933815002, R2 0.5984820127487183\n",
      "epoch 1082, loss 0.9115095734596252, R2 0.5956766605377197\n",
      "Eval loss 0.9570678472518921, R2 0.5986747741699219\n",
      "epoch 1083, loss 0.9112297296524048, R2 0.5958685278892517\n",
      "Eval loss 0.9567590355873108, R2 0.5988669991493225\n",
      "epoch 1084, loss 0.9109506607055664, R2 0.5960599780082703\n",
      "Eval loss 0.9564510583877563, R2 0.5990587472915649\n",
      "epoch 1085, loss 0.9106724858283997, R2 0.5962508916854858\n",
      "Eval loss 0.9561439156532288, R2 0.599250078201294\n",
      "epoch 1086, loss 0.910395085811615, R2 0.5964413285255432\n",
      "Eval loss 0.955837607383728, R2 0.5994408130645752\n",
      "epoch 1087, loss 0.9101183414459229, R2 0.5966312885284424\n",
      "Eval loss 0.9555321335792542, R2 0.5996311902999878\n",
      "epoch 1088, loss 0.9098426103591919, R2 0.5968207716941833\n",
      "Eval loss 0.9552276134490967, R2 0.5998210906982422\n",
      "epoch 1089, loss 0.909567654132843, R2 0.5970097184181213\n",
      "Eval loss 0.9549238085746765, R2 0.6000104546546936\n",
      "epoch 1090, loss 0.9092933535575867, R2 0.5971982479095459\n",
      "Eval loss 0.954620897769928, R2 0.6001994013786316\n",
      "epoch 1091, loss 0.9090198874473572, R2 0.5973861813545227\n",
      "Eval loss 0.9543188214302063, R2 0.6003878116607666\n",
      "epoch 1092, loss 0.9087472558021545, R2 0.5975736975669861\n",
      "Eval loss 0.9540175795555115, R2 0.600575864315033\n",
      "epoch 1093, loss 0.9084753394126892, R2 0.5977607369422913\n",
      "Eval loss 0.9537171721458435, R2 0.6007634401321411\n",
      "epoch 1094, loss 0.9082042574882507, R2 0.597947359085083\n",
      "Eval loss 0.9534174203872681, R2 0.6009504795074463\n",
      "epoch 1095, loss 0.9079340100288391, R2 0.5981334447860718\n",
      "Eval loss 0.953118622303009, R2 0.601137101650238\n",
      "epoch 1096, loss 0.9076645970344543, R2 0.5983189940452576\n",
      "Eval loss 0.9528206586837769, R2 0.6013233065605164\n",
      "epoch 1097, loss 0.9073957800865173, R2 0.5985041260719299\n",
      "Eval loss 0.9525235891342163, R2 0.6015089750289917\n",
      "epoch 1098, loss 0.907127857208252, R2 0.5986887812614441\n",
      "Eval loss 0.9522271752357483, R2 0.6016942262649536\n",
      "epoch 1099, loss 0.9068605899810791, R2 0.5988729596138\n",
      "Eval loss 0.9519317150115967, R2 0.6018790602684021\n",
      "epoch 1100, loss 0.9065942764282227, R2 0.5990566611289978\n",
      "Eval loss 0.9516369104385376, R2 0.6020633578300476\n",
      "epoch 1101, loss 0.9063286781311035, R2 0.5992399454116821\n",
      "Eval loss 0.9513430595397949, R2 0.6022472381591797\n",
      "epoch 1102, loss 0.9060636758804321, R2 0.5994226932525635\n",
      "Eval loss 0.9510500431060791, R2 0.6024306416511536\n",
      "epoch 1103, loss 0.9057996869087219, R2 0.5996049642562866\n",
      "Eval loss 0.950757622718811, R2 0.6026135683059692\n",
      "epoch 1104, loss 0.9055362343788147, R2 0.5997868180274963\n",
      "Eval loss 0.9504660964012146, R2 0.6027961373329163\n",
      "epoch 1105, loss 0.9052736163139343, R2 0.5999681949615479\n",
      "Eval loss 0.950175404548645, R2 0.6029782295227051\n",
      "epoch 1106, loss 0.9050118327140808, R2 0.6001491546630859\n",
      "Eval loss 0.949885368347168, R2 0.6031598448753357\n",
      "epoch 1107, loss 0.9047507047653198, R2 0.600329577922821\n",
      "Eval loss 0.9495963454246521, R2 0.6033410429954529\n",
      "epoch 1108, loss 0.9044903516769409, R2 0.6005095839500427\n",
      "Eval loss 0.9493078589439392, R2 0.6035217642784119\n",
      "epoch 1109, loss 0.9042306542396545, R2 0.6006891131401062\n",
      "Eval loss 0.9490204453468323, R2 0.6037021279335022\n",
      "epoch 1110, loss 0.9039719700813293, R2 0.6008681654930115\n",
      "Eval loss 0.9487335681915283, R2 0.6038818955421448\n",
      "epoch 1111, loss 0.9037137627601624, R2 0.6010468006134033\n",
      "Eval loss 0.9484476447105408, R2 0.6040613651275635\n",
      "epoch 1112, loss 0.9034563302993774, R2 0.601224958896637\n",
      "Eval loss 0.9481624364852905, R2 0.6042404174804688\n",
      "epoch 1113, loss 0.9031997323036194, R2 0.6014026999473572\n",
      "Eval loss 0.947877824306488, R2 0.6044188737869263\n",
      "epoch 1114, loss 0.9029437899589539, R2 0.6015799045562744\n",
      "Eval loss 0.9475942850112915, R2 0.6045970320701599\n",
      "epoch 1115, loss 0.9026886820793152, R2 0.6017566323280334\n",
      "Eval loss 0.9473113417625427, R2 0.6047747135162354\n",
      "epoch 1116, loss 0.9024342894554138, R2 0.6019330620765686\n",
      "Eval loss 0.9470292329788208, R2 0.6049519181251526\n",
      "epoch 1117, loss 0.902180552482605, R2 0.6021089553833008\n",
      "Eval loss 0.946747899055481, R2 0.6051287055015564\n",
      "epoch 1118, loss 0.9019275307655334, R2 0.6022844910621643\n",
      "Eval loss 0.9464671611785889, R2 0.6053051352500916\n",
      "epoch 1119, loss 0.9016752243041992, R2 0.6024594306945801\n",
      "Eval loss 0.9461873173713684, R2 0.6054810881614685\n",
      "epoch 1120, loss 0.9014237523078918, R2 0.6026340126991272\n",
      "Eval loss 0.9459081888198853, R2 0.6056565642356873\n",
      "epoch 1121, loss 0.901172935962677, R2 0.6028082370758057\n",
      "Eval loss 0.9456299543380737, R2 0.6058316826820374\n",
      "epoch 1122, loss 0.9009227752685547, R2 0.6029818058013916\n",
      "Eval loss 0.9453523755073547, R2 0.6060063242912292\n",
      "epoch 1123, loss 0.9006733894348145, R2 0.6031551361083984\n",
      "Eval loss 0.9450754523277283, R2 0.6061805486679077\n",
      "epoch 1124, loss 0.9004247188568115, R2 0.6033279299736023\n",
      "Eval loss 0.9447993636131287, R2 0.6063543558120728\n",
      "epoch 1125, loss 0.9001766443252563, R2 0.6035003066062927\n",
      "Eval loss 0.9445241689682007, R2 0.6065277457237244\n",
      "epoch 1126, loss 0.8999295234680176, R2 0.6036722660064697\n",
      "Eval loss 0.9442495703697205, R2 0.6067007184028625\n",
      "epoch 1127, loss 0.899682879447937, R2 0.6038436889648438\n",
      "Eval loss 0.9439756870269775, R2 0.6068732738494873\n",
      "epoch 1128, loss 0.8994371294975281, R2 0.6040148138999939\n",
      "Eval loss 0.9437025785446167, R2 0.6070454120635986\n",
      "epoch 1129, loss 0.8991919159889221, R2 0.6041854619979858\n",
      "Eval loss 0.9434302449226379, R2 0.6072171330451965\n",
      "epoch 1130, loss 0.8989474177360535, R2 0.6043556332588196\n",
      "Eval loss 0.9431585669517517, R2 0.607388436794281\n",
      "epoch 1131, loss 0.8987036347389221, R2 0.6045254468917847\n",
      "Eval loss 0.9428877234458923, R2 0.6075592637062073\n",
      "epoch 1132, loss 0.8984605669975281, R2 0.6046947240829468\n",
      "Eval loss 0.9426175355911255, R2 0.6077297329902649\n",
      "epoch 1133, loss 0.8982182145118713, R2 0.604863703250885\n",
      "Eval loss 0.9423481225967407, R2 0.6078998446464539\n",
      "epoch 1134, loss 0.8979765772819519, R2 0.605032205581665\n",
      "Eval loss 0.9420793652534485, R2 0.6080694198608398\n",
      "epoch 1135, loss 0.8977354168891907, R2 0.6052002310752869\n",
      "Eval loss 0.9418114423751831, R2 0.6082386374473572\n",
      "epoch 1136, loss 0.8974952101707458, R2 0.6053679585456848\n",
      "Eval loss 0.9415441155433655, R2 0.6084075570106506\n",
      "epoch 1137, loss 0.8972554802894592, R2 0.605535089969635\n",
      "Eval loss 0.9412775039672852, R2 0.6085759401321411\n",
      "epoch 1138, loss 0.897016704082489, R2 0.6057019233703613\n",
      "Eval loss 0.9410117268562317, R2 0.6087439060211182\n",
      "epoch 1139, loss 0.8967782258987427, R2 0.6058683395385742\n",
      "Eval loss 0.9407466053962708, R2 0.6089115142822266\n",
      "epoch 1140, loss 0.896540641784668, R2 0.6060342788696289\n",
      "Eval loss 0.9404821395874023, R2 0.6090788245201111\n",
      "epoch 1141, loss 0.8963037729263306, R2 0.6061998009681702\n",
      "Eval loss 0.9402185678482056, R2 0.6092455387115479\n",
      "epoch 1142, loss 0.8960675597190857, R2 0.6063649654388428\n",
      "Eval loss 0.9399555325508118, R2 0.609411895275116\n",
      "epoch 1143, loss 0.8958319425582886, R2 0.6065296530723572\n",
      "Eval loss 0.9396931529045105, R2 0.6095778942108154\n",
      "epoch 1144, loss 0.8955970406532288, R2 0.6066939830780029\n",
      "Eval loss 0.9394315481185913, R2 0.6097434759140015\n",
      "epoch 1145, loss 0.8953627347946167, R2 0.6068578362464905\n",
      "Eval loss 0.9391707181930542, R2 0.6099086403846741\n",
      "epoch 1146, loss 0.8951290845870972, R2 0.6070212721824646\n",
      "Eval loss 0.9389104843139648, R2 0.610073447227478\n",
      "epoch 1147, loss 0.8948961496353149, R2 0.6071844696998596\n",
      "Eval loss 0.9386510848999023, R2 0.6102378368377686\n",
      "epoch 1148, loss 0.8946638703346252, R2 0.6073470711708069\n",
      "Eval loss 0.9383923411369324, R2 0.6104018688201904\n",
      "epoch 1149, loss 0.8944323658943176, R2 0.6075092554092407\n",
      "Eval loss 0.9381341338157654, R2 0.6105654239654541\n",
      "epoch 1150, loss 0.8942013382911682, R2 0.6076710820198059\n",
      "Eval loss 0.9378767609596252, R2 0.6107286810874939\n",
      "epoch 1151, loss 0.8939709663391113, R2 0.6078325510025024\n",
      "Eval loss 0.9376200437545776, R2 0.6108914017677307\n",
      "epoch 1152, loss 0.8937413096427917, R2 0.6079936027526855\n",
      "Eval loss 0.9373639822006226, R2 0.6110538840293884\n",
      "epoch 1153, loss 0.8935122489929199, R2 0.6081542372703552\n",
      "Eval loss 0.9371086359024048, R2 0.6112159490585327\n",
      "epoch 1154, loss 0.8932839035987854, R2 0.6083144545555115\n",
      "Eval loss 0.9368540644645691, R2 0.6113776564598083\n",
      "epoch 1155, loss 0.8930562138557434, R2 0.6084742546081543\n",
      "Eval loss 0.9365999698638916, R2 0.6115388870239258\n",
      "epoch 1156, loss 0.8928290009498596, R2 0.6086336970329285\n",
      "Eval loss 0.9363467693328857, R2 0.6116997599601746\n",
      "epoch 1157, loss 0.8926026225090027, R2 0.6087927222251892\n",
      "Eval loss 0.9360941052436829, R2 0.6118602752685547\n",
      "epoch 1158, loss 0.8923767805099487, R2 0.6089513301849365\n",
      "Eval loss 0.9358420372009277, R2 0.6120203733444214\n",
      "epoch 1159, loss 0.8921516537666321, R2 0.6091095805168152\n",
      "Eval loss 0.9355907440185547, R2 0.6121801137924194\n",
      "epoch 1160, loss 0.8919271230697632, R2 0.6092674136161804\n",
      "Eval loss 0.9353402256965637, R2 0.612339437007904\n",
      "epoch 1161, loss 0.891703188419342, R2 0.6094249486923218\n",
      "Eval loss 0.935090184211731, R2 0.6124983429908752\n",
      "epoch 1162, loss 0.8914798498153687, R2 0.6095819473266602\n",
      "Eval loss 0.9348409175872803, R2 0.6126569509506226\n",
      "epoch 1163, loss 0.8912572860717773, R2 0.6097385883331299\n",
      "Eval loss 0.9345923066139221, R2 0.6128152012825012\n",
      "epoch 1164, loss 0.8910351395606995, R2 0.6098948121070862\n",
      "Eval loss 0.9343442916870117, R2 0.6129729747772217\n",
      "epoch 1165, loss 0.8908137679100037, R2 0.6100507974624634\n",
      "Eval loss 0.9340970516204834, R2 0.613130509853363\n",
      "epoch 1166, loss 0.8905929327011108, R2 0.6102062463760376\n",
      "Eval loss 0.9338504076004028, R2 0.6132875084877014\n",
      "epoch 1167, loss 0.8903727531433105, R2 0.6103613972663879\n",
      "Eval loss 0.9336044192314148, R2 0.6134442090988159\n",
      "epoch 1168, loss 0.8901532292366028, R2 0.6105161309242249\n",
      "Eval loss 0.9333590269088745, R2 0.613600492477417\n",
      "epoch 1169, loss 0.8899343013763428, R2 0.6106704473495483\n",
      "Eval loss 0.9331143498420715, R2 0.6137564778327942\n",
      "epoch 1170, loss 0.8897159695625305, R2 0.6108244061470032\n",
      "Eval loss 0.9328702688217163, R2 0.613912045955658\n",
      "epoch 1171, loss 0.889498233795166, R2 0.6109780073165894\n",
      "Eval loss 0.9326269626617432, R2 0.6140672564506531\n",
      "epoch 1172, loss 0.8892812728881836, R2 0.6111311316490173\n",
      "Eval loss 0.9323842525482178, R2 0.6142221093177795\n",
      "epoch 1173, loss 0.889064610004425, R2 0.6112840175628662\n",
      "Eval loss 0.9321420192718506, R2 0.6143765449523926\n",
      "epoch 1174, loss 0.8888487219810486, R2 0.6114364266395569\n",
      "Eval loss 0.9319005608558655, R2 0.614530622959137\n",
      "epoch 1175, loss 0.8886335492134094, R2 0.6115884780883789\n",
      "Eval loss 0.9316598176956177, R2 0.6146844029426575\n",
      "epoch 1176, loss 0.8884187936782837, R2 0.6117401719093323\n",
      "Eval loss 0.9314195513725281, R2 0.6148377656936646\n",
      "epoch 1177, loss 0.8882046937942505, R2 0.6118914484977722\n",
      "Eval loss 0.9311800599098206, R2 0.614990770816803\n",
      "epoch 1178, loss 0.8879913091659546, R2 0.6120423674583435\n",
      "Eval loss 0.9309410452842712, R2 0.6151434183120728\n",
      "epoch 1179, loss 0.8877783417701721, R2 0.6121929287910461\n",
      "Eval loss 0.9307027459144592, R2 0.6152956485748291\n",
      "epoch 1180, loss 0.8875660300254822, R2 0.6123430728912354\n",
      "Eval loss 0.9304651021957397, R2 0.6154476404190063\n",
      "epoch 1181, loss 0.88735431432724, R2 0.6124929189682007\n",
      "Eval loss 0.930228054523468, R2 0.6155991554260254\n",
      "epoch 1182, loss 0.8871431946754456, R2 0.6126423478126526\n",
      "Eval loss 0.9299916625022888, R2 0.6157503724098206\n",
      "epoch 1183, loss 0.8869327306747437, R2 0.6127913594245911\n",
      "Eval loss 0.9297558665275574, R2 0.6159012317657471\n",
      "epoch 1184, loss 0.8867226839065552, R2 0.6129400730133057\n",
      "Eval loss 0.9295207858085632, R2 0.6160516738891602\n",
      "epoch 1185, loss 0.8865134716033936, R2 0.6130884289741516\n",
      "Eval loss 0.9292862415313721, R2 0.6162018179893494\n",
      "epoch 1186, loss 0.8863046765327454, R2 0.6132363677024841\n",
      "Eval loss 0.9290522933006287, R2 0.6163515448570251\n",
      "epoch 1187, loss 0.8860964179039001, R2 0.6133840084075928\n",
      "Eval loss 0.9288188815116882, R2 0.616500973701477\n",
      "epoch 1188, loss 0.8858888745307922, R2 0.6135311722755432\n",
      "Eval loss 0.9285861253738403, R2 0.6166500449180603\n",
      "epoch 1189, loss 0.8856818079948425, R2 0.6136780977249146\n",
      "Eval loss 0.9283541440963745, R2 0.6167987585067749\n",
      "epoch 1190, loss 0.8854753375053406, R2 0.6138246059417725\n",
      "Eval loss 0.9281226992607117, R2 0.6169471144676208\n",
      "epoch 1191, loss 0.8852694630622864, R2 0.6139707565307617\n",
      "Eval loss 0.9278917908668518, R2 0.6170951724052429\n",
      "epoch 1192, loss 0.8850641250610352, R2 0.6141165494918823\n",
      "Eval loss 0.9276614785194397, R2 0.6172428131103516\n",
      "epoch 1193, loss 0.8848593831062317, R2 0.6142619848251343\n",
      "Eval loss 0.9274318218231201, R2 0.6173900961875916\n",
      "epoch 1194, loss 0.8846551775932312, R2 0.6144070625305176\n",
      "Eval loss 0.9272027611732483, R2 0.6175371408462524\n",
      "epoch 1195, loss 0.8844515085220337, R2 0.6145517826080322\n",
      "Eval loss 0.9269742369651794, R2 0.6176837086677551\n",
      "epoch 1196, loss 0.8842485547065735, R2 0.6146961450576782\n",
      "Eval loss 0.9267464280128479, R2 0.6178300380706787\n",
      "epoch 1197, loss 0.8840460777282715, R2 0.6148402094841003\n",
      "Eval loss 0.9265191555023193, R2 0.6179759502410889\n",
      "epoch 1198, loss 0.8838440775871277, R2 0.6149837970733643\n",
      "Eval loss 0.9262925386428833, R2 0.6181215643882751\n",
      "epoch 1199, loss 0.8836427330970764, R2 0.6151271462440491\n",
      "Eval loss 0.9260663986206055, R2 0.6182668209075928\n",
      "epoch 1200, loss 0.8834419250488281, R2 0.6152700185775757\n",
      "Eval loss 0.9258409142494202, R2 0.618411660194397\n",
      "epoch 1201, loss 0.8832416534423828, R2 0.6154126524925232\n",
      "Eval loss 0.9256161451339722, R2 0.6185562014579773\n",
      "epoch 1202, loss 0.8830420970916748, R2 0.6155548691749573\n",
      "Eval loss 0.9253917932510376, R2 0.6187005043029785\n",
      "epoch 1203, loss 0.8828428387641907, R2 0.6156968474388123\n",
      "Eval loss 0.925167977809906, R2 0.6188443899154663\n",
      "epoch 1204, loss 0.8826443552970886, R2 0.615838348865509\n",
      "Eval loss 0.9249449372291565, R2 0.6189879179000854\n",
      "epoch 1205, loss 0.8824462294578552, R2 0.6159795522689819\n",
      "Eval loss 0.9247223734855652, R2 0.6191311478614807\n",
      "epoch 1206, loss 0.8822487592697144, R2 0.6161203980445862\n",
      "Eval loss 0.9245003461837769, R2 0.6192740797996521\n",
      "epoch 1207, loss 0.8820517659187317, R2 0.6162608861923218\n",
      "Eval loss 0.9242790341377258, R2 0.6194164752960205\n",
      "epoch 1208, loss 0.8818553686141968, R2 0.6164011359214783\n",
      "Eval loss 0.9240581393241882, R2 0.6195587515830994\n",
      "epoch 1209, loss 0.8816594481468201, R2 0.6165409088134766\n",
      "Eval loss 0.9238380789756775, R2 0.6197006106376648\n",
      "epoch 1210, loss 0.8814641237258911, R2 0.616680383682251\n",
      "Eval loss 0.9236182570457458, R2 0.6198421716690063\n",
      "epoch 1211, loss 0.8812693357467651, R2 0.6168195009231567\n",
      "Eval loss 0.9233991503715515, R2 0.6199833750724792\n",
      "epoch 1212, loss 0.8810752034187317, R2 0.6169583797454834\n",
      "Eval loss 0.9231806397438049, R2 0.6201242804527283\n",
      "epoch 1213, loss 0.8808813691139221, R2 0.6170968413352966\n",
      "Eval loss 0.9229627251625061, R2 0.6202647686004639\n",
      "epoch 1214, loss 0.8806881904602051, R2 0.6172348856925964\n",
      "Eval loss 0.922745406627655, R2 0.6204050779342651\n",
      "epoch 1215, loss 0.880495548248291, R2 0.6173727512359619\n",
      "Eval loss 0.9225285053253174, R2 0.620544970035553\n",
      "epoch 1216, loss 0.8803034424781799, R2 0.617510199546814\n",
      "Eval loss 0.9223122596740723, R2 0.6206845045089722\n",
      "epoch 1217, loss 0.8801118731498718, R2 0.6176472902297974\n",
      "Eval loss 0.9220966100692749, R2 0.6208237409591675\n",
      "epoch 1218, loss 0.8799207806587219, R2 0.6177840828895569\n",
      "Eval loss 0.921881377696991, R2 0.6209626793861389\n",
      "epoch 1219, loss 0.8797302842140198, R2 0.6179205179214478\n",
      "Eval loss 0.9216667413711548, R2 0.6211012601852417\n",
      "epoch 1220, loss 0.8795403242111206, R2 0.6180565357208252\n",
      "Eval loss 0.9214527606964111, R2 0.6212396025657654\n",
      "epoch 1221, loss 0.8793507814407349, R2 0.6181924343109131\n",
      "Eval loss 0.9212393760681152, R2 0.6213775277137756\n",
      "epoch 1222, loss 0.8791617751121521, R2 0.6183278560638428\n",
      "Eval loss 0.921026349067688, R2 0.6215150952339172\n",
      "epoch 1223, loss 0.8789733052253723, R2 0.6184629797935486\n",
      "Eval loss 0.9208140969276428, R2 0.6216524243354797\n",
      "epoch 1224, loss 0.8787853121757507, R2 0.6185977458953857\n",
      "Eval loss 0.9206022024154663, R2 0.6217894554138184\n",
      "epoch 1225, loss 0.8785980343818665, R2 0.6187323331832886\n",
      "Eval loss 0.9203909039497375, R2 0.6219260692596436\n",
      "epoch 1226, loss 0.8784109354019165, R2 0.6188663840293884\n",
      "Eval loss 0.9201802015304565, R2 0.6220624446868896\n",
      "epoch 1227, loss 0.8782246708869934, R2 0.619000256061554\n",
      "Eval loss 0.9199700951576233, R2 0.6221985220909119\n",
      "epoch 1228, loss 0.8780387043952942, R2 0.619133710861206\n",
      "Eval loss 0.9197602868080139, R2 0.6223342418670654\n",
      "epoch 1229, loss 0.8778533339500427, R2 0.619266927242279\n",
      "Eval loss 0.9195513129234314, R2 0.6224696040153503\n",
      "epoch 1230, loss 0.8776684999465942, R2 0.6193997263908386\n",
      "Eval loss 0.9193427562713623, R2 0.6226047277450562\n",
      "epoch 1231, loss 0.8774840235710144, R2 0.6195322275161743\n",
      "Eval loss 0.9191347360610962, R2 0.6227394938468933\n",
      "epoch 1232, loss 0.8773002624511719, R2 0.6196644306182861\n",
      "Eval loss 0.9189273715019226, R2 0.6228739023208618\n",
      "epoch 1233, loss 0.8771169185638428, R2 0.6197963356971741\n",
      "Eval loss 0.9187203645706177, R2 0.6230080723762512\n",
      "epoch 1234, loss 0.8769340515136719, R2 0.6199279427528381\n",
      "Eval loss 0.918513834476471, R2 0.6231419444084167\n",
      "epoch 1235, loss 0.8767516613006592, R2 0.6200591325759888\n",
      "Eval loss 0.9183080792427063, R2 0.6232753992080688\n",
      "epoch 1236, loss 0.8765698075294495, R2 0.6201900839805603\n",
      "Eval loss 0.9181026220321655, R2 0.6234086751937866\n",
      "epoch 1237, loss 0.8763883113861084, R2 0.620320737361908\n",
      "Eval loss 0.9178978800773621, R2 0.6235415935516357\n",
      "epoch 1238, loss 0.8762075901031494, R2 0.6204509735107422\n",
      "Eval loss 0.917693555355072, R2 0.623674213886261\n",
      "epoch 1239, loss 0.8760271668434143, R2 0.6205809116363525\n",
      "Eval loss 0.917489767074585, R2 0.6238064765930176\n",
      "epoch 1240, loss 0.8758472800254822, R2 0.6207106113433838\n",
      "Eval loss 0.9172865748405457, R2 0.6239385008811951\n",
      "epoch 1241, loss 0.8756678700447083, R2 0.6208400130271912\n",
      "Eval loss 0.9170839190483093, R2 0.6240702271461487\n",
      "epoch 1242, loss 0.8754889965057373, R2 0.6209690570831299\n",
      "Eval loss 0.9168816804885864, R2 0.6242015957832336\n",
      "epoch 1243, loss 0.875310480594635, R2 0.6210977435112\n",
      "Eval loss 0.9166800379753113, R2 0.6243326663970947\n",
      "epoch 1244, loss 0.8751325607299805, R2 0.6212261915206909\n",
      "Eval loss 0.9164788722991943, R2 0.6244634389877319\n",
      "epoch 1245, loss 0.8749550580978394, R2 0.6213542819023132\n",
      "Eval loss 0.9162782430648804, R2 0.62459397315979\n",
      "epoch 1246, loss 0.8747782111167908, R2 0.6214820742607117\n",
      "Eval loss 0.9160779714584351, R2 0.6247241497039795\n",
      "epoch 1247, loss 0.8746017217636108, R2 0.6216095685958862\n",
      "Eval loss 0.9158784747123718, R2 0.6248540282249451\n",
      "epoch 1248, loss 0.8744255900382996, R2 0.6217367053031921\n",
      "Eval loss 0.9156793355941772, R2 0.6249836087226868\n",
      "epoch 1249, loss 0.8742501735687256, R2 0.621863603591919\n",
      "Eval loss 0.9154808521270752, R2 0.6251128911972046\n",
      "epoch 1250, loss 0.8740751147270203, R2 0.6219902038574219\n",
      "Eval loss 0.915282666683197, R2 0.6252418756484985\n",
      "epoch 1251, loss 0.8739004731178284, R2 0.6221165060997009\n",
      "Eval loss 0.9150850772857666, R2 0.6253705620765686\n",
      "epoch 1252, loss 0.8737263679504395, R2 0.6222423911094666\n",
      "Eval loss 0.9148879647254944, R2 0.6254990100860596\n",
      "epoch 1253, loss 0.8735527396202087, R2 0.6223680973052979\n",
      "Eval loss 0.9146913886070251, R2 0.6256271004676819\n",
      "epoch 1254, loss 0.8733795881271362, R2 0.6224934458732605\n",
      "Eval loss 0.9144952893257141, R2 0.6257548332214355\n",
      "epoch 1255, loss 0.8732068538665771, R2 0.6226184964179993\n",
      "Eval loss 0.9142999053001404, R2 0.6258823871612549\n",
      "epoch 1256, loss 0.8730347752571106, R2 0.6227432489395142\n",
      "Eval loss 0.9141047596931458, R2 0.6260095834732056\n",
      "epoch 1257, loss 0.8728629350662231, R2 0.62286776304245\n",
      "Eval loss 0.9139101505279541, R2 0.6261364817619324\n",
      "epoch 1258, loss 0.8726915717124939, R2 0.6229919195175171\n",
      "Eval loss 0.9137160778045654, R2 0.6262632012367249\n",
      "epoch 1259, loss 0.8725207448005676, R2 0.6231157779693604\n",
      "Eval loss 0.913522481918335, R2 0.6263895034790039\n",
      "epoch 1260, loss 0.8723503947257996, R2 0.6232393383979797\n",
      "Eval loss 0.9133293628692627, R2 0.6265156269073486\n",
      "epoch 1261, loss 0.8721805810928345, R2 0.6233626008033752\n",
      "Eval loss 0.9131367802619934, R2 0.6266412734985352\n",
      "epoch 1262, loss 0.8720110058784485, R2 0.6234855651855469\n",
      "Eval loss 0.9129446744918823, R2 0.6267668008804321\n",
      "epoch 1263, loss 0.871842086315155, R2 0.6236082315444946\n",
      "Eval loss 0.9127530455589294, R2 0.6268920302391052\n",
      "epoch 1264, loss 0.8716734647750854, R2 0.6237306594848633\n",
      "Eval loss 0.9125619530677795, R2 0.6270169615745544\n",
      "epoch 1265, loss 0.8715054392814636, R2 0.6238527297973633\n",
      "Eval loss 0.9123712778091431, R2 0.627141535282135\n",
      "epoch 1266, loss 0.8713378310203552, R2 0.6239745616912842\n",
      "Eval loss 0.9121810793876648, R2 0.6272658705711365\n",
      "epoch 1267, loss 0.8711706399917603, R2 0.6240960359573364\n",
      "Eval loss 0.9119914770126343, R2 0.6273899078369141\n",
      "epoch 1268, loss 0.8710039258003235, R2 0.6242172718048096\n",
      "Eval loss 0.9118022918701172, R2 0.6275136470794678\n",
      "epoch 1269, loss 0.8708376288414001, R2 0.6243382096290588\n",
      "Eval loss 0.9116136431694031, R2 0.6276372671127319\n",
      "epoch 1270, loss 0.8706718683242798, R2 0.6244588494300842\n",
      "Eval loss 0.9114252328872681, R2 0.6277604699134827\n",
      "epoch 1271, loss 0.8705064058303833, R2 0.6245791912078857\n",
      "Eval loss 0.9112375974655151, R2 0.6278833746910095\n",
      "epoch 1272, loss 0.870341420173645, R2 0.6246992349624634\n",
      "Eval loss 0.9110502600669861, R2 0.6280059218406677\n",
      "epoch 1273, loss 0.8701770305633545, R2 0.6248190402984619\n",
      "Eval loss 0.91086345911026, R2 0.6281283497810364\n",
      "epoch 1274, loss 0.8700129389762878, R2 0.6249384880065918\n",
      "Eval loss 0.9106771945953369, R2 0.6282504200935364\n",
      "epoch 1275, loss 0.869849443435669, R2 0.6250577569007874\n",
      "Eval loss 0.9104912281036377, R2 0.6283721923828125\n",
      "epoch 1276, loss 0.8696862459182739, R2 0.6251766681671143\n",
      "Eval loss 0.910305917263031, R2 0.6284937262535095\n",
      "epoch 1277, loss 0.8695234656333923, R2 0.6252952814102173\n",
      "Eval loss 0.9101209044456482, R2 0.6286150217056274\n",
      "epoch 1278, loss 0.869361162185669, R2 0.6254136562347412\n",
      "Eval loss 0.9099366068840027, R2 0.6287360191345215\n",
      "epoch 1279, loss 0.8691993951797485, R2 0.625531792640686\n",
      "Eval loss 0.9097525477409363, R2 0.6288567185401917\n",
      "epoch 1280, loss 0.869037926197052, R2 0.6256495118141174\n",
      "Eval loss 0.9095689058303833, R2 0.6289771795272827\n",
      "epoch 1281, loss 0.8688768744468689, R2 0.6257671117782593\n",
      "Eval loss 0.9093859791755676, R2 0.6290972828865051\n",
      "epoch 1282, loss 0.8687163591384888, R2 0.6258842349052429\n",
      "Eval loss 0.909203290939331, R2 0.6292172074317932\n",
      "epoch 1283, loss 0.8685562014579773, R2 0.6260012984275818\n",
      "Eval loss 0.9090211391448975, R2 0.6293368339538574\n",
      "epoch 1284, loss 0.8683964610099792, R2 0.6261179447174072\n",
      "Eval loss 0.9088394641876221, R2 0.6294561624526978\n",
      "epoch 1285, loss 0.8682371973991394, R2 0.6262342929840088\n",
      "Eval loss 0.9086582660675049, R2 0.6295751929283142\n",
      "epoch 1286, loss 0.8680784702301025, R2 0.626350462436676\n",
      "Eval loss 0.9084775447845459, R2 0.6296940445899963\n",
      "epoch 1287, loss 0.867919921875, R2 0.6264662742614746\n",
      "Eval loss 0.9082972407341003, R2 0.6298125982284546\n",
      "epoch 1288, loss 0.8677619695663452, R2 0.6265819072723389\n",
      "Eval loss 0.9081173539161682, R2 0.6299309134483337\n",
      "epoch 1289, loss 0.8676043748855591, R2 0.6266971826553345\n",
      "Eval loss 0.9079380035400391, R2 0.6300488114356995\n",
      "epoch 1290, loss 0.8674472570419312, R2 0.626812219619751\n",
      "Eval loss 0.9077590703964233, R2 0.6301665902137756\n",
      "epoch 1291, loss 0.8672904968261719, R2 0.6269269585609436\n",
      "Eval loss 0.9075804352760315, R2 0.6302840709686279\n",
      "epoch 1292, loss 0.8671342134475708, R2 0.6270414590835571\n",
      "Eval loss 0.907402515411377, R2 0.6304013133049011\n",
      "epoch 1293, loss 0.8669783473014832, R2 0.6271557211875916\n",
      "Eval loss 0.9072248935699463, R2 0.6305181980133057\n",
      "epoch 1294, loss 0.8668227791786194, R2 0.6272696256637573\n",
      "Eval loss 0.907047688961029, R2 0.6306348443031311\n",
      "epoch 1295, loss 0.8666678071022034, R2 0.627383291721344\n",
      "Eval loss 0.9068710803985596, R2 0.6307512521743774\n",
      "epoch 1296, loss 0.8665131330490112, R2 0.6274967193603516\n",
      "Eval loss 0.906694769859314, R2 0.6308673620223999\n",
      "epoch 1297, loss 0.8663588166236877, R2 0.6276098489761353\n",
      "Eval loss 0.9065189361572266, R2 0.630983293056488\n",
      "epoch 1298, loss 0.866205096244812, R2 0.6277226805686951\n",
      "Eval loss 0.9063435792922974, R2 0.6310989856719971\n",
      "epoch 1299, loss 0.8660515546798706, R2 0.6278353333473206\n",
      "Eval loss 0.9061685800552368, R2 0.6312143206596375\n",
      "epoch 1300, loss 0.865898609161377, R2 0.6279477477073669\n",
      "Eval loss 0.9059940576553345, R2 0.6313294172286987\n",
      "epoch 1301, loss 0.865746021270752, R2 0.6280597448348999\n",
      "Eval loss 0.9058200716972351, R2 0.6314442157745361\n",
      "epoch 1302, loss 0.8655938506126404, R2 0.6281715631484985\n",
      "Eval loss 0.9056463241577148, R2 0.6315588355064392\n",
      "epoch 1303, loss 0.8654419183731079, R2 0.6282830834388733\n",
      "Eval loss 0.9054731726646423, R2 0.6316731572151184\n",
      "epoch 1304, loss 0.865290641784668, R2 0.628394365310669\n",
      "Eval loss 0.9053003191947937, R2 0.6317872405052185\n",
      "epoch 1305, loss 0.8651397228240967, R2 0.6285054683685303\n",
      "Eval loss 0.905128002166748, R2 0.6319010257720947\n",
      "epoch 1306, loss 0.8649891018867493, R2 0.628616213798523\n",
      "Eval loss 0.9049561619758606, R2 0.6320146322250366\n",
      "epoch 1307, loss 0.8648388385772705, R2 0.6287266612052917\n",
      "Eval loss 0.904784619808197, R2 0.6321278810501099\n",
      "epoch 1308, loss 0.8646891117095947, R2 0.6288368701934814\n",
      "Eval loss 0.9046136140823364, R2 0.6322409510612488\n",
      "epoch 1309, loss 0.8645398020744324, R2 0.6289469003677368\n",
      "Eval loss 0.9044430255889893, R2 0.6323537826538086\n",
      "epoch 1310, loss 0.8643907904624939, R2 0.6290566325187683\n",
      "Eval loss 0.9042727947235107, R2 0.6324663758277893\n",
      "epoch 1311, loss 0.8642420768737793, R2 0.6291660666465759\n",
      "Eval loss 0.9041029810905457, R2 0.6325786113739014\n",
      "epoch 1312, loss 0.8640938997268677, R2 0.6292752623558044\n",
      "Eval loss 0.9039337635040283, R2 0.6326906681060791\n",
      "epoch 1313, loss 0.8639461994171143, R2 0.6293842196464539\n",
      "Eval loss 0.9037647247314453, R2 0.632802426815033\n",
      "epoch 1314, loss 0.8637988567352295, R2 0.6294928789138794\n",
      "Eval loss 0.9035962224006653, R2 0.6329140067100525\n",
      "epoch 1315, loss 0.863651692867279, R2 0.6296014189720154\n",
      "Eval loss 0.9034280776977539, R2 0.6330252289772034\n",
      "epoch 1316, loss 0.863504946231842, R2 0.6297095417976379\n",
      "Eval loss 0.9032604694366455, R2 0.6331363320350647\n",
      "epoch 1317, loss 0.8633588552474976, R2 0.6298174858093262\n",
      "Eval loss 0.9030932188034058, R2 0.6332471370697021\n",
      "epoch 1318, loss 0.8632128834724426, R2 0.6299251914024353\n",
      "Eval loss 0.9029263854026794, R2 0.6333577036857605\n",
      "epoch 1319, loss 0.8630674481391907, R2 0.6300325989723206\n",
      "Eval loss 0.9027599692344666, R2 0.6334679126739502\n",
      "epoch 1320, loss 0.8629224300384521, R2 0.6301397681236267\n",
      "Eval loss 0.9025939106941223, R2 0.6335780024528503\n",
      "epoch 1321, loss 0.8627777099609375, R2 0.6302466988563538\n",
      "Eval loss 0.9024283289909363, R2 0.6336877942085266\n",
      "epoch 1322, loss 0.8626333475112915, R2 0.6303533911705017\n",
      "Eval loss 0.9022631645202637, R2 0.6337974071502686\n",
      "epoch 1323, loss 0.8624893426895142, R2 0.6304597854614258\n",
      "Eval loss 0.9020983576774597, R2 0.6339067220687866\n",
      "epoch 1324, loss 0.8623458743095398, R2 0.6305659413337708\n",
      "Eval loss 0.901934027671814, R2 0.6340157389640808\n",
      "epoch 1325, loss 0.8622025847434998, R2 0.6306718587875366\n",
      "Eval loss 0.9017700552940369, R2 0.6341246366500854\n",
      "epoch 1326, loss 0.8620598316192627, R2 0.6307775378227234\n",
      "Eval loss 0.901606559753418, R2 0.6342331767082214\n",
      "epoch 1327, loss 0.8619174361228943, R2 0.630882978439331\n",
      "Eval loss 0.901443362236023, R2 0.6343415379524231\n",
      "epoch 1328, loss 0.8617752194404602, R2 0.6309881806373596\n",
      "Eval loss 0.9012806415557861, R2 0.6344496607780457\n",
      "epoch 1329, loss 0.8616337180137634, R2 0.6310930848121643\n",
      "Eval loss 0.901118278503418, R2 0.6345575451850891\n",
      "epoch 1330, loss 0.8614923357963562, R2 0.6311978101730347\n",
      "Eval loss 0.9009563326835632, R2 0.6346650719642639\n",
      "epoch 1331, loss 0.8613513708114624, R2 0.6313022971153259\n",
      "Eval loss 0.9007947444915771, R2 0.634772539138794\n",
      "epoch 1332, loss 0.8612107634544373, R2 0.6314064860343933\n",
      "Eval loss 0.9006336331367493, R2 0.6348796486854553\n",
      "epoch 1333, loss 0.8610706925392151, R2 0.6315104365348816\n",
      "Eval loss 0.9004728198051453, R2 0.6349865794181824\n",
      "epoch 1334, loss 0.860930860042572, R2 0.6316141486167908\n",
      "Eval loss 0.9003124833106995, R2 0.6350932717323303\n",
      "epoch 1335, loss 0.8607913851737976, R2 0.6317176222801208\n",
      "Eval loss 0.9001526236534119, R2 0.6351997256278992\n",
      "epoch 1336, loss 0.8606521487236023, R2 0.6318208575248718\n",
      "Eval loss 0.8999930024147034, R2 0.6353058815002441\n",
      "epoch 1337, loss 0.8605135083198547, R2 0.6319238543510437\n",
      "Eval loss 0.8998337984085083, R2 0.6354118585586548\n",
      "epoch 1338, loss 0.860375165939331, R2 0.6320266127586365\n",
      "Eval loss 0.8996750712394714, R2 0.6355176568031311\n",
      "epoch 1339, loss 0.8602372407913208, R2 0.6321291923522949\n",
      "Eval loss 0.899516761302948, R2 0.6356230974197388\n",
      "epoch 1340, loss 0.8600994348526001, R2 0.6322314143180847\n",
      "Eval loss 0.8993586897850037, R2 0.6357284188270569\n",
      "epoch 1341, loss 0.8599621653556824, R2 0.6323334574699402\n",
      "Eval loss 0.8992011547088623, R2 0.6358333826065063\n",
      "epoch 1342, loss 0.8598253130912781, R2 0.6324352622032166\n",
      "Eval loss 0.8990438580513, R2 0.6359381675720215\n",
      "epoch 1343, loss 0.8596886396408081, R2 0.6325368285179138\n",
      "Eval loss 0.8988870978355408, R2 0.6360427737236023\n",
      "epoch 1344, loss 0.8595525622367859, R2 0.6326382160186768\n",
      "Eval loss 0.8987306356430054, R2 0.6361470818519592\n",
      "epoch 1345, loss 0.8594167232513428, R2 0.632739245891571\n",
      "Eval loss 0.8985745906829834, R2 0.6362512111663818\n",
      "epoch 1346, loss 0.8592812418937683, R2 0.6328401565551758\n",
      "Eval loss 0.8984189629554749, R2 0.6363550424575806\n",
      "epoch 1347, loss 0.8591461181640625, R2 0.6329408288002014\n",
      "Eval loss 0.8982635736465454, R2 0.6364587545394897\n",
      "epoch 1348, loss 0.859011173248291, R2 0.6330412030220032\n",
      "Eval loss 0.8981086611747742, R2 0.636562168598175\n",
      "epoch 1349, loss 0.8588769435882568, R2 0.633141279220581\n",
      "Eval loss 0.8979541063308716, R2 0.6366653442382812\n",
      "epoch 1350, loss 0.8587427139282227, R2 0.6332412362098694\n",
      "Eval loss 0.8977999687194824, R2 0.6367682814598083\n",
      "epoch 1351, loss 0.8586090207099915, R2 0.6333410143852234\n",
      "Eval loss 0.8976461887359619, R2 0.6368709802627563\n",
      "epoch 1352, loss 0.8584756851196289, R2 0.6334404349327087\n",
      "Eval loss 0.8974927067756653, R2 0.6369735598564148\n",
      "epoch 1353, loss 0.8583425879478455, R2 0.6335397362709045\n",
      "Eval loss 0.8973398208618164, R2 0.6370758414268494\n",
      "epoch 1354, loss 0.8582098484039307, R2 0.6336386799812317\n",
      "Eval loss 0.8971871733665466, R2 0.6371778845787048\n",
      "epoch 1355, loss 0.8580775856971741, R2 0.6337375044822693\n",
      "Eval loss 0.8970348238945007, R2 0.637279748916626\n",
      "epoch 1356, loss 0.8579455614089966, R2 0.633836030960083\n",
      "Eval loss 0.8968830108642578, R2 0.6373813152313232\n",
      "epoch 1357, loss 0.8578140139579773, R2 0.6339343786239624\n",
      "Eval loss 0.8967313766479492, R2 0.6374827027320862\n",
      "epoch 1358, loss 0.8576827049255371, R2 0.6340324878692627\n",
      "Eval loss 0.8965802788734436, R2 0.6375837922096252\n",
      "epoch 1359, loss 0.8575516939163208, R2 0.6341302990913391\n",
      "Eval loss 0.8964293599128723, R2 0.6376848220825195\n",
      "epoch 1360, loss 0.8574209809303284, R2 0.634227991104126\n",
      "Eval loss 0.8962790966033936, R2 0.6377855539321899\n",
      "epoch 1361, loss 0.8572907447814941, R2 0.6343254446983337\n",
      "Eval loss 0.8961290121078491, R2 0.6378860473632812\n",
      "epoch 1362, loss 0.8571608066558838, R2 0.6344225406646729\n",
      "Eval loss 0.8959794044494629, R2 0.6379863023757935\n",
      "epoch 1363, loss 0.8570312261581421, R2 0.6345195174217224\n",
      "Eval loss 0.8958299160003662, R2 0.6380863189697266\n",
      "epoch 1364, loss 0.856902003288269, R2 0.6346163749694824\n",
      "Eval loss 0.8956810235977173, R2 0.6381862163543701\n",
      "epoch 1365, loss 0.8567730188369751, R2 0.634712815284729\n",
      "Eval loss 0.8955324292182922, R2 0.6382858157157898\n",
      "epoch 1366, loss 0.856644332408905, R2 0.634809136390686\n",
      "Eval loss 0.8953841328620911, R2 0.6383852362632751\n",
      "epoch 1367, loss 0.8565161824226379, R2 0.6349052786827087\n",
      "Eval loss 0.8952363133430481, R2 0.6384844183921814\n",
      "epoch 1368, loss 0.8563881516456604, R2 0.6350011229515076\n",
      "Eval loss 0.895088791847229, R2 0.6385833621025085\n",
      "epoch 1369, loss 0.8562606573104858, R2 0.6350967884063721\n",
      "Eval loss 0.8949416875839233, R2 0.6386821269989014\n",
      "epoch 1370, loss 0.8561333417892456, R2 0.6351921558380127\n",
      "Eval loss 0.894794762134552, R2 0.6387806534767151\n",
      "epoch 1371, loss 0.856006383895874, R2 0.635287344455719\n",
      "Eval loss 0.8946483731269836, R2 0.6388790011405945\n",
      "epoch 1372, loss 0.8558797836303711, R2 0.6353824138641357\n",
      "Eval loss 0.8945022821426392, R2 0.6389771103858948\n",
      "epoch 1373, loss 0.8557535409927368, R2 0.6354771256446838\n",
      "Eval loss 0.8943565487861633, R2 0.639074981212616\n",
      "epoch 1374, loss 0.8556276559829712, R2 0.6355717182159424\n",
      "Eval loss 0.8942111134529114, R2 0.6391726136207581\n",
      "epoch 1375, loss 0.8555019497871399, R2 0.6356659531593323\n",
      "Eval loss 0.8940660357475281, R2 0.6392701864242554\n",
      "epoch 1376, loss 0.855376660823822, R2 0.6357601284980774\n",
      "Eval loss 0.8939214944839478, R2 0.639367401599884\n",
      "epoch 1377, loss 0.855251669883728, R2 0.6358540058135986\n",
      "Eval loss 0.8937771916389465, R2 0.6394644379615784\n",
      "epoch 1378, loss 0.8551269769668579, R2 0.6359476447105408\n",
      "Eval loss 0.8936331868171692, R2 0.6395612359046936\n",
      "epoch 1379, loss 0.8550026416778564, R2 0.6360411643981934\n",
      "Eval loss 0.8934895396232605, R2 0.6396579146385193\n",
      "epoch 1380, loss 0.8548786640167236, R2 0.6361343860626221\n",
      "Eval loss 0.8933462500572205, R2 0.6397542953491211\n",
      "epoch 1381, loss 0.8547549843788147, R2 0.6362274289131165\n",
      "Eval loss 0.8932033181190491, R2 0.6398504972457886\n",
      "epoch 1382, loss 0.8546315431594849, R2 0.6363202929496765\n",
      "Eval loss 0.8930607438087463, R2 0.639946460723877\n",
      "epoch 1383, loss 0.8545085191726685, R2 0.6364128589630127\n",
      "Eval loss 0.8929185271263123, R2 0.640042245388031\n",
      "epoch 1384, loss 0.8543858528137207, R2 0.6365053057670593\n",
      "Eval loss 0.892776608467102, R2 0.640137791633606\n",
      "epoch 1385, loss 0.8542633056640625, R2 0.6365974545478821\n",
      "Eval loss 0.8926350474357605, R2 0.6402330994606018\n",
      "epoch 1386, loss 0.854141354560852, R2 0.6366894245147705\n",
      "Eval loss 0.8924939036369324, R2 0.6403283476829529\n",
      "epoch 1387, loss 0.8540195226669312, R2 0.6367812156677246\n",
      "Eval loss 0.8923529982566833, R2 0.6404232978820801\n",
      "epoch 1388, loss 0.8538979887962341, R2 0.6368728280067444\n",
      "Eval loss 0.892212450504303, R2 0.6405180096626282\n",
      "epoch 1389, loss 0.8537768125534058, R2 0.6369640827178955\n",
      "Eval loss 0.8920722603797913, R2 0.6406125426292419\n",
      "epoch 1390, loss 0.853655993938446, R2 0.6370553374290466\n",
      "Eval loss 0.8919323682785034, R2 0.6407069563865662\n",
      "epoch 1391, loss 0.853535532951355, R2 0.6371462941169739\n",
      "Eval loss 0.891792893409729, R2 0.6408010125160217\n",
      "epoch 1392, loss 0.853415310382843, R2 0.6372369527816772\n",
      "Eval loss 0.8916535973548889, R2 0.6408949494361877\n",
      "epoch 1393, loss 0.8532952666282654, R2 0.6373274922370911\n",
      "Eval loss 0.8915148973464966, R2 0.6409886479377747\n",
      "epoch 1394, loss 0.853175699710846, R2 0.637417733669281\n",
      "Eval loss 0.8913763165473938, R2 0.641082227230072\n",
      "epoch 1395, loss 0.8530564904212952, R2 0.6375078558921814\n",
      "Eval loss 0.8912380933761597, R2 0.6411755084991455\n",
      "epoch 1396, loss 0.8529373407363892, R2 0.6375978589057922\n",
      "Eval loss 0.8911003470420837, R2 0.6412686705589294\n",
      "epoch 1397, loss 0.8528187274932861, R2 0.6376875638961792\n",
      "Eval loss 0.8909627795219421, R2 0.6413614749908447\n",
      "epoch 1398, loss 0.8527002930641174, R2 0.6377769708633423\n",
      "Eval loss 0.8908255696296692, R2 0.6414542198181152\n",
      "epoch 1399, loss 0.8525822162628174, R2 0.6378662586212158\n",
      "Eval loss 0.8906887173652649, R2 0.6415467858314514\n",
      "epoch 1400, loss 0.8524643778800964, R2 0.6379554271697998\n",
      "Eval loss 0.8905522227287292, R2 0.6416391134262085\n",
      "epoch 1401, loss 0.8523469567298889, R2 0.6380442976951599\n",
      "Eval loss 0.8904160261154175, R2 0.6417310833930969\n",
      "epoch 1402, loss 0.8522297739982605, R2 0.6381329894065857\n",
      "Eval loss 0.8902801871299744, R2 0.6418231129646301\n",
      "epoch 1403, loss 0.8521128296852112, R2 0.6382215023040771\n",
      "Eval loss 0.8901445269584656, R2 0.6419147849082947\n",
      "epoch 1404, loss 0.8519962430000305, R2 0.6383097171783447\n",
      "Eval loss 0.8900094032287598, R2 0.6420062780380249\n",
      "epoch 1405, loss 0.8518800735473633, R2 0.638397753238678\n",
      "Eval loss 0.8898744583129883, R2 0.6420975923538208\n",
      "epoch 1406, loss 0.8517640233039856, R2 0.6384856700897217\n",
      "Eval loss 0.8897398710250854, R2 0.6421887278556824\n",
      "epoch 1407, loss 0.8516482710838318, R2 0.638573408126831\n",
      "Eval loss 0.8896056413650513, R2 0.6422797441482544\n",
      "epoch 1408, loss 0.851533055305481, R2 0.6386608481407166\n",
      "Eval loss 0.8894717693328857, R2 0.6423704028129578\n",
      "epoch 1409, loss 0.8514179587364197, R2 0.6387481093406677\n",
      "Eval loss 0.8893380761146545, R2 0.6424609422683716\n",
      "epoch 1410, loss 0.8513031005859375, R2 0.6388351917266846\n",
      "Eval loss 0.8892048001289368, R2 0.6425513625144958\n",
      "epoch 1411, loss 0.8511885404586792, R2 0.6389220952987671\n",
      "Eval loss 0.8890717625617981, R2 0.6426414847373962\n",
      "epoch 1412, loss 0.8510743975639343, R2 0.6390088200569153\n",
      "Eval loss 0.8889390826225281, R2 0.6427314281463623\n",
      "epoch 1413, loss 0.8509604334831238, R2 0.6390953063964844\n",
      "Eval loss 0.8888068199157715, R2 0.6428211331367493\n",
      "epoch 1414, loss 0.8508467674255371, R2 0.6391816139221191\n",
      "Eval loss 0.888674795627594, R2 0.6429107189178467\n",
      "epoch 1415, loss 0.8507333993911743, R2 0.6392677426338196\n",
      "Eval loss 0.8885429501533508, R2 0.643000066280365\n",
      "epoch 1416, loss 0.8506203889846802, R2 0.6393536329269409\n",
      "Eval loss 0.8884116411209106, R2 0.6430892944335938\n",
      "epoch 1417, loss 0.8505076169967651, R2 0.6394393444061279\n",
      "Eval loss 0.8882805109024048, R2 0.6431782841682434\n",
      "epoch 1418, loss 0.8503950834274292, R2 0.6395248770713806\n",
      "Eval loss 0.8881497979164124, R2 0.6432670950889587\n",
      "epoch 1419, loss 0.8502829074859619, R2 0.639610230922699\n",
      "Eval loss 0.8880193829536438, R2 0.6433556079864502\n",
      "epoch 1420, loss 0.8501709699630737, R2 0.6396953463554382\n",
      "Eval loss 0.8878892660140991, R2 0.6434441208839417\n",
      "epoch 1421, loss 0.8500595092773438, R2 0.6397802829742432\n",
      "Eval loss 0.8877593874931335, R2 0.6435323357582092\n",
      "epoch 1422, loss 0.8499481678009033, R2 0.639864981174469\n",
      "Eval loss 0.8876298069953918, R2 0.6436204314231873\n",
      "epoch 1423, loss 0.8498370051383972, R2 0.6399495601654053\n",
      "Eval loss 0.887500524520874, R2 0.6437082886695862\n",
      "epoch 1424, loss 0.8497262001037598, R2 0.6400339603424072\n",
      "Eval loss 0.8873715996742249, R2 0.643795907497406\n",
      "epoch 1425, loss 0.849615752696991, R2 0.6401181221008301\n",
      "Eval loss 0.8872430920600891, R2 0.6438834071159363\n",
      "epoch 1426, loss 0.8495054841041565, R2 0.6402021050453186\n",
      "Eval loss 0.8871147632598877, R2 0.6439706683158875\n",
      "epoch 1427, loss 0.8493955731391907, R2 0.6402859687805176\n",
      "Eval loss 0.8869866728782654, R2 0.6440578103065491\n",
      "epoch 1428, loss 0.8492860198020935, R2 0.6403695940971375\n",
      "Eval loss 0.8868589997291565, R2 0.6441446542739868\n",
      "epoch 1429, loss 0.8491765856742859, R2 0.6404529809951782\n",
      "Eval loss 0.8867316842079163, R2 0.6442314386367798\n",
      "epoch 1430, loss 0.8490674495697021, R2 0.6405361890792847\n",
      "Eval loss 0.8866046071052551, R2 0.6443179845809937\n",
      "epoch 1431, loss 0.8489586114883423, R2 0.6406192779541016\n",
      "Eval loss 0.8864778280258179, R2 0.6444043517112732\n",
      "epoch 1432, loss 0.8488501310348511, R2 0.6407021880149841\n",
      "Eval loss 0.8863514065742493, R2 0.6444905996322632\n",
      "epoch 1433, loss 0.8487418293952942, R2 0.6407848596572876\n",
      "Eval loss 0.886225163936615, R2 0.6445765495300293\n",
      "epoch 1434, loss 0.848633885383606, R2 0.640867292881012\n",
      "Eval loss 0.8860992789268494, R2 0.6446623206138611\n",
      "epoch 1435, loss 0.848526120185852, R2 0.6409496068954468\n",
      "Eval loss 0.8859735727310181, R2 0.6447479724884033\n",
      "epoch 1436, loss 0.848418653011322, R2 0.6410317420959473\n",
      "Eval loss 0.885848343372345, R2 0.6448333859443665\n",
      "epoch 1437, loss 0.8483116030693054, R2 0.6411136984825134\n",
      "Eval loss 0.885723352432251, R2 0.64491868019104\n",
      "epoch 1438, loss 0.8482046127319336, R2 0.6411953568458557\n",
      "Eval loss 0.8855985403060913, R2 0.6450037956237793\n",
      "epoch 1439, loss 0.8480980396270752, R2 0.641277015209198\n",
      "Eval loss 0.8854742646217346, R2 0.6450886726379395\n",
      "epoch 1440, loss 0.8479916453361511, R2 0.6413583755493164\n",
      "Eval loss 0.8853500485420227, R2 0.6451734304428101\n",
      "epoch 1441, loss 0.8478854894638062, R2 0.6414395570755005\n",
      "Eval loss 0.8852261900901794, R2 0.6452580094337463\n",
      "epoch 1442, loss 0.8477796316146851, R2 0.641520619392395\n",
      "Eval loss 0.8851026892662048, R2 0.6453422904014587\n",
      "epoch 1443, loss 0.8476741909980774, R2 0.6416015028953552\n",
      "Eval loss 0.8849796056747437, R2 0.6454265117645264\n",
      "epoch 1444, loss 0.8475688099861145, R2 0.6416820883750916\n",
      "Eval loss 0.8848565220832825, R2 0.6455104351043701\n",
      "epoch 1445, loss 0.8474637866020203, R2 0.6417626738548279\n",
      "Eval loss 0.8847338557243347, R2 0.6455943584442139\n",
      "epoch 1446, loss 0.8473590016365051, R2 0.6418429017066956\n",
      "Eval loss 0.8846114873886108, R2 0.6456779837608337\n",
      "epoch 1447, loss 0.8472546339035034, R2 0.6419230699539185\n",
      "Eval loss 0.8844893574714661, R2 0.6457614302635193\n",
      "epoch 1448, loss 0.8471503257751465, R2 0.6420029997825623\n",
      "Eval loss 0.8843676447868347, R2 0.6458446979522705\n",
      "epoch 1449, loss 0.8470463156700134, R2 0.6420827507972717\n",
      "Eval loss 0.8842461109161377, R2 0.6459278464317322\n",
      "epoch 1450, loss 0.846942663192749, R2 0.6421623229980469\n",
      "Eval loss 0.8841248154640198, R2 0.6460108160972595\n",
      "epoch 1451, loss 0.8468391299247742, R2 0.6422417163848877\n",
      "Eval loss 0.8840038776397705, R2 0.6460936069488525\n",
      "epoch 1452, loss 0.846735954284668, R2 0.642320990562439\n",
      "Eval loss 0.8838832378387451, R2 0.6461760997772217\n",
      "epoch 1453, loss 0.8466330170631409, R2 0.6424000263214111\n",
      "Eval loss 0.883762776851654, R2 0.6462585926055908\n",
      "epoch 1454, loss 0.8465303778648376, R2 0.6424789428710938\n",
      "Eval loss 0.8836427330970764, R2 0.6463408470153809\n",
      "epoch 1455, loss 0.8464279174804688, R2 0.6425576210021973\n",
      "Eval loss 0.8835229873657227, R2 0.6464228630065918\n",
      "epoch 1456, loss 0.8463258147239685, R2 0.6426361799240112\n",
      "Eval loss 0.883403480052948, R2 0.6465047597885132\n",
      "epoch 1457, loss 0.8462238907814026, R2 0.6427145600318909\n",
      "Eval loss 0.8832840919494629, R2 0.6465864777565002\n",
      "epoch 1458, loss 0.8461222648620605, R2 0.6427927017211914\n",
      "Eval loss 0.8831652998924255, R2 0.646668016910553\n",
      "epoch 1459, loss 0.8460208773612976, R2 0.6428706645965576\n",
      "Eval loss 0.8830465078353882, R2 0.6467494368553162\n",
      "epoch 1460, loss 0.8459197282791138, R2 0.6429485082626343\n",
      "Eval loss 0.8829280734062195, R2 0.6468306183815002\n",
      "epoch 1461, loss 0.8458188772201538, R2 0.6430261731147766\n",
      "Eval loss 0.8828099370002747, R2 0.6469116806983948\n",
      "epoch 1462, loss 0.8457182049751282, R2 0.6431036591529846\n",
      "Eval loss 0.8826920986175537, R2 0.6469926238059998\n",
      "epoch 1463, loss 0.8456178307533264, R2 0.6431809663772583\n",
      "Eval loss 0.8825744986534119, R2 0.6470732688903809\n",
      "epoch 1464, loss 0.8455176949501038, R2 0.6432580947875977\n",
      "Eval loss 0.8824571967124939, R2 0.6471537947654724\n",
      "epoch 1465, loss 0.8454177975654602, R2 0.6433350443840027\n",
      "Eval loss 0.8823401927947998, R2 0.6472340822219849\n",
      "epoch 1466, loss 0.8453182578086853, R2 0.6434118151664734\n",
      "Eval loss 0.88222336769104, R2 0.6473143696784973\n",
      "epoch 1467, loss 0.8452187776565552, R2 0.6434884667396545\n",
      "Eval loss 0.8821069002151489, R2 0.6473943591117859\n",
      "epoch 1468, loss 0.8451196551322937, R2 0.6435649394989014\n",
      "Eval loss 0.8819906711578369, R2 0.6474742293357849\n",
      "epoch 1469, loss 0.8450207710266113, R2 0.6436411738395691\n",
      "Eval loss 0.8818746209144592, R2 0.6475538611412048\n",
      "epoch 1470, loss 0.8449222445487976, R2 0.643717348575592\n",
      "Eval loss 0.8817590475082397, R2 0.64763343334198\n",
      "epoch 1471, loss 0.8448238372802734, R2 0.6437932848930359\n",
      "Eval loss 0.8816436529159546, R2 0.647712767124176\n",
      "epoch 1472, loss 0.8447256684303284, R2 0.6438690423965454\n",
      "Eval loss 0.8815284967422485, R2 0.6477919220924377\n",
      "epoch 1473, loss 0.844627857208252, R2 0.6439446806907654\n",
      "Eval loss 0.8814136385917664, R2 0.6478709578514099\n",
      "epoch 1474, loss 0.8445301651954651, R2 0.6440200805664062\n",
      "Eval loss 0.8812990188598633, R2 0.6479498147964478\n",
      "epoch 1475, loss 0.8444327116012573, R2 0.6440954208374023\n",
      "Eval loss 0.8811846375465393, R2 0.6480284929275513\n",
      "epoch 1476, loss 0.8443355560302734, R2 0.6441705226898193\n",
      "Eval loss 0.8810706734657288, R2 0.6481069922447205\n",
      "epoch 1477, loss 0.8442386388778687, R2 0.6442453861236572\n",
      "Eval loss 0.8809568285942078, R2 0.6481853127479553\n",
      "epoch 1478, loss 0.844141960144043, R2 0.6443202495574951\n",
      "Eval loss 0.8808432817459106, R2 0.6482635140419006\n",
      "epoch 1479, loss 0.8440454602241516, R2 0.6443948745727539\n",
      "Eval loss 0.8807300329208374, R2 0.6483415961265564\n",
      "epoch 1480, loss 0.8439491987228394, R2 0.6444693207740784\n",
      "Eval loss 0.8806170225143433, R2 0.6484194397926331\n",
      "epoch 1481, loss 0.8438533544540405, R2 0.6445435881614685\n",
      "Eval loss 0.8805042505264282, R2 0.6484971642494202\n",
      "epoch 1482, loss 0.8437576293945312, R2 0.6446176767349243\n",
      "Eval loss 0.8803918957710266, R2 0.648574709892273\n",
      "epoch 1483, loss 0.8436620831489563, R2 0.6446917057037354\n",
      "Eval loss 0.8802794814109802, R2 0.6486520767211914\n",
      "epoch 1484, loss 0.8435668349266052, R2 0.6447654366493225\n",
      "Eval loss 0.8801676034927368, R2 0.6487293243408203\n",
      "epoch 1485, loss 0.8434719443321228, R2 0.6448390483856201\n",
      "Eval loss 0.8800559043884277, R2 0.6488063335418701\n",
      "epoch 1486, loss 0.8433771133422852, R2 0.6449124813079834\n",
      "Eval loss 0.8799444437026978, R2 0.6488832235336304\n",
      "epoch 1487, loss 0.8432825803756714, R2 0.6449857950210571\n",
      "Eval loss 0.8798331618309021, R2 0.6489599943161011\n",
      "epoch 1488, loss 0.8431882858276367, R2 0.6450589895248413\n",
      "Eval loss 0.8797222971916199, R2 0.6490365862846375\n",
      "epoch 1489, loss 0.8430942893028259, R2 0.6451320052146912\n",
      "Eval loss 0.879611611366272, R2 0.6491129398345947\n",
      "epoch 1490, loss 0.8430003523826599, R2 0.6452047824859619\n",
      "Eval loss 0.8795010447502136, R2 0.6491892337799072\n",
      "epoch 1491, loss 0.8429068922996521, R2 0.6452774405479431\n",
      "Eval loss 0.8793909549713135, R2 0.6492652893066406\n",
      "epoch 1492, loss 0.8428133130073547, R2 0.64534991979599\n",
      "Eval loss 0.8792810440063477, R2 0.6493412852287292\n",
      "epoch 1493, loss 0.8427202701568604, R2 0.6454223394393921\n",
      "Eval loss 0.8791714310646057, R2 0.6494170427322388\n",
      "epoch 1494, loss 0.8426274061203003, R2 0.6454945206642151\n",
      "Eval loss 0.8790619969367981, R2 0.6494926810264587\n",
      "epoch 1495, loss 0.8425347208976746, R2 0.6455665230751038\n",
      "Eval loss 0.8789527416229248, R2 0.6495680809020996\n",
      "epoch 1496, loss 0.8424422740936279, R2 0.6456384062767029\n",
      "Eval loss 0.8788439035415649, R2 0.6496434807777405\n",
      "epoch 1497, loss 0.8423499464988708, R2 0.6457100510597229\n",
      "Eval loss 0.8787352442741394, R2 0.6497185230255127\n",
      "epoch 1498, loss 0.8422579765319824, R2 0.6457816958427429\n",
      "Eval loss 0.8786268830299377, R2 0.6497935652732849\n",
      "epoch 1499, loss 0.8421661853790283, R2 0.6458530426025391\n",
      "Eval loss 0.8785186409950256, R2 0.649868369102478\n",
      "epoch 1500, loss 0.8420747518539429, R2 0.6459242105484009\n",
      "Eval loss 0.8784108757972717, R2 0.6499430537223816\n",
      "epoch 1501, loss 0.8419834971427917, R2 0.6459953188896179\n",
      "Eval loss 0.8783031105995178, R2 0.6500176787376404\n",
      "epoch 1502, loss 0.8418923020362854, R2 0.6460662484169006\n",
      "Eval loss 0.8781957626342773, R2 0.6500919461250305\n",
      "epoch 1503, loss 0.8418014645576477, R2 0.6461370587348938\n",
      "Eval loss 0.8780885934829712, R2 0.6501662135124207\n",
      "epoch 1504, loss 0.8417107462882996, R2 0.6462076902389526\n",
      "Eval loss 0.8779816031455994, R2 0.6502403020858765\n",
      "epoch 1505, loss 0.8416203856468201, R2 0.6462780833244324\n",
      "Eval loss 0.8778749704360962, R2 0.6503141522407532\n",
      "epoch 1506, loss 0.8415302634239197, R2 0.6463484168052673\n",
      "Eval loss 0.8777686357498169, R2 0.6503879427909851\n",
      "epoch 1507, loss 0.8414402604103088, R2 0.646418571472168\n",
      "Eval loss 0.8776623010635376, R2 0.6504616141319275\n",
      "epoch 1508, loss 0.8413504958152771, R2 0.6464885473251343\n",
      "Eval loss 0.8775564432144165, R2 0.6505350470542908\n",
      "epoch 1509, loss 0.8412610292434692, R2 0.6465584635734558\n",
      "Eval loss 0.8774508237838745, R2 0.6506083011627197\n",
      "epoch 1510, loss 0.8411716818809509, R2 0.6466281414031982\n",
      "Eval loss 0.8773453235626221, R2 0.6506814360618591\n",
      "epoch 1511, loss 0.8410825729370117, R2 0.6466976404190063\n",
      "Eval loss 0.8772400617599487, R2 0.650754451751709\n",
      "epoch 1512, loss 0.8409937620162964, R2 0.6467670202255249\n",
      "Eval loss 0.8771350979804993, R2 0.6508273482322693\n",
      "epoch 1513, loss 0.8409051895141602, R2 0.6468362808227539\n",
      "Eval loss 0.8770303726196289, R2 0.6509000062942505\n",
      "epoch 1514, loss 0.8408167362213135, R2 0.6469053626060486\n",
      "Eval loss 0.8769257664680481, R2 0.6509724855422974\n",
      "epoch 1515, loss 0.8407285213470459, R2 0.6469742655754089\n",
      "Eval loss 0.8768216371536255, R2 0.6510449051856995\n",
      "epoch 1516, loss 0.8406405448913574, R2 0.6470430493354797\n",
      "Eval loss 0.8767176866531372, R2 0.651117205619812\n",
      "epoch 1517, loss 0.8405527472496033, R2 0.647111713886261\n",
      "Eval loss 0.8766139149665833, R2 0.6511893272399902\n",
      "epoch 1518, loss 0.8404651880264282, R2 0.6471801400184631\n",
      "Eval loss 0.8765103220939636, R2 0.6512612700462341\n",
      "epoch 1519, loss 0.8403778672218323, R2 0.6472485065460205\n",
      "Eval loss 0.8764070272445679, R2 0.6513330936431885\n",
      "epoch 1520, loss 0.8402907252311707, R2 0.6473166942596436\n",
      "Eval loss 0.8763039708137512, R2 0.651404619216919\n",
      "epoch 1521, loss 0.8402038216590881, R2 0.6473847031593323\n",
      "Eval loss 0.8762011528015137, R2 0.651476263999939\n",
      "epoch 1522, loss 0.8401170372962952, R2 0.6474525928497314\n",
      "Eval loss 0.8760985732078552, R2 0.6515476107597351\n",
      "epoch 1523, loss 0.8400305509567261, R2 0.6475203037261963\n",
      "Eval loss 0.8759961128234863, R2 0.6516187191009521\n",
      "epoch 1524, loss 0.8399442434310913, R2 0.6475879549980164\n",
      "Eval loss 0.8758940100669861, R2 0.6516898274421692\n",
      "epoch 1525, loss 0.8398582935333252, R2 0.6476554274559021\n",
      "Eval loss 0.8757921457290649, R2 0.6517608165740967\n",
      "epoch 1526, loss 0.8397723436355591, R2 0.6477227210998535\n",
      "Eval loss 0.8756905198097229, R2 0.6518315672874451\n",
      "epoch 1527, loss 0.8396868109703064, R2 0.6477898359298706\n",
      "Eval loss 0.8755890727043152, R2 0.6519021391868591\n",
      "epoch 1528, loss 0.8396013975143433, R2 0.6478567719459534\n",
      "Eval loss 0.8754879236221313, R2 0.6519726514816284\n",
      "epoch 1529, loss 0.8395160436630249, R2 0.6479236483573914\n",
      "Eval loss 0.8753868937492371, R2 0.6520430445671082\n",
      "epoch 1530, loss 0.83943110704422, R2 0.6479904055595398\n",
      "Eval loss 0.8752861618995667, R2 0.6521131992340088\n",
      "epoch 1531, loss 0.8393462300300598, R2 0.6480570435523987\n",
      "Eval loss 0.8751857280731201, R2 0.6521832942962646\n",
      "epoch 1532, loss 0.8392616510391235, R2 0.6481233835220337\n",
      "Eval loss 0.8750852942466736, R2 0.6522531509399414\n",
      "epoch 1533, loss 0.8391771912574768, R2 0.6481896638870239\n",
      "Eval loss 0.8749854564666748, R2 0.6523229479789734\n",
      "epoch 1534, loss 0.839093029499054, R2 0.6482557654380798\n",
      "Eval loss 0.8748854994773865, R2 0.652392566204071\n",
      "epoch 1535, loss 0.8390090465545654, R2 0.648321807384491\n",
      "Eval loss 0.874785840511322, R2 0.6524620056152344\n",
      "epoch 1536, loss 0.8389252424240112, R2 0.6483876705169678\n",
      "Eval loss 0.8746865391731262, R2 0.6525313854217529\n",
      "epoch 1537, loss 0.8388417363166809, R2 0.6484533548355103\n",
      "Eval loss 0.87458735704422, R2 0.6526005268096924\n",
      "epoch 1538, loss 0.8387583494186401, R2 0.6485189199447632\n",
      "Eval loss 0.8744884729385376, R2 0.6526696681976318\n",
      "epoch 1539, loss 0.8386752605438232, R2 0.6485843658447266\n",
      "Eval loss 0.8743897080421448, R2 0.6527385711669922\n",
      "epoch 1540, loss 0.8385922312736511, R2 0.6486496329307556\n",
      "Eval loss 0.874291181564331, R2 0.6528072953224182\n",
      "epoch 1541, loss 0.8385095000267029, R2 0.6487147808074951\n",
      "Eval loss 0.874193012714386, R2 0.6528759002685547\n",
      "epoch 1542, loss 0.8384268283843994, R2 0.6487798094749451\n",
      "Eval loss 0.8740950226783752, R2 0.6529445052146912\n",
      "epoch 1543, loss 0.8383445739746094, R2 0.6488446593284607\n",
      "Eval loss 0.873997151851654, R2 0.6530128121376038\n",
      "epoch 1544, loss 0.8382624387741089, R2 0.648909330368042\n",
      "Eval loss 0.8738995790481567, R2 0.6530808806419373\n",
      "epoch 1545, loss 0.838180422782898, R2 0.6489738821983337\n",
      "Eval loss 0.8738021850585938, R2 0.6531490683555603\n",
      "epoch 1546, loss 0.8380987048149109, R2 0.6490383744239807\n",
      "Eval loss 0.8737049698829651, R2 0.6532169580459595\n",
      "epoch 1547, loss 0.8380171060562134, R2 0.6491027474403381\n",
      "Eval loss 0.8736081123352051, R2 0.6532847285270691\n",
      "epoch 1548, loss 0.837935745716095, R2 0.6491668224334717\n",
      "Eval loss 0.8735114336013794, R2 0.6533524394035339\n",
      "epoch 1549, loss 0.8378545641899109, R2 0.6492308974266052\n",
      "Eval loss 0.873414933681488, R2 0.6534199118614197\n",
      "epoch 1550, loss 0.8377736210823059, R2 0.6492947936058044\n",
      "Eval loss 0.873318612575531, R2 0.6534872651100159\n",
      "epoch 1551, loss 0.8376927971839905, R2 0.6493585109710693\n",
      "Eval loss 0.8732226490974426, R2 0.6535545587539673\n",
      "epoch 1552, loss 0.8376123309135437, R2 0.6494221091270447\n",
      "Eval loss 0.8731268048286438, R2 0.6536216139793396\n",
      "epoch 1553, loss 0.8375319838523865, R2 0.6494855880737305\n",
      "Eval loss 0.8730310797691345, R2 0.6536886692047119\n",
      "epoch 1554, loss 0.837451696395874, R2 0.6495488882064819\n",
      "Eval loss 0.8729357123374939, R2 0.6537554264068604\n",
      "epoch 1555, loss 0.8373716473579407, R2 0.6496120691299438\n",
      "Eval loss 0.8728405833244324, R2 0.653822124004364\n",
      "epoch 1556, loss 0.8372918963432312, R2 0.6496751308441162\n",
      "Eval loss 0.8727455735206604, R2 0.6538887619972229\n",
      "epoch 1557, loss 0.8372123837471008, R2 0.6497381329536438\n",
      "Eval loss 0.8726507425308228, R2 0.6539551615715027\n",
      "epoch 1558, loss 0.8371328115463257, R2 0.6498008370399475\n",
      "Eval loss 0.872556209564209, R2 0.6540215015411377\n",
      "epoch 1559, loss 0.8370537161827087, R2 0.6498635411262512\n",
      "Eval loss 0.8724617958068848, R2 0.6540876030921936\n",
      "epoch 1560, loss 0.8369746208190918, R2 0.6499260663986206\n",
      "Eval loss 0.8723677396774292, R2 0.6541536450386047\n",
      "epoch 1561, loss 0.8368958830833435, R2 0.6499884724617004\n",
      "Eval loss 0.8722738027572632, R2 0.6542195677757263\n",
      "epoch 1562, loss 0.8368171453475952, R2 0.650050699710846\n",
      "Eval loss 0.8721801042556763, R2 0.6542853116989136\n",
      "epoch 1563, loss 0.8367387056350708, R2 0.6501128077507019\n",
      "Eval loss 0.8720866441726685, R2 0.6543508768081665\n",
      "epoch 1564, loss 0.8366605043411255, R2 0.6501747965812683\n",
      "Eval loss 0.871993362903595, R2 0.6544163823127747\n",
      "epoch 1565, loss 0.836582362651825, R2 0.6502366065979004\n",
      "Eval loss 0.871900200843811, R2 0.654481828212738\n",
      "epoch 1566, loss 0.8365046381950378, R2 0.6502983570098877\n",
      "Eval loss 0.871807336807251, R2 0.6545469760894775\n",
      "epoch 1567, loss 0.836426854133606, R2 0.6503599286079407\n",
      "Eval loss 0.8717148303985596, R2 0.6546120643615723\n",
      "epoch 1568, loss 0.8363492488861084, R2 0.6504213809967041\n",
      "Eval loss 0.8716222643852234, R2 0.6546770334243774\n",
      "epoch 1569, loss 0.8362720012664795, R2 0.650482714176178\n",
      "Eval loss 0.8715299963951111, R2 0.6547418236732483\n",
      "epoch 1570, loss 0.8361947536468506, R2 0.6505439281463623\n",
      "Eval loss 0.8714380264282227, R2 0.6548065543174744\n",
      "epoch 1571, loss 0.8361179232597351, R2 0.6506049633026123\n",
      "Eval loss 0.8713461756706238, R2 0.6548711061477661\n",
      "epoch 1572, loss 0.8360410928726196, R2 0.6506658792495728\n",
      "Eval loss 0.8712545037269592, R2 0.6549355983734131\n",
      "epoch 1573, loss 0.8359645009040833, R2 0.6507267355918884\n",
      "Eval loss 0.8711630702018738, R2 0.6549997925758362\n",
      "epoch 1574, loss 0.835888147354126, R2 0.650787353515625\n",
      "Eval loss 0.8710719347000122, R2 0.655064046382904\n",
      "epoch 1575, loss 0.835811972618103, R2 0.650847852230072\n",
      "Eval loss 0.8709807991981506, R2 0.6551280617713928\n",
      "epoch 1576, loss 0.8357357978820801, R2 0.6509082913398743\n",
      "Eval loss 0.8708900809288025, R2 0.655191957950592\n",
      "epoch 1577, loss 0.8356599807739258, R2 0.650968611240387\n",
      "Eval loss 0.8707995414733887, R2 0.6552557349205017\n",
      "epoch 1578, loss 0.8355844020843506, R2 0.6510286927223206\n",
      "Eval loss 0.8707090020179749, R2 0.6553194522857666\n",
      "epoch 1579, loss 0.8355088829994202, R2 0.6510887145996094\n",
      "Eval loss 0.8706187605857849, R2 0.6553829908370972\n",
      "epoch 1580, loss 0.8354336023330688, R2 0.6511486768722534\n",
      "Eval loss 0.8705287575721741, R2 0.6554464101791382\n",
      "epoch 1581, loss 0.8353585004806519, R2 0.6512084007263184\n",
      "Eval loss 0.8704389929771423, R2 0.6555097103118896\n",
      "epoch 1582, loss 0.8352835774421692, R2 0.6512680649757385\n",
      "Eval loss 0.8703494071960449, R2 0.6555728316307068\n",
      "epoch 1583, loss 0.8352088332176208, R2 0.6513275504112244\n",
      "Eval loss 0.8702600598335266, R2 0.6556358337402344\n",
      "epoch 1584, loss 0.8351342082023621, R2 0.6513869762420654\n",
      "Eval loss 0.8701708316802979, R2 0.6556987166404724\n",
      "epoch 1585, loss 0.8350597620010376, R2 0.6514461636543274\n",
      "Eval loss 0.8700817823410034, R2 0.6557615399360657\n",
      "epoch 1586, loss 0.834985613822937, R2 0.6515053510665894\n",
      "Eval loss 0.8699929118156433, R2 0.6558240652084351\n",
      "epoch 1587, loss 0.8349114656448364, R2 0.651564359664917\n",
      "Eval loss 0.8699042797088623, R2 0.6558866500854492\n",
      "epoch 1588, loss 0.8348376154899597, R2 0.6516233086585999\n",
      "Eval loss 0.8698158860206604, R2 0.655949056148529\n",
      "epoch 1589, loss 0.8347640037536621, R2 0.6516819596290588\n",
      "Eval loss 0.869727611541748, R2 0.6560113430023193\n",
      "epoch 1590, loss 0.834690511226654, R2 0.6517406105995178\n",
      "Eval loss 0.8696397542953491, R2 0.6560734510421753\n",
      "epoch 1591, loss 0.8346170783042908, R2 0.6517991423606873\n",
      "Eval loss 0.8695518374443054, R2 0.6561354994773865\n",
      "epoch 1592, loss 0.8345439434051514, R2 0.6518574357032776\n",
      "Eval loss 0.8694642186164856, R2 0.6561973690986633\n",
      "epoch 1593, loss 0.8344710469245911, R2 0.6519157290458679\n",
      "Eval loss 0.8693768382072449, R2 0.6562591791152954\n",
      "epoch 1594, loss 0.8343982696533203, R2 0.6519739031791687\n",
      "Eval loss 0.8692895174026489, R2 0.6563207507133484\n",
      "epoch 1595, loss 0.8343254327774048, R2 0.6520318984985352\n",
      "Eval loss 0.8692024946212769, R2 0.6563823223114014\n",
      "epoch 1596, loss 0.8342530131340027, R2 0.6520897150039673\n",
      "Eval loss 0.8691155910491943, R2 0.6564436554908752\n",
      "epoch 1597, loss 0.8341807723045349, R2 0.6521475315093994\n",
      "Eval loss 0.8690289258956909, R2 0.6565049290657043\n",
      "epoch 1598, loss 0.8341086506843567, R2 0.6522051692008972\n",
      "Eval loss 0.8689424991607666, R2 0.6565661430358887\n",
      "epoch 1599, loss 0.8340365886688232, R2 0.6522626876831055\n",
      "Eval loss 0.8688561916351318, R2 0.6566271781921387\n",
      "epoch 1600, loss 0.8339649438858032, R2 0.6523200273513794\n",
      "Eval loss 0.8687700629234314, R2 0.6566880941390991\n",
      "epoch 1601, loss 0.8338932991027832, R2 0.6523773670196533\n",
      "Eval loss 0.8686841130256653, R2 0.6567488312721252\n",
      "epoch 1602, loss 0.8338218927383423, R2 0.6524344086647034\n",
      "Eval loss 0.868598461151123, R2 0.6568095684051514\n",
      "epoch 1603, loss 0.8337506055831909, R2 0.6524915099143982\n",
      "Eval loss 0.8685129880905151, R2 0.6568700671195984\n",
      "epoch 1604, loss 0.8336794972419739, R2 0.6525484323501587\n",
      "Eval loss 0.8684276342391968, R2 0.6569305062294006\n",
      "epoch 1605, loss 0.8336086869239807, R2 0.6526052355766296\n",
      "Eval loss 0.8683425188064575, R2 0.6569907665252686\n",
      "epoch 1606, loss 0.8335378170013428, R2 0.6526618599891663\n",
      "Eval loss 0.868257462978363, R2 0.6570509672164917\n",
      "epoch 1607, loss 0.8334671258926392, R2 0.6527184844017029\n",
      "Eval loss 0.8681726455688477, R2 0.6571111083030701\n",
      "epoch 1608, loss 0.833396852016449, R2 0.6527748703956604\n",
      "Eval loss 0.8680881857872009, R2 0.6571710109710693\n",
      "epoch 1609, loss 0.8333266973495483, R2 0.6528311371803284\n",
      "Eval loss 0.8680037260055542, R2 0.657230794429779\n",
      "epoch 1610, loss 0.8332565426826477, R2 0.6528873443603516\n",
      "Eval loss 0.8679195642471313, R2 0.657290518283844\n",
      "epoch 1611, loss 0.8331866264343262, R2 0.6529433727264404\n",
      "Eval loss 0.867835521697998, R2 0.6573501229286194\n",
      "epoch 1612, loss 0.8331168293952942, R2 0.6529994010925293\n",
      "Eval loss 0.8677515983581543, R2 0.6574096083641052\n",
      "epoch 1613, loss 0.8330472111701965, R2 0.6530551314353943\n",
      "Eval loss 0.8676679730415344, R2 0.6574689745903015\n",
      "epoch 1614, loss 0.832977831363678, R2 0.653110921382904\n",
      "Eval loss 0.8675845265388489, R2 0.6575281620025635\n",
      "epoch 1615, loss 0.832908570766449, R2 0.6531665325164795\n",
      "Eval loss 0.8675012588500977, R2 0.6575872302055359\n",
      "epoch 1616, loss 0.8328394889831543, R2 0.6532220244407654\n",
      "Eval loss 0.8674181699752808, R2 0.6576462984085083\n",
      "epoch 1617, loss 0.832770586013794, R2 0.6532773971557617\n",
      "Eval loss 0.8673352003097534, R2 0.6577050685882568\n",
      "epoch 1618, loss 0.8327018618583679, R2 0.6533325910568237\n",
      "Eval loss 0.8672524690628052, R2 0.6577638387680054\n",
      "epoch 1619, loss 0.8326331973075867, R2 0.6533876657485962\n",
      "Eval loss 0.8671699166297913, R2 0.6578224897384644\n",
      "epoch 1620, loss 0.8325648307800293, R2 0.6534426808357239\n",
      "Eval loss 0.8670875430107117, R2 0.6578810214996338\n",
      "epoch 1621, loss 0.8324964642524719, R2 0.6534976363182068\n",
      "Eval loss 0.8670053482055664, R2 0.6579394936561584\n",
      "epoch 1622, loss 0.8324282765388489, R2 0.6535524129867554\n",
      "Eval loss 0.8669232726097107, R2 0.657997727394104\n",
      "epoch 1623, loss 0.8323604464530945, R2 0.6536071300506592\n",
      "Eval loss 0.8668414354324341, R2 0.6580559611320496\n",
      "epoch 1624, loss 0.8322926163673401, R2 0.6536616086959839\n",
      "Eval loss 0.8667597770690918, R2 0.6581140160560608\n",
      "epoch 1625, loss 0.83222496509552, R2 0.6537160277366638\n",
      "Eval loss 0.8666782379150391, R2 0.6581720113754272\n",
      "epoch 1626, loss 0.8321574926376343, R2 0.653770387172699\n",
      "Eval loss 0.8665969371795654, R2 0.6582298278808594\n",
      "epoch 1627, loss 0.8320901989936829, R2 0.6538245677947998\n",
      "Eval loss 0.8665159940719604, R2 0.658287525177002\n",
      "epoch 1628, loss 0.8320230841636658, R2 0.6538786292076111\n",
      "Eval loss 0.8664348125457764, R2 0.6583451628684998\n",
      "epoch 1629, loss 0.831956148147583, R2 0.6539326310157776\n",
      "Eval loss 0.86635422706604, R2 0.6584026217460632\n",
      "epoch 1630, loss 0.831889271736145, R2 0.6539865732192993\n",
      "Eval loss 0.8662735223770142, R2 0.6584600210189819\n",
      "epoch 1631, loss 0.8318226337432861, R2 0.6540402770042419\n",
      "Eval loss 0.8661931753158569, R2 0.6585173010826111\n",
      "epoch 1632, loss 0.8317559361457825, R2 0.6540939211845398\n",
      "Eval loss 0.8661129474639893, R2 0.6585744023323059\n",
      "epoch 1633, loss 0.8316896557807922, R2 0.6541473865509033\n",
      "Eval loss 0.8660327792167664, R2 0.658631443977356\n",
      "epoch 1634, loss 0.831623375415802, R2 0.6542007923126221\n",
      "Eval loss 0.8659529685974121, R2 0.6586883664131165\n",
      "epoch 1635, loss 0.8315574526786804, R2 0.654254138469696\n",
      "Eval loss 0.8658731579780579, R2 0.6587452292442322\n",
      "epoch 1636, loss 0.8314914703369141, R2 0.6543073058128357\n",
      "Eval loss 0.8657937049865723, R2 0.6588019132614136\n",
      "epoch 1637, loss 0.8314257860183716, R2 0.6543604135513306\n",
      "Eval loss 0.8657143115997314, R2 0.6588585376739502\n",
      "epoch 1638, loss 0.8313601613044739, R2 0.6544134020805359\n",
      "Eval loss 0.8656349182128906, R2 0.6589149236679077\n",
      "epoch 1639, loss 0.8312947154045105, R2 0.6544662117958069\n",
      "Eval loss 0.8655561208724976, R2 0.65897136926651\n",
      "epoch 1640, loss 0.8312295079231262, R2 0.6545189619064331\n",
      "Eval loss 0.8654772043228149, R2 0.6590275168418884\n",
      "epoch 1641, loss 0.8311644196510315, R2 0.6545715928077698\n",
      "Eval loss 0.8653985261917114, R2 0.6590837836265564\n",
      "epoch 1642, loss 0.8310994505882263, R2 0.6546241044998169\n",
      "Eval loss 0.8653199672698975, R2 0.6591397523880005\n",
      "epoch 1643, loss 0.8310345411300659, R2 0.6546765565872192\n",
      "Eval loss 0.8652416467666626, R2 0.6591957211494446\n",
      "epoch 1644, loss 0.8309699296951294, R2 0.6547288298606873\n",
      "Eval loss 0.8651635050773621, R2 0.6592515707015991\n",
      "epoch 1645, loss 0.8309053778648376, R2 0.6547811031341553\n",
      "Eval loss 0.8650854229927063, R2 0.6593072414398193\n",
      "epoch 1646, loss 0.8308410048484802, R2 0.6548330783843994\n",
      "Eval loss 0.8650076389312744, R2 0.65936279296875\n",
      "epoch 1647, loss 0.8307768106460571, R2 0.6548850536346436\n",
      "Eval loss 0.8649298548698425, R2 0.6594182848930359\n",
      "epoch 1648, loss 0.8307127356529236, R2 0.6549369096755981\n",
      "Eval loss 0.8648524284362793, R2 0.659473717212677\n",
      "epoch 1649, loss 0.8306487798690796, R2 0.654988706111908\n",
      "Eval loss 0.8647751212120056, R2 0.659528911113739\n",
      "epoch 1650, loss 0.8305850625038147, R2 0.6550403833389282\n",
      "Eval loss 0.8646979928016663, R2 0.659584105014801\n",
      "epoch 1651, loss 0.8305214643478394, R2 0.6550918817520142\n",
      "Eval loss 0.8646209239959717, R2 0.6596391797065735\n",
      "epoch 1652, loss 0.8304579854011536, R2 0.6551433205604553\n",
      "Eval loss 0.8645440936088562, R2 0.6596940755844116\n",
      "epoch 1653, loss 0.8303946852684021, R2 0.6551946997642517\n",
      "Eval loss 0.8644675016403198, R2 0.6597489714622498\n",
      "epoch 1654, loss 0.830331563949585, R2 0.655245840549469\n",
      "Eval loss 0.8643909692764282, R2 0.6598036289215088\n",
      "epoch 1655, loss 0.8302684426307678, R2 0.655297040939331\n",
      "Eval loss 0.864314615726471, R2 0.6598582863807678\n",
      "epoch 1656, loss 0.8302056789398193, R2 0.6553480625152588\n",
      "Eval loss 0.864238440990448, R2 0.6599127054214478\n",
      "epoch 1657, loss 0.8301429152488708, R2 0.6553989052772522\n",
      "Eval loss 0.8641624450683594, R2 0.6599671840667725\n",
      "epoch 1658, loss 0.8300804495811462, R2 0.6554497480392456\n",
      "Eval loss 0.8640866279602051, R2 0.6600215435028076\n",
      "epoch 1659, loss 0.8300179243087769, R2 0.6555004119873047\n",
      "Eval loss 0.8640109896659851, R2 0.6600757241249084\n",
      "epoch 1660, loss 0.8299556374549866, R2 0.6555509567260742\n",
      "Eval loss 0.8639354705810547, R2 0.6601297855377197\n",
      "epoch 1661, loss 0.8298934698104858, R2 0.655601441860199\n",
      "Eval loss 0.8638600707054138, R2 0.6601837277412415\n",
      "epoch 1662, loss 0.8298314809799194, R2 0.655651867389679\n",
      "Eval loss 0.8637849688529968, R2 0.6602376103401184\n",
      "epoch 1663, loss 0.8297697305679321, R2 0.6557021141052246\n",
      "Eval loss 0.8637098670005798, R2 0.6602914333343506\n",
      "epoch 1664, loss 0.8297079801559448, R2 0.6557522416114807\n",
      "Eval loss 0.8636349439620972, R2 0.6603450775146484\n",
      "epoch 1665, loss 0.8296464681625366, R2 0.6558023691177368\n",
      "Eval loss 0.8635603785514832, R2 0.6603986620903015\n",
      "epoch 1666, loss 0.829585075378418, R2 0.6558522582054138\n",
      "Eval loss 0.8634858131408691, R2 0.6604520678520203\n",
      "epoch 1667, loss 0.8295236825942993, R2 0.655902087688446\n",
      "Eval loss 0.8634114265441895, R2 0.660505473613739\n",
      "epoch 1668, loss 0.8294625282287598, R2 0.6559519171714783\n",
      "Eval loss 0.8633371591567993, R2 0.6605587005615234\n",
      "epoch 1669, loss 0.8294016122817993, R2 0.6560015082359314\n",
      "Eval loss 0.8632631301879883, R2 0.6606118679046631\n",
      "epoch 1670, loss 0.8293407559394836, R2 0.6560510993003845\n",
      "Eval loss 0.8631892800331116, R2 0.6606648564338684\n",
      "epoch 1671, loss 0.8292800784111023, R2 0.6561005115509033\n",
      "Eval loss 0.8631156086921692, R2 0.660717785358429\n",
      "epoch 1672, loss 0.8292194604873657, R2 0.6561498641967773\n",
      "Eval loss 0.8630419969558716, R2 0.6607706546783447\n",
      "epoch 1673, loss 0.8291590809822083, R2 0.656199038028717\n",
      "Eval loss 0.8629685640335083, R2 0.6608233451843262\n",
      "epoch 1674, loss 0.8290988206863403, R2 0.656248152256012\n",
      "Eval loss 0.8628952503204346, R2 0.6608760356903076\n",
      "epoch 1675, loss 0.8290386199951172, R2 0.6562972664833069\n",
      "Eval loss 0.8628221154212952, R2 0.6609285473823547\n",
      "epoch 1676, loss 0.8289785981178284, R2 0.6563461422920227\n",
      "Eval loss 0.8627490997314453, R2 0.6609809994697571\n",
      "epoch 1677, loss 0.8289188742637634, R2 0.6563949584960938\n",
      "Eval loss 0.8626763820648193, R2 0.6610332727432251\n",
      "epoch 1678, loss 0.8288590908050537, R2 0.6564436554908752\n",
      "Eval loss 0.8626037836074829, R2 0.6610855460166931\n",
      "epoch 1679, loss 0.8287995457649231, R2 0.656492292881012\n",
      "Eval loss 0.8625312447547913, R2 0.661137580871582\n",
      "epoch 1680, loss 0.8287400603294373, R2 0.6565409302711487\n",
      "Eval loss 0.8624590039253235, R2 0.661189615726471\n",
      "epoch 1681, loss 0.8286807537078857, R2 0.6565892696380615\n",
      "Eval loss 0.8623867034912109, R2 0.6612415313720703\n",
      "epoch 1682, loss 0.8286216259002686, R2 0.6566376686096191\n",
      "Eval loss 0.8623148202896118, R2 0.6612933278083801\n",
      "epoch 1683, loss 0.8285624980926514, R2 0.6566858291625977\n",
      "Eval loss 0.8622428774833679, R2 0.6613450646400452\n",
      "epoch 1684, loss 0.8285036683082581, R2 0.6567339301109314\n",
      "Eval loss 0.8621711730957031, R2 0.6613967418670654\n",
      "epoch 1685, loss 0.8284448981285095, R2 0.6567819714546204\n",
      "Eval loss 0.8620995879173279, R2 0.6614482402801514\n",
      "epoch 1686, loss 0.8283863663673401, R2 0.6568298935890198\n",
      "Eval loss 0.8620282411575317, R2 0.6614996194839478\n",
      "epoch 1687, loss 0.8283277750015259, R2 0.6568777561187744\n",
      "Eval loss 0.8619569540023804, R2 0.6615509390830994\n",
      "epoch 1688, loss 0.828269362449646, R2 0.6569254398345947\n",
      "Eval loss 0.8618859052658081, R2 0.6616021394729614\n",
      "epoch 1689, loss 0.8282111883163452, R2 0.6569730639457703\n",
      "Eval loss 0.8618149757385254, R2 0.6616532802581787\n",
      "epoch 1690, loss 0.8281530737876892, R2 0.6570205688476562\n",
      "Eval loss 0.8617440462112427, R2 0.6617043018341064\n",
      "epoch 1691, loss 0.8280951976776123, R2 0.6570680141448975\n",
      "Eval loss 0.8616734743118286, R2 0.6617552042007446\n",
      "epoch 1692, loss 0.828037440776825, R2 0.6571153998374939\n",
      "Eval loss 0.8616029620170593, R2 0.6618061065673828\n",
      "epoch 1693, loss 0.8279796838760376, R2 0.657162606716156\n",
      "Eval loss 0.8615326881408691, R2 0.6618567109107971\n",
      "epoch 1694, loss 0.8279221653938293, R2 0.6572097539901733\n",
      "Eval loss 0.8614624738693237, R2 0.6619073748588562\n",
      "epoch 1695, loss 0.8278647065162659, R2 0.6572568416595459\n",
      "Eval loss 0.8613923788070679, R2 0.6619579195976257\n",
      "epoch 1696, loss 0.8278073668479919, R2 0.6573038101196289\n",
      "Eval loss 0.8613224625587463, R2 0.6620083451271057\n",
      "epoch 1697, loss 0.8277502655982971, R2 0.6573505997657776\n",
      "Eval loss 0.8612526655197144, R2 0.6620586514472961\n",
      "epoch 1698, loss 0.8276933431625366, R2 0.6573973298072815\n",
      "Eval loss 0.8611831665039062, R2 0.6621089577674866\n",
      "epoch 1699, loss 0.8276363611221313, R2 0.6574439406394958\n",
      "Eval loss 0.8611136078834534, R2 0.6621590852737427\n",
      "epoch 1700, loss 0.8275797367095947, R2 0.6574905514717102\n",
      "Eval loss 0.8610444068908691, R2 0.6622090935707092\n",
      "epoch 1701, loss 0.8275230526924133, R2 0.657537043094635\n",
      "Eval loss 0.8609751462936401, R2 0.6622591018676758\n",
      "epoch 1702, loss 0.827466607093811, R2 0.6575834155082703\n",
      "Eval loss 0.860906183719635, R2 0.6623088717460632\n",
      "epoch 1703, loss 0.8274101614952087, R2 0.6576297283172607\n",
      "Eval loss 0.8608373403549194, R2 0.6623586416244507\n",
      "epoch 1704, loss 0.8273538947105408, R2 0.6576758027076721\n",
      "Eval loss 0.8607686161994934, R2 0.6624082922935486\n",
      "epoch 1705, loss 0.8272977471351624, R2 0.6577219367027283\n",
      "Eval loss 0.8607000112533569, R2 0.6624578833580017\n",
      "epoch 1706, loss 0.827241837978363, R2 0.6577679514884949\n",
      "Eval loss 0.8606316447257996, R2 0.6625072956085205\n",
      "epoch 1707, loss 0.827186107635498, R2 0.6578137874603271\n",
      "Eval loss 0.8605633974075317, R2 0.6625567078590393\n",
      "epoch 1708, loss 0.8271303176879883, R2 0.6578596234321594\n",
      "Eval loss 0.8604952096939087, R2 0.6626060009002686\n",
      "epoch 1709, loss 0.8270747661590576, R2 0.6579053401947021\n",
      "Eval loss 0.86042720079422, R2 0.6626551151275635\n",
      "epoch 1710, loss 0.8270192742347717, R2 0.6579509377479553\n",
      "Eval loss 0.8603593707084656, R2 0.6627042889595032\n",
      "epoch 1711, loss 0.8269639611244202, R2 0.657996416091919\n",
      "Eval loss 0.8602917194366455, R2 0.6627532839775085\n",
      "epoch 1712, loss 0.8269087076187134, R2 0.6580418944358826\n",
      "Eval loss 0.8602240681648254, R2 0.6628021001815796\n",
      "epoch 1713, loss 0.8268536925315857, R2 0.6580871939659119\n",
      "Eval loss 0.8601567149162292, R2 0.6628509163856506\n",
      "epoch 1714, loss 0.826798677444458, R2 0.6581324934959412\n",
      "Eval loss 0.8600894212722778, R2 0.6628996133804321\n",
      "epoch 1715, loss 0.8267439603805542, R2 0.6581776142120361\n",
      "Eval loss 0.8600223660469055, R2 0.6629482507705688\n",
      "epoch 1716, loss 0.8266891837120056, R2 0.6582226157188416\n",
      "Eval loss 0.8599552512168884, R2 0.662996768951416\n",
      "epoch 1717, loss 0.8266345858573914, R2 0.658267617225647\n",
      "Eval loss 0.85988849401474, R2 0.6630452275276184\n",
      "epoch 1718, loss 0.8265801668167114, R2 0.6583125591278076\n",
      "Eval loss 0.8598217368125916, R2 0.6630935668945312\n",
      "epoch 1719, loss 0.8265257477760315, R2 0.6583572626113892\n",
      "Eval loss 0.8597551584243774, R2 0.6631417870521545\n",
      "epoch 1720, loss 0.8264715671539307, R2 0.6584020256996155\n",
      "Eval loss 0.8596886396408081, R2 0.6631899476051331\n",
      "epoch 1721, loss 0.8264176249504089, R2 0.6584465503692627\n",
      "Eval loss 0.8596224188804626, R2 0.663237988948822\n",
      "epoch 1722, loss 0.8263635635375977, R2 0.6584910750389099\n",
      "Eval loss 0.8595561981201172, R2 0.663286030292511\n",
      "epoch 1723, loss 0.8263096809387207, R2 0.6585354804992676\n",
      "Eval loss 0.8594902157783508, R2 0.6633338928222656\n",
      "epoch 1724, loss 0.8262560367584229, R2 0.6585797667503357\n",
      "Eval loss 0.859424352645874, R2 0.6633816361427307\n",
      "epoch 1725, loss 0.826202392578125, R2 0.6586240530014038\n",
      "Eval loss 0.859358549118042, R2 0.663429319858551\n",
      "epoch 1726, loss 0.8261489868164062, R2 0.6586681604385376\n",
      "Eval loss 0.8592930436134338, R2 0.6634769439697266\n",
      "epoch 1727, loss 0.826095700263977, R2 0.6587122082710266\n",
      "Eval loss 0.8592276573181152, R2 0.6635244488716125\n",
      "epoch 1728, loss 0.8260424137115479, R2 0.6587561964988708\n",
      "Eval loss 0.8591622114181519, R2 0.6635718941688538\n",
      "epoch 1729, loss 0.8259893655776978, R2 0.6588000655174255\n",
      "Eval loss 0.8590970635414124, R2 0.6636192202568054\n",
      "epoch 1730, loss 0.8259363174438477, R2 0.6588438153266907\n",
      "Eval loss 0.8590320348739624, R2 0.6636664271354675\n",
      "epoch 1731, loss 0.8258835077285767, R2 0.658887505531311\n",
      "Eval loss 0.8589670658111572, R2 0.6637136340141296\n",
      "epoch 1732, loss 0.8258308172225952, R2 0.6589311361312866\n",
      "Eval loss 0.8589023351669312, R2 0.6637606024742126\n",
      "epoch 1733, loss 0.8257782459259033, R2 0.6589745879173279\n",
      "Eval loss 0.8588377237319946, R2 0.6638076305389404\n",
      "epoch 1734, loss 0.8257256746292114, R2 0.6590180993080139\n",
      "Eval loss 0.8587732315063477, R2 0.6638545393943787\n",
      "epoch 1735, loss 0.8256732821464539, R2 0.6590614318847656\n",
      "Eval loss 0.8587087988853455, R2 0.6639013290405273\n",
      "epoch 1736, loss 0.8256210088729858, R2 0.6591047048568726\n",
      "Eval loss 0.8586446046829224, R2 0.6639479994773865\n",
      "epoch 1737, loss 0.8255689144134521, R2 0.6591477990150452\n",
      "Eval loss 0.8585805892944336, R2 0.6639946103096008\n",
      "epoch 1738, loss 0.8255168795585632, R2 0.6591909527778625\n",
      "Eval loss 0.8585165143013, R2 0.6640412211418152\n",
      "epoch 1739, loss 0.8254650235176086, R2 0.6592339277267456\n",
      "Eval loss 0.8584527969360352, R2 0.6640876531600952\n",
      "epoch 1740, loss 0.825413167476654, R2 0.6592768430709839\n",
      "Eval loss 0.8583890199661255, R2 0.6641339659690857\n",
      "epoch 1741, loss 0.8253614902496338, R2 0.6593195796012878\n",
      "Eval loss 0.8583254814147949, R2 0.6641802191734314\n",
      "epoch 1742, loss 0.8253099918365479, R2 0.659362256526947\n",
      "Eval loss 0.8582620620727539, R2 0.6642264127731323\n",
      "epoch 1743, loss 0.8252584934234619, R2 0.659404993057251\n",
      "Eval loss 0.8581987619400024, R2 0.6642724275588989\n",
      "epoch 1744, loss 0.8252072334289551, R2 0.6594475507736206\n",
      "Eval loss 0.858135461807251, R2 0.6643184423446655\n",
      "epoch 1745, loss 0.8251560926437378, R2 0.6594900488853455\n",
      "Eval loss 0.8580726385116577, R2 0.6643643975257874\n",
      "epoch 1746, loss 0.8251049518585205, R2 0.659532368183136\n",
      "Eval loss 0.8580096960067749, R2 0.6644102334976196\n",
      "epoch 1747, loss 0.8250539898872375, R2 0.6595746278762817\n",
      "Eval loss 0.8579469323158264, R2 0.6644559502601624\n",
      "epoch 1748, loss 0.8250032067298889, R2 0.6596168875694275\n",
      "Eval loss 0.8578842282295227, R2 0.6645016074180603\n",
      "epoch 1749, loss 0.8249524235725403, R2 0.6596590280532837\n",
      "Eval loss 0.8578217625617981, R2 0.6645471453666687\n",
      "epoch 1750, loss 0.824901819229126, R2 0.6597009897232056\n",
      "Eval loss 0.857759416103363, R2 0.6645925641059875\n",
      "epoch 1751, loss 0.8248512744903564, R2 0.6597429513931274\n",
      "Eval loss 0.857697069644928, R2 0.6646379828453064\n",
      "epoch 1752, loss 0.824800968170166, R2 0.6597848534584045\n",
      "Eval loss 0.857634961605072, R2 0.6646833419799805\n",
      "epoch 1753, loss 0.8247507214546204, R2 0.6598266363143921\n",
      "Eval loss 0.8575729727745056, R2 0.6647284626960754\n",
      "epoch 1754, loss 0.8247004747390747, R2 0.6598682999610901\n",
      "Eval loss 0.8575111627578735, R2 0.6647736430168152\n",
      "epoch 1755, loss 0.8246504068374634, R2 0.6599099636077881\n",
      "Eval loss 0.8574494123458862, R2 0.6648187041282654\n",
      "epoch 1756, loss 0.8246005177497864, R2 0.6599514484405518\n",
      "Eval loss 0.8573877811431885, R2 0.664863646030426\n",
      "epoch 1757, loss 0.8245508074760437, R2 0.6599929332733154\n",
      "Eval loss 0.857326328754425, R2 0.6649084687232971\n",
      "epoch 1758, loss 0.8245009779930115, R2 0.6600342392921448\n",
      "Eval loss 0.8572649955749512, R2 0.6649532914161682\n",
      "epoch 1759, loss 0.8244514465332031, R2 0.6600755453109741\n",
      "Eval loss 0.8572036623954773, R2 0.6649980545043945\n",
      "epoch 1760, loss 0.8244019746780396, R2 0.6601167917251587\n",
      "Eval loss 0.8571427464485168, R2 0.6650426387786865\n",
      "epoch 1761, loss 0.8243526220321655, R2 0.6601578593254089\n",
      "Eval loss 0.8570816516876221, R2 0.6650871634483337\n",
      "epoch 1762, loss 0.8243032693862915, R2 0.6601989269256592\n",
      "Eval loss 0.8570208549499512, R2 0.6651316285133362\n",
      "epoch 1763, loss 0.8242540955543518, R2 0.6602398157119751\n",
      "Eval loss 0.8569602370262146, R2 0.6651759743690491\n",
      "epoch 1764, loss 0.8242051601409912, R2 0.6602806448936462\n",
      "Eval loss 0.856899619102478, R2 0.665220320224762\n",
      "epoch 1765, loss 0.8241560459136963, R2 0.6603214740753174\n",
      "Eval loss 0.856839120388031, R2 0.6652644276618958\n",
      "epoch 1766, loss 0.8241073489189148, R2 0.660362184047699\n",
      "Eval loss 0.8567787408828735, R2 0.6653085947036743\n",
      "epoch 1767, loss 0.8240585923194885, R2 0.6604028344154358\n",
      "Eval loss 0.8567185997962952, R2 0.6653527021408081\n",
      "epoch 1768, loss 0.8240099549293518, R2 0.6604433655738831\n",
      "Eval loss 0.8566585779190063, R2 0.6653966307640076\n",
      "epoch 1769, loss 0.8239615559577942, R2 0.660483717918396\n",
      "Eval loss 0.8565985560417175, R2 0.6654404997825623\n",
      "epoch 1770, loss 0.8239131569862366, R2 0.6605241894721985\n",
      "Eval loss 0.856538712978363, R2 0.6654842495918274\n",
      "epoch 1771, loss 0.8238648772239685, R2 0.6605644822120667\n",
      "Eval loss 0.8564789891242981, R2 0.6655280590057373\n",
      "epoch 1772, loss 0.8238167762756348, R2 0.6606046557426453\n",
      "Eval loss 0.8564193844795227, R2 0.6655715703964233\n",
      "epoch 1773, loss 0.8237687349319458, R2 0.6606447696685791\n",
      "Eval loss 0.8563598990440369, R2 0.6656151413917542\n",
      "epoch 1774, loss 0.8237208127975464, R2 0.6606848239898682\n",
      "Eval loss 0.8563005924224854, R2 0.6656585931777954\n",
      "epoch 1775, loss 0.8236729502677917, R2 0.6607247591018677\n",
      "Eval loss 0.8562412858009338, R2 0.6657019257545471\n",
      "epoch 1776, loss 0.8236252069473267, R2 0.6607646942138672\n",
      "Eval loss 0.856182336807251, R2 0.6657452583312988\n",
      "epoch 1777, loss 0.8235775828361511, R2 0.6608044505119324\n",
      "Eval loss 0.8561232089996338, R2 0.665788471698761\n",
      "epoch 1778, loss 0.8235300183296204, R2 0.6608442068099976\n",
      "Eval loss 0.8560644388198853, R2 0.6658316254615784\n",
      "epoch 1779, loss 0.8234826922416687, R2 0.6608838438987732\n",
      "Eval loss 0.8560056686401367, R2 0.6658746600151062\n",
      "epoch 1780, loss 0.823435366153717, R2 0.660923421382904\n",
      "Eval loss 0.8559470772743225, R2 0.6659176349639893\n",
      "epoch 1781, loss 0.8233881592750549, R2 0.6609628200531006\n",
      "Eval loss 0.8558884859085083, R2 0.6659605503082275\n",
      "epoch 1782, loss 0.8233410716056824, R2 0.6610023379325867\n",
      "Eval loss 0.8558302521705627, R2 0.6660032868385315\n",
      "epoch 1783, loss 0.8232940435409546, R2 0.6610416769981384\n",
      "Eval loss 0.8557720184326172, R2 0.6660460829734802\n",
      "epoch 1784, loss 0.8232472538948059, R2 0.6610808968544006\n",
      "Eval loss 0.8557137846946716, R2 0.6660888195037842\n",
      "epoch 1785, loss 0.8232004642486572, R2 0.6611200571060181\n",
      "Eval loss 0.8556557297706604, R2 0.6661312580108643\n",
      "epoch 1786, loss 0.8231537938117981, R2 0.6611591577529907\n",
      "Eval loss 0.8555980324745178, R2 0.6661737561225891\n",
      "epoch 1787, loss 0.8231072425842285, R2 0.6611981987953186\n",
      "Eval loss 0.8555401563644409, R2 0.666216254234314\n",
      "epoch 1788, loss 0.8230607509613037, R2 0.6612371206283569\n",
      "Eval loss 0.8554825186729431, R2 0.6662585735321045\n",
      "epoch 1789, loss 0.8230143785476685, R2 0.6612760424613953\n",
      "Eval loss 0.8554248809814453, R2 0.6663007736206055\n",
      "epoch 1790, loss 0.8229681253433228, R2 0.6613147258758545\n",
      "Eval loss 0.8553675413131714, R2 0.6663429737091064\n",
      "epoch 1791, loss 0.8229220509529114, R2 0.6613534688949585\n",
      "Eval loss 0.8553102016448975, R2 0.6663850545883179\n",
      "epoch 1792, loss 0.8228759169578552, R2 0.6613920331001282\n",
      "Eval loss 0.8552530407905579, R2 0.6664270758628845\n",
      "epoch 1793, loss 0.822830080986023, R2 0.6614305973052979\n",
      "Eval loss 0.8551959991455078, R2 0.6664690375328064\n",
      "epoch 1794, loss 0.8227841854095459, R2 0.6614691019058228\n",
      "Eval loss 0.8551390171051025, R2 0.666510820388794\n",
      "epoch 1795, loss 0.8227384686470032, R2 0.6615074872970581\n",
      "Eval loss 0.8550822138786316, R2 0.6665526628494263\n",
      "epoch 1796, loss 0.8226928114891052, R2 0.6615458130836487\n",
      "Eval loss 0.8550254702568054, R2 0.666594386100769\n",
      "epoch 1797, loss 0.822647213935852, R2 0.6615840792655945\n",
      "Eval loss 0.8549687266349792, R2 0.6666359901428223\n",
      "epoch 1798, loss 0.8226017355918884, R2 0.661622166633606\n",
      "Eval loss 0.8549124598503113, R2 0.6666775345802307\n",
      "epoch 1799, loss 0.8225564360618591, R2 0.6616602540016174\n",
      "Eval loss 0.854856014251709, R2 0.6667189598083496\n",
      "epoch 1800, loss 0.8225111961364746, R2 0.6616983413696289\n",
      "Eval loss 0.8547996878623962, R2 0.6667603850364685\n",
      "epoch 1801, loss 0.8224660754203796, R2 0.661736249923706\n",
      "Eval loss 0.8547435998916626, R2 0.6668016910552979\n",
      "epoch 1802, loss 0.8224210739135742, R2 0.6617740988731384\n",
      "Eval loss 0.8546876907348633, R2 0.6668429374694824\n",
      "epoch 1803, loss 0.8223762512207031, R2 0.661811888217926\n",
      "Eval loss 0.8546317219734192, R2 0.6668840646743774\n",
      "epoch 1804, loss 0.822331428527832, R2 0.6618496179580688\n",
      "Eval loss 0.8545759916305542, R2 0.6669251918792725\n",
      "epoch 1805, loss 0.8222866654396057, R2 0.6618872284889221\n",
      "Eval loss 0.8545202612876892, R2 0.6669661998748779\n",
      "epoch 1806, loss 0.822242021560669, R2 0.6619247794151306\n",
      "Eval loss 0.8544646501541138, R2 0.6670070886611938\n",
      "epoch 1807, loss 0.822197437286377, R2 0.6619622707366943\n",
      "Eval loss 0.8544091582298279, R2 0.6670480370521545\n",
      "epoch 1808, loss 0.8221530318260193, R2 0.6619997620582581\n",
      "Eval loss 0.8543538451194763, R2 0.6670887470245361\n",
      "epoch 1809, loss 0.8221087455749512, R2 0.6620370745658875\n",
      "Eval loss 0.8542986512184143, R2 0.6671294569969177\n",
      "epoch 1810, loss 0.8220644593238831, R2 0.6620742678642273\n",
      "Eval loss 0.8542435169219971, R2 0.6671701073646545\n",
      "epoch 1811, loss 0.822020411491394, R2 0.6621115207672119\n",
      "Eval loss 0.8541885614395142, R2 0.6672106385231018\n",
      "epoch 1812, loss 0.8219763040542603, R2 0.6621485352516174\n",
      "Eval loss 0.8541337251663208, R2 0.6672511100769043\n",
      "epoch 1813, loss 0.8219323754310608, R2 0.6621856093406677\n",
      "Eval loss 0.8540788292884827, R2 0.6672915816307068\n",
      "epoch 1814, loss 0.8218885064125061, R2 0.6622226238250732\n",
      "Eval loss 0.8540242314338684, R2 0.667331874370575\n",
      "epoch 1815, loss 0.8218448162078857, R2 0.6622595191001892\n",
      "Eval loss 0.8539696335792542, R2 0.6673721671104431\n",
      "epoch 1816, loss 0.8218011260032654, R2 0.6622963547706604\n",
      "Eval loss 0.8539151549339294, R2 0.6674123406410217\n",
      "epoch 1817, loss 0.8217575550079346, R2 0.662333071231842\n",
      "Eval loss 0.8538607954978943, R2 0.6674524545669556\n",
      "epoch 1818, loss 0.8217141628265381, R2 0.6623697876930237\n",
      "Eval loss 0.8538066148757935, R2 0.6674924492835999\n",
      "epoch 1819, loss 0.8216708302497864, R2 0.6624063849449158\n",
      "Eval loss 0.8537524938583374, R2 0.6675324440002441\n",
      "epoch 1820, loss 0.8216274976730347, R2 0.6624428033828735\n",
      "Eval loss 0.8536984920501709, R2 0.6675723195075989\n",
      "epoch 1821, loss 0.8215843439102173, R2 0.6624793410301208\n",
      "Eval loss 0.8536445498466492, R2 0.6676120758056641\n",
      "epoch 1822, loss 0.8215413093566895, R2 0.6625156998634338\n",
      "Eval loss 0.8535908460617065, R2 0.6676518321037292\n",
      "epoch 1823, loss 0.8214983344078064, R2 0.6625520586967468\n",
      "Eval loss 0.8535370826721191, R2 0.6676914691925049\n",
      "epoch 1824, loss 0.8214554190635681, R2 0.6625882387161255\n",
      "Eval loss 0.8534835577011108, R2 0.6677311062812805\n",
      "epoch 1825, loss 0.8214126229286194, R2 0.6626244187355042\n",
      "Eval loss 0.8534300327301025, R2 0.6677706241607666\n",
      "epoch 1826, loss 0.8213698267936707, R2 0.662660539150238\n",
      "Eval loss 0.8533767461776733, R2 0.6678100824356079\n",
      "epoch 1827, loss 0.8213273882865906, R2 0.6626965999603271\n",
      "Eval loss 0.8533235192298889, R2 0.6678493618965149\n",
      "epoch 1828, loss 0.8212848901748657, R2 0.6627324819564819\n",
      "Eval loss 0.853270411491394, R2 0.6678887009620667\n",
      "epoch 1829, loss 0.8212424516677856, R2 0.6627684235572815\n",
      "Eval loss 0.8532173037528992, R2 0.6679279804229736\n",
      "epoch 1830, loss 0.8212001323699951, R2 0.662804126739502\n",
      "Eval loss 0.8531644344329834, R2 0.6679670810699463\n",
      "epoch 1831, loss 0.8211579322814941, R2 0.6628398895263672\n",
      "Eval loss 0.8531116843223572, R2 0.668006181716919\n",
      "epoch 1832, loss 0.8211157917976379, R2 0.6628755331039429\n",
      "Eval loss 0.853058934211731, R2 0.668045163154602\n",
      "epoch 1833, loss 0.8210737109184265, R2 0.6629111766815186\n",
      "Eval loss 0.8530063629150391, R2 0.6680841445922852\n",
      "epoch 1834, loss 0.8210318088531494, R2 0.6629467010498047\n",
      "Eval loss 0.8529538512229919, R2 0.6681230068206787\n",
      "epoch 1835, loss 0.8209900856018066, R2 0.6629822254180908\n",
      "Eval loss 0.8529013395309448, R2 0.6681618094444275\n",
      "epoch 1836, loss 0.8209481239318848, R2 0.6630175113677979\n",
      "Eval loss 0.8528490662574768, R2 0.6682005524635315\n",
      "epoch 1837, loss 0.8209065794944763, R2 0.6630529165267944\n",
      "Eval loss 0.8527968525886536, R2 0.668239176273346\n",
      "epoch 1838, loss 0.8208649754524231, R2 0.6630881428718567\n",
      "Eval loss 0.8527448773384094, R2 0.6682777404785156\n",
      "epoch 1839, loss 0.8208234906196594, R2 0.6631233096122742\n",
      "Eval loss 0.8526928424835205, R2 0.6683162450790405\n",
      "epoch 1840, loss 0.8207821249961853, R2 0.6631583571434021\n",
      "Eval loss 0.8526409864425659, R2 0.6683547496795654\n",
      "epoch 1841, loss 0.8207407593727112, R2 0.6631934642791748\n",
      "Eval loss 0.8525891304016113, R2 0.668393075466156\n",
      "epoch 1842, loss 0.8206995725631714, R2 0.663228452205658\n",
      "Eval loss 0.8525376319885254, R2 0.6684313416481018\n",
      "epoch 1843, loss 0.8206585645675659, R2 0.6632632613182068\n",
      "Eval loss 0.8524860739707947, R2 0.6684694886207581\n",
      "epoch 1844, loss 0.8206174373626709, R2 0.6632981896400452\n",
      "Eval loss 0.8524344563484192, R2 0.6685077548027039\n",
      "epoch 1845, loss 0.820576548576355, R2 0.6633328795433044\n",
      "Eval loss 0.8523832559585571, R2 0.6685457229614258\n",
      "epoch 1846, loss 0.8205356001853943, R2 0.6633676290512085\n",
      "Eval loss 0.8523320555686951, R2 0.6685838103294373\n",
      "epoch 1847, loss 0.8204948306083679, R2 0.6634021997451782\n",
      "Eval loss 0.8522809147834778, R2 0.6686217784881592\n",
      "epoch 1848, loss 0.8204541802406311, R2 0.6634368300437927\n",
      "Eval loss 0.8522298336029053, R2 0.6686596274375916\n",
      "epoch 1849, loss 0.8204135894775391, R2 0.6634712815284729\n",
      "Eval loss 0.8521789908409119, R2 0.6686974167823792\n",
      "epoch 1850, loss 0.8203732371330261, R2 0.6635057926177979\n",
      "Eval loss 0.8521280884742737, R2 0.6687352657318115\n",
      "epoch 1851, loss 0.8203327655792236, R2 0.6635401248931885\n",
      "Eval loss 0.852077305316925, R2 0.66877281665802\n",
      "epoch 1852, loss 0.8202924132347107, R2 0.6635743379592896\n",
      "Eval loss 0.8520267009735107, R2 0.6688104867935181\n",
      "epoch 1853, loss 0.8202522993087769, R2 0.6636085510253906\n",
      "Eval loss 0.851976215839386, R2 0.6688479781150818\n",
      "epoch 1854, loss 0.820212185382843, R2 0.6636428236961365\n",
      "Eval loss 0.851925790309906, R2 0.6688854694366455\n",
      "epoch 1855, loss 0.8201720714569092, R2 0.6636767983436584\n",
      "Eval loss 0.8518754243850708, R2 0.6689228415489197\n",
      "epoch 1856, loss 0.8201320767402649, R2 0.6637108325958252\n",
      "Eval loss 0.8518252968788147, R2 0.6689601540565491\n",
      "epoch 1857, loss 0.8200921416282654, R2 0.6637448072433472\n",
      "Eval loss 0.8517751097679138, R2 0.6689974069595337\n",
      "epoch 1858, loss 0.8200523853302002, R2 0.6637786626815796\n",
      "Eval loss 0.851725161075592, R2 0.6690346002578735\n",
      "epoch 1859, loss 0.8200128078460693, R2 0.663812518119812\n",
      "Eval loss 0.851675271987915, R2 0.6690717339515686\n",
      "epoch 1860, loss 0.8199731111526489, R2 0.6638462543487549\n",
      "Eval loss 0.851625382900238, R2 0.6691087484359741\n",
      "epoch 1861, loss 0.8199335932731628, R2 0.663879930973053\n",
      "Eval loss 0.8515756130218506, R2 0.6691457629203796\n",
      "epoch 1862, loss 0.8198940753936768, R2 0.6639134883880615\n",
      "Eval loss 0.8515260815620422, R2 0.6691826581954956\n",
      "epoch 1863, loss 0.819854736328125, R2 0.6639471054077148\n",
      "Eval loss 0.8514763712882996, R2 0.6692195534706116\n",
      "epoch 1864, loss 0.8198155164718628, R2 0.6639806032180786\n",
      "Eval loss 0.8514271378517151, R2 0.6692562699317932\n",
      "epoch 1865, loss 0.8197763562202454, R2 0.6640140414237976\n",
      "Eval loss 0.8513776659965515, R2 0.6692929863929749\n",
      "epoch 1866, loss 0.8197371959686279, R2 0.664047360420227\n",
      "Eval loss 0.8513284921646118, R2 0.6693297028541565\n",
      "epoch 1867, loss 0.8196982145309448, R2 0.6640806794166565\n",
      "Eval loss 0.8512793779373169, R2 0.6693662405014038\n",
      "epoch 1868, loss 0.8196592926979065, R2 0.6641138792037964\n",
      "Eval loss 0.851230263710022, R2 0.6694028377532959\n",
      "epoch 1869, loss 0.8196203708648682, R2 0.6641470789909363\n",
      "Eval loss 0.8511813879013062, R2 0.6694392561912537\n",
      "epoch 1870, loss 0.8195816874504089, R2 0.6641801595687866\n",
      "Eval loss 0.8511326313018799, R2 0.6694755554199219\n",
      "epoch 1871, loss 0.8195431232452393, R2 0.664213240146637\n",
      "Eval loss 0.8510838150978088, R2 0.6695118546485901\n",
      "epoch 1872, loss 0.81950443983078, R2 0.664246141910553\n",
      "Eval loss 0.8510351777076721, R2 0.6695481538772583\n",
      "epoch 1873, loss 0.8194659352302551, R2 0.664279043674469\n",
      "Eval loss 0.850986659526825, R2 0.669584333896637\n",
      "epoch 1874, loss 0.8194275498390198, R2 0.6643118262290955\n",
      "Eval loss 0.8509382009506226, R2 0.6696204543113708\n",
      "epoch 1875, loss 0.8193892240524292, R2 0.6643446087837219\n",
      "Eval loss 0.8508898019790649, R2 0.66965651512146\n",
      "epoch 1876, loss 0.8193510174751282, R2 0.6643773317337036\n",
      "Eval loss 0.8508416414260864, R2 0.6696924567222595\n",
      "epoch 1877, loss 0.8193126916885376, R2 0.6644100546836853\n",
      "Eval loss 0.8507933020591736, R2 0.6697284579277039\n",
      "epoch 1878, loss 0.8192746639251709, R2 0.6644425988197327\n",
      "Eval loss 0.8507453799247742, R2 0.6697642803192139\n",
      "epoch 1879, loss 0.819236695766449, R2 0.66447514295578\n",
      "Eval loss 0.8506974577903748, R2 0.6698000431060791\n",
      "epoch 1880, loss 0.8191986680030823, R2 0.6645076274871826\n",
      "Eval loss 0.8506494760513306, R2 0.6698358058929443\n",
      "epoch 1881, loss 0.8191609382629395, R2 0.6645399332046509\n",
      "Eval loss 0.8506017327308655, R2 0.6698715090751648\n",
      "epoch 1882, loss 0.8191231489181519, R2 0.6645722985267639\n",
      "Eval loss 0.8505540490150452, R2 0.6699070334434509\n",
      "epoch 1883, loss 0.8190855979919434, R2 0.6646046042442322\n",
      "Eval loss 0.8505063652992249, R2 0.6699426174163818\n",
      "epoch 1884, loss 0.8190479278564453, R2 0.6646368503570557\n",
      "Eval loss 0.8504589796066284, R2 0.6699780821800232\n",
      "epoch 1885, loss 0.8190104961395264, R2 0.6646689772605896\n",
      "Eval loss 0.850411593914032, R2 0.6700134873390198\n",
      "epoch 1886, loss 0.8189730048179626, R2 0.664700984954834\n",
      "Eval loss 0.8503643274307251, R2 0.6700487732887268\n",
      "epoch 1887, loss 0.8189355731010437, R2 0.6647329926490784\n",
      "Eval loss 0.8503169417381287, R2 0.6700840592384338\n",
      "epoch 1888, loss 0.8188983201980591, R2 0.6647650003433228\n",
      "Eval loss 0.8502699136734009, R2 0.6701192855834961\n",
      "epoch 1889, loss 0.8188611268997192, R2 0.6647968888282776\n",
      "Eval loss 0.8502228260040283, R2 0.6701544523239136\n",
      "epoch 1890, loss 0.818824052810669, R2 0.6648287177085876\n",
      "Eval loss 0.8501759767532349, R2 0.6701894998550415\n",
      "epoch 1891, loss 0.8187870979309082, R2 0.6648604869842529\n",
      "Eval loss 0.8501291871070862, R2 0.6702245473861694\n",
      "epoch 1892, loss 0.8187502026557922, R2 0.6648921370506287\n",
      "Eval loss 0.850082278251648, R2 0.6702594757080078\n",
      "epoch 1893, loss 0.8187132477760315, R2 0.6649237871170044\n",
      "Eval loss 0.8500356674194336, R2 0.6702944040298462\n",
      "epoch 1894, loss 0.8186764121055603, R2 0.6649553775787354\n",
      "Eval loss 0.8499891757965088, R2 0.6703291535377502\n",
      "epoch 1895, loss 0.8186397552490234, R2 0.6649868488311768\n",
      "Eval loss 0.8499426245689392, R2 0.6703639626502991\n",
      "epoch 1896, loss 0.8186030983924866, R2 0.6650184392929077\n",
      "Eval loss 0.8498963117599487, R2 0.6703986525535583\n",
      "epoch 1897, loss 0.8185665607452393, R2 0.6650497913360596\n",
      "Eval loss 0.8498499393463135, R2 0.6704332828521729\n",
      "epoch 1898, loss 0.8185300230979919, R2 0.6650811433792114\n",
      "Eval loss 0.8498038053512573, R2 0.6704679727554321\n",
      "epoch 1899, loss 0.8184936046600342, R2 0.6651124358177185\n",
      "Eval loss 0.8497576117515564, R2 0.6705024242401123\n",
      "epoch 1900, loss 0.8184573650360107, R2 0.665143609046936\n",
      "Eval loss 0.8497116565704346, R2 0.6705369353294373\n",
      "epoch 1901, loss 0.8184211850166321, R2 0.6651747226715088\n",
      "Eval loss 0.8496658205986023, R2 0.6705712676048279\n",
      "epoch 1902, loss 0.8183849453926086, R2 0.6652058959007263\n",
      "Eval loss 0.8496199250221252, R2 0.6706055998802185\n",
      "epoch 1903, loss 0.8183488845825195, R2 0.6652368903160095\n",
      "Eval loss 0.8495742082595825, R2 0.6706399321556091\n",
      "epoch 1904, loss 0.81831294298172, R2 0.6652678847312927\n",
      "Eval loss 0.8495286107063293, R2 0.6706740856170654\n",
      "epoch 1905, loss 0.8182770013809204, R2 0.6652987599372864\n",
      "Eval loss 0.8494829535484314, R2 0.6707082390785217\n",
      "epoch 1906, loss 0.8182411193847656, R2 0.6653296947479248\n",
      "Eval loss 0.8494374752044678, R2 0.6707422733306885\n",
      "epoch 1907, loss 0.8182054162025452, R2 0.6653604507446289\n",
      "Eval loss 0.8493921160697937, R2 0.6707764267921448\n",
      "epoch 1908, loss 0.8181697726249695, R2 0.665391206741333\n",
      "Eval loss 0.8493467569351196, R2 0.670810341835022\n",
      "epoch 1909, loss 0.818134069442749, R2 0.6654219031333923\n",
      "Eval loss 0.8493016958236694, R2 0.6708441376686096\n",
      "epoch 1910, loss 0.8180985450744629, R2 0.6654524803161621\n",
      "Eval loss 0.8492563962936401, R2 0.670877993106842\n",
      "epoch 1911, loss 0.8180630207061768, R2 0.6654830574989319\n",
      "Eval loss 0.8492114543914795, R2 0.6709117889404297\n",
      "epoch 1912, loss 0.818027675151825, R2 0.6655135750770569\n",
      "Eval loss 0.8491664528846741, R2 0.6709455847740173\n",
      "epoch 1913, loss 0.8179923892021179, R2 0.6655439734458923\n",
      "Eval loss 0.849121630191803, R2 0.6709792613983154\n",
      "epoch 1914, loss 0.8179571032524109, R2 0.665574312210083\n",
      "Eval loss 0.8490768671035767, R2 0.671012818813324\n",
      "epoch 1915, loss 0.817922055721283, R2 0.6656046509742737\n",
      "Eval loss 0.8490322232246399, R2 0.6710463166236877\n",
      "epoch 1916, loss 0.8178868889808655, R2 0.6656348705291748\n",
      "Eval loss 0.8489875793457031, R2 0.6710798144340515\n",
      "epoch 1917, loss 0.8178519010543823, R2 0.6656651496887207\n",
      "Eval loss 0.8489431738853455, R2 0.6711132526397705\n",
      "epoch 1918, loss 0.8178169131278992, R2 0.6656953692436218\n",
      "Eval loss 0.8488985896110535, R2 0.6711465716362\n",
      "epoch 1919, loss 0.8177821636199951, R2 0.6657253503799438\n",
      "Eval loss 0.8488544225692749, R2 0.6711798310279846\n",
      "epoch 1920, loss 0.8177473545074463, R2 0.6657554507255554\n",
      "Eval loss 0.8488101959228516, R2 0.6712130904197693\n",
      "epoch 1921, loss 0.8177125453948975, R2 0.6657854318618774\n",
      "Eval loss 0.8487659692764282, R2 0.6712462902069092\n",
      "epoch 1922, loss 0.8176780939102173, R2 0.6658152937889099\n",
      "Eval loss 0.8487218618392944, R2 0.6712793707847595\n",
      "epoch 1923, loss 0.8176434636116028, R2 0.6658451557159424\n",
      "Eval loss 0.848677933216095, R2 0.6713125109672546\n",
      "epoch 1924, loss 0.8176089525222778, R2 0.6658750176429749\n",
      "Eval loss 0.8486340641975403, R2 0.6713454723358154\n",
      "epoch 1925, loss 0.8175745010375977, R2 0.6659047603607178\n",
      "Eval loss 0.8485901951789856, R2 0.6713784337043762\n",
      "epoch 1926, loss 0.8175401091575623, R2 0.6659344434738159\n",
      "Eval loss 0.8485465049743652, R2 0.6714113354682922\n",
      "epoch 1927, loss 0.817505955696106, R2 0.6659640669822693\n",
      "Eval loss 0.8485028147697449, R2 0.6714441776275635\n",
      "epoch 1928, loss 0.8174717426300049, R2 0.6659936308860779\n",
      "Eval loss 0.8484592437744141, R2 0.6714769005775452\n",
      "epoch 1929, loss 0.8174376487731934, R2 0.6660231351852417\n",
      "Eval loss 0.848415732383728, R2 0.6715095639228821\n",
      "epoch 1930, loss 0.8174036145210266, R2 0.6660525798797607\n",
      "Eval loss 0.8483723402023315, R2 0.671542227268219\n",
      "epoch 1931, loss 0.8173696994781494, R2 0.666081964969635\n",
      "Eval loss 0.8483291864395142, R2 0.6715748310089111\n",
      "epoch 1932, loss 0.8173356652259827, R2 0.6661113500595093\n",
      "Eval loss 0.8482858538627625, R2 0.6716073155403137\n",
      "epoch 1933, loss 0.817301869392395, R2 0.666140615940094\n",
      "Eval loss 0.8482427000999451, R2 0.6716398596763611\n",
      "epoch 1934, loss 0.8172681331634521, R2 0.6661698818206787\n",
      "Eval loss 0.848199725151062, R2 0.6716722846031189\n",
      "epoch 1935, loss 0.8172343969345093, R2 0.6661989688873291\n",
      "Eval loss 0.8481566905975342, R2 0.6717045307159424\n",
      "epoch 1936, loss 0.8172008395195007, R2 0.6662281155586243\n",
      "Eval loss 0.8481138944625854, R2 0.6717368960380554\n",
      "epoch 1937, loss 0.817167341709137, R2 0.6662571430206299\n",
      "Eval loss 0.8480710387229919, R2 0.6717691421508789\n",
      "epoch 1938, loss 0.817133903503418, R2 0.6662862300872803\n",
      "Eval loss 0.8480284214019775, R2 0.6718013286590576\n",
      "epoch 1939, loss 0.8171005249023438, R2 0.6663151383399963\n",
      "Eval loss 0.8479856252670288, R2 0.6718334555625916\n",
      "epoch 1940, loss 0.8170670866966248, R2 0.6663440465927124\n",
      "Eval loss 0.8479431867599487, R2 0.6718655228614807\n",
      "epoch 1941, loss 0.8170338869094849, R2 0.6663728952407837\n",
      "Eval loss 0.8479006290435791, R2 0.6718975305557251\n",
      "epoch 1942, loss 0.817000687122345, R2 0.6664016842842102\n",
      "Eval loss 0.8478583097457886, R2 0.6719294786453247\n",
      "epoch 1943, loss 0.8169674873352051, R2 0.6664304137229919\n",
      "Eval loss 0.8478160500526428, R2 0.6719614267349243\n",
      "epoch 1944, loss 0.8169344663619995, R2 0.6664590835571289\n",
      "Eval loss 0.8477737903594971, R2 0.6719932556152344\n",
      "epoch 1945, loss 0.8169016242027283, R2 0.6664876937866211\n",
      "Eval loss 0.8477316498756409, R2 0.6720251441001892\n",
      "epoch 1946, loss 0.8168687224388123, R2 0.6665163040161133\n",
      "Eval loss 0.8476896286010742, R2 0.6720567345619202\n",
      "epoch 1947, loss 0.8168358206748962, R2 0.6665447950363159\n",
      "Eval loss 0.8476475477218628, R2 0.6720884442329407\n",
      "epoch 1948, loss 0.8168030977249146, R2 0.6665732860565186\n",
      "Eval loss 0.8476056456565857, R2 0.6721200942993164\n",
      "epoch 1949, loss 0.8167704343795776, R2 0.6666017174720764\n",
      "Eval loss 0.8475639224052429, R2 0.6721516251564026\n",
      "epoch 1950, loss 0.8167377710342407, R2 0.6666299700737\n",
      "Eval loss 0.8475220203399658, R2 0.672183096408844\n",
      "epoch 1951, loss 0.8167051672935486, R2 0.666658341884613\n",
      "Eval loss 0.8474804759025574, R2 0.6722145676612854\n",
      "epoch 1952, loss 0.8166727423667908, R2 0.666686475276947\n",
      "Eval loss 0.8474389314651489, R2 0.6722460389137268\n",
      "epoch 1953, loss 0.8166402578353882, R2 0.6667146682739258\n",
      "Eval loss 0.8473974466323853, R2 0.6722772717475891\n",
      "epoch 1954, loss 0.8166079521179199, R2 0.6667428612709045\n",
      "Eval loss 0.8473560214042664, R2 0.6723085641860962\n",
      "epoch 1955, loss 0.8165756464004517, R2 0.6667709350585938\n",
      "Eval loss 0.8473146557807922, R2 0.6723398566246033\n",
      "epoch 1956, loss 0.8165435194969177, R2 0.6667989492416382\n",
      "Eval loss 0.8472734093666077, R2 0.672370970249176\n",
      "epoch 1957, loss 0.8165113925933838, R2 0.6668269038200378\n",
      "Eval loss 0.8472322225570679, R2 0.6724021434783936\n",
      "epoch 1958, loss 0.8164792060852051, R2 0.6668547987937927\n",
      "Eval loss 0.8471911549568176, R2 0.6724331378936768\n",
      "epoch 1959, loss 0.8164472579956055, R2 0.6668827533721924\n",
      "Eval loss 0.8471501469612122, R2 0.6724641919136047\n",
      "epoch 1960, loss 0.8164153099060059, R2 0.6669104695320129\n",
      "Eval loss 0.8471091985702515, R2 0.6724951267242432\n",
      "epoch 1961, loss 0.8163835406303406, R2 0.666938304901123\n",
      "Eval loss 0.8470683693885803, R2 0.6725260615348816\n",
      "epoch 1962, loss 0.8163515329360962, R2 0.6669659614562988\n",
      "Eval loss 0.8470275402069092, R2 0.6725568175315857\n",
      "epoch 1963, loss 0.8163199424743652, R2 0.6669936776161194\n",
      "Eval loss 0.8469868898391724, R2 0.6725876331329346\n",
      "epoch 1964, loss 0.8162881731987, R2 0.6670212149620056\n",
      "Eval loss 0.8469461798667908, R2 0.6726184487342834\n",
      "epoch 1965, loss 0.8162566423416138, R2 0.6670488119125366\n",
      "Eval loss 0.8469057083129883, R2 0.6726490259170532\n",
      "epoch 1966, loss 0.8162251710891724, R2 0.6670762896537781\n",
      "Eval loss 0.8468652367591858, R2 0.6726797223091125\n",
      "epoch 1967, loss 0.816193699836731, R2 0.6671037077903748\n",
      "Eval loss 0.8468248248100281, R2 0.6727102398872375\n",
      "epoch 1968, loss 0.8161622881889343, R2 0.6671310663223267\n",
      "Eval loss 0.8467846512794495, R2 0.6727408766746521\n",
      "epoch 1969, loss 0.8161308765411377, R2 0.6671584844589233\n",
      "Eval loss 0.8467442989349365, R2 0.6727712750434875\n",
      "epoch 1970, loss 0.8160995841026306, R2 0.6671857833862305\n",
      "Eval loss 0.8467041850090027, R2 0.6728016138076782\n",
      "epoch 1971, loss 0.8160684704780579, R2 0.667212963104248\n",
      "Eval loss 0.8466640710830688, R2 0.6728320717811584\n",
      "epoch 1972, loss 0.8160372972488403, R2 0.6672401428222656\n",
      "Eval loss 0.8466241359710693, R2 0.6728622913360596\n",
      "epoch 1973, loss 0.8160063028335571, R2 0.6672672629356384\n",
      "Eval loss 0.846584141254425, R2 0.6728925704956055\n",
      "epoch 1974, loss 0.8159752488136292, R2 0.6672943830490112\n",
      "Eval loss 0.8465443253517151, R2 0.6729227304458618\n",
      "epoch 1975, loss 0.815944254398346, R2 0.6673214435577393\n",
      "Eval loss 0.8465045094490051, R2 0.6729528903961182\n",
      "epoch 1976, loss 0.8159134387969971, R2 0.667348325252533\n",
      "Eval loss 0.8464648723602295, R2 0.672982931137085\n",
      "epoch 1977, loss 0.8158826231956482, R2 0.6673753261566162\n",
      "Eval loss 0.8464252352714539, R2 0.6730129718780518\n",
      "epoch 1978, loss 0.8158519268035889, R2 0.6674022078514099\n",
      "Eval loss 0.846385657787323, R2 0.6730430126190186\n",
      "epoch 1979, loss 0.8158211708068848, R2 0.6674289703369141\n",
      "Eval loss 0.8463462591171265, R2 0.673072874546051\n",
      "epoch 1980, loss 0.815790593624115, R2 0.6674557328224182\n",
      "Eval loss 0.8463068008422852, R2 0.6731027960777283\n",
      "epoch 1981, loss 0.8157600164413452, R2 0.6674824953079224\n",
      "Eval loss 0.8462674021720886, R2 0.6731326580047607\n",
      "epoch 1982, loss 0.8157294988632202, R2 0.667509138584137\n",
      "Eval loss 0.8462281823158264, R2 0.6731624007225037\n",
      "epoch 1983, loss 0.8156991004943848, R2 0.6675357818603516\n",
      "Eval loss 0.8461890816688538, R2 0.6731920838356018\n",
      "epoch 1984, loss 0.8156687021255493, R2 0.6675623655319214\n",
      "Eval loss 0.8461500406265259, R2 0.6732217669487\n",
      "epoch 1985, loss 0.8156384825706482, R2 0.6675889492034912\n",
      "Eval loss 0.846110999584198, R2 0.6732513904571533\n",
      "epoch 1986, loss 0.8156082034111023, R2 0.6676153540611267\n",
      "Eval loss 0.8460720777511597, R2 0.6732810139656067\n",
      "epoch 1987, loss 0.8155781030654907, R2 0.6676416993141174\n",
      "Eval loss 0.8460332155227661, R2 0.6733104586601257\n",
      "epoch 1988, loss 0.8155479431152344, R2 0.6676681041717529\n",
      "Eval loss 0.8459944128990173, R2 0.6733399033546448\n",
      "epoch 1989, loss 0.8155179619789124, R2 0.6676944494247437\n",
      "Eval loss 0.8459556698799133, R2 0.6733693480491638\n",
      "epoch 1990, loss 0.8154879212379456, R2 0.6677207350730896\n",
      "Eval loss 0.8459171056747437, R2 0.6733987331390381\n",
      "epoch 1991, loss 0.8154579997062683, R2 0.6677469611167908\n",
      "Eval loss 0.8458784818649292, R2 0.6734279990196228\n",
      "epoch 1992, loss 0.8154280781745911, R2 0.6677730679512024\n",
      "Eval loss 0.8458400368690491, R2 0.6734572649002075\n",
      "epoch 1993, loss 0.8153984546661377, R2 0.6677992343902588\n",
      "Eval loss 0.8458014726638794, R2 0.6734864115715027\n",
      "epoch 1994, loss 0.8153685927391052, R2 0.6678252816200256\n",
      "Eval loss 0.8457631468772888, R2 0.6735156774520874\n",
      "epoch 1995, loss 0.8153389692306519, R2 0.6678513288497925\n",
      "Eval loss 0.8457247614860535, R2 0.6735447645187378\n",
      "epoch 1996, loss 0.8153092861175537, R2 0.6678772568702698\n",
      "Eval loss 0.845686674118042, R2 0.6735737919807434\n",
      "epoch 1997, loss 0.8152798414230347, R2 0.6679031848907471\n",
      "Eval loss 0.8456485867500305, R2 0.673602819442749\n",
      "epoch 1998, loss 0.8152503371238708, R2 0.6679290533065796\n",
      "Eval loss 0.8456104397773743, R2 0.6736317276954651\n",
      "epoch 1999, loss 0.8152208924293518, R2 0.6679548621177673\n",
      "Eval loss 0.8455724716186523, R2 0.6736606359481812\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TEST with model trainer\n",
    "import omegaconf\n",
    "\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.util.model_trainer import ModelTrainerOverriden\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "import mbrl.util.common\n",
    "from mbrl.util.replay_buffer import BootstrapIterator\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 1\n",
    "device = \"cpu\"\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "optim_lr=learningRate\n",
    "model_wd=0\n",
    "model_batch_size=dataset_size\n",
    "validation_ratio=test_split_ratio\n",
    "num_epochs=epochs\n",
    "\n",
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "#Seed\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# torch_generator = torch.Generator(device=device)\n",
    "# if seed is not None:\n",
    "#     torch_generator.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#Dynamics model\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    linearRegression(in_size, out_size, device),\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = SimpleModelTrainer(\n",
    "    criterion=criterion,\n",
    "    metric=metric,\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "#Load replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    dataset_size,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=dataset_size)\n",
    "\n",
    "data = replay_buffer.get_all(shuffle=False)\n",
    "val_size = int(replay_buffer.num_stored * validation_ratio)\n",
    "train_size = replay_buffer.num_stored - val_size\n",
    "train_data = data[:train_size]\n",
    "dataset_train = BootstrapIterator(\n",
    "    train_data,\n",
    "    model_batch_size,\n",
    "    ensemble_size=1,\n",
    "    shuffle_each_epoch=False,\n",
    "    permute_indices=False,\n",
    "    rng=replay_buffer.rng,\n",
    ")\n",
    "val_iter = None\n",
    "if val_size > 0:\n",
    "    val_data = data[train_size:]\n",
    "    dataset_val = TransitionIterator(\n",
    "        val_data, model_batch_size, shuffle_each_epoch=False, rng=replay_buffer.rng\n",
    "    )\n",
    "\n",
    "# if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "#     dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "train_losses, test_losses, train_metrics, test_metrics = model_trainer.train(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_x,\n",
    "    test_y,\n",
    "    num_epochs=num_epochs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = lr_model(train_x)\n",
    "    test_preds = lr_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.simple import Simple\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.001\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "NN_model = Simple(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                NN_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 5.323896408081055, R2 -0.4018963575363159\n",
      "Eval loss 5.323896408081055, R2 -0.40800949931144714\n",
      "epoch 1, loss 4.939913272857666, R2 -0.25594255328178406\n",
      "Eval loss 4.939913272857666, R2 -0.18786725401878357\n",
      "epoch 2, loss 3.9092791080474854, R2 -0.16295231878757477\n",
      "Eval loss 3.9092791080474854, R2 -0.604037344455719\n",
      "epoch 3, loss 3.834420680999756, R2 -0.7803119421005249\n",
      "Eval loss 3.834420680999756, R2 -0.4473435580730438\n",
      "epoch 4, loss 3.371290445327759, R2 0.05224437639117241\n",
      "Eval loss 3.371290445327759, R2 0.07332278043031693\n",
      "epoch 5, loss 3.4254508018493652, R2 0.06424520909786224\n",
      "Eval loss 3.4254508018493652, R2 0.05064111575484276\n",
      "epoch 6, loss 3.348079204559326, R2 0.045901037752628326\n",
      "Eval loss 3.348079204559326, R2 0.02455156482756138\n",
      "epoch 7, loss 3.3848602771759033, R2 0.025363124907016754\n",
      "Eval loss 3.3848602771759033, R2 0.016881713643670082\n",
      "epoch 8, loss 3.3062312602996826, R2 0.046125736087560654\n",
      "Eval loss 3.3062312602996826, R2 0.04103560000658035\n",
      "epoch 9, loss 3.3096539974212646, R2 0.06484391540288925\n",
      "Eval loss 3.3096539974212646, R2 0.06700757145881653\n",
      "epoch 10, loss 3.2726452350616455, R2 0.09313616156578064\n",
      "Eval loss 3.2726452350616455, R2 0.08706977963447571\n",
      "epoch 11, loss 3.274146318435669, R2 0.09592190384864807\n",
      "Eval loss 3.274146318435669, R2 0.0883624330163002\n",
      "epoch 12, loss 3.2493398189544678, R2 0.10129399597644806\n",
      "Eval loss 3.2493398189544678, R2 0.09411159157752991\n",
      "epoch 13, loss 3.244776725769043, R2 0.10128099471330643\n",
      "Eval loss 3.244776725769043, R2 0.09449050575494766\n",
      "epoch 14, loss 3.229379415512085, R2 0.0999612808227539\n",
      "Eval loss 3.229379415512085, R2 0.09051227569580078\n",
      "epoch 15, loss 3.217719316482544, R2 0.10154340416193008\n",
      "Eval loss 3.217719316482544, R2 0.09678056836128235\n",
      "epoch 16, loss 3.203242301940918, R2 0.10683426260948181\n",
      "Eval loss 3.203242301940918, R2 0.10137978196144104\n",
      "epoch 17, loss 3.179955244064331, R2 0.11137198656797409\n",
      "Eval loss 3.179955244064331, R2 0.10453293472528458\n",
      "epoch 18, loss 3.1541330814361572, R2 0.1158190667629242\n",
      "Eval loss 3.1541330814361572, R2 0.10933423042297363\n",
      "epoch 19, loss 3.118581533432007, R2 0.1223062202334404\n",
      "Eval loss 3.118581533432007, R2 0.11584433913230896\n",
      "epoch 20, loss 3.073164939880371, R2 0.1296682357788086\n",
      "Eval loss 3.073164939880371, R2 0.12321216613054276\n",
      "epoch 21, loss 3.019923686981201, R2 0.13729651272296906\n",
      "Eval loss 3.019923686981201, R2 0.13068436086177826\n",
      "epoch 22, loss 2.9619507789611816, R2 0.1448778659105301\n",
      "Eval loss 2.9619507789611816, R2 0.13588067889213562\n",
      "epoch 23, loss 2.901885986328125, R2 0.1522626429796219\n",
      "Eval loss 2.901885986328125, R2 0.14075520634651184\n",
      "epoch 24, loss 2.8387866020202637, R2 0.1590794175863266\n",
      "Eval loss 2.8387866020202637, R2 0.1432419866323471\n",
      "epoch 25, loss 2.7747511863708496, R2 0.16255496442317963\n",
      "Eval loss 2.7747511863708496, R2 0.14380034804344177\n",
      "epoch 26, loss 2.718090772628784, R2 0.16843855381011963\n",
      "Eval loss 2.718090772628784, R2 0.14861327409744263\n",
      "epoch 27, loss 2.663081169128418, R2 0.17842388153076172\n",
      "Eval loss 2.663081169128418, R2 0.16062414646148682\n",
      "epoch 28, loss 2.5945379734039307, R2 0.18962378799915314\n",
      "Eval loss 2.5945379734039307, R2 0.1713990420103073\n",
      "epoch 29, loss 2.5146071910858154, R2 0.19438162446022034\n",
      "Eval loss 2.5146071910858154, R2 0.17525352537631989\n",
      "epoch 30, loss 2.4349544048309326, R2 0.1865132749080658\n",
      "Eval loss 2.4349544048309326, R2 0.1679902970790863\n",
      "epoch 31, loss 2.3619654178619385, R2 0.1735285520553589\n",
      "Eval loss 2.3619654178619385, R2 0.15650413930416107\n",
      "epoch 32, loss 2.2857470512390137, R2 0.17348119616508484\n",
      "Eval loss 2.2857470512390137, R2 0.161806121468544\n",
      "epoch 33, loss 2.203887701034546, R2 0.20126689970493317\n",
      "Eval loss 2.203887701034546, R2 0.18508559465408325\n",
      "epoch 34, loss 2.128127336502075, R2 0.23836053907871246\n",
      "Eval loss 2.128127336502075, R2 0.21246324479579926\n",
      "epoch 35, loss 2.080873727798462, R2 0.26577746868133545\n",
      "Eval loss 2.080873727798462, R2 0.2374952882528305\n",
      "epoch 36, loss 2.0152153968811035, R2 0.2957301139831543\n",
      "Eval loss 2.0152153968811035, R2 0.2600712180137634\n",
      "epoch 37, loss 1.9534615278244019, R2 0.3193061351776123\n",
      "Eval loss 1.9534615278244019, R2 0.2811076045036316\n",
      "epoch 38, loss 1.8927006721496582, R2 0.3386382758617401\n",
      "Eval loss 1.8927006721496582, R2 0.297244131565094\n",
      "epoch 39, loss 1.8345057964324951, R2 0.35320404171943665\n",
      "Eval loss 1.8345057964324951, R2 0.30929529666900635\n",
      "epoch 40, loss 1.7798044681549072, R2 0.3646615147590637\n",
      "Eval loss 1.7798044681549072, R2 0.31807568669319153\n",
      "epoch 41, loss 1.7277518510818481, R2 0.37430083751678467\n",
      "Eval loss 1.7277518510818481, R2 0.32576608657836914\n",
      "epoch 42, loss 1.6763761043548584, R2 0.38417014479637146\n",
      "Eval loss 1.6763761043548584, R2 0.3332109749317169\n",
      "epoch 43, loss 1.6294513940811157, R2 0.39333537220954895\n",
      "Eval loss 1.6294513940811157, R2 0.34179359674453735\n",
      "epoch 44, loss 1.5898447036743164, R2 0.40245047211647034\n",
      "Eval loss 1.5898447036743164, R2 0.34894871711730957\n",
      "epoch 45, loss 1.5673490762710571, R2 0.4086456000804901\n",
      "Eval loss 1.5673490762710571, R2 0.35792800784111023\n",
      "epoch 46, loss 1.5222132205963135, R2 0.42431479692459106\n",
      "Eval loss 1.5222132205963135, R2 0.36976879835128784\n",
      "epoch 47, loss 1.4904708862304688, R2 0.43697643280029297\n",
      "Eval loss 1.4904708862304688, R2 0.3806411623954773\n",
      "epoch 48, loss 1.4608508348464966, R2 0.4470244348049164\n",
      "Eval loss 1.4608508348464966, R2 0.38920536637306213\n",
      "epoch 49, loss 1.4321855306625366, R2 0.45608389377593994\n",
      "Eval loss 1.4321855306625366, R2 0.39561206102371216\n",
      "epoch 50, loss 1.4046671390533447, R2 0.4640016555786133\n",
      "Eval loss 1.4046671390533447, R2 0.4021168649196625\n",
      "epoch 51, loss 1.3781150579452515, R2 0.4720873534679413\n",
      "Eval loss 1.3781150579452515, R2 0.40889090299606323\n",
      "epoch 52, loss 1.3529021739959717, R2 0.4791528880596161\n",
      "Eval loss 1.3529021739959717, R2 0.41484129428863525\n",
      "epoch 53, loss 1.3292691707611084, R2 0.485740065574646\n",
      "Eval loss 1.3292691707611084, R2 0.4205494225025177\n",
      "epoch 54, loss 1.3146171569824219, R2 0.4905272424221039\n",
      "Eval loss 1.3146171569824219, R2 0.42228251695632935\n",
      "epoch 55, loss 1.3063105344772339, R2 0.49425724148750305\n",
      "Eval loss 1.3063105344772339, R2 0.43045738339424133\n",
      "epoch 56, loss 1.2769826650619507, R2 0.5019212961196899\n",
      "Eval loss 1.2769826650619507, R2 0.43437862396240234\n",
      "epoch 57, loss 1.2493157386779785, R2 0.5094982981681824\n",
      "Eval loss 1.2493157386779785, R2 0.43972378969192505\n",
      "epoch 58, loss 1.2260041236877441, R2 0.51665860414505\n",
      "Eval loss 1.2260041236877441, R2 0.444802463054657\n",
      "epoch 59, loss 1.2041126489639282, R2 0.5230696797370911\n",
      "Eval loss 1.2041126489639282, R2 0.45031702518463135\n",
      "epoch 60, loss 1.183255910873413, R2 0.5289703607559204\n",
      "Eval loss 1.183255910873413, R2 0.45689570903778076\n",
      "epoch 61, loss 1.1633212566375732, R2 0.53413325548172\n",
      "Eval loss 1.1633212566375732, R2 0.4638504683971405\n",
      "epoch 62, loss 1.1434763669967651, R2 0.5392552614212036\n",
      "Eval loss 1.1434763669967651, R2 0.470482736825943\n",
      "epoch 63, loss 1.1232273578643799, R2 0.5444135069847107\n",
      "Eval loss 1.1232273578643799, R2 0.4752771556377411\n",
      "epoch 64, loss 1.103455662727356, R2 0.5493125915527344\n",
      "Eval loss 1.103455662727356, R2 0.479763001203537\n",
      "epoch 65, loss 1.0856794118881226, R2 0.5536301136016846\n",
      "Eval loss 1.0856794118881226, R2 0.4829864501953125\n",
      "epoch 66, loss 1.0682677030563354, R2 0.5581094026565552\n",
      "Eval loss 1.0682677030563354, R2 0.4865805506706238\n",
      "epoch 67, loss 1.0530729293823242, R2 0.5621321797370911\n",
      "Eval loss 1.0530729293823242, R2 0.4915057420730591\n",
      "epoch 68, loss 1.057271957397461, R2 0.5610286593437195\n",
      "Eval loss 1.057271957397461, R2 0.47411927580833435\n",
      "epoch 69, loss 1.0513882637023926, R2 0.5636657476425171\n",
      "Eval loss 1.0513882637023926, R2 0.4887337386608124\n",
      "epoch 70, loss 1.026208519935608, R2 0.5701600909233093\n",
      "Eval loss 1.026208519935608, R2 0.4944770038127899\n",
      "epoch 71, loss 1.0088527202606201, R2 0.574737548828125\n",
      "Eval loss 1.0088527202606201, R2 0.49353474378585815\n",
      "epoch 72, loss 1.0017268657684326, R2 0.5757549405097961\n",
      "Eval loss 1.0017268657684326, R2 0.498392790555954\n",
      "epoch 73, loss 0.9897353053092957, R2 0.578517735004425\n",
      "Eval loss 0.9897353053092957, R2 0.5042902827262878\n",
      "epoch 74, loss 0.9691761136054993, R2 0.5838080048561096\n",
      "Eval loss 0.9691761136054993, R2 0.5062525272369385\n",
      "epoch 75, loss 0.9575741291046143, R2 0.5867404937744141\n",
      "Eval loss 0.9575741291046143, R2 0.507879376411438\n",
      "epoch 76, loss 0.9496657252311707, R2 0.5880671143531799\n",
      "Eval loss 0.9496657252311707, R2 0.5100837349891663\n",
      "epoch 77, loss 0.937531054019928, R2 0.5907403826713562\n",
      "Eval loss 0.937531054019928, R2 0.511113703250885\n",
      "epoch 78, loss 0.9254652857780457, R2 0.5937778949737549\n",
      "Eval loss 0.9254652857780457, R2 0.5121618509292603\n",
      "epoch 79, loss 0.915941596031189, R2 0.5966508388519287\n",
      "Eval loss 0.915941596031189, R2 0.5139744877815247\n",
      "epoch 80, loss 0.9070864915847778, R2 0.6000364422798157\n",
      "Eval loss 0.9070864915847778, R2 0.5171797275543213\n",
      "epoch 81, loss 0.8979930281639099, R2 0.6040488481521606\n",
      "Eval loss 0.8979930281639099, R2 0.520541787147522\n",
      "epoch 82, loss 0.8898810148239136, R2 0.6078987717628479\n",
      "Eval loss 0.8898810148239136, R2 0.5219663381576538\n",
      "epoch 83, loss 0.8906905651092529, R2 0.609902560710907\n",
      "Eval loss 0.8906905651092529, R2 0.5237615704536438\n",
      "epoch 84, loss 0.8909624814987183, R2 0.6126012206077576\n",
      "Eval loss 0.8909624814987183, R2 0.5273167490959167\n",
      "epoch 85, loss 0.8700068593025208, R2 0.6190387010574341\n",
      "Eval loss 0.8700068593025208, R2 0.528329074382782\n",
      "epoch 86, loss 0.8594485521316528, R2 0.622977614402771\n",
      "Eval loss 0.8594485521316528, R2 0.5326259136199951\n",
      "epoch 87, loss 0.8545351028442383, R2 0.625688374042511\n",
      "Eval loss 0.8545351028442383, R2 0.5358755588531494\n",
      "epoch 88, loss 0.8460226058959961, R2 0.6289631724357605\n",
      "Eval loss 0.8460226058959961, R2 0.5385809540748596\n",
      "epoch 89, loss 0.8372164368629456, R2 0.6322245001792908\n",
      "Eval loss 0.8372164368629456, R2 0.5412968993186951\n",
      "epoch 90, loss 0.8296207189559937, R2 0.6352847814559937\n",
      "Eval loss 0.8296207189559937, R2 0.5434436798095703\n",
      "epoch 91, loss 0.8221973776817322, R2 0.6381932497024536\n",
      "Eval loss 0.8221973776817322, R2 0.5451253056526184\n",
      "epoch 92, loss 0.815433144569397, R2 0.6407566666603088\n",
      "Eval loss 0.815433144569397, R2 0.5468294024467468\n",
      "epoch 93, loss 0.808121919631958, R2 0.6433449983596802\n",
      "Eval loss 0.808121919631958, R2 0.5492523908615112\n",
      "epoch 94, loss 0.8009641170501709, R2 0.645818293094635\n",
      "Eval loss 0.8009641170501709, R2 0.5524076223373413\n",
      "epoch 95, loss 0.7945981621742249, R2 0.6481678485870361\n",
      "Eval loss 0.7945981621742249, R2 0.5542228817939758\n",
      "epoch 96, loss 0.7877450585365295, R2 0.6507458090782166\n",
      "Eval loss 0.7877450585365295, R2 0.5559375286102295\n",
      "epoch 97, loss 0.7826557159423828, R2 0.6527777314186096\n",
      "Eval loss 0.7826557159423828, R2 0.5566662549972534\n",
      "epoch 98, loss 0.7839885950088501, R2 0.652704656124115\n",
      "Eval loss 0.7839885950088501, R2 0.5549930930137634\n",
      "epoch 99, loss 0.7871474623680115, R2 0.6531407833099365\n",
      "Eval loss 0.7871474623680115, R2 0.5580767393112183\n",
      "epoch 100, loss 0.764458954334259, R2 0.6592950820922852\n",
      "Eval loss 0.764458954334259, R2 0.5611475706100464\n",
      "epoch 101, loss 0.764141857624054, R2 0.6602085828781128\n",
      "Eval loss 0.764141857624054, R2 0.5634884834289551\n",
      "epoch 102, loss 0.7570518851280212, R2 0.6626288890838623\n",
      "Eval loss 0.7570518851280212, R2 0.5631715655326843\n",
      "epoch 103, loss 0.7475873827934265, R2 0.665862500667572\n",
      "Eval loss 0.7475873827934265, R2 0.565822422504425\n",
      "epoch 104, loss 0.7456557750701904, R2 0.6669067144393921\n",
      "Eval loss 0.7456557750701904, R2 0.5683081150054932\n",
      "epoch 105, loss 0.7383595108985901, R2 0.6694031357765198\n",
      "Eval loss 0.7383595108985901, R2 0.5697497725486755\n",
      "epoch 106, loss 0.7330517172813416, R2 0.6714367270469666\n",
      "Eval loss 0.7330517172813416, R2 0.5705475211143494\n",
      "epoch 107, loss 0.7291679382324219, R2 0.673268735408783\n",
      "Eval loss 0.7291679382324219, R2 0.5718381404876709\n",
      "epoch 108, loss 0.7237625122070312, R2 0.6753512024879456\n",
      "Eval loss 0.7237625122070312, R2 0.5719278454780579\n",
      "epoch 109, loss 0.7258654832839966, R2 0.6751456260681152\n",
      "Eval loss 0.7258654832839966, R2 0.5730535984039307\n",
      "epoch 110, loss 0.7362929582595825, R2 0.673335611820221\n",
      "Eval loss 0.7362929582595825, R2 0.5704785585403442\n",
      "epoch 111, loss 0.7120051980018616, R2 0.6803301572799683\n",
      "Eval loss 0.7120051980018616, R2 0.5758534073829651\n",
      "epoch 112, loss 0.7165605425834656, R2 0.6796295642852783\n",
      "Eval loss 0.7165605425834656, R2 0.5779114961624146\n",
      "epoch 113, loss 0.7023780941963196, R2 0.6840437054634094\n",
      "Eval loss 0.7023780941963196, R2 0.5788543224334717\n",
      "epoch 114, loss 0.6981338858604431, R2 0.6859337687492371\n",
      "Eval loss 0.6981338858604431, R2 0.5806100368499756\n",
      "epoch 115, loss 0.6971319317817688, R2 0.6868206858634949\n",
      "Eval loss 0.6971319317817688, R2 0.5817395448684692\n",
      "epoch 116, loss 0.6898270845413208, R2 0.6894563436508179\n",
      "Eval loss 0.6898270845413208, R2 0.582738995552063\n",
      "epoch 117, loss 0.685862123966217, R2 0.690971314907074\n",
      "Eval loss 0.685862123966217, R2 0.5857323408126831\n",
      "epoch 118, loss 0.6851764917373657, R2 0.6920230388641357\n",
      "Eval loss 0.6851764917373657, R2 0.5835645794868469\n",
      "epoch 119, loss 0.6847144961357117, R2 0.6928414106369019\n",
      "Eval loss 0.6847144961357117, R2 0.586185872554779\n",
      "epoch 120, loss 0.6822620630264282, R2 0.6944797039031982\n",
      "Eval loss 0.6822620630264282, R2 0.5857219099998474\n",
      "epoch 121, loss 0.6700987815856934, R2 0.6975351572036743\n",
      "Eval loss 0.6700987815856934, R2 0.588833749294281\n",
      "epoch 122, loss 0.6692997217178345, R2 0.6984370946884155\n",
      "Eval loss 0.6692997217178345, R2 0.5905352234840393\n",
      "epoch 123, loss 0.6648091077804565, R2 0.7003785967826843\n",
      "Eval loss 0.6648091077804565, R2 0.5908278822898865\n",
      "epoch 124, loss 0.6580916047096252, R2 0.7025064826011658\n",
      "Eval loss 0.6580916047096252, R2 0.5915960073471069\n",
      "epoch 125, loss 0.6565075516700745, R2 0.7034061551094055\n",
      "Eval loss 0.6565075516700745, R2 0.5932669043540955\n",
      "epoch 126, loss 0.6527806520462036, R2 0.70510333776474\n",
      "Eval loss 0.6527806520462036, R2 0.5932124853134155\n",
      "epoch 127, loss 0.6474490165710449, R2 0.7069182395935059\n",
      "Eval loss 0.6474490165710449, R2 0.5954118967056274\n",
      "epoch 128, loss 0.6442403793334961, R2 0.7082472443580627\n",
      "Eval loss 0.6442403793334961, R2 0.5967265367507935\n",
      "epoch 129, loss 0.6465064287185669, R2 0.708191454410553\n",
      "Eval loss 0.6465064287185669, R2 0.590429961681366\n",
      "epoch 130, loss 0.6565355062484741, R2 0.7048096060752869\n",
      "Eval loss 0.6565355062484741, R2 0.5940365195274353\n",
      "epoch 131, loss 0.6373541951179504, R2 0.7116726636886597\n",
      "Eval loss 0.6373541951179504, R2 0.5995654463768005\n",
      "epoch 132, loss 0.6402451992034912, R2 0.7115144729614258\n",
      "Eval loss 0.6402451992034912, R2 0.593711793422699\n",
      "epoch 133, loss 0.6280579566955566, R2 0.7151334285736084\n",
      "Eval loss 0.6280579566955566, R2 0.598821222782135\n",
      "epoch 134, loss 0.6294451951980591, R2 0.7151148915290833\n",
      "Eval loss 0.6294451951980591, R2 0.6009334325790405\n",
      "epoch 135, loss 0.6230270862579346, R2 0.717326819896698\n",
      "Eval loss 0.6230270862579346, R2 0.6019616723060608\n",
      "epoch 136, loss 0.6218708753585815, R2 0.7187031507492065\n",
      "Eval loss 0.6218708753585815, R2 0.6005563139915466\n",
      "epoch 137, loss 0.6226549744606018, R2 0.7188059687614441\n",
      "Eval loss 0.6226549744606018, R2 0.6010363698005676\n",
      "epoch 138, loss 0.6204931139945984, R2 0.7200940847396851\n",
      "Eval loss 0.6204931139945984, R2 0.6004945635795593\n",
      "epoch 139, loss 0.6130757331848145, R2 0.7220559120178223\n",
      "Eval loss 0.6130757331848145, R2 0.6047835946083069\n",
      "epoch 140, loss 0.6071860790252686, R2 0.7240672707557678\n",
      "Eval loss 0.6071860790252686, R2 0.605248212814331\n",
      "epoch 141, loss 0.6067325472831726, R2 0.7249943614006042\n",
      "Eval loss 0.6067325472831726, R2 0.6038822531700134\n",
      "epoch 142, loss 0.6049824953079224, R2 0.7259334325790405\n",
      "Eval loss 0.6049824953079224, R2 0.6056109666824341\n",
      "epoch 143, loss 0.5995714664459229, R2 0.727827250957489\n",
      "Eval loss 0.5995714664459229, R2 0.605579137802124\n",
      "epoch 144, loss 0.5963729619979858, R2 0.7289729714393616\n",
      "Eval loss 0.5963729619979858, R2 0.6070935130119324\n",
      "epoch 145, loss 0.5935474038124084, R2 0.7302812337875366\n",
      "Eval loss 0.5935474038124084, R2 0.6080451607704163\n",
      "epoch 146, loss 0.5931792855262756, R2 0.7309443950653076\n",
      "Eval loss 0.5931792855262756, R2 0.6052700877189636\n",
      "epoch 147, loss 0.5941428542137146, R2 0.731141984462738\n",
      "Eval loss 0.5941428542137146, R2 0.6082279682159424\n",
      "epoch 148, loss 0.6001178622245789, R2 0.7306573390960693\n",
      "Eval loss 0.6001178622245789, R2 0.6040398478507996\n",
      "epoch 149, loss 0.5950829982757568, R2 0.7317952513694763\n",
      "Eval loss 0.5950829982757568, R2 0.6078828573226929\n",
      "epoch 150, loss 0.5834910869598389, R2 0.7352152466773987\n",
      "Eval loss 0.5834910869598389, R2 0.6093364357948303\n",
      "epoch 151, loss 0.5859310626983643, R2 0.7353220582008362\n",
      "Eval loss 0.5859310626983643, R2 0.6082839369773865\n",
      "epoch 152, loss 0.5786685943603516, R2 0.7373227477073669\n",
      "Eval loss 0.5786685943603516, R2 0.6086265444755554\n",
      "epoch 153, loss 0.5797899961471558, R2 0.7375152707099915\n",
      "Eval loss 0.5797899961471558, R2 0.6109704375267029\n",
      "epoch 154, loss 0.5741211175918579, R2 0.7393850684165955\n",
      "Eval loss 0.5741211175918579, R2 0.6099424362182617\n",
      "epoch 155, loss 0.5737914443016052, R2 0.7400661706924438\n",
      "Eval loss 0.5737914443016052, R2 0.6086459755897522\n",
      "epoch 156, loss 0.5733327865600586, R2 0.7402286529541016\n",
      "Eval loss 0.5733327865600586, R2 0.6106548309326172\n",
      "epoch 157, loss 0.5720753073692322, R2 0.7409208416938782\n",
      "Eval loss 0.5720753073692322, R2 0.6099362969398499\n",
      "epoch 158, loss 0.5695419311523438, R2 0.7423359155654907\n",
      "Eval loss 0.5695419311523438, R2 0.6094371676445007\n",
      "epoch 159, loss 0.5632818341255188, R2 0.7441337704658508\n",
      "Eval loss 0.5632818341255188, R2 0.611998975276947\n",
      "epoch 160, loss 0.5616804957389832, R2 0.7450006008148193\n",
      "Eval loss 0.5616804957389832, R2 0.6115602850914001\n",
      "epoch 161, loss 0.5616790652275085, R2 0.7453765869140625\n",
      "Eval loss 0.5616790652275085, R2 0.6113184094429016\n",
      "epoch 162, loss 0.5585418343544006, R2 0.7465200424194336\n",
      "Eval loss 0.5585418343544006, R2 0.6116296052932739\n",
      "epoch 163, loss 0.5568497776985168, R2 0.7473488450050354\n",
      "Eval loss 0.5568497776985168, R2 0.6123142838478088\n",
      "epoch 164, loss 0.5633541345596313, R2 0.7462732791900635\n",
      "Eval loss 0.5633541345596313, R2 0.606842041015625\n",
      "epoch 165, loss 0.5663480162620544, R2 0.7457435727119446\n",
      "Eval loss 0.5663480162620544, R2 0.6121374368667603\n",
      "epoch 166, loss 0.5502400398254395, R2 0.7499896883964539\n",
      "Eval loss 0.5502400398254395, R2 0.6120137572288513\n",
      "epoch 167, loss 0.5553320050239563, R2 0.7489804625511169\n",
      "Eval loss 0.5553320050239563, R2 0.6118378043174744\n",
      "epoch 168, loss 0.5460079312324524, R2 0.7517673969268799\n",
      "Eval loss 0.5460079312324524, R2 0.6118049621582031\n",
      "epoch 169, loss 0.5475510954856873, R2 0.7517048120498657\n",
      "Eval loss 0.5475510954856873, R2 0.6130427718162537\n",
      "epoch 170, loss 0.542102038860321, R2 0.7533596158027649\n",
      "Eval loss 0.542102038860321, R2 0.6133285164833069\n",
      "epoch 171, loss 0.5422391295433044, R2 0.7537450790405273\n",
      "Eval loss 0.5422391295433044, R2 0.6123206615447998\n",
      "epoch 172, loss 0.5391162037849426, R2 0.7547205090522766\n",
      "Eval loss 0.5391162037849426, R2 0.6137222051620483\n",
      "epoch 173, loss 0.5426000356674194, R2 0.7543861865997314\n",
      "Eval loss 0.5426000356674194, R2 0.6084580421447754\n",
      "epoch 174, loss 0.5566002130508423, R2 0.7506089806556702\n",
      "Eval loss 0.5566002130508423, R2 0.6110672950744629\n",
      "epoch 175, loss 0.5431624054908752, R2 0.7548589110374451\n",
      "Eval loss 0.5431624054908752, R2 0.6127561926841736\n",
      "epoch 176, loss 0.5416568517684937, R2 0.755576491355896\n",
      "Eval loss 0.5416568517684937, R2 0.6085864305496216\n",
      "epoch 177, loss 0.532210111618042, R2 0.7580487728118896\n",
      "Eval loss 0.532210111618042, R2 0.6128535866737366\n",
      "epoch 178, loss 0.5362716913223267, R2 0.757358193397522\n",
      "Eval loss 0.5362716913223267, R2 0.6140934228897095\n",
      "epoch 179, loss 0.5272506475448608, R2 0.7599210739135742\n",
      "Eval loss 0.5272506475448608, R2 0.6134873032569885\n",
      "epoch 180, loss 0.5280812382698059, R2 0.7600468993186951\n",
      "Eval loss 0.5280812382698059, R2 0.6127235889434814\n",
      "epoch 181, loss 0.5249626636505127, R2 0.7611211538314819\n",
      "Eval loss 0.5249626636505127, R2 0.6128045916557312\n",
      "epoch 182, loss 0.522704005241394, R2 0.7618763446807861\n",
      "Eval loss 0.522704005241394, R2 0.6144971251487732\n",
      "epoch 183, loss 0.5241965651512146, R2 0.7621479630470276\n",
      "Eval loss 0.5241965651512146, R2 0.6132158637046814\n",
      "epoch 184, loss 0.5328370928764343, R2 0.7605343461036682\n",
      "Eval loss 0.5328370928764343, R2 0.6093154549598694\n",
      "epoch 185, loss 0.5381304621696472, R2 0.759105920791626\n",
      "Eval loss 0.5381304621696472, R2 0.611727774143219\n",
      "epoch 186, loss 0.5174232721328735, R2 0.764578104019165\n",
      "Eval loss 0.5174232721328735, R2 0.6114003658294678\n",
      "epoch 187, loss 0.522409200668335, R2 0.7639207243919373\n",
      "Eval loss 0.522409200668335, R2 0.6136678457260132\n",
      "epoch 188, loss 0.5163769721984863, R2 0.7655301690101624\n",
      "Eval loss 0.5163769721984863, R2 0.6136270761489868\n",
      "epoch 189, loss 0.5129250288009644, R2 0.7666326761245728\n",
      "Eval loss 0.5129250288009644, R2 0.6143323183059692\n",
      "epoch 190, loss 0.5117366313934326, R2 0.7674190402030945\n",
      "Eval loss 0.5117366313934326, R2 0.6133215427398682\n",
      "epoch 191, loss 0.5096736550331116, R2 0.76809161901474\n",
      "Eval loss 0.5096736550331116, R2 0.6139703392982483\n",
      "epoch 192, loss 0.506104588508606, R2 0.7693831920623779\n",
      "Eval loss 0.506104588508606, R2 0.6141618490219116\n",
      "epoch 193, loss 0.5057572722434998, R2 0.7697622179985046\n",
      "Eval loss 0.5057572722434998, R2 0.6150574088096619\n",
      "epoch 194, loss 0.5023719668388367, R2 0.7710546851158142\n",
      "Eval loss 0.5023719668388367, R2 0.614185094833374\n",
      "epoch 195, loss 0.5030317306518555, R2 0.7709773778915405\n",
      "Eval loss 0.5030317306518555, R2 0.6157346367835999\n",
      "epoch 196, loss 0.5031473636627197, R2 0.771536648273468\n",
      "Eval loss 0.5031473636627197, R2 0.6119280457496643\n",
      "epoch 197, loss 0.5048761367797852, R2 0.7711465954780579\n",
      "Eval loss 0.5048761367797852, R2 0.6147124767303467\n",
      "epoch 198, loss 0.5012946128845215, R2 0.7727788686752319\n",
      "Eval loss 0.5012946128845215, R2 0.6141765117645264\n",
      "epoch 199, loss 0.4963555335998535, R2 0.7741556763648987\n",
      "Eval loss 0.4963555335998535, R2 0.613586962223053\n",
      "epoch 200, loss 0.5017609000205994, R2 0.7725799679756165\n",
      "Eval loss 0.5017609000205994, R2 0.6142296195030212\n",
      "epoch 201, loss 0.5003282427787781, R2 0.7730388641357422\n",
      "Eval loss 0.5003282427787781, R2 0.6130625605583191\n",
      "epoch 202, loss 0.4906190037727356, R2 0.7766116857528687\n",
      "Eval loss 0.4906190037727356, R2 0.6142947673797607\n",
      "epoch 203, loss 0.4924263656139374, R2 0.7763965129852295\n",
      "Eval loss 0.4924263656139374, R2 0.6158540844917297\n",
      "epoch 204, loss 0.4881938695907593, R2 0.7777227759361267\n",
      "Eval loss 0.4881938695907593, R2 0.6143835783004761\n",
      "epoch 205, loss 0.48580431938171387, R2 0.7787010669708252\n",
      "Eval loss 0.48580431938171387, R2 0.6156507134437561\n",
      "epoch 206, loss 0.48498791456222534, R2 0.7792825102806091\n",
      "Eval loss 0.48498791456222534, R2 0.6163796186447144\n",
      "epoch 207, loss 0.482687771320343, R2 0.7799810767173767\n",
      "Eval loss 0.482687771320343, R2 0.6139929890632629\n",
      "epoch 208, loss 0.4818885326385498, R2 0.7803863883018494\n",
      "Eval loss 0.4818885326385498, R2 0.6168259978294373\n",
      "epoch 209, loss 0.4829045534133911, R2 0.7806764245033264\n",
      "Eval loss 0.4829045534133911, R2 0.6130691170692444\n",
      "epoch 210, loss 0.4833565354347229, R2 0.7808569073677063\n",
      "Eval loss 0.4833565354347229, R2 0.6149429678916931\n",
      "epoch 211, loss 0.48227599263191223, R2 0.7815501689910889\n",
      "Eval loss 0.48227599263191223, R2 0.6148975491523743\n",
      "epoch 212, loss 0.4783133268356323, R2 0.782740592956543\n",
      "Eval loss 0.4783133268356323, R2 0.6147423982620239\n",
      "epoch 213, loss 0.4751035273075104, R2 0.7834722399711609\n",
      "Eval loss 0.4751035273075104, R2 0.6147179007530212\n",
      "epoch 214, loss 0.4744203984737396, R2 0.7838819622993469\n",
      "Eval loss 0.4744203984737396, R2 0.6153340935707092\n",
      "epoch 215, loss 0.47173771262168884, R2 0.7850738167762756\n",
      "Eval loss 0.47173771262168884, R2 0.6161622405052185\n",
      "epoch 216, loss 0.4733773469924927, R2 0.7842181921005249\n",
      "Eval loss 0.4733773469924927, R2 0.6099509596824646\n",
      "epoch 217, loss 0.4751660227775574, R2 0.7841621041297913\n",
      "Eval loss 0.4751660227775574, R2 0.6167038083076477\n",
      "epoch 218, loss 0.46984636783599854, R2 0.7864070534706116\n",
      "Eval loss 0.46984636783599854, R2 0.6143772602081299\n",
      "epoch 219, loss 0.4681015908718109, R2 0.7868850231170654\n",
      "Eval loss 0.4681015908718109, R2 0.6136981844902039\n",
      "epoch 220, loss 0.46526435017585754, R2 0.7880221009254456\n",
      "Eval loss 0.46526435017585754, R2 0.6159533858299255\n",
      "epoch 221, loss 0.4656476378440857, R2 0.7879704833030701\n",
      "Eval loss 0.4656476378440857, R2 0.6137171387672424\n",
      "epoch 222, loss 0.46518218517303467, R2 0.7882909178733826\n",
      "Eval loss 0.46518218517303467, R2 0.6157739162445068\n",
      "epoch 223, loss 0.4602530300617218, R2 0.7903931140899658\n",
      "Eval loss 0.4602530300617218, R2 0.6141651272773743\n",
      "epoch 224, loss 0.46284806728363037, R2 0.7896131873130798\n",
      "Eval loss 0.46284806728363037, R2 0.6146784424781799\n",
      "epoch 225, loss 0.4649321436882019, R2 0.7894787192344666\n",
      "Eval loss 0.4649321436882019, R2 0.612896740436554\n",
      "epoch 226, loss 0.4647602438926697, R2 0.7891084551811218\n",
      "Eval loss 0.4647602438926697, R2 0.6141257882118225\n",
      "epoch 227, loss 0.4635889232158661, R2 0.7904603481292725\n",
      "Eval loss 0.4635889232158661, R2 0.6096329092979431\n",
      "epoch 228, loss 0.45671898126602173, R2 0.7923049926757812\n",
      "Eval loss 0.45671898126602173, R2 0.6160881519317627\n",
      "epoch 229, loss 0.4538480341434479, R2 0.7932042479515076\n",
      "Eval loss 0.4538480341434479, R2 0.6166670322418213\n",
      "epoch 230, loss 0.4560944139957428, R2 0.7929084300994873\n",
      "Eval loss 0.4560944139957428, R2 0.6132773756980896\n",
      "epoch 231, loss 0.44871053099632263, R2 0.7950830459594727\n",
      "Eval loss 0.44871053099632263, R2 0.6136593222618103\n",
      "epoch 232, loss 0.4504335820674896, R2 0.7949421405792236\n",
      "Eval loss 0.4504335820674896, R2 0.6165236830711365\n",
      "epoch 233, loss 0.4471411108970642, R2 0.7961919903755188\n",
      "Eval loss 0.4471411108970642, R2 0.6155123710632324\n",
      "epoch 234, loss 0.44733232259750366, R2 0.7964505553245544\n",
      "Eval loss 0.44733232259750366, R2 0.6149525046348572\n",
      "epoch 235, loss 0.4460309147834778, R2 0.7968513369560242\n",
      "Eval loss 0.4460309147834778, R2 0.612976610660553\n",
      "epoch 236, loss 0.4532242715358734, R2 0.7949001789093018\n",
      "Eval loss 0.4532242715358734, R2 0.6143975853919983\n",
      "epoch 237, loss 0.4523448646068573, R2 0.7956748604774475\n",
      "Eval loss 0.4523448646068573, R2 0.6122779250144958\n",
      "epoch 238, loss 0.4440183937549591, R2 0.7978654503822327\n",
      "Eval loss 0.4440183937549591, R2 0.6145371794700623\n",
      "epoch 239, loss 0.4424695372581482, R2 0.7986316680908203\n",
      "Eval loss 0.4424695372581482, R2 0.614269495010376\n",
      "epoch 240, loss 0.4390389621257782, R2 0.7999426126480103\n",
      "Eval loss 0.4390389621257782, R2 0.6143484711647034\n",
      "epoch 241, loss 0.44025468826293945, R2 0.7994714975357056\n",
      "Eval loss 0.44025468826293945, R2 0.6161251664161682\n",
      "epoch 242, loss 0.43789878487586975, R2 0.8003892302513123\n",
      "Eval loss 0.43789878487586975, R2 0.6131023168563843\n",
      "epoch 243, loss 0.43429797887802124, R2 0.8016868829727173\n",
      "Eval loss 0.43429797887802124, R2 0.6150925159454346\n",
      "epoch 244, loss 0.4333416819572449, R2 0.8021553158760071\n",
      "Eval loss 0.4333416819572449, R2 0.6153056621551514\n",
      "epoch 245, loss 0.43129339814186096, R2 0.8029083013534546\n",
      "Eval loss 0.43129339814186096, R2 0.6147217154502869\n",
      "epoch 246, loss 0.43055325746536255, R2 0.8031215667724609\n",
      "Eval loss 0.43055325746536255, R2 0.6152158379554749\n",
      "epoch 247, loss 0.4309309124946594, R2 0.8035886883735657\n",
      "Eval loss 0.4309309124946594, R2 0.6128814816474915\n",
      "epoch 248, loss 0.43686234951019287, R2 0.8021745681762695\n",
      "Eval loss 0.43686234951019287, R2 0.6146934032440186\n",
      "epoch 249, loss 0.43715858459472656, R2 0.8025487661361694\n",
      "Eval loss 0.43715858459472656, R2 0.610281229019165\n",
      "epoch 250, loss 0.4297459125518799, R2 0.8039995431900024\n",
      "Eval loss 0.4297459125518799, R2 0.6149831414222717\n",
      "epoch 251, loss 0.42948609590530396, R2 0.8047447204589844\n",
      "Eval loss 0.42948609590530396, R2 0.6131044626235962\n",
      "epoch 252, loss 0.42707541584968567, R2 0.8052818775177002\n",
      "Eval loss 0.42707541584968567, R2 0.6129316687583923\n",
      "epoch 253, loss 0.422548770904541, R2 0.8067781329154968\n",
      "Eval loss 0.422548770904541, R2 0.615500271320343\n",
      "epoch 254, loss 0.4240456819534302, R2 0.806763231754303\n",
      "Eval loss 0.4240456819534302, R2 0.6131751537322998\n",
      "epoch 255, loss 0.42059123516082764, R2 0.807802140712738\n",
      "Eval loss 0.42059123516082764, R2 0.614223301410675\n",
      "epoch 256, loss 0.42108696699142456, R2 0.8078494668006897\n",
      "Eval loss 0.42108696699142456, R2 0.6137908697128296\n",
      "epoch 257, loss 0.4209705591201782, R2 0.8080850839614868\n",
      "Eval loss 0.4209705591201782, R2 0.6141448616981506\n",
      "epoch 258, loss 0.4212082624435425, R2 0.8080589175224304\n",
      "Eval loss 0.4212082624435425, R2 0.6097463965415955\n",
      "epoch 259, loss 0.4237018823623657, R2 0.8068069219589233\n",
      "Eval loss 0.4237018823623657, R2 0.6143762469291687\n",
      "epoch 260, loss 0.4166521728038788, R2 0.8099862933158875\n",
      "Eval loss 0.4166521728038788, R2 0.6130910515785217\n",
      "epoch 261, loss 0.41639336943626404, R2 0.8098533749580383\n",
      "Eval loss 0.41639336943626404, R2 0.6116191148757935\n",
      "epoch 262, loss 0.41372764110565186, R2 0.8107547163963318\n",
      "Eval loss 0.41372764110565186, R2 0.6136197447776794\n",
      "epoch 263, loss 0.4120606482028961, R2 0.8115711808204651\n",
      "Eval loss 0.4120606482028961, R2 0.6139740347862244\n",
      "epoch 264, loss 0.4131239354610443, R2 0.8111211657524109\n",
      "Eval loss 0.4131239354610443, R2 0.6116422414779663\n",
      "epoch 265, loss 0.4130130112171173, R2 0.8115754723548889\n",
      "Eval loss 0.4130130112171173, R2 0.6129289269447327\n",
      "epoch 266, loss 0.4253462553024292, R2 0.8091415762901306\n",
      "Eval loss 0.4253462553024292, R2 0.6071022152900696\n",
      "epoch 267, loss 0.4208693206310272, R2 0.8101425170898438\n",
      "Eval loss 0.4208693206310272, R2 0.6139687299728394\n",
      "epoch 268, loss 0.4131411015987396, R2 0.8122244477272034\n",
      "Eval loss 0.4131411015987396, R2 0.6091676354408264\n",
      "epoch 269, loss 0.4100251793861389, R2 0.8131017088890076\n",
      "Eval loss 0.4100251793861389, R2 0.6111574769020081\n",
      "epoch 270, loss 0.4079286754131317, R2 0.8137136101722717\n",
      "Eval loss 0.4079286754131317, R2 0.6128096580505371\n",
      "epoch 271, loss 0.40632110834121704, R2 0.8145315051078796\n",
      "Eval loss 0.40632110834121704, R2 0.6124500036239624\n",
      "epoch 272, loss 0.4047337472438812, R2 0.8152924180030823\n",
      "Eval loss 0.4047337472438812, R2 0.6135403513908386\n",
      "epoch 273, loss 0.40123251080513, R2 0.8160817623138428\n",
      "Eval loss 0.40123251080513, R2 0.6108062267303467\n",
      "epoch 274, loss 0.4007345736026764, R2 0.8164200186729431\n",
      "Eval loss 0.4007345736026764, R2 0.6129242181777954\n",
      "epoch 275, loss 0.39852872490882874, R2 0.8172245025634766\n",
      "Eval loss 0.39852872490882874, R2 0.6132442951202393\n",
      "epoch 276, loss 0.39982104301452637, R2 0.8171088695526123\n",
      "Eval loss 0.39982104301452637, R2 0.6097476482391357\n",
      "epoch 277, loss 0.4008907377719879, R2 0.8167116045951843\n",
      "Eval loss 0.4008907377719879, R2 0.6107081770896912\n",
      "epoch 278, loss 0.40336233377456665, R2 0.8159698247909546\n",
      "Eval loss 0.40336233377456665, R2 0.6100766658782959\n",
      "epoch 279, loss 0.3977293372154236, R2 0.8181772828102112\n",
      "Eval loss 0.3977293372154236, R2 0.6106333136558533\n",
      "epoch 280, loss 0.40135854482650757, R2 0.8175024390220642\n",
      "Eval loss 0.40135854482650757, R2 0.6125274300575256\n",
      "epoch 281, loss 0.40859293937683105, R2 0.8149703741073608\n",
      "Eval loss 0.40859293937683105, R2 0.6015436053276062\n",
      "epoch 282, loss 0.39692816138267517, R2 0.8183273673057556\n",
      "Eval loss 0.39692816138267517, R2 0.6123371124267578\n",
      "epoch 283, loss 0.3941083252429962, R2 0.8193935751914978\n",
      "Eval loss 0.3941083252429962, R2 0.6130594611167908\n",
      "epoch 284, loss 0.393084317445755, R2 0.8202056288719177\n",
      "Eval loss 0.393084317445755, R2 0.610198974609375\n",
      "epoch 285, loss 0.3916929066181183, R2 0.8205680847167969\n",
      "Eval loss 0.3916929066181183, R2 0.6079428791999817\n",
      "epoch 286, loss 0.39100751280784607, R2 0.8210189938545227\n",
      "Eval loss 0.39100751280784607, R2 0.6116192936897278\n",
      "epoch 287, loss 0.3868381381034851, R2 0.8223115801811218\n",
      "Eval loss 0.3868381381034851, R2 0.6123085618019104\n",
      "epoch 288, loss 0.38709765672683716, R2 0.822344183921814\n",
      "Eval loss 0.38709765672683716, R2 0.6091232895851135\n",
      "epoch 289, loss 0.38430070877075195, R2 0.8233595490455627\n",
      "Eval loss 0.38430070877075195, R2 0.6101725101470947\n",
      "epoch 290, loss 0.38447919487953186, R2 0.8232085704803467\n",
      "Eval loss 0.38447919487953186, R2 0.612172544002533\n",
      "epoch 291, loss 0.38403066992759705, R2 0.8236707448959351\n",
      "Eval loss 0.38403066992759705, R2 0.6079168319702148\n",
      "epoch 292, loss 0.3917669653892517, R2 0.8215075135231018\n",
      "Eval loss 0.3917669653892517, R2 0.6074028611183167\n",
      "epoch 293, loss 0.40418604016304016, R2 0.8185091018676758\n",
      "Eval loss 0.40418604016304016, R2 0.6033605337142944\n",
      "epoch 294, loss 0.3824883699417114, R2 0.8244447112083435\n",
      "Eval loss 0.3824883699417114, R2 0.6097699999809265\n",
      "epoch 295, loss 0.39291125535964966, R2 0.8219018578529358\n",
      "Eval loss 0.39291125535964966, R2 0.6087134480476379\n",
      "epoch 296, loss 0.38084477186203003, R2 0.8251922130584717\n",
      "Eval loss 0.38084477186203003, R2 0.6083212494850159\n",
      "epoch 297, loss 0.3830035924911499, R2 0.8248926401138306\n",
      "Eval loss 0.3830035924911499, R2 0.60672527551651\n",
      "epoch 298, loss 0.37949901819229126, R2 0.8256375789642334\n",
      "Eval loss 0.37949901819229126, R2 0.6088544726371765\n",
      "epoch 299, loss 0.37822017073631287, R2 0.826388418674469\n",
      "Eval loss 0.37822017073631287, R2 0.609224796295166\n",
      "epoch 300, loss 0.37954777479171753, R2 0.8263025879859924\n",
      "Eval loss 0.37954777479171753, R2 0.6087421774864197\n",
      "epoch 301, loss 0.38093680143356323, R2 0.8259090185165405\n",
      "Eval loss 0.38093680143356323, R2 0.6030359864234924\n",
      "epoch 302, loss 0.38552403450012207, R2 0.8240102529525757\n",
      "Eval loss 0.38552403450012207, R2 0.6083455085754395\n",
      "epoch 303, loss 0.3763127624988556, R2 0.8276855945587158\n",
      "Eval loss 0.3763127624988556, R2 0.6081076264381409\n",
      "epoch 304, loss 0.372560977935791, R2 0.8285856246948242\n",
      "Eval loss 0.372560977935791, R2 0.6079269647598267\n",
      "epoch 305, loss 0.3749220669269562, R2 0.8282194137573242\n",
      "Eval loss 0.3749220669269562, R2 0.608206033706665\n",
      "epoch 306, loss 0.37222957611083984, R2 0.8291286826133728\n",
      "Eval loss 0.37222957611083984, R2 0.6059059500694275\n",
      "epoch 307, loss 0.3695601522922516, R2 0.8297393918037415\n",
      "Eval loss 0.3695601522922516, R2 0.6084954738616943\n",
      "epoch 308, loss 0.36853909492492676, R2 0.8302678465843201\n",
      "Eval loss 0.36853909492492676, R2 0.6077752113342285\n",
      "epoch 309, loss 0.36821249127388, R2 0.8305564522743225\n",
      "Eval loss 0.36821249127388, R2 0.6062370538711548\n",
      "epoch 310, loss 0.36842551827430725, R2 0.8306195735931396\n",
      "Eval loss 0.36842551827430725, R2 0.6081460118293762\n",
      "epoch 311, loss 0.3690752387046814, R2 0.8305507898330688\n",
      "Eval loss 0.3690752387046814, R2 0.6051108241081238\n",
      "epoch 312, loss 0.37631651759147644, R2 0.8285270929336548\n",
      "Eval loss 0.37631651759147644, R2 0.6027357578277588\n",
      "epoch 313, loss 0.383405476808548, R2 0.8275152444839478\n",
      "Eval loss 0.383405476808548, R2 0.6033128499984741\n",
      "epoch 314, loss 0.3708934783935547, R2 0.8308050036430359\n",
      "Eval loss 0.3708934783935547, R2 0.6041370034217834\n",
      "epoch 315, loss 0.36996638774871826, R2 0.830812394618988\n",
      "Eval loss 0.36996638774871826, R2 0.6066130995750427\n",
      "epoch 316, loss 0.36459413170814514, R2 0.8326385021209717\n",
      "Eval loss 0.36459413170814514, R2 0.6049743294715881\n",
      "epoch 317, loss 0.36631444096565247, R2 0.8317222595214844\n",
      "Eval loss 0.36631444096565247, R2 0.6052488684654236\n",
      "epoch 318, loss 0.35988426208496094, R2 0.8338005542755127\n",
      "Eval loss 0.35988426208496094, R2 0.6059399247169495\n",
      "epoch 319, loss 0.3624696433544159, R2 0.8333473205566406\n",
      "Eval loss 0.3624696433544159, R2 0.6043229699134827\n",
      "epoch 320, loss 0.35823512077331543, R2 0.8346545696258545\n",
      "Eval loss 0.35823512077331543, R2 0.6056072115898132\n",
      "epoch 321, loss 0.35942548513412476, R2 0.8344472646713257\n",
      "Eval loss 0.35942548513412476, R2 0.6038368344306946\n",
      "epoch 322, loss 0.36068493127822876, R2 0.8336643576622009\n",
      "Eval loss 0.36068493127822876, R2 0.6055076122283936\n",
      "epoch 323, loss 0.36968177556991577, R2 0.8314392566680908\n",
      "Eval loss 0.36968177556991577, R2 0.5952160358428955\n",
      "epoch 324, loss 0.36444026231765747, R2 0.8329854607582092\n",
      "Eval loss 0.36444026231765747, R2 0.6056517362594604\n",
      "epoch 325, loss 0.35459837317466736, R2 0.8362576961517334\n",
      "Eval loss 0.35459837317466736, R2 0.6061102747917175\n",
      "epoch 326, loss 0.35967838764190674, R2 0.8348373770713806\n",
      "Eval loss 0.35967838764190674, R2 0.5997236967086792\n",
      "epoch 327, loss 0.35457658767700195, R2 0.836199164390564\n",
      "Eval loss 0.35457658767700195, R2 0.6051538586616516\n",
      "epoch 328, loss 0.3523276448249817, R2 0.837327778339386\n",
      "Eval loss 0.3523276448249817, R2 0.6046674251556396\n",
      "epoch 329, loss 0.3531774580478668, R2 0.8372660279273987\n",
      "Eval loss 0.3531774580478668, R2 0.6015856862068176\n",
      "epoch 330, loss 0.3512532114982605, R2 0.8378036022186279\n",
      "Eval loss 0.3512532114982605, R2 0.6037313938140869\n",
      "epoch 331, loss 0.35188043117523193, R2 0.8375797271728516\n",
      "Eval loss 0.35188043117523193, R2 0.6035993695259094\n",
      "epoch 332, loss 0.36028677225112915, R2 0.8346097469329834\n",
      "Eval loss 0.36028677225112915, R2 0.5959954857826233\n",
      "epoch 333, loss 0.36309099197387695, R2 0.8337755799293518\n",
      "Eval loss 0.36309099197387695, R2 0.5999669432640076\n",
      "epoch 334, loss 0.3528377413749695, R2 0.8372913599014282\n",
      "Eval loss 0.3528377413749695, R2 0.603127658367157\n",
      "epoch 335, loss 0.3543117046356201, R2 0.8370898962020874\n",
      "Eval loss 0.3543117046356201, R2 0.6002871990203857\n",
      "epoch 336, loss 0.3484456539154053, R2 0.8392820954322815\n",
      "Eval loss 0.3484456539154053, R2 0.599452793598175\n",
      "epoch 337, loss 0.34871819615364075, R2 0.8389658331871033\n",
      "Eval loss 0.34871819615364075, R2 0.6028348207473755\n",
      "epoch 338, loss 0.34583598375320435, R2 0.8402580618858337\n",
      "Eval loss 0.34583598375320435, R2 0.6024402379989624\n",
      "epoch 339, loss 0.3448871970176697, R2 0.8405618667602539\n",
      "Eval loss 0.3448871970176697, R2 0.5995800495147705\n",
      "epoch 340, loss 0.3459327220916748, R2 0.8402105569839478\n",
      "Eval loss 0.3459327220916748, R2 0.6019778847694397\n",
      "epoch 341, loss 0.34459221363067627, R2 0.8409349918365479\n",
      "Eval loss 0.34459221363067627, R2 0.6017534136772156\n",
      "epoch 342, loss 0.347381591796875, R2 0.8403909802436829\n",
      "Eval loss 0.347381591796875, R2 0.5977389812469482\n",
      "epoch 343, loss 0.3484155237674713, R2 0.8403348922729492\n",
      "Eval loss 0.3484155237674713, R2 0.6006212830543518\n",
      "epoch 344, loss 0.3427481949329376, R2 0.8417579531669617\n",
      "Eval loss 0.3427481949329376, R2 0.5998654961585999\n",
      "epoch 345, loss 0.34147369861602783, R2 0.842129647731781\n",
      "Eval loss 0.34147369861602783, R2 0.5993199348449707\n",
      "epoch 346, loss 0.3420354723930359, R2 0.8417865633964539\n",
      "Eval loss 0.3420354723930359, R2 0.598946750164032\n",
      "epoch 347, loss 0.3442701995372772, R2 0.8413285613059998\n",
      "Eval loss 0.3442701995372772, R2 0.5997910499572754\n",
      "epoch 348, loss 0.3433187007904053, R2 0.8417714834213257\n",
      "Eval loss 0.3433187007904053, R2 0.5959822535514832\n",
      "epoch 349, loss 0.3405711054801941, R2 0.8426992297172546\n",
      "Eval loss 0.3405711054801941, R2 0.6010572910308838\n",
      "epoch 350, loss 0.3391895592212677, R2 0.843295156955719\n",
      "Eval loss 0.3391895592212677, R2 0.5969429016113281\n",
      "epoch 351, loss 0.3353605568408966, R2 0.8443485498428345\n",
      "Eval loss 0.3353605568408966, R2 0.5995686054229736\n",
      "epoch 352, loss 0.3359414339065552, R2 0.8442651629447937\n",
      "Eval loss 0.3359414339065552, R2 0.5996509790420532\n",
      "epoch 353, loss 0.3368476927280426, R2 0.8441381454467773\n",
      "Eval loss 0.3368476927280426, R2 0.5963186621665955\n",
      "epoch 354, loss 0.3365677297115326, R2 0.8446339964866638\n",
      "Eval loss 0.3365677297115326, R2 0.598639190196991\n",
      "epoch 355, loss 0.3365001380443573, R2 0.8449447751045227\n",
      "Eval loss 0.3365001380443573, R2 0.5995197296142578\n",
      "epoch 356, loss 0.3391445577144623, R2 0.8443086743354797\n",
      "Eval loss 0.3391445577144623, R2 0.5926515460014343\n",
      "epoch 357, loss 0.34423181414604187, R2 0.8426503539085388\n",
      "Eval loss 0.34423181414604187, R2 0.5991290807723999\n",
      "epoch 358, loss 0.33723515272140503, R2 0.8450075387954712\n",
      "Eval loss 0.33723515272140503, R2 0.5969472527503967\n",
      "epoch 359, loss 0.33107978105545044, R2 0.8464184403419495\n",
      "Eval loss 0.33107978105545044, R2 0.5956364870071411\n",
      "epoch 360, loss 0.33210572600364685, R2 0.8464704155921936\n",
      "Eval loss 0.33210572600364685, R2 0.5984528064727783\n",
      "epoch 361, loss 0.33142411708831787, R2 0.8468528985977173\n",
      "Eval loss 0.33142411708831787, R2 0.5970820188522339\n",
      "epoch 362, loss 0.3281552791595459, R2 0.8476648926734924\n",
      "Eval loss 0.3281552791595459, R2 0.597573459148407\n",
      "epoch 363, loss 0.3277462422847748, R2 0.8480491042137146\n",
      "Eval loss 0.3277462422847748, R2 0.596425473690033\n",
      "epoch 364, loss 0.32914096117019653, R2 0.8476871252059937\n",
      "Eval loss 0.32914096117019653, R2 0.5977503061294556\n",
      "epoch 365, loss 0.329125314950943, R2 0.8475765585899353\n",
      "Eval loss 0.329125314950943, R2 0.5927678942680359\n",
      "epoch 366, loss 0.33470621705055237, R2 0.8458490371704102\n",
      "Eval loss 0.33470621705055237, R2 0.5957610607147217\n",
      "epoch 367, loss 0.33398258686065674, R2 0.8460569977760315\n",
      "Eval loss 0.33398258686065674, R2 0.5919426083564758\n",
      "epoch 368, loss 0.3253422975540161, R2 0.8490143418312073\n",
      "Eval loss 0.3253422975540161, R2 0.5953701734542847\n",
      "epoch 369, loss 0.3297679126262665, R2 0.8477321267127991\n",
      "Eval loss 0.3297679126262665, R2 0.596272885799408\n",
      "epoch 370, loss 0.32592862844467163, R2 0.8486640453338623\n",
      "Eval loss 0.32592862844467163, R2 0.5928483605384827\n",
      "epoch 371, loss 0.3262145519256592, R2 0.8492286205291748\n",
      "Eval loss 0.3262145519256592, R2 0.5947316288948059\n",
      "epoch 372, loss 0.32611867785453796, R2 0.8493177890777588\n",
      "Eval loss 0.32611867785453796, R2 0.5952466130256653\n",
      "epoch 373, loss 0.32609307765960693, R2 0.8490102291107178\n",
      "Eval loss 0.32609307765960693, R2 0.5921751856803894\n",
      "epoch 374, loss 0.3218182921409607, R2 0.8504697680473328\n",
      "Eval loss 0.3218182921409607, R2 0.5939933657646179\n",
      "epoch 375, loss 0.32341867685317993, R2 0.8502817153930664\n",
      "Eval loss 0.32341867685317993, R2 0.5943728089332581\n",
      "epoch 376, loss 0.3204512298107147, R2 0.8513275384902954\n",
      "Eval loss 0.3204512298107147, R2 0.5954418778419495\n",
      "epoch 377, loss 0.3186738193035126, R2 0.8517196178436279\n",
      "Eval loss 0.3186738193035126, R2 0.5909044742584229\n",
      "epoch 378, loss 0.31930649280548096, R2 0.8514148592948914\n",
      "Eval loss 0.31930649280548096, R2 0.5963221192359924\n",
      "epoch 379, loss 0.3195137083530426, R2 0.8518516421318054\n",
      "Eval loss 0.3195137083530426, R2 0.5900309681892395\n",
      "epoch 380, loss 0.32567232847213745, R2 0.8504854440689087\n",
      "Eval loss 0.32567232847213745, R2 0.594018280506134\n",
      "epoch 381, loss 0.3239946663379669, R2 0.8512140512466431\n",
      "Eval loss 0.3239946663379669, R2 0.5907840132713318\n",
      "epoch 382, loss 0.3159993588924408, R2 0.853022575378418\n",
      "Eval loss 0.3159993588924408, R2 0.5924515724182129\n",
      "epoch 383, loss 0.318437397480011, R2 0.8522275686264038\n",
      "Eval loss 0.318437397480011, R2 0.593213677406311\n",
      "epoch 384, loss 0.3224441707134247, R2 0.8511024117469788\n",
      "Eval loss 0.3224441707134247, R2 0.5890169143676758\n",
      "epoch 385, loss 0.3186001777648926, R2 0.8524990677833557\n",
      "Eval loss 0.3186001777648926, R2 0.5885554552078247\n",
      "epoch 386, loss 0.3217141032218933, R2 0.8518466949462891\n",
      "Eval loss 0.3217141032218933, R2 0.5942957401275635\n",
      "epoch 387, loss 0.3108556568622589, R2 0.8550897240638733\n",
      "Eval loss 0.3108556568622589, R2 0.5925886034965515\n",
      "epoch 388, loss 0.3134407103061676, R2 0.8546462655067444\n",
      "Eval loss 0.3134407103061676, R2 0.5903663635253906\n",
      "epoch 389, loss 0.3122662901878357, R2 0.8550524115562439\n",
      "Eval loss 0.3122662901878357, R2 0.591876745223999\n",
      "epoch 390, loss 0.30992254614830017, R2 0.855445921421051\n",
      "Eval loss 0.30992254614830017, R2 0.5928972363471985\n",
      "epoch 391, loss 0.30996859073638916, R2 0.8556379079818726\n",
      "Eval loss 0.30996859073638916, R2 0.5897836089134216\n",
      "epoch 392, loss 0.30809491872787476, R2 0.8562032580375671\n",
      "Eval loss 0.30809491872787476, R2 0.5912451148033142\n",
      "epoch 393, loss 0.30820679664611816, R2 0.8562677502632141\n",
      "Eval loss 0.30820679664611816, R2 0.5924685001373291\n",
      "epoch 394, loss 0.30958014726638794, R2 0.8557633757591248\n",
      "Eval loss 0.30958014726638794, R2 0.5873713493347168\n",
      "epoch 395, loss 0.32240036129951477, R2 0.8513174057006836\n",
      "Eval loss 0.32240036129951477, R2 0.5852175354957581\n",
      "epoch 396, loss 0.3332551419734955, R2 0.8476213216781616\n",
      "Eval loss 0.3332551419734955, R2 0.5840585231781006\n",
      "epoch 397, loss 0.3064142167568207, R2 0.8571957945823669\n",
      "Eval loss 0.3064142167568207, R2 0.587303876876831\n",
      "epoch 398, loss 0.3149050772190094, R2 0.8540826439857483\n",
      "Eval loss 0.3149050772190094, R2 0.5905404686927795\n",
      "epoch 399, loss 0.30732789635658264, R2 0.8569186925888062\n",
      "Eval loss 0.30732789635658264, R2 0.5878300070762634\n",
      "epoch 400, loss 0.3046291172504425, R2 0.8578710556030273\n",
      "Eval loss 0.3046291172504425, R2 0.58957439661026\n",
      "epoch 401, loss 0.3071286678314209, R2 0.8574228882789612\n",
      "Eval loss 0.3071286678314209, R2 0.5885188579559326\n",
      "epoch 402, loss 0.3024815320968628, R2 0.8585630059242249\n",
      "Eval loss 0.3024815320968628, R2 0.5909265875816345\n",
      "epoch 403, loss 0.3073537349700928, R2 0.8566129803657532\n",
      "Eval loss 0.3073537349700928, R2 0.5833050608634949\n",
      "epoch 404, loss 0.3086485266685486, R2 0.8569116592407227\n",
      "Eval loss 0.3086485266685486, R2 0.5889661908149719\n",
      "epoch 405, loss 0.3081154227256775, R2 0.8574013113975525\n",
      "Eval loss 0.3081154227256775, R2 0.5869307518005371\n",
      "epoch 406, loss 0.3020133674144745, R2 0.8588451743125916\n",
      "Eval loss 0.3020133674144745, R2 0.5875992178916931\n",
      "epoch 407, loss 0.30210795998573303, R2 0.8590675592422485\n",
      "Eval loss 0.30210795998573303, R2 0.589910089969635\n",
      "epoch 408, loss 0.3044872581958771, R2 0.8581342697143555\n",
      "Eval loss 0.3044872581958771, R2 0.5832744240760803\n",
      "epoch 409, loss 0.30081623792648315, R2 0.8597202301025391\n",
      "Eval loss 0.30081623792648315, R2 0.5882413983345032\n",
      "epoch 410, loss 0.3011462986469269, R2 0.8596862554550171\n",
      "Eval loss 0.3011462986469269, R2 0.5872332453727722\n",
      "epoch 411, loss 0.3043600022792816, R2 0.8589203357696533\n",
      "Eval loss 0.3043600022792816, R2 0.5836721062660217\n",
      "epoch 412, loss 0.3008844554424286, R2 0.8592396378517151\n",
      "Eval loss 0.3008844554424286, R2 0.5877235531806946\n",
      "epoch 413, loss 0.2966625392436981, R2 0.861461877822876\n",
      "Eval loss 0.2966625392436981, R2 0.5885199904441833\n",
      "epoch 414, loss 0.2982105612754822, R2 0.8606952428817749\n",
      "Eval loss 0.2982105612754822, R2 0.5833438634872437\n",
      "epoch 415, loss 0.29865261912345886, R2 0.8609705567359924\n",
      "Eval loss 0.29865261912345886, R2 0.5870985388755798\n",
      "epoch 416, loss 0.2984108626842499, R2 0.8611621260643005\n",
      "Eval loss 0.2984108626842499, R2 0.586610734462738\n",
      "epoch 417, loss 0.2985551953315735, R2 0.8610317707061768\n",
      "Eval loss 0.2985551953315735, R2 0.5842326879501343\n",
      "epoch 418, loss 0.2947465479373932, R2 0.8620336055755615\n",
      "Eval loss 0.2947465479373932, R2 0.5860222578048706\n",
      "epoch 419, loss 0.2921468913555145, R2 0.8630741238594055\n",
      "Eval loss 0.2921468913555145, R2 0.5876688957214355\n",
      "epoch 420, loss 0.29377785325050354, R2 0.8627397418022156\n",
      "Eval loss 0.29377785325050354, R2 0.5827620625495911\n",
      "epoch 421, loss 0.2947767674922943, R2 0.8620809316635132\n",
      "Eval loss 0.2947767674922943, R2 0.5866744518280029\n",
      "epoch 422, loss 0.29652801156044006, R2 0.8612716197967529\n",
      "Eval loss 0.29652801156044006, R2 0.5804913640022278\n",
      "epoch 423, loss 0.29991254210472107, R2 0.8604559302330017\n",
      "Eval loss 0.29991254210472107, R2 0.5837830305099487\n",
      "epoch 424, loss 0.29687339067459106, R2 0.8621921539306641\n",
      "Eval loss 0.29687339067459106, R2 0.5826337933540344\n",
      "epoch 425, loss 0.29541054368019104, R2 0.8626627326011658\n",
      "Eval loss 0.29541054368019104, R2 0.5850947499275208\n",
      "epoch 426, loss 0.29772621393203735, R2 0.8616623282432556\n",
      "Eval loss 0.29772621393203735, R2 0.5803282260894775\n",
      "epoch 427, loss 0.28935179114341736, R2 0.8644599914550781\n",
      "Eval loss 0.28935179114341736, R2 0.5839954018592834\n",
      "epoch 428, loss 0.2898695766925812, R2 0.86441570520401\n",
      "Eval loss 0.2898695766925812, R2 0.585509717464447\n",
      "epoch 429, loss 0.2873570919036865, R2 0.8652384281158447\n",
      "Eval loss 0.2873570919036865, R2 0.583568274974823\n",
      "epoch 430, loss 0.28790727257728577, R2 0.8652178049087524\n",
      "Eval loss 0.28790727257728577, R2 0.5838484168052673\n",
      "epoch 431, loss 0.29067182540893555, R2 0.8646303415298462\n",
      "Eval loss 0.29067182540893555, R2 0.5819420218467712\n",
      "epoch 432, loss 0.28657957911491394, R2 0.8658004999160767\n",
      "Eval loss 0.28657957911491394, R2 0.5846554040908813\n",
      "epoch 433, loss 0.2886761426925659, R2 0.865644633769989\n",
      "Eval loss 0.2886761426925659, R2 0.5802218914031982\n",
      "epoch 434, loss 0.28919506072998047, R2 0.8653507232666016\n",
      "Eval loss 0.28919506072998047, R2 0.5851238369941711\n",
      "epoch 435, loss 0.28834953904151917, R2 0.8654471039772034\n",
      "Eval loss 0.28834953904151917, R2 0.5799499154090881\n",
      "epoch 436, loss 0.288280189037323, R2 0.8656795620918274\n",
      "Eval loss 0.288280189037323, R2 0.5803234577178955\n",
      "epoch 437, loss 0.28638291358947754, R2 0.8656631708145142\n",
      "Eval loss 0.28638291358947754, R2 0.5842371582984924\n",
      "epoch 438, loss 0.2885269522666931, R2 0.8650203943252563\n",
      "Eval loss 0.2885269522666931, R2 0.5765828490257263\n",
      "epoch 439, loss 0.2847847044467926, R2 0.866450309753418\n",
      "Eval loss 0.2847847044467926, R2 0.5824127793312073\n",
      "epoch 440, loss 0.28168419003486633, R2 0.8677015900611877\n",
      "Eval loss 0.28168419003486633, R2 0.5831208825111389\n",
      "epoch 441, loss 0.28132346272468567, R2 0.8680707812309265\n",
      "Eval loss 0.28132346272468567, R2 0.5807502269744873\n",
      "epoch 442, loss 0.2803282141685486, R2 0.8680749535560608\n",
      "Eval loss 0.2803282141685486, R2 0.5809540152549744\n",
      "epoch 443, loss 0.2823497951030731, R2 0.8673418164253235\n",
      "Eval loss 0.2823497951030731, R2 0.580070436000824\n",
      "epoch 444, loss 0.28162750601768494, R2 0.8675426244735718\n",
      "Eval loss 0.28162750601768494, R2 0.5804566740989685\n",
      "epoch 445, loss 0.277483731508255, R2 0.8693906664848328\n",
      "Eval loss 0.277483731508255, R2 0.5804582238197327\n",
      "epoch 446, loss 0.28004103899002075, R2 0.8688320517539978\n",
      "Eval loss 0.28004103899002075, R2 0.5806863903999329\n",
      "epoch 447, loss 0.2827569842338562, R2 0.8682901263237\n",
      "Eval loss 0.2827569842338562, R2 0.5787664651870728\n",
      "epoch 448, loss 0.290313720703125, R2 0.8668540716171265\n",
      "Eval loss 0.290313720703125, R2 0.5753711462020874\n",
      "epoch 449, loss 0.2915283441543579, R2 0.8663661479949951\n",
      "Eval loss 0.2915283441543579, R2 0.5793401598930359\n",
      "epoch 450, loss 0.28535082936286926, R2 0.8670293688774109\n",
      "Eval loss 0.28535082936286926, R2 0.5756996273994446\n",
      "epoch 451, loss 0.28197675943374634, R2 0.8682588338851929\n",
      "Eval loss 0.28197675943374634, R2 0.5791545510292053\n",
      "epoch 452, loss 0.27956321835517883, R2 0.8693093657493591\n",
      "Eval loss 0.27956321835517883, R2 0.574539065361023\n",
      "epoch 453, loss 0.27999672293663025, R2 0.8690519332885742\n",
      "Eval loss 0.27999672293663025, R2 0.5807231068611145\n",
      "epoch 454, loss 0.27341362833976746, R2 0.8711209893226624\n",
      "Eval loss 0.27341362833976746, R2 0.5772202014923096\n",
      "epoch 455, loss 0.2781410813331604, R2 0.8695447444915771\n",
      "Eval loss 0.2781410813331604, R2 0.5784485936164856\n",
      "epoch 456, loss 0.27475258708000183, R2 0.8710705041885376\n",
      "Eval loss 0.27475258708000183, R2 0.5747732520103455\n",
      "epoch 457, loss 0.27774637937545776, R2 0.8700532913208008\n",
      "Eval loss 0.27774637937545776, R2 0.5798487663269043\n",
      "epoch 458, loss 0.2765231430530548, R2 0.8703022599220276\n",
      "Eval loss 0.2765231430530548, R2 0.5755230784416199\n",
      "epoch 459, loss 0.2746080756187439, R2 0.8710489869117737\n",
      "Eval loss 0.2746080756187439, R2 0.5774055123329163\n",
      "epoch 460, loss 0.26999422907829285, R2 0.8726492524147034\n",
      "Eval loss 0.26999422907829285, R2 0.5781117081642151\n",
      "epoch 461, loss 0.2723940908908844, R2 0.8720506429672241\n",
      "Eval loss 0.2723940908908844, R2 0.5750941634178162\n",
      "epoch 462, loss 0.2734560966491699, R2 0.871573269367218\n",
      "Eval loss 0.2734560966491699, R2 0.5794748663902283\n",
      "epoch 463, loss 0.27384448051452637, R2 0.871573269367218\n",
      "Eval loss 0.27384448051452637, R2 0.5713695883750916\n",
      "epoch 464, loss 0.2692579925060272, R2 0.8729331493377686\n",
      "Eval loss 0.2692579925060272, R2 0.5782213807106018\n",
      "epoch 465, loss 0.2686716616153717, R2 0.8733789920806885\n",
      "Eval loss 0.2686716616153717, R2 0.5782397985458374\n",
      "epoch 466, loss 0.26933953166007996, R2 0.8728773593902588\n",
      "Eval loss 0.26933953166007996, R2 0.5734871625900269\n",
      "epoch 467, loss 0.26746225357055664, R2 0.8739070296287537\n",
      "Eval loss 0.26746225357055664, R2 0.5768736600875854\n",
      "epoch 468, loss 0.26635265350341797, R2 0.8744430541992188\n",
      "Eval loss 0.26635265350341797, R2 0.5759674310684204\n",
      "epoch 469, loss 0.2701526880264282, R2 0.8731855154037476\n",
      "Eval loss 0.2701526880264282, R2 0.5729246139526367\n",
      "epoch 470, loss 0.28286802768707275, R2 0.868793249130249\n",
      "Eval loss 0.28286802768707275, R2 0.5693833231925964\n",
      "epoch 471, loss 0.2759706676006317, R2 0.8714771270751953\n",
      "Eval loss 0.2759706676006317, R2 0.5747883319854736\n",
      "epoch 472, loss 0.268239289522171, R2 0.8739246129989624\n",
      "Eval loss 0.268239289522171, R2 0.5739715099334717\n",
      "epoch 473, loss 0.26977968215942383, R2 0.873417317867279\n",
      "Eval loss 0.26977968215942383, R2 0.5717931389808655\n",
      "epoch 474, loss 0.2646922767162323, R2 0.8747907876968384\n",
      "Eval loss 0.2646922767162323, R2 0.5747823119163513\n",
      "epoch 475, loss 0.26692721247673035, R2 0.8743070363998413\n",
      "Eval loss 0.26692721247673035, R2 0.5734118223190308\n",
      "epoch 476, loss 0.2640331983566284, R2 0.8750202655792236\n",
      "Eval loss 0.2640331983566284, R2 0.5721889734268188\n",
      "epoch 477, loss 0.2814929783344269, R2 0.8689731359481812\n",
      "Eval loss 0.2814929783344269, R2 0.5644885897636414\n",
      "epoch 478, loss 0.2973586916923523, R2 0.8631324172019958\n",
      "Eval loss 0.2973586916923523, R2 0.5651257038116455\n",
      "epoch 479, loss 0.2618838846683502, R2 0.8762624263763428\n",
      "Eval loss 0.2618838846683502, R2 0.5705356001853943\n",
      "epoch 480, loss 0.27337515354156494, R2 0.871980607509613\n",
      "Eval loss 0.27337515354156494, R2 0.5722541213035583\n",
      "epoch 481, loss 0.26381775736808777, R2 0.8753446340560913\n",
      "Eval loss 0.26381775736808777, R2 0.5722533464431763\n",
      "epoch 482, loss 0.2599722146987915, R2 0.8769792318344116\n",
      "Eval loss 0.2599722146987915, R2 0.5744588375091553\n",
      "epoch 483, loss 0.26353880763053894, R2 0.8755627274513245\n",
      "Eval loss 0.26353880763053894, R2 0.5697715282440186\n",
      "epoch 484, loss 0.2571694254875183, R2 0.8780595660209656\n",
      "Eval loss 0.2571694254875183, R2 0.5713022947311401\n",
      "epoch 485, loss 0.25882846117019653, R2 0.8774848580360413\n",
      "Eval loss 0.25882846117019653, R2 0.5718584060668945\n",
      "epoch 486, loss 0.2599336504936218, R2 0.8773589730262756\n",
      "Eval loss 0.2599336504936218, R2 0.5737994313240051\n",
      "epoch 487, loss 0.26500046253204346, R2 0.8765390515327454\n",
      "Eval loss 0.26500046253204346, R2 0.5669022798538208\n",
      "epoch 488, loss 0.27349114418029785, R2 0.8738677501678467\n",
      "Eval loss 0.27349114418029785, R2 0.5726200342178345\n",
      "epoch 489, loss 0.2585671842098236, R2 0.8779733180999756\n",
      "Eval loss 0.2585671842098236, R2 0.5705835819244385\n",
      "epoch 490, loss 0.2575836181640625, R2 0.8786085247993469\n",
      "Eval loss 0.2575836181640625, R2 0.5678864121437073\n",
      "epoch 491, loss 0.2607463002204895, R2 0.8776048421859741\n",
      "Eval loss 0.2607463002204895, R2 0.5726813673973083\n",
      "epoch 492, loss 0.25306037068367004, R2 0.8798086047172546\n",
      "Eval loss 0.25306037068367004, R2 0.5716935396194458\n",
      "epoch 493, loss 0.2558947205543518, R2 0.8793872594833374\n",
      "Eval loss 0.2558947205543518, R2 0.5696380138397217\n",
      "epoch 494, loss 0.25545886158943176, R2 0.8793938159942627\n",
      "Eval loss 0.25545886158943176, R2 0.5702784061431885\n",
      "epoch 495, loss 0.2529517710208893, R2 0.8800098299980164\n",
      "Eval loss 0.2529517710208893, R2 0.5708997845649719\n",
      "epoch 496, loss 0.2566189467906952, R2 0.878993570804596\n",
      "Eval loss 0.2566189467906952, R2 0.5663219094276428\n",
      "epoch 497, loss 0.2604648172855377, R2 0.8778652548789978\n",
      "Eval loss 0.2604648172855377, R2 0.570378303527832\n",
      "epoch 498, loss 0.259682834148407, R2 0.8782339096069336\n",
      "Eval loss 0.259682834148407, R2 0.5660665035247803\n",
      "epoch 499, loss 0.25358593463897705, R2 0.880344808101654\n",
      "Eval loss 0.25358593463897705, R2 0.5686771869659424\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    k = 0\n",
    "    l = batch_size\n",
    "    batch_loss = []\n",
    "    while l < train_x.shape[0]:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = NN_model(train_x)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, train_y)\n",
    "        batch_loss.append(loss.item())\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update k,l\n",
    "        k = l\n",
    "        l = min(l+batch_size, train_x.shape[0])\n",
    "\n",
    "    #Append train loss\n",
    "    train_losses.append(np.mean(batch_loss))\n",
    "\n",
    "    #Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "            preds = NN_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    train_preds = NN_model(train_x)\n",
    "    test_preds = NN_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 44]) torch.Size([600, 44]) torch.Size([200, 44]) torch.Size([200, 44])\n"
     ]
    }
   ],
   "source": [
    "from src.model.gaussian_process import MultiOutputGP\n",
    "\n",
    "#Hyperparams\n",
    "learningRate = 0.1\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "mean = \"Linear\"\n",
    "kernel = \"Linear\"\n",
    "\n",
    "epochs = 50\n",
    "eval_epoch_freq = 1\n",
    "in_size=train_x.shape[-1]\n",
    "out_size=train_y.shape[-1]\n",
    "device=\"cpu\"\n",
    "GP_model = MultiOutputGP(in_size, out_size, device, mean, kernel)\n",
    "mll = gpytorch.mlls.SumMarginalLogLikelihood(GP_model.likelihood, GP_model.gp)\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "                GP_model.parameters(),\n",
    "                lr=learningRate,\n",
    "                weight_decay=weight_decay,\n",
    "                eps=optim_eps,\n",
    "            )\n",
    "#torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "dataset_size_gp = 1000\n",
    "gp_idx_split = int(test_split_ratio*dataset_size_gp)\n",
    "\n",
    "train_x_gp = train_x[gp_idx_split:dataset_size_gp, ...]\n",
    "train_y_gp = train_y[gp_idx_split:dataset_size_gp, ...]\n",
    "test_x_gp = test_x[:gp_idx_split, ...]\n",
    "test_y_gp = test_y[:gp_idx_split, ...]\n",
    "\n",
    "print(train_x_gp.shape, train_y_gp.shape, test_x_gp.shape, test_y_gp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.665695071220398, R2 -193.2993621826172\n",
      "Eval loss 1.665695071220398, R2 0.3234846889972687\n",
      "epoch 1, loss 1.6216813325881958, R2 -172.3352508544922\n",
      "Eval loss 1.6216813325881958, R2 0.3712581694126129\n",
      "epoch 2, loss 1.5875122547149658, R2 -147.14016723632812\n",
      "Eval loss 1.5875122547149658, R2 0.41509801149368286\n",
      "epoch 3, loss 1.560895562171936, R2 -124.4258041381836\n",
      "Eval loss 1.560895562171936, R2 0.4552282989025116\n",
      "epoch 4, loss 1.539678931236267, R2 -105.1559829711914\n",
      "Eval loss 1.539678931236267, R2 0.49158209562301636\n",
      "epoch 5, loss 1.5222197771072388, R2 -89.65971374511719\n",
      "Eval loss 1.5222197771072388, R2 0.5242920517921448\n",
      "epoch 6, loss 1.5074580907821655, R2 -77.09456634521484\n",
      "Eval loss 1.5074580907821655, R2 0.5532590746879578\n",
      "epoch 7, loss 1.4947861433029175, R2 -66.50433349609375\n",
      "Eval loss 1.4947861433029175, R2 0.5786527991294861\n",
      "epoch 8, loss 1.4837571382522583, R2 -57.077491760253906\n",
      "Eval loss 1.4837571382522583, R2 0.6005467772483826\n",
      "epoch 9, loss 1.4739845991134644, R2 -48.13608169555664\n",
      "Eval loss 1.4739845991134644, R2 0.6190318465232849\n",
      "epoch 10, loss 1.4651297330856323, R2 -39.68383026123047\n",
      "Eval loss 1.4651297330856323, R2 0.6342481374740601\n",
      "epoch 11, loss 1.456931233406067, R2 -32.11955261230469\n",
      "Eval loss 1.456931233406067, R2 0.6463204622268677\n",
      "epoch 12, loss 1.4491914510726929, R2 -25.66987419128418\n",
      "Eval loss 1.4491914510726929, R2 0.6554266810417175\n",
      "epoch 13, loss 1.4417616128921509, R2 -20.365787506103516\n",
      "Eval loss 1.4417616128921509, R2 0.6617349982261658\n",
      "epoch 14, loss 1.4345927238464355, R2 -15.800434112548828\n",
      "Eval loss 1.4345927238464355, R2 0.6655288338661194\n",
      "epoch 15, loss 1.4276105165481567, R2 -11.594304084777832\n",
      "Eval loss 1.4276105165481567, R2 0.6671352386474609\n",
      "epoch 16, loss 1.420789361000061, R2 -8.03861141204834\n",
      "Eval loss 1.420789361000061, R2 0.6670459508895874\n",
      "epoch 17, loss 1.4140626192092896, R2 -5.871542453765869\n",
      "Eval loss 1.4140626192092896, R2 0.6658582091331482\n",
      "epoch 18, loss 1.407378911972046, R2 -4.915627479553223\n",
      "Eval loss 1.407378911972046, R2 0.664299726486206\n",
      "epoch 19, loss 1.4006344079971313, R2 -4.24060583114624\n",
      "Eval loss 1.4006344079971313, R2 0.6630567908287048\n",
      "epoch 20, loss 1.393762469291687, R2 -3.481429100036621\n",
      "Eval loss 1.393762469291687, R2 0.662632405757904\n",
      "epoch 21, loss 1.386672854423523, R2 -2.7870569229125977\n",
      "Eval loss 1.386672854423523, R2 0.6632146835327148\n",
      "epoch 22, loss 1.3792866468429565, R2 -2.0984621047973633\n",
      "Eval loss 1.3792866468429565, R2 0.6646735668182373\n",
      "epoch 23, loss 1.3715378046035767, R2 -1.511911392211914\n",
      "Eval loss 1.3715378046035767, R2 0.6666529774665833\n",
      "epoch 24, loss 1.363436222076416, R2 -1.2024880647659302\n",
      "Eval loss 1.363436222076416, R2 0.6686844229698181\n",
      "epoch 25, loss 1.3551138639450073, R2 -0.8004502654075623\n",
      "Eval loss 1.3551138639450073, R2 0.6703107953071594\n",
      "epoch 26, loss 1.3467415571212769, R2 -0.5101746320724487\n",
      "Eval loss 1.3467415571212769, R2 0.6711938977241516\n",
      "epoch 27, loss 1.3384283781051636, R2 -0.3321249783039093\n",
      "Eval loss 1.3384283781051636, R2 0.6711347699165344\n",
      "epoch 28, loss 1.3302401304244995, R2 -0.08973856270313263\n",
      "Eval loss 1.3302401304244995, R2 0.6702631115913391\n",
      "epoch 29, loss 1.3221503496170044, R2 0.10403559356927872\n",
      "Eval loss 1.3221503496170044, R2 0.6692230701446533\n",
      "epoch 30, loss 1.3141074180603027, R2 0.20947088301181793\n",
      "Eval loss 1.3141074180603027, R2 0.6685822010040283\n",
      "epoch 31, loss 1.3060208559036255, R2 0.2938460409641266\n",
      "Eval loss 1.3060208559036255, R2 0.6684844493865967\n",
      "epoch 32, loss 1.2976759672164917, R2 0.3280123472213745\n",
      "Eval loss 1.2976759672164917, R2 0.6690071225166321\n",
      "epoch 33, loss 1.2892206907272339, R2 0.4097738265991211\n",
      "Eval loss 1.2892206907272339, R2 0.6699920296669006\n",
      "epoch 34, loss 1.2805445194244385, R2 0.4831410348415375\n",
      "Eval loss 1.2805445194244385, R2 0.6703713536262512\n",
      "epoch 35, loss 1.272159457206726, R2 0.5163478851318359\n",
      "Eval loss 1.272159457206726, R2 0.6705754399299622\n",
      "epoch 36, loss 1.2641682624816895, R2 0.4885173738002777\n",
      "Eval loss 1.2641682624816895, R2 0.6711703538894653\n",
      "epoch 37, loss 1.2565369606018066, R2 0.5782256722450256\n",
      "Eval loss 1.2565369606018066, R2 0.6715769171714783\n",
      "epoch 38, loss 1.2494709491729736, R2 0.6108365654945374\n",
      "Eval loss 1.2494709491729736, R2 0.6721785664558411\n",
      "epoch 39, loss 1.2419873476028442, R2 0.5998634696006775\n",
      "Eval loss 1.2419873476028442, R2 0.6717971563339233\n",
      "epoch 40, loss 1.2346850633621216, R2 0.6178608536720276\n",
      "Eval loss 1.2346850633621216, R2 0.6709835529327393\n",
      "epoch 41, loss 1.2275595664978027, R2 0.6505540013313293\n",
      "Eval loss 1.2275595664978027, R2 0.6703683733940125\n",
      "epoch 42, loss 1.2213364839553833, R2 0.6472702026367188\n",
      "Eval loss 1.2213364839553833, R2 0.6696606278419495\n",
      "epoch 43, loss 1.2157483100891113, R2 0.6408264636993408\n",
      "Eval loss 1.2157483100891113, R2 0.6687657833099365\n",
      "epoch 44, loss 1.209984302520752, R2 0.6617204546928406\n",
      "Eval loss 1.209984302520752, R2 0.6696370840072632\n",
      "epoch 45, loss 1.2046273946762085, R2 0.6409029364585876\n",
      "Eval loss 1.2046273946762085, R2 0.669524073600769\n",
      "epoch 46, loss 1.200682520866394, R2 0.6509211659431458\n",
      "Eval loss 1.200682520866394, R2 0.6692743301391602\n",
      "epoch 47, loss 1.197773814201355, R2 0.6287735104560852\n",
      "Eval loss 1.197773814201355, R2 0.6700400114059448\n",
      "epoch 48, loss 1.193537712097168, R2 0.6353809237480164\n",
      "Eval loss 1.193537712097168, R2 0.6694669127464294\n",
      "epoch 49, loss 1.19364595413208, R2 0.6075271368026733\n",
      "Eval loss 1.19364595413208, R2 0.6701174378395081\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    #Set training mode REQUIRED FOR GP\n",
    "    GP_model.gp.train()\n",
    "\n",
    "    #Set the training data\n",
    "    if epoch==0:\n",
    "        GP_model.set_train_data(train_x_gp, train_y_gp)\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = GP_model.forward()\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = -mll(outputs, GP_model.gp.train_targets)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Compute metric\n",
    "    train_pred_output = GP_model.likelihood(*outputs)\n",
    "    train_pred_mean = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_pred_output], axis=-1\n",
    "    )\n",
    "    train_metric = metric(train_pred_mean, train_y_gp)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print('epoch {}, loss {}, R2 {}'.format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch%eval_epoch_freq==0:\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            GP_model.gp.eval()\n",
    "            GP_model.likelihood.eval()\n",
    "            preds = GP_model.forward(test_x_gp)\n",
    "            test_loss = -mll(preds, [test_y_gp[..., i] for i in range(len(GP_model.gp.models))])\n",
    "            test_losses.append(test_loss.item())\n",
    "            #Compute metric\n",
    "            test_pred_output = GP_model.likelihood(*preds)\n",
    "            test_pred_mean = torch.cat(\n",
    "                [pred.mean.unsqueeze(-1) for pred in test_pred_output], axis=-1\n",
    "            )\n",
    "            test_metric = metric(test_pred_mean, test_y_gp)\n",
    "            test_metrics.append(test_metric)\n",
    "            print('Eval loss {}, R2 {}'.format(loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, 'b', label='Training loss')\n",
    "ax[0].plot(test_losses, 'g', label='Test loss')\n",
    "ax[1].plot(train_metrics, 'b', label='Train R2')\n",
    "ax[1].plot(test_metrics, 'g', label='Test R2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = GP_model.likelihood(*GP_model(train_x_gp))\n",
    "    test_preds = GP_model.likelihood(*GP_model(test_x_gp))\n",
    "    train_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_preds], axis=-1\n",
    "    )\n",
    "    test_preds = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in test_preds], axis=-1\n",
    "    )\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y_gp, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y_gp, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y_gp, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y_gp, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Colors and labels\n",
    "if len(relative_train_preds_mean)<num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean)==num_centroids:\n",
    "    color=\"green\"\n",
    "    label='centroids'\n",
    "else:\n",
    "    colors=[\"green\"]*num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label=[\"centroids\"]*num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "#Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(\n",
    "    x,\n",
    "    relative_train_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_train_preds_mean, relative_train_preds_std, colors):\n",
    "    ax[0].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text('Training set')\n",
    "\n",
    "#Test bar plots\n",
    "ax[1].bar(\n",
    "    x,\n",
    "    relative_test_preds_mean,\n",
    "    color=colors,\n",
    "    label=label\n",
    ")\n",
    "for pos, y, err, color in zip(x, relative_test_preds_mean, relative_test_preds_std, colors):\n",
    "    ax[1].errorbar(pos, y, err, lw = 2,capsize = 2, capthick = 1, color = color, alpha=0.3)\n",
    "ax[1].title.set_text('Test set')\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600,) (200,)\n",
      "Lower train: -1.8210667371749878+-0.01887916587293148\n",
      "Upper train: 1.8210667371749878+-0.01887916401028633\n",
      "Lower train: -1.8232992887496948+-0.018743911758065224\n",
      "Lower train: 1.8232992887496948+-0.01874392479658127\n"
     ]
    }
   ],
   "source": [
    "gp_id = 0\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](train_x_gp))\n",
    "    test_pred = GP_model.likelihood.likelihoods[gp_id](GP_model.gp.models[gp_id](test_x_gp))\n",
    "\n",
    "    train_lower, train_upper = train_pred.confidence_region()\n",
    "    mean_lower_train = np.mean(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    std_lower_train = np.std(train_lower.numpy()-train_pred.mean.numpy())\n",
    "    mean_upper_train = np.mean(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    std_upper_train = np.std(train_upper.numpy()-train_pred.mean.numpy())\n",
    "    test_lower, test_upper = test_pred.confidence_region()\n",
    "    mean_lower_test = np.mean(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    std_lower_test = np.std(test_lower.numpy()-test_pred.mean.numpy())\n",
    "    mean_upper_test = np.mean(test_upper.numpy()-test_pred.mean.numpy())\n",
    "    std_upper_test = np.std(test_upper.numpy()-test_pred.mean.numpy())\n",
    "\n",
    "    print(train_pred.mean.numpy().shape, test_pred.mean.numpy().shape)\n",
    "    print(f\"Lower train: {mean_lower_train}+-{std_lower_train}\")\n",
    "    print(f\"Upper train: {mean_upper_train}+-{std_upper_train}\")\n",
    "    print(f\"Lower train: {mean_lower_test}+-{std_lower_test}\")\n",
    "    print(f\"Lower train: {mean_upper_test}+-{std_upper_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MBRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 10\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = lr_model\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL loop (with pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1, reward: 2.573672088377971.\n",
      "Trial: 2, reward: 2.705402098585997.\n",
      "Trial: 3, reward: 2.322563968184047.\n",
      "Trial: 4, reward: 2.677491420881591.\n",
      "Trial: 5, reward: 2.8242376956986925.\n",
      "Trial: 6, reward: 3.101734248162826.\n",
      "Trial: 7, reward: 1.843696885892893.\n",
      "Trial: 8, reward: 1.903616935018447.\n",
      "Trial: 9, reward: 2.8298462815498127.\n",
      "Trial: 10, reward: 2.7504850048228.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "\n",
    "while (\n",
    "    current_trial < num_episodes\n",
    "):\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "\n",
    "        # --- Doing env step using the agent ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "        \n",
    "        #print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative Reward')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sum(rewards, axis=-1))\n",
    "plt.xlabel('Day'); plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training MBRL agent with a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from mbrl.models.model import Model\n",
    "from mbrl.util.logger import Logger\n",
    "import mbrl.util.common\n",
    "import mbrl.models\n",
    "\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "from src.util.model_trainer import ModelTrainer, ModelTrainerOverriden\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "\n",
    "#WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "#Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5, #10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\", #sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    #\"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None, #src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "#Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 20\n",
    "num_steps = num_episodes*env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False #True\n",
    "num_particles = 20\n",
    "optim_lr = 0.1\n",
    "model_wd = 0\n",
    "freq_train_model = 10\n",
    "model_batch_size = num_steps+initial_exploration_steps #Make sense for LR or GP\n",
    "validation_ratio = 0\n",
    "num_epochs = 100\n",
    "\n",
    "#Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "#Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(\n",
    "    env_config,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "#Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "#Dynamics model\n",
    "model = lr_model\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "#Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "#Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n",
    "\n",
    "#Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "\n",
    "#Replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    num_steps+initial_exploration_steps,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    #max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=initial_exploration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training MBRL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500\n",
      "Epoch: 0 Train loss 4.989, Test loss 4.970\n",
      "1 500\n",
      "Epoch: 1 Train loss 4.970, Test loss 4.952\n",
      "1 500\n",
      "Epoch: 2 Train loss 4.952, Test loss 4.934\n",
      "1 500\n",
      "Epoch: 3 Train loss 4.934, Test loss 4.916\n",
      "1 500\n",
      "Epoch: 4 Train loss 4.916, Test loss 4.898\n",
      "1 500\n",
      "Epoch: 5 Train loss 4.898, Test loss 4.880\n",
      "1 500\n",
      "Epoch: 6 Train loss 4.880, Test loss 4.863\n",
      "1 500\n",
      "Epoch: 7 Train loss 4.863, Test loss 4.846\n",
      "1 500\n",
      "Epoch: 8 Train loss 4.846, Test loss 4.829\n",
      "1 500\n",
      "Epoch: 9 Train loss 4.829, Test loss 4.812\n",
      "1 500\n",
      "Epoch: 10 Train loss 4.812, Test loss 4.795\n",
      "1 500\n",
      "Epoch: 11 Train loss 4.795, Test loss 4.779\n",
      "1 500\n",
      "Epoch: 12 Train loss 4.779, Test loss 4.763\n",
      "1 500\n",
      "Epoch: 13 Train loss 4.763, Test loss 4.746\n",
      "1 500\n",
      "Epoch: 14 Train loss 4.746, Test loss 4.731\n",
      "1 500\n",
      "Epoch: 15 Train loss 4.731, Test loss 4.715\n",
      "1 500\n",
      "Epoch: 16 Train loss 4.715, Test loss 4.699\n",
      "1 500\n",
      "Epoch: 17 Train loss 4.699, Test loss 4.684\n",
      "1 500\n",
      "Epoch: 18 Train loss 4.684, Test loss 4.669\n",
      "1 500\n",
      "Epoch: 19 Train loss 4.669, Test loss 4.654\n",
      "1 500\n",
      "Epoch: 20 Train loss 4.654, Test loss 4.639\n",
      "1 500\n",
      "Epoch: 21 Train loss 4.639, Test loss 4.624\n",
      "1 500\n",
      "Epoch: 22 Train loss 4.624, Test loss 4.610\n",
      "1 500\n",
      "Epoch: 23 Train loss 4.610, Test loss 4.595\n",
      "1 500\n",
      "Epoch: 24 Train loss 4.595, Test loss 4.581\n",
      "1 500\n",
      "Epoch: 25 Train loss 4.581, Test loss 4.567\n",
      "1 500\n",
      "Epoch: 26 Train loss 4.567, Test loss 4.553\n",
      "1 500\n",
      "Epoch: 27 Train loss 4.553, Test loss 4.539\n",
      "1 500\n",
      "Epoch: 28 Train loss 4.539, Test loss 4.526\n",
      "1 500\n",
      "Epoch: 29 Train loss 4.526, Test loss 4.512\n",
      "1 500\n",
      "Epoch: 30 Train loss 4.512, Test loss 4.499\n",
      "1 500\n",
      "Epoch: 31 Train loss 4.499, Test loss 4.486\n",
      "1 500\n",
      "Epoch: 32 Train loss 4.486, Test loss 4.473\n",
      "1 500\n",
      "Epoch: 33 Train loss 4.473, Test loss 4.460\n",
      "1 500\n",
      "Epoch: 34 Train loss 4.460, Test loss 4.447\n",
      "1 500\n",
      "Epoch: 35 Train loss 4.447, Test loss 4.435\n",
      "1 500\n",
      "Epoch: 36 Train loss 4.435, Test loss 4.423\n",
      "1 500\n",
      "Epoch: 37 Train loss 4.423, Test loss 4.410\n",
      "1 500\n",
      "Epoch: 38 Train loss 4.410, Test loss 4.398\n",
      "1 500\n",
      "Epoch: 39 Train loss 4.398, Test loss 4.386\n",
      "1 500\n",
      "Epoch: 40 Train loss 4.386, Test loss 4.374\n",
      "1 500\n",
      "Epoch: 41 Train loss 4.374, Test loss 4.362\n",
      "1 500\n",
      "Epoch: 42 Train loss 4.362, Test loss 4.351\n",
      "1 500\n",
      "Epoch: 43 Train loss 4.351, Test loss 4.339\n",
      "1 500\n",
      "Epoch: 44 Train loss 4.339, Test loss 4.328\n",
      "1 500\n",
      "Epoch: 45 Train loss 4.328, Test loss 4.317\n",
      "1 500\n",
      "Epoch: 46 Train loss 4.317, Test loss 4.306\n",
      "1 500\n",
      "Epoch: 47 Train loss 4.306, Test loss 4.295\n",
      "1 500\n",
      "Epoch: 48 Train loss 4.295, Test loss 4.284\n",
      "1 500\n",
      "Epoch: 49 Train loss 4.284, Test loss 4.273\n",
      "1 500\n",
      "Epoch: 50 Train loss 4.273, Test loss 4.263\n",
      "1 500\n",
      "Epoch: 51 Train loss 4.263, Test loss 4.252\n",
      "1 500\n",
      "Epoch: 52 Train loss 4.252, Test loss 4.242\n",
      "1 500\n",
      "Epoch: 53 Train loss 4.242, Test loss 4.231\n",
      "1 500\n",
      "Epoch: 54 Train loss 4.231, Test loss 4.221\n",
      "1 500\n",
      "Epoch: 55 Train loss 4.221, Test loss 4.211\n",
      "1 500\n",
      "Epoch: 56 Train loss 4.211, Test loss 4.201\n",
      "1 500\n",
      "Epoch: 57 Train loss 4.201, Test loss 4.191\n",
      "1 500\n",
      "Epoch: 58 Train loss 4.191, Test loss 4.182\n",
      "1 500\n",
      "Epoch: 59 Train loss 4.182, Test loss 4.172\n",
      "1 500\n",
      "Epoch: 60 Train loss 4.172, Test loss 4.163\n",
      "1 500\n",
      "Epoch: 61 Train loss 4.163, Test loss 4.153\n",
      "1 500\n",
      "Epoch: 62 Train loss 4.153, Test loss 4.144\n",
      "1 500\n",
      "Epoch: 63 Train loss 4.144, Test loss 4.135\n",
      "1 500\n",
      "Epoch: 64 Train loss 4.135, Test loss 4.126\n",
      "1 500\n",
      "Epoch: 65 Train loss 4.126, Test loss 4.117\n",
      "1 500\n",
      "Epoch: 66 Train loss 4.117, Test loss 4.108\n",
      "1 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 Train loss 4.108, Test loss 4.099\n",
      "1 500\n",
      "Epoch: 68 Train loss 4.099, Test loss 4.090\n",
      "1 500\n",
      "Epoch: 69 Train loss 4.090, Test loss 4.082\n",
      "1 500\n",
      "Epoch: 70 Train loss 4.082, Test loss 4.073\n",
      "1 500\n",
      "Epoch: 71 Train loss 4.073, Test loss 4.065\n",
      "1 500\n",
      "Epoch: 72 Train loss 4.065, Test loss 4.056\n",
      "1 500\n",
      "Epoch: 73 Train loss 4.056, Test loss 4.048\n",
      "1 500\n",
      "Epoch: 74 Train loss 4.048, Test loss 4.040\n",
      "1 500\n",
      "Epoch: 75 Train loss 4.040, Test loss 4.032\n",
      "1 500\n",
      "Epoch: 76 Train loss 4.032, Test loss 4.024\n",
      "1 500\n",
      "Epoch: 77 Train loss 4.024, Test loss 4.016\n",
      "1 500\n",
      "Epoch: 78 Train loss 4.016, Test loss 4.008\n",
      "1 500\n",
      "Epoch: 79 Train loss 4.008, Test loss 4.001\n",
      "1 500\n",
      "Epoch: 80 Train loss 4.001, Test loss 3.993\n",
      "1 500\n",
      "Epoch: 81 Train loss 3.993, Test loss 3.985\n",
      "1 500\n",
      "Epoch: 82 Train loss 3.985, Test loss 3.978\n",
      "1 500\n",
      "Epoch: 83 Train loss 3.978, Test loss 3.971\n",
      "1 500\n",
      "Epoch: 84 Train loss 3.971, Test loss 3.963\n",
      "1 500\n",
      "Epoch: 85 Train loss 3.963, Test loss 3.956\n",
      "1 500\n",
      "Epoch: 86 Train loss 3.956, Test loss 3.949\n",
      "1 500\n",
      "Epoch: 87 Train loss 3.949, Test loss 3.942\n",
      "1 500\n",
      "Epoch: 88 Train loss 3.942, Test loss 3.935\n",
      "1 500\n",
      "Epoch: 89 Train loss 3.935, Test loss 3.928\n",
      "1 500\n",
      "Epoch: 90 Train loss 3.928, Test loss 3.921\n",
      "1 500\n",
      "Epoch: 91 Train loss 3.921, Test loss 3.914\n",
      "1 500\n",
      "Epoch: 92 Train loss 3.914, Test loss 3.908\n",
      "1 500\n",
      "Epoch: 93 Train loss 3.908, Test loss 3.901\n",
      "1 500\n",
      "Epoch: 94 Train loss 3.901, Test loss 3.895\n",
      "1 500\n",
      "Epoch: 95 Train loss 3.895, Test loss 3.888\n",
      "1 500\n",
      "Epoch: 96 Train loss 3.888, Test loss 3.882\n",
      "1 500\n",
      "Epoch: 97 Train loss 3.882, Test loss 3.875\n",
      "1 500\n",
      "Epoch: 98 Train loss 3.875, Test loss 3.869\n",
      "1 500\n",
      "Epoch: 99 Train loss 3.869, Test loss 3.863\n",
      "Mean train loss: 4.329 Mean val score: 4.318\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.\n",
      "   3. -1. -1.  2.  1.  0.  2.  2. -1.  0. -1.  0.  0.  0. -1.  0.  0.  0.\n",
      "  -1.  0. -1.  0. -1. -1.  0.  1.  1.  0.]] [[0.0102827]] False\n",
      "Step 1: Reward 0.167.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  3.\n",
      "   2. -1. -1.  2.  0.  3.  5.  2.  4.  0. -1.  0.  0.  0. -1.  0.  0.  1.\n",
      "  -1.  0. -1.  0. -1. -1.  0.  1.  1.  1.]] [[0.1667741]] False\n",
      "Step 2: Reward 0.333.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0. -1.  3.\n",
      "   6. -1. -1.  2. -1.  5.  8.  6.  4.  0. -1.  0.  0.  0. -1. -1.  0.  2.\n",
      "  -1.  0. -1.  0. -1. -1.  0.  1.  1.  2.]] [[-0.06944429]] False\n",
      "Step 3: Reward 0.111.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0. -1. -1.  3.\n",
      "   6. -1. -1.  2.  3. 12.  3. 10.  7.  0. -1.  0.  0. -1. -1. -1.  0.  1.\n",
      "  -1. -1. -1.  0. -1. -1.  0.  1.  1.  3.]] [[-0.01850341]] False\n",
      "Step 4: Reward 0.174.\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  2. -1.  0.  5. -1. -1. -1.  2.\n",
      "   6.  5.  0.  2.  2. 12.  0. 10.  7.  8. -1.  0.  0. -1. -1. -1.  1.  0.\n",
      "   0. -1. -1.  0.  0. -1.  0.  1.  1.  4.]] [[0.28333068]] False\n",
      "Step 5: Reward 0.485.\n",
      "[[ 0.  0.  0.  0.  0.  1.  1.  0.  0. -1.  3.  0.  0.  5. -1. -1. -1.  4.\n",
      "   5.  8. -1.  6.  1.  7.  0. 10. 10. 12. -1.  0.  0. -1. -1.  1.  2. -1.\n",
      "   1. -1. -1.  0.  0. -1.  0.  1.  1.  4.]] [[0.22494417]] False\n",
      "Step 6: Reward 0.439.\n",
      "[[ 1.  0.  0.  0.  1.  1.  0.  1.  0. -1.  3.  4.  0.  4. -1. -1. -1.  4.\n",
      "   5.  8. -1.  4.  0.  9.  0. 14. 10.  9.  7. -1.  3. -1. -1.  0.  1. -1.\n",
      "   4. -1.  0. -1.  0. -1.  0.  1.  1.  5.]] [[0.27606803]] False\n",
      "Step 7: Reward 0.521.\n",
      "[[ 1.  0.  2.  0.  1.  1.  0.  1.  0. -1.  3.  4.  0.  4. -1. -1. -1.  4.\n",
      "   5.  8.  3.  6.  0. 12.  4. 14. 10. 13.  7.  3.  2. -1. -1.  0.  1. -1.\n",
      "   4. -1.  0. -1.  0. -1.  0.  1.  1.  6.]] [[0.00581704]] True\n",
      "Step 8: Reward 0.250.\n",
      "Trial: 1, reward: 2.4805455449637037.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.\n",
      "   0. -1. -1.  2.  3.  0.  1.  3. -1.  0. -1.  0.  0.  0. -1.  0.  0.  0.\n",
      "  -1.  0. -1.  0. -1. -1. -1. 13.  6.  0.]] [[0.04676713]] False\n",
      "Step 9: Reward 0.203.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  1.  0.  0.  2.  2.\n",
      "   0. -1. -1.  3.  2.  0.  0.  2.  5.  0.  1.  0.  1.  0. -1.  0.  0.  0.\n",
      "  -1.  0. -1.  0. -1. -1. -1. 13.  6.  1.]] [[-0.02346547]] False\n",
      "Step 10: Reward 0.136.\n",
      "1 510\n",
      "Epoch: 0 Train loss 3.976, Test loss 3.969\n",
      "1 510\n",
      "Epoch: 1 Train loss 3.969, Test loss 3.963\n",
      "1 510\n",
      "Epoch: 2 Train loss 3.963, Test loss 3.956\n",
      "1 510\n",
      "Epoch: 3 Train loss 3.956, Test loss 3.950\n",
      "1 510\n",
      "Epoch: 4 Train loss 3.950, Test loss 3.944\n",
      "1 510\n",
      "Epoch: 5 Train loss 3.944, Test loss 3.937\n",
      "1 510\n",
      "Epoch: 6 Train loss 3.937, Test loss 3.931\n",
      "1 510\n",
      "Epoch: 7 Train loss 3.931, Test loss 3.925\n",
      "1 510\n",
      "Epoch: 8 Train loss 3.925, Test loss 3.919\n",
      "1 510\n",
      "Epoch: 9 Train loss 3.919, Test loss 3.913\n",
      "1 510\n",
      "Epoch: 10 Train loss 3.913, Test loss 3.907\n",
      "1 510\n",
      "Epoch: 11 Train loss 3.907, Test loss 3.901\n",
      "1 510\n",
      "Epoch: 12 Train loss 3.901, Test loss 3.895\n",
      "1 510\n",
      "Epoch: 13 Train loss 3.895, Test loss 3.890\n",
      "1 510\n",
      "Epoch: 14 Train loss 3.890, Test loss 3.884\n",
      "1 510\n",
      "Epoch: 15 Train loss 3.884, Test loss 3.878\n",
      "1 510\n",
      "Epoch: 16 Train loss 3.878, Test loss 3.873\n",
      "1 510\n",
      "Epoch: 17 Train loss 3.873, Test loss 3.867\n",
      "1 510\n",
      "Epoch: 18 Train loss 3.867, Test loss 3.862\n",
      "1 510\n",
      "Epoch: 19 Train loss 3.862, Test loss 3.856\n",
      "1 510\n",
      "Epoch: 20 Train loss 3.856, Test loss 3.851\n",
      "1 510\n",
      "Epoch: 21 Train loss 3.851, Test loss 3.846\n",
      "1 510\n",
      "Epoch: 22 Train loss 3.846, Test loss 3.841\n",
      "1 510\n",
      "Epoch: 23 Train loss 3.841, Test loss 3.835\n",
      "1 510\n",
      "Epoch: 24 Train loss 3.835, Test loss 3.830\n",
      "1 510\n",
      "Epoch: 25 Train loss 3.830, Test loss 3.825\n",
      "1 510\n",
      "Epoch: 26 Train loss 3.825, Test loss 3.820\n",
      "1 510\n",
      "Epoch: 27 Train loss 3.820, Test loss 3.815\n",
      "1 510\n",
      "Epoch: 28 Train loss 3.815, Test loss 3.810\n",
      "1 510\n",
      "Epoch: 29 Train loss 3.810, Test loss 3.806\n",
      "1 510\n",
      "Epoch: 30 Train loss 3.806, Test loss 3.801\n",
      "1 510\n",
      "Epoch: 31 Train loss 3.801, Test loss 3.796\n",
      "1 510\n",
      "Epoch: 32 Train loss 3.796, Test loss 3.791\n",
      "1 510\n",
      "Epoch: 33 Train loss 3.791, Test loss 3.787\n",
      "1 510\n",
      "Epoch: 34 Train loss 3.787, Test loss 3.782\n",
      "1 510\n",
      "Epoch: 35 Train loss 3.782, Test loss 3.778\n",
      "1 510\n",
      "Epoch: 36 Train loss 3.778, Test loss 3.773\n",
      "1 510\n",
      "Epoch: 37 Train loss 3.773, Test loss 3.769\n",
      "1 510\n",
      "Epoch: 38 Train loss 3.769, Test loss 3.764\n",
      "1 510\n",
      "Epoch: 39 Train loss 3.764, Test loss 3.760\n",
      "1 510\n",
      "Epoch: 40 Train loss 3.760, Test loss 3.755\n",
      "1 510\n",
      "Epoch: 41 Train loss 3.755, Test loss 3.751\n",
      "1 510\n",
      "Epoch: 42 Train loss 3.751, Test loss 3.747\n",
      "1 510\n",
      "Epoch: 43 Train loss 3.747, Test loss 3.743\n",
      "1 510\n",
      "Epoch: 44 Train loss 3.743, Test loss 3.739\n",
      "1 510\n",
      "Epoch: 45 Train loss 3.739, Test loss 3.735\n",
      "1 510\n",
      "Epoch: 46 Train loss 3.735, Test loss 3.731\n",
      "1 510\n",
      "Epoch: 47 Train loss 3.731, Test loss 3.727\n",
      "1 510\n",
      "Epoch: 48 Train loss 3.727, Test loss 3.723\n",
      "1 510\n",
      "Epoch: 49 Train loss 3.723, Test loss 3.719\n",
      "1 510\n",
      "Epoch: 50 Train loss 3.719, Test loss 3.715\n",
      "1 510\n",
      "Epoch: 51 Train loss 3.715, Test loss 3.711\n",
      "1 510\n",
      "Epoch: 52 Train loss 3.711, Test loss 3.707\n",
      "1 510\n",
      "Epoch: 53 Train loss 3.707, Test loss 3.703\n",
      "1 510\n",
      "Epoch: 54 Train loss 3.703, Test loss 3.700\n",
      "1 510\n",
      "Epoch: 55 Train loss 3.700, Test loss 3.696\n",
      "1 510\n",
      "Epoch: 56 Train loss 3.696, Test loss 3.692\n",
      "1 510\n",
      "Epoch: 57 Train loss 3.692, Test loss 3.689\n",
      "1 510\n",
      "Epoch: 58 Train loss 3.689, Test loss 3.685\n",
      "1 510\n",
      "Epoch: 59 Train loss 3.685, Test loss 3.682\n",
      "1 510\n",
      "Epoch: 60 Train loss 3.682, Test loss 3.678\n",
      "1 510\n",
      "Epoch: 61 Train loss 3.678, Test loss 3.675\n",
      "1 510\n",
      "Epoch: 62 Train loss 3.675, Test loss 3.671\n",
      "1 510\n",
      "Epoch: 63 Train loss 3.671, Test loss 3.668\n",
      "1 510\n",
      "Epoch: 64 Train loss 3.668, Test loss 3.664\n",
      "1 510\n",
      "Epoch: 65 Train loss 3.664, Test loss 3.661\n",
      "1 510\n",
      "Epoch: 66 Train loss 3.661, Test loss 3.658\n",
      "1 510\n",
      "Epoch: 67 Train loss 3.658, Test loss 3.655\n",
      "1 510\n",
      "Epoch: 68 Train loss 3.655, Test loss 3.651\n",
      "1 510\n",
      "Epoch: 69 Train loss 3.651, Test loss 3.648\n",
      "1 510\n",
      "Epoch: 70 Train loss 3.648, Test loss 3.645\n",
      "1 510\n",
      "Epoch: 71 Train loss 3.645, Test loss 3.642\n",
      "1 510\n",
      "Epoch: 72 Train loss 3.642, Test loss 3.639\n",
      "1 510\n",
      "Epoch: 73 Train loss 3.639, Test loss 3.636\n",
      "1 510\n",
      "Epoch: 74 Train loss 3.636, Test loss 3.633\n",
      "1 510\n",
      "Epoch: 75 Train loss 3.633, Test loss 3.630\n",
      "1 510\n",
      "Epoch: 76 Train loss 3.630, Test loss 3.627\n",
      "1 510\n",
      "Epoch: 77 Train loss 3.627, Test loss 3.624\n",
      "1 510\n",
      "Epoch: 78 Train loss 3.624, Test loss 3.621\n",
      "1 510\n",
      "Epoch: 79 Train loss 3.621, Test loss 3.618\n",
      "1 510\n",
      "Epoch: 80 Train loss 3.618, Test loss 3.615\n",
      "1 510\n",
      "Epoch: 81 Train loss 3.615, Test loss 3.612\n",
      "1 510\n",
      "Epoch: 82 Train loss 3.612, Test loss 3.610\n",
      "1 510\n",
      "Epoch: 83 Train loss 3.610, Test loss 3.607\n",
      "1 510\n",
      "Epoch: 84 Train loss 3.607, Test loss 3.604\n",
      "1 510\n",
      "Epoch: 85 Train loss 3.604, Test loss 3.601\n",
      "1 510\n",
      "Epoch: 86 Train loss 3.601, Test loss 3.599\n",
      "1 510\n",
      "Epoch: 87 Train loss 3.599, Test loss 3.596\n",
      "1 510\n",
      "Epoch: 88 Train loss 3.596, Test loss 3.593\n",
      "1 510\n",
      "Epoch: 89 Train loss 3.593, Test loss 3.591\n",
      "1 510\n",
      "Epoch: 90 Train loss 3.591, Test loss 3.588\n",
      "1 510\n",
      "Epoch: 91 Train loss 3.588, Test loss 3.586\n",
      "1 510\n",
      "Epoch: 92 Train loss 3.586, Test loss 3.583\n",
      "1 510\n",
      "Epoch: 93 Train loss 3.583, Test loss 3.581\n",
      "1 510\n",
      "Epoch: 94 Train loss 3.581, Test loss 3.578\n",
      "1 510\n",
      "Epoch: 95 Train loss 3.578, Test loss 3.576\n",
      "1 510\n",
      "Epoch: 96 Train loss 3.576, Test loss 3.573\n",
      "1 510\n",
      "Epoch: 97 Train loss 3.573, Test loss 3.571\n",
      "1 510\n",
      "Epoch: 98 Train loss 3.571, Test loss 3.568\n",
      "1 510\n",
      "Epoch: 99 Train loss 3.568, Test loss 3.566\n",
      "Mean train loss: 3.738 Mean val score: 3.734\n",
      "[[ 3.  0.  1.  0.  1. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1.  2.  0.\n",
      "  -1. -1.  0.  7.  6. -1.  0.  2.  4.  2. -1.  0.  1. -1.  0. -1. -1. -1.\n",
      "  -1. -1.  0. -1. -1. -1. -1. 13.  6.  2.]] [[0.11308126]] False\n",
      "Step 11: Reward 0.343.\n",
      "[[ 0.  0.  0.  0.  0. -1.  1.  0.  0.  0. -1. -1.  0.  0. -1. -1.  1.  0.\n",
      "   0. -1.  2.  9.  7.  0.  0.  2.  7.  0. -1. -1.  1. -1.  0. -1. -1. -1.\n",
      "  -1. -1. -1.  0. -1. -1.  1. 13.  6.  3.]] [[0.02962351]] False\n",
      "Step 12: Reward 0.275.\n",
      "[[ 0.  0.  0.  1.  0.  0. -1.  0.  0. -1.  1.  1.  0.  0. -1.  0.  1.  0.\n",
      "  -1.  0.  0.  3.  6.  0. -1.  1. 12.  1. -1. -1.  4.  0.  1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -2.  2. 13.  6.  4.]] [[-0.02041605]] False\n",
      "Step 13: Reward 0.233.\n",
      "[[ 1.  0.  0.  0.  0. -1.  2.  1.  0. -1. -1. -1.  0.  0. -1. -1.  1.  0.\n",
      "  -1.  6. -1.  3.  3.  3. -1.  2. 20.  0. -1.  0. -1.  2.  1.  0. -1.  0.\n",
      "  -1.  1. -1.  0.  0.  2.  3. 13.  6.  4.]] [[0.03493366]] False\n",
      "Step 14: Reward 0.308.\n",
      "[[ 1.  1.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1.  0. -1. -1.  0.\n",
      "  -1.  7. -1.  5.  3.  1. -1.  2. 26.  2. -1. -1. -1. -1.  1.  0. -1.  3.\n",
      "  -1.  2.  5.  0.  1.  3.  3. 13.  6.  5.]] [[0.06726679]] False\n",
      "Step 15: Reward 0.355.\n",
      "[[ 0.  3.  2. -1.  0. -1.  2.  1.  1.  2. -1.  2.  0. -1. -1.  0. -1.  0.\n",
      "  -1.  6.  1.  6.  4.  1.  0.  4. 29. -1.  0.  1. -1.  0.  1.  1. -1.  3.\n",
      "  -1.  2.  7.  0.  3.  8.  4. 13.  6.  6.]] [[0.08396864]] True\n",
      "Step 16: Reward 0.390.\n",
      "Trial: 2, reward: 2.2421172536025535.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1.  0.  0.  2. -1. -1.  0.\n",
      "  -1.  2.  2. -1. -1.  2.  3. -1. -1.  0. -1.  0.  0. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1.  0. -1. -1. -1. 25.  8.  0.]] [[-0.19762728]] False\n",
      "Step 17: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  1. -1. -1. -1.  0.  0.  5. -1. -1.  0.\n",
      "   2.  9.  2. -1. -1.  0.  2. -1. -1.  0. -1. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 25.  8.  1.]] [[-0.06298311]] False\n",
      "Step 18: Reward 0.159.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0.  4.  0.  0.  0.\n",
      "   2. 11.  6. -1. -1.  0.  0. -1. -1.  1.  2.  0. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -1. -1. -1. -1.  0. 25.  8.  2.]] [[0.07626984]] False\n",
      "Step 19: Reward 0.308.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1.  0.  0.  1.  3. -1.  1.  1.\n",
      "   0. 12.  4.  0.  3. -1.  2. -1.  3.  1.  5. -1. -1. -1. -1. -1. -1. -1.\n",
      "   1. -1. -1. -1.  2. -1.  1. 25.  8.  3.]] [[0.17465694]] False\n",
      "Step 20: Reward 0.421.\n",
      "1 520\n",
      "Epoch: 0 Train loss 3.665, Test loss 3.663\n",
      "1 520\n",
      "Epoch: 1 Train loss 3.663, Test loss 3.660\n",
      "1 520\n",
      "Epoch: 2 Train loss 3.660, Test loss 3.658\n",
      "1 520\n",
      "Epoch: 3 Train loss 3.658, Test loss 3.655\n",
      "1 520\n",
      "Epoch: 4 Train loss 3.655, Test loss 3.653\n",
      "1 520\n",
      "Epoch: 5 Train loss 3.653, Test loss 3.650\n",
      "1 520\n",
      "Epoch: 6 Train loss 3.650, Test loss 3.648\n",
      "1 520\n",
      "Epoch: 7 Train loss 3.648, Test loss 3.646\n",
      "1 520\n",
      "Epoch: 8 Train loss 3.646, Test loss 3.643\n",
      "1 520\n",
      "Epoch: 9 Train loss 3.643, Test loss 3.641\n",
      "1 520\n",
      "Epoch: 10 Train loss 3.641, Test loss 3.639\n",
      "1 520\n",
      "Epoch: 11 Train loss 3.639, Test loss 3.636\n",
      "1 520\n",
      "Epoch: 12 Train loss 3.636, Test loss 3.634\n",
      "1 520\n",
      "Epoch: 13 Train loss 3.634, Test loss 3.632\n",
      "1 520\n",
      "Epoch: 14 Train loss 3.632, Test loss 3.630\n",
      "1 520\n",
      "Epoch: 15 Train loss 3.630, Test loss 3.627\n",
      "1 520\n",
      "Epoch: 16 Train loss 3.627, Test loss 3.625\n",
      "1 520\n",
      "Epoch: 17 Train loss 3.625, Test loss 3.623\n",
      "1 520\n",
      "Epoch: 18 Train loss 3.623, Test loss 3.621\n",
      "1 520\n",
      "Epoch: 19 Train loss 3.621, Test loss 3.619\n",
      "1 520\n",
      "Epoch: 20 Train loss 3.619, Test loss 3.617\n",
      "1 520\n",
      "Epoch: 21 Train loss 3.617, Test loss 3.615\n",
      "1 520\n",
      "Epoch: 22 Train loss 3.615, Test loss 3.613\n",
      "1 520\n",
      "Epoch: 23 Train loss 3.613, Test loss 3.611\n",
      "1 520\n",
      "Epoch: 24 Train loss 3.611, Test loss 3.609\n",
      "1 520\n",
      "Epoch: 25 Train loss 3.609, Test loss 3.607\n",
      "1 520\n",
      "Epoch: 26 Train loss 3.607, Test loss 3.605\n",
      "1 520\n",
      "Epoch: 27 Train loss 3.605, Test loss 3.603\n",
      "1 520\n",
      "Epoch: 28 Train loss 3.603, Test loss 3.601\n",
      "1 520\n",
      "Epoch: 29 Train loss 3.601, Test loss 3.599\n",
      "1 520\n",
      "Epoch: 30 Train loss 3.599, Test loss 3.597\n",
      "1 520\n",
      "Epoch: 31 Train loss 3.597, Test loss 3.595\n",
      "1 520\n",
      "Epoch: 32 Train loss 3.595, Test loss 3.593\n",
      "1 520\n",
      "Epoch: 33 Train loss 3.593, Test loss 3.591\n",
      "1 520\n",
      "Epoch: 34 Train loss 3.591, Test loss 3.590\n",
      "1 520\n",
      "Epoch: 35 Train loss 3.590, Test loss 3.588\n",
      "1 520\n",
      "Epoch: 36 Train loss 3.588, Test loss 3.586\n",
      "1 520\n",
      "Epoch: 37 Train loss 3.586, Test loss 3.584\n",
      "1 520\n",
      "Epoch: 38 Train loss 3.584, Test loss 3.582\n",
      "1 520\n",
      "Epoch: 39 Train loss 3.582, Test loss 3.581\n",
      "1 520\n",
      "Epoch: 40 Train loss 3.581, Test loss 3.579\n",
      "1 520\n",
      "Epoch: 41 Train loss 3.579, Test loss 3.577\n",
      "1 520\n",
      "Epoch: 42 Train loss 3.577, Test loss 3.576\n",
      "1 520\n",
      "Epoch: 43 Train loss 3.576, Test loss 3.574\n",
      "1 520\n",
      "Epoch: 44 Train loss 3.574, Test loss 3.572\n",
      "1 520\n",
      "Epoch: 45 Train loss 3.572, Test loss 3.571\n",
      "1 520\n",
      "Epoch: 46 Train loss 3.571, Test loss 3.569\n",
      "1 520\n",
      "Epoch: 47 Train loss 3.569, Test loss 3.567\n",
      "1 520\n",
      "Epoch: 48 Train loss 3.567, Test loss 3.566\n",
      "1 520\n",
      "Epoch: 49 Train loss 3.566, Test loss 3.564\n",
      "1 520\n",
      "Epoch: 50 Train loss 3.564, Test loss 3.562\n",
      "1 520\n",
      "Epoch: 51 Train loss 3.562, Test loss 3.561\n",
      "1 520\n",
      "Epoch: 52 Train loss 3.561, Test loss 3.559\n",
      "1 520\n",
      "Epoch: 53 Train loss 3.559, Test loss 3.558\n",
      "1 520\n",
      "Epoch: 54 Train loss 3.558, Test loss 3.556\n",
      "1 520\n",
      "Epoch: 55 Train loss 3.556, Test loss 3.555\n",
      "1 520\n",
      "Epoch: 56 Train loss 3.555, Test loss 3.553\n",
      "1 520\n",
      "Epoch: 57 Train loss 3.553, Test loss 3.552\n",
      "1 520\n",
      "Epoch: 58 Train loss 3.552, Test loss 3.550\n",
      "1 520\n",
      "Epoch: 59 Train loss 3.550, Test loss 3.549\n",
      "1 520\n",
      "Epoch: 60 Train loss 3.549, Test loss 3.547\n",
      "1 520\n",
      "Epoch: 61 Train loss 3.547, Test loss 3.546\n",
      "1 520\n",
      "Epoch: 62 Train loss 3.546, Test loss 3.545\n",
      "1 520\n",
      "Epoch: 63 Train loss 3.545, Test loss 3.543\n",
      "1 520\n",
      "Epoch: 64 Train loss 3.543, Test loss 3.542\n",
      "1 520\n",
      "Epoch: 65 Train loss 3.542, Test loss 3.540\n",
      "1 520\n",
      "Epoch: 66 Train loss 3.540, Test loss 3.539\n",
      "1 520\n",
      "Epoch: 67 Train loss 3.539, Test loss 3.538\n",
      "1 520\n",
      "Epoch: 68 Train loss 3.538, Test loss 3.536\n",
      "1 520\n",
      "Epoch: 69 Train loss 3.536, Test loss 3.535\n",
      "1 520\n",
      "Epoch: 70 Train loss 3.535, Test loss 3.534\n",
      "1 520\n",
      "Epoch: 71 Train loss 3.534, Test loss 3.532\n",
      "1 520\n",
      "Epoch: 72 Train loss 3.532, Test loss 3.531\n",
      "1 520\n",
      "Epoch: 73 Train loss 3.531, Test loss 3.530\n",
      "1 520\n",
      "Epoch: 74 Train loss 3.530, Test loss 3.528\n",
      "1 520\n",
      "Epoch: 75 Train loss 3.528, Test loss 3.527\n",
      "1 520\n",
      "Epoch: 76 Train loss 3.527, Test loss 3.526\n",
      "1 520\n",
      "Epoch: 77 Train loss 3.526, Test loss 3.525\n",
      "1 520\n",
      "Epoch: 78 Train loss 3.525, Test loss 3.523\n",
      "1 520\n",
      "Epoch: 79 Train loss 3.523, Test loss 3.522\n",
      "1 520\n",
      "Epoch: 80 Train loss 3.522, Test loss 3.521\n",
      "1 520\n",
      "Epoch: 81 Train loss 3.521, Test loss 3.520\n",
      "1 520\n",
      "Epoch: 82 Train loss 3.520, Test loss 3.518\n",
      "1 520\n",
      "Epoch: 83 Train loss 3.518, Test loss 3.517\n",
      "1 520\n",
      "Epoch: 84 Train loss 3.517, Test loss 3.516\n",
      "1 520\n",
      "Epoch: 85 Train loss 3.516, Test loss 3.515\n",
      "1 520\n",
      "Epoch: 86 Train loss 3.515, Test loss 3.514\n",
      "1 520\n",
      "Epoch: 87 Train loss 3.514, Test loss 3.513\n",
      "1 520\n",
      "Epoch: 88 Train loss 3.513, Test loss 3.511\n",
      "1 520\n",
      "Epoch: 89 Train loss 3.511, Test loss 3.510\n",
      "1 520\n",
      "Epoch: 90 Train loss 3.510, Test loss 3.509\n",
      "1 520\n",
      "Epoch: 91 Train loss 3.509, Test loss 3.508\n",
      "1 520\n",
      "Epoch: 92 Train loss 3.508, Test loss 3.507\n",
      "1 520\n",
      "Epoch: 93 Train loss 3.507, Test loss 3.506\n",
      "1 520\n",
      "Epoch: 94 Train loss 3.506, Test loss 3.505\n",
      "1 520\n",
      "Epoch: 95 Train loss 3.505, Test loss 3.504\n",
      "1 520\n",
      "Epoch: 96 Train loss 3.504, Test loss 3.502\n",
      "1 520\n",
      "Epoch: 97 Train loss 3.502, Test loss 3.501\n",
      "1 520\n",
      "Epoch: 98 Train loss 3.501, Test loss 3.500\n",
      "1 520\n",
      "Epoch: 99 Train loss 3.500, Test loss 3.499\n",
      "Mean train loss: 3.571 Mean val score: 3.569\n",
      "[[ 0.  1. -1.  0.  0.  0. -1.  0. -1. -1.  0.  0.  0. -1.  3. -1.  2. -1.\n",
      "   0. 16.  7. -1.  2. -1. -1. -1.  1. -1.  3.  1. -1. -1. -1. -1. -1.  1.\n",
      "  -2.  0. -2. -1.  3. -2.  0. 25.  8.  4.]] [[0.17458266]] False\n",
      "Step 21: Reward 0.472.\n",
      "[[ 0.  1.  0. -1.  0. -1.  0.  1. -1.  0.  0.  0.  0.  0.  3. -1.  7.  0.\n",
      "  -1. 17.  8.  0.  3.  0. -1.  0. 10. -1.  0.  3. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1.  3. -2.  0. 25.  8.  4.]] [[0.1005196]] False\n",
      "Step 22: Reward 0.406.\n",
      "[[ 4.  3.  1.  0.  0.  0.  4.  0. -1.  4.  0.  2.  0.  0.  5. -1.  6. -1.\n",
      "   1. 14.  7.  2.  6. -1. -1.  7.  9. -1. -2.  3. -1.  3. -2. -1.  1.  1.\n",
      "  -1.  0. -2. -1.  3. -2.  0. 25.  8.  5.]] [[0.05272466]] False\n",
      "Step 23: Reward 0.387.\n",
      "[[ 3.  2.  4.  3.  0.  0.  5.  0. -1.  5.  0.  5.  4.  0.  4. -1.  6. -1.\n",
      "   0. 14.  7.  2.  8. -1. -1.  7.  8.  1.  2.  0. -1.  2. -2.  0.  0.  1.\n",
      "  -2. -1. -2. -1.  2. -2.  0. 25.  8.  6.]] [[0.3228656]] True\n",
      "Step 24: Reward 0.680.\n",
      "Trial: 3, reward: 2.8337320179961254.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  2. -1.  1.  6. -1.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 19.  8.  0.]] [[-0.07534947]] False\n",
      "Step 25: Reward 0.143.\n",
      "[[ 0.  0.  1.  0.  0. -1. -1.  0.  2. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  5. -1. -1.  6.  1.  1. -1.  2.  3. -1.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 19.  8.  1.]] [[0.05668372]] False\n",
      "Step 26: Reward 0.295.\n",
      "[[ 0.  0.  0.  0.  0. -1.  0.  0.  0.  3. -1. -1.  0.  1. -1. -1.  0.  1.\n",
      "  -1.  8. -1. -1.  7.  3. -1. -1.  1.  2. -1.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -1. -1.  0. -2. -1. 19.  8.  2.]] [[0.0265069]] False\n",
      "Step 27: Reward 0.291.\n",
      "[[ 3.  0. -1.  0.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1.  2. -1. -1.  1.\n",
      "  -1. 10. -1. -1.  6.  0. -1.  6.  1. -1.  2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  2. -2.  1. 19.  8.  3.]] [[0.17395064]] False\n",
      "Step 28: Reward 0.464.\n",
      "[[ 0.  0. -1. -1.  0.  0. -1.  1. -1. -1. -1.  0.  0. -1.  3. -1.  0.  2.\n",
      "   0. 11.  0. -1.  9. -1.  0.  9.  6. -1. -2. -1.  0. -1. -1. -1.  1. -1.\n",
      "  -1. -1. -2.  1.  2. -2. -1. 19.  8.  4.]] [[0.09478664]] False\n",
      "Step 29: Reward 0.376.\n",
      "[[-1.  2.  0. -1.  0.  0. -1.  0.  0. -1.  1.  3.  0. -1.  3. -1.  2. -1.\n",
      "   1. 10.  0.  4. 10. -1. -1.  9.  6. -1.  3.  3. -1. -1. -2. -1. -1. -1.\n",
      "   2. -1. -2.  1.  2. -2.  0. 19.  8.  4.]] [[0.11809206]] False\n",
      "Step 30: Reward 0.424.\n",
      "1 530\n",
      "Epoch: 0 Train loss 3.600, Test loss 3.599\n",
      "1 530\n",
      "Epoch: 1 Train loss 3.599, Test loss 3.597\n",
      "1 530\n",
      "Epoch: 2 Train loss 3.597, Test loss 3.596\n",
      "1 530\n",
      "Epoch: 3 Train loss 3.596, Test loss 3.595\n",
      "1 530\n",
      "Epoch: 4 Train loss 3.595, Test loss 3.594\n",
      "1 530\n",
      "Epoch: 5 Train loss 3.594, Test loss 3.592\n",
      "1 530\n",
      "Epoch: 6 Train loss 3.592, Test loss 3.591\n",
      "1 530\n",
      "Epoch: 7 Train loss 3.591, Test loss 3.590\n",
      "1 530\n",
      "Epoch: 8 Train loss 3.590, Test loss 3.589\n",
      "1 530\n",
      "Epoch: 9 Train loss 3.589, Test loss 3.587\n",
      "1 530\n",
      "Epoch: 10 Train loss 3.587, Test loss 3.586\n",
      "1 530\n",
      "Epoch: 11 Train loss 3.586, Test loss 3.585\n",
      "1 530\n",
      "Epoch: 12 Train loss 3.585, Test loss 3.584\n",
      "1 530\n",
      "Epoch: 13 Train loss 3.584, Test loss 3.582\n",
      "1 530\n",
      "Epoch: 14 Train loss 3.582, Test loss 3.581\n",
      "1 530\n",
      "Epoch: 15 Train loss 3.581, Test loss 3.580\n",
      "1 530\n",
      "Epoch: 16 Train loss 3.580, Test loss 3.579\n",
      "1 530\n",
      "Epoch: 17 Train loss 3.579, Test loss 3.578\n",
      "1 530\n",
      "Epoch: 18 Train loss 3.578, Test loss 3.577\n",
      "1 530\n",
      "Epoch: 19 Train loss 3.577, Test loss 3.576\n",
      "1 530\n",
      "Epoch: 20 Train loss 3.576, Test loss 3.574\n",
      "1 530\n",
      "Epoch: 21 Train loss 3.574, Test loss 3.573\n",
      "1 530\n",
      "Epoch: 22 Train loss 3.573, Test loss 3.572\n",
      "1 530\n",
      "Epoch: 23 Train loss 3.572, Test loss 3.571\n",
      "1 530\n",
      "Epoch: 24 Train loss 3.571, Test loss 3.570\n",
      "1 530\n",
      "Epoch: 25 Train loss 3.570, Test loss 3.569\n",
      "1 530\n",
      "Epoch: 26 Train loss 3.569, Test loss 3.568\n",
      "1 530\n",
      "Epoch: 27 Train loss 3.568, Test loss 3.567\n",
      "1 530\n",
      "Epoch: 28 Train loss 3.567, Test loss 3.566\n",
      "1 530\n",
      "Epoch: 29 Train loss 3.566, Test loss 3.565\n",
      "1 530\n",
      "Epoch: 30 Train loss 3.565, Test loss 3.564\n",
      "1 530\n",
      "Epoch: 31 Train loss 3.564, Test loss 3.563\n",
      "1 530\n",
      "Epoch: 32 Train loss 3.563, Test loss 3.562\n",
      "1 530\n",
      "Epoch: 33 Train loss 3.562, Test loss 3.561\n",
      "1 530\n",
      "Epoch: 34 Train loss 3.561, Test loss 3.559\n",
      "1 530\n",
      "Epoch: 35 Train loss 3.559, Test loss 3.558\n",
      "1 530\n",
      "Epoch: 36 Train loss 3.558, Test loss 3.557\n",
      "1 530\n",
      "Epoch: 37 Train loss 3.557, Test loss 3.556\n",
      "1 530\n",
      "Epoch: 38 Train loss 3.556, Test loss 3.556\n",
      "1 530\n",
      "Epoch: 39 Train loss 3.556, Test loss 3.555\n",
      "1 530\n",
      "Epoch: 40 Train loss 3.555, Test loss 3.554\n",
      "1 530\n",
      "Epoch: 41 Train loss 3.554, Test loss 3.553\n",
      "1 530\n",
      "Epoch: 42 Train loss 3.553, Test loss 3.552\n",
      "1 530\n",
      "Epoch: 43 Train loss 3.552, Test loss 3.551\n",
      "1 530\n",
      "Epoch: 44 Train loss 3.551, Test loss 3.550\n",
      "1 530\n",
      "Epoch: 45 Train loss 3.550, Test loss 3.549\n",
      "1 530\n",
      "Epoch: 46 Train loss 3.549, Test loss 3.548\n",
      "1 530\n",
      "Epoch: 47 Train loss 3.548, Test loss 3.547\n",
      "1 530\n",
      "Epoch: 48 Train loss 3.547, Test loss 3.546\n",
      "1 530\n",
      "Epoch: 49 Train loss 3.546, Test loss 3.545\n",
      "1 530\n",
      "Epoch: 50 Train loss 3.545, Test loss 3.544\n",
      "1 530\n",
      "Epoch: 51 Train loss 3.544, Test loss 3.543\n",
      "1 530\n",
      "Epoch: 52 Train loss 3.543, Test loss 3.542\n",
      "1 530\n",
      "Epoch: 53 Train loss 3.542, Test loss 3.542\n",
      "1 530\n",
      "Epoch: 54 Train loss 3.542, Test loss 3.541\n",
      "1 530\n",
      "Epoch: 55 Train loss 3.541, Test loss 3.540\n",
      "1 530\n",
      "Epoch: 56 Train loss 3.540, Test loss 3.539\n",
      "1 530\n",
      "Epoch: 57 Train loss 3.539, Test loss 3.538\n",
      "1 530\n",
      "Epoch: 58 Train loss 3.538, Test loss 3.537\n",
      "1 530\n",
      "Epoch: 59 Train loss 3.537, Test loss 3.536\n",
      "1 530\n",
      "Epoch: 60 Train loss 3.536, Test loss 3.535\n",
      "1 530\n",
      "Epoch: 61 Train loss 3.535, Test loss 3.535\n",
      "1 530\n",
      "Epoch: 62 Train loss 3.535, Test loss 3.534\n",
      "1 530\n",
      "Epoch: 63 Train loss 3.534, Test loss 3.533\n",
      "1 530\n",
      "Epoch: 64 Train loss 3.533, Test loss 3.532\n",
      "1 530\n",
      "Epoch: 65 Train loss 3.532, Test loss 3.531\n",
      "1 530\n",
      "Epoch: 66 Train loss 3.531, Test loss 3.530\n",
      "1 530\n",
      "Epoch: 67 Train loss 3.530, Test loss 3.530\n",
      "1 530\n",
      "Epoch: 68 Train loss 3.530, Test loss 3.529\n",
      "1 530\n",
      "Epoch: 69 Train loss 3.529, Test loss 3.528\n",
      "1 530\n",
      "Epoch: 70 Train loss 3.528, Test loss 3.527\n",
      "1 530\n",
      "Epoch: 71 Train loss 3.527, Test loss 3.526\n",
      "1 530\n",
      "Epoch: 72 Train loss 3.526, Test loss 3.526\n",
      "1 530\n",
      "Epoch: 73 Train loss 3.526, Test loss 3.525\n",
      "1 530\n",
      "Epoch: 74 Train loss 3.525, Test loss 3.524\n",
      "1 530\n",
      "Epoch: 75 Train loss 3.524, Test loss 3.523\n",
      "1 530\n",
      "Epoch: 76 Train loss 3.523, Test loss 3.523\n",
      "1 530\n",
      "Epoch: 77 Train loss 3.523, Test loss 3.522\n",
      "1 530\n",
      "Epoch: 78 Train loss 3.522, Test loss 3.521\n",
      "1 530\n",
      "Epoch: 79 Train loss 3.521, Test loss 3.520\n",
      "1 530\n",
      "Epoch: 80 Train loss 3.520, Test loss 3.520\n",
      "1 530\n",
      "Epoch: 81 Train loss 3.520, Test loss 3.519\n",
      "1 530\n",
      "Epoch: 82 Train loss 3.519, Test loss 3.518\n",
      "1 530\n",
      "Epoch: 83 Train loss 3.518, Test loss 3.517\n",
      "1 530\n",
      "Epoch: 84 Train loss 3.517, Test loss 3.517\n",
      "1 530\n",
      "Epoch: 85 Train loss 3.517, Test loss 3.516\n",
      "1 530\n",
      "Epoch: 86 Train loss 3.516, Test loss 3.515\n",
      "1 530\n",
      "Epoch: 87 Train loss 3.515, Test loss 3.514\n",
      "1 530\n",
      "Epoch: 88 Train loss 3.514, Test loss 3.514\n",
      "1 530\n",
      "Epoch: 89 Train loss 3.514, Test loss 3.513\n",
      "1 530\n",
      "Epoch: 90 Train loss 3.513, Test loss 3.512\n",
      "1 530\n",
      "Epoch: 91 Train loss 3.512, Test loss 3.512\n",
      "1 530\n",
      "Epoch: 92 Train loss 3.512, Test loss 3.511\n",
      "1 530\n",
      "Epoch: 93 Train loss 3.511, Test loss 3.510\n",
      "1 530\n",
      "Epoch: 94 Train loss 3.510, Test loss 3.510\n",
      "1 530\n",
      "Epoch: 95 Train loss 3.510, Test loss 3.509\n",
      "1 530\n",
      "Epoch: 96 Train loss 3.509, Test loss 3.508\n",
      "1 530\n",
      "Epoch: 97 Train loss 3.508, Test loss 3.507\n",
      "1 530\n",
      "Epoch: 98 Train loss 3.507, Test loss 3.507\n",
      "1 530\n",
      "Epoch: 99 Train loss 3.507, Test loss 3.506\n",
      "Mean train loss: 3.548 Mean val score: 3.547\n",
      "[[-1.  0. -1. -1.  1. -1.  0.  0.  1.  1.  0.  5.  0. -1.  5. -1.  2. -1.\n",
      "   0.  9.  0.  5. 10. -1. -1.  8. 11.  3. -1. -1. -1. -1. -2.  1. -1. -1.\n",
      "   2.  1. -2.  2.  1. -2.  0. 19.  8.  5.]] [[0.08555412]] False\n",
      "Step 31: Reward 0.441.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1. -1.  1.  2.  7.  4.  0. -1.  1.  1.  2. -1.\n",
      "   4.  9. -1.  4. 11.  1. -1. 16. 12.  6. -1.  2. -1. -2. -2.  1.  0.  1.\n",
      "   0.  1. -2.  2.  2. -3.  0. 19.  8.  6.]] [[0.05395183]] True\n",
      "Step 32: Reward 0.438.\n",
      "Trial: 4, reward: 2.871645481987997.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  3.\n",
      "   2. -1.  7. -1. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 10.  5.  0.]] [[-0.10956194]] False\n",
      "Step 33: Reward 0.115.\n",
      "[[ 0.  0.  0.  0.  1. -1. -1.  1.  1. -1. -1. -1.  0.  0. -1. -1.  1.  0.\n",
      "   2. -1.  5.  2. -1.  5. -1. -1.  1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 10.  5.  1.]] [[0.09808728]] False\n",
      "Step 34: Reward 0.347.\n",
      "[[ 0.  2.  0.  0.  0. -1. -1.  0.  1.  0.  1. -1.  0. -1. -1.  3.  6.  0.\n",
      "   2. -1.  4.  0. -2.  8. -1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  3. -2. -1. 10.  5.  2.]] [[-0.04156831]] False\n",
      "Step 35: Reward 0.229.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1.  3.  5.  5. -1.\n",
      "   2. -2.  3. -1. -2.  8.  0. -2.  3. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  4. -1.  1. 10.  5.  3.]] [[0.08286816]] False\n",
      "Step 36: Reward 0.378.\n",
      "[[-1.  5.  0. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0.  7.  5. -1.\n",
      "   2.  1.  3.  2.  3. 12. -1.  2.  3. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1.  1.  3. -1.  2. 10.  5.  4.]] [[-0.02668238]] False\n",
      "Step 37: Reward 0.287.\n",
      "[[ 0. -1.  2.  0.  0.  1. -1. -1. -1.  0.  2.  2.  0. -1. -1.  5.  5. -1.\n",
      "   2. -1.  4.  2.  7. 12. -1.  3.  7. -1. -2. -1.  0. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2.  1.  3.  0.  3. 10.  5.  4.]] [[-0.05448455]] False\n",
      "Step 38: Reward 0.282.\n",
      "[[ 0.  1.  0.  1.  1.  0. -1. -1.  0. -1.  2.  1.  0. -1.  0.  5.  5.  1.\n",
      "   5. -2.  4.  4.  4. 10. -1.  3.  9.  0. -2.  0.  3. -1. -2. -1. -1. -1.\n",
      "  -1.  1. -1.  2.  2.  1.  3. 10.  5.  5.]] [[0.22745731]] False\n",
      "Step 39: Reward 0.582.\n",
      "[[-1. -1.  1. -1.  0.  1.  0. -1.  0.  0.  5.  1. -1. -1.  1.  5.  5.  6.\n",
      "   7. -2.  4.  3.  8. 14. -1.  3. 14. -1.  3. -1.  3. -1. -2. -2. -1. -1.\n",
      "  -1.  0. -1.  2.  2.  1.  3. 10.  5.  6.]] [[0.16907778]] True\n",
      "Step 40: Reward 0.543.\n",
      "Trial: 5, reward: 2.7639626224163254.\n",
      "1 540\n",
      "Epoch: 0 Train loss 3.617, Test loss 3.616\n",
      "1 540\n",
      "Epoch: 1 Train loss 3.616, Test loss 3.615\n",
      "1 540\n",
      "Epoch: 2 Train loss 3.615, Test loss 3.614\n",
      "1 540\n",
      "Epoch: 3 Train loss 3.614, Test loss 3.613\n",
      "1 540\n",
      "Epoch: 4 Train loss 3.613, Test loss 3.612\n",
      "1 540\n",
      "Epoch: 5 Train loss 3.612, Test loss 3.611\n",
      "1 540\n",
      "Epoch: 6 Train loss 3.611, Test loss 3.610\n",
      "1 540\n",
      "Epoch: 7 Train loss 3.610, Test loss 3.609\n",
      "1 540\n",
      "Epoch: 8 Train loss 3.609, Test loss 3.608\n",
      "1 540\n",
      "Epoch: 9 Train loss 3.608, Test loss 3.608\n",
      "1 540\n",
      "Epoch: 10 Train loss 3.608, Test loss 3.607\n",
      "1 540\n",
      "Epoch: 11 Train loss 3.607, Test loss 3.606\n",
      "1 540\n",
      "Epoch: 12 Train loss 3.606, Test loss 3.605\n",
      "1 540\n",
      "Epoch: 13 Train loss 3.605, Test loss 3.604\n",
      "1 540\n",
      "Epoch: 14 Train loss 3.604, Test loss 3.603\n",
      "1 540\n",
      "Epoch: 15 Train loss 3.603, Test loss 3.602\n",
      "1 540\n",
      "Epoch: 16 Train loss 3.602, Test loss 3.602\n",
      "1 540\n",
      "Epoch: 17 Train loss 3.602, Test loss 3.601\n",
      "1 540\n",
      "Epoch: 18 Train loss 3.601, Test loss 3.600\n",
      "1 540\n",
      "Epoch: 19 Train loss 3.600, Test loss 3.599\n",
      "1 540\n",
      "Epoch: 20 Train loss 3.599, Test loss 3.598\n",
      "1 540\n",
      "Epoch: 21 Train loss 3.598, Test loss 3.597\n",
      "1 540\n",
      "Epoch: 22 Train loss 3.597, Test loss 3.597\n",
      "1 540\n",
      "Epoch: 23 Train loss 3.597, Test loss 3.596\n",
      "1 540\n",
      "Epoch: 24 Train loss 3.596, Test loss 3.595\n",
      "1 540\n",
      "Epoch: 25 Train loss 3.595, Test loss 3.594\n",
      "1 540\n",
      "Epoch: 26 Train loss 3.594, Test loss 3.593\n",
      "1 540\n",
      "Epoch: 27 Train loss 3.593, Test loss 3.593\n",
      "1 540\n",
      "Epoch: 28 Train loss 3.593, Test loss 3.592\n",
      "1 540\n",
      "Epoch: 29 Train loss 3.592, Test loss 3.591\n",
      "1 540\n",
      "Epoch: 30 Train loss 3.591, Test loss 3.590\n",
      "1 540\n",
      "Epoch: 31 Train loss 3.590, Test loss 3.590\n",
      "1 540\n",
      "Epoch: 32 Train loss 3.590, Test loss 3.589\n",
      "1 540\n",
      "Epoch: 33 Train loss 3.589, Test loss 3.588\n",
      "1 540\n",
      "Epoch: 34 Train loss 3.588, Test loss 3.587\n",
      "1 540\n",
      "Epoch: 35 Train loss 3.587, Test loss 3.586\n",
      "1 540\n",
      "Epoch: 36 Train loss 3.586, Test loss 3.586\n",
      "1 540\n",
      "Epoch: 37 Train loss 3.586, Test loss 3.585\n",
      "1 540\n",
      "Epoch: 38 Train loss 3.585, Test loss 3.584\n",
      "1 540\n",
      "Epoch: 39 Train loss 3.584, Test loss 3.584\n",
      "1 540\n",
      "Epoch: 40 Train loss 3.584, Test loss 3.583\n",
      "1 540\n",
      "Epoch: 41 Train loss 3.583, Test loss 3.582\n",
      "1 540\n",
      "Epoch: 42 Train loss 3.582, Test loss 3.581\n",
      "1 540\n",
      "Epoch: 43 Train loss 3.581, Test loss 3.581\n",
      "1 540\n",
      "Epoch: 44 Train loss 3.581, Test loss 3.580\n",
      "1 540\n",
      "Epoch: 45 Train loss 3.580, Test loss 3.579\n",
      "1 540\n",
      "Epoch: 46 Train loss 3.579, Test loss 3.578\n",
      "1 540\n",
      "Epoch: 47 Train loss 3.578, Test loss 3.578\n",
      "1 540\n",
      "Epoch: 48 Train loss 3.578, Test loss 3.577\n",
      "1 540\n",
      "Epoch: 49 Train loss 3.577, Test loss 3.576\n",
      "1 540\n",
      "Epoch: 50 Train loss 3.576, Test loss 3.576\n",
      "1 540\n",
      "Epoch: 51 Train loss 3.576, Test loss 3.575\n",
      "1 540\n",
      "Epoch: 52 Train loss 3.575, Test loss 3.574\n",
      "1 540\n",
      "Epoch: 53 Train loss 3.574, Test loss 3.574\n",
      "1 540\n",
      "Epoch: 54 Train loss 3.574, Test loss 3.573\n",
      "1 540\n",
      "Epoch: 55 Train loss 3.573, Test loss 3.572\n",
      "1 540\n",
      "Epoch: 56 Train loss 3.572, Test loss 3.572\n",
      "1 540\n",
      "Epoch: 57 Train loss 3.572, Test loss 3.571\n",
      "1 540\n",
      "Epoch: 58 Train loss 3.571, Test loss 3.570\n",
      "1 540\n",
      "Epoch: 59 Train loss 3.570, Test loss 3.570\n",
      "1 540\n",
      "Epoch: 60 Train loss 3.570, Test loss 3.569\n",
      "1 540\n",
      "Epoch: 61 Train loss 3.569, Test loss 3.568\n",
      "1 540\n",
      "Epoch: 62 Train loss 3.568, Test loss 3.568\n",
      "1 540\n",
      "Epoch: 63 Train loss 3.568, Test loss 3.567\n",
      "1 540\n",
      "Epoch: 64 Train loss 3.567, Test loss 3.566\n",
      "1 540\n",
      "Epoch: 65 Train loss 3.566, Test loss 3.566\n",
      "1 540\n",
      "Epoch: 66 Train loss 3.566, Test loss 3.565\n",
      "1 540\n",
      "Epoch: 67 Train loss 3.565, Test loss 3.564\n",
      "1 540\n",
      "Epoch: 68 Train loss 3.564, Test loss 3.564\n",
      "1 540\n",
      "Epoch: 69 Train loss 3.564, Test loss 3.563\n",
      "1 540\n",
      "Epoch: 70 Train loss 3.563, Test loss 3.563\n",
      "1 540\n",
      "Epoch: 71 Train loss 3.563, Test loss 3.562\n",
      "1 540\n",
      "Epoch: 72 Train loss 3.562, Test loss 3.561\n",
      "1 540\n",
      "Epoch: 73 Train loss 3.561, Test loss 3.561\n",
      "1 540\n",
      "Epoch: 74 Train loss 3.561, Test loss 3.560\n",
      "1 540\n",
      "Epoch: 75 Train loss 3.560, Test loss 3.559\n",
      "1 540\n",
      "Epoch: 76 Train loss 3.559, Test loss 3.559\n",
      "1 540\n",
      "Epoch: 77 Train loss 3.559, Test loss 3.558\n",
      "1 540\n",
      "Epoch: 78 Train loss 3.558, Test loss 3.558\n",
      "1 540\n",
      "Epoch: 79 Train loss 3.558, Test loss 3.557\n",
      "1 540\n",
      "Epoch: 80 Train loss 3.557, Test loss 3.556\n",
      "1 540\n",
      "Epoch: 81 Train loss 3.556, Test loss 3.556\n",
      "1 540\n",
      "Epoch: 82 Train loss 3.556, Test loss 3.555\n",
      "1 540\n",
      "Epoch: 83 Train loss 3.555, Test loss 3.555\n",
      "1 540\n",
      "Epoch: 84 Train loss 3.555, Test loss 3.554\n",
      "1 540\n",
      "Epoch: 85 Train loss 3.554, Test loss 3.553\n",
      "1 540\n",
      "Epoch: 86 Train loss 3.553, Test loss 3.553\n",
      "1 540\n",
      "Epoch: 87 Train loss 3.553, Test loss 3.552\n",
      "1 540\n",
      "Epoch: 88 Train loss 3.552, Test loss 3.552\n",
      "1 540\n",
      "Epoch: 89 Train loss 3.552, Test loss 3.551\n",
      "1 540\n",
      "Epoch: 90 Train loss 3.551, Test loss 3.550\n",
      "1 540\n",
      "Epoch: 91 Train loss 3.550, Test loss 3.550\n",
      "1 540\n",
      "Epoch: 92 Train loss 3.550, Test loss 3.549\n",
      "1 540\n",
      "Epoch: 93 Train loss 3.549, Test loss 3.549\n",
      "1 540\n",
      "Epoch: 94 Train loss 3.549, Test loss 3.548\n",
      "1 540\n",
      "Epoch: 95 Train loss 3.548, Test loss 3.548\n",
      "1 540\n",
      "Epoch: 96 Train loss 3.548, Test loss 3.547\n",
      "1 540\n",
      "Epoch: 97 Train loss 3.547, Test loss 3.546\n",
      "1 540\n",
      "Epoch: 98 Train loss 3.546, Test loss 3.546\n",
      "1 540\n",
      "Epoch: 99 Train loss 3.546, Test loss 3.545\n",
      "Mean train loss: 3.578 Mean val score: 3.578\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  2. -1.  5.  2. -1.  3. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 22.  5.  0.]] [[0.01581305]] False\n",
      "Step 41: Reward 0.250.\n",
      "[[ 1.  0.  0.  0.  0. -1.  0.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  8. -1.  4.  0. -1.  4. -1.  1.  0.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 22.  5.  1.]] [[-0.02309741]] False\n",
      "Step 42: Reward 0.240.\n",
      "[[ 1.  0. -1. -1.  0. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  2.  1.\n",
      "   0.  6. -1.  4.  4. -1.  0.  1.  5. -1.  2.  0. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2.  0. 22.  5.  2.]] [[0.04234472]] False\n",
      "Step 43: Reward 0.333.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1.  0.  0. -1. -1.  1.  3.  0.\n",
      "   0. 10. -2.  9.  5. -1. -1.  1.  9. -1.  2.  2. -1. -1. -1. -1.  0. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0. 22.  5.  3.]] [[-0.00920039]] False\n",
      "Step 44: Reward 0.292.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  1.  3. -1.  0. -1.  0.  1.  4. -1.\n",
      "   0.  8. -2.  9.  5. -1. -1.  9.  9.  0.  7.  0. -1. -1. -2. -1.  0. -1.\n",
      "  -2.  0. -2.  0. -1. -2.  1. 22.  5.  4.]] [[-0.01375964]] False\n",
      "Step 45: Reward 0.320.\n",
      "[[-1.  1. -1. -1.  0.  0.  1. -1. -1. -1.  2. -2.  0. -1.  0.  1.  6. -1.\n",
      "   3.  8. -1.  9.  2.  5. -1.  8. 13. -1.  9. -1. -1. -1. -2. -1.  0. -1.\n",
      "  -2. -1. -2.  0. -1. -1.  2. 22.  5.  4.]] [[-0.14735793]] False\n",
      "Step 46: Reward 0.206.\n",
      "[[ 1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  4. -2. -1.  1.  2. -1.  6.  0.\n",
      "   2.  9.  1. 10. -1.  5. -1.  8. 15.  4.  7. -1. -1. -2. -2. -2. -1. -1.\n",
      "  -2. -2. -1.  0. -2. -2.  4. 22.  5.  5.]] [[0.06681418]] False\n",
      "Step 47: Reward 0.446.\n",
      "[[ 0. -1.  0.  1.  0.  0.  2. -1.  1.  2.  1. -1. -1.  1.  0. -1.  6. -1.\n",
      "   7.  9.  1. 14.  0.  3. -1.  8. 17.  7. 14. -1. -1. -2. -2. -1. -1. -2.\n",
      "  -3. -2. -1.  0. -2. -2.  4. 22.  5.  6.]] [[0.04371512]] True\n",
      "Step 48: Reward 0.438.\n",
      "Trial: 6, reward: 2.524415316704233.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  5. -1. -1.  2.  4. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 16.  3.  0.]] [[0.26195633]] False\n",
      "Step 49: Reward 0.500.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0.  2. -1. -1.  0.\n",
      "  -1. -1.  7. -1.  6.  5.  4. -1. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 16.  3.  1.]] [[0.16432008]] False\n",
      "Step 50: Reward 0.429.\n",
      "1 550\n",
      "Epoch: 0 Train loss 3.664, Test loss 3.663\n",
      "1 550\n",
      "Epoch: 1 Train loss 3.663, Test loss 3.662\n",
      "1 550\n",
      "Epoch: 2 Train loss 3.662, Test loss 3.661\n",
      "1 550\n",
      "Epoch: 3 Train loss 3.661, Test loss 3.660\n",
      "1 550\n",
      "Epoch: 4 Train loss 3.660, Test loss 3.659\n",
      "1 550\n",
      "Epoch: 5 Train loss 3.659, Test loss 3.658\n",
      "1 550\n",
      "Epoch: 6 Train loss 3.658, Test loss 3.658\n",
      "1 550\n",
      "Epoch: 7 Train loss 3.658, Test loss 3.657\n",
      "1 550\n",
      "Epoch: 8 Train loss 3.657, Test loss 3.656\n",
      "1 550\n",
      "Epoch: 9 Train loss 3.656, Test loss 3.655\n",
      "1 550\n",
      "Epoch: 10 Train loss 3.655, Test loss 3.654\n",
      "1 550\n",
      "Epoch: 11 Train loss 3.654, Test loss 3.654\n",
      "1 550\n",
      "Epoch: 12 Train loss 3.654, Test loss 3.653\n",
      "1 550\n",
      "Epoch: 13 Train loss 3.653, Test loss 3.652\n",
      "1 550\n",
      "Epoch: 14 Train loss 3.652, Test loss 3.651\n",
      "1 550\n",
      "Epoch: 15 Train loss 3.651, Test loss 3.650\n",
      "1 550\n",
      "Epoch: 16 Train loss 3.650, Test loss 3.650\n",
      "1 550\n",
      "Epoch: 17 Train loss 3.650, Test loss 3.649\n",
      "1 550\n",
      "Epoch: 18 Train loss 3.649, Test loss 3.648\n",
      "1 550\n",
      "Epoch: 19 Train loss 3.648, Test loss 3.647\n",
      "1 550\n",
      "Epoch: 20 Train loss 3.647, Test loss 3.647\n",
      "1 550\n",
      "Epoch: 21 Train loss 3.647, Test loss 3.646\n",
      "1 550\n",
      "Epoch: 22 Train loss 3.646, Test loss 3.645\n",
      "1 550\n",
      "Epoch: 23 Train loss 3.645, Test loss 3.644\n",
      "1 550\n",
      "Epoch: 24 Train loss 3.644, Test loss 3.644\n",
      "1 550\n",
      "Epoch: 25 Train loss 3.644, Test loss 3.643\n",
      "1 550\n",
      "Epoch: 26 Train loss 3.643, Test loss 3.642\n",
      "1 550\n",
      "Epoch: 27 Train loss 3.642, Test loss 3.641\n",
      "1 550\n",
      "Epoch: 28 Train loss 3.641, Test loss 3.641\n",
      "1 550\n",
      "Epoch: 29 Train loss 3.641, Test loss 3.640\n",
      "1 550\n",
      "Epoch: 30 Train loss 3.640, Test loss 3.639\n",
      "1 550\n",
      "Epoch: 31 Train loss 3.639, Test loss 3.638\n",
      "1 550\n",
      "Epoch: 32 Train loss 3.638, Test loss 3.638\n",
      "1 550\n",
      "Epoch: 33 Train loss 3.638, Test loss 3.637\n",
      "1 550\n",
      "Epoch: 34 Train loss 3.637, Test loss 3.636\n",
      "1 550\n",
      "Epoch: 35 Train loss 3.636, Test loss 3.636\n",
      "1 550\n",
      "Epoch: 36 Train loss 3.636, Test loss 3.635\n",
      "1 550\n",
      "Epoch: 37 Train loss 3.635, Test loss 3.634\n",
      "1 550\n",
      "Epoch: 38 Train loss 3.634, Test loss 3.633\n",
      "1 550\n",
      "Epoch: 39 Train loss 3.633, Test loss 3.633\n",
      "1 550\n",
      "Epoch: 40 Train loss 3.633, Test loss 3.632\n",
      "1 550\n",
      "Epoch: 41 Train loss 3.632, Test loss 3.631\n",
      "1 550\n",
      "Epoch: 42 Train loss 3.631, Test loss 3.631\n",
      "1 550\n",
      "Epoch: 43 Train loss 3.631, Test loss 3.630\n",
      "1 550\n",
      "Epoch: 44 Train loss 3.630, Test loss 3.629\n",
      "1 550\n",
      "Epoch: 45 Train loss 3.629, Test loss 3.629\n",
      "1 550\n",
      "Epoch: 46 Train loss 3.629, Test loss 3.628\n",
      "1 550\n",
      "Epoch: 47 Train loss 3.628, Test loss 3.627\n",
      "1 550\n",
      "Epoch: 48 Train loss 3.627, Test loss 3.627\n",
      "1 550\n",
      "Epoch: 49 Train loss 3.627, Test loss 3.626\n",
      "1 550\n",
      "Epoch: 50 Train loss 3.626, Test loss 3.625\n",
      "1 550\n",
      "Epoch: 51 Train loss 3.625, Test loss 3.625\n",
      "1 550\n",
      "Epoch: 52 Train loss 3.625, Test loss 3.624\n",
      "1 550\n",
      "Epoch: 53 Train loss 3.624, Test loss 3.623\n",
      "1 550\n",
      "Epoch: 54 Train loss 3.623, Test loss 3.623\n",
      "1 550\n",
      "Epoch: 55 Train loss 3.623, Test loss 3.622\n",
      "1 550\n",
      "Epoch: 56 Train loss 3.622, Test loss 3.621\n",
      "1 550\n",
      "Epoch: 57 Train loss 3.621, Test loss 3.621\n",
      "1 550\n",
      "Epoch: 58 Train loss 3.621, Test loss 3.620\n",
      "1 550\n",
      "Epoch: 59 Train loss 3.620, Test loss 3.619\n",
      "1 550\n",
      "Epoch: 60 Train loss 3.619, Test loss 3.619\n",
      "1 550\n",
      "Epoch: 61 Train loss 3.619, Test loss 3.618\n",
      "1 550\n",
      "Epoch: 62 Train loss 3.618, Test loss 3.618\n",
      "1 550\n",
      "Epoch: 63 Train loss 3.618, Test loss 3.617\n",
      "1 550\n",
      "Epoch: 64 Train loss 3.617, Test loss 3.616\n",
      "1 550\n",
      "Epoch: 65 Train loss 3.616, Test loss 3.616\n",
      "1 550\n",
      "Epoch: 66 Train loss 3.616, Test loss 3.615\n",
      "1 550\n",
      "Epoch: 67 Train loss 3.615, Test loss 3.614\n",
      "1 550\n",
      "Epoch: 68 Train loss 3.614, Test loss 3.614\n",
      "1 550\n",
      "Epoch: 69 Train loss 3.614, Test loss 3.613\n",
      "1 550\n",
      "Epoch: 70 Train loss 3.613, Test loss 3.613\n",
      "1 550\n",
      "Epoch: 71 Train loss 3.613, Test loss 3.612\n",
      "1 550\n",
      "Epoch: 72 Train loss 3.612, Test loss 3.611\n",
      "1 550\n",
      "Epoch: 73 Train loss 3.611, Test loss 3.611\n",
      "1 550\n",
      "Epoch: 74 Train loss 3.611, Test loss 3.610\n",
      "1 550\n",
      "Epoch: 75 Train loss 3.610, Test loss 3.610\n",
      "1 550\n",
      "Epoch: 76 Train loss 3.610, Test loss 3.609\n",
      "1 550\n",
      "Epoch: 77 Train loss 3.609, Test loss 3.608\n",
      "1 550\n",
      "Epoch: 78 Train loss 3.608, Test loss 3.608\n",
      "1 550\n",
      "Epoch: 79 Train loss 3.608, Test loss 3.607\n",
      "1 550\n",
      "Epoch: 80 Train loss 3.607, Test loss 3.607\n",
      "1 550\n",
      "Epoch: 81 Train loss 3.607, Test loss 3.606\n",
      "1 550\n",
      "Epoch: 82 Train loss 3.606, Test loss 3.605\n",
      "1 550\n",
      "Epoch: 83 Train loss 3.605, Test loss 3.605\n",
      "1 550\n",
      "Epoch: 84 Train loss 3.605, Test loss 3.604\n",
      "1 550\n",
      "Epoch: 85 Train loss 3.604, Test loss 3.604\n",
      "1 550\n",
      "Epoch: 86 Train loss 3.604, Test loss 3.603\n",
      "1 550\n",
      "Epoch: 87 Train loss 3.603, Test loss 3.602\n",
      "1 550\n",
      "Epoch: 88 Train loss 3.602, Test loss 3.602\n",
      "1 550\n",
      "Epoch: 89 Train loss 3.602, Test loss 3.601\n",
      "1 550\n",
      "Epoch: 90 Train loss 3.601, Test loss 3.601\n",
      "1 550\n",
      "Epoch: 91 Train loss 3.601, Test loss 3.600\n",
      "1 550\n",
      "Epoch: 92 Train loss 3.600, Test loss 3.600\n",
      "1 550\n",
      "Epoch: 93 Train loss 3.600, Test loss 3.599\n",
      "1 550\n",
      "Epoch: 94 Train loss 3.599, Test loss 3.598\n",
      "1 550\n",
      "Epoch: 95 Train loss 3.598, Test loss 3.598\n",
      "1 550\n",
      "Epoch: 96 Train loss 3.598, Test loss 3.597\n",
      "1 550\n",
      "Epoch: 97 Train loss 3.597, Test loss 3.597\n",
      "1 550\n",
      "Epoch: 98 Train loss 3.597, Test loss 3.596\n",
      "1 550\n",
      "Epoch: 99 Train loss 3.596, Test loss 3.596\n",
      "Mean train loss: 3.628 Mean val score: 3.627\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1.  2.  2. -1.  0.\n",
      "  -1.  2.  6. -2.  5.  5.  2.  2. -2.  7. -1. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1. 16.  3.  2.]] [[0.15038991]] False\n",
      "Step 51: Reward 0.444.\n",
      "[[-1.  0.  0. -1.  0. -1. -1.  1.  0. -1. -1. -1.  0. -1.  2.  2. -1. -1.\n",
      "  -1.  6.  6. -2.  4.  5.  4.  1. -2. 14.  1.  3. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1.  0. -1. -2. -2. -2. 16.  3.  3.]] [[0.04041049]] False\n",
      "Step 52: Reward 0.366.\n",
      "[[-1.  0. -1. -1.  0. -1.  1.  1.  1. -1. -1. -1.  0.  0.  2.  2. -1. -1.\n",
      "  -1.  7.  7. -2.  1.  6. -1. 13.  1. 11.  3.  2. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1.  0.  0. -2. -2. -2. 16.  3.  4.]] [[0.08417568]] False\n",
      "Step 53: Reward 0.451.\n",
      "[[-1.  0. -1. -1.  0. -1.  7.  1. -1. -1. -1. -1.  3.  0.  2.  2. -2.  1.\n",
      "  -2.  6.  6. -2.  0.  4. -1. 12.  5.  8.  8.  8. -1. -1. -2. -1. -1.  1.\n",
      "   0. -1.  2.  0. -2. -2. -2. 16.  3.  4.]] [[0.285344]] False\n",
      "Step 54: Reward 0.657.\n",
      "[[ 0. -1. -1. -1.  2. -1.  6.  1. -1.  0.  2. -2.  3.  0.  1.  1. -2.  0.\n",
      "  -2.  8.  6. -2. -2.  4.  0. 12. 12.  5.  8.  9.  0.  2. -2. -1. -1.  1.\n",
      "   0. -2.  1.  0. -3. -2. -2. 16.  3.  5.]] [[0.2901935]] False\n",
      "Step 55: Reward 0.686.\n",
      "[[ 1. -1. -1. -1.  2. -1.  6.  1.  0.  0.  2. -2.  3.  0.  1.  1. -2.  1.\n",
      "  -2.  8.  5. -3. -2.  8.  0. 12. 20.  5. 11. 12. -1.  2. -2. -2. -1.  1.\n",
      "  -1. -2.  2.  0. -3. -3. -2. 16.  3.  6.]] [[0.27606988]] True\n",
      "Step 56: Reward 0.700.\n",
      "Trial: 7, reward: 4.231849685726643.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   2. -1. -1.  2.  2.  2.  2. -1. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 25.  9.  0.]] [[-0.24384487]] False\n",
      "Step 57: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   2. -1.  2.  1.  1.  2.  8.  7. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 25.  9.  1.]] [[-0.26915485]] False\n",
      "Step 58: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1.  2.  0. -1. -1. -1. -1. -1.\n",
      "   2. -2.  2.  1.  1.  2. 11. 10. -2.  3. -1.  3. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1. 25.  9.  2.]] [[-0.10481767]] False\n",
      "Step 59: Reward 0.200.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1. -1. -1. -1.  2.\n",
      "   5. -2.  1.  5. -1.  2. 11. 14. -2.  0. -2.  4. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2. 25.  9.  3.]] [[-0.09318793]] False\n",
      "Step 60: Reward 0.234.\n",
      "1 560\n",
      "Epoch: 0 Train loss 3.739, Test loss 3.738\n",
      "1 560\n",
      "Epoch: 1 Train loss 3.738, Test loss 3.738\n",
      "1 560\n",
      "Epoch: 2 Train loss 3.738, Test loss 3.737\n",
      "1 560\n",
      "Epoch: 3 Train loss 3.737, Test loss 3.736\n",
      "1 560\n",
      "Epoch: 4 Train loss 3.736, Test loss 3.735\n",
      "1 560\n",
      "Epoch: 5 Train loss 3.735, Test loss 3.735\n",
      "1 560\n",
      "Epoch: 6 Train loss 3.735, Test loss 3.734\n",
      "1 560\n",
      "Epoch: 7 Train loss 3.734, Test loss 3.733\n",
      "1 560\n",
      "Epoch: 8 Train loss 3.733, Test loss 3.732\n",
      "1 560\n",
      "Epoch: 9 Train loss 3.732, Test loss 3.732\n",
      "1 560\n",
      "Epoch: 10 Train loss 3.732, Test loss 3.731\n",
      "1 560\n",
      "Epoch: 11 Train loss 3.731, Test loss 3.730\n",
      "1 560\n",
      "Epoch: 12 Train loss 3.730, Test loss 3.729\n",
      "1 560\n",
      "Epoch: 13 Train loss 3.729, Test loss 3.729\n",
      "1 560\n",
      "Epoch: 14 Train loss 3.729, Test loss 3.728\n",
      "1 560\n",
      "Epoch: 15 Train loss 3.728, Test loss 3.727\n",
      "1 560\n",
      "Epoch: 16 Train loss 3.727, Test loss 3.727\n",
      "1 560\n",
      "Epoch: 17 Train loss 3.727, Test loss 3.726\n",
      "1 560\n",
      "Epoch: 18 Train loss 3.726, Test loss 3.725\n",
      "1 560\n",
      "Epoch: 19 Train loss 3.725, Test loss 3.724\n",
      "1 560\n",
      "Epoch: 20 Train loss 3.724, Test loss 3.724\n",
      "1 560\n",
      "Epoch: 21 Train loss 3.724, Test loss 3.723\n",
      "1 560\n",
      "Epoch: 22 Train loss 3.723, Test loss 3.722\n",
      "1 560\n",
      "Epoch: 23 Train loss 3.722, Test loss 3.722\n",
      "1 560\n",
      "Epoch: 24 Train loss 3.722, Test loss 3.721\n",
      "1 560\n",
      "Epoch: 25 Train loss 3.721, Test loss 3.720\n",
      "1 560\n",
      "Epoch: 26 Train loss 3.720, Test loss 3.719\n",
      "1 560\n",
      "Epoch: 27 Train loss 3.719, Test loss 3.719\n",
      "1 560\n",
      "Epoch: 28 Train loss 3.719, Test loss 3.718\n",
      "1 560\n",
      "Epoch: 29 Train loss 3.718, Test loss 3.717\n",
      "1 560\n",
      "Epoch: 30 Train loss 3.717, Test loss 3.717\n",
      "1 560\n",
      "Epoch: 31 Train loss 3.717, Test loss 3.716\n",
      "1 560\n",
      "Epoch: 32 Train loss 3.716, Test loss 3.715\n",
      "1 560\n",
      "Epoch: 33 Train loss 3.715, Test loss 3.715\n",
      "1 560\n",
      "Epoch: 34 Train loss 3.715, Test loss 3.714\n",
      "1 560\n",
      "Epoch: 35 Train loss 3.714, Test loss 3.713\n",
      "1 560\n",
      "Epoch: 36 Train loss 3.713, Test loss 3.713\n",
      "1 560\n",
      "Epoch: 37 Train loss 3.713, Test loss 3.712\n",
      "1 560\n",
      "Epoch: 38 Train loss 3.712, Test loss 3.711\n",
      "1 560\n",
      "Epoch: 39 Train loss 3.711, Test loss 3.711\n",
      "1 560\n",
      "Epoch: 40 Train loss 3.711, Test loss 3.710\n",
      "1 560\n",
      "Epoch: 41 Train loss 3.710, Test loss 3.709\n",
      "1 560\n",
      "Epoch: 42 Train loss 3.709, Test loss 3.709\n",
      "1 560\n",
      "Epoch: 43 Train loss 3.709, Test loss 3.708\n",
      "1 560\n",
      "Epoch: 44 Train loss 3.708, Test loss 3.707\n",
      "1 560\n",
      "Epoch: 45 Train loss 3.707, Test loss 3.707\n",
      "1 560\n",
      "Epoch: 46 Train loss 3.707, Test loss 3.706\n",
      "1 560\n",
      "Epoch: 47 Train loss 3.706, Test loss 3.706\n",
      "1 560\n",
      "Epoch: 48 Train loss 3.706, Test loss 3.705\n",
      "1 560\n",
      "Epoch: 49 Train loss 3.705, Test loss 3.704\n",
      "1 560\n",
      "Epoch: 50 Train loss 3.704, Test loss 3.704\n",
      "1 560\n",
      "Epoch: 51 Train loss 3.704, Test loss 3.703\n",
      "1 560\n",
      "Epoch: 52 Train loss 3.703, Test loss 3.702\n",
      "1 560\n",
      "Epoch: 53 Train loss 3.702, Test loss 3.702\n",
      "1 560\n",
      "Epoch: 54 Train loss 3.702, Test loss 3.701\n",
      "1 560\n",
      "Epoch: 55 Train loss 3.701, Test loss 3.701\n",
      "1 560\n",
      "Epoch: 56 Train loss 3.701, Test loss 3.700\n",
      "1 560\n",
      "Epoch: 57 Train loss 3.700, Test loss 3.699\n",
      "1 560\n",
      "Epoch: 58 Train loss 3.699, Test loss 3.699\n",
      "1 560\n",
      "Epoch: 59 Train loss 3.699, Test loss 3.698\n",
      "1 560\n",
      "Epoch: 60 Train loss 3.698, Test loss 3.697\n",
      "1 560\n",
      "Epoch: 61 Train loss 3.697, Test loss 3.697\n",
      "1 560\n",
      "Epoch: 62 Train loss 3.697, Test loss 3.696\n",
      "1 560\n",
      "Epoch: 63 Train loss 3.696, Test loss 3.696\n",
      "1 560\n",
      "Epoch: 64 Train loss 3.696, Test loss 3.695\n",
      "1 560\n",
      "Epoch: 65 Train loss 3.695, Test loss 3.694\n",
      "1 560\n",
      "Epoch: 66 Train loss 3.694, Test loss 3.694\n",
      "1 560\n",
      "Epoch: 67 Train loss 3.694, Test loss 3.693\n",
      "1 560\n",
      "Epoch: 68 Train loss 3.693, Test loss 3.693\n",
      "1 560\n",
      "Epoch: 69 Train loss 3.693, Test loss 3.692\n",
      "1 560\n",
      "Epoch: 70 Train loss 3.692, Test loss 3.691\n",
      "1 560\n",
      "Epoch: 71 Train loss 3.691, Test loss 3.691\n",
      "1 560\n",
      "Epoch: 72 Train loss 3.691, Test loss 3.690\n",
      "1 560\n",
      "Epoch: 73 Train loss 3.690, Test loss 3.690\n",
      "1 560\n",
      "Epoch: 74 Train loss 3.690, Test loss 3.689\n",
      "1 560\n",
      "Epoch: 75 Train loss 3.689, Test loss 3.688\n",
      "1 560\n",
      "Epoch: 76 Train loss 3.688, Test loss 3.688\n",
      "1 560\n",
      "Epoch: 77 Train loss 3.688, Test loss 3.687\n",
      "1 560\n",
      "Epoch: 78 Train loss 3.687, Test loss 3.687\n",
      "1 560\n",
      "Epoch: 79 Train loss 3.687, Test loss 3.686\n",
      "1 560\n",
      "Epoch: 80 Train loss 3.686, Test loss 3.686\n",
      "1 560\n",
      "Epoch: 81 Train loss 3.686, Test loss 3.685\n",
      "1 560\n",
      "Epoch: 82 Train loss 3.685, Test loss 3.684\n",
      "1 560\n",
      "Epoch: 83 Train loss 3.684, Test loss 3.684\n",
      "1 560\n",
      "Epoch: 84 Train loss 3.684, Test loss 3.683\n",
      "1 560\n",
      "Epoch: 85 Train loss 3.683, Test loss 3.683\n",
      "1 560\n",
      "Epoch: 86 Train loss 3.683, Test loss 3.682\n",
      "1 560\n",
      "Epoch: 87 Train loss 3.682, Test loss 3.682\n",
      "1 560\n",
      "Epoch: 88 Train loss 3.682, Test loss 3.681\n",
      "1 560\n",
      "Epoch: 89 Train loss 3.681, Test loss 3.680\n",
      "1 560\n",
      "Epoch: 90 Train loss 3.680, Test loss 3.680\n",
      "1 560\n",
      "Epoch: 91 Train loss 3.680, Test loss 3.679\n",
      "1 560\n",
      "Epoch: 92 Train loss 3.679, Test loss 3.679\n",
      "1 560\n",
      "Epoch: 93 Train loss 3.679, Test loss 3.678\n",
      "1 560\n",
      "Epoch: 94 Train loss 3.678, Test loss 3.678\n",
      "1 560\n",
      "Epoch: 95 Train loss 3.678, Test loss 3.677\n",
      "1 560\n",
      "Epoch: 96 Train loss 3.677, Test loss 3.677\n",
      "1 560\n",
      "Epoch: 97 Train loss 3.677, Test loss 3.676\n",
      "1 560\n",
      "Epoch: 98 Train loss 3.676, Test loss 3.675\n",
      "1 560\n",
      "Epoch: 99 Train loss 3.675, Test loss 3.675\n",
      "Mean train loss: 3.706 Mean val score: 3.705\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1.  0.  1.  0. -1.  0. -1. -1. -1.\n",
      "   6. -2.  0.  5. 10.  3. 14. 13.  2. -1. -2.  2. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2. 25.  9.  4.]] [[-0.00122738]] False\n",
      "Step 61: Reward 0.346.\n",
      "[[ 4.  2.  1.  0.  0. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -2. -1.\n",
      "   5. -1. -1.  8.  4.  1.  2. 21.  2. -1. -1.  0. -1. -1. -2.  3.  1.  0.\n",
      "   0.  0.  1. -1. -2.  0. -1. 25.  9.  4.]] [[0.06137145]] False\n",
      "Step 62: Reward 0.447.\n",
      "[[ 9.  0.  0.  6.  1. -1.  3. -1.  4.  0.  2.  3. -1. -1. -1. -2. -2. -1.\n",
      "   5. -2.  3.  8. -3. -1.  0. 20.  4. -1.  1.  2. -1. -2. -2.  6.  1.  0.\n",
      "   0.  1.  1. -1. -3. -1. -1. 25.  9.  5.]] [[0.17770436]] False\n",
      "Step 63: Reward 0.559.\n",
      "[[ 9.  0.  0.  6.  1. -1.  3. -1.  3.  0.  1.  3. -1. -1. -2. -2. -2. -1.\n",
      "   5. -2.  6.  8. -2. -1.  0. 20.  6.  8.  2.  4. -1. -2. -2.  9.  1.  0.\n",
      "   0.  1.  1. -1. -3. -1. -1. 25.  9.  6.]] [[0.49291888]] True\n",
      "Step 64: Reward 0.889.\n",
      "Trial: 8, reward: 2.6749232900061055.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   5. -1. -1.  3. -1.  2.  2. -1. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 16.  7.  0.]] [[-0.23804973]] False\n",
      "Step 65: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   5. -2.  3.  2. -2. 16.  2. -1. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 16.  7.  1.]] [[-0.2048577]] False\n",
      "Step 66: Reward 0.064.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  2. -1. -1. -1. -1.\n",
      "   4. -2.  2.  9. -2. 19.  3. -2. -2. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1. 16.  7.  2.]] [[-0.2262612]] False\n",
      "Step 67: Reward 0.070.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0.  1. -1. -1. -1.  0.  1. -1. -1. -1. -1.\n",
      "   8.  4.  2.  8. -1. 18. -1. -2.  2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1.  1. -1. -2. -2. -2. 16.  7.  3.]] [[-0.13801844]] False\n",
      "Step 68: Reward 0.182.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0. -1. -1.  0.\n",
      "   8.  5.  1.  5.  1. 14. -1.  3.  5. -1. -1.  0. -1. -1. -1.  2. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -2. -2. 16.  7.  4.]] [[-0.09607071]] False\n",
      "Step 69: Reward 0.247.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -2.  0. -1.  0. -1. -1. -1.\n",
      "   7.  4.  2. 12. -3. 10.  1. 10. 12. -1. -2. -1. -1.  0. -2. -1. -1. -1.\n",
      "   2. -1. -2. -1. -1. -2. -2. 16.  7.  4.]] [[-0.04410622]] False\n",
      "Step 70: Reward 0.307.\n",
      "1 570\n",
      "Epoch: 0 Train loss 3.851, Test loss 3.850\n",
      "1 570\n",
      "Epoch: 1 Train loss 3.850, Test loss 3.850\n",
      "1 570\n",
      "Epoch: 2 Train loss 3.850, Test loss 3.849\n",
      "1 570\n",
      "Epoch: 3 Train loss 3.849, Test loss 3.848\n",
      "1 570\n",
      "Epoch: 4 Train loss 3.848, Test loss 3.847\n",
      "1 570\n",
      "Epoch: 5 Train loss 3.847, Test loss 3.847\n",
      "1 570\n",
      "Epoch: 6 Train loss 3.847, Test loss 3.846\n",
      "1 570\n",
      "Epoch: 7 Train loss 3.846, Test loss 3.845\n",
      "1 570\n",
      "Epoch: 8 Train loss 3.845, Test loss 3.845\n",
      "1 570\n",
      "Epoch: 9 Train loss 3.845, Test loss 3.844\n",
      "1 570\n",
      "Epoch: 10 Train loss 3.844, Test loss 3.843\n",
      "1 570\n",
      "Epoch: 11 Train loss 3.843, Test loss 3.842\n",
      "1 570\n",
      "Epoch: 12 Train loss 3.842, Test loss 3.842\n",
      "1 570\n",
      "Epoch: 13 Train loss 3.842, Test loss 3.841\n",
      "1 570\n",
      "Epoch: 14 Train loss 3.841, Test loss 3.840\n",
      "1 570\n",
      "Epoch: 15 Train loss 3.840, Test loss 3.839\n",
      "1 570\n",
      "Epoch: 16 Train loss 3.839, Test loss 3.839\n",
      "1 570\n",
      "Epoch: 17 Train loss 3.839, Test loss 3.838\n",
      "1 570\n",
      "Epoch: 18 Train loss 3.838, Test loss 3.837\n",
      "1 570\n",
      "Epoch: 19 Train loss 3.837, Test loss 3.837\n",
      "1 570\n",
      "Epoch: 20 Train loss 3.837, Test loss 3.836\n",
      "1 570\n",
      "Epoch: 21 Train loss 3.836, Test loss 3.835\n",
      "1 570\n",
      "Epoch: 22 Train loss 3.835, Test loss 3.835\n",
      "1 570\n",
      "Epoch: 23 Train loss 3.835, Test loss 3.834\n",
      "1 570\n",
      "Epoch: 24 Train loss 3.834, Test loss 3.833\n",
      "1 570\n",
      "Epoch: 25 Train loss 3.833, Test loss 3.832\n",
      "1 570\n",
      "Epoch: 26 Train loss 3.832, Test loss 3.832\n",
      "1 570\n",
      "Epoch: 27 Train loss 3.832, Test loss 3.831\n",
      "1 570\n",
      "Epoch: 28 Train loss 3.831, Test loss 3.830\n",
      "1 570\n",
      "Epoch: 29 Train loss 3.830, Test loss 3.830\n",
      "1 570\n",
      "Epoch: 30 Train loss 3.830, Test loss 3.829\n",
      "1 570\n",
      "Epoch: 31 Train loss 3.829, Test loss 3.828\n",
      "1 570\n",
      "Epoch: 32 Train loss 3.828, Test loss 3.828\n",
      "1 570\n",
      "Epoch: 33 Train loss 3.828, Test loss 3.827\n",
      "1 570\n",
      "Epoch: 34 Train loss 3.827, Test loss 3.826\n",
      "1 570\n",
      "Epoch: 35 Train loss 3.826, Test loss 3.826\n",
      "1 570\n",
      "Epoch: 36 Train loss 3.826, Test loss 3.825\n",
      "1 570\n",
      "Epoch: 37 Train loss 3.825, Test loss 3.824\n",
      "1 570\n",
      "Epoch: 38 Train loss 3.824, Test loss 3.824\n",
      "1 570\n",
      "Epoch: 39 Train loss 3.824, Test loss 3.823\n",
      "1 570\n",
      "Epoch: 40 Train loss 3.823, Test loss 3.822\n",
      "1 570\n",
      "Epoch: 41 Train loss 3.822, Test loss 3.822\n",
      "1 570\n",
      "Epoch: 42 Train loss 3.822, Test loss 3.821\n",
      "1 570\n",
      "Epoch: 43 Train loss 3.821, Test loss 3.820\n",
      "1 570\n",
      "Epoch: 44 Train loss 3.820, Test loss 3.820\n",
      "1 570\n",
      "Epoch: 45 Train loss 3.820, Test loss 3.819\n",
      "1 570\n",
      "Epoch: 46 Train loss 3.819, Test loss 3.818\n",
      "1 570\n",
      "Epoch: 47 Train loss 3.818, Test loss 3.818\n",
      "1 570\n",
      "Epoch: 48 Train loss 3.818, Test loss 3.817\n",
      "1 570\n",
      "Epoch: 49 Train loss 3.817, Test loss 3.817\n",
      "1 570\n",
      "Epoch: 50 Train loss 3.817, Test loss 3.816\n",
      "1 570\n",
      "Epoch: 51 Train loss 3.816, Test loss 3.815\n",
      "1 570\n",
      "Epoch: 52 Train loss 3.815, Test loss 3.815\n",
      "1 570\n",
      "Epoch: 53 Train loss 3.815, Test loss 3.814\n",
      "1 570\n",
      "Epoch: 54 Train loss 3.814, Test loss 3.813\n",
      "1 570\n",
      "Epoch: 55 Train loss 3.813, Test loss 3.813\n",
      "1 570\n",
      "Epoch: 56 Train loss 3.813, Test loss 3.812\n",
      "1 570\n",
      "Epoch: 57 Train loss 3.812, Test loss 3.811\n",
      "1 570\n",
      "Epoch: 58 Train loss 3.811, Test loss 3.811\n",
      "1 570\n",
      "Epoch: 59 Train loss 3.811, Test loss 3.810\n",
      "1 570\n",
      "Epoch: 60 Train loss 3.810, Test loss 3.810\n",
      "1 570\n",
      "Epoch: 61 Train loss 3.810, Test loss 3.809\n",
      "1 570\n",
      "Epoch: 62 Train loss 3.809, Test loss 3.808\n",
      "1 570\n",
      "Epoch: 63 Train loss 3.808, Test loss 3.808\n",
      "1 570\n",
      "Epoch: 64 Train loss 3.808, Test loss 3.807\n",
      "1 570\n",
      "Epoch: 65 Train loss 3.807, Test loss 3.807\n",
      "1 570\n",
      "Epoch: 66 Train loss 3.807, Test loss 3.806\n",
      "1 570\n",
      "Epoch: 67 Train loss 3.806, Test loss 3.805\n",
      "1 570\n",
      "Epoch: 68 Train loss 3.805, Test loss 3.805\n",
      "1 570\n",
      "Epoch: 69 Train loss 3.805, Test loss 3.804\n",
      "1 570\n",
      "Epoch: 70 Train loss 3.804, Test loss 3.804\n",
      "1 570\n",
      "Epoch: 71 Train loss 3.804, Test loss 3.803\n",
      "1 570\n",
      "Epoch: 72 Train loss 3.803, Test loss 3.802\n",
      "1 570\n",
      "Epoch: 73 Train loss 3.802, Test loss 3.802\n",
      "1 570\n",
      "Epoch: 74 Train loss 3.802, Test loss 3.801\n",
      "1 570\n",
      "Epoch: 75 Train loss 3.801, Test loss 3.801\n",
      "1 570\n",
      "Epoch: 76 Train loss 3.801, Test loss 3.800\n",
      "1 570\n",
      "Epoch: 77 Train loss 3.800, Test loss 3.799\n",
      "1 570\n",
      "Epoch: 78 Train loss 3.799, Test loss 3.799\n",
      "1 570\n",
      "Epoch: 79 Train loss 3.799, Test loss 3.798\n",
      "1 570\n",
      "Epoch: 80 Train loss 3.798, Test loss 3.798\n",
      "1 570\n",
      "Epoch: 81 Train loss 3.798, Test loss 3.797\n",
      "1 570\n",
      "Epoch: 82 Train loss 3.797, Test loss 3.796\n",
      "1 570\n",
      "Epoch: 83 Train loss 3.796, Test loss 3.796\n",
      "1 570\n",
      "Epoch: 84 Train loss 3.796, Test loss 3.795\n",
      "1 570\n",
      "Epoch: 85 Train loss 3.795, Test loss 3.795\n",
      "1 570\n",
      "Epoch: 86 Train loss 3.795, Test loss 3.794\n",
      "1 570\n",
      "Epoch: 87 Train loss 3.794, Test loss 3.793\n",
      "1 570\n",
      "Epoch: 88 Train loss 3.793, Test loss 3.793\n",
      "1 570\n",
      "Epoch: 89 Train loss 3.793, Test loss 3.792\n",
      "1 570\n",
      "Epoch: 90 Train loss 3.792, Test loss 3.792\n",
      "1 570\n",
      "Epoch: 91 Train loss 3.792, Test loss 3.791\n",
      "1 570\n",
      "Epoch: 92 Train loss 3.791, Test loss 3.791\n",
      "1 570\n",
      "Epoch: 93 Train loss 3.791, Test loss 3.790\n",
      "1 570\n",
      "Epoch: 94 Train loss 3.790, Test loss 3.789\n",
      "1 570\n",
      "Epoch: 95 Train loss 3.789, Test loss 3.789\n",
      "1 570\n",
      "Epoch: 96 Train loss 3.789, Test loss 3.788\n",
      "1 570\n",
      "Epoch: 97 Train loss 3.788, Test loss 3.788\n",
      "1 570\n",
      "Epoch: 98 Train loss 3.788, Test loss 3.787\n",
      "1 570\n",
      "Epoch: 99 Train loss 3.787, Test loss 3.787\n",
      "Mean train loss: 3.818 Mean val score: 3.817\n",
      "[[-1. -1. -1. -1.  0. -1. -1.  1. -1. -1. -2. -1.  2. -1. -1. -2. -1. -1.\n",
      "   6.  5.  2. 13. -2. 12. -1. 14. 16. -1.  0. -1. -1. -1. -2.  0. -1. -2.\n",
      "   0. -1.  1. -1. -2. -3. -2. 16.  7.  5.]] [[-0.13482468]] False\n",
      "Step 71: Reward 0.249.\n",
      "[[-1. -1. -1.  0.  0. -1. -1. -1.  2. -1. -2. -2. -1.  2. -2. -2.  0. -1.\n",
      "   4.  5.  0. 12.  0. 10.  0. 18. 21. -1. -2.  0. -1. -2. -2.  0.  2. -2.\n",
      "   0.  0.  0. -1. -2. -1. -1. 16.  7.  6.]] [[-0.08290038]] True\n",
      "Step 72: Reward 0.328.\n",
      "Trial: 9, reward: 1.447322157718051.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  1. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  5.  1.  2. -1.  0. -1. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "   0. -1. -1. -1. -1. -1. -1. 27.  6.  0.]] [[-0.00492349]] False\n",
      "Step 73: Reward 0.241.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0. -1. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   2. -2.  3.  2.  8.  0. -1. -1. -2.  2. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 27.  6.  1.]] [[0.07076651]] False\n",
      "Step 74: Reward 0.342.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1.  1. -1. -1. -1.\n",
      "   5. -2.  3.  1.  9.  1. -1. -2.  2.  0. -1. -1. -1. -1. -1. -1.  0.  1.\n",
      "  -2. -1. -1.  0. -2. -2. -1. 27.  6.  2.]] [[-0.03444651]] False\n",
      "Step 75: Reward 0.264.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  0.  2. -1.\n",
      "   4. -2.  1.  5. 12. -1. -1.  3.  1. -1.  3.  0. -1. -1. -1. -1. -1.  0.\n",
      "   0.  0. -2. -1.  0. -2. -2. 27.  6.  3.]] [[-0.06838822]] False\n",
      "Step 76: Reward 0.251.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  1. -1. -1.  2. -1.  0.  0. -1. -1. -2.  0.\n",
      "   4. -2.  1.  5.  9. -1. -1.  6. 13. -1.  2. -1.  1. -1. -1. -1. -1.  0.\n",
      "  -2.  1. -1. -1.  0. -2. -2. 27.  6.  4.]] [[-0.10185379]] False\n",
      "Step 77: Reward 0.236.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0. -2. -1. -1.  1.  0. -1.  1.\n",
      "   5. -3.  0.  7.  8. -1.  4.  7. 14. -1.  4.  1. -1.  2. -2. -1.  0. -1.\n",
      "  -1.  1. -2.  0.  0. -2. -1. 27.  6.  4.]] [[0.00373232]] False\n",
      "Step 78: Reward 0.372.\n",
      "[[-1.  0.  0.  1.  0. -1.  1.  1. -1. -1. -2.  1. -1. -1. -2. -2. -1.  0.\n",
      "   5. -2.  1.  8.  4.  2.  2.  7. 18. -1.  8. -1. -1.  7. -1. -2. -1. -2.\n",
      "  -2.  2. -1.  0.  0. -3.  0. 27.  6.  5.]] [[0.14184439]] False\n",
      "Step 79: Reward 0.538.\n",
      "[[ 0.  0. -1.  1.  0.  0.  1.  1. -1.  3.  1.  1. -1.  0.  2.  0. -1.  1.\n",
      "   5. -2. -1.  6.  4.  2.  1.  6. 25.  1.  5.  0. -1.  7. -1. -2. -1. -1.\n",
      "  -1.  2.  0.  0.  0. -2.  0. 27.  6.  6.]] [[0.17154038]] True\n",
      "Step 80: Reward 0.593.\n",
      "Trial: 10, reward: 2.8382755631166487.\n",
      "1 580\n",
      "Epoch: 0 Train loss 3.940, Test loss 3.939\n",
      "1 580\n",
      "Epoch: 1 Train loss 3.939, Test loss 3.938\n",
      "1 580\n",
      "Epoch: 2 Train loss 3.938, Test loss 3.938\n",
      "1 580\n",
      "Epoch: 3 Train loss 3.938, Test loss 3.937\n",
      "1 580\n",
      "Epoch: 4 Train loss 3.937, Test loss 3.936\n",
      "1 580\n",
      "Epoch: 5 Train loss 3.936, Test loss 3.935\n",
      "1 580\n",
      "Epoch: 6 Train loss 3.935, Test loss 3.934\n",
      "1 580\n",
      "Epoch: 7 Train loss 3.934, Test loss 3.934\n",
      "1 580\n",
      "Epoch: 8 Train loss 3.934, Test loss 3.933\n",
      "1 580\n",
      "Epoch: 9 Train loss 3.933, Test loss 3.932\n",
      "1 580\n",
      "Epoch: 10 Train loss 3.932, Test loss 3.931\n",
      "1 580\n",
      "Epoch: 11 Train loss 3.931, Test loss 3.930\n",
      "1 580\n",
      "Epoch: 12 Train loss 3.930, Test loss 3.929\n",
      "1 580\n",
      "Epoch: 13 Train loss 3.929, Test loss 3.929\n",
      "1 580\n",
      "Epoch: 14 Train loss 3.929, Test loss 3.928\n",
      "1 580\n",
      "Epoch: 15 Train loss 3.928, Test loss 3.927\n",
      "1 580\n",
      "Epoch: 16 Train loss 3.927, Test loss 3.926\n",
      "1 580\n",
      "Epoch: 17 Train loss 3.926, Test loss 3.926\n",
      "1 580\n",
      "Epoch: 18 Train loss 3.926, Test loss 3.925\n",
      "1 580\n",
      "Epoch: 19 Train loss 3.925, Test loss 3.924\n",
      "1 580\n",
      "Epoch: 20 Train loss 3.924, Test loss 3.923\n",
      "1 580\n",
      "Epoch: 21 Train loss 3.923, Test loss 3.922\n",
      "1 580\n",
      "Epoch: 22 Train loss 3.922, Test loss 3.922\n",
      "1 580\n",
      "Epoch: 23 Train loss 3.922, Test loss 3.921\n",
      "1 580\n",
      "Epoch: 24 Train loss 3.921, Test loss 3.920\n",
      "1 580\n",
      "Epoch: 25 Train loss 3.920, Test loss 3.919\n",
      "1 580\n",
      "Epoch: 26 Train loss 3.919, Test loss 3.919\n",
      "1 580\n",
      "Epoch: 27 Train loss 3.919, Test loss 3.918\n",
      "1 580\n",
      "Epoch: 28 Train loss 3.918, Test loss 3.917\n",
      "1 580\n",
      "Epoch: 29 Train loss 3.917, Test loss 3.916\n",
      "1 580\n",
      "Epoch: 30 Train loss 3.916, Test loss 3.916\n",
      "1 580\n",
      "Epoch: 31 Train loss 3.916, Test loss 3.915\n",
      "1 580\n",
      "Epoch: 32 Train loss 3.915, Test loss 3.914\n",
      "1 580\n",
      "Epoch: 33 Train loss 3.914, Test loss 3.913\n",
      "1 580\n",
      "Epoch: 34 Train loss 3.913, Test loss 3.913\n",
      "1 580\n",
      "Epoch: 35 Train loss 3.913, Test loss 3.912\n",
      "1 580\n",
      "Epoch: 36 Train loss 3.912, Test loss 3.911\n",
      "1 580\n",
      "Epoch: 37 Train loss 3.911, Test loss 3.910\n",
      "1 580\n",
      "Epoch: 38 Train loss 3.910, Test loss 3.910\n",
      "1 580\n",
      "Epoch: 39 Train loss 3.910, Test loss 3.909\n",
      "1 580\n",
      "Epoch: 40 Train loss 3.909, Test loss 3.908\n",
      "1 580\n",
      "Epoch: 41 Train loss 3.908, Test loss 3.907\n",
      "1 580\n",
      "Epoch: 42 Train loss 3.907, Test loss 3.907\n",
      "1 580\n",
      "Epoch: 43 Train loss 3.907, Test loss 3.906\n",
      "1 580\n",
      "Epoch: 44 Train loss 3.906, Test loss 3.905\n",
      "1 580\n",
      "Epoch: 45 Train loss 3.905, Test loss 3.905\n",
      "1 580\n",
      "Epoch: 46 Train loss 3.905, Test loss 3.904\n",
      "1 580\n",
      "Epoch: 47 Train loss 3.904, Test loss 3.903\n",
      "1 580\n",
      "Epoch: 48 Train loss 3.903, Test loss 3.902\n",
      "1 580\n",
      "Epoch: 49 Train loss 3.902, Test loss 3.902\n",
      "1 580\n",
      "Epoch: 50 Train loss 3.902, Test loss 3.901\n",
      "1 580\n",
      "Epoch: 51 Train loss 3.901, Test loss 3.900\n",
      "1 580\n",
      "Epoch: 52 Train loss 3.900, Test loss 3.900\n",
      "1 580\n",
      "Epoch: 53 Train loss 3.900, Test loss 3.899\n",
      "1 580\n",
      "Epoch: 54 Train loss 3.899, Test loss 3.898\n",
      "1 580\n",
      "Epoch: 55 Train loss 3.898, Test loss 3.897\n",
      "1 580\n",
      "Epoch: 56 Train loss 3.897, Test loss 3.897\n",
      "1 580\n",
      "Epoch: 57 Train loss 3.897, Test loss 3.896\n",
      "1 580\n",
      "Epoch: 58 Train loss 3.896, Test loss 3.895\n",
      "1 580\n",
      "Epoch: 59 Train loss 3.895, Test loss 3.895\n",
      "1 580\n",
      "Epoch: 60 Train loss 3.895, Test loss 3.894\n",
      "1 580\n",
      "Epoch: 61 Train loss 3.894, Test loss 3.893\n",
      "1 580\n",
      "Epoch: 62 Train loss 3.893, Test loss 3.893\n",
      "1 580\n",
      "Epoch: 63 Train loss 3.893, Test loss 3.892\n",
      "1 580\n",
      "Epoch: 64 Train loss 3.892, Test loss 3.891\n",
      "1 580\n",
      "Epoch: 65 Train loss 3.891, Test loss 3.891\n",
      "1 580\n",
      "Epoch: 66 Train loss 3.891, Test loss 3.890\n",
      "1 580\n",
      "Epoch: 67 Train loss 3.890, Test loss 3.889\n",
      "1 580\n",
      "Epoch: 68 Train loss 3.889, Test loss 3.888\n",
      "1 580\n",
      "Epoch: 69 Train loss 3.888, Test loss 3.888\n",
      "1 580\n",
      "Epoch: 70 Train loss 3.888, Test loss 3.887\n",
      "1 580\n",
      "Epoch: 71 Train loss 3.887, Test loss 3.886\n",
      "1 580\n",
      "Epoch: 72 Train loss 3.886, Test loss 3.886\n",
      "1 580\n",
      "Epoch: 73 Train loss 3.886, Test loss 3.885\n",
      "1 580\n",
      "Epoch: 74 Train loss 3.885, Test loss 3.884\n",
      "1 580\n",
      "Epoch: 75 Train loss 3.884, Test loss 3.884\n",
      "1 580\n",
      "Epoch: 76 Train loss 3.884, Test loss 3.883\n",
      "1 580\n",
      "Epoch: 77 Train loss 3.883, Test loss 3.882\n",
      "1 580\n",
      "Epoch: 78 Train loss 3.882, Test loss 3.882\n",
      "1 580\n",
      "Epoch: 79 Train loss 3.882, Test loss 3.881\n",
      "1 580\n",
      "Epoch: 80 Train loss 3.881, Test loss 3.880\n",
      "1 580\n",
      "Epoch: 81 Train loss 3.880, Test loss 3.880\n",
      "1 580\n",
      "Epoch: 82 Train loss 3.880, Test loss 3.879\n",
      "1 580\n",
      "Epoch: 83 Train loss 3.879, Test loss 3.879\n",
      "1 580\n",
      "Epoch: 84 Train loss 3.879, Test loss 3.878\n",
      "1 580\n",
      "Epoch: 85 Train loss 3.878, Test loss 3.877\n",
      "1 580\n",
      "Epoch: 86 Train loss 3.877, Test loss 3.877\n",
      "1 580\n",
      "Epoch: 87 Train loss 3.877, Test loss 3.876\n",
      "1 580\n",
      "Epoch: 88 Train loss 3.876, Test loss 3.875\n",
      "1 580\n",
      "Epoch: 89 Train loss 3.875, Test loss 3.875\n",
      "1 580\n",
      "Epoch: 90 Train loss 3.875, Test loss 3.874\n",
      "1 580\n",
      "Epoch: 91 Train loss 3.874, Test loss 3.873\n",
      "1 580\n",
      "Epoch: 92 Train loss 3.873, Test loss 3.873\n",
      "1 580\n",
      "Epoch: 93 Train loss 3.873, Test loss 3.872\n",
      "1 580\n",
      "Epoch: 94 Train loss 3.872, Test loss 3.871\n",
      "1 580\n",
      "Epoch: 95 Train loss 3.871, Test loss 3.871\n",
      "1 580\n",
      "Epoch: 96 Train loss 3.871, Test loss 3.870\n",
      "1 580\n",
      "Epoch: 97 Train loss 3.870, Test loss 3.869\n",
      "1 580\n",
      "Epoch: 98 Train loss 3.869, Test loss 3.869\n",
      "1 580\n",
      "Epoch: 99 Train loss 3.869, Test loss 3.868\n",
      "Mean train loss: 3.903 Mean val score: 3.902\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   2. -1. -1. -1.  2.  2. -1. -1.  1.  3. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 15.  8.  0.]] [[-0.24445422]] False\n",
      "Step 81: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1. -1. -1. -1.  0.\n",
      "   2. -2.  2. -2.  5.  2.  1. -2.  0.  9.  0.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 15.  8.  1.]] [[-0.18074393]] False\n",
      "Step 82: Reward 0.100.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1. -1. -1.\n",
      "   1. -2.  1. -1. 12.  0. -1. -2.  3.  1.  3. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -1. 15.  8.  2.]] [[-0.06696802]] False\n",
      "Step 83: Reward 0.248.\n",
      "[[-1.  0.  3. -1.  0. -1. -1. -1. -1.  2. -1. -1.  4. -1.  0. -1.  3. -1.\n",
      "   2. -2.  0. -1. 11.  4. -1.  1.  0. -1.  2. -1.  0. -1. -1. -1. -1.  1.\n",
      "  -1. -1. -1.  2. -1. -2. -1. 15.  8.  3.]] [[0.07123348]] False\n",
      "Step 84: Reward 0.400.\n",
      "[[-1.  2.  1. -1.  0. -1. -1. -1.  1. 10.  0. -1. -1. -1.  1. -1.  2. -1.\n",
      "   2. -2.  0. -1. 12.  1.  0.  5.  4. -1. -2.  0. -1. -1. -1. -1. -1.  2.\n",
      "  -2. -1. -1.  4.  0. -2. -1. 15.  8.  4.]] [[0.12353748]] False\n",
      "Step 85: Reward 0.472.\n",
      "[[-1.  2.  1.  0.  0. -1.  0.  0. -1.  8. -1. -2.  1. -1.  1. -1.  1. -1.\n",
      "   1. -2.  0. -3. 12.  0. -1.  9.  6.  0. -1. -1.  3.  0. -2. -1. -1.  1.\n",
      "   1.  0. -2.  8. -1.  2. -1. 15.  8.  4.]] [[0.12321711]] False\n",
      "Step 86: Reward 0.505.\n",
      "[[-1.  4. -1. -1.  0. -1. -1.  0.  2.  5. -2. -2.  2. -1. -1. -2.  1.  0.\n",
      "   3. -3.  0. -1. 13.  2.  2.  9.  9.  4. -2.  4.  3. -1. -2.  1. -1.  0.\n",
      "  -2. -1.  0.  9. -1.  1. -1. 15.  8.  5.]] [[0.15349674]] False\n",
      "Step 87: Reward 0.561.\n",
      "[[ 2.  4.  1. -1.  0. -1. -1.  1.  0.  4. -2. -2.  2. -1. -1. -2.  1.  0.\n",
      "   3. -3.  0. -1. 14.  6.  3. 16.  9.  4.  4.  4.  2. -1. -2.  3. -1.  0.\n",
      "  -3. -2.  0.  8. -1.  1. -1. 15.  8.  6.]] [[0.18601981]] True\n",
      "Step 88: Reward 0.636.\n",
      "Trial: 11, reward: 2.922073916737468.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  1. -1. -1. -1.  5.  2.  2. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 11.  5.  0.]] [[-0.2451632]] False\n",
      "Step 89: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1.  0. -1.  1. -1. -1. -1. -1.  0.\n",
      "   0.  4.  2. -2. -2.  9. -1.  1. -2. -1. -1.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -1. -1. 11.  5.  1.]] [[-0.06361301]] False\n",
      "Step 90: Reward 0.216.\n",
      "1 590\n",
      "Epoch: 0 Train loss 3.950, Test loss 3.949\n",
      "1 590\n",
      "Epoch: 1 Train loss 3.949, Test loss 3.949\n",
      "1 590\n",
      "Epoch: 2 Train loss 3.949, Test loss 3.948\n",
      "1 590\n",
      "Epoch: 3 Train loss 3.948, Test loss 3.947\n",
      "1 590\n",
      "Epoch: 4 Train loss 3.947, Test loss 3.946\n",
      "1 590\n",
      "Epoch: 5 Train loss 3.946, Test loss 3.945\n",
      "1 590\n",
      "Epoch: 6 Train loss 3.945, Test loss 3.945\n",
      "1 590\n",
      "Epoch: 7 Train loss 3.945, Test loss 3.944\n",
      "1 590\n",
      "Epoch: 8 Train loss 3.944, Test loss 3.943\n",
      "1 590\n",
      "Epoch: 9 Train loss 3.943, Test loss 3.942\n",
      "1 590\n",
      "Epoch: 10 Train loss 3.942, Test loss 3.942\n",
      "1 590\n",
      "Epoch: 11 Train loss 3.942, Test loss 3.941\n",
      "1 590\n",
      "Epoch: 12 Train loss 3.941, Test loss 3.940\n",
      "1 590\n",
      "Epoch: 13 Train loss 3.940, Test loss 3.939\n",
      "1 590\n",
      "Epoch: 14 Train loss 3.939, Test loss 3.939\n",
      "1 590\n",
      "Epoch: 15 Train loss 3.939, Test loss 3.938\n",
      "1 590\n",
      "Epoch: 16 Train loss 3.938, Test loss 3.937\n",
      "1 590\n",
      "Epoch: 17 Train loss 3.937, Test loss 3.936\n",
      "1 590\n",
      "Epoch: 18 Train loss 3.936, Test loss 3.936\n",
      "1 590\n",
      "Epoch: 19 Train loss 3.936, Test loss 3.935\n",
      "1 590\n",
      "Epoch: 20 Train loss 3.935, Test loss 3.934\n",
      "1 590\n",
      "Epoch: 21 Train loss 3.934, Test loss 3.933\n",
      "1 590\n",
      "Epoch: 22 Train loss 3.933, Test loss 3.933\n",
      "1 590\n",
      "Epoch: 23 Train loss 3.933, Test loss 3.932\n",
      "1 590\n",
      "Epoch: 24 Train loss 3.932, Test loss 3.931\n",
      "1 590\n",
      "Epoch: 25 Train loss 3.931, Test loss 3.931\n",
      "1 590\n",
      "Epoch: 26 Train loss 3.931, Test loss 3.930\n",
      "1 590\n",
      "Epoch: 27 Train loss 3.930, Test loss 3.929\n",
      "1 590\n",
      "Epoch: 28 Train loss 3.929, Test loss 3.928\n",
      "1 590\n",
      "Epoch: 29 Train loss 3.928, Test loss 3.928\n",
      "1 590\n",
      "Epoch: 30 Train loss 3.928, Test loss 3.927\n",
      "1 590\n",
      "Epoch: 31 Train loss 3.927, Test loss 3.926\n",
      "1 590\n",
      "Epoch: 32 Train loss 3.926, Test loss 3.926\n",
      "1 590\n",
      "Epoch: 33 Train loss 3.926, Test loss 3.925\n",
      "1 590\n",
      "Epoch: 34 Train loss 3.925, Test loss 3.924\n",
      "1 590\n",
      "Epoch: 35 Train loss 3.924, Test loss 3.923\n",
      "1 590\n",
      "Epoch: 36 Train loss 3.923, Test loss 3.923\n",
      "1 590\n",
      "Epoch: 37 Train loss 3.923, Test loss 3.922\n",
      "1 590\n",
      "Epoch: 38 Train loss 3.922, Test loss 3.921\n",
      "1 590\n",
      "Epoch: 39 Train loss 3.921, Test loss 3.921\n",
      "1 590\n",
      "Epoch: 40 Train loss 3.921, Test loss 3.920\n",
      "1 590\n",
      "Epoch: 41 Train loss 3.920, Test loss 3.919\n",
      "1 590\n",
      "Epoch: 42 Train loss 3.919, Test loss 3.919\n",
      "1 590\n",
      "Epoch: 43 Train loss 3.919, Test loss 3.918\n",
      "1 590\n",
      "Epoch: 44 Train loss 3.918, Test loss 3.917\n",
      "1 590\n",
      "Epoch: 45 Train loss 3.917, Test loss 3.916\n",
      "1 590\n",
      "Epoch: 46 Train loss 3.916, Test loss 3.916\n",
      "1 590\n",
      "Epoch: 47 Train loss 3.916, Test loss 3.915\n",
      "1 590\n",
      "Epoch: 48 Train loss 3.915, Test loss 3.914\n",
      "1 590\n",
      "Epoch: 49 Train loss 3.914, Test loss 3.914\n",
      "1 590\n",
      "Epoch: 50 Train loss 3.914, Test loss 3.913\n",
      "1 590\n",
      "Epoch: 51 Train loss 3.913, Test loss 3.912\n",
      "1 590\n",
      "Epoch: 52 Train loss 3.912, Test loss 3.912\n",
      "1 590\n",
      "Epoch: 53 Train loss 3.912, Test loss 3.911\n",
      "1 590\n",
      "Epoch: 54 Train loss 3.911, Test loss 3.910\n",
      "1 590\n",
      "Epoch: 55 Train loss 3.910, Test loss 3.910\n",
      "1 590\n",
      "Epoch: 56 Train loss 3.910, Test loss 3.909\n",
      "1 590\n",
      "Epoch: 57 Train loss 3.909, Test loss 3.908\n",
      "1 590\n",
      "Epoch: 58 Train loss 3.908, Test loss 3.908\n",
      "1 590\n",
      "Epoch: 59 Train loss 3.908, Test loss 3.907\n",
      "1 590\n",
      "Epoch: 60 Train loss 3.907, Test loss 3.906\n",
      "1 590\n",
      "Epoch: 61 Train loss 3.906, Test loss 3.906\n",
      "1 590\n",
      "Epoch: 62 Train loss 3.906, Test loss 3.905\n",
      "1 590\n",
      "Epoch: 63 Train loss 3.905, Test loss 3.904\n",
      "1 590\n",
      "Epoch: 64 Train loss 3.904, Test loss 3.904\n",
      "1 590\n",
      "Epoch: 65 Train loss 3.904, Test loss 3.903\n",
      "1 590\n",
      "Epoch: 66 Train loss 3.903, Test loss 3.902\n",
      "1 590\n",
      "Epoch: 67 Train loss 3.902, Test loss 3.902\n",
      "1 590\n",
      "Epoch: 68 Train loss 3.902, Test loss 3.901\n",
      "1 590\n",
      "Epoch: 69 Train loss 3.901, Test loss 3.900\n",
      "1 590\n",
      "Epoch: 70 Train loss 3.900, Test loss 3.900\n",
      "1 590\n",
      "Epoch: 71 Train loss 3.900, Test loss 3.899\n",
      "1 590\n",
      "Epoch: 72 Train loss 3.899, Test loss 3.898\n",
      "1 590\n",
      "Epoch: 73 Train loss 3.898, Test loss 3.898\n",
      "1 590\n",
      "Epoch: 74 Train loss 3.898, Test loss 3.897\n",
      "1 590\n",
      "Epoch: 75 Train loss 3.897, Test loss 3.896\n",
      "1 590\n",
      "Epoch: 76 Train loss 3.896, Test loss 3.896\n",
      "1 590\n",
      "Epoch: 77 Train loss 3.896, Test loss 3.895\n",
      "1 590\n",
      "Epoch: 78 Train loss 3.895, Test loss 3.895\n",
      "1 590\n",
      "Epoch: 79 Train loss 3.895, Test loss 3.894\n",
      "1 590\n",
      "Epoch: 80 Train loss 3.894, Test loss 3.893\n",
      "1 590\n",
      "Epoch: 81 Train loss 3.893, Test loss 3.893\n",
      "1 590\n",
      "Epoch: 82 Train loss 3.893, Test loss 3.892\n",
      "1 590\n",
      "Epoch: 83 Train loss 3.892, Test loss 3.891\n",
      "1 590\n",
      "Epoch: 84 Train loss 3.891, Test loss 3.891\n",
      "1 590\n",
      "Epoch: 85 Train loss 3.891, Test loss 3.890\n",
      "1 590\n",
      "Epoch: 86 Train loss 3.890, Test loss 3.889\n",
      "1 590\n",
      "Epoch: 87 Train loss 3.889, Test loss 3.889\n",
      "1 590\n",
      "Epoch: 88 Train loss 3.889, Test loss 3.888\n",
      "1 590\n",
      "Epoch: 89 Train loss 3.888, Test loss 3.888\n",
      "1 590\n",
      "Epoch: 90 Train loss 3.888, Test loss 3.887\n",
      "1 590\n",
      "Epoch: 91 Train loss 3.887, Test loss 3.886\n",
      "1 590\n",
      "Epoch: 92 Train loss 3.886, Test loss 3.886\n",
      "1 590\n",
      "Epoch: 93 Train loss 3.886, Test loss 3.885\n",
      "1 590\n",
      "Epoch: 94 Train loss 3.885, Test loss 3.884\n",
      "1 590\n",
      "Epoch: 95 Train loss 3.884, Test loss 3.884\n",
      "1 590\n",
      "Epoch: 96 Train loss 3.884, Test loss 3.883\n",
      "1 590\n",
      "Epoch: 97 Train loss 3.883, Test loss 3.883\n",
      "1 590\n",
      "Epoch: 98 Train loss 3.883, Test loss 3.882\n",
      "1 590\n",
      "Epoch: 99 Train loss 3.882, Test loss 3.881\n",
      "Mean train loss: 3.915 Mean val score: 3.914\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0.  0. -1.  1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   0.  3.  2. -1.  1. 12.  0.  5.  1. -1. -1.  2. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1. 11.  5.  2.]] [[-0.12450346]] False\n",
      "Step 91: Reward 0.172.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  -1.  2.  1.  0.  0. 15. -1. 12.  3. -1. -2. -1. -1.  1. -1. -1. -1.  0.\n",
      "  -2.  1. -1. -1. -1. -2. -2. 11.  5.  3.]] [[-0.15476017]] False\n",
      "Step 92: Reward 0.176.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  3.  0. -1.  0.  0. -1. -1. -1.  0.\n",
      "   0.  1. -2.  1.  2. 14. -1. 13.  8. -1.  0. -1. -1.  1. -1.  0. -1. -1.\n",
      "   1.  2. -2. -1. -2. -2. -2. 11.  5.  4.]] [[-0.09058315]] False\n",
      "Step 93: Reward 0.256.\n",
      "[[-1. -1. -1. -1.  0. -1.  0. -1.  2.  8.  3. -2.  1. -1. -1.  0. -2. -1.\n",
      "   3.  2. -2.  0.  1. 15.  1. 16.  7.  1.  4.  2. -1.  0. -1. -1.  0. -1.\n",
      "   0. -1. -2. -1. -1. -2. -2. 11.  5.  4.]] [[0.0312109]] False\n",
      "Step 94: Reward 0.411.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1. -1.  7.  2. -2. -1. -1. -2. -1. -2. -1.\n",
      "   2.  3. -1.  1.  1. 18. -1. 19. 13.  7.  7.  3. -1.  0. -2. -1.  0. -1.\n",
      "  -1. -1. -2. -1. -2. -2. -2. 11.  5.  5.]] [[-0.01000321]] False\n",
      "Step 95: Reward 0.408.\n",
      "[[ 0. -1.  1. -1.  0. -1.  0. -1.  0.  6.  3.  2. -1. -1. -2. -1. -1. -1.\n",
      "   2.  1. -3.  1.  0. 18. -1. 19. 21.  9. 11. -1. -1.  0. -2. -1.  0. -1.\n",
      "  -2.  0. -2. -1. -2. -3. -2. 11.  5.  6.]] [[0.01651454]] True\n",
      "Step 96: Reward 0.469.\n",
      "Trial: 12, reward: 2.1074360553794484.\n",
      "[[ 0.  0.  1.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  1.  2.  1. -2.  2.  0. -1. -2.  0. -1.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  8.  6.  0.]] [[-0.00071952]] False\n",
      "Step 97: Reward 0.250.\n",
      "[[ 0.  0.  1.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1.  0.  0.\n",
      "  -1.  6.  0.  3.  2.  5. -1. -2.  1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -1. -1.  8.  6.  1.]] [[0.03141814]] False\n",
      "Step 98: Reward 0.300.\n",
      "[[-1.  0. -1. -1.  1. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "   6.  5.  0.  7.  2.  5. -1. -2. -1. -1. -2.  4. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -1.  8.  6.  2.]] [[0.03474838]] False\n",
      "Step 99: Reward 0.333.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1.  1. -1. -1. -1.\n",
      "   5.  3.  0.  6.  0.  4. -1.  2.  3.  3.  0. -1. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -1. -1. -2. -2. -2.  8.  6.  3.]] [[-0.03397548]] False\n",
      "Step 100: Reward 0.297.\n",
      "1 600\n",
      "Epoch: 0 Train loss 4.026, Test loss 4.026\n",
      "1 600\n",
      "Epoch: 1 Train loss 4.026, Test loss 4.025\n",
      "1 600\n",
      "Epoch: 2 Train loss 4.025, Test loss 4.024\n",
      "1 600\n",
      "Epoch: 3 Train loss 4.024, Test loss 4.023\n",
      "1 600\n",
      "Epoch: 4 Train loss 4.023, Test loss 4.022\n",
      "1 600\n",
      "Epoch: 5 Train loss 4.022, Test loss 4.021\n",
      "1 600\n",
      "Epoch: 6 Train loss 4.021, Test loss 4.020\n",
      "1 600\n",
      "Epoch: 7 Train loss 4.020, Test loss 4.020\n",
      "1 600\n",
      "Epoch: 8 Train loss 4.020, Test loss 4.019\n",
      "1 600\n",
      "Epoch: 9 Train loss 4.019, Test loss 4.018\n",
      "1 600\n",
      "Epoch: 10 Train loss 4.018, Test loss 4.017\n",
      "1 600\n",
      "Epoch: 11 Train loss 4.017, Test loss 4.016\n",
      "1 600\n",
      "Epoch: 12 Train loss 4.016, Test loss 4.015\n",
      "1 600\n",
      "Epoch: 13 Train loss 4.015, Test loss 4.015\n",
      "1 600\n",
      "Epoch: 14 Train loss 4.015, Test loss 4.014\n",
      "1 600\n",
      "Epoch: 15 Train loss 4.014, Test loss 4.013\n",
      "1 600\n",
      "Epoch: 16 Train loss 4.013, Test loss 4.012\n",
      "1 600\n",
      "Epoch: 17 Train loss 4.012, Test loss 4.011\n",
      "1 600\n",
      "Epoch: 18 Train loss 4.011, Test loss 4.011\n",
      "1 600\n",
      "Epoch: 19 Train loss 4.011, Test loss 4.010\n",
      "1 600\n",
      "Epoch: 20 Train loss 4.010, Test loss 4.009\n",
      "1 600\n",
      "Epoch: 21 Train loss 4.009, Test loss 4.008\n",
      "1 600\n",
      "Epoch: 22 Train loss 4.008, Test loss 4.007\n",
      "1 600\n",
      "Epoch: 23 Train loss 4.007, Test loss 4.007\n",
      "1 600\n",
      "Epoch: 24 Train loss 4.007, Test loss 4.006\n",
      "1 600\n",
      "Epoch: 25 Train loss 4.006, Test loss 4.005\n",
      "1 600\n",
      "Epoch: 26 Train loss 4.005, Test loss 4.004\n",
      "1 600\n",
      "Epoch: 27 Train loss 4.004, Test loss 4.003\n",
      "1 600\n",
      "Epoch: 28 Train loss 4.003, Test loss 4.003\n",
      "1 600\n",
      "Epoch: 29 Train loss 4.003, Test loss 4.002\n",
      "1 600\n",
      "Epoch: 30 Train loss 4.002, Test loss 4.001\n",
      "1 600\n",
      "Epoch: 31 Train loss 4.001, Test loss 4.000\n",
      "1 600\n",
      "Epoch: 32 Train loss 4.000, Test loss 3.999\n",
      "1 600\n",
      "Epoch: 33 Train loss 3.999, Test loss 3.999\n",
      "1 600\n",
      "Epoch: 34 Train loss 3.999, Test loss 3.998\n",
      "1 600\n",
      "Epoch: 35 Train loss 3.998, Test loss 3.997\n",
      "1 600\n",
      "Epoch: 36 Train loss 3.997, Test loss 3.996\n",
      "1 600\n",
      "Epoch: 37 Train loss 3.996, Test loss 3.996\n",
      "1 600\n",
      "Epoch: 38 Train loss 3.996, Test loss 3.995\n",
      "1 600\n",
      "Epoch: 39 Train loss 3.995, Test loss 3.994\n",
      "1 600\n",
      "Epoch: 40 Train loss 3.994, Test loss 3.993\n",
      "1 600\n",
      "Epoch: 41 Train loss 3.993, Test loss 3.992\n",
      "1 600\n",
      "Epoch: 42 Train loss 3.992, Test loss 3.992\n",
      "1 600\n",
      "Epoch: 43 Train loss 3.992, Test loss 3.991\n",
      "1 600\n",
      "Epoch: 44 Train loss 3.991, Test loss 3.990\n",
      "1 600\n",
      "Epoch: 45 Train loss 3.990, Test loss 3.989\n",
      "1 600\n",
      "Epoch: 46 Train loss 3.989, Test loss 3.989\n",
      "1 600\n",
      "Epoch: 47 Train loss 3.989, Test loss 3.988\n",
      "1 600\n",
      "Epoch: 48 Train loss 3.988, Test loss 3.987\n",
      "1 600\n",
      "Epoch: 49 Train loss 3.987, Test loss 3.986\n",
      "1 600\n",
      "Epoch: 50 Train loss 3.986, Test loss 3.986\n",
      "1 600\n",
      "Epoch: 51 Train loss 3.986, Test loss 3.985\n",
      "1 600\n",
      "Epoch: 52 Train loss 3.985, Test loss 3.984\n",
      "1 600\n",
      "Epoch: 53 Train loss 3.984, Test loss 3.983\n",
      "1 600\n",
      "Epoch: 54 Train loss 3.983, Test loss 3.983\n",
      "1 600\n",
      "Epoch: 55 Train loss 3.983, Test loss 3.982\n",
      "1 600\n",
      "Epoch: 56 Train loss 3.982, Test loss 3.981\n",
      "1 600\n",
      "Epoch: 57 Train loss 3.981, Test loss 3.980\n",
      "1 600\n",
      "Epoch: 58 Train loss 3.980, Test loss 3.980\n",
      "1 600\n",
      "Epoch: 59 Train loss 3.980, Test loss 3.979\n",
      "1 600\n",
      "Epoch: 60 Train loss 3.979, Test loss 3.978\n",
      "1 600\n",
      "Epoch: 61 Train loss 3.978, Test loss 3.978\n",
      "1 600\n",
      "Epoch: 62 Train loss 3.978, Test loss 3.977\n",
      "1 600\n",
      "Epoch: 63 Train loss 3.977, Test loss 3.976\n",
      "1 600\n",
      "Epoch: 64 Train loss 3.976, Test loss 3.975\n",
      "1 600\n",
      "Epoch: 65 Train loss 3.975, Test loss 3.975\n",
      "1 600\n",
      "Epoch: 66 Train loss 3.975, Test loss 3.974\n",
      "1 600\n",
      "Epoch: 67 Train loss 3.974, Test loss 3.973\n",
      "1 600\n",
      "Epoch: 68 Train loss 3.973, Test loss 3.972\n",
      "1 600\n",
      "Epoch: 69 Train loss 3.972, Test loss 3.972\n",
      "1 600\n",
      "Epoch: 70 Train loss 3.972, Test loss 3.971\n",
      "1 600\n",
      "Epoch: 71 Train loss 3.971, Test loss 3.970\n",
      "1 600\n",
      "Epoch: 72 Train loss 3.970, Test loss 3.970\n",
      "1 600\n",
      "Epoch: 73 Train loss 3.970, Test loss 3.969\n",
      "1 600\n",
      "Epoch: 74 Train loss 3.969, Test loss 3.968\n",
      "1 600\n",
      "Epoch: 75 Train loss 3.968, Test loss 3.967\n",
      "1 600\n",
      "Epoch: 76 Train loss 3.967, Test loss 3.967\n",
      "1 600\n",
      "Epoch: 77 Train loss 3.967, Test loss 3.966\n",
      "1 600\n",
      "Epoch: 78 Train loss 3.966, Test loss 3.965\n",
      "1 600\n",
      "Epoch: 79 Train loss 3.965, Test loss 3.965\n",
      "1 600\n",
      "Epoch: 80 Train loss 3.965, Test loss 3.964\n",
      "1 600\n",
      "Epoch: 81 Train loss 3.964, Test loss 3.963\n",
      "1 600\n",
      "Epoch: 82 Train loss 3.963, Test loss 3.962\n",
      "1 600\n",
      "Epoch: 83 Train loss 3.962, Test loss 3.962\n",
      "1 600\n",
      "Epoch: 84 Train loss 3.962, Test loss 3.961\n",
      "1 600\n",
      "Epoch: 85 Train loss 3.961, Test loss 3.960\n",
      "1 600\n",
      "Epoch: 86 Train loss 3.960, Test loss 3.960\n",
      "1 600\n",
      "Epoch: 87 Train loss 3.960, Test loss 3.959\n",
      "1 600\n",
      "Epoch: 88 Train loss 3.959, Test loss 3.958\n",
      "1 600\n",
      "Epoch: 89 Train loss 3.958, Test loss 3.958\n",
      "1 600\n",
      "Epoch: 90 Train loss 3.958, Test loss 3.957\n",
      "1 600\n",
      "Epoch: 91 Train loss 3.957, Test loss 3.956\n",
      "1 600\n",
      "Epoch: 92 Train loss 3.956, Test loss 3.955\n",
      "1 600\n",
      "Epoch: 93 Train loss 3.955, Test loss 3.955\n",
      "1 600\n",
      "Epoch: 94 Train loss 3.955, Test loss 3.954\n",
      "1 600\n",
      "Epoch: 95 Train loss 3.954, Test loss 3.953\n",
      "1 600\n",
      "Epoch: 96 Train loss 3.953, Test loss 3.953\n",
      "1 600\n",
      "Epoch: 97 Train loss 3.953, Test loss 3.952\n",
      "1 600\n",
      "Epoch: 98 Train loss 3.952, Test loss 3.951\n",
      "1 600\n",
      "Epoch: 99 Train loss 3.951, Test loss 3.951\n",
      "Mean train loss: 3.987 Mean val score: 3.987\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1. -2. -1.\n",
      "   6.  5.  1.  6. -3.  8. -1.  6.  1.  0. -2. -1. -1.  0. -1. -1. -1.  0.\n",
      "   0.  0. -1. -1. -1. -2. -2.  8.  6.  4.]] [[-0.08066237]] False\n",
      "Step 101: Reward 0.268.\n",
      "[[-1.  0.  0.  0.  1. -1. -1. -1. -1. -1. -1. -2. -1. -1.  2.  0.  2. -1.\n",
      "   5.  6.  3. 13.  0.  8. -1.  5.  5.  0.  2.  0. -1.  0.  0. -1. -1.  0.\n",
      "   0. -1.  0.  1. -2. -2. -2.  8.  6.  4.]] [[-0.0508391]] False\n",
      "Step 102: Reward 0.309.\n",
      "[[ 1.  3. -1. -1.  0.  0.  0. -1. -1.  4.  1. -2. -1. -1.  1. -2.  2. -1.\n",
      "   5.  6.  5. 10. -1.  7. -1.  5.  4.  1. 13.  0. -1. -1. -2. -2. -1. -1.\n",
      "  -1. -2.  1.  0. -3. -1. -2.  8.  6.  5.]] [[0.04249898]] False\n",
      "Step 103: Reward 0.436.\n",
      "[[-1.  4.  1.  0.  2. -1. -1. -1. -1.  8.  0. -2.  0. -1.  1. -2.  2. -1.\n",
      "   5.  5.  4. 15. -2.  4.  0.  5.  3.  7. 12.  7.  3. -1. -2. -2. -1. -1.\n",
      "  -1. -1.  2.  0. -3. -2. -2.  8.  6.  6.]] [[0.1148738]] True\n",
      "Step 104: Reward 0.550.\n",
      "Trial: 13, reward: 2.7439083829230277.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1.  2. -1. -1.  2.  2. -1.  5. -2.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 11.  5.  0.]] [[-0.24878432]] False\n",
      "Step 105: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0.  2. -1. -1.  0.\n",
      "  -1.  1. -1. -2.  8.  2. -1.  7.  2. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 11.  5.  1.]] [[-0.27464646]] False\n",
      "Step 106: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1.  0. -1.  0. -1.  2. -1. -1. -1.\n",
      "  -2.  0. -2.  2.  6.  5. -1. 10.  4. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1. 11.  5.  2.]] [[-0.20663472]] False\n",
      "Step 107: Reward 0.096.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1.  0. -1. -1.  0. -1.  0. -1. -2. -1.\n",
      "  -2. -1. -2.  6.  9.  4. -1.  9. 11. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2.  1. -1. -1. -2. -2. -2. 11.  5.  3.]] [[-0.20026138]] False\n",
      "Step 108: Reward 0.124.\n",
      "[[-1.  1. -1. -1.  0. -1.  0. -1. -1.  2. -1. -2. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -2. -2.  8.  8.  3. -1. 10. 20. -1. -1. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0.  2.  0. -1. -2. -2. -2. 11.  5.  4.]] [[-0.10362005]] False\n",
      "Step 109: Reward 0.253.\n",
      "[[ 0.  0. -1. -1.  0. -1.  0. -1.  2.  2.  1. -2.  0. -1. -1.  0. -2. -1.\n",
      "  -2. -1. -2.  6. 10.  7.  2. 10. 19. -1. -2.  1.  0.  2. -1. -2.  0. -2.\n",
      "  -2. -1.  0. -1. -2. -2. -2. 11.  5.  4.]] [[-0.01025349]] False\n",
      "Step 110: Reward 0.377.\n",
      "1 610\n",
      "Epoch: 0 Train loss 4.073, Test loss 4.072\n",
      "1 610\n",
      "Epoch: 1 Train loss 4.072, Test loss 4.071\n",
      "1 610\n",
      "Epoch: 2 Train loss 4.071, Test loss 4.070\n",
      "1 610\n",
      "Epoch: 3 Train loss 4.070, Test loss 4.069\n",
      "1 610\n",
      "Epoch: 4 Train loss 4.069, Test loss 4.069\n",
      "1 610\n",
      "Epoch: 5 Train loss 4.069, Test loss 4.068\n",
      "1 610\n",
      "Epoch: 6 Train loss 4.068, Test loss 4.067\n",
      "1 610\n",
      "Epoch: 7 Train loss 4.067, Test loss 4.066\n",
      "1 610\n",
      "Epoch: 8 Train loss 4.066, Test loss 4.065\n",
      "1 610\n",
      "Epoch: 9 Train loss 4.065, Test loss 4.064\n",
      "1 610\n",
      "Epoch: 10 Train loss 4.064, Test loss 4.063\n",
      "1 610\n",
      "Epoch: 11 Train loss 4.063, Test loss 4.062\n",
      "1 610\n",
      "Epoch: 12 Train loss 4.062, Test loss 4.061\n",
      "1 610\n",
      "Epoch: 13 Train loss 4.061, Test loss 4.060\n",
      "1 610\n",
      "Epoch: 14 Train loss 4.060, Test loss 4.060\n",
      "1 610\n",
      "Epoch: 15 Train loss 4.060, Test loss 4.059\n",
      "1 610\n",
      "Epoch: 16 Train loss 4.059, Test loss 4.058\n",
      "1 610\n",
      "Epoch: 17 Train loss 4.058, Test loss 4.057\n",
      "1 610\n",
      "Epoch: 18 Train loss 4.057, Test loss 4.056\n",
      "1 610\n",
      "Epoch: 19 Train loss 4.056, Test loss 4.055\n",
      "1 610\n",
      "Epoch: 20 Train loss 4.055, Test loss 4.054\n",
      "1 610\n",
      "Epoch: 21 Train loss 4.054, Test loss 4.053\n",
      "1 610\n",
      "Epoch: 22 Train loss 4.053, Test loss 4.053\n",
      "1 610\n",
      "Epoch: 23 Train loss 4.053, Test loss 4.052\n",
      "1 610\n",
      "Epoch: 24 Train loss 4.052, Test loss 4.051\n",
      "1 610\n",
      "Epoch: 25 Train loss 4.051, Test loss 4.050\n",
      "1 610\n",
      "Epoch: 26 Train loss 4.050, Test loss 4.049\n",
      "1 610\n",
      "Epoch: 27 Train loss 4.049, Test loss 4.048\n",
      "1 610\n",
      "Epoch: 28 Train loss 4.048, Test loss 4.047\n",
      "1 610\n",
      "Epoch: 29 Train loss 4.047, Test loss 4.047\n",
      "1 610\n",
      "Epoch: 30 Train loss 4.047, Test loss 4.046\n",
      "1 610\n",
      "Epoch: 31 Train loss 4.046, Test loss 4.045\n",
      "1 610\n",
      "Epoch: 32 Train loss 4.045, Test loss 4.044\n",
      "1 610\n",
      "Epoch: 33 Train loss 4.044, Test loss 4.043\n",
      "1 610\n",
      "Epoch: 34 Train loss 4.043, Test loss 4.042\n",
      "1 610\n",
      "Epoch: 35 Train loss 4.042, Test loss 4.042\n",
      "1 610\n",
      "Epoch: 36 Train loss 4.042, Test loss 4.041\n",
      "1 610\n",
      "Epoch: 37 Train loss 4.041, Test loss 4.040\n",
      "1 610\n",
      "Epoch: 38 Train loss 4.040, Test loss 4.039\n",
      "1 610\n",
      "Epoch: 39 Train loss 4.039, Test loss 4.038\n",
      "1 610\n",
      "Epoch: 40 Train loss 4.038, Test loss 4.037\n",
      "1 610\n",
      "Epoch: 41 Train loss 4.037, Test loss 4.037\n",
      "1 610\n",
      "Epoch: 42 Train loss 4.037, Test loss 4.036\n",
      "1 610\n",
      "Epoch: 43 Train loss 4.036, Test loss 4.035\n",
      "1 610\n",
      "Epoch: 44 Train loss 4.035, Test loss 4.034\n",
      "1 610\n",
      "Epoch: 45 Train loss 4.034, Test loss 4.033\n",
      "1 610\n",
      "Epoch: 46 Train loss 4.033, Test loss 4.033\n",
      "1 610\n",
      "Epoch: 47 Train loss 4.033, Test loss 4.032\n",
      "1 610\n",
      "Epoch: 48 Train loss 4.032, Test loss 4.031\n",
      "1 610\n",
      "Epoch: 49 Train loss 4.031, Test loss 4.030\n",
      "1 610\n",
      "Epoch: 50 Train loss 4.030, Test loss 4.029\n",
      "1 610\n",
      "Epoch: 51 Train loss 4.029, Test loss 4.028\n",
      "1 610\n",
      "Epoch: 52 Train loss 4.028, Test loss 4.028\n",
      "1 610\n",
      "Epoch: 53 Train loss 4.028, Test loss 4.027\n",
      "1 610\n",
      "Epoch: 54 Train loss 4.027, Test loss 4.026\n",
      "1 610\n",
      "Epoch: 55 Train loss 4.026, Test loss 4.025\n",
      "1 610\n",
      "Epoch: 56 Train loss 4.025, Test loss 4.024\n",
      "1 610\n",
      "Epoch: 57 Train loss 4.024, Test loss 4.024\n",
      "1 610\n",
      "Epoch: 58 Train loss 4.024, Test loss 4.023\n",
      "1 610\n",
      "Epoch: 59 Train loss 4.023, Test loss 4.022\n",
      "1 610\n",
      "Epoch: 60 Train loss 4.022, Test loss 4.021\n",
      "1 610\n",
      "Epoch: 61 Train loss 4.021, Test loss 4.021\n",
      "1 610\n",
      "Epoch: 62 Train loss 4.021, Test loss 4.020\n",
      "1 610\n",
      "Epoch: 63 Train loss 4.020, Test loss 4.019\n",
      "1 610\n",
      "Epoch: 64 Train loss 4.019, Test loss 4.018\n",
      "1 610\n",
      "Epoch: 65 Train loss 4.018, Test loss 4.017\n",
      "1 610\n",
      "Epoch: 66 Train loss 4.017, Test loss 4.017\n",
      "1 610\n",
      "Epoch: 67 Train loss 4.017, Test loss 4.016\n",
      "1 610\n",
      "Epoch: 68 Train loss 4.016, Test loss 4.015\n",
      "1 610\n",
      "Epoch: 69 Train loss 4.015, Test loss 4.014\n",
      "1 610\n",
      "Epoch: 70 Train loss 4.014, Test loss 4.014\n",
      "1 610\n",
      "Epoch: 71 Train loss 4.014, Test loss 4.013\n",
      "1 610\n",
      "Epoch: 72 Train loss 4.013, Test loss 4.012\n",
      "1 610\n",
      "Epoch: 73 Train loss 4.012, Test loss 4.011\n",
      "1 610\n",
      "Epoch: 74 Train loss 4.011, Test loss 4.010\n",
      "1 610\n",
      "Epoch: 75 Train loss 4.010, Test loss 4.010\n",
      "1 610\n",
      "Epoch: 76 Train loss 4.010, Test loss 4.009\n",
      "1 610\n",
      "Epoch: 77 Train loss 4.009, Test loss 4.008\n",
      "1 610\n",
      "Epoch: 78 Train loss 4.008, Test loss 4.007\n",
      "1 610\n",
      "Epoch: 79 Train loss 4.007, Test loss 4.007\n",
      "1 610\n",
      "Epoch: 80 Train loss 4.007, Test loss 4.006\n",
      "1 610\n",
      "Epoch: 81 Train loss 4.006, Test loss 4.005\n",
      "1 610\n",
      "Epoch: 82 Train loss 4.005, Test loss 4.004\n",
      "1 610\n",
      "Epoch: 83 Train loss 4.004, Test loss 4.004\n",
      "1 610\n",
      "Epoch: 84 Train loss 4.004, Test loss 4.003\n",
      "1 610\n",
      "Epoch: 85 Train loss 4.003, Test loss 4.002\n",
      "1 610\n",
      "Epoch: 86 Train loss 4.002, Test loss 4.001\n",
      "1 610\n",
      "Epoch: 87 Train loss 4.001, Test loss 4.001\n",
      "1 610\n",
      "Epoch: 88 Train loss 4.001, Test loss 4.000\n",
      "1 610\n",
      "Epoch: 89 Train loss 4.000, Test loss 3.999\n",
      "1 610\n",
      "Epoch: 90 Train loss 3.999, Test loss 3.998\n",
      "1 610\n",
      "Epoch: 91 Train loss 3.998, Test loss 3.998\n",
      "1 610\n",
      "Epoch: 92 Train loss 3.998, Test loss 3.997\n",
      "1 610\n",
      "Epoch: 93 Train loss 3.997, Test loss 3.996\n",
      "1 610\n",
      "Epoch: 94 Train loss 3.996, Test loss 3.995\n",
      "1 610\n",
      "Epoch: 95 Train loss 3.995, Test loss 3.995\n",
      "1 610\n",
      "Epoch: 96 Train loss 3.995, Test loss 3.994\n",
      "1 610\n",
      "Epoch: 97 Train loss 3.994, Test loss 3.993\n",
      "1 610\n",
      "Epoch: 98 Train loss 3.993, Test loss 3.992\n",
      "1 610\n",
      "Epoch: 99 Train loss 3.992, Test loss 3.992\n",
      "Mean train loss: 4.031 Mean val score: 4.030\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1. -1.  2.  1. -2. -1. -1. -1. -1. -2. -1.\n",
      "  -1. -1.  1.  7.  9. 10. -1.  9. 19.  9.  0.  6. -1.  2. -2. -2.  0. -1.\n",
      "  -2. -1.  0. -1. -2. -3. -2. 11.  5.  5.]] [[0.06103706]] False\n",
      "Step 111: Reward 0.487.\n",
      "[[ 0. -1.  1. -1.  0. -1.  0. -1.  0.  1.  2. -2. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -2.  4.  7.  9. 10. -1. 13. 19. 11.  8.  1. -1.  2. -2. -2.  0. -1.\n",
      "  -3.  0.  0. -1. -2. -3. -2. 11.  5.  6.]] [[0.00776887]] True\n",
      "Step 112: Reward 0.469.\n",
      "Trial: 14, reward: 1.805571480080955.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "   3.  1. -1. -2.  2. -1. -1.  1. -2.  2. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  5.  2.  0.]] [[-0.04258524]] False\n",
      "Step 113: Reward 0.200.\n",
      "[[ 0.  0.  0.  0.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  3.\n",
      "   3. -1. -1. -2. -2. -1. -1.  1.  0. 13. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  5.  2.  1.]] [[0.12196696]] False\n",
      "Step 114: Reward 0.389.\n",
      "[[ 0.  0. -1.  0.  0. -1. -1.  1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  3.\n",
      "   7. -1. -2.  4. -2. -1. -1.  4.  4. 12. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -1. -1.  5.  2.  2.]] [[-0.05701531]] False\n",
      "Step 115: Reward 0.240.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0.  2.\n",
      "   7.  1. -2.  7. -1. -2. -1.  4.  7. 12.  2. -1. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -1. -1. -2. -2. -2.  5.  2.  3.]] [[0.04698777]] False\n",
      "Step 116: Reward 0.380.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1.  0.  2. -1.  1.\n",
      "   7.  1.  2. 10.  2.  0. -1.  4.  5. 17.  2. -1. -1. -1. -1.  1. -1. -1.\n",
      "  -2.  1. -2. -1. -2. -2. -1.  5.  2.  4.]] [[-0.05301404]] False\n",
      "Step 117: Reward 0.312.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1.  1. -1. -2. -2. -1. -1. -1.  1. -1.  1.\n",
      "   6.  1.  2.  9.  4. -1.  0.  7.  8. 28.  1. -1. -1. -1. -1.  1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  5.  2.  4.]] [[0.00176007]] False\n",
      "Step 118: Reward 0.405.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1.  0. -1.  0. -2. -1. -1. -1.  1. -1.  2.\n",
      "   6.  4.  1.  9.  3. -2. -1.  6. 11. 36.  4. -1. -1. -2. -2.  0.  0. -1.\n",
      "  -2. -1. -2. -1. -3. -2.  0.  5.  2.  5.]] [[0.10462153]] False\n",
      "Step 119: Reward 0.545.\n",
      "[[ 0. -1. -1. -1.  0. -1. -1. -1.  0. -2.  0. -2. -1. -1. -2.  1. -1.  2.\n",
      "   6.  4.  1.  8.  3. -3.  8.  6. 18. 39.  4. -1. -1. -2. -2.  1.  0. -1.\n",
      "  -3. -1. -2. -1. -3. -3.  0.  5.  2.  6.]] [[-0.19106644]] True\n",
      "Step 120: Reward 0.286.\n",
      "Trial: 15, reward: 2.75760131496047.\n",
      "1 620\n",
      "Epoch: 0 Train loss 4.269, Test loss 4.268\n",
      "1 620\n",
      "Epoch: 1 Train loss 4.268, Test loss 4.267\n",
      "1 620\n",
      "Epoch: 2 Train loss 4.267, Test loss 4.265\n",
      "1 620\n",
      "Epoch: 3 Train loss 4.265, Test loss 4.264\n",
      "1 620\n",
      "Epoch: 4 Train loss 4.264, Test loss 4.263\n",
      "1 620\n",
      "Epoch: 5 Train loss 4.263, Test loss 4.262\n",
      "1 620\n",
      "Epoch: 6 Train loss 4.262, Test loss 4.261\n",
      "1 620\n",
      "Epoch: 7 Train loss 4.261, Test loss 4.260\n",
      "1 620\n",
      "Epoch: 8 Train loss 4.260, Test loss 4.259\n",
      "1 620\n",
      "Epoch: 9 Train loss 4.259, Test loss 4.258\n",
      "1 620\n",
      "Epoch: 10 Train loss 4.258, Test loss 4.257\n",
      "1 620\n",
      "Epoch: 11 Train loss 4.257, Test loss 4.256\n",
      "1 620\n",
      "Epoch: 12 Train loss 4.256, Test loss 4.255\n",
      "1 620\n",
      "Epoch: 13 Train loss 4.255, Test loss 4.254\n",
      "1 620\n",
      "Epoch: 14 Train loss 4.254, Test loss 4.252\n",
      "1 620\n",
      "Epoch: 15 Train loss 4.252, Test loss 4.251\n",
      "1 620\n",
      "Epoch: 16 Train loss 4.251, Test loss 4.250\n",
      "1 620\n",
      "Epoch: 17 Train loss 4.250, Test loss 4.249\n",
      "1 620\n",
      "Epoch: 18 Train loss 4.249, Test loss 4.248\n",
      "1 620\n",
      "Epoch: 19 Train loss 4.248, Test loss 4.247\n",
      "1 620\n",
      "Epoch: 20 Train loss 4.247, Test loss 4.246\n",
      "1 620\n",
      "Epoch: 21 Train loss 4.246, Test loss 4.245\n",
      "1 620\n",
      "Epoch: 22 Train loss 4.245, Test loss 4.244\n",
      "1 620\n",
      "Epoch: 23 Train loss 4.244, Test loss 4.243\n",
      "1 620\n",
      "Epoch: 24 Train loss 4.243, Test loss 4.242\n",
      "1 620\n",
      "Epoch: 25 Train loss 4.242, Test loss 4.241\n",
      "1 620\n",
      "Epoch: 26 Train loss 4.241, Test loss 4.240\n",
      "1 620\n",
      "Epoch: 27 Train loss 4.240, Test loss 4.239\n",
      "1 620\n",
      "Epoch: 28 Train loss 4.239, Test loss 4.238\n",
      "1 620\n",
      "Epoch: 29 Train loss 4.238, Test loss 4.237\n",
      "1 620\n",
      "Epoch: 30 Train loss 4.237, Test loss 4.236\n",
      "1 620\n",
      "Epoch: 31 Train loss 4.236, Test loss 4.235\n",
      "1 620\n",
      "Epoch: 32 Train loss 4.235, Test loss 4.234\n",
      "1 620\n",
      "Epoch: 33 Train loss 4.234, Test loss 4.233\n",
      "1 620\n",
      "Epoch: 34 Train loss 4.233, Test loss 4.232\n",
      "1 620\n",
      "Epoch: 35 Train loss 4.232, Test loss 4.231\n",
      "1 620\n",
      "Epoch: 36 Train loss 4.231, Test loss 4.230\n",
      "1 620\n",
      "Epoch: 37 Train loss 4.230, Test loss 4.229\n",
      "1 620\n",
      "Epoch: 38 Train loss 4.229, Test loss 4.228\n",
      "1 620\n",
      "Epoch: 39 Train loss 4.228, Test loss 4.227\n",
      "1 620\n",
      "Epoch: 40 Train loss 4.227, Test loss 4.226\n",
      "1 620\n",
      "Epoch: 41 Train loss 4.226, Test loss 4.225\n",
      "1 620\n",
      "Epoch: 42 Train loss 4.225, Test loss 4.224\n",
      "1 620\n",
      "Epoch: 43 Train loss 4.224, Test loss 4.223\n",
      "1 620\n",
      "Epoch: 44 Train loss 4.223, Test loss 4.222\n",
      "1 620\n",
      "Epoch: 45 Train loss 4.222, Test loss 4.221\n",
      "1 620\n",
      "Epoch: 46 Train loss 4.221, Test loss 4.220\n",
      "1 620\n",
      "Epoch: 47 Train loss 4.220, Test loss 4.219\n",
      "1 620\n",
      "Epoch: 48 Train loss 4.219, Test loss 4.218\n",
      "1 620\n",
      "Epoch: 49 Train loss 4.218, Test loss 4.217\n",
      "1 620\n",
      "Epoch: 50 Train loss 4.217, Test loss 4.216\n",
      "1 620\n",
      "Epoch: 51 Train loss 4.216, Test loss 4.215\n",
      "1 620\n",
      "Epoch: 52 Train loss 4.215, Test loss 4.214\n",
      "1 620\n",
      "Epoch: 53 Train loss 4.214, Test loss 4.213\n",
      "1 620\n",
      "Epoch: 54 Train loss 4.213, Test loss 4.212\n",
      "1 620\n",
      "Epoch: 55 Train loss 4.212, Test loss 4.211\n",
      "1 620\n",
      "Epoch: 56 Train loss 4.211, Test loss 4.210\n",
      "1 620\n",
      "Epoch: 57 Train loss 4.210, Test loss 4.209\n",
      "1 620\n",
      "Epoch: 58 Train loss 4.209, Test loss 4.209\n",
      "1 620\n",
      "Epoch: 59 Train loss 4.209, Test loss 4.208\n",
      "1 620\n",
      "Epoch: 60 Train loss 4.208, Test loss 4.207\n",
      "1 620\n",
      "Epoch: 61 Train loss 4.207, Test loss 4.206\n",
      "1 620\n",
      "Epoch: 62 Train loss 4.206, Test loss 4.205\n",
      "1 620\n",
      "Epoch: 63 Train loss 4.205, Test loss 4.204\n",
      "1 620\n",
      "Epoch: 64 Train loss 4.204, Test loss 4.203\n",
      "1 620\n",
      "Epoch: 65 Train loss 4.203, Test loss 4.202\n",
      "1 620\n",
      "Epoch: 66 Train loss 4.202, Test loss 4.201\n",
      "1 620\n",
      "Epoch: 67 Train loss 4.201, Test loss 4.200\n",
      "1 620\n",
      "Epoch: 68 Train loss 4.200, Test loss 4.199\n",
      "1 620\n",
      "Epoch: 69 Train loss 4.199, Test loss 4.198\n",
      "1 620\n",
      "Epoch: 70 Train loss 4.198, Test loss 4.197\n",
      "1 620\n",
      "Epoch: 71 Train loss 4.197, Test loss 4.196\n",
      "1 620\n",
      "Epoch: 72 Train loss 4.196, Test loss 4.196\n",
      "1 620\n",
      "Epoch: 73 Train loss 4.196, Test loss 4.195\n",
      "1 620\n",
      "Epoch: 74 Train loss 4.195, Test loss 4.194\n",
      "1 620\n",
      "Epoch: 75 Train loss 4.194, Test loss 4.193\n",
      "1 620\n",
      "Epoch: 76 Train loss 4.193, Test loss 4.192\n",
      "1 620\n",
      "Epoch: 77 Train loss 4.192, Test loss 4.191\n",
      "1 620\n",
      "Epoch: 78 Train loss 4.191, Test loss 4.190\n",
      "1 620\n",
      "Epoch: 79 Train loss 4.190, Test loss 4.189\n",
      "1 620\n",
      "Epoch: 80 Train loss 4.189, Test loss 4.188\n",
      "1 620\n",
      "Epoch: 81 Train loss 4.188, Test loss 4.187\n",
      "1 620\n",
      "Epoch: 82 Train loss 4.187, Test loss 4.186\n",
      "1 620\n",
      "Epoch: 83 Train loss 4.186, Test loss 4.186\n",
      "1 620\n",
      "Epoch: 84 Train loss 4.186, Test loss 4.185\n",
      "1 620\n",
      "Epoch: 85 Train loss 4.185, Test loss 4.184\n",
      "1 620\n",
      "Epoch: 86 Train loss 4.184, Test loss 4.183\n",
      "1 620\n",
      "Epoch: 87 Train loss 4.183, Test loss 4.182\n",
      "1 620\n",
      "Epoch: 88 Train loss 4.182, Test loss 4.181\n",
      "1 620\n",
      "Epoch: 89 Train loss 4.181, Test loss 4.180\n",
      "1 620\n",
      "Epoch: 90 Train loss 4.180, Test loss 4.179\n",
      "1 620\n",
      "Epoch: 91 Train loss 4.179, Test loss 4.178\n",
      "1 620\n",
      "Epoch: 92 Train loss 4.178, Test loss 4.178\n",
      "1 620\n",
      "Epoch: 93 Train loss 4.178, Test loss 4.177\n",
      "1 620\n",
      "Epoch: 94 Train loss 4.177, Test loss 4.176\n",
      "1 620\n",
      "Epoch: 95 Train loss 4.176, Test loss 4.175\n",
      "1 620\n",
      "Epoch: 96 Train loss 4.175, Test loss 4.174\n",
      "1 620\n",
      "Epoch: 97 Train loss 4.174, Test loss 4.173\n",
      "1 620\n",
      "Epoch: 98 Train loss 4.173, Test loss 4.172\n",
      "1 620\n",
      "Epoch: 99 Train loss 4.172, Test loss 4.171\n",
      "Mean train loss: 4.219 Mean val score: 4.218\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  3.  2. -1.  6. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 16.  6.  0.]] [[-0.1476316]] False\n",
      "Step 121: Reward 0.097.\n",
      "[[ 0.  2.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1.  2.  0.\n",
      "  -1. -1.  1.  0.  2.  8.  2. -2.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 16.  6.  1.]] [[-0.13347302]] False\n",
      "Step 122: Reward 0.133.\n",
      "[[ 0.  0. -1.  0.  0. -1.  1.  0. -1. -1. -1. -1.  0. -1.  1. -1.  2. -1.\n",
      "  -2. -2.  1. -2. 10. 12.  1. -2.  0.  1. -1.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1. 16.  6.  2.]] [[0.00578129]] False\n",
      "Step 123: Reward 0.316.\n",
      "[[-1.  0. -1. -1.  0. -1.  4. -1. -1. -1. -1. -1.  0. -1.  1. -1.  0. -1.\n",
      "  -2.  0.  1.  0. 11. 11. -1.  1.  2.  2.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -1. -1. -2. -2. -1. 16.  6.  3.]] [[-0.03340232]] False\n",
      "Step 124: Reward 0.310.\n",
      "[[-1.  0.  2. -1.  0. -1. -1. -1. -1.  3.  0. -1.  1. -1.  1. -1. -1. -1.\n",
      "   0. -1.  2.  2.  9. 16. -1.  3.  2. -1.  1.  0. -1. -1. -1.  1. -1. -1.\n",
      "  -2.  1. -2. -1. -2. -2. -1. 16.  6.  4.]] [[-0.06599379]] False\n",
      "Step 125: Reward 0.298.\n",
      "[[ 2.  0. -1. -1.  0. -1. -1. -1. -1.  6. -2. -1. -1. -1.  0. -1. -2. -1.\n",
      "   0. -1.  0.  0. 10. 13. -1.  6.  6. -1.  4.  1. -1. -1. -1.  1.  1.  0.\n",
      "  -2.  3.  0. -1. -2. -2. -1. 16.  6.  4.]] [[0.04031503]] False\n",
      "Step 126: Reward 0.426.\n",
      "[[ 3.  0.  0.  0.  0.  1. -1. -1. -1.  6.  2. -1. -1. -1. -1. -2. -2.  0.\n",
      "  -1.  1. -1. -2. 11. 17.  1.  9. 10. -2.  7. -1. -1. -2.  0.  1.  1. -1.\n",
      "  -2.  2.  0. -1. -3. -2. -1. 16.  6.  5.]] [[0.12513822]] False\n",
      "Step 127: Reward 0.547.\n",
      "[[ 2.  0. -1.  0.  0.  1.  1. -1.  0.  6.  2. -1. -1. -1. -2. -2. -2.  0.\n",
      "  -1.  0. -1.  4. 10. 16.  0.  8. 17.  6.  7. -1. -1. -2.  0.  1.  1. -2.\n",
      "  -3.  2.  0. -1. -3. -3. -1. 16.  6.  6.]] [[0.33330092]] True\n",
      "Step 128: Reward 0.778.\n",
      "Trial: 16, reward: 2.9032055990206844.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -1. -1. -1.  1. -1.  2.  7. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 30.  3.  0.]] [[-0.24423996]] False\n",
      "Step 129: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -2.  1. -1. -2. -1. -1. -1. 13.  5. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. 30.  3.  1.]] [[0.2673866]] False\n",
      "Step 130: Reward 0.533.\n",
      "1 630\n",
      "Epoch: 0 Train loss 4.247, Test loss 4.246\n",
      "1 630\n",
      "Epoch: 1 Train loss 4.246, Test loss 4.245\n",
      "1 630\n",
      "Epoch: 2 Train loss 4.245, Test loss 4.244\n",
      "1 630\n",
      "Epoch: 3 Train loss 4.244, Test loss 4.243\n",
      "1 630\n",
      "Epoch: 4 Train loss 4.243, Test loss 4.242\n",
      "1 630\n",
      "Epoch: 5 Train loss 4.242, Test loss 4.241\n",
      "1 630\n",
      "Epoch: 6 Train loss 4.241, Test loss 4.240\n",
      "1 630\n",
      "Epoch: 7 Train loss 4.240, Test loss 4.239\n",
      "1 630\n",
      "Epoch: 8 Train loss 4.239, Test loss 4.238\n",
      "1 630\n",
      "Epoch: 9 Train loss 4.238, Test loss 4.237\n",
      "1 630\n",
      "Epoch: 10 Train loss 4.237, Test loss 4.236\n",
      "1 630\n",
      "Epoch: 11 Train loss 4.236, Test loss 4.235\n",
      "1 630\n",
      "Epoch: 12 Train loss 4.235, Test loss 4.234\n",
      "1 630\n",
      "Epoch: 13 Train loss 4.234, Test loss 4.233\n",
      "1 630\n",
      "Epoch: 14 Train loss 4.233, Test loss 4.232\n",
      "1 630\n",
      "Epoch: 15 Train loss 4.232, Test loss 4.231\n",
      "1 630\n",
      "Epoch: 16 Train loss 4.231, Test loss 4.230\n",
      "1 630\n",
      "Epoch: 17 Train loss 4.230, Test loss 4.229\n",
      "1 630\n",
      "Epoch: 18 Train loss 4.229, Test loss 4.228\n",
      "1 630\n",
      "Epoch: 19 Train loss 4.228, Test loss 4.227\n",
      "1 630\n",
      "Epoch: 20 Train loss 4.227, Test loss 4.226\n",
      "1 630\n",
      "Epoch: 21 Train loss 4.226, Test loss 4.225\n",
      "1 630\n",
      "Epoch: 22 Train loss 4.225, Test loss 4.224\n",
      "1 630\n",
      "Epoch: 23 Train loss 4.224, Test loss 4.223\n",
      "1 630\n",
      "Epoch: 24 Train loss 4.223, Test loss 4.222\n",
      "1 630\n",
      "Epoch: 25 Train loss 4.222, Test loss 4.221\n",
      "1 630\n",
      "Epoch: 26 Train loss 4.221, Test loss 4.220\n",
      "1 630\n",
      "Epoch: 27 Train loss 4.220, Test loss 4.219\n",
      "1 630\n",
      "Epoch: 28 Train loss 4.219, Test loss 4.219\n",
      "1 630\n",
      "Epoch: 29 Train loss 4.219, Test loss 4.218\n",
      "1 630\n",
      "Epoch: 30 Train loss 4.218, Test loss 4.217\n",
      "1 630\n",
      "Epoch: 31 Train loss 4.217, Test loss 4.216\n",
      "1 630\n",
      "Epoch: 32 Train loss 4.216, Test loss 4.215\n",
      "1 630\n",
      "Epoch: 33 Train loss 4.215, Test loss 4.214\n",
      "1 630\n",
      "Epoch: 34 Train loss 4.214, Test loss 4.213\n",
      "1 630\n",
      "Epoch: 35 Train loss 4.213, Test loss 4.212\n",
      "1 630\n",
      "Epoch: 36 Train loss 4.212, Test loss 4.211\n",
      "1 630\n",
      "Epoch: 37 Train loss 4.211, Test loss 4.210\n",
      "1 630\n",
      "Epoch: 38 Train loss 4.210, Test loss 4.209\n",
      "1 630\n",
      "Epoch: 39 Train loss 4.209, Test loss 4.208\n",
      "1 630\n",
      "Epoch: 40 Train loss 4.208, Test loss 4.207\n",
      "1 630\n",
      "Epoch: 41 Train loss 4.207, Test loss 4.206\n",
      "1 630\n",
      "Epoch: 42 Train loss 4.206, Test loss 4.205\n",
      "1 630\n",
      "Epoch: 43 Train loss 4.205, Test loss 4.205\n",
      "1 630\n",
      "Epoch: 44 Train loss 4.205, Test loss 4.204\n",
      "1 630\n",
      "Epoch: 45 Train loss 4.204, Test loss 4.203\n",
      "1 630\n",
      "Epoch: 46 Train loss 4.203, Test loss 4.202\n",
      "1 630\n",
      "Epoch: 47 Train loss 4.202, Test loss 4.201\n",
      "1 630\n",
      "Epoch: 48 Train loss 4.201, Test loss 4.200\n",
      "1 630\n",
      "Epoch: 49 Train loss 4.200, Test loss 4.199\n",
      "1 630\n",
      "Epoch: 50 Train loss 4.199, Test loss 4.198\n",
      "1 630\n",
      "Epoch: 51 Train loss 4.198, Test loss 4.197\n",
      "1 630\n",
      "Epoch: 52 Train loss 4.197, Test loss 4.196\n",
      "1 630\n",
      "Epoch: 53 Train loss 4.196, Test loss 4.195\n",
      "1 630\n",
      "Epoch: 54 Train loss 4.195, Test loss 4.194\n",
      "1 630\n",
      "Epoch: 55 Train loss 4.194, Test loss 4.194\n",
      "1 630\n",
      "Epoch: 56 Train loss 4.194, Test loss 4.193\n",
      "1 630\n",
      "Epoch: 57 Train loss 4.193, Test loss 4.192\n",
      "1 630\n",
      "Epoch: 58 Train loss 4.192, Test loss 4.191\n",
      "1 630\n",
      "Epoch: 59 Train loss 4.191, Test loss 4.190\n",
      "1 630\n",
      "Epoch: 60 Train loss 4.190, Test loss 4.189\n",
      "1 630\n",
      "Epoch: 61 Train loss 4.189, Test loss 4.188\n",
      "1 630\n",
      "Epoch: 62 Train loss 4.188, Test loss 4.187\n",
      "1 630\n",
      "Epoch: 63 Train loss 4.187, Test loss 4.186\n",
      "1 630\n",
      "Epoch: 64 Train loss 4.186, Test loss 4.186\n",
      "1 630\n",
      "Epoch: 65 Train loss 4.186, Test loss 4.185\n",
      "1 630\n",
      "Epoch: 66 Train loss 4.185, Test loss 4.184\n",
      "1 630\n",
      "Epoch: 67 Train loss 4.184, Test loss 4.183\n",
      "1 630\n",
      "Epoch: 68 Train loss 4.183, Test loss 4.182\n",
      "1 630\n",
      "Epoch: 69 Train loss 4.182, Test loss 4.181\n",
      "1 630\n",
      "Epoch: 70 Train loss 4.181, Test loss 4.180\n",
      "1 630\n",
      "Epoch: 71 Train loss 4.180, Test loss 4.179\n",
      "1 630\n",
      "Epoch: 72 Train loss 4.179, Test loss 4.178\n",
      "1 630\n",
      "Epoch: 73 Train loss 4.178, Test loss 4.178\n",
      "1 630\n",
      "Epoch: 74 Train loss 4.178, Test loss 4.177\n",
      "1 630\n",
      "Epoch: 75 Train loss 4.177, Test loss 4.176\n",
      "1 630\n",
      "Epoch: 76 Train loss 4.176, Test loss 4.175\n",
      "1 630\n",
      "Epoch: 77 Train loss 4.175, Test loss 4.174\n",
      "1 630\n",
      "Epoch: 78 Train loss 4.174, Test loss 4.173\n",
      "1 630\n",
      "Epoch: 79 Train loss 4.173, Test loss 4.172\n",
      "1 630\n",
      "Epoch: 80 Train loss 4.172, Test loss 4.172\n",
      "1 630\n",
      "Epoch: 81 Train loss 4.172, Test loss 4.171\n",
      "1 630\n",
      "Epoch: 82 Train loss 4.171, Test loss 4.170\n",
      "1 630\n",
      "Epoch: 83 Train loss 4.170, Test loss 4.169\n",
      "1 630\n",
      "Epoch: 84 Train loss 4.169, Test loss 4.168\n",
      "1 630\n",
      "Epoch: 85 Train loss 4.168, Test loss 4.167\n",
      "1 630\n",
      "Epoch: 86 Train loss 4.167, Test loss 4.166\n",
      "1 630\n",
      "Epoch: 87 Train loss 4.166, Test loss 4.165\n",
      "1 630\n",
      "Epoch: 88 Train loss 4.165, Test loss 4.165\n",
      "1 630\n",
      "Epoch: 89 Train loss 4.165, Test loss 4.164\n",
      "1 630\n",
      "Epoch: 90 Train loss 4.164, Test loss 4.163\n",
      "1 630\n",
      "Epoch: 91 Train loss 4.163, Test loss 4.162\n",
      "1 630\n",
      "Epoch: 92 Train loss 4.162, Test loss 4.161\n",
      "1 630\n",
      "Epoch: 93 Train loss 4.161, Test loss 4.160\n",
      "1 630\n",
      "Epoch: 94 Train loss 4.160, Test loss 4.160\n",
      "1 630\n",
      "Epoch: 95 Train loss 4.160, Test loss 4.159\n",
      "1 630\n",
      "Epoch: 96 Train loss 4.159, Test loss 4.158\n",
      "1 630\n",
      "Epoch: 97 Train loss 4.158, Test loss 4.157\n",
      "1 630\n",
      "Epoch: 98 Train loss 4.157, Test loss 4.156\n",
      "1 630\n",
      "Epoch: 99 Train loss 4.156, Test loss 4.155\n",
      "Mean train loss: 4.200 Mean val score: 4.199\n",
      "[[-1.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1. -1.\n",
      "  -2.  1. -2.  1. -2.  0.  5. 12.  7. -1. -2. -1. -1. -1. -1.  1.  0. -1.\n",
      "  -1. -1. -1.  2. -1. -1. -1. 30.  3.  2.]] [[-0.01762679]] False\n",
      "Step 131: Reward 0.274.\n",
      "[[-1.  0. -1.  0.  0. -1.  2.  0.  0.  1. -1. -1.  0.  0. -1. -1. -1. -1.\n",
      "  -2.  0. -2.  2. -3.  1. -1. 15.  9. -2.  5.  0. -1. -1. -1.  1.  0. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -2. 30.  3.  3.]] [[0.00117061]] False\n",
      "Step 132: Reward 0.339.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  0.  0.  1. -1. -1. -2. -1.\n",
      "   0.  0.  2.  5. -3.  1. -1. 15. 10.  4.  4.  1. -1. -1. -1. -1. -1. -1.\n",
      "   1. -1. -2. -1. -2. -2. -2. 30.  3.  4.]] [[0.06974438]] False\n",
      "Step 133: Reward 0.430.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0.  0. -2. -1.  0.  1. -1.  2. -1.  2.\n",
      "  -1. -2.  1.  5. -4.  0. -1. 14. 17.  4.  8.  1.  0. -1. -1. -1. -1.  0.\n",
      "   1.  1. -2. -1.  0. -2. -2. 30.  3.  4.]] [[-0.06852061]] False\n",
      "Step 134: Reward 0.317.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2.  2.  1.\n",
      "  -1. -2.  1.  5. -4. -3. -1. 21. 16.  4. 15.  5. -1. -2. -1. -1. -1.  0.\n",
      "   0.  0. -2. -1. -1. -2. -2. 30.  3.  5.]] [[0.0243392]] False\n",
      "Step 135: Reward 0.452.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1. -1.  0. -2.  2.  2.  5.\n",
      "   0. -2.  1.  5. -5. -2. -1. 24. 15.  7. 19.  4.  0. -2. -1. -1. -1.  0.\n",
      "   0.  0. -2. -1. -1. -3. -2. 30.  3.  6.]] [[-0.28866744]] True\n",
      "Step 136: Reward 0.167.\n",
      "Trial: 17, reward: 2.5115262753201115.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -2.  2. -2.  4. -1.  2. -2.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  7.  8.  0.]] [[-0.24634361]] False\n",
      "Step 137: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0. -1.  0.  1. -1. -1. -1.  0.\n",
      "  -1. -2.  1.  1.  4.  4.  0. -2.  0.  2. -1. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  7.  8.  1.]] [[-0.0321506]] False\n",
      "Step 138: Reward 0.245.\n",
      "[[ 0.  0. -1.  0.  0. -1. -1.  0. -1.  3. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  -2. -2.  1.  0.  1.  5. -1.  6. -1.  4. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1. -1. -1. -2.  0. -1.  7.  8.  2.]] [[0.00652382]] False\n",
      "Step 139: Reward 0.314.\n",
      "[[-1.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0. -1. -1. -1.\n",
      "  -2. -2.  0.  0. -3.  3. -1.  5.  1.  2. -2.  3. -1. -1. -1. -1.  0.  0.\n",
      "   3.  1. -1. -1. -2.  1. -1.  7.  8.  3.]] [[-0.0156827]] False\n",
      "Step 140: Reward 0.316.\n",
      "1 640\n",
      "Epoch: 0 Train loss 4.257, Test loss 4.256\n",
      "1 640\n",
      "Epoch: 1 Train loss 4.256, Test loss 4.255\n",
      "1 640\n",
      "Epoch: 2 Train loss 4.255, Test loss 4.254\n",
      "1 640\n",
      "Epoch: 3 Train loss 4.254, Test loss 4.253\n",
      "1 640\n",
      "Epoch: 4 Train loss 4.253, Test loss 4.252\n",
      "1 640\n",
      "Epoch: 5 Train loss 4.252, Test loss 4.251\n",
      "1 640\n",
      "Epoch: 6 Train loss 4.251, Test loss 4.250\n",
      "1 640\n",
      "Epoch: 7 Train loss 4.250, Test loss 4.249\n",
      "1 640\n",
      "Epoch: 8 Train loss 4.249, Test loss 4.248\n",
      "1 640\n",
      "Epoch: 9 Train loss 4.248, Test loss 4.247\n",
      "1 640\n",
      "Epoch: 10 Train loss 4.247, Test loss 4.246\n",
      "1 640\n",
      "Epoch: 11 Train loss 4.246, Test loss 4.245\n",
      "1 640\n",
      "Epoch: 12 Train loss 4.245, Test loss 4.244\n",
      "1 640\n",
      "Epoch: 13 Train loss 4.244, Test loss 4.243\n",
      "1 640\n",
      "Epoch: 14 Train loss 4.243, Test loss 4.242\n",
      "1 640\n",
      "Epoch: 15 Train loss 4.242, Test loss 4.241\n",
      "1 640\n",
      "Epoch: 16 Train loss 4.241, Test loss 4.240\n",
      "1 640\n",
      "Epoch: 17 Train loss 4.240, Test loss 4.239\n",
      "1 640\n",
      "Epoch: 18 Train loss 4.239, Test loss 4.238\n",
      "1 640\n",
      "Epoch: 19 Train loss 4.238, Test loss 4.237\n",
      "1 640\n",
      "Epoch: 20 Train loss 4.237, Test loss 4.236\n",
      "1 640\n",
      "Epoch: 21 Train loss 4.236, Test loss 4.235\n",
      "1 640\n",
      "Epoch: 22 Train loss 4.235, Test loss 4.234\n",
      "1 640\n",
      "Epoch: 23 Train loss 4.234, Test loss 4.233\n",
      "1 640\n",
      "Epoch: 24 Train loss 4.233, Test loss 4.232\n",
      "1 640\n",
      "Epoch: 25 Train loss 4.232, Test loss 4.232\n",
      "1 640\n",
      "Epoch: 26 Train loss 4.232, Test loss 4.231\n",
      "1 640\n",
      "Epoch: 27 Train loss 4.231, Test loss 4.230\n",
      "1 640\n",
      "Epoch: 28 Train loss 4.230, Test loss 4.229\n",
      "1 640\n",
      "Epoch: 29 Train loss 4.229, Test loss 4.228\n",
      "1 640\n",
      "Epoch: 30 Train loss 4.228, Test loss 4.227\n",
      "1 640\n",
      "Epoch: 31 Train loss 4.227, Test loss 4.226\n",
      "1 640\n",
      "Epoch: 32 Train loss 4.226, Test loss 4.225\n",
      "1 640\n",
      "Epoch: 33 Train loss 4.225, Test loss 4.224\n",
      "1 640\n",
      "Epoch: 34 Train loss 4.224, Test loss 4.223\n",
      "1 640\n",
      "Epoch: 35 Train loss 4.223, Test loss 4.222\n",
      "1 640\n",
      "Epoch: 36 Train loss 4.222, Test loss 4.221\n",
      "1 640\n",
      "Epoch: 37 Train loss 4.221, Test loss 4.220\n",
      "1 640\n",
      "Epoch: 38 Train loss 4.220, Test loss 4.219\n",
      "1 640\n",
      "Epoch: 39 Train loss 4.219, Test loss 4.218\n",
      "1 640\n",
      "Epoch: 40 Train loss 4.218, Test loss 4.217\n",
      "1 640\n",
      "Epoch: 41 Train loss 4.217, Test loss 4.216\n",
      "1 640\n",
      "Epoch: 42 Train loss 4.216, Test loss 4.215\n",
      "1 640\n",
      "Epoch: 43 Train loss 4.215, Test loss 4.214\n",
      "1 640\n",
      "Epoch: 44 Train loss 4.214, Test loss 4.213\n",
      "1 640\n",
      "Epoch: 45 Train loss 4.213, Test loss 4.212\n",
      "1 640\n",
      "Epoch: 46 Train loss 4.212, Test loss 4.211\n",
      "1 640\n",
      "Epoch: 47 Train loss 4.211, Test loss 4.210\n",
      "1 640\n",
      "Epoch: 48 Train loss 4.210, Test loss 4.210\n",
      "1 640\n",
      "Epoch: 49 Train loss 4.210, Test loss 4.209\n",
      "1 640\n",
      "Epoch: 50 Train loss 4.209, Test loss 4.208\n",
      "1 640\n",
      "Epoch: 51 Train loss 4.208, Test loss 4.207\n",
      "1 640\n",
      "Epoch: 52 Train loss 4.207, Test loss 4.206\n",
      "1 640\n",
      "Epoch: 53 Train loss 4.206, Test loss 4.205\n",
      "1 640\n",
      "Epoch: 54 Train loss 4.205, Test loss 4.204\n",
      "1 640\n",
      "Epoch: 55 Train loss 4.204, Test loss 4.203\n",
      "1 640\n",
      "Epoch: 56 Train loss 4.203, Test loss 4.202\n",
      "1 640\n",
      "Epoch: 57 Train loss 4.202, Test loss 4.201\n",
      "1 640\n",
      "Epoch: 58 Train loss 4.201, Test loss 4.200\n",
      "1 640\n",
      "Epoch: 59 Train loss 4.200, Test loss 4.199\n",
      "1 640\n",
      "Epoch: 60 Train loss 4.199, Test loss 4.198\n",
      "1 640\n",
      "Epoch: 61 Train loss 4.198, Test loss 4.198\n",
      "1 640\n",
      "Epoch: 62 Train loss 4.198, Test loss 4.197\n",
      "1 640\n",
      "Epoch: 63 Train loss 4.197, Test loss 4.196\n",
      "1 640\n",
      "Epoch: 64 Train loss 4.196, Test loss 4.195\n",
      "1 640\n",
      "Epoch: 65 Train loss 4.195, Test loss 4.194\n",
      "1 640\n",
      "Epoch: 66 Train loss 4.194, Test loss 4.193\n",
      "1 640\n",
      "Epoch: 67 Train loss 4.193, Test loss 4.192\n",
      "1 640\n",
      "Epoch: 68 Train loss 4.192, Test loss 4.191\n",
      "1 640\n",
      "Epoch: 69 Train loss 4.191, Test loss 4.190\n",
      "1 640\n",
      "Epoch: 70 Train loss 4.190, Test loss 4.189\n",
      "1 640\n",
      "Epoch: 71 Train loss 4.189, Test loss 4.188\n",
      "1 640\n",
      "Epoch: 72 Train loss 4.188, Test loss 4.187\n",
      "1 640\n",
      "Epoch: 73 Train loss 4.187, Test loss 4.187\n",
      "1 640\n",
      "Epoch: 74 Train loss 4.187, Test loss 4.186\n",
      "1 640\n",
      "Epoch: 75 Train loss 4.186, Test loss 4.185\n",
      "1 640\n",
      "Epoch: 76 Train loss 4.185, Test loss 4.184\n",
      "1 640\n",
      "Epoch: 77 Train loss 4.184, Test loss 4.183\n",
      "1 640\n",
      "Epoch: 78 Train loss 4.183, Test loss 4.182\n",
      "1 640\n",
      "Epoch: 79 Train loss 4.182, Test loss 4.181\n",
      "1 640\n",
      "Epoch: 80 Train loss 4.181, Test loss 4.180\n",
      "1 640\n",
      "Epoch: 81 Train loss 4.180, Test loss 4.179\n",
      "1 640\n",
      "Epoch: 82 Train loss 4.179, Test loss 4.179\n",
      "1 640\n",
      "Epoch: 83 Train loss 4.179, Test loss 4.178\n",
      "1 640\n",
      "Epoch: 84 Train loss 4.178, Test loss 4.177\n",
      "1 640\n",
      "Epoch: 85 Train loss 4.177, Test loss 4.176\n",
      "1 640\n",
      "Epoch: 86 Train loss 4.176, Test loss 4.175\n",
      "1 640\n",
      "Epoch: 87 Train loss 4.175, Test loss 4.174\n",
      "1 640\n",
      "Epoch: 88 Train loss 4.174, Test loss 4.173\n",
      "1 640\n",
      "Epoch: 89 Train loss 4.173, Test loss 4.172\n",
      "1 640\n",
      "Epoch: 90 Train loss 4.172, Test loss 4.171\n",
      "1 640\n",
      "Epoch: 91 Train loss 4.171, Test loss 4.171\n",
      "1 640\n",
      "Epoch: 92 Train loss 4.171, Test loss 4.170\n",
      "1 640\n",
      "Epoch: 93 Train loss 4.170, Test loss 4.169\n",
      "1 640\n",
      "Epoch: 94 Train loss 4.169, Test loss 4.168\n",
      "1 640\n",
      "Epoch: 95 Train loss 4.168, Test loss 4.167\n",
      "1 640\n",
      "Epoch: 96 Train loss 4.167, Test loss 4.166\n",
      "1 640\n",
      "Epoch: 97 Train loss 4.166, Test loss 4.165\n",
      "1 640\n",
      "Epoch: 98 Train loss 4.165, Test loss 4.164\n",
      "1 640\n",
      "Epoch: 99 Train loss 4.164, Test loss 4.164\n",
      "Mean train loss: 4.210 Mean val score: 4.209\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  2. -1.  1. -1. -2. -1.\n",
      "  -2.  2.  1.  1. -3. -2. -1.  4.  7. -1. -2. -1. -1.  0. -1. -1.  2.  0.\n",
      "   9. -1.  0. -1. -1.  1. -1.  7.  8.  4.]] [[-0.01895627]] False\n",
      "Step 141: Reward 0.322.\n",
      "[[ 0. -1. -1. -1.  0. -1.  1. -1. -1. -1.  0.  1.  0.  0.  1. -1. -1. -1.\n",
      "  -2.  3.  6. -2. -3. -2.  0.  2.  9. -1. -2. -1. -1.  0. -1. -1.  5. -1.\n",
      "  11.  0.  0. -1.  0.  1.  0.  7.  8.  4.]] [[-0.03005064]] False\n",
      "Step 142: Reward 0.337.\n",
      "[[-1. -1. -1.  0.  0.  0. -1.  0. -1. -2. -2. -2.  0.  1.  3. -1. -1. -1.\n",
      "  -2.  3.  6. -4. -4. -3. -1.  2. 16. -2.  1. -1. -1. -2. -2. -2.  0.  1.\n",
      "  12. -1.  2. -1.  2.  2.  1.  7.  8.  5.]] [[-0.01248398]] False\n",
      "Step 143: Reward 0.394.\n",
      "[[ 1. -1.  0. -1.  0.  1. -1. -1. -1. -2. -2. -1. -1.  0.  5. -1. -1.  0.\n",
      "  -3.  3.  9. -4. -4. -2. -1.  6. 18. -1.  2.  3.  0. -2. -2. -1. -1.  0.\n",
      "  12. -1.  4.  4.  2.  2.  1.  7.  8.  6.]] [[-0.07259962]] True\n",
      "Step 144: Reward 0.367.\n",
      "Trial: 18, reward: 2.295181898071736.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1.  0.  0.  0. -1. -1.  2.  2.\n",
      "  -1. -2.  2.  0.  0. -1.  2. -2. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1.  0. -1. -1. -1. -1. -1.  8.  8.  0.]] [[-0.14092392]] False\n",
      "Step 145: Reward 0.095.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1.  2.  1.\n",
      "  -1.  1.  1. -1.  3. -2.  5. -2. -3.  5. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1.  0. -1. -1. -1. -1. -1.  8.  8.  1.]] [[0.1912359]] False\n",
      "Step 146: Reward 0.458.\n",
      "[[ 0.  0.  0.  2.  0. -1. -1.  1. -1. -1.  1. -1.  2. -1.  0.  2.  1. -1.\n",
      "  -2.  1.  1. -1.  2. -2.  7.  3. -3.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1.  0. -1. -1. -1.  0. -1.  8.  8.  2.]] [[0.04932803]] False\n",
      "Step 147: Reward 0.360.\n",
      "[[-1.  0. -1.  1.  0. -1. -1.  1.  1.  0. -1. -1.  0. -1. -1.  5.  0. -1.\n",
      "  -2.  0.  2. -3. -2.  0. -1.  2. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0.\n",
      "  -1. -1.  1.  0. -2.  0. -1.  8.  8.  3.]] [[0.12163696]] False\n",
      "Step 148: Reward 0.460.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1.  0.  1. -1.  0.  4. -1. -1.\n",
      "  -2. -1.  0. -2.  1.  0. -1.  7.  0. -1. -2. -1.  0.  1. -1.  1. -1. -1.\n",
      "  -1.  0.  1.  0. -1.  1. -2.  8.  8.  4.]] [[0.11693215]] False\n",
      "Step 149: Reward 0.460.\n",
      "[[-1.  2. -1.  0.  0.  0.  0.  1. -1. -1.  1. -2.  0.  0. -1.  6. -1.  1.\n",
      "  -2. -1.  2. -3.  2. -1. -1. 10.  4. -1.  0. -1.  3.  1. -1. -1.  2.  0.\n",
      "   2. -1.  5. -1.  2.  0. -2.  8.  8.  4.]] [[0.08271831]] False\n",
      "Step 150: Reward 0.447.\n",
      "1 650\n",
      "Epoch: 0 Train loss 4.195, Test loss 4.194\n",
      "1 650\n",
      "Epoch: 1 Train loss 4.194, Test loss 4.193\n",
      "1 650\n",
      "Epoch: 2 Train loss 4.193, Test loss 4.193\n",
      "1 650\n",
      "Epoch: 3 Train loss 4.193, Test loss 4.192\n",
      "1 650\n",
      "Epoch: 4 Train loss 4.192, Test loss 4.191\n",
      "1 650\n",
      "Epoch: 5 Train loss 4.191, Test loss 4.190\n",
      "1 650\n",
      "Epoch: 6 Train loss 4.190, Test loss 4.189\n",
      "1 650\n",
      "Epoch: 7 Train loss 4.189, Test loss 4.188\n",
      "1 650\n",
      "Epoch: 8 Train loss 4.188, Test loss 4.187\n",
      "1 650\n",
      "Epoch: 9 Train loss 4.187, Test loss 4.186\n",
      "1 650\n",
      "Epoch: 10 Train loss 4.186, Test loss 4.185\n",
      "1 650\n",
      "Epoch: 11 Train loss 4.185, Test loss 4.185\n",
      "1 650\n",
      "Epoch: 12 Train loss 4.185, Test loss 4.184\n",
      "1 650\n",
      "Epoch: 13 Train loss 4.184, Test loss 4.183\n",
      "1 650\n",
      "Epoch: 14 Train loss 4.183, Test loss 4.182\n",
      "1 650\n",
      "Epoch: 15 Train loss 4.182, Test loss 4.181\n",
      "1 650\n",
      "Epoch: 16 Train loss 4.181, Test loss 4.180\n",
      "1 650\n",
      "Epoch: 17 Train loss 4.180, Test loss 4.179\n",
      "1 650\n",
      "Epoch: 18 Train loss 4.179, Test loss 4.178\n",
      "1 650\n",
      "Epoch: 19 Train loss 4.178, Test loss 4.177\n",
      "1 650\n",
      "Epoch: 20 Train loss 4.177, Test loss 4.177\n",
      "1 650\n",
      "Epoch: 21 Train loss 4.177, Test loss 4.176\n",
      "1 650\n",
      "Epoch: 22 Train loss 4.176, Test loss 4.175\n",
      "1 650\n",
      "Epoch: 23 Train loss 4.175, Test loss 4.174\n",
      "1 650\n",
      "Epoch: 24 Train loss 4.174, Test loss 4.173\n",
      "1 650\n",
      "Epoch: 25 Train loss 4.173, Test loss 4.172\n",
      "1 650\n",
      "Epoch: 26 Train loss 4.172, Test loss 4.171\n",
      "1 650\n",
      "Epoch: 27 Train loss 4.171, Test loss 4.170\n",
      "1 650\n",
      "Epoch: 28 Train loss 4.170, Test loss 4.169\n",
      "1 650\n",
      "Epoch: 29 Train loss 4.169, Test loss 4.169\n",
      "1 650\n",
      "Epoch: 30 Train loss 4.169, Test loss 4.168\n",
      "1 650\n",
      "Epoch: 31 Train loss 4.168, Test loss 4.167\n",
      "1 650\n",
      "Epoch: 32 Train loss 4.167, Test loss 4.166\n",
      "1 650\n",
      "Epoch: 33 Train loss 4.166, Test loss 4.165\n",
      "1 650\n",
      "Epoch: 34 Train loss 4.165, Test loss 4.164\n",
      "1 650\n",
      "Epoch: 35 Train loss 4.164, Test loss 4.163\n",
      "1 650\n",
      "Epoch: 36 Train loss 4.163, Test loss 4.163\n",
      "1 650\n",
      "Epoch: 37 Train loss 4.163, Test loss 4.162\n",
      "1 650\n",
      "Epoch: 38 Train loss 4.162, Test loss 4.161\n",
      "1 650\n",
      "Epoch: 39 Train loss 4.161, Test loss 4.160\n",
      "1 650\n",
      "Epoch: 40 Train loss 4.160, Test loss 4.159\n",
      "1 650\n",
      "Epoch: 41 Train loss 4.159, Test loss 4.158\n",
      "1 650\n",
      "Epoch: 42 Train loss 4.158, Test loss 4.157\n",
      "1 650\n",
      "Epoch: 43 Train loss 4.157, Test loss 4.157\n",
      "1 650\n",
      "Epoch: 44 Train loss 4.157, Test loss 4.156\n",
      "1 650\n",
      "Epoch: 45 Train loss 4.156, Test loss 4.155\n",
      "1 650\n",
      "Epoch: 46 Train loss 4.155, Test loss 4.154\n",
      "1 650\n",
      "Epoch: 47 Train loss 4.154, Test loss 4.153\n",
      "1 650\n",
      "Epoch: 48 Train loss 4.153, Test loss 4.152\n",
      "1 650\n",
      "Epoch: 49 Train loss 4.152, Test loss 4.151\n",
      "1 650\n",
      "Epoch: 50 Train loss 4.151, Test loss 4.151\n",
      "1 650\n",
      "Epoch: 51 Train loss 4.151, Test loss 4.150\n",
      "1 650\n",
      "Epoch: 52 Train loss 4.150, Test loss 4.149\n",
      "1 650\n",
      "Epoch: 53 Train loss 4.149, Test loss 4.148\n",
      "1 650\n",
      "Epoch: 54 Train loss 4.148, Test loss 4.147\n",
      "1 650\n",
      "Epoch: 55 Train loss 4.147, Test loss 4.146\n",
      "1 650\n",
      "Epoch: 56 Train loss 4.146, Test loss 4.145\n",
      "1 650\n",
      "Epoch: 57 Train loss 4.145, Test loss 4.145\n",
      "1 650\n",
      "Epoch: 58 Train loss 4.145, Test loss 4.144\n",
      "1 650\n",
      "Epoch: 59 Train loss 4.144, Test loss 4.143\n",
      "1 650\n",
      "Epoch: 60 Train loss 4.143, Test loss 4.142\n",
      "1 650\n",
      "Epoch: 61 Train loss 4.142, Test loss 4.141\n",
      "1 650\n",
      "Epoch: 62 Train loss 4.141, Test loss 4.140\n",
      "1 650\n",
      "Epoch: 63 Train loss 4.140, Test loss 4.140\n",
      "1 650\n",
      "Epoch: 64 Train loss 4.140, Test loss 4.139\n",
      "1 650\n",
      "Epoch: 65 Train loss 4.139, Test loss 4.138\n",
      "1 650\n",
      "Epoch: 66 Train loss 4.138, Test loss 4.137\n",
      "1 650\n",
      "Epoch: 67 Train loss 4.137, Test loss 4.136\n",
      "1 650\n",
      "Epoch: 68 Train loss 4.136, Test loss 4.135\n",
      "1 650\n",
      "Epoch: 69 Train loss 4.135, Test loss 4.135\n",
      "1 650\n",
      "Epoch: 70 Train loss 4.135, Test loss 4.134\n",
      "1 650\n",
      "Epoch: 71 Train loss 4.134, Test loss 4.133\n",
      "1 650\n",
      "Epoch: 72 Train loss 4.133, Test loss 4.132\n",
      "1 650\n",
      "Epoch: 73 Train loss 4.132, Test loss 4.131\n",
      "1 650\n",
      "Epoch: 74 Train loss 4.131, Test loss 4.130\n",
      "1 650\n",
      "Epoch: 75 Train loss 4.130, Test loss 4.130\n",
      "1 650\n",
      "Epoch: 76 Train loss 4.130, Test loss 4.129\n",
      "1 650\n",
      "Epoch: 77 Train loss 4.129, Test loss 4.128\n",
      "1 650\n",
      "Epoch: 78 Train loss 4.128, Test loss 4.127\n",
      "1 650\n",
      "Epoch: 79 Train loss 4.127, Test loss 4.126\n",
      "1 650\n",
      "Epoch: 80 Train loss 4.126, Test loss 4.125\n",
      "1 650\n",
      "Epoch: 81 Train loss 4.125, Test loss 4.125\n",
      "1 650\n",
      "Epoch: 82 Train loss 4.125, Test loss 4.124\n",
      "1 650\n",
      "Epoch: 83 Train loss 4.124, Test loss 4.123\n",
      "1 650\n",
      "Epoch: 84 Train loss 4.123, Test loss 4.122\n",
      "1 650\n",
      "Epoch: 85 Train loss 4.122, Test loss 4.121\n",
      "1 650\n",
      "Epoch: 86 Train loss 4.121, Test loss 4.121\n",
      "1 650\n",
      "Epoch: 87 Train loss 4.121, Test loss 4.120\n",
      "1 650\n",
      "Epoch: 88 Train loss 4.120, Test loss 4.119\n",
      "1 650\n",
      "Epoch: 89 Train loss 4.119, Test loss 4.118\n",
      "1 650\n",
      "Epoch: 90 Train loss 4.118, Test loss 4.117\n",
      "1 650\n",
      "Epoch: 91 Train loss 4.117, Test loss 4.116\n",
      "1 650\n",
      "Epoch: 92 Train loss 4.116, Test loss 4.116\n",
      "1 650\n",
      "Epoch: 93 Train loss 4.116, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 94 Train loss 4.115, Test loss 4.114\n",
      "1 650\n",
      "Epoch: 95 Train loss 4.114, Test loss 4.113\n",
      "1 650\n",
      "Epoch: 96 Train loss 4.113, Test loss 4.112\n",
      "1 650\n",
      "Epoch: 97 Train loss 4.112, Test loss 4.112\n",
      "1 650\n",
      "Epoch: 98 Train loss 4.112, Test loss 4.111\n",
      "1 650\n",
      "Epoch: 99 Train loss 4.111, Test loss 4.110\n",
      "Mean train loss: 4.152 Mean val score: 4.151\n",
      "[[-1. -1. -1.  0.  0.  2. -1.  0.  2.  0.  1. -2. -1. -1. -1.  4. -1. -1.\n",
      "  -3. -1.  1. -1.  0. -3.  0. 10. 13.  3.  6.  1.  2. -1. -2.  5.  3. -1.\n",
      "   0. -2.  4.  1.  2. -1. -2.  8.  8.  5.]] [[0.17669645]] False\n",
      "Step 151: Reward 0.589.\n",
      "[[-1. -1.  1. -1.  0.  4. -1.  0.  5.  2.  1. -2.  1. -1. -2.  4. -1. -1.\n",
      "  -3. -2.  1. -3.  5. -3. -1.  9. 11.  6. 14. -2.  2. -1. -2.  7.  2. -1.\n",
      "  -1. -2.  4.  1.  2. -2. -2.  8.  8.  6.]] [[-0.04096386]] True\n",
      "Step 152: Reward 0.421.\n",
      "Trial: 19, reward: 3.2900051196400937.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -1. -1. -1.  1.  2.  2.  4. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  6.  2.  0.]] [[-0.239616]] False\n",
      "Step 153: Reward 0.000.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1. -1.  0.  0. -1. -1. -1.  0.\n",
      "  -1. -2.  2.  1. -1.  1.  2. 10. -3.  3. -1. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  6.  2.  1.]] [[-0.09399831]] False\n",
      "Step 154: Reward 0.182.\n",
      "[[ 0.  0.  0.  0.  0.  0. -1.  0.  0. -1. -1.  3.  0.  0. -1. -1. -1. -1.\n",
      "  -2. -2.  2.  0. -2.  0. 10.  9.  4.  3. -2. -1. -1. -1. -1.  1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  6.  2.  2.]] [[0.04018059]] False\n",
      "Step 155: Reward 0.343.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1.  0. -1. -1.  3.  0. -1. -1. -1. -1. -1.\n",
      "  -2. -3.  3.  4. -3.  2.  9. 16.  3.  4. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -1. -1.  6.  2.  3.]] [[-0.03391719]] False\n",
      "Step 156: Reward 0.318.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1.  1. -1. -1.  3.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -3.  3.  4. -2.  4.  7. 20.  9.  6. -2. -1. -1. -1. -1.  1. -1.  0.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  6.  2.  4.]] [[-0.1629842]] False\n",
      "Step 157: Reward 0.207.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1.  0. -2. -1.  2.  0. -1.  1. -1. -1. -1.\n",
      "  -2. -4.  1.  7.  1.  7.  5. 19. 15.  4. -3. -1. -1. -1. -1. -2. -1.  1.\n",
      "  -2. -2. -2. -1. -2. -2. -1.  6.  2.  4.]] [[-0.07013783]] False\n",
      "Step 158: Reward 0.333.\n",
      "[[-1. -1. -1. -1.  0. -1. -1. -1.  1.  2. -1.  1. -1. -1.  0.  3. -1.  1.\n",
      "  -2. -4.  1.  8.  3.  6.  1. 18. 17.  8. -4.  0. -1. -1. -1. -2. -1.  0.\n",
      "  -2. -2. -2. -1. -2. -2. -1.  6.  2.  5.]] [[0.00620264]] False\n",
      "Step 159: Reward 0.448.\n",
      "[[-1. -1. -1. -1.  0.  1. -1. -1.  1.  3. -2.  1. -1. -1.  0.  2. -1.  1.\n",
      "  -3. -4.  1.  7.  2.  5.  1. 17. 29. 12. -4. -1. -1. -2.  3. -1. -1.  0.\n",
      "  -2. -2. -2. -1. -3. -2. -1.  6.  2.  6.]] [[-0.15641916]] True\n",
      "Step 160: Reward 0.308.\n",
      "Trial: 20, reward: 2.138680385082184.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "all_training_losses = []\n",
    "all_val_scores = []\n",
    "\n",
    "while current_trial < num_episodes:\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "        # --------------- Model Training -----------------\n",
    "        if env_steps % freq_train_model == 0:\n",
    "            dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                model_batch_size,\n",
    "                validation_ratio,\n",
    "                ensemble_size=len(dynamics_model),\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,\n",
    "            )\n",
    "            if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "            training_losses, val_scores = model_trainer.train(\n",
    "                dataset_train,\n",
    "                dataset_val=dataset_val,\n",
    "                num_epochs=num_epochs,\n",
    "                patience=num_epochs,\n",
    "                improvement_threshold=0.01,\n",
    "            )\n",
    "            all_training_losses += training_losses\n",
    "            all_val_scores += val_scores\n",
    "            print(f\"Mean train loss: {np.mean(training_losses):.3f} Mean val score: {np.mean(val_scores):.3f}\")\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        replay_buffer.add(obs, action, next_obs, reward, terminated, truncated)\n",
    "\n",
    "        model_action = action[None, ...]\n",
    "        model_observation = {\n",
    "            \"obs\": obs[None, ...],\n",
    "            \"propagation_indices\": None,\n",
    "        }\n",
    "        model_next_obs, model_reward, model_dones, next_model_state = model_env.step(model_action, model_observation)\n",
    "        #TODO: compare actual next_obs and rewards with model env\n",
    "        print(next_obs-torch.round(model_next_obs).numpy(), reward-model_reward.numpy(), terminated)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "\n",
    "        print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAFzCAYAAAD2eXw5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACZZElEQVR4nOzdd3xT9foH8E9G96S7hQJllg2WVRBUpohcrwuUrYAiQ4YDKyrgqnoVex2gKOOnIOCAe/GKaFHKkCGUVhlFEcpuqQW66E7O749vT9p0Jm12Pu/XK6+mycnJ93Qk58nzfJ+vQpIkCURERERERA5Eae0BEBERERERmRoDHSIiIiIicjgMdIiIiIiIyOEw0CEiIiIiIofDQIeIiIiIiBwOAx0iIiIiInI4DHSIiIiIiMjhMNAhIiIiIiKHo7b2AAyh1Wpx5coV+Pj4QKFQWHs4REROQ5Ik5OfnIyIiAkolPxuriu9NRETWYeh7k10EOleuXEFkZKS1h0FE5LQuXryIFi1aWHsYNoXvTURE1tXQe5NdBDo+Pj4AxMH4+vpaeTRERM4jLy8PkZGRutdhqsT3JiIi6zD0vckuAh25JMDX15dvJkREVsDSrJr43kREZF0NvTex4JqIiIiIiBwOAx0iIiIiInI4DHSIiIiIiMjh2MUcHSIiIiKqnyRJKC8vh0ajsfZQiJpEpVJBrVY3eX6oUYHO0qVLsWzZMr3bQkNDkZmZWedjdu/ejYULF+LEiROIiIjAs88+i5kzZzZutERERERUQ2lpKTIyMlBYWGjtoRCZhKenJ8LDw+Hq6trofRid0enSpQt27typ+16lUtW5bXp6Ou666y7MmDED69evxy+//IJZs2YhODgY999/f+NGTEREREQ6Wq0W6enpUKlUiIiIgKurKzslkt2SJAmlpaX4+++/kZ6ejvbt2zd6wWqjAx21Wo2wsDCDtv3oo4/QsmVLJCQkAAA6deqEI0eO4O2332agQ0RERGQCpaWl0Gq1iIyMhKenp7WHQ9RkHh4ecHFxwfnz51FaWgp3d/dG7cfo8Oj06dOIiIhAVFQUHnroIZw9e7bObQ8cOIARI0bo3TZy5EgcOXIEZWVlxo+WiIiIiGrV2E+9iWyRKf6ejdpDv3798Nlnn+GHH37AJ598gszMTAwYMADXrl2rdfvMzEyEhobq3RYaGory8nJkZ2fX+TwlJSXIy8vTuxARERERERnKqEBn1KhRuP/++9GtWzcMGzYM3333HQDg//7v/+p8TPUaUUmSar29qvj4ePj5+ekukZGRxgxTT04O8O23wLZtjd4FEREROThJAg4fBnJzrT0SIjKVJuWEvLy80K1bN5w+fbrW+8PCwmp0ZMvKyoJarUZgYGCd+42Li0Nubq7ucvHixUaP8cwZ4B//AGbPbvQuiIiIyMH98gvQty8wfbq1R0K2RKFQ4D//+Y/N7MeRLF26FD179jTrczQp0CkpKUFaWhrCw8NrvT82NhaJiYl6t/3444/o3bs3XFxc6tyvm5sbfH199S6N5ecnvubkNHoXRERE5OCOHRNff//duuNwNpmZmZg7dy7atGkDNzc3REZGYsyYMfjpp5+sPbRGqevkPSMjA6NGjbL8gJycUYHO008/jd27dyM9PR2HDh3CAw88gLy8PEyZMgWAyMRMnjxZt/3MmTNx/vx5LFy4EGlpaVizZg1Wr16Np59+2rRHUQ9/f/G1oAAoL7fY0xIREZEdycgQXy9dEmVsZH7nzp1DTEwMfv75Z7z11ls4duwYduzYgTvuuAOzHawUJywsDG5ubhZ9Tltp/GXNcRgV6Fy6dAkPP/wwOnbsiPvuuw+urq44ePAgWrVqBUBEqxcuXNBtHxUVhe3btyMpKQk9e/bEK6+8gvfee8+iraUvlhwHHr8FmDIE7GlAREREtZEDncJC+5+nI0nAzZvWuRgTJM6aNQsKhQK//vorHnjgAXTo0AFdunTBwoULcfDgQQAiGFIoFEhNTdU9LicnBwqFAklJSQCApKQkKBQK/PDDD+jVqxc8PDwwZMgQZGVl4fvvv0enTp3g6+uLhx9+WG9B1datW+uWQJH17NkTS5curXPMixYtQocOHeDp6Yk2bdrgxRdf1J3Ir1u3DsuWLcNvv/0GhUIBhUKBdevWAdAvXYuNjcVzzz2nt9+///4bLi4u2LVrFwDRMvzZZ59F8+bN4eXlhX79+umOty4KhQIfffQR7rnnHnh5eeHVV18FAHz77beIiYmBu7s72rRpg2XLlqG84tP/p556CmPGjNHtIyEhAQqFQjcPHwA6duyIjz/+GABw+PBhDB8+HEFBQfDz88Ntt92Go0ePGjSON954A6GhofDx8cG0adNQXFxc7/GYglHr6GzatKne++VfZlW1/QAsydVFAYSnADeDkJsLBARYbShERERko+RABxBZHbkixB4VFgLe3tZ57oICwMur4e2uX7+OHTt24LXXXoNXLQ/wb8QvYOnSpfjggw/g6emJsWPHYuzYsXBzc8MXX3yBgoIC3HvvvXj//fexaNEio/ct8/Hxwbp16xAREYFjx45hxowZ8PHxwbPPPotx48bh+PHj2LFjB3bu3AkA8JPnUFQxYcIE/Otf/0J8fLyuOdfmzZsRGhqK2267DQDwyCOP4Ny5c9i0aRMiIiKwdetW3HnnnTh27Bjat29f5/iWLFmC+Ph4vPvuu1CpVPjhhx8wceJEvPfeexg0aBDOnDmDxx57TLft7bffjtWrV0Or1UKpVGL37t0ICgrC7t27MXr0aGRmZuLPP//UjSs/Px9TpkzBe++9BwB45513cNddd+H06dPw8fGpcxxffvkllixZgg8//BCDBg3C559/jvfeew9t2rRp9O/CIJIdyM3NlQBIubm5Rj/2Yu5FCUsh4UW1dPSo1gyjIyJyXE15/XV0/Nk4ll69JEnkIyRpxw5rj8Y4RUVF0smTJ6WioiJJkiSpoKDyWCx9KSgwbMyHDh2SAEhbtmypd7v09HQJgJSSkqK77caNGxIAadeuXZIkSdKuXbskANLOnTt128THx0sApDNnzuhue/zxx6WRI0fqvm/VqpX07rvv6j1fjx49pCVLlui+ByBt3bq1zvG99dZbUkxMjO77JUuWSD169KixXdX9ZGVlSWq1WtqzZ4/u/tjYWOmZZ56RJEmS/vrrL0mhUEiXL1/W28fQoUOluLi4OscCQJo/f77ebYMGDZJef/11vds+//xzKTw8XJIkScrJyZGUSqV05MgRSavVSoGBgVJ8fLzUp08fSZIk6YsvvpBCQ0PrfM7y8nLJx8dH+vbbb+sdR2xsrDRz5ky92/r161frz0pW/e+6KkNff43K6Ngjf3d/cUVVjqvXiwBwxWAiIiLSVz2jY888PUVmxVrPbQjJgOVGjNW9e3fd9dDQUF15WdXbfv311yY9x9dff42EhAT89ddfKCgoQHl5udFNs4KDgzF8+HBs2LABgwYNQnp6Og4cOICVK1cCAI4ePQpJktChQwe9x5WUlNTbtRgAevfurfd9cnIyDh8+jNdee013m0ajQXFxMQoLC+Hn54eePXsiKSkJLi4uUCqVePzxx7FkyRLk5+cjKSlJl80BRPfkl156CT///DOuXr0KjUaDwsJCvakrtY0jLS0NM2fO1LstNjZWV6pnLg4f6Hi5eAFaFaDUIONGDhjoEBERUVUaDZCVVfm9vQc6CoVh5WPW1L59eygUCqSlpeGf//xnndsplWI6uRwYAXVPbq/a0VehUNTo8KtQKKDVavX2XXW/9e0bAA4ePIiHHnoIy5Ytw8iRI+Hn54dNmzbhnXfeqfMxdZkwYQLmzZuH999/H1988QW6dOmCHj16AAC0Wi1UKhWSk5OhUqn0HufdQE1i9TJArVaLZcuW4b777quxrbu7OwDg9ttvR1JSElxdXXHbbbehWbNm6NKlC3755RckJSVh/vz5usdMnToVf//9NxISEtCqVSu4ubkhNjYWpaWl9Y7DWhw+0FEoFHDR+KFMeb0i0Imw9pCIiIjIhmRlAVXOf3H5svXG4iwCAgIwcuRIfPjhh3jyySdrnBjn5OTA398fwcHBAETDq169egGAXmOCpggODkZGlVReXl4e0tPT69z+l19+QatWrbB48WLdbefPn9fbxtXVFRqNpsHn/uc//4nHH38cO3bswBdffIFJkybp7uvVqxc0Gg2ysrIwaNAgYw6phltuuQV//PEH2rVrV+c28jwdtVqNYcOGARBz7Ddt2qQ3PwcA9u7dixUrVuCuu+4CAFy8eBHZ2dkNjqNTp044ePCgXndmueGEOTVpHR174Sr5AwD+zrfzNipERERkclXL1gD7z+jYixUrVkCj0aBv37745ptvcPr0aaSlpeG9995DbGwsAMDDwwP9+/fHG2+8gZMnT2LPnj144YUXTPL8Q4YMweeff469e/fi+PHjmDJlSo0MSlXt2rXDhQsXsGnTJpw5cwbvvfcetm7dqrdN69atkZ6ejtTUVGRnZ6OkpKTWfXl5eeGee+7Biy++iLS0NIwfP153X4cOHTBhwgRMnjwZW7ZsQXp6Og4fPow333wT27dvN+oYX3rpJXz22WdYunQpTpw4gbS0NGzevFnvZzh48GDk5+fj22+/xe233w5ABD/r169HcHAwOnfurPcz+Pzzz5GWloZDhw5hwoQJ8PDwaHAc8+bNw5o1a7BmzRr8+eefWLJkCU6cOGHUsTSGUwQ67hAdL7Lzc6w7ECIiIrI51QMdZnQsIyoqCkePHsUdd9yBp556Cl27dsXw4cPx008/6earAMCaNWtQVlaG3r17Y968ebp2xU0VFxeHwYMH4+6778Zdd92Ff/7zn2jbtm2d299zzz1YsGAB5syZg549e2L//v148cUX9ba5//77ceedd+KOO+5AcHAwNm7cWOf+JkyYgN9++w2DBg1Cy5Yt9e5bu3YtJk+ejKeeegodO3bEP/7xDxw6dAiRkZFGHePIkSPxv//9D4mJiejTpw/69++P5cuX65aGAURnuF69eiEgIEAX1AwaNAharVYvmwOI38WNGzfQq1cvTJo0CU8++SRCQkIaHMe4cePw0ksvYdGiRYiJicH58+fxxBNPGHUsjaGQqhcn2qC8vDz4+fkhNzfX6AlfANDqpSG4oNqFuwo34rs3HzLDCImIHFNTX38dGX82juPTT4EZM4CWLYELF8RSFNeuWXtUhisuLkZ6ejqioqJ08y6I7F19f9eGvv46RUbHW+0PAMgpzrHqOIiIiMj2yBmdPn3E1+vXgaIi642HiEzDKQIdH1dRupZXmmPdgRAREZHNuXJFfO3UqbI9MsvXiOyfUwQ6zSrW0ikoYzMCIiIi0idndCIigBYtxHU2JCCyf84R6HiKjM5NTY51B0JEREQ2Rw50wsOB5s3FdWZ0iOyfUwQ6Qd7+AIAiMKNDRERE+qoGOvac0bGD/lJEBjPF37NTBDohvv4AgFJFjlXHQURERLZFkoDMTHHdXjM6Li4uAIDCwkIrj4TIdOS/Z/nvuzHUphqMLQvxE6VrZaoc6w6EiIiIbMq1a0BZmbgeFmafGR2VSgV/f39kZWUBADw9PaFQKKw8KqLGkSQJhYWFyMrKgr+/f72LuDbEKQKdiAB/AIDkmoviYoAt5omIiAioLFsLDARcXe0zowMAYWFhAKALdojsnb+/v+7vurGcItAJ8xcZHbjnICdHfGJDRET2Y8WKFfjXv/6FjIwMdOnSBQkJCRg0aFCt2yYlJeGOO+6ocXtaWhqio6N133/zzTd48cUXcebMGbRt2xavvfYa7r33XrMdA9mmqvNzAPvM6ACAQqFAeHg4QkJCUCanqIjslIuLS5MyOTKnCHQCPP3FFfdc5OYy0CEisiebN2/G/PnzsWLFCgwcOBAff/wxRo0ahZMnT6Jly5Z1Pu6PP/7QWzE7ODhYd/3AgQMYN24cXnnlFdx7773YunUrxo4di3379qFfv35mPR6yLfIaOhER4quc0cnMBMrLAbWdnSmpVCqTnCASOQKnaEbg51aR0XG9iezr/JSDiMieLF++HNOmTcP06dPRqVMnJCQkIDIyEitXrqz3cSEhIQgLC9Ndqp78JSQkYPjw4YiLi0N0dDTi4uIwdOhQJCQkmPloyNZUz+iEhIjgRqutbFJARPbJOQIddz/d9YzreVYcCRERGaO0tBTJyckYMWKE3u0jRozA/v37631sr169EB4ejqFDh2LXrl169x04cKDGPkeOHFnvPktKSpCXl6d3IftXPdBRqSqzO/ZWvkZE+pwi0FEr1VCVewMAMnJyrDsYIiIyWHZ2NjQaDUJDQ/VuDw0NRWYdH7eHh4dj1apV+Oabb7BlyxZ07NgRQ4cOxZ49e3TbZGZmGrVPAIiPj4efn5/uEhkZ2YQjI1tRPdAB7LchARHps7PK08Zz0fpBgwJcZaBDRGR3qrfKlSSpzva5HTt2RMeOHXXfx8bG4uLFi3j77bcxePDgRu0TAOLi4rBw4ULd93l5eQx2HEBtgY69NiQgIn1OkdEBADf4AwCyC3KtOxAiIjJYUFAQVCpVjUxLVlZWjYxMffr374/Tp0/rvg8LCzN6n25ubvD19dW7kP1jRofIcTlNoOOhEPN0sgtyrDsQIiIymKurK2JiYpCYmKh3e2JiIgYMGGDwflJSUhBe5Uw2Nja2xj5//PFHo/ZJ9k+SmNEhcmROU7rmrfIHANwoYkaHiMieLFy4EJMmTULv3r0RGxuLVatW4cKFC5g5cyYAUVJ2+fJlfPbZZwBER7XWrVujS5cuKC0txfr16/HNN9/gm2++0e1z3rx5GDx4MN58803cc889+O9//4udO3di3759VjlGso7cXKCoSFxnRofI8ThNoOPj4g9ogNziHGsPhYiIjDBu3Dhcu3YNL7/8MjIyMtC1a1ds374drVq1AgBkZGTgwoULuu1LS0vx9NNP4/Lly/Dw8ECXLl3w3Xff4a677tJtM2DAAGzatAkvvPACXnzxRbRt2xabN2/mGjpORs7m+PkBnp6VtzOjQ+QYFJIkSdYeREPy8vLg5+eH3NzcRtdE3/6vWdhduBIdrryEPz5eZuIREhE5JlO8/joq/mzs388/A0OHAtHRQFpa5e3nzgFRUYCbm8j41NOjgoiswNDXX6eZo9PMwx8AcFPD0jUiIiKqfX4OULmOTkkJcO2aZcdERKbTpEAnPj4eCoUC8+fPr3e7DRs2oEePHvD09ER4eDgeeeQRXLPwK0eAl2hGUKjNsejzEhERkW2qK9BxdQVCQsR1lq8R2a9GBzqHDx/GqlWr0L1793q327dvHyZPnoxp06bhxIkT+Oqrr3D48GFMnz69sU/dKME+/gCAYjCjQ0RERHUHOgAbEhA5gkYFOgUFBZgwYQI++eQTNGvWrN5tDx48iNatW+PJJ59EVFQUbr31Vjz++OM4cuRIowbcWGF+/gCAUmWORZ+XiIiIbFN9gQ4bEhDZv0YFOrNnz8bo0aMxbNiwBrcdMGAALl26hO3bt0OSJFy9ehVff/01Ro8eXedjSkpKkJeXp3dpqvAAUbqmUedAo2ny7oiIiMjOMaND5NiMDnQ2bdqEo0ePIj4+3qDtBwwYgA0bNmDcuHFwdXVFWFgY/P398f7779f5mPj4ePj5+ekukZGRxg6zhuaB/uKKey5ycpq8OyIiIrJzV66Ir3LzgaqY0SGyf0YFOhcvXsS8efOwfv16uLu7G/SYkydP4sknn8RLL72E5ORk7NixA+np6bqF3moTFxeH3Nxc3eXixYvGDLNWQd7+4or7DVy/3uTdERERkZ1jRofIsRm1YGhycjKysrIQExOju02j0WDPnj344IMPUFJSApVKpfeY+Ph4DBw4EM888wwAoHv37vDy8sKgQYPw6quvIryWVxc3Nze4ubk15njq1My9Yi6Rey6yr2vQHqr6H0BEREQO6+ZNID9fXOccHSLHZFSgM3ToUBw7dkzvtkceeQTR0dFYtGhRjSAHAAoLC6FW6z+NvJ0l1ypt5lER6CgkXM7OBRBgsecmIiIi2yJnczw9AR+fmvfLgQ4zOkT2y6hAx8fHB127dtW7zcvLC4GBgbrb4+LicPnyZXz22WcAgDFjxmDGjBlYuXIlRo4ciYyMDMyfPx99+/ZFRG1FsWbiqnKFUuMFreomLl+7AQY6REREzqtq2ZpCUfN+uXQtN1dkfmoLhojItjVpwdDaZGRk4MKFC7rvp06diuXLl+ODDz5A165d8eCDD6Jjx47YsmWLqZ+6QW4aEdxk5Nyw+HMTERGR7ahvfg4gAhtfX3GdWR0i+2RURqc2SUlJet+vW7euxjZz587F3Llzm/pUTeaBZijCRWTmshsBERGRM2so0AFEVicvTwQ60dGWGRcRmY7JMzq2zEspMjrZN5nRISIicmaGBDpsSEBk35wq0PFxEQ0Jrhcxo0NEROTM6ltDR8YW00T2zakCHX83EejkljCjQ0RE5MyY0SFyfE4V6AR4itK1vDJmdIiIiJyZMYEOMzpE9smpAp0gL5HRuallRoeIiMiZGdqMAGBGh8heOVWgE+orMjrFYKBDRETkrEpKgOsVxR3M6BA5LqcKdCKaiYxOiYqla0RERM4qM1N8dXUFAupZP1zO6Fy9CpSWmn9cRGRaThXoNA8Ur2ZalxsoK7PyYIiIiMgq5LK1sDBAoah7u6AgEQwBlV3aiMh+OFWg0yJQZHTgcR03WL1GRETklORAp77W0oAIgthimsh+OVWgE+xdkZ/2uMFAh4iIyEnJ2Zn65ufI2GKayH45VaDTzL0io+N6E1nXWGxLRETkjAzpuCZjRofIfjlVoOPn7gdIohj3YjZTOkRERM7ImECHGR0i++VUgY5SoYS63B8AcPkaAx0iIiJn1JhAhxkdIvvjVIEOALhJonwtI4ctpomIiJxRY0rXmNEhsj9OF+h4QjQkyMpnRoeIiMgZsXSNyDk4XaDjpRIZneybzOgQERE5m/JyICtLXDcmo3PlCqDVmm9cRGR6Thfo+LqIjM6NImZ0iIiInM3Vq4AkASoVEBzc8PZhYYBSqR8gEZF9cLpAx7+ixXRuCQMdIiJ7sWLFCkRFRcHd3R0xMTHYu3evQY/75ZdfoFar0bNnT73b161bB4VCUeNSXFxshtGTLZHL1kJDRbDTEBcXsS3AhgRE9sbpAp1AT5HRydewdI2IyB5s3rwZ8+fPx+LFi5GSkoJBgwZh1KhRuHDhQr2Py83NxeTJkzF06NBa7/f19UVGRobexd3d3RyHQDbEmPk5Ms7TIbJPThfoBHuLjE6hlhkdIiJ7sHz5ckybNg3Tp09Hp06dkJCQgMjISKxcubLexz3++OMYP348YmNja71foVAgLCxM70KOrymBDjM6RPbF6QKdMD+R0SkCMzpERLautLQUycnJGDFihN7tI0aMwP79++t83Nq1a3HmzBksWbKkzm0KCgrQqlUrtGjRAnfffTdSUlLqHUtJSQny8vL0LmR/GhPosMU0kX1yukAnIkBkdEpVNyBJVh4MERHVKzs7GxqNBqHyJIkKoaGhyMzMrPUxp0+fxnPPPYcNGzZArVbXuk10dDTWrVuHbdu2YePGjXB3d8fAgQNx+vTpOscSHx8PPz8/3SUyMrLxB0ZWw4wOkfNwukCnZbAIdCS36ygstPJgiIjIIAqFQu97SZJq3AYAGo0G48ePx7Jly9ChQ4c699e/f39MnDgRPXr0wKBBg/Dll1+iQ4cOeP/99+t8TFxcHHJzc3WXixcvNv6AyGqY0SFyHrV/1OXAIpqJ0jV4XMe1a4CXl3XHQ0REdQsKCoJKpaqRvcnKyqqR5QGA/Px8HDlyBCkpKZgzZw4AQKvVQpIkqNVq/PjjjxgyZEiNxymVSvTp06fejI6bmxvc3NyaeERkbXKgExFh+GPYjIDIPjldRifIM1Bc8biO7GzWrhER2TJXV1fExMQgMTFR7/bExEQMGDCgxva+vr44duwYUlNTdZeZM2eiY8eOSE1NRb9+/Wp9HkmSkJqainBjPuYnu3TlivjamIzO5ctg2TuRHXG6jE6gHOioynHhah5ugZ91B0RERPVauHAhJk2ahN69eyM2NharVq3ChQsXMHPmTACipOzy5cv47LPPoFQq0bVrV73Hh4SEwN3dXe/2ZcuWoX///mjfvj3y8vLw3nvvITU1FR9++KFFj40sS6sVC4YCjQt0bt4EcnMBf3+TD42IzMDpAh13tTuUGi9oVTdx/u9sgIEOEZFNGzduHK5du4aXX34ZGRkZ6Nq1K7Zv345WrVoBADIyMhpcU6e6nJwcPPbYY8jMzISfnx969eqFPXv2oG/fvuY4BLIR2dlAeTmgUFQuAmoIT0+gWTPgxg2R1WGgQ2QfmlS6Fh8fD4VCgfnz59e7XUlJCRYvXoxWrVrBzc0Nbdu2xZo1a5ry1E3irgkCAFy6fs1qYyAiIsPNmjUL586dQ0lJCZKTkzF48GDdfevWrUNSUlKdj126dClSU1P1bnv33Xdx/vx5lJSUICsrCz/88EOd6+2Q45Dn5wQFAS4uxj2W83SI7E+jMzqHDx/GqlWr0L179wa3HTt2LK5evYrVq1ejXbt2yMrKQnl5eWOfusk8FYEoxHlcyc222hiIiIjIshrTcU3WogVw7BhbTBPZk0YFOgUFBZgwYQI++eQTvPrqq/Vuu2PHDuzevRtnz55FQIDoeNa6devGPK3J+KiCkA3g73xmdIiIiJxFUwIdtpgmsj+NKl2bPXs2Ro8ejWHDhjW47bZt29C7d2+89dZbaN68OTp06ICnn34aRUVFdT7G3KtP+7uKhgTXipjRISIichZNzegAzOgQ2ROjMzqbNm3C0aNHcfjwYYO2P3v2LPbt2wd3d3ds3boV2dnZmDVrFq5fv17nPJ34+HgsW7bM2KEZLMgzCMgDbpQwo0NEROQsGrOGjowZHSL7Y1RG5+LFi5g3bx7Wr18Pd3d3gx6j1WqhUCiwYcMG9O3bF3fddReWL1+OdevW1ZnVMffq08HeIqOTr2FGh4iIyFk0Zg0dGZsRENkfozI6ycnJyMrKQkxMjO42jUaDPXv24IMPPkBJSQlUKpXeY8LDw9G8eXP4+VW2ce7UqRMkScKlS5fQvn37Gs9j7tWnw/2CgCtAIRjoEBEROQtTzNFh6RqR/TAq0Bk6dCiOHTumd9sjjzyC6OhoLFq0qEaQAwADBw7EV199hYKCAnh7ewMA/vzzTyiVSrSQPx6xsObNREanRMnSNSIiImdhijk6164BRUWAh4fpxkVE5mFU6ZqPjw+6du2qd/Hy8kJgYKBuxem4uDhMnjxZ95jx48cjMDAQjzzyCE6ePIk9e/bgmWeewaOPPgoPK71KtA4R6+ho3LJRWmqVIRAREZEFSVLTAh1//8rgRi6BIyLb1qQFQ2tTfYVqb29vJCYmIicnB71798aECRMwZswYvPfee6Z+aoO1DBIZHXhewzUmdYiIiBxeTg5QUiKuNybQUSg4T4fI3jR6wVBZ9dWo161bV2Ob6OhoJCYmNvWpTCbEW2R04JmNv/+WEB6usO6AiIiIyKzkbI6/P2BgP6UaWrQATp/mPB0ie2HyjI49CPSsyOioynApq8C6gyEiIiKza0praRlbTBPZF6cMdDxdPKHUiELbc3+z8xoREZGja8r8HBkXDSWyL04Z6ACAq0ZkdS5xkg4REZHDa8oaOjJmdIjsi9MGOp4Qgc6VHGZ0iIiIHJ0pMzoMdIjsg9MGOt4q0ZAgK58ZHSIiIkdnikCHi4YS2RenDXT8XURGJ7uQGR0iIiJHZ8qMTkYGUF7e9DERkXk5baAT4CEyOjdKmNEhIiJydKYIdEJCALUa0GqBq1dNMy4iMh+nDXSCvURGJ1/DjA4REZGjM0Wgo1JVPp7zdIhsn9MGOuF+IqNzU2KgQ0RE5Mjy84GCimXzmhLoAGwxTWRPnDbQad5MZHSKFSxdIyIicmRyNsfbG/Dxadq+2GKayH44baDTOlRkdMpds1FaauXBEBERkdmYomxNxowOkf1w3kAnRGR04JmNbFavEREROSxTBjrM6BDZD6cNdMJ8QsQVz2xkZUnWHQwRERGZjTkyOgx0iGyf0wY6wZ7B4oq6FOcy8qw7GCIiIjIbc2R0WLpGzmrtWmD9emuPwjBOG+h4uHhAVe4NADh7NcvKoyEiIiJzMVdGR2JBCDmZrCxg2jRg6tTKToa2zGkDHQBw14rytXPZDHSIiIgclRzoREQ0fV/yPkpKgOvXm74/Inty7JgI8DUa4K+/rD2ahjl1oOOjEIHO5RwGOkRERI7KlBkdNzcguKL6nfN0yNmcOFF5/c8/rTcOQzl1oOPvIgKdqwUMdIiIiBzVlSviqykCHYAtpsl5VQ10Tp+23jgM5dSBTqCHCHSyixjoEBEROaKiIiAnR1w3VaDDFtPkrBjo2JFQb5F7zi3728ojISKi+qxYsQJRUVFwd3dHTEwM9u7da9DjfvnlF6jVavTs2bPGfd988w06d+4MNzc3dO7cGVu3bjXxqMkWZGaKr25ugL+/afbJjA45I0lioGNXmvuLjE6BxIwOEZGt2rx5M+bPn4/FixcjJSUFgwYNwqhRo3DhwoV6H5ebm4vJkydj6NChNe47cOAAxo0bh0mTJuG3337DpEmTMHbsWBw6dMhch0FWUnV+jkJhmn0yo0POKCOjMjsKcI6OzWsVJAKdIiUDHSIiW7V8+XJMmzYN06dPR6dOnZCQkIDIyEisXLmy3sc9/vjjGD9+PGJjY2vcl5CQgOHDhyMuLg7R0dGIi4vD0KFDkZCQYKajIGsxZSMCGRcNJWckZ3PkQD87Wz/wsUVOHei0CRWBjsY9C8XFVh4MERHVUFpaiuTkZIwYMULv9hEjRmD//v11Pm7t2rU4c+YMlixZUuv9Bw4cqLHPkSNH1rvPkpIS5OXl6V3I9pkz0GHpGjkTOdDp16/y/8nWy9cY6ACAVxb+5jQdIiKbk52dDY1Gg9DQUL3bQ0NDkSlPvqjm9OnTeO6557Bhwwao1epat8nMzDRqnwAQHx8PPz8/3SUyMtLIoyFrMOUaOjKWrpEzOn5cfO3SBWjfXlxnoGPDQr0rAh3PbGRc1Vh3MEREVCdFtckVkiTVuA0ANBoNxo8fj2XLlqFDhw4m2acsLi4Oubm5usvFixeNOAKyFnNmdHJz7WN1eCJTkDM6VQMdW5+nU/tHXU4i0CNQXFFIOJtxDX0RYt0BERGRnqCgIKhUqhqZlqysrBoZGQDIz8/HkSNHkJKSgjlz5gAAtFotJEmCWq3Gjz/+iCFDhiAsLMzgfcrc3Nzg5uZmgqMiSzL1GjoA4OMjLvn5onytY0fT7ZvIFkkScPKkuN6lC3DunLju0Bmd+Ph4KBQKzJ8/36Dt62vzaQ0uKhe4lAUAAM5eZe0aEZGtcXV1RUxMDBITE/VuT0xMxIABA2ps7+vri2PHjiE1NVV3mTlzJjp27IjU1FT069cPABAbG1tjnz/++GOt+yT7Zo6MDsB5OuRcLl0C8vIAtRro0EFcANsPdBqd0Tl8+DBWrVqF7t27G7R91TafV69ebezTmpyHNgRluI6L17IAdLH2cIiIqJqFCxdi0qRJ6N27N2JjY7Fq1SpcuHABM2fOBCBKyi5fvozPPvsMSqUSXbt21Xt8SEgI3N3d9W6fN28eBg8ejDfffBP33HMP/vvf/2Lnzp3Yt2+fRY+NzM9cgU7z5kBaGufpkHOQy9batwdcXfVL1yTJdK3bTa1RGZ2CggJMmDABn3zyCZo1a2bQY+pr82lNPipRrnY5ly2miYhs0bhx45CQkICXX34ZPXv2xJ49e7B9+3a0atUKAJCRkdHgmjrVDRgwAJs2bcLatWvRvXt3rFu3Dps3b9ZlfMgxlJVB12zIXBkdBjrkDKrOzwGAtm3F19xc0WbaVjUq0Jk9ezZGjx6NYcOGGbR9Q20+q7NkC89mLiLQySpgoENEZKtmzZqFc+fOoaSkBMnJyRg8eLDuvnXr1iEpKanOxy5duhSpqak1bn/ggQdw6tQplJaWIi0tDffdd58ZRk7WJBeQqNVAUJBp9y13XmPpGjkDOdCRE+MeHoDceNKWy9eMDnQ2bdqEo0ePIj4+3qDtDWnzWZ0lW3gGeYpA51oJAx0iIiJHIpethYUBShP3mWVGh5xJ1dbSMnuYp2PUv/3Fixcxb948rF+/Hu7u7g1ub0ybz6os2cIz1DsYAJBbxkCHiIjIkZhrfg7AZgTkPLRa/Y5rMntoMW1UM4Lk5GRkZWUhJiZGd5tGo8GePXvwwQcfoKSkBCqVSnefoW0+q7NkC88WzUKAq0ABGOgQERE5EnMGOlw0lJzFhQvAzZuAiwvQrl3l7fawaKhRgc7QoUNx7NgxvdseeeQRREdHY9GiRXpBDlDZ5rOqFStW4Oeff8bXX3+NqKioRg7bdKJCQoBTQInqb5vuGkFERETGMccaOjI5o5OVBZSWik5URI5Inp/TsaMIdmT2ULpmVKDj4+NTo22nl5cXAgMDdbc3ps2nNbULF3N0tB5ZyMsD/PysPCAiIiIyCXNmdIKCRHBTWiqep6IJIJHDqd5xTVY1o2OryQITT81rXJtPa2oVJAIdeGei2iLZREREZMfMGegoFOy8Rs6hrkAnKko0+bh5s/J/zdY0esFQWfWWnuvWrat3+6VLl2Lp0qVNfVqTCfeuePVzy8e5y4Xo2NHTugMiIiIikzBnoAOIQCc9nfN0yLHVFei4ugKtWwNnz4qsTkSExYfWIJNndOyNr5svlBrRQe6Py0zpEBEROQpzBzpsMU2OrmrHtdpmndj6PB2nD3QUCgXcNWEAgLNZDHSIiIgcgUZTuWCouT5pZotpcnTp6UBREeDmBrRtW/N+W++85vSBDgD4KsRHPeev22iBIRERERklO1sEOwoFEBJinudgi2lydHLZWnQ0UK25MgDbX0uHgQ6AAFeR0cnIZ0aHiIjIEcitpUNCAHWTZyTXjhkdcnR1zc+RMaNjB0K9REYnu4iBDhERkSMw9/wcgBkdcnwNBTryHJ2//hLzeWwNAx0Azf1ERudGOUvXiIiIHIElAh05o3Plim2e5BE1VUOBTsuWYhHRkhLg4kXLjctQDHQAtA4SgU4BmNEhIiJyBJYIdMLCxBygsjLg77/N9zxE1qDRAKdOiet1BTpqNdCmjbhui+VrDHQAtA8Tr4KlrhnQaKw8GCIiImoySwQ6Li4i2AE4T4ccz9mzQHEx4OEhFgetiy3P02GgA6Bj84pXKe9MfiJDRETkAORAx9yLGHKeDjmq48fF106dau+4JrPltXQY6ABo4VfxcY/3VVzJYJEtERGRvbNERgfgoqHkuBqanyOz5RbTDHQAhHiFAJICUGrw56Vsaw+HiIiImsjSgQ5L18jRGBvoMKNjo1xULnAtDwIAnM5gQwIiIiJ7JkmWC3RYukaOytBARy5dO3sWKC8375iMxUCngpck5umkZ7PFNBERkT27fh0oLRXX5WYB5sKMDjmi8nLgjz/E9YYCnebNAXd38Zjz580/NmMw0KngpxKvhJdymNEhIiKyZ3I2JyAAcHMz73M5YkanuBj45BPg44+tPRKylr/+Eh8WeHoCrVrVv61SCbRrJ67b2jwdBjoVgtxFbjuzgIEOERGRPbNU2Rqg34xAksz/fOZUXAx88AHQti3w2GPAzJnA4cPWHhVZQ9WyNaUB0YKtztNhoFMh3EdkdK6XsnSNiIjInlky0JEzOjdvAnl55n8+c6ga4MydC1y5UnnfoUPWG5cj2bkTmDgRuHjR2iMxjNxauqGyNZmttphWW3sAtqJls3DgOpCnZUaHiIjInllqDR1AlPY0awbcuCHm6fj5mf85TaW4GPj0UyA+vjK4adECeP55MdfizTeZ0TGFVauAWbMAjUYsMrt2rbVH1DBDGxHImNGxcW1CREbnppKBDpEtufeljWg2dRr2Hrlm7aEQkZ2wZEYHsL+1dGrL4LRoAaxYIeZmPPEEcOutYlsGOo2n1QJxccDjj4sgBwA2boRdLE7f2ECHc3RsVKdIEehoPTKQn2/lwRCRzvaCV5ATtQZD1g3D/pTr1h4OEVUoLweOHhUnzKtWiZNnW2HpQMdeGhIYEuDIzRt69xZfT50Cz4saobgYGD8eeOMN8f3SpUCfPkBJiWj0YMtKSysDFmMDnfPnKzse2gIGOhXaBFe8GvpcYYtIIhsiKcoAAOXBqbht9TDsT7WtYOf5j/egzYznkXw819pDITKra9eA774DFi8G7rhDlGjFxIgT5scfB3r0APbssfYoBbkMy9IZHVs9fzAmwJGFhYltJEkEtGS4a9eA4cOBzZsBtRpYtw5YskT87AHxcy8rs+oQ63X6tPggw8cHiIw07DFhYYC3t8hinT1r3vEZg4FOhea+FR/HuBXgz/N2OpuQyAFJqGhjpFGjPDgFt306HId+v2HdQVXx/snFSG8Rj/4f3Y6fD2VZezhEJqHVAidPivkbjz4KdOoEBAUBd98NvP46kJQEFBYC/v7AnXeKgOLPP4HbbhMnzblWjvuZ0REaE+BU1aeP+MryNcOdOQPExgL79okPA374AZgyRdw3diwQEiIC4v/8x6rDrJdctta5M6BQGPYYhcI2y9cY6FTwdvWGulzMIDx50UY/kiFyRgoR6MxtsxLqkmCUBx/FrR8Px6/HGhfszE34Aa1mPIPDx3JMMjyN8iYAkXEa/sUgfPPTBZPsl8iS8vOBn34CXn0VuOsuIDBQlKzMmCEmTp86Jbbr2BF45BFRenPihPjk+vvvRVA0Y4bY5qOPxGO3bbPOsUiS9ebo2EpGp6SkaQGOTA50jhwx73gdxYEDQP/+IiPSsiXwyy/AkCGV97u5iZbdAPDee9YZoyGMnZ8js8WGBAx0qvDSio9kTl+1kVcqIgIqMjrdw7ripyk/QVUShPKQZAz8eAh2/Wp8BmX1ucW40OJtxK66HT/ub3rzEQkVM0w1rtAG/IkHtw/EJ1tPNXm/pG/FihWIioqCu7s7YmJisHfv3jq33bdvHwYOHIjAwEB4eHggOjoa7777rt4269atg0KhqHEptqWJJhbwv/8BvXqJzMywYcCLL4rAJScH8PAAbr9ddOD63/+A7GwR8KxZA0yfLj7tldfX8PcX83R27RILB16+DNxzj/gE++pVyx5Tfr7INgHOm9GZMaNpAY5MnqfDjE7DvvlGBDXZ2cAttwAHD9YeKDz+uChn27cPSEmx/DgNIQc6Xbsa9zgGOjYuQC1eqc5dt5FXKiKCHOgoFAoM7tgNP0/5GeriEJQHp2LYhsH47y7j/l81SnEGpAn6DXd+NRBffN+0YmJJoQUALGj7CbyKoiH5XsJjBwbhzc9Y1G4qmzdvxvz587F48WKkpKRg0KBBGDVqFC5cqD175uXlhTlz5mDPnj1IS0vDCy+8gBdeeAGrVq3S287X1xcZGRl6F3d3d0scks145hkgNVWUqrVsCTz0kPik+cgRUX62axfw2mvA6NEiy9OQ228Hfv8dWLQIUKmAr74SZW9r11puMU05m+PrC3h5WeY5bS2jI699s2RJ4wIcmRzonD0rsndUkyQBy5cDDz4oSgXvvhvYvbvuIDsiQmwLAO+/b7lxGsPYNXRktriWDgOdKkI9RKCTcdNGXqmISDdHR1lRKDy4YzccfHwvXIsioQ34A/d+eys++99fhu9PUQ4AUJR5QvI/iwk/D0TCxt+bMD6R0Wnp1xJ/PLcHzYpiAK9sPHfqdjz7oY3MzLZzy5cvx7Rp0zB9+nR06tQJCQkJiIyMxMqVK2vdvlevXnj44YfRpUsXtG7dGhMnTsTIkSNrZIEUCgXCwsL0Ls7k3DmRoVGpxMnw+fOi9e3cuaLJgItL4/br4SE6TR0+LD7ZvnFDzPMZPtwyk5QtXbYGVGZ0srOt331OkgD5M4DJkxsX4MiaNRMZOgBITm762BxNebn4f3nqKfFznz1bzL3x9q7/cXJTgi++EH8ztqSkRLweAI0vXeMcHRsV6S8+kskuZaBDZDMU+oEOAMS07oDf5u2DZ1F7SH7nMWX3IPx70zGDdicpRGDycs8N8C3qDnhnYsHvg/H8R/saNTx5f2qVCs39g3HmpZ8RXnIb4JaPf2WOwLhlWyz2SbYjKi0tRXJyMkaMGKF3+4gRI7B//36D9pGSkoL9+/fjtttu07u9oKAArVq1QosWLXD33XcjxVbrSMxkxw7xdcAAMZfD1Hr1EpmFt94C3N3FHKCuXYF33hEniOZijUCnWTMR4AGVHd+s5e+/RbClUFRmmpqC5Wu1KygA7r0X+PBD8bN+5x2RoVGpGn5s//7i52qLrab/+EOs+ePnZ/yCu3Kgc+lSZfmotTHQqaJtsPhIJl9ioENkOypL16qKDm+JtGf36oKV+am34YWPDzS8t4qMTkv/Fjjzwm6EFt8KuOci/tJwTH5tm9FBiZzRUavEy2kzT1/8tfR7tNf+A1CX4EvpAQxc8L5ZT+wcWXZ2NjQaDUJDQ/VuDw0NRWZm/XOsWrRoATc3N/Tu3RuzZ8/G9OnTdfdFR0dj3bp12LZtGzZu3Ah3d3cMHDgQp+upuSgpKUFeXp7exZ59/734OmqU+Z5DrRblcceOifkLRUXA00+LE73UVPM8p6VbSwP6QYW15+mcPy++RkQArq5N3x87r9WUkSE6DP7vfyKI/+orYOFC4zqUVW01bUvvD1UbERh6PLLAQDFfDxDd52xBkwKd+Ph4KBQKzJ8/v85ttmzZguHDhyM4OBi+vr6IjY3FDz/80JSnNZvoitxzseslm/qjI3JuNTM6spYBofhrcRKCivsDHjfw2sUhmPDKf+sNVuRAx1WtQpC3P84s+wFtykcDLsX4vPReDFr4oXH//xVzdNTKyo/xPF09cHLJNxjk8TigkHCg2ZPoMOcZ5OVrjdgxVVU90JUkqcZt1e3duxdHjhzBRx99hISEBGzcuFF3X//+/TFx4kT06NEDgwYNwpdffokOHTrg/XqK5uPj4+Hn56e7RBq6wIQNKi0VGRZAtIc2t3btgJ07gdWrxYlQcrL4RPv550XwY0rWyOgAttOQQA50WrY0zf7YeU3fiRMiUD96VLRc37ULuP9+4/czbpxoNX3pkm21mm5sxzVABEa2Nk+n0YHO4cOHsWrVKnTv3r3e7fbs2YPhw4dj+/btSE5Oxh133IExY8bYZIlAl8iKj2N8L1u8SwwR1U6qI6MjC/ZphrNLdqKt5m7ApRhflN9Xb7CiC3Rc1AAAL1dPnFq6FbFu0wGlFr/4z0H72U8hN8+woEQuXXOpVq+gVqqx+5mVmBT+OgAgPfxtRD01AReulBi0XxKCgoKgUqlqZG+ysrJqZHmqi4qKQrdu3TBjxgwsWLAAS5curXNbpVKJPn361JvRiYuLQ25uru5y8eJFo47FluzbB9y8KRb569nTMs+pUIi5OidPihNDjQaIjxcLje7ebbrnsVagYysNCeRAp1Ur0+yvVy/RXe/y5cqfrbP6+Wdg4EAxB6p9e9FZrX//xu3LzQ147DFx3ZZaTTcl0AFsb55OowKdgoICTJgwAZ988gmaNWtW77YJCQl49tln0adPH7Rv3x6vv/462rdvj2+//bZRAzanlv4VH8d4ZeH8RRtespbImSjkQKfuTXzcvXBq6VYMdH9MF6xEz3kO+QW1BCsVgYmrWq27yUXlgl8WrcLEiqDkXMRytHrmQZy50HCRcfXSNb2nUijw2WNxeKnbZ4BGjevNN6HjKyORfCKnwf2S4OrqipiYGCQmJurdnpiYiAEDBhi8H0mSUFJSd5ApSRJSU1MRXs/ZsZubG3x9ffUu9kqenzNypPHlKU0VHg58/TWwZYu4fvq06NY2f75pOrMxoyO+mirQ8fYWnfMA5y5f+7//E/8vubnArbeKNXOaOrdt5kxR3rl3r/lKOY3V2NbSMltrMd2oQGf27NkYPXo0hg0bZvRjtVot8vPzERAQUOc21qqDDvIMgkLrAigkHD/v5B9bENmMukvXqlIr1dj77EcYH/oqAOBM+JuIWjgJFzP0T26rlq5VpVAo8PljcXi5xxeAxhW5EVvQ+c0hSDrcwFo9culaPTNQl903CWuHfQ9FqQ+Kw3aj38e31rmwqCSJNUyo0sKFC/Hpp59izZo1SEtLw4IFC3DhwgXMrFh5Ly4uDpMnT9Zt/+GHH+Lbb7/F6dOncfr0aaxduxZvv/02Jk6cqNtm2bJl+OGHH3D27FmkpqZi2rRpSE1N1e3T0Vlifk5D7r1XZHfkT7X//W/TTMxmRkd8NVWgA7B87bvvgKlTxVyahx4CEhMNa7fekObNgQceENdtodV0UVHjO67J7L50bdOmTTh69Cji4+Mb9YTvvPMObt68ibFjx9a5jbXqoJUKJTzKRYuJU1e4lg6Rbai/dK0qhUKBDTMXY3GXdYBGjWvNv0CHl0dif0qVBSCUcqCjrnUfL/7zYWy6ayeUJQEoDTmEIV/0x8db0uoeXZWua/WZOngYdo7fC5eiCGgCT+CBHX2x9NNDNbYb8MxbaLasNR5543/s1lZh3LhxSEhIwMsvv4yePXtiz5492L59O1pVnMllZGToramj1WoRFxeHnj17onfv3nj//ffxxhtv4OWXX9Ztk5OTg8ceewydOnXCiBEjcPnyZezZswd9+/a1+PFZ2qVLYp0MpVK0fLYmf3/g44+Bt98W3y9Y0PQTJDnQMbZjVFPZSkZH/lcwZaBjrc5rWi2wdKnI/lnTtm3i6/jxwIYNogGBqdhSq+lTp8SHbQEBQAOVwXWy69K1ixcvYt68eVi/fn2jFlXbuHEjli5dis2bNyMkJKTO7axZB+2nEB/JpGez8xqRTZDbSysNr6959YEpWD10uy6Dcutn/fDJ1lMV+6soXXOpOzAZ138Q9k/bD/eiNpD80zHzSD9Mf3N77YGHbo5Owy+nQ7r0wLF5Bys6xV3FsvO34b6XNkFbpcLuWPkWwP881hX/A33n/wvFxYx2AGDWrFk4d+4cSkpKkJycjMGDB+vuW7duHZKSknTfz507F8ePH8fNmzeRm5uLo0eP4oknnoBSWfk7evfdd3H+/HmUlJQgKysLP/zwA2JjYy15SFYjl6316ydOaGzBggWiK1thITBpUuO7UBUVidIigBkdc2R0Dh+23MKvgGhgsWwZMH26ZZ+3umMVqxeMGSM+IDCl2FixblVxMfDpp6bdt7Ga0nFNJgc6V68CttCY0qhfV3JyMrKyshATEwO1Wg21Wo3du3fjvffeg1qthkajqfOxmzdvxrRp0/Dll182WPJmzTroYDfxkcylPAY6RLag+oKhhnr0tuHYPXk/3IpaQ/I/g8d+7Y9Zy3/UZXTcXGrP6Mj6te2Iv547iNDiQYBbPlYX3V174CEHOmoDFk8A0DE8EheW7EM7zRhAXYKtqofRbe4y3Lwp9qtRVLSgUkg4EvAsWs17pEb5HVFTyIGOJbqtGUqpBNatExmeQ4eA115r3H7kbI6HB2DpKVRyRicjw3rtgvPzxQKtgOm6rgGiYYSLC3DtWmUgZQk7d4qvN25YL1MmSSIDCgDdupl+/7bUarqpjQgAsf5OcLC4LpfBWZNRgc7QoUNx7NgxpKam6i69e/fGhAkTkJqaClUdpRsbN27E1KlT8cUXX2D06NEmGbi5RPiIV6qsIgY6RLbB8NK16gZ17Iq/Fv2qWytnZe5dgKoi0KmjdK2q5v7BOP/KTsS6ztAFHpFPTsG5S5VLn0u1tJduiJ+HD04t3Yo7fZ8GAJwMWYqWC8bj7MUiaJQi0OkmTQS0SmRF/B/av2LAXCEiA5SVifkFgHXn59QmMlKc6AHAK6+IgMdYVdfQsXSThdBQsVikRgOrdW6Vg5BmzQAfH9Pt182t8iTfkuVrcgt0APj9d8s9b1Xnz4sA0sWlcv6JqY0bJ4KDixeB//7XPM9hCFMEOoBtzdMxKtDx8fFB165d9S5eXl4IDAxE14r2DNUnhW7cuBGTJ0/GO++8g/79+yMzMxOZmZnIlXPLNiYqUOSeb2g5R4fIJshd19C4s5YWzYJx7uWd6K2aCigrs871la5V5aZ2xS/PfYxZUe8BWhWym3+OjvG348cDFR8dG5nRkamUKny/4F94tuOnuo5snd64A2Ve5wAAzw6eh0/v+B7KUj+UhO7HkE198N5mK73Tk8M4cECUkwQFiXIZW/Pww+Ki0QATJ4rV541hrUYEgAhy5HlB1ipfM0fZmszSC4deuwZUXYlELh+zNPl5O3USwY45uLvbRqtpUwU6tjRPx8SVhjUnhX788ccoLy/H7NmzER4errvMmzfP1E9tEh3CREbnpvIyJwIT2YTGla5V5e7ihl8Xr8GUiLcASQGUeSK0mZfBj1coFPhw8lysHboDypJmKA05hDu/6YM3PjuiC3Rqay9tiDcfmob1IxN1zQ+gEq3tfT08MO32Edj/6CF4FLaH5HsB834fgCnx9S+ISlSfqm2lTT3XwFQ+/FDMd/nrL+Cpp4x7rDUDHcD6DQksEehYqvParl3683KsldGRn9ccZWtVPfGECJb37AF++828z1WbmzeB9HRxvbGtpWW21GK6yS9zSUlJSEhI0H1ffVJoUlISJEmqcVm3bl1Tn9osurUWr1Ja70u6OlcisqaKjI4RzQhqo1AosG7GM9g59jC+H7cLfl4eRu9j6uBh+HXGIXgVRkPyuYy407cCLqKMrfqCocaYMPB2/DrjIDwLK+siAnzF+Pq17Yj0xQfRvGQo4HoTn5Xcix5PvoKCm4YtaEpUldxW2pbm51TXrJlYswQAVq0CjFl2z9qBjrUbElgi0ElOhl4DFXORy9bkk2ZrBTpyRsfcgU7z5mIhXcA6rabT0kRgGRxcOcemsRwq0HE07YIrWln7XMa58zyRILI6RdMzOlUN7RyDO7s1voVwTFR7nH/xINqUjwbUlU0CjC1dq22/6YsPohseQpvy0Yjt1Fp3X6hvANJf+R6D3WcDCgnHgl5C84X34bdTNtDShuxGRoZYlFChEBkdWzZkSGU2Z/p0IMvAKWrWai0ts3ZGxxytpWWdO4smD3l5lilJkhsRyAVAp04B9az5azaWCnQA4MknxdcNG0TpniWZqmwNsOM5Os4gwicCkJSAuhS/n7XSbEIiqsK0gY4pBHr74fTL23BfwEviBo0LWgQ3feZviG8z/L5kI8688j+oqtUVuahcsHvRB1gUvRrQuCIv4r+4ZWVffPqfutf4Iarqhx/E15iYpn9iawmvvSZOLrOyDG8vbO2MjrzsX5UKfosyZ0ZHrQZ69RLXzV2+dv68KF1UqcRcLX9/MW/r1CnzPm91JSXAH3+I65YIdAYMED9ja7SaNmWg066d+HrtGnD9etP31xQMdKpxUbnAo0x8JHPcWq9URKQjNaHrmjkpFUp8M3cZvr3nADbe9SNC/E3Y4qgeb4x7FN/euw+uRS2gDfgDM37ti0mvbeW8HWqQPD/H1rqt1cXNTXyy7eoqytc++aThx1g70GnTRnw9c8Y6zy8HOqZsLV2VpRoSyGVrffqIdsVykGHp8rVTp0SA5edXWZZoTgpFZVbH0q2mTRnoeHlVZlWtndVhoFOLZkrxCvFnlgWbxRNR7UxcumZqd/fsj4f6327Z5+zVB389m4ywktsAtwKsL78PnecsRk5u3WuZkXMrLwd+/FFct+X5OdV16wa8/rq4vmBBwydN1g502rYVX60R6JSWVh6/OTI6ANC7t/hqqUBHXnaxe3fx1dKBTtWyNUu9BT30kOiKeOECsG2bZZ4TMG2gA9jOPB0GOrUIcxevEOdzmdEhsj7bDnSsJTIgBBdeScRQr/kAgFMhr6PFotE4fNzKdQJkkw4fFosuNmsG9G38FDWrWLAAuOMOoLAQmDSp7k+5S0uB7Gxx3doZnevXgZwcyz73xYuivM/Dw3yliXJGJyXFfNkGSaoMdIYOFV/lQMfSLaYtOT9HZo1W0wUFldlAUwU6tjJPh4FOLVr5i4zO1WJmdIisTs7oNLHrmiNyUblg59PvYmm3DUCZB26G/4B+q3vj3Y2p1h4a2Ri529rw4WKuhT1RKkUXNj8/sYjoa6/Vvl1mpvjq4gIEBlpufFV5e4uFQwHLZ3Wqlq2Z63Oh9u0BX18xh0TOAJjaiRNiwVUPDyA2VtxmCxkdS5JbTe/ebZljPnlSfA0LM93/jq2spcNApxYdQkVGJ1diRofI+mxzjo4tWXLfeCQ+dABuhVGQ/NOx8GR/3Bm32qL13WTb7G1+TnWRkWLOAgC88ooIeKqTy7bCwixXZlQba5WvmbMRgUyprFxo1lzla3I2Z9AgMU8LqMwyZGQAf/9tnuetjbUCnRYtgPvuE9ct0Wra1GVrAEvXbFr3ViKjU+x+HqWlVh4MkdNj6ZohhnXtgfOLjyCqTLS9/sF9OlrMfhRnLhZae2hkZX//Xdkly9bbStdn/Hgxf0GjEZ24Cgr077f2/ByZtQIdc7aWrsrcC4fKbaXlsjUA8PGpLAu0VPnajRuVbcKbuoBmY1iy1fTx4+KrKQOdqqVr1myWw0CnFt1bVrxK+J232qJfRFTBxpsR2JJQ3wD89co2jA99HdAqcTViLaLfisWG7/+y9tDIin74QZxo9Oxp/SCgqVasEJ92//VX5To7MmuvoSNz5IwOYN7Oa+XlolwLqGxEILP0PB355L9lS9He2tIGDhT/s0VFwOrV5n0uc2R02rQRmdW8PMPXwTIHBjq1kOfowCMHJ89wQT4i6+IcHWMoFUpsmBmHdUMToS4OQXnQ75i4JwZvbNpt7aGRldh72VpVzZqJ+ToAsGqVaDstc/aMjrlbS8vkzmu//y7m6pjS4cNAfj4QECBO8quy9Dwda5Wtyaq2mv7wQ/O2mjZHoOPuXvm3aM3yNQY6tfBx84G6rBkA4LdznKdDZFUKztFpjCmDh+DUghSEFN0KteSNh4Z2svaQyAq02sqFQu2prXR9hgwBFi4U16dPr/y0mIGO+GrujE6rVqL9cXm56YMOuWxtyBAxH6gqS6+lY+1ABxClmoGBoiyxalBvSrm5lSV6pgx0ANuYp8NApw6+knilOJXBQIfIuli61lhtQyJw8dWfcWhmEloHh1h7OGQFycmi5bKvb2UHK0fw2mviBDQrSwQ7kmR7gc6lS0BJiWWeU6sV7aUB8wc6CoX5yteqt5WuSs7onDgh5mmZmy0EOh4ela2mzdWUQO64FhFh+hI9W2gxzUCnDsEuIt929jpbTBNZFefoNImr2gW3tG5v7WGQlchtpYcNE22XHYW7O7B+PeDqKj7p/vRT4MoVcZ+1A53gYNFmWpKA9HTLPGdmplhHSKUCmjc3//OZY+HQmzeB/fvF9erzcwARQHp4iDkr5s6WSZJtBDpAZavpXbvMMz/JHGVrMltoMc1Apw4tfMRHIlduMqNDZF2co0PUWI40P6e67t0r19SZP180KACsH+goFJYvX5PL1po3t8w6SebovLZvH1BWJuZ1yD+/qlSqyu5n5i5fu3BBTKJXq4GOHc37XA2JjATuvVdcN0dWRw50zNFZjqVrNqxtkMjoXCtnRofIqjhHh6hRrl+vXG/GUebnVLdwIXD77UBhoZjEDlg/0AEsH+hYqrW0TM7opKXVbPPdWFXbStf1cm+peTpy5iQ6WmQNrU1uSrB+vfi/NiVztJaWyYHOX39Zr8U0A506dGkhXi0KVBes2v+byN6UlQHXr5vyn4ala0SNkZgo5m507SpaMjsipVJ0YfPzq/w+xAamo1kro2OpQCc8XGSPtFrg6FHT7FOen1Nb2ZrMUi2mbaVsTXbrrUCPHuZpNW3O0rWoKJGJKyysLC21NAY6dbiljcjoaH3O4+pVKw+GyI60e3IOAl9pg0Ur9phmhwqWrhE1hjw/x1GzObKWLUX7XQBo106cWFmbHOicPWuZ57NUa+mqTFm+lp0NpKaK60OG1L2dpVpM21qgU73VtKmaMdy4UdnEo3Nn0+yzKhcXEewA1punw0CnDh1CKn4zvpfxxxkLtU0hcgCXvbcB/ufwVuYwjHhuddNfkNmMgMhoWq1jz8+pbvx4Edht2WLtkQiOntEBTNt5bdcuUdrUtSsQFlb3dnLgcfZsZamiOdhaoAMADz8sWk2fP18Z2DeVnM2JjBSdGc3B2vN0GOjUIdgzGMpyL0Ah4fCfnKdDZCgJWnFFVYZEj+loP/sp3Mhp+sdPDHSIDPfbb8DVq4CXl1hh3dEpFCJzZY7ym8Zo00Z8PXtWBJ3mZo1Ax5Sd1+prK11VUFDlHCx5bomplZYCp06J67YU6Hh4AEuWiOtPPw0cOND0fZqzbE3GQMdGKRQK+GnFK9XvFy2UeyZyBArxrt5JMw4AkB6+HC2f+wd+/yPP6F1ptZVzfdiMgByJJAGffy4W6zMHuWxt6FDAzc08z0F1a9lSdOwqKTH/3ARJsm6gc+aMKIFqiqqNCBpi7nk6f/whFkP19bVsKaAh5swBHnxQzIV98MHKxXIbyxKBjryWDkvXbFCoqwh0Tmcz0CEyWEWg88qI5/Far81AuTsKwrfjlhX9sfq/p4zaVdVGIJyjQ45k3jxg8uTKuntTk8vWHH1+jq1SqyuDDnOXr+XkVJZxWfLEPCCgskSvKfN0zp8XPyOVCrjttoa3N/c8HTmA6tq17u5v1qJQiGYE0dHA5cuinK0p5eHmbC0tY0bHhrX2E4HOpZsMdIgMJSnEq65aqcLz/xiL7+7fC5fiCGgC0jD9UB+Mf/Ubg0s5tFUiHZaukSMZN050CPvsM+DLL02775ycyoUXGehYj6Xm6citpYODAU9P8z5XdaYoX5PL1vr2NWyeiLlbTNvi/JyqfHyAb74RZak//wy8+GLj92XO1tIyOdA5c8Z0TRSMwUCnHp3DRaBzTctAh8hwIopRqcTLy109e+PPp5IRVnIb4FaAjZoH0GH2M8i+Xt7wnrQMdMgxDRwIPP+8uD5zJnDpkun2/dNP4oSiY8fKjkdkeZYKdKzRcU1mis5rctlafW2lq6paumaO5T9sPdABRIe0Tz8V1+PjgW3bjN9HdnZl6VunTqYbW3UtW4q1iEpLgYsXzfc8dWGgU49bokSgU+R+FqWlVh4MkZ2QKkrXVMrKl5fWQWG4+OpOjPR+GgBwJuxtRC4ehp9/zax3X3oZHZaukYN56SXxifiNG8DUqaabtC7Pz3GGbmu2zNKBjiXn58ia2nlNkgxvRCCLjhalgTk5pv2AQCYHOnJAZaseeqiy9HXyZOP/zuSytdatAW9vkw5Nj0pV+b9gjXk6DHTqIQc6aHYW585x1VByTCkn87Hp+4um+2SsItBxUem/vKiVaux46l9445avoCjzRnHYbgz98hYsWrGnzudmRoccmYsLsGGDKDf66Sfg3/9u+j4lifNzbIUzBDq9eol5I5cuAZn1f25Vq+PHRVbB0xPo39+wx7i5iWAHMH35Wm5uZSmgOeetmMq//gUMGCDGff/9YmFOQ1miEYHMmvN0GOjUI6pZa3HFLR+pf16z6liIzGXgRyPw8MHWiJn3BgpumuAjZXmOTh2r9i0a8wD2Tz0Mr8JOgE8G3rp6B7o/uRTXc2qWsjGjQ7IVK1YgKioK7u7uiImJwd69e+vcdt++fRg4cCACAwPh4eGB6OhovPvuuzW2++abb9C5c2e4ubmhc+fO2Lp1qzkPoVYdOgDLl4vrzz3X9E5Sx4+LScoeHoZN7CbzcYZAx8ensuypMeVrcjZn0CDjugOaa56OPGelRQugWTPT7tscXF3FHL/gYNFSftYsw8v5GOgYID4+HgqFAvPnz693u927dyMmJgbu7u5o06YNPvroo6Y8rcW4q93hXtocAHA0nfN0yDEVe5wFlFqkBMah+cJ7cfRkTtN2WJHRUavqfnnp3y4aV5b9ihjlVECpxfGgZWj+/BD8b49+AS/bSxMAbN68GfPnz8fixYuRkpKCQYMGYdSoUbggf/RajZeXF+bMmYM9e/YgLS0NL7zwAl544QWsWrVKt82BAwcwbtw4TJo0Cb/99hsmTZqEsWPH4tChQ5Y6LJ3HHgPuvlvUsE+YABQXN35fcjbn9tsBd3eTDI8aSV5L58aNprdfro81Ax2gaeVrxrSVrspcLabtYX5Odc2bA5s2ieYm//d/lXN3GmLJQEduMW1Xgc7hw4exatUqdG+giDE9PR133XUXBg0ahJSUFDz//PN48skn8c033zT2qS0qQCFeqU5cYaBDDkpRmcXJi9iG3p/E4N2NqU3eX32BDgD4unvjyItrsbTbBihKfVAcuhdjvuuJR978r26uQtWMDuMc57V8+XJMmzYN06dPR6dOnZCQkIDIyEisXLmy1u179eqFhx9+GF26dEHr1q0xceJEjBw5Ui8LlJCQgOHDhyMuLg7R0dGIi4vD0KFDkZCQYKGjqiS3jA0JESdaixc3fl+cn2M7vLyAsDBx3ZxZHTnet1ag09jOa2VlwO7d4rqhjQhk5moxLe/PngIdABgyBHj9dXF9zhzDsmuWaC0tkzM6djNHp6CgABMmTMAnn3yCZg3k9j766CO0bNkSCQkJ6NSpE6ZPn45HH30Ub7/9dqMGbGnNPUWgk57DQIcclYgq4jqsh1tRa0j+Z7HwRCxGv7AW5Q03RqtJod91rSFL7huP5BkpaFbYG/C8jnXF/0Sb2XNx/nIxJLaXdnqlpaVITk7GiBEj9G4fMWIE9ss9lBuQkpKC/fv347YqtVwHDhyosc+RI0fWu8+SkhLk5eXpXUwlJEQEO4AoZZNLeoyRnw/s2yeuc36ObTB3+VpxMXD1qrhu7YzOkSPGdUE7fBgoKAACA4EePYx7TjnQOXVKLMpqKvaY0ZE9+yxwzz0iM/zAA8C1emZcZGWJrmsKReV8J3OSA530dBHgWlKjAp3Zs2dj9OjRGGZACF7Xm8mRI0dQZumjbYT2QSLQySxhoEMOqiIwGdy+N84/n4zWZXcBLsXY7vIowmdPxLE/jTyZq5ij41LHHJ3a9GrdFhmv/YJhnk8BAM6HfYC2b/bGh1tSddtwjo5zys7OhkajQWhoqN7toaGhyGxg9nOLFi3g5uaG3r17Y/bs2Zg+fbruvszMTKP3GR8fDz8/P90lMjKyEUdUt7vvFq2mAWDKFOD6deMe//PP4iSibdvKEwuyLnMHOnI2x8vLenNKevQQXdD+/rtyPIaQy9aGDBFlV8Zo3hzw9xdt1NPSjHtsXSTJvgMdhQJYt078zZ0/D0ycWHcnR3kuUps2lll7KSJCzBvUaIBz58z/fFUZHehs2rQJR48eRXx8vEHb1/VmUl5ejuzs7FofY85PzYzVrYUIdHIUZ83Sr53I2iSIP2y1UolQ3wCceeVbPBT8KqBVITtiA3qs7IlX1h40fIdKrW5/xnBTuyLxmbexcuD3UBeHQhN4Ai+dHVy5W2Z0nFr1OVqSJDU4b2vv3r04cuQIPvroIyQkJGDjxo1N2mdcXBxyc3N1l4tmWBTi7bdFPfvly8ATTxj3CTm7rdkecwc6VefnWOsl0t29MjAwpnzN2LbSVSkUpp+nc+mS6F6mUlkmy2EO/v7Ali0iqNixA3jlldq3s+T8HEAEstZqSGDUmcjFixcxb948rF+/Hu5GzHKs7c2ktttl5v7UzBj9OohAR+N7BnXEZUT2rSKjo6wITJQKJTbOWoyvR++BW1ErSP7peOncrbhl/mvIzat/WeOqzQPU6sZNAZw57E6cfeYY2pbdA6gqa+cY6DinoKAgqFSqGpmWrKysGh+iVRcVFYVu3bphxowZWLBgAZYuXaq7LywszOh9urm5wdfXV+9ial5eouW0Wi26KW3YYNjjJInzc2yRJQMdazJ24dCbN4EDB8R1Y+fnyEw9T0cOmDp2NK4DnK3p3h2Qe34tW1b5AUhVlg50AOvN0zHqTCQ5ORlZWVmIiYmBWq2GWq3G7t278d5770GtVkOjqXkSVNebiVqtRmBgYK3PY4lPzQzVJbyduOJ3EcfSmtAKh8hWKWrPwNzfdwAuvZCKLtqHAKUGKc1eQPiiYdi+r+4V2srKK/PkxpSuVRcZEIzTr2zF0+0/haLMC65FkWjmwxZSzsjV1RUxMTFITEzUuz0xMREDBgwweD+SJKGkSjF/bGxsjX3++OOPRu3TXHr3BuSYbPZsw0o9Tp0SJ71ubqLjGtkGZwt0DM3o7N0ryixbtarsTmcscwU69li2Vt3kyaIMVpJEJ0f570RmzUDHpjM6Q4cOxbFjx5Camqq79O7dGxMmTEBqaipUtZzY1PVm0rt3b7i4uNT6PJb41MxQwZ7BUJf7AQoJB06ZuRk+kTXU0zwgyNsfx5Z+geei10JR5oWisCSM/q4r7l36OUpLa9bUlFX5sKOhrmsNDkuhwL/GT8ONF64g48Vjda7LQ45v4cKF+PTTT7FmzRqkpaVhwYIFuHDhAmZWTGiJi4vD5MmTddt/+OGH+Pbbb3H69GmcPn0aa9euxdtvv42JEyfqtpk3bx5+/PFHvPnmmzh16hTefPNN7Ny5s8HlEizlueeAgQOBvDxx0lLL54h65E9tBw8WWSGyDXKgc/ly09qG18VWAh2589qRI3XPC6mqatlaY5P1ckBiqtI1Rwp0ACAhQfxerl8XzQnkvz9JYqBTJx8fH3Tt2lXv4uXlhcDAQHSt6E9X/Q1n5syZOH/+PBYuXIi0tDSsWbMGq1evxtNPP23aIzEThUKBAEk0AD964U+UlYlVaJ94wsoDIzKVOjI6ursVCsSPm4qDjxxFs8I+gHsu/qOYjJAn70XSkat625ZrKt/hVMbOLq2Dn7svArz8TLIvsk/jxo1DQkICXn75ZfTs2RN79uzB9u3b0ari7C4jI0NvTR2tVou4uDj07NkTvXv3xvvvv4833ngDL7/8sm6bAQMGYNOmTVi7di26d++OdevWYfPmzejXr5/Fj682KhXw+ediQca9e8UK6PVh2ZptCgoSv0NJEh2nTM3araVlXbqIuTp5ecBffzW8vdyIoLFla0BlW+SMDNEIoakcLdBxcwO+/hoICBABqPwZTmamWNdJqbTsXCR5LR2bLl0zRPU3nKioKGzfvh1JSUno2bMnXnnlFbz33nu4//77Tf3UZhPpKX47f177E6dPSzjgGYePD/2fSVsaElmNge2g+7btgKz4/RgX9CqgcUFu+H9xx1ddMPH1r3WfNlcNdFwaOUeHqDazZs3CuXPnUFJSguTkZAweXNmoYt26dUhKStJ9P3fuXBw/fhw3b95Ebm4ujh49iieeeEI3D032wAMP4NSpUygtLUVaWhruu+8+Sx2OQaKigPffF9dffBE4erT27W7erFyPhI0IbItCYd7yNVvJ6Li4AL16iesNla9lZwOpqeL6kCGNf05v78qfbVOzOmVlld3bHCXQAcTfxRdfiL/Djz8WC4rK2Zx27Sy7qLCc0blwwTzZzbo0+UwkKSlJb4G16m84AHDbbbfh6NGjKCkpQXp6uq7cwF5EB4tA53LxnziffxYY9AakUbNw8lQDtQRE9sDABT4BQK1UY9PsxUgcexg+hd0Bz2vYUPYgQmc/jEPHrplsjg4RCZMni7KT8nJRa19YWHObpCSxdkarVvbbLcqRmSvQ0WhEpzAAaNnStPtuDEMXDv35Z/G1WzeggX4iDZKDkqbO0/nzTxHseHtbP2g0tZEjK+f8zZwpAh/AsmVrgFgrTM5unrXgii38yNUAMa1FoJOr+hOFpRVhqGshdqWYIQ9NZGlyRseIdWqGde2Bv187jDF+LwBaFa6Fb0L/z7pg5ntf6bZp6hwdIhKfxH70kViH4tQpsShgdVXbSrM5oe0xV6Bz5YoIgNVqIDzctPtuDEM7rzWlrXR1pmoxLT++a1fj1/SxBy+8IMpai4uBtWvFbZYOdBQK68zTccBfp+kNjBaBjrbZn7iSUfmJ9f6/jltrSOTEJMmwyZ6G0GolQCGaChg7p8ZN7Ypt81/Bt/ccgFdhJ8D7KrYpH9Xdz9I1ItMIDBQLAQLAhx8C27fr38/5ObbNXIGOXLYWGSnmdFmbHOgcPSoCsLrI83NMGeg0NaMjBzry/hyNUgmsX6+frbJ0oANYZ54Oz0QM0CmkIgT1zsKJs5VLVZ/4m4EOWV77OU/BY9ZgvLsxpcn70lZZjbCxzQPuvqUP/n71KEb7LAY0at3tzOgQmc7w4ZWTiR99tHLy9V9/iRNoF5emzXcg85HbJ5sr0LGVUqsOHURpUlERcPJk7ducOyfKllQq4Lbbmv6ccmBy/HjDnQnr42iNCGoTEAB88w3g6iq+l+dUWRIzOjbKx80HbqUiL3wsI013+6XSE9YaEtmRwkIJublGLG/egDO+a1AavhcL0/qh74K3GlzEsz4aTZUFPpsQmHi4uON/C1/FDw8eRnjxEHTXToaL2gY+YiRyIPHx4lPYq1eBGTP0Fwm99VZxkkm2R87opKebLhsP2F6go1QCMTHiel3la3LZWr9+pvl7bdMG8PAQJVlNCSSdIdABxO9n927Rja1jR8s/PwMdGxasFPm29PxTutsKPI6jqMhaIyJ70Xzh/Wj2UgfErdxrmh0qK2oCVGU47L8IYc8NxbbdF+p/TB2qdkkzRQZmRLeeuBL/E35b9n9N3hcR6XN3BzZsEJ/I/ve/wOrV+vNzyDZFRop5NKWlYj0dU7GV1tJVNbRwqCnaSlelUlW2mW5s+VpeXuWivI4e6ABA//6AtRofy6VrDHRsUCtv8dv5G5UZHQT9gd9PlFlpRGQvcoJ2QAr4C29k3oGYefHIL2jiR3oKEej8038pFKXeKA7djXt2dMcDyzbUuohnfTRa0697Q0Tm06MH8Prr4vq8eZUdrDg/x3ap1UDr1uK6KcvXbC2jA9TfeU2rNW0jAllT5+kcr5iFEBEhyrvIfOSMzuXLoi2+JfDMxkBdwkSgU+5XmdGBqgy7frPwEq9kfyq6mkGpwdGA5xH+9F34JaUJq5spRana83dOw5HpqQgsjAXcc/ENJiLkyXvx44ErBu/K1BkdIjK/BQuAO+4QraaLi4HmzSs/1SbbZI6GBHKgYwutpWVyRuf331FjrcHjx8XcMk9PkVUwlaYGOs5StmYLAgIqg0lDFpY1BZ7ZGKhvu4p8m79+idCBs2xIQA2oCHQeCHgFKHfHzfAfMGhDTyxasRtSY6buVJSuuapVuCWqLTJe34MHAl7WLeI5cltn/GPJWhQXN7xzBjpE9kepFAv/+fuL79lW2vaZOtCRJNvM6LRuLboElpXVDDzkbM7gwZUT4k1BDlAa22KagY5lWXqeDs9sDDRALiysJi2bgQ41oCLQWXzXNOx6+Fd4FkZD8rmCt7LuQIfZz+JSZkkDO6ik0VS2g3Z1ER3OXFRqfDX3Rex66CiaFfYB3HPxrfJRBC24E9v2nK93f+V6pWs8UyKyF5GRwKZN4qRR7sZGtsvUgc61a5WLx0ZGmmafpqBQ1D1Pxxxla0BlgHL2LJCfb/zjGehYlqXn6TDQMVC7gLZQaF1q3H6ZndeoIRWBjlqpxO2du+HK0sPorXoUUEj4K/RfiHq9Dz782rCce2l5ZYc1Nxf9rma3d+6KrPj9mBj6lsgchf2Ie37ogqFxH9TZmY0ZHSL7NXKk6KDEsjXbZ+pAR87mhIWJJhW2RJ6nU7XzWlmZ+FsFTNeIQBYUJObXAJXzbQwlSQx0LE3O6FhqLR2e2RjIReUCv/KaWZ1Cn2PIzbXCgMh+VGRglBUZEz8Pbxx+YTWW9/0PVMXBKA88hjm/9UHsU/9CXn79raJLyipXYautfbNaqcbnM5/BwSm/IbjoVsD1Jn52n4vg5/vjXxuO1CiV02jYjICIyNzMFejYUtmarLaMzq+/AgUFIigxx6KcjZ2nc+UKcOOG6N7WqZPpx0U1sXTNhkV6dNZddymq+Pgg4C8cSrFQ6wiyO1pt3QtyLhh1D848fQxtysYA6lIc9H0WIYsGY/W2tOq70Sktq5LRUavr3K5fuw7IjN+NJ1p9CGWpH8qCj+DZ030RNXsOfvsjR7cdMzpEROYnLxqakwNcv17vpgaxxdbSMjmjc/JkZWctua30kCFijpmpNXaejrx9+/a2lxlzVAx0bFjXkCqBTmm4WERUIWHH0Ua2+iCH11Ag0SowFH+98l8sbPcJFGXeKAndj+mHeyJmwau4ml1aY/uSqoGOa/0LcioVSqyYOgun551CV+0EQCHhfOiH6PlpNMa9ugElJZLeHB0GOkRE5uHpCYSLdcdNktWxxY5rsogIcdFqgZQUcZu55ufIGpvRYdma5cmBTlYWLFIRxTMbI/RvVxnoKCQVIl16AgAOnk+1zoDI5hmSMVEoFHhnwnQcf+IEWpWOAtSlOOr/Ilq83BtvbdCfzVlaXlm65uZSd0anqjYhYTi2bD3W3PYTPAo7At5X8aVmIgIWDMOGHyvnmCnZjICIyGxMWb5my6VrgH75WkEBcOCA+N4SgY4x3UwZ6Fiery8QGiquWyKrw0DHCLd1qgx0SouV6BHaCwDwZ16KtYZENq5qoNNQING5eUukv/odXuyyHqqSQJQHHsOiP/ujw5yncfqcyP9XbUbgWsscnfo8cvsQXH/1NzzQ7FWg3B2FoT/jtUtDxJ1avhQQEZmTswY6e/cC5eWi9bRcwmdq0dFiYdbcXODSJcMfx0DHOixZvsazGyNEB7fXXS9zy8QdnUSgc901BaU1q4yIoKlnjk5tFAoFXn5gAtKfTkNnzcOAUovTwe+g4/udMO1f/8HNooqMjqRoVAbG3cUNXz25GCnTT6BN6T26RgkAszlERObkTIFO1c5rVcvWzLXek6urCHYAw8vXysuBtIopseZokEB1Y6Bjo9zUbpXfNDuHEd16AgCkkGM4frK89geRU9NUnQNjxAzMyIBgnHj5C7zX739wLWoFyfci1hTei0H/His20BpWtlaXnq3a4Mxr/8GKAd/DvzAGnbTjmrQ/IiKqn6kCnZs3xTo6gO0HOqdPA1u3iuumbitdnbHzdE6fBkpKAC8vkW0iy5HX0rFEi2kGOk3QNqANVOU+gLoEO46csvZwyAY1tavZ3DtH49qykxjl/TygcUFx6D5xh9a4srW6PDH8Ttx48whOvrLBJPsjIqLamSrQkTuu+fmJiy0KDKwsUzt7VnwdMsS8z2lsoCOXrXXtap5OcFQ3ZnRsWJhrZYGpUqFEqNQTALDnNOfpUE2maN/s7eaJ7U+9hl8m/o6wojvEvkqDTDI+IiKyDDnQuXwZKCpq/H5svWxNJmd1ABGEhISY9/nkQMfQFtNyQMT5OZbHQMeGLR66AADQs6IRQeeAngCAE9cZ6FBNenN0VE0rTh7QIRpX4n/C6tt/xI7J3zV1aEREZEGBgaLjFACkpzd+P7bcWroquSEBYL5ua1XJAcupU6IkrSFsRGA9HToA27YB+/cb1yWvMZpW6O+EZveZjVZ+rdCvRT8AwK1te2FnKpAppUKSzDfRjizny8R0ZF0vxOyxXZr8+9RWmaNjSDOChigUCjx62/Am74eIiCxLoRBZnZQUUb7WuXPDj6mNvWR0LB3oNG8ONGsG3Lghmgz07Fn/9gx0rMfdHRgzxjLPxYyOkRQKBcZ0HIMQL5GDHXVLTwBAeVAKTp82c1hKFvHQj7di7qmuiJr1JM5cKGzSvkxRukZERI7BFPN07CXQueWWynlEgweb//kUCsPn6eTnV2bVGOg4Np55NVGviK5QaNwAjxxs+8UCxYZkdpJXBgDgfNj76LC8J15ee6DR+yqvktFRMt1HROTU5An6zhDo+PiI0qQDB8R1S5CDlobm6ZyoWCs7LAwI4pRXh8ZAp4lcVC4I1d4CANiZ9quVR0MmUbG2jKokENpmp7Hk3K3oNDcOlzINKPqtRmPEgqFEROTYnCmjA4jyvE6dLPd8hmZ0WLbmPBjomECPoL4AgN+yGejYO22V5gG7Jx9Ad2kSoNTiVNAbaPVGdzz3UZJRE+d0zQi0/FcjInJ2TQ10ysqAK1fEdXsIdCyNgQ5Vx7MvExjRWTQmuKr+FWVlVh4MNUnVOTUh3gH4belneCtmC9TFodA2+xNvXr0DEU88ikPHrhm0P92CoRL/1YiInJ0c6KSnAxqN8Y+/fBnQagE3N/O3a7ZHXbqIuTqZmcDff9e9HQMd58GzLxMYc4vI6EihKTj6m/HlTWQ7tFXSNXLzgGfuvhdXnj+F/qqZAIDM8LXovz4a97/8GYqL60/vMNAhIiJZZCTg4iIyM5cvG/94uWwtMpKLXNbG27tyHlRd83QkiYGOMzHq32TlypXo3r07fH194evri9jYWHz//ff1PmbDhg3o0aMHPD09ER4ejkceeQTXrhn2abi9aBfYBi5lAYC6FP85aOCSvGSTyuuYUxPs448DL6zElyN/gXdhV8AzG1ukKWj21GAkbD5aZzlb5f44P4eIyNmpVEDr1uJ6Y8rX7Gl+jrU0VL6WmQlcuyYCxca2+Cb7YVSg06JFC7zxxhs4cuQIjhw5giFDhuCee+7BCbl9RTX79u3D5MmTMW3aNJw4cQJfffUVDh8+jOnTp5tk8LZCoVAgUiWyOkmnOU/Hnmk0VRb4rOXjsgf7D8C1145iXNDrQJkHikP2YUFab0TOnoFfUrNq7k+eo8OMDlGTrFixAlFRUXB3d0dMTAz27t1b57ZbtmzB8OHDERwcrPtQ7ocfftDbZt26dVAoFDUuxcXF5j4UcnJNmafDQKdhDQU6cjanXTvAw8MyYyLrMersa8yYMbjrrrvQoUMHdOjQAa+99hq8vb1x8ODBWrc/ePAgWrdujSeffBJRUVG49dZb8fjjj+PIkSMmGbwt6RMuAp1TeQx07JlG23CXNFe1CzbNjsPJWX+gU/nDgELC5dBPcevm9rjtueXIulaq21bL0jWiJtu8eTPmz5+PxYsXIyUlBYMGDcKoUaNw4cKFWrffs2cPhg8fju3btyM5ORl33HEHxowZg5SUFL3tfH19kZGRoXdxd3e3xCGRE2OgY15yoFNX6RrL1pxLo8++NBoNNm3ahJs3byI2NrbWbQYMGIBLly5h+/btkCQJV69exddff43Ro0fXu++SkhLk5eXpXWzd3b1EQ4Ic70PIzbXyYKjR9Bb4bKAAulNEJE6+8gU2Dt8Lv8JbAPc87PF4ChGvdMO0t7eipESqXEeHgQ5Roy1fvhzTpk3D9OnT0alTJyQkJCAyMhIrV66sdfuEhAQ8++yz6NOnD9q3b4/XX38d7du3x7fffqu3nUKhQFhYmN6FyNwY6JiXHMAcP157wwc50JEDInJsRp99HTt2DN7e3nBzc8PMmTOxdetWdK6jyHHAgAHYsGEDxo0bB1dXV4SFhcHf3x/vv/9+vc8RHx8PPz8/3SUyMtLYYVrcnd1ERgdBf+CHvdnWHQw1WtVmBCqVYfNqHhpwK67F/4r5bT6FqjgEmmZ/Ys3N++C38Fas/E4ur+EcHaLGKC0tRXJyMkaMGKF3+4gRI7B//36D9qHVapGfn4+AgAC92wsKCtCqVSu0aNECd999d42MT3X2+CEc2Z6mBDpyEpOBTt3atAE8PYHiYuCvv2rez4yOczE60OnYsSNSU1Nx8OBBPPHEE5gyZQpOnjxZ67YnT57Ek08+iZdeegnJycnYsWMH0tPTMXPmzHqfIy4uDrm5ubrLxYsXjR2mxQV5BsGvVKyK9fWvv1h5NNRYVTM6tc3RqYtKqcK7k6Yh64XTuMvrBaDMEyUh+/F1+SMAAAUzOkSNkp2dDY1Gg9DQUL3bQ0NDkZmZadA+3nnnHdy8eRNjx47V3RYdHY1169Zh27Zt2LhxI9zd3TFw4ECcPn26zv3Y44dwZHuqBjrGrMsmSQx0DKFSAV27iuvV5+mUlwPyKSsDHedg9NmXq6sr2rVrh969eyM+Ph49evTAv//971q3jY+Px8CBA/HMM8+ge/fuGDlyJFasWIE1a9YgIyOjzudwc3PTdXaTL/agu/+tAICDl/dZeSTUWBpt1WYExmdhArx88d3Tr+DPOacRgxm6hUIVkovJxkjkjBQK/f9HSZJq3FabjRs3YunSpdi8eTNCqiw80r9/f0ycOBE9evTAoEGD8OWXX6JDhw71VhzY44dwZHvk9se5ucD164Y/LitLZCkUCqB5c/OMzVHUNU/nr7/Ez9DTs/L3QI6tyR8zS5KEkpLa144pLCyEstqn4iqVSvc4RzO66yAAwCX1XpSWNrAx2SSttnEZnerah0XgyJJV2PPwcfTUPooprV8yxfCInE5QUBBUKlWN7E1WVlaNLE91mzdvxrRp0/Dll19i2LBh9W6rVCrRp0+fejM69vohHNkWDw8gIkJcN6Z8TZ6fExEBuLqaflyORM7WVM/oyIFPly5ch8hZGPVrfv7557F3716cO3cOx44dw+LFi5GUlIQJEyYAEJ92TZ48Wbf9mDFjsGXLFqxcuRJnz57FL7/8gieffBJ9+/ZFhPxf7kDG9heBjhSajANHCq08GmqMqhkdecHQphgU3Qkpy1Zjzcw5Td4XkTNydXVFTEwMEhMT9W5PTEzEgAED6nzcxo0bMXXqVHzxxRcNNsABxIdvqampCA8Pb/KYiRrSmHk6bERguLpaTHN+jvNRG7Px1atXMWnSJGRkZMDPzw/du3fHjh07MHz4cABARkaGXrvPqVOnIj8/Hx988AGeeuop+Pv7Y8iQIXjzzTdNexQ2orV/K7iXNkex62Vs2nsItw24w9pDIiMZ0l6aiCxr4cKFmDRpEnr37o3Y2FisWrUKFy5c0M33jIuLw+XLl/HZZ58BEEHO5MmT8e9//xv9+/fXZYM8PDzg5+cHAFi2bBn69++P9u3bIy8vD++99x5SU1Px4YcfWucgyam0bQvs3ctAx1zkQCY9HcjPB3x8xPcMdJyPUYHO6tWr671/3bp1NW6bO3cu5s6da9Sg7JVCoUBHj0H4TbMJSel7ATDQsTdVMzpKA+r/icj8xo0bh2vXruHll19GRkYGunbtiu3bt6NVxRlf9Q/ZPv74Y5SXl2P27NmYPXu27vYpU6bo3qdycnLw2GOPITMzE35+fujVqxf27NmDvn37WvTYyDkxo2NegYFiHtPly6LNtLwKCgMd52NUoEMNG9Z+EH47tQlnyvZBksSkQbIfWmZ0iGzSrFmzMGvWrFrvq/4hW1JSUoP7e/fdd/Huu++aYGRExmtMoMOOa8bp1k0EOr//LgKdmzeBs2cr7yPnwKlYJvbwQNF5rSz0AI6fLLfyaMhYugU+tfzXICIi82hKRqdlS9OPxxFVn6dz4oRo0R0SIi7kHHg2Z2K9mneFuswfcCvA//2YbO3hkJE0Grl0jdkcIiIyDznQuXIFKCoy7DEsXTNO9RbTLFtzTgx0TEypUKKDq5ibs/3UT1YejXPYsS8DX/9omvUstFJFRocLfBIRkZkEBAAVfTF05VT1ycsDcnLEdQY6hqnaYlqSGOg4K57NmcHdncV6DX+W74RGY+XBOLhyjRZ3bbsFD+5rg17zXsW1G00rF9RldCRmdIiIyDwUCuPK1+RsTkAA4O1tvnE5kuhoQK0WC7NevMhAx1kx0DGDqYNFoKOJ+AW//Mr1dMypuLQcklcmoCpHasCLiHhhEL7YUfeCfw3RMKNDREQW0KaN+GpMoMNsjuFcXYFOncT1Y8cY6Dgrns2ZQXRwe3iURgLqUqz7eZ+1h+PQyjWVXdIUZV4oDTmICXt7YvjzK1FYKNXzyNpptZyjQ0RE5teYjA4DHePI83QSE4G//xaZtC5drDsmsiwGOmagUCjQw0csovpT+k4rj8axVQ10ksYfRnjxEMC1EDvdZiHw6Tvwf//7w6j96RYMZUaHiIjMyJhAh62lG0fO3mzaJL62awd4elpvPGR5PJszkwd6ifK1iy47De6oQsarGuh0bt4Kl15PxPQWCVCUeaI4dDemHuqOWxa8gsy/Sw3aHwMdIiKyhMZkdNha2jhyRufqVfGVZWvOh2dzZjJhwBAAgBSWgu1J2VYejeOqGuioVUooFUp8Mm0efnv8BFqW3AmoS5Hi/xJavNoTL36yF1ID1Wwalq4REZEFyIHOuXNosHERS9caRw50ZAx0nA8DHTMJ8wlFs1LxH7YmKdHKo3FcugwMRKAj6xbZGude245l3TZCVRwCTUAaXr0yGMFPPIz/7a27FbW8PwUzOkREZEYtWgAuLkBZmegKVh8GOo0TESE61ckY6Dgfns2Z0eDwOwEAe69+Z+WROK7KBT71Ax1AzJV66b6HcHFRGvooHwMkBa6Fb8KYHzoiZuHLOHe5Zkc8NiMgIiJLUKmAqChxvb61dEpKgIwMcZ2BjnEUCv3ghoGO82GgY0ZPDB0DAMgP/R5pfzRtfReqXXkdGZ2qwv0D8OuLH2PHvckILhoEuBThqN8StFneCRPf2Iyiospgie2liYjIUgyZpyNnezw8gKAg84/J0cjlax4elT9vch48mzOjoR37Q10WAHhex4pvD1h7OA6p6hwdpaL+LMzIHr1wNX43lnXbBJeiSEi+F7Ch5CH4PdMHz6wUi7syo0NERJZiSKBTteNaA29zVIsePcTXLl1EFo2cCwMdM1Ir1ejuOQoA8O2p/1l5NI5JWyWjo1Q2/A4gytnG4fqyU/in3zIoSr1RFpyMt7OGw2/ucKz/+bDYkBkdIiIyM0MCHc7PaZqHHwYefxx4801rj4SsgWdzZjaxz90AgPPu/0NenpUH44B0pWta4/6Uvd08sXX+S0hfcAa3qp8ENC64GboT32geAQBIWn5sRkRE5mVMoMPW0o3j6Ql89BEwZIi1R0LWwEDHzB4ZdCegVQHBJ7H+u3pmG1Kj6ErXGpmBaRUUgr2L/42UR/9Al/JJgCQCHJXCxVRDJCIiqlXVQKeu5Q+Y0SFqPAY6Zubv7o8W2kEAgLW/fFvrNomJQHKyJUdlXXl5EgoKTLMvTRMDHVnP1lE4/spn+PG+VHTTTMG09i+YYHRERER1k7uu5eUB167Vvg0DHaLGY6BjAfd2+gcAIKXkG5SU6N+XmVWOEZsHoN/y+5Cd3cBqlg6gvFxCyHOD4BfXDW9vSGn6/rSm7ZI2vHt3/P7yOnw843GT7I+IiKguHh5A8+biel3lawx0iBqPgY4FPDXqAQCApvk+bN5+Re++k5cvAJEHoOmwFYs+/skaw7OorNwClIT+Am3QcTxzKhbDFq1CaWnjAzxTZXSIiIisob55OlptZXtpBjpExuPZoQW0ahaJsLJYQCHhw13f6N1XptHorm84+w5KSy09Osuq2g4a6hL85Pk4ms+dhLQzjatlq1xHh3/KRERkf+oLdDIzgbIy0RY5IsKy4yJyBDw7tJAHOj0IAEgu+kovmCmvEuiUtNyBd9efsPTQLKq8vDLQGR/yOqBVITtiA7p+0BMvffJLnZMx68KMDhER2bP6Ah25bK1FC0CtttyYiBwFzw4tpGr52pffV5avVc3oAMDb+xKMPtm3J1WP9/8eW4RNdybBtSgSWv8zeOXSYHSeG4crWSX17EGfpmKBTwUDHSIiskOGBDpsLU3UODw7tJDWAZXlax/8XFm+VlZeceJf0dY4u/nn+HpHpjWGaBFVS9eUCgXGxd6KKy8cQ09MAZRanAp+A61e6YuETYY1KtCYuBkBERGRJRkS6HB+DlHj8OzQgh7oNBYAcKRoE4qKxG1yhkNZGIawsv6AugQLvv6XtYZodpXr3iigVIrgLtDbDylL1uFfMVugKg5CedDvWJDWBx3mPI0/z9U/d0cjcY4OERHZLznQycgACgv172OgQ9Q0PDu0oOfuHgdoldA034+VX54GUJnRUUgq/OvuJQCAy+ErsTXxqtXGaU660jWtqsZ9T999L/566jiiy8cCSg1OB7+D6Pe7YOa7/0O1Cj8dztEhIiJ71qwZ4Ocnrp+ttq44Ax2ipuHZoQU19wtHO8VIAMCH+/4PQNVSLhUm9BuJkLK+gEsR5m22nazOr6n5+O1EkUn21VBg0jooFGmvbMYHsd/BtbAVJN8L+DhvDIJn348tP6fX3F9F6Rrn6BARkT1SKOouX2OgQ9Q0Rp0drly5Et27d4evry98fX0RGxuL77//vt7HlJSUYPHixWjVqhXc3NzQtm1brFmzpkmDtmdzbp0CADjr8xkuXdbquq4pJBUUCgXeumspAOBi2Ar89yfrZ3Xyb5ah//po9PwsEgvf29Xk/ZUZmIGZPeIu/L30BIZ5PANoVbgRvgX37+qELvPicOpsvm47DdtLExGRnast0JEkBjpETWXU2WGLFi3wxhtv4MiRIzhy5AiGDBmCe+65BydO1N0SeezYsfjpp5+wevVq/PHHH9i4cSOio6ObPHB79fjt90BV5gf4XcTLn+9CubYi0Kn4VUyOvRMhpSKr88SGN6zege3ytVxIPlcAz2t4N3sE7nhmJcrLG78/XTttqWbpWnW+Hl5IfPYt/DQ2BeFFQwF1CU4GvIFOK9vjn8vWIL9Ao8uIKaBo/KCIiIisqLZAJycHKKiYphoZafEhETkEowKdMWPG4K677kKHDh3QoUMHvPbaa/D29sbBgwdr3X7Hjh3YvXs3tm/fjmHDhqF169bo27cvBgwYYJLB2yN3tTsG+j0EAPjyj//TzVlRVJz4KxQKvPuPVwAAGS0+xMrNfxn9HBkZEq5dM02EVHWdH6jKkeQ9C1FzZ+HK1catbNqYLmlDunTD5fhEvNP7v3AvbAd4X8V/MQ2Bcb2xMvFHo/dHRERkS2oLdORsTnAw4Olp+TEROYJGnx1qNBps2rQJN2/eRGxsbK3bbNu2Db1798Zbb72F5s2bo0OHDnj66adRVFT/fI+SkhLk5eXpXRzJC3dPBQDkNv8aB3+7BqAy0AGA8f1GoI12JKAqw6LE51BWZvi+C25qELlsAMIWDUPKyfyGH9AAXamZVokJofGApMClsJWIenkwvt55zvj9lTeu1EyhUGDh6H8g57UTmBTyDhQlfigLSsUv7nGN2h8R2ZcVK1YgKioK7u7uiImJwd69e+vcdsuWLRg+fDiCg4N1ZdY//PBDje2++eYbdO7cGW5ubujcuTO2bt1qzkMgqlN9gQ7L1ogaz+izw2PHjsHb2xtubm6YOXMmtm7dis6dO9e67dmzZ7Fv3z4cP34cW7duRUJCAr7++mvMnj273ueIj4+Hn5+f7hLpYDnbYdH90Ky0G+BShC3nVgMAFNAv5fpi6tuAVomClt/g2Q/2GbzvtEuZ0IQfRHnkz7g14WFkZTehzgz6pWbrZz6Hf8f+F8pSf5SGHMKDP/XExNe/gVZb/z709ieX6tXSdc0QbmpXfPbEQlx4+i/c5roQ0LgCAFRa90btj4hs3+bNmzF//nwsXrwYKSkpGDRoEEaNGoULFy7Uuv2ePXswfPhwbN++HcnJybjjjjswZswYpKRUrs914MABjBs3DpMmTcJvv/2GSZMmYezYsTh06JClDotIRw50zp2DrjycgQ5R0xkd6HTs2BGpqak4ePAgnnjiCUyZMgUnT56sdVutVguFQoENGzagb9++uOuuu7B8+XKsW7eu3qxOXFwccnNzdZeLFy8aO0ybplAo8ESMCPbKWm8Xt1Wbs9Ivqitu9ZoGAHj/z4XI+tuwaEK3ACmAwubf4Za4hShtXJVZxf70S82eHDkGv89KRWBRf8A9FxvKHkCrWU/g1Nn617uR6bquNTED0yIgCElx7+DYjNMY6vYslt76ZpP2R0S2a/ny5Zg2bRqmT5+OTp06ISEhAZGRkVi5cmWt2yckJODZZ59Fnz590L59e7z++uto3749vv32W71thg8fjri4OERHRyMuLg5Dhw5FQkKChY6KqFLz5oCrqwhy5FMeBjpETWf02aarqyvatWuH3r17Iz4+Hj169MC///3vWrcNDw9H8+bN4Sc3iAfQqVMnSJKES5cu1fkcbm5uus5u8sXRxN09AaqyyuOqntEBgI2PvQxlmQ80YYdx76urDNpvebUFZy63eB8Dn/q3UVmXWvdXJQPTpXkrXHl1D+70XgQAuBT+Ebq83wNxH+9usHlCuYnbQXeNbImdz72JuHvvMcn+iMi2lJaWIjk5GSNGjNC7fcSIEdi/f79B+9BqtcjPz0dAQIDutgMHDtTY58iRI+vdp6OXVZP1qFRAVJS4LpevMdAharomn21KkoSSkpJa7xs4cCCuXLmCgoLKT/v//PNPKJVKtGjRoqlPbde8Xb0xJGBqlVtq/ipa+Ifhya6vAQD2ez6H/+zMbHC/ujk1pd54NFJkOY4EzcfwZ9Y2qoNbWR0ZGFe1C75/6g2svi0RrkUtofU/izcyb0fbOU/ir/M369yfMV3XiIiys7Oh0WgQGhqqd3toaCgyMxt+TQSAd955Bzdv3sTYsWN1t2VmZhq9T0cvqybrqj5PR67MZKBD1HhGBTrPP/889u7di3PnzuHYsWNYvHgxkpKSMGHCBACi5Gzy5Mm67cePH4/AwEA88sgjOHnyJPbs2YNnnnkGjz76KDw8PEx7JHboX2Nn6a4Xqmovz3t77CwElfYG3HMxZeOCBsvQdKVrWhU+feQZ3Om3AADws/d03PviZqPHWFtGp6pHbx+GjBePoY9yBgAgPeR9dPh3F8x4e1utbajLDVxHh4ioKoVCv4W8JEk1bqvNxo0bsXTpUmzevBkhISFN2qejl1WTdcmBztmz4quc0WnZ0jrjIXIERp1tXr16FZMmTULHjh0xdOhQHDp0CDt27MDw4cMBABkZGXqTQ729vZGYmIicnBz07t0bEyZMwJgxY/Dee++Z9ijsVI8WHSu/8an9U0SVUoUvJ30MaJXIa7kJU1//rt596ib7VyxAun3eOxjk8Rig1OK/yokYt3SrUZmdynVq6v5TCfDyxa8vrsJHA3fAtSgSkt95fHrzHgTOHYMtu87qbSu3l66tVI+IqLqgoCCoVKoamZasrKwaGZnqNm/ejGnTpuHLL7/EsGHD9O4LCwszep/OUFZN1lM1o1NUBGRlie+Z0SFqPKMCndWrV+PcuXMoKSlBVlYWdu7cqQtyAGDdunVISkrSe0x0dDQSExNRWFiIixcv4p133mE2p4qv7vkekBS4K2RmndvcEX0L7gqcDwDYeHMadu7PrnPbymYE4lerUCiw6+kVuEU9AVCV40vpQYyO+9zgYMeYUrPHh41E9tI0jPRaBGjUyAv7H+7/qQtiFr6MMxcLK/Zn2jk6ROTYXF1dERMTg8TERL3bExMT612TbePGjZg6dSq++OILjB49usb9sbGxNfb5448/OvU6b2RdVQMd+TNjb2+gWTPrjYnI3vFs08oe6Hknsp65im2Pf1Dvdl8/8Sp8SzoD3ldx75rHUFhYe6RSGUhUBiYqpQqH4tahr+tUQKnB9x6TMWjhh6jWt6D2/RnZPMDH3Qs7nn4Deyf8jrCiIYBLMY76LUH7f7fHPcvWIO+mXHvHPz0iMszChQvx6aefYs2aNUhLS8OCBQtw4cIFzJwpPiCqXja9ceNGTJ48Ge+88w769++PzMxMZGZmIjc3V7fNvHnz8OOPP+LNN9/EqVOn8Oabb2Lnzp2YP3++pQ+PCIB+oFO1EYEBFZpEVAeebdqAYK9gqJT1Z0w8XDzwn8nrAY0LCiK34p4l/1frdmV1ZGDUSjUOPLcaQ72fBAD84j8H3eYuwc2b9ad2Gts84NaOnXAlfieWddsE16JWkHyuYBumYcGBBwDUbKdNRFSXcePGISEhAS+//DJ69uyJPXv2YPv27WhVUdNTvWz6448/Rnl5OWbPno3w8HDdZd68ebptBgwYgE2bNmHt2rXo3r071q1bh82bN6Nfv34WPz4iQHRdUyiA/HwgOVncxrI1oqZhoGNH7ojuhamtXgYA7HSdg39vSKuxjRyY1BZIKBVKJC5MwP1BLwEA0kJfRssF43H2Yt1rGhkyR6cuCoUCL903DjmvnMLEkLehLPEHvK8CACQt//SIyHCzZs3SlU4nJydj8ODBuvuql00nJSVBkqQal3Xr1unt84EHHsCpU6dQWlqKtLQ03HfffRY6GqKa3N3FejoA8PPP4isDHaKm4dmmnfn0kWfQUjMEcL2JhYfuw9ET+Xr3lzewIKdCocDXs5fh2ehPAY0a15tvQqc37sDOg7U3QzBFO2gPF3d8/sRTuPTsGdzm8hSgcUVbj96N3h8REZEjksvX9u0TXxnoEDUNAx07o1KqcOCpjXAtbg5t4CnckfCoXvlZfRmdqt4cNw3rRyZCWRKA0pBDGP51DOJW7q2xnSkX+Az3D0DS82+j6KVcnHztsybvj4iIyJHIgU5xsfjK1tJETcNAxw5F+IXg63FfARoX5LX4Gv2ffhMV8Yhee+mGTBh4Ow7NOAivwk6AzxW8kXkHes9/EwU3tbptzLHAp7vaHUoF//SIiIiqkgMdGTM6RE3Ds007NaZnLBZ2TgAAHA+L0y0GWmZgRkfWO6o9riz7FT0UEwGlBsnNnkP4U2Ow61cxl6Zy3Rv+qRAREZkTAx0i0+LZqx1756FZGOknughtU07Gsx/ubVTzAF93b6S8+BkWtv8EKHdDQfh2DPmmKya+/g1Ky2q2qyYiIiLTqxrouLgA4eHWGwuRI2CgY+e+e/IdREv3AupS/OviPfjPL8cBAAoYF5goFAq8M346fn7oMHwLewCe2dhQ9gAW7Hu4Yn/8UyEiIjKnNm0qr0dGAkq+9RI1Cf+F7JxKqcKR59cjuLg/4HED+7znA2h8BuaOLt2Q9dqvuNt3MaBVAr5XAADlqjxTDZmIiIhqERAA+PuL6yxbI2o6BjoOwMvVE8ef/x/8irsDSrl0rfGlZm5qV3y74FVsHbMfqjI/AICPJsokYyUiIqK6yeVr7LhG1HQMdBxEiE8gjj+bCO+iTgAAtcK1yfv8Z+9+KFhyFc9Fr8O3j3/c5P0RERFR/dq1E19bt7bqMIgcgtraAyDTadEsBL89tRNjP1mECcPvN8k+3V3cED9uikn2RURERPVbsAAoLwem8K2XqMkY6DiYNsEROPL859YeBhERETVCv37A119bexREjoGla0RERERE5HAY6BARERERkcNhoENERERERA6HgQ4RERERETkcBjpERERERORwGOgQEREREZHDYaBDREREREQOh4EOERERERE5HAY6RERERETkcBjoEBERERGRw2GgQ0REREREDkdt7QEYQpIkAEBeXp6VR0JE5Fzk1135dZgq8b2JiMg6DH1vsotAJz8/HwAQGRlp5ZEQETmn/Px8+Pn5WXsYNoXvTURE1tXQe5NCsoOP6bRaLa5cuQIfHx8oFAqjH5+Xl4fIyEhcvHgRvr6+ZhihZfA4bAuPw7bwOMxDkiTk5+cjIiICSiWrnavie5Nt4c/TtPjzNC3+PE3L0Pcmu8joKJVKtGjRosn78fX1dYg/Lh6HbeFx2BYeh+kxk1M7vjfZJv48TYs/T9Piz9N0DHlv4sdzRERERETkcBjoEBERERGRw3GKQMfNzQ1LliyBm5ubtYfSJDwO28LjsC08DrI3/F2bFn+epsWfp2nx52kddtGMgIiIiIiIyBhOkdEhIiIiIiLnwkCHiIiIiIgcDgMdIiIiIiJyOAx0iIiIiIjI4Th8oLNixQpERUXB3d0dMTEx2Lt3r7WHpCc+Ph59+vSBj48PQkJC8M9//hN//PGH3jZTp06FQqHQu/Tv319vm5KSEsydOxdBQUHw8vLCP/7xD1y6dMlix7F06dIaYwwLC9PdL0kSli5dioiICHh4eOD222/HiRMnbOoYAKB169Y1jkOhUGD27NkAbPd3sWfPHowZMwYRERFQKBT4z3/+o3e/qX7+N27cwKRJk+Dn5wc/Pz9MmjQJOTk5FjmOsrIyLFq0CN26dYOXlxciIiIwefJkXLlyRW8ft99+e43f0UMPPWQzxwGY7u/I3MdB5mPr7032oqH3HmqYKd4/qJIpXv/JdBw60Nm8eTPmz5+PxYsXIyUlBYMGDcKoUaNw4cIFaw9NZ/fu3Zg9ezYOHjyIxMRElJeXY8SIEbh586bednfeeScyMjJ0l+3bt+vdP3/+fGzduhWbNm3Cvn37UFBQgLvvvhsajcZix9KlSxe9MR47dkx331tvvYXly5fjgw8+wOHDhxEWFobhw4cjPz/fpo7h8OHDeseQmJgIAHjwwQd129ji7+LmzZvo0aMHPvjgg1rvN9XPf/z48UhNTcWOHTuwY8cOpKamYtKkSRY5jsLCQhw9ehQvvvgijh49ii1btuDPP//EP/7xjxrbzpgxQ+939PHHH+vdb83jkJni78jcx0HmYQ/vTfakvvceapgp3j+okile/8mEJAfWt29faebMmXq3RUdHS88995yVRtSwrKwsCYC0e/du3W1TpkyR7rnnnjofk5OTI7m4uEibNm3S3Xb58mVJqVRKO3bsMOdwdZYsWSL16NGj1vu0Wq0UFhYmvfHGG7rbiouLJT8/P+mjjz6SJMk2jqE28+bNk9q2bStptVpJkuzjdwFA2rp1q+57U/38T548KQGQDh48qNvmwIEDEgDp1KlTZj+O2vz6668SAOn8+fO622677TZp3rx5dT7GFo7DFH9Hlj4OMh17fG+yVfW995DxGvP+QXVrzOs/mZbDZnRKS0uRnJyMESNG6N0+YsQI7N+/30qjalhubi4AICAgQO/2pKQkhISEoEOHDpgxYwaysrJ09yUnJ6OsrEzvWCMiItC1a1eLHuvp06cRERGBqKgoPPTQQzh79iwAID09HZmZmXrjc3Nzw2233aYbn60cQ1WlpaVYv349Hn30USgUCt3t9vC7qMpUP/8DBw7Az88P/fr1023Tv39/+Pn5We3YcnNzoVAo4O/vr3f7hg0bEBQUhC5duuDpp5/W++TRVo6jqX9HtnIcZBx7fW+yZXW991DTGfL+Qcar7/WfTEtt7QGYS3Z2NjQaDUJDQ/VuDw0NRWZmppVGVT9JkrBw4ULceuut6Nq1q+72UaNG4cEHH0SrVq2Qnp6OF198EUOGDEFycjLc3NyQmZkJV1dXNGvWTG9/ljzWfv364bPPPkOHDh1w9epVvPrqqxgwYABOnDihG0Ntv4vz588DgE0cQ3X/+c9/kJOTg6lTp+pus4ffRXWm+vlnZmYiJCSkxv5DQkKscmzFxcV47rnnMH78ePj6+upunzBhAqKiohAWFobjx48jLi4Ov/32m64M0RaOwxR/R7ZwHGQ8e3xvsmX1vfcEBgZae3h2z5D3DzJOQ6//ZFoOG+jIqn4SD4hgovpttmLOnDn4/fffsW/fPr3bx40bp7vetWtX9O7dG61atcJ3332H++67r879WfJYR40apbverVs3xMbGom3btvi///s/3SS7xvwurPn7Wr16NUaNGoWIiAjdbfbwu6iLKX7+tW1vjWMrKyvDQw89BK1WixUrVujdN2PGDN31rl27on379ujduzeOHj2KW265BYD1j8NUf0fWPg5qPHt6b7Jl9b33LFy40Iojcyz8ezWdxr7+U+M4bOlaUFAQVCpVjU/IsrKyanwyYQvmzp2Lbdu2YdeuXWjRokW924aHh6NVq1Y4ffo0ACAsLAylpaW4ceOG3nbWPFYvLy9069YNp0+f1nXAqe93YWvHcP78eezcuRPTp0+vdzt7+F2Y6ucfFhaGq1ev1tj/33//bdFjKysrw9ixY5Geno7ExES9bE5tbrnlFri4uOj9jmzhOKpqzN+RLR4HNcze3pvsTdX3Hmo6Q94/qGmqv/6TaTlsoOPq6oqYmBhduYosMTERAwYMsNKoapIkCXPmzMGWLVvw888/IyoqqsHHXLt2DRcvXkR4eDgAICYmBi4uLnrHmpGRgePHj1vtWEtKSpCWlobw8HBdGVHV8ZWWlmL37t268dnaMaxduxYhISEYPXp0vdvZw+/CVD//2NhY5Obm4tdff9Vtc+jQIeTm5lrs2OQg5/Tp09i5c6dBpSknTpxAWVmZ7ndkC8dRXWP+jmzxOKhh9vLeZK+qvvdQ0xny/kFNU/31n0zMCg0QLGbTpk2Si4uLtHr1aunkyZPS/PnzJS8vL+ncuXPWHprOE088Ifn5+UlJSUlSRkaG7lJYWChJkiTl5+dLTz31lLR//34pPT1d2rVrlxQbGys1b95cysvL0+1n5syZUosWLaSdO3dKR48elYYMGSL16NFDKi8vt8hxPPXUU1JSUpJ09uxZ6eDBg9Ldd98t+fj46H7Wb7zxhuTn5ydt2bJFOnbsmPTwww9L4eHhNnUMMo1GI7Vs2VJatGiR3u22/LvIz8+XUlJSpJSUFAmAtHz5ciklJUXXjcxUP/8777xT6t69u3TgwAHpwIEDUrdu3aS7777bIsdRVlYm/eMf/5BatGghpaam6v2/lJSUSJIkSX/99Ze0bNky6fDhw1J6err03XffSdHR0VKvXr1s5jhM+Xdk7uMg87CH9yZ70dB7DzXMFO8fVMkUr/9kOg4d6EiSJH344YdSq1atJFdXV+mWW27Ra9tsCwDUelm7dq0kSZJUWFgojRgxQgoODpZcXFykli1bSlOmTJEuXLigt5+ioiJpzpw5UkBAgOTh4SHdfffdNbYxp3Hjxknh4eGSi4uLFBERId13333SiRMndPdrtVppyZIlUlhYmOTm5iYNHjxYOnbsmE0dg+yHH36QAEh//PGH3u22/LvYtWtXrX9HU6ZMkSTJdD//a9euSRMmTJB8fHwkHx8facKECdKNGzcschzp6el1/r/s2rVLkiRJunDhgjR48GApICBAcnV1ldq2bSs9+eST0rVr12zmOEz5d2Tu4yDzsfX3JnvR0HsPNcwU7x9UyRSv/2Q6CkmSJDMli4iIiIiIiKzCYefoEBERERGR82KgQ0REREREDoeBDhERERERORwGOkRERERE5HAY6BARERERkcNhoENERERERA6HgQ4RERERETkcBjpETXDu3DkoFAqkpqaa7TmmTp2Kf/7zn2bbPxEREZEjYqBDTm3q1KlQKBQ1LnfeeadBj4+MjERGRga6du1q5pESERERkTHU1h4AkbXdeeedWLt2rd5tbm5uBj1WpVIhLCzMHMMiIiIioiZgRoecnpubG8LCwvQuzZo1AwAoFAqsXLkSo0aNgoeHB6KiovDVV1/pHlu9dO3GjRuYMGECgoOD4eHhgfbt2+sFUceOHcOQIUPg4eGBwMBAPPbYYygoKNDdr9FosHDhQvj7+yMwMBDPPvssJEnSG68kSXjrrbfQpk0beHh4oEePHvj666/N+BMiIiIisj8MdIga8OKLL+L+++/Hb7/9hokTJ+Lhhx9GWlpanduePHkS33//PdLS0rBy5UoEBQUBAAoLC3HnnXeiWbNmOHz4ML766ivs3LkTc+bM0T3+nXfewZo1a7B69Wrs27cP169fx9atW/+/nbsHaXMLwDj+JEirpkUpVk2LQ0BrEwfbxIK2dihiwaElkrFKgjhYRYIohIqfdNBBQqcKLa2LSsDBDqYoSMFBKYIfi1+LWAUjdGmFSCMS73TDTau9V6hcTP4/COSc97zn5D3bk/ecEzdGR0eHhoaGNDg4qJWVFbW0tKimpkYzMzPnNwkAAAAXjOH457+LgSTi8Xg0PDys1NTUuHqfz6fOzk4ZDAY1NDRocHAwdq20tFR2u12vX7/W1taWLBaLlpaWdOfOHT19+lRZWVl6//79L2O9fftWPp9POzs7MplMkqSPHz/qyZMn2t3dVU5Ojm7cuCGv1yufzydJOjo6ksVikcPh0IcPHxQOh5WVlaVPnz6prKws1nd9fb0ODg40Ojp6HtMEAABw4bBHB0nv0aNHcUFGkq5duxb7/s9A8Xf5tFPWnj9/LpfLpcXFRT1+/FhOp1P379+XJK2tram4uDgWciTpwYMHikaj2tjYUGpqqkKhUNx4KSkpKikpiS1fW11d1Y8fP1RZWRk37uHhoe7evXv2hwcAAEhQBB0kPZPJpPz8/DPdYzAYTqyvqqrSly9fFAwGNT09rYqKCjU1NWlgYEDHx8en3nda/c+i0agkKRgM6ubNm3HX/usBCgAAAMmAPTrAv/j8+fMv5du3b5/a/vr167Elca9evdKbN28kSTabTcvLywqHw7G2s7OzMhqNunXrljIyMmQ2m+PGOzo60sLCQqxss9l0+fJlbW9vKz8/P+6Tl5f3px4ZAADgwuONDpJeJBLR3t5eXF1KSkrsEIGxsTGVlJSovLxcIyMjmp+f17t3707sq6urSw6HQ0VFRYpEIpqYmJDVapUkPXv2TN3d3XK73erp6dHXr1/V3Nys2tpa5eTkSJK8Xq/6+/tVUFAgq9Uqv9+vb9++xfq/evWq2tra1NLSomg0qvLycu3v72tubk5XrlyR2+0+hxkCAAC4eAg6SHqTk5Mym81xdYWFhVpfX5ck9fb2KhAIqLGxUbm5uRoZGZHNZjuxr0uXLunFixfa2tpSWlqaHj58qEAgIElKT0/X1NSUvF6v7t27p/T0dLlcLvn9/tj9ra2tCoVC8ng8MhqNqqurU3V1tb5//x5r8/LlS2VnZ6uvr0+bm5vKzMyU3W5Xe3v7n54aAACAC4tT14DfMBgMGh8fl9Pp/L9/CgAAAM6APToAAAAAEg5BBwAAAEDCYY8O8Bus7AQAALiYeKMDAAAAIOEQdAAAAAAkHIIOAAAAgIRD0AEAAACQcAg6AAAAABIOQQcAAABAwiHoAAAAAEg4BB0AAAAACYegAwAAACDh/AWBL9B7a+6/uQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(all_training_losses, 'b', label='Training loss')\n",
    "ax[0].plot(all_val_scores, 'g', label='Eval score')\n",
    "ax[0].set_xlabel('train epoch')\n",
    "ax[1].plot(np.mean(rewards, axis=-1), 'b', label='Cumulative reward')\n",
    "ax[0].set_xlabel('Episode')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
