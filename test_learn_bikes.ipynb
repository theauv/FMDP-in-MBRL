{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load env dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 46]) torch.Size([10000, 10])\n",
      "{'bikes_distr': slice(0, 43, None), 'day': slice(43, 44, None), 'month': slice(44, 45, None), 'time_counter': slice(45, 46, None)} {'truck_centroid': slice(0, 5, None), 'truck_num_bikes': slice(5, 10, None)}\n"
     ]
    }
   ],
   "source": [
    "load_dir = \"datasets/Bikes/factors_radius_1-2/8\"\n",
    "path = pathlib.Path(load_dir) / \"replay_buffer.npz\"\n",
    "buffer = np.load(path)\n",
    "next_obs = torch.tensor(buffer[\"next_obs\"], dtype=torch.float32)\n",
    "obs = torch.tensor(buffer[\"obs\"], dtype=torch.float32)\n",
    "act = torch.round(torch.tensor(buffer[\"action\"], dtype=torch.float32))\n",
    "reward = torch.tensor(buffer[\"reward\"], dtype=torch.float32)\n",
    "print(obs.shape, act.shape)\n",
    "\n",
    "num_centroids = 43\n",
    "map_obs = {\n",
    "    \"bikes_distr\": slice(0, num_centroids),\n",
    "    \"day\": slice(num_centroids, 44),\n",
    "    \"month\": slice(44, 45),\n",
    "    \"time_counter\": slice(45, 46),\n",
    "}\n",
    "num_trucks = act.shape[-1] // 2\n",
    "map_act = {\n",
    "    \"truck_centroid\": slice(0, num_trucks),\n",
    "    \"truck_num_bikes\": slice(num_trucks, 2 * num_trucks),\n",
    "}\n",
    "\n",
    "print(map_obs, map_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute delta bikes and obs += delta_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_before_action = obs.clone()\n",
    "\n",
    "resize = False\n",
    "while obs.ndim < 3:\n",
    "    assert act.ndim == obs.ndim\n",
    "    obs = obs[None, ...]\n",
    "    act = act[None, ...]\n",
    "    resize = True\n",
    "\n",
    "ensemble_size = obs.shape[0]\n",
    "batch_size = obs.shape[1]\n",
    "distr_size = len(obs[0, 0, map_obs[\"bikes_distr\"]])  # self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "# Compute delta_bikes in a parallel way\n",
    "delta_bikes = np.zeros((ensemble_size, batch_size, distr_size), dtype=int)\n",
    "truck_centroids = act[..., map_act[\"truck_centroid\"]]  # self.map_act[\"truck_centroid\"]\n",
    "truck_bikes = act[..., map_act[\"truck_num_bikes\"]]  # self.map_act[\"truck_num_bikes\"]\n",
    "n = distr_size\n",
    "truck_centroids = np.reshape(\n",
    "    truck_centroids, (truck_centroids.shape[0] * truck_centroids.shape[1], -1)\n",
    ")\n",
    "offset = np.arange(truck_centroids.shape[0])[..., None]\n",
    "truck_centroids_offset = truck_centroids + offset * n\n",
    "unq, inv = np.unique(truck_centroids_offset.ravel(), return_inverse=True)\n",
    "unq = unq.astype(int)\n",
    "sol = np.bincount(inv, truck_bikes.ravel())\n",
    "delta_bikes[\n",
    "    unq // (batch_size * n),\n",
    "    (unq % (batch_size * n)) // n,\n",
    "    (unq % (batch_size * n)) % n,\n",
    "] = sol\n",
    "\n",
    "if resize:\n",
    "    delta_bikes = delta_bikes.reshape((batch_size, -1))\n",
    "    act = act.reshape((batch_size, -1))\n",
    "    obs = obs.reshape((batch_size, -1))\n",
    "\n",
    "# Update obs\n",
    "obs[..., map_obs[\"bikes_distr\"]] += delta_bikes  # self.map_obs[\"bikes_distr\"]\n",
    "\n",
    "assert torch.all(\n",
    "    torch.sum(obs - obs_before_action, axis=-1)\n",
    "    == torch.sum(act[..., map_act[\"truck_num_bikes\"]], axis=-1)\n",
    ")\n",
    "\n",
    "# Super long check to see if preprocess is good, and it is so far\n",
    "for obs_, previous_obs_, truck_centroids, truck_num_bikes in zip(\n",
    "    obs,\n",
    "    obs_before_action,\n",
    "    act[..., map_act[\"truck_centroid\"]],\n",
    "    act[..., map_act[\"truck_num_bikes\"]],\n",
    "):\n",
    "    bikes_idx = torch.nonzero(truck_num_bikes, as_tuple=True)[0]\n",
    "    truck_centroids = truck_centroids[bikes_idx]\n",
    "    centroids_new_bikes = torch.nonzero(obs_ - previous_obs_, as_tuple=True)[0]\n",
    "    # print(torch.sort(centroids_new_bikes).values)\n",
    "    # print(torch.unique(truck_centroids))\n",
    "    # print(torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids))\n",
    "    assert torch.sort(centroids_new_bikes).values in torch.unique(truck_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y from obs and next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 44]) torch.Size([1000, 43])\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 1000\n",
    "input_obs_keys = [\"bikes_distr\", \"time_counter\"]\n",
    "input_act_keys = []  # not implemented\n",
    "output_keys = [\"bikes_distr\"]\n",
    "\n",
    "input_mask = np.zeros(obs.shape[-1])\n",
    "for key in input_obs_keys:\n",
    "    input_mask[map_obs[key]] = 1\n",
    "input_mask = np.ma.make_mask(input_mask)\n",
    "\n",
    "output_mask = np.zeros(obs.shape[-1])\n",
    "for key in output_keys:\n",
    "    output_mask[map_obs[key]] = 1\n",
    "output_mask = np.ma.make_mask(output_mask)\n",
    "\n",
    "assert obs.ndim == 2\n",
    "assert next_obs.ndim == 2\n",
    "x = obs[:dataset_size, input_mask]\n",
    "y = next_obs[:dataset_size, output_mask]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe add a date proxy, weather ? holiday, week-end ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional output preds (e.g. reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 43]) torch.Size([1000, 1])\n",
      "torch.Size([1000, 44])\n"
     ]
    }
   ],
   "source": [
    "learned_rewards = True\n",
    "if learned_rewards:\n",
    "    reward_ = reward[:dataset_size, ...].unsqueeze(-1)\n",
    "    print(y.shape, reward_.shape)\n",
    "    y = torch.cat([y, reward_], dim=-1)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_x, test_x and train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 44]) torch.Size([800, 44]) torch.Size([200, 44]) torch.Size([200, 44])\n"
     ]
    }
   ],
   "source": [
    "test_split_ratio = 0.2\n",
    "\n",
    "train_x = x[int(test_split_ratio * dataset_size) :, ...]\n",
    "train_y = y[int(test_split_ratio * dataset_size) :, ...]\n",
    "test_x = x[: int(test_split_ratio * dataset_size), ...]\n",
    "test_y = y[: int(test_split_ratio * dataset_size), ...]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m      5\u001b[0m eval_epoch_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m in_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m out_size \u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "from src.model.linear_regression import LinearRegression\n",
    "\n",
    "learningRate = 0.01\n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size = train_x.shape[-1]\n",
    "out_size = train_y.shape[-1]\n",
    "device = \"cpu\"\n",
    "lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 4.858578205108643, R2 -0.2092592716217041\n",
      "Eval loss 4.731804847717285, R2 -0.21264058351516724\n",
      "epoch 1, loss 4.854655742645264, R2 -0.20863628387451172\n",
      "Eval loss 4.728012561798096, R2 -0.21200090646743774\n",
      "epoch 2, loss 4.850732803344727, R2 -0.20801307260990143\n",
      "Eval loss 4.724234580993652, R2 -0.2113618403673172\n",
      "epoch 3, loss 4.846813678741455, R2 -0.20738981664180756\n",
      "Eval loss 4.720470905303955, R2 -0.210723415017128\n",
      "epoch 4, loss 4.842899322509766, R2 -0.20676666498184204\n",
      "Eval loss 4.716723442077637, R2 -0.2100858837366104\n",
      "epoch 5, loss 4.838991165161133, R2 -0.20614375174045563\n",
      "Eval loss 4.712994575500488, R2 -0.2094493806362152\n",
      "epoch 6, loss 4.835090160369873, R2 -0.2055210918188095\n",
      "Eval loss 4.709283351898193, R2 -0.20881390571594238\n",
      "epoch 7, loss 4.8311991691589355, R2 -0.20489905774593353\n",
      "Eval loss 4.705592632293701, R2 -0.2081795632839203\n",
      "epoch 8, loss 4.82731819152832, R2 -0.20427750051021576\n",
      "Eval loss 4.701921463012695, R2 -0.20754645764827728\n",
      "epoch 9, loss 4.823448657989502, R2 -0.20365667343139648\n",
      "Eval loss 4.698271751403809, R2 -0.20691464841365814\n",
      "epoch 10, loss 4.819592475891113, R2 -0.20303656160831451\n",
      "Eval loss 4.694642543792725, R2 -0.20628422498703003\n",
      "epoch 11, loss 4.815749645233154, R2 -0.20241735875606537\n",
      "Eval loss 4.691036701202393, R2 -0.20565520226955414\n",
      "epoch 12, loss 4.811922550201416, R2 -0.20179906487464905\n",
      "Eval loss 4.6874518394470215, R2 -0.2050277143716812\n",
      "epoch 13, loss 4.808110237121582, R2 -0.2011817991733551\n",
      "Eval loss 4.683891296386719, R2 -0.20440174639225006\n",
      "epoch 14, loss 4.804314613342285, R2 -0.2005656361579895\n",
      "Eval loss 4.680352687835693, R2 -0.20377740263938904\n",
      "epoch 15, loss 4.800535202026367, R2 -0.19995059072971344\n",
      "Eval loss 4.6768364906311035, R2 -0.20315466821193695\n",
      "epoch 16, loss 4.796773910522461, R2 -0.19933676719665527\n",
      "Eval loss 4.673343181610107, R2 -0.20253361761569977\n",
      "epoch 17, loss 4.79302978515625, R2 -0.19872413575649261\n",
      "Eval loss 4.669873237609863, R2 -0.20191426575183868\n",
      "epoch 18, loss 4.789303779602051, R2 -0.19811291992664337\n",
      "Eval loss 4.666425704956055, R2 -0.20129670202732086\n",
      "epoch 19, loss 4.7855963706970215, R2 -0.197503000497818\n",
      "Eval loss 4.66300106048584, R2 -0.20068088173866272\n",
      "epoch 20, loss 4.78190803527832, R2 -0.19689440727233887\n",
      "Eval loss 4.6595988273620605, R2 -0.20006686449050903\n",
      "epoch 21, loss 4.778237342834473, R2 -0.19628721475601196\n",
      "Eval loss 4.656218528747559, R2 -0.19945472478866577\n",
      "epoch 22, loss 4.774585247039795, R2 -0.19568151235580444\n",
      "Eval loss 4.652860641479492, R2 -0.19884441792964935\n",
      "epoch 23, loss 4.7709527015686035, R2 -0.19507724046707153\n",
      "Eval loss 4.649523735046387, R2 -0.19823594391345978\n",
      "epoch 24, loss 4.767338275909424, R2 -0.19447436928749084\n",
      "Eval loss 4.646209239959717, R2 -0.19762945175170898\n",
      "epoch 25, loss 4.763742923736572, R2 -0.19387315213680267\n",
      "Eval loss 4.64291524887085, R2 -0.19702483713626862\n",
      "epoch 26, loss 4.760165214538574, R2 -0.1932733952999115\n",
      "Eval loss 4.63964319229126, R2 -0.19642215967178345\n",
      "epoch 27, loss 4.7566070556640625, R2 -0.19267500936985016\n",
      "Eval loss 4.636390686035156, R2 -0.19582146406173706\n",
      "epoch 28, loss 4.7530670166015625, R2 -0.1920783519744873\n",
      "Eval loss 4.6331586837768555, R2 -0.19522275030612946\n",
      "epoch 29, loss 4.749545097351074, R2 -0.19148321449756622\n",
      "Eval loss 4.629947662353516, R2 -0.19462595880031586\n",
      "epoch 30, loss 4.746041297912598, R2 -0.1908896416425705\n",
      "Eval loss 4.626755237579346, R2 -0.19403128325939178\n",
      "epoch 31, loss 4.742555141448975, R2 -0.1902976930141449\n",
      "Eval loss 4.6235833168029785, R2 -0.1934385746717453\n",
      "epoch 32, loss 4.7390875816345215, R2 -0.18970727920532227\n",
      "Eval loss 4.620429515838623, R2 -0.19284787774085999\n",
      "epoch 33, loss 4.7356367111206055, R2 -0.18911848962306976\n",
      "Eval loss 4.617295265197754, R2 -0.1922593116760254\n",
      "epoch 34, loss 4.732203960418701, R2 -0.18853138387203217\n",
      "Eval loss 4.6141791343688965, R2 -0.19167286157608032\n",
      "epoch 35, loss 4.728787899017334, R2 -0.18794593214988708\n",
      "Eval loss 4.611082077026367, R2 -0.19108839333057404\n",
      "epoch 36, loss 4.72538948059082, R2 -0.1873619705438614\n",
      "Eval loss 4.60800313949585, R2 -0.19050604104995728\n",
      "epoch 37, loss 4.722008228302002, R2 -0.18677979707717896\n",
      "Eval loss 4.604941368103027, R2 -0.18992595374584198\n",
      "epoch 38, loss 4.718643665313721, R2 -0.18619927763938904\n",
      "Eval loss 4.601898193359375, R2 -0.18934784829616547\n",
      "epoch 39, loss 4.715296745300293, R2 -0.18562036752700806\n",
      "Eval loss 4.598872184753418, R2 -0.18877194821834564\n",
      "epoch 40, loss 4.711965084075928, R2 -0.18504327535629272\n",
      "Eval loss 4.595862865447998, R2 -0.18819822371006012\n",
      "epoch 41, loss 4.708650588989258, R2 -0.18446780741214752\n",
      "Eval loss 4.592871189117432, R2 -0.1876266449689865\n",
      "epoch 42, loss 4.705352306365967, R2 -0.1838940680027008\n",
      "Eval loss 4.589895725250244, R2 -0.1870572715997696\n",
      "epoch 43, loss 4.702071189880371, R2 -0.18332210183143616\n",
      "Eval loss 4.586936950683594, R2 -0.18649014830589294\n",
      "epoch 44, loss 4.698805809020996, R2 -0.1827518790960312\n",
      "Eval loss 4.583994388580322, R2 -0.18592515587806702\n",
      "epoch 45, loss 4.695556640625, R2 -0.18218344449996948\n",
      "Eval loss 4.581068992614746, R2 -0.18536244332790375\n",
      "epoch 46, loss 4.692323684692383, R2 -0.18161676824092865\n",
      "Eval loss 4.578158855438232, R2 -0.18480201065540314\n",
      "epoch 47, loss 4.6891069412231445, R2 -0.18105179071426392\n",
      "Eval loss 4.5752644538879395, R2 -0.18424376845359802\n",
      "epoch 48, loss 4.685906410217285, R2 -0.18048880994319916\n",
      "Eval loss 4.572385787963867, R2 -0.18368782103061676\n",
      "epoch 49, loss 4.682722091674805, R2 -0.17992760241031647\n",
      "Eval loss 4.569522857666016, R2 -0.18313418328762054\n",
      "epoch 50, loss 4.6795525550842285, R2 -0.17936810851097107\n",
      "Eval loss 4.566675662994385, R2 -0.18258272111415863\n",
      "epoch 51, loss 4.6763997077941895, R2 -0.17881061136722565\n",
      "Eval loss 4.5638427734375, R2 -0.18203362822532654\n",
      "epoch 52, loss 4.673262596130371, R2 -0.17825502157211304\n",
      "Eval loss 4.561026096343994, R2 -0.18148687481880188\n",
      "epoch 53, loss 4.670141220092773, R2 -0.1777011901140213\n",
      "Eval loss 4.558224201202393, R2 -0.18094244599342346\n",
      "epoch 54, loss 4.667036056518555, R2 -0.1771494746208191\n",
      "Eval loss 4.555437088012695, R2 -0.18040022253990173\n",
      "epoch 55, loss 4.663947105407715, R2 -0.17659936845302582\n",
      "Eval loss 4.5526652336120605, R2 -0.1798604130744934\n",
      "epoch 56, loss 4.6608734130859375, R2 -0.17605149745941162\n",
      "Eval loss 4.549907684326172, R2 -0.17932286858558655\n",
      "epoch 57, loss 4.657815456390381, R2 -0.17550544440746307\n",
      "Eval loss 4.547164440155029, R2 -0.17878778278827667\n",
      "epoch 58, loss 4.6547746658325195, R2 -0.17496144771575928\n",
      "Eval loss 4.544436454772949, R2 -0.17825503647327423\n",
      "epoch 59, loss 4.651748180389404, R2 -0.17441941797733307\n",
      "Eval loss 4.541722297668457, R2 -0.17772462964057922\n",
      "epoch 60, loss 4.648738384246826, R2 -0.17387937009334564\n",
      "Eval loss 4.539022445678711, R2 -0.17719656229019165\n",
      "epoch 61, loss 4.645744323730469, R2 -0.17334137856960297\n",
      "Eval loss 4.536336898803711, R2 -0.1766708940267563\n",
      "epoch 62, loss 4.64276647567749, R2 -0.17280545830726624\n",
      "Eval loss 4.533665657043457, R2 -0.17614765465259552\n",
      "epoch 63, loss 4.639804840087891, R2 -0.1722715049982071\n",
      "Eval loss 4.531008720397949, R2 -0.175626739859581\n",
      "epoch 64, loss 4.636858940124512, R2 -0.17173971235752106\n",
      "Eval loss 4.528364658355713, R2 -0.17510820925235748\n",
      "epoch 65, loss 4.6339287757873535, R2 -0.17121003568172455\n",
      "Eval loss 4.525735855102539, R2 -0.17459207773208618\n",
      "epoch 66, loss 4.631015777587891, R2 -0.17068248987197876\n",
      "Eval loss 4.523120880126953, R2 -0.17407836019992828\n",
      "epoch 67, loss 4.62811803817749, R2 -0.1701570302248001\n",
      "Eval loss 4.520519733428955, R2 -0.17356708645820618\n",
      "epoch 68, loss 4.625236988067627, R2 -0.16963370144367218\n",
      "Eval loss 4.5179314613342285, R2 -0.1730581670999527\n",
      "epoch 69, loss 4.622372150421143, R2 -0.1691126674413681\n",
      "Eval loss 4.515357971191406, R2 -0.17255163192749023\n",
      "epoch 70, loss 4.619523048400879, R2 -0.16859371960163116\n",
      "Eval loss 4.5127973556518555, R2 -0.17204762995243073\n",
      "epoch 71, loss 4.6166911125183105, R2 -0.16807688772678375\n",
      "Eval loss 4.510251045227051, R2 -0.17154604196548462\n",
      "epoch 72, loss 4.613875389099121, R2 -0.16756241023540497\n",
      "Eval loss 4.507719039916992, R2 -0.17104680836200714\n",
      "epoch 73, loss 4.6110758781433105, R2 -0.16705010831356049\n",
      "Eval loss 4.505199909210205, R2 -0.17054997384548187\n",
      "epoch 74, loss 4.608293533325195, R2 -0.16654004156589508\n",
      "Eval loss 4.502694606781006, R2 -0.17005561292171478\n",
      "epoch 75, loss 4.605527400970459, R2 -0.1660323292016983\n",
      "Eval loss 4.500202655792236, R2 -0.16956372559070587\n",
      "epoch 76, loss 4.60277795791626, R2 -0.1655268520116806\n",
      "Eval loss 4.497725009918213, R2 -0.16907422244548798\n",
      "epoch 77, loss 4.600045204162598, R2 -0.1650235950946808\n",
      "Eval loss 4.495260715484619, R2 -0.16858719289302826\n",
      "epoch 78, loss 4.597329616546631, R2 -0.16452281177043915\n",
      "Eval loss 4.492809772491455, R2 -0.16810260713100433\n",
      "epoch 79, loss 4.594630718231201, R2 -0.1640242487192154\n",
      "Eval loss 4.490372657775879, R2 -0.16762052476406097\n",
      "epoch 80, loss 4.591948509216309, R2 -0.16352806985378265\n",
      "Eval loss 4.487949371337891, R2 -0.1671408861875534\n",
      "epoch 81, loss 4.589283466339111, R2 -0.16303423047065735\n",
      "Eval loss 4.485539436340332, R2 -0.16666360199451447\n",
      "epoch 82, loss 4.586635112762451, R2 -0.16254274547100067\n",
      "Eval loss 4.4831438064575195, R2 -0.1661888211965561\n",
      "epoch 83, loss 4.5840044021606445, R2 -0.162053644657135\n",
      "Eval loss 4.480761528015137, R2 -0.1657165139913559\n",
      "epoch 84, loss 4.581390380859375, R2 -0.16156698763370514\n",
      "Eval loss 4.4783935546875, R2 -0.16524666547775269\n",
      "epoch 85, loss 4.578793525695801, R2 -0.16108274459838867\n",
      "Eval loss 4.476037979125977, R2 -0.16477929055690765\n",
      "epoch 86, loss 4.57621431350708, R2 -0.16060088574886322\n",
      "Eval loss 4.473697662353516, R2 -0.164314404129982\n",
      "epoch 87, loss 4.573651313781738, R2 -0.1601213663816452\n",
      "Eval loss 4.471370220184326, R2 -0.16385199129581451\n",
      "epoch 88, loss 4.571106910705566, R2 -0.15964451432228088\n",
      "Eval loss 4.469056606292725, R2 -0.16339202225208282\n",
      "epoch 89, loss 4.56857967376709, R2 -0.1591700166463852\n",
      "Eval loss 4.466757297515869, R2 -0.1629345417022705\n",
      "epoch 90, loss 4.566069602966309, R2 -0.15869790315628052\n",
      "Eval loss 4.46447229385376, R2 -0.16247950494289398\n",
      "epoch 91, loss 4.563576698303223, R2 -0.15822841227054596\n",
      "Eval loss 4.462200164794922, R2 -0.16202706098556519\n",
      "epoch 92, loss 4.561101913452148, R2 -0.157761350274086\n",
      "Eval loss 4.4599432945251465, R2 -0.16157697141170502\n",
      "epoch 93, loss 4.558643341064453, R2 -0.15729667246341705\n",
      "Eval loss 4.457699775695801, R2 -0.1611294448375702\n",
      "epoch 94, loss 4.5562028884887695, R2 -0.15683472156524658\n",
      "Eval loss 4.455470561981201, R2 -0.16068436205387115\n",
      "epoch 95, loss 4.553781032562256, R2 -0.15637506544589996\n",
      "Eval loss 4.453255653381348, R2 -0.16024179756641388\n",
      "epoch 96, loss 4.551375389099121, R2 -0.15591801702976227\n",
      "Eval loss 4.451054096221924, R2 -0.15980170667171478\n",
      "epoch 97, loss 4.548987865447998, R2 -0.15546347200870514\n",
      "Eval loss 4.4488677978515625, R2 -0.15936411917209625\n",
      "epoch 98, loss 4.5466179847717285, R2 -0.155011385679245\n",
      "Eval loss 4.446695804595947, R2 -0.1589289754629135\n",
      "epoch 99, loss 4.544264793395996, R2 -0.15456196665763855\n",
      "Eval loss 4.444537162780762, R2 -0.15849639475345612\n",
      "epoch 100, loss 4.541930198669434, R2 -0.15411503612995148\n",
      "Eval loss 4.442392349243164, R2 -0.1580662578344345\n",
      "epoch 101, loss 4.539612770080566, R2 -0.15367057919502258\n",
      "Eval loss 4.440263271331787, R2 -0.15763862431049347\n",
      "epoch 102, loss 4.5373125076293945, R2 -0.15322870016098022\n",
      "Eval loss 4.438148498535156, R2 -0.1572134643793106\n",
      "epoch 103, loss 4.535030364990234, R2 -0.15278930962085724\n",
      "Eval loss 4.436047077178955, R2 -0.15679079294204712\n",
      "epoch 104, loss 4.5327653884887695, R2 -0.15235257148742676\n",
      "Eval loss 4.433960914611816, R2 -0.1563706398010254\n",
      "epoch 105, loss 4.530517578125, R2 -0.15191830694675446\n",
      "Eval loss 4.431889057159424, R2 -0.15595294535160065\n",
      "epoch 106, loss 4.528287887573242, R2 -0.1514865756034851\n",
      "Eval loss 4.429831504821777, R2 -0.15553773939609528\n",
      "epoch 107, loss 4.5260748863220215, R2 -0.1510573923587799\n",
      "Eval loss 4.427789211273193, R2 -0.1551249921321869\n",
      "epoch 108, loss 4.523879528045654, R2 -0.15063072741031647\n",
      "Eval loss 4.425760269165039, R2 -0.15471476316452026\n",
      "epoch 109, loss 4.521701812744141, R2 -0.15020662546157837\n",
      "Eval loss 4.423746585845947, R2 -0.1543070375919342\n",
      "epoch 110, loss 4.519540786743164, R2 -0.1497850865125656\n",
      "Eval loss 4.421746253967285, R2 -0.15390163660049438\n",
      "epoch 111, loss 4.517396926879883, R2 -0.14936605095863342\n",
      "Eval loss 4.419761657714844, R2 -0.1534988284111023\n",
      "epoch 112, loss 4.515270709991455, R2 -0.1489494889974594\n",
      "Eval loss 4.417791366577148, R2 -0.1530984789133072\n",
      "epoch 113, loss 4.5131611824035645, R2 -0.1485355645418167\n",
      "Eval loss 4.415834903717041, R2 -0.1527005434036255\n",
      "epoch 114, loss 4.511068344116211, R2 -0.14812403917312622\n",
      "Eval loss 4.413893222808838, R2 -0.15230517089366913\n",
      "epoch 115, loss 4.508993148803711, R2 -0.14771506190299988\n",
      "Eval loss 4.411966323852539, R2 -0.15191207826137543\n",
      "epoch 116, loss 4.506934642791748, R2 -0.14730864763259888\n",
      "Eval loss 4.410053253173828, R2 -0.1515214890241623\n",
      "epoch 117, loss 4.504892349243164, R2 -0.1469046026468277\n",
      "Eval loss 4.40815544128418, R2 -0.15113335847854614\n",
      "epoch 118, loss 4.502867698669434, R2 -0.14650310575962067\n",
      "Eval loss 4.4062724113464355, R2 -0.1507476419210434\n",
      "epoch 119, loss 4.500858306884766, R2 -0.1461041420698166\n",
      "Eval loss 4.404402732849121, R2 -0.15036438405513763\n",
      "epoch 120, loss 4.498866081237793, R2 -0.14570748805999756\n",
      "Eval loss 4.402547836303711, R2 -0.14998355507850647\n",
      "epoch 121, loss 4.496890068054199, R2 -0.14531347155570984\n",
      "Eval loss 4.400707721710205, R2 -0.14960502088069916\n",
      "epoch 122, loss 4.494930744171143, R2 -0.14492176473140717\n",
      "Eval loss 4.398880958557129, R2 -0.14922894537448883\n",
      "epoch 123, loss 4.492987155914307, R2 -0.14453262090682983\n",
      "Eval loss 4.397068977355957, R2 -0.14885522425174713\n",
      "epoch 124, loss 4.49105978012085, R2 -0.14414580166339874\n",
      "Eval loss 4.395271301269531, R2 -0.1484839767217636\n",
      "epoch 125, loss 4.489149570465088, R2 -0.14376147091388702\n",
      "Eval loss 4.393487930297852, R2 -0.14811502397060394\n",
      "epoch 126, loss 4.4872541427612305, R2 -0.14337952435016632\n",
      "Eval loss 4.391718864440918, R2 -0.14774848520755768\n",
      "epoch 127, loss 4.4853739738464355, R2 -0.14299993216991425\n",
      "Eval loss 4.389963150024414, R2 -0.14738430082798004\n",
      "epoch 128, loss 4.483510494232178, R2 -0.14262281358242035\n",
      "Eval loss 4.388221740722656, R2 -0.14702238142490387\n",
      "epoch 129, loss 4.481661796569824, R2 -0.1422479748725891\n",
      "Eval loss 4.386494159698486, R2 -0.14666281640529633\n",
      "epoch 130, loss 4.47982931137085, R2 -0.14187555015087128\n",
      "Eval loss 4.3847808837890625, R2 -0.14630559086799622\n",
      "epoch 131, loss 4.478011131286621, R2 -0.1415054202079773\n",
      "Eval loss 4.383081436157227, R2 -0.1459508091211319\n",
      "epoch 132, loss 4.4762091636657715, R2 -0.14113765954971313\n",
      "Eval loss 4.3813958168029785, R2 -0.14559811353683472\n",
      "epoch 133, loss 4.474421977996826, R2 -0.14077211916446686\n",
      "Eval loss 4.379724025726318, R2 -0.14524783194065094\n",
      "epoch 134, loss 4.472649097442627, R2 -0.14040903747081757\n",
      "Eval loss 4.37806510925293, R2 -0.14489977061748505\n",
      "epoch 135, loss 4.47089147567749, R2 -0.14004813134670258\n",
      "Eval loss 4.376420497894287, R2 -0.1445540189743042\n",
      "epoch 136, loss 4.469148635864258, R2 -0.13968949019908905\n",
      "Eval loss 4.374789237976074, R2 -0.1442105621099472\n",
      "epoch 137, loss 4.4674201011657715, R2 -0.13933324813842773\n",
      "Eval loss 4.373171329498291, R2 -0.14386917650699615\n",
      "epoch 138, loss 4.4657063484191895, R2 -0.13897916674613953\n",
      "Eval loss 4.3715667724609375, R2 -0.14353016018867493\n",
      "epoch 139, loss 4.464006423950195, R2 -0.1386273354291916\n",
      "Eval loss 4.369975566864014, R2 -0.14319340884685516\n",
      "epoch 140, loss 4.4623212814331055, R2 -0.13827762007713318\n",
      "Eval loss 4.368397235870361, R2 -0.14285868406295776\n",
      "epoch 141, loss 4.4606499671936035, R2 -0.1379302442073822\n",
      "Eval loss 4.3668317794799805, R2 -0.1425262689590454\n",
      "epoch 142, loss 4.4589924812316895, R2 -0.13758501410484314\n",
      "Eval loss 4.3652801513671875, R2 -0.14219599962234497\n",
      "epoch 143, loss 4.457348823547363, R2 -0.1372418850660324\n",
      "Eval loss 4.363740921020508, R2 -0.14186789095401764\n",
      "epoch 144, loss 4.455718040466309, R2 -0.13690103590488434\n",
      "Eval loss 4.362215042114258, R2 -0.1415419578552246\n",
      "epoch 145, loss 4.454102039337158, R2 -0.13656224310398102\n",
      "Eval loss 4.360701560974121, R2 -0.1412181705236435\n",
      "epoch 146, loss 4.452498912811279, R2 -0.13622558116912842\n",
      "Eval loss 4.359200477600098, R2 -0.14089646935462952\n",
      "epoch 147, loss 4.450909614562988, R2 -0.13589102029800415\n",
      "Eval loss 4.357712268829346, R2 -0.14057695865631104\n",
      "epoch 148, loss 4.449332237243652, R2 -0.13555863499641418\n",
      "Eval loss 4.356236457824707, R2 -0.1402594894170761\n",
      "epoch 149, loss 4.4477691650390625, R2 -0.13522829115390778\n",
      "Eval loss 4.35477352142334, R2 -0.1399441361427307\n",
      "epoch 150, loss 4.446218490600586, R2 -0.13489986956119537\n",
      "Eval loss 4.353322505950928, R2 -0.13963079452514648\n",
      "epoch 151, loss 4.444680213928223, R2 -0.13457365334033966\n",
      "Eval loss 4.351883888244629, R2 -0.13931958377361298\n",
      "epoch 152, loss 4.443155288696289, R2 -0.1342494636774063\n",
      "Eval loss 4.350457191467285, R2 -0.13901036977767944\n",
      "epoch 153, loss 4.4416422843933105, R2 -0.13392728567123413\n",
      "Eval loss 4.349042892456055, R2 -0.13870322704315186\n",
      "epoch 154, loss 4.4401421546936035, R2 -0.13360707461833954\n",
      "Eval loss 4.347640037536621, R2 -0.13839806616306305\n",
      "epoch 155, loss 4.438653469085693, R2 -0.13328884541988373\n",
      "Eval loss 4.346249580383301, R2 -0.13809488713741302\n",
      "epoch 156, loss 4.437177658081055, R2 -0.1329725682735443\n",
      "Eval loss 4.344870090484619, R2 -0.13779376447200775\n",
      "epoch 157, loss 4.435713768005371, R2 -0.13265828788280487\n",
      "Eval loss 4.343502521514893, R2 -0.1374945491552353\n",
      "epoch 158, loss 4.434261322021484, R2 -0.13234587013721466\n",
      "Eval loss 4.342146873474121, R2 -0.1371973603963852\n",
      "epoch 159, loss 4.4328203201293945, R2 -0.132035493850708\n",
      "Eval loss 4.340802192687988, R2 -0.1369020789861679\n",
      "epoch 160, loss 4.431392192840576, R2 -0.13172698020935059\n",
      "Eval loss 4.339468479156494, R2 -0.13660873472690582\n",
      "epoch 161, loss 4.4299750328063965, R2 -0.13142035901546478\n",
      "Eval loss 4.338147163391113, R2 -0.13631729781627655\n",
      "epoch 162, loss 4.428569793701172, R2 -0.1311156451702118\n",
      "Eval loss 4.336836338043213, R2 -0.13602778315544128\n",
      "epoch 163, loss 4.427175045013428, R2 -0.13081276416778564\n",
      "Eval loss 4.335536479949951, R2 -0.13574017584323883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# get gradients w.r.t to parameters\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# update parameters\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[idx, :]\n",
    "    train_y = train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    test_x = test_x[idx, :]\n",
    "    test_y = test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = lr_model(train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print(\"epoch {}, loss {}, R2 {}\".format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch % eval_epoch_freq == 0:\n",
    "        with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "            preds = lr_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            # Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print(\"Eval loss {}, R2 {}\".format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = lr_model(train_x)\n",
    "    test_preds = lr_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Env\u001b[39;00m\n\u001b[1;32m     42\u001b[0m env_config \u001b[38;5;241m=\u001b[39m omegaconf\u001b[38;5;241m.\u001b[39mDictConfig(env_config)\n\u001b[0;32m---> 43\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mBikes\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     46\u001b[0m act_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mnt/c/users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/src/env/bikes.py:276\u001b[0m, in \u001b[0;36mBikes.__init__\u001b[0;34m(self, env_config, render_mode)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDict(\n\u001b[1;32m    258\u001b[0m     {\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruck_num_bikes\u001b[39m\u001b[38;5;124m\"\u001b[39m: spaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m     }\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# TODO: could have a negative number of bikes meaning that we remove some bikes\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# in reward the more we have unused bikes the worst it is\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_trips_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_trip_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTripID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStartDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEndDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEndTime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weather_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir \u001b[38;5;241m+\u001b[39m env_config\u001b[38;5;241m.\u001b[39mweather_data,\n\u001b[1;32m    282\u001b[0m     usecols\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     ],\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# self.period = (\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#     \"Month > 0 & Month < 13 & Year == 19 & DayOfWeek >=0 and DayOfWeek <=8\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdm_env/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1011\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/users/theau/OneDrive/Documents/theau_epfl/12.PDM/code/HUCRL_for_FMDP/src/env/bikes.py:278\u001b[0m, in \u001b[0;36mBikes.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDict(\n\u001b[1;32m    258\u001b[0m     {\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruck_num_bikes\u001b[39m\u001b[38;5;124m\"\u001b[39m: spaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m     }\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# TODO: could have a negative number of bikes meaning that we remove some bikes\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# in reward the more we have unused bikes the worst it is\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_trips_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir \u001b[38;5;241m+\u001b[39m env_config\u001b[38;5;241m.\u001b[39mpast_trip_data,\n\u001b[0;32m--> 278\u001b[0m     usecols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTripID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStartDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndTime\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weather_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir \u001b[38;5;241m+\u001b[39m env_config\u001b[38;5;241m.\u001b[39mweather_data,\n\u001b[1;32m    282\u001b[0m     usecols\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     ],\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# self.period = (\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#     \"Month > 0 & Month < 13 & Year == 19 & DayOfWeek >=0 and DayOfWeek <=8\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TEST with model trainer\n",
    "import omegaconf\n",
    "\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.simple import Simple, FactoredSimple\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.util.model_trainer import ModelTrainerOverriden, MultiModelsTrainer\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "import mbrl.util.common\n",
    "\n",
    "# WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "# Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5,  # 10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\",  # sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.0,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    # \"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None,  # src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "# Params\n",
    "seed = 1\n",
    "device = \"cpu\"\n",
    "target_is_delta = True #Falses\n",
    "normalize = False\n",
    "use_double_dtype = False  # True\n",
    "optim_lr = learningRate\n",
    "model_wd = 0.#000001\n",
    "model_batch_size = dataset_size\n",
    "validation_ratio = test_split_ratio\n",
    "num_epochs = epochs\n",
    "\n",
    "# Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(env_config, render_mode=None)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# Seed\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# torch_generator = torch.Generator(device=device)\n",
    "# if seed is not None:\n",
    "#     torch_generator.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dynamics model\n",
    "from src.model.simple import Simple, FactoredSimple\n",
    "from src.model.gaussian_process import MultiOutputGP\n",
    "from src.util.util import get_base_dir_path\n",
    "\n",
    "base_dir = get_base_dir_path()\n",
    "adjacency = np.load(base_dir + \"src/env/bikes_data/factors_radius_1-2.npy\")\n",
    "factors = [[i for i, e in enumerate(station) if e != 0] for station in adjacency]\n",
    "# Compute the additional scopes\n",
    "input_obs_keys = input_obs_keys\n",
    "input_act_keys = input_act_keys\n",
    "n_scopes_before = 0\n",
    "n_scopes_after = 0\n",
    "before = True\n",
    "for key in env.dict_observation_space.keys():\n",
    "    if key in input_obs_keys:\n",
    "        if key != \"bikes_distr\":\n",
    "            scope_length = env.dict_observation_space[key].shape[0]\n",
    "            if before:\n",
    "                n_scopes_before += scope_length\n",
    "            else:\n",
    "                n_scopes_after += scope_length\n",
    "        else:\n",
    "            before = False\n",
    "    elif key == \"bikes_distr\":\n",
    "        raise ValueError(\n",
    "            \"Key 'bikes_distr' must be in the input keys\"\n",
    "            \"when using a factored model (useless otherwise)\"\n",
    "        )\n",
    "for key in input_act_keys:\n",
    "    n_scopes_after += env.dict_action_space[key].shape[0]\n",
    "\n",
    "scopes_after = [i + adjacency.shape[0] for i in range(n_scopes_after)]\n",
    "factors = [factor + scopes_after for factor in factors]\n",
    "factors = [[scope + n_scopes_before for scope in factor] for factor in factors]\n",
    "scopes_before = [i for i in range(n_scopes_before)]\n",
    "factors = [scopes_before + factor for factor in factors]\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    Simple(in_size, out_size, device), #factors, num_layers=2, hid_size=100),\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "# Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "# Load replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    dataset_size,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    # max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=500)\n",
    "\n",
    "dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    model_batch_size,\n",
    "    validation_ratio,\n",
    "    ensemble_size=len(dynamics_model),\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,\n",
    ")\n",
    "\n",
    "if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "    dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "train_losses, test_losses, train_metrics, test_metrics = model_trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=num_epochs,\n",
    "    evaluate=True,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAAFzCAYAAAD7Zwx+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABW0UlEQVR4nO3deXhU5f3+8XsySSZ7AsSQRBLCIquALCrBBRBZv6IWasWFRQUKChTRorgAohRbQRFtof5qwa2KbdDWDcFKQMsiQRBUiIhAEBIDKAkQMiGT8/tjyJCwhEySWTLn/bquc03mzFk+cxLmcM/znOdYDMMwBAAAAAABKMjXBQAAAACApxB4AAAAAAQsAg8AAACAgEXgAQAAABCwCDwAAAAAAhaBBwAAAEDAIvAAAAAACFgEHgAAAAABK9jXBVRHWVmZDhw4oOjoaFksFl+XAwCmYRiGjh49quTkZAUF8R1ZRZybAMA33D031YvAc+DAAaWkpPi6DAAwrX379qlJkya+LsOvcG4CAN+q7rmpXgSe6OhoSc43FRMT4+NqAMA8CgsLlZKS4vocxmmcmwDAN9w9N9WLwFPeVSAmJoaTCgD4AF22zsa5CQB8q7rnJjpkAwAAAAhYBB4AAAAAAYvAAwAAACBg1YtreADUDcMwVFpaKofD4etS4CesVquCg4O5RgcAELAIPIBJlJSUKDc3V0VFRb4uBX4mIiJCSUlJCg0N9XUpAADUOQIPYAJlZWXavXu3rFarkpOTFRoayjf6kGEYKikp0cGDB7V7925dcskl3FwUABBwCDyACZSUlKisrEwpKSmKiIjwdTnwI+Hh4QoJCdHevXtVUlKisLAwX5cEAECd4qs8wET49h7nwt8FACCQcZYDAAAAELACvkvbkSPSp59KYWHSoEG+rgYAgLpTViYdOyYdPeqcjh+XSkslh+P0Y8WfDaPy+hd6Xt1551qmpty9vNDTy3tjH/5Ykzf24Y812e1SSUn1lq3J3315Pe481mSd2uyruNj5WeLpAV0jIqQBAzy7j3IBH3j27JGGDpWSkqQDB3xdDQBf69Wrly677DLNnz+/Wsvv2bNHzZo10+bNm3XZZZd5rK7MzEz17t1bv/zyi+Li4jy2H9RPR49Kq1dLX34pbdki5eRIP/4o5efXbdgAAG9p2tT5/3RvCPjAU941nRMCUL9caBS5kSNHasmSJW5vd9myZQoJCan28ikpKcrNzVV8fLzb+wJq68svpT/9SXr3Xec3z+djtUrR0VJkpBQS4nweHFz50Wo9fU6s6Mx/auf6p1edeXUx8KO752pPL++NffhjTd7Yh7/WZLdLDRue+9/Kubjzd19eT3Ue3Vm2Lh9PnHB+ViQkOD9LPCkx0bPbryjgA0/5H2JZmW/rAOCe3Nxc189Lly7V9OnTlZ2d7ZoXHh5eafmTJ09WK8g0bNjQrTqsVqsSvfmpDMj5n4/HHpP+8IfT81q0kK66Surc2flzkybO3guxsc5u24w0DwDnFvCDFpSfAGjhASozDGcfXW9P1f23mJiY6JpiY2NlsVhcz4uLixUXF6e3335bvXr1UlhYmF5//XUdPnxYt912m5o0aaKIiAh16NBBb775ZqXt9urVS5MnT3Y9T0tL0x/+8Afdfffdio6OVmpqql566SXX63v27JHFYtGWLVskObueWSwW/fe//1W3bt0UERGhHj16VApjkvTUU08pISFB0dHRGj16tB5++GG3u8RlZGSoffv2stlsSktL07x58yq9/pe//EWXXHKJwsLC1LhxY/361792vfavf/1LHTp0UHh4uBo1aqTrr79ex48fd2v/8I2TJ6W77joddm6/3dnSs3On9Mor0uTJ0uDBzuCTmCiFhxN2AKAqBB7ApIqKpKgo709FRXX3Hh566CFNmjRJ27dvV//+/VVcXKyuXbvq/fff19dff62xY8dq+PDh2rBhQ5XbmTdvnrp166bNmzfr3nvv1fjx47Vjx44q13n00Uc1b948ZWVlKTg4WHfffbfrtTfeeEOzZ8/WH//4R23atEmpqalauHChW+9t06ZN+s1vfqNhw4Zp27Ztmjlzph5//HFXN76srCxNmjRJs2bNUnZ2tpYvX65rr71WkrN17LbbbtPdd9+t7du3KzMzU0OGDJHBB6HfKyuT7rzTGWysVulvf5PeeMMZbgg1AFAzAd+lrbwPJl3agMAzefJkDRkypNK8Bx980PXzxIkTtXz5cv3zn//UlVdeed7tDBo0SPfee68kZ4h67rnnlJmZqTZt2px3ndmzZ6tnz56SpIcfflj/93//p+LiYoWFhemFF17QPffco7vuukuSNH36dK1YsULHjh2r9nt79tln1adPHz3++OOSpFatWunbb7/VM888o1GjRiknJ0eRkZG64YYbFB0draZNm6pz586SnIGntLRUQ4YMUdOmTSVJHTp0qPa+4Tv//rf09tvOvvMZGc6WHABA7QR84KGFBzi3iAjncLa+2G9d6datW6XnDodDTz/9tJYuXar9+/fLbrfLbrcrMjKyyu107NjR9XN517n8/Pxqr5OUlCRJys/PV2pqqrKzs10BqtwVV1yhTz/9tFrvS5K2b9+um266qdK8q666SvPnz5fD4VDfvn3VtGlTNW/eXAMGDNCAAQP0q1/9ShEREerUqZP69OmjDh06qH///urXr59+/etfq0GDBtXeP3zj5Zedjw88ULuw4yhz6GTZSUmq1LJnqMLPp+ZXnBcILKrfTWEXGrDF39Xn428NsiokyHktaJlRpjKjzGf/Ps48jmf+Xbjzurt/U2VGmYIsQc73X8V/oC90bKrTqyDE6uGREU4h8AAmZbE4R3Sqz84MMvPmzdNzzz2n+fPnq0OHDoqMjNTkyZNVcoGbKpw52IHFYlHZBZqFK65TfjKpuM6ZJxh3u5MZhlHlNqKjo/Xll18qMzNTK1as0PTp0zVz5kxt3LhRcXFxWrlypdauXasVK1bohRde0KOPPqoNGzaoWbNmbtUB73E4pM8+c/78m9+c/fqR4iP6X87/tPWnrdpbsFf5x/Nd07GSY7I77LKX2mV32FVaVurd4oEAYpEl4L4IqErFcFT+vr1xDJrGNtWeyXs8uo9yAR946NIGmMdnn32mm266SXfeeackZwDZuXOn2rZt69U6WrdurS+++ELDhw93zcvKynJrG+3atdPnn39ead7atWvVqlUrWa1WSVJwcLCuv/56XX/99ZoxY4bi4uL06aefasiQIbJYLLrqqqt01VVXafr06WratKneeecdTZkypfZvEB6xdatUWCjFxEjlDYiGYejDnR/qT2v/pM9zPleZwckM8DQzhR3p3O830I5BwAceWngA82jZsqUyMjK0du1aNWjQQM8++6zy8vK8HngmTpyoMWPGqFu3burRo4eWLl2qrVu3qnnz5tXexgMPPKDLL79cTz75pG699VatW7dOL774ov7yl79Ikt5//3398MMPuvbaa9WgQQN9+OGHKisrU+vWrbVhwwb997//Vb9+/ZSQkKANGzbo4MGDXj8OcM+aNc7Hq692DljwTf43GvnuSG3K3eRaplWjVuqW3E2XNLxEjSMbKyEyQRdFXqQYW4xsVptswTbXY6g19LzdWi7UHaa+qu8Dc9T3/2TW9+NfYC/Q8ZLjirHFKDgoWMFBwT75t3HmcTzz76Kq12uzriQdLzmu8JBwWWRRiDVEQZaqxze7UBfGqo6fN7s/EngABIzHH39cu3fvVv/+/RUREaGxY8fq5ptvVkFBgVfruOOOO/TDDz/owQcfVHFxsX7zm99o1KhR+uKLL6q9jS5duujtt9/W9OnT9eSTTyopKUmzZs3SqFGjJElxcXFatmyZZs6cqeLiYl1yySV688031b59e23fvl1r1qzR/PnzVVhYqKZNm2revHkaOHCgh94x6sLq1c7Ha66RPs/5XANeH6DjJ48rKjRK47uN14QrJig1NtW3RQIBLNoW7esS4CEWox7E8cLCQsXGxqqgoEAxMTFurbt7t9S8ufNaBV9coA34g+LiYu3evVvNmjVTWFiYr8sxpb59+yoxMVGvvfaar0s5S1V/H7X5/A10dXlsysqcdzY/fFh6Y8W3GpfVXUdLjqpXWi+9OfRNJUZx81sAKOfu569pWni4hgeAtxQVFWnRokXq37+/rFar3nzzTX3yySdauXKlr0uDn/r2W2fYCY8s1R923qqjJUd1Teo1+uD2DxQRUodDGwKACZkm8Ph/OxaAQGGxWPThhx/qqaeekt1uV+vWrZWRkaHrr7/e16XBT5Vfv9Pyhne07eDXahjeUP+85Z+EHQCoAwEfeMpHaSPwAPCW8PBwffLJJ74uA/VI+fU7lnbvSoY0tstYNY5q7NOaACBQVD30QgCgSxsAwJ8ZRnngMZQb6mzqub45rYEAUFdME3ho4QEAnOkvf/mLa7CGrl276rPyO3960XffST/9JIUm7NVB+48KDgpW9ybdvV4HAASqgA88dGkDAJzL0qVLNXnyZD366KPavHmzrrnmGg0cOFA5OTleraM8YzXr5Wzd6ZrUVZGhkV6tAQACWcAHHrq0AQDO5dlnn9U999yj0aNHq23btpo/f75SUlK0cOFCr9axfr3z0dY6U5LUK62XV/cPAIHONIGHFh4AQLmSkhJt2rRJ/fr1qzS/X79+Wrt27TnXsdvtKiwsrDTVhQ0bJFnt2mt7X5LUO613nWwXAODkVuBZuHChOnbsqJiYGMXExCg9PV0fffTReZfPzMyUxWI5a9qxY0etC6+u8sAjEXoAuMdisejdd9/1dRnwgEOHDsnhcKhx48ojoTVu3Fh5eXnnXGfOnDmKjY11TSkpKbWu48QJ6ZtvJLVdpoLSg0qOTlaf5n1qvV0AwGluBZ4mTZro6aefVlZWlrKysnTdddfppptu0jfffFPletnZ2crNzXVNl1xySa2KdkdQhXdI4AHqj3N9WVJxGjVqVI23nZaWpvnz59dZrai/LBW/FZNkGMZZ88pNmzZNBQUFrmnfvn213n9OjvPcZO3u7EY3pssYBQcF/B0jAMCr3PpUHTx4cKXns2fP1sKFC7V+/Xq1b9/+vOslJCQoLi6uRgXW1tGSAqnVGskRKsPo75MaALgvNzfX9fPSpUs1ffp0ZWdnu+aFh4f7oiwEiPj4eFmt1rNac/Lz889q9Slns9lks9nqtI6cHEmNvpOjyWcKsgRpdJfRdbp9AEAtruFxOBx66623dPz4caWnp1e5bOfOnZWUlKQ+ffpo1apVF9x2XfaT3nd0j3T7jdLNd9HCA9QjiYmJrik2NlYWi6XSvDVr1qhr164KCwtT8+bN9cQTT6i0tNS1/syZM5Wamiqbzabk5GRNmjRJktSrVy/t3btX999/v6u1qLq2bdum6667TuHh4WrUqJHGjh2rY8eOuV7PzMzUFVdcocjISMXFxemqq67S3r17JUlfffWVevfurejoaMXExKhr167Kysqqo6MFd4WGhqpr165auXJlpfkrV65Ujx49vFbH/v2SOrwhSRrQcoCaxDTx2r4BwCzcbjfftm2b0tPTVVxcrKioKL3zzjtq167dOZdNSkrSSy+9pK5du8put+u1115Tnz59lJmZqWuvvfa8+5gzZ46eeOIJd0s7p6CgU/+ZsZQxUhtQgWEYKjpZ5PX9RoREuBUyzuXjjz/WnXfeqQULFuiaa67Rrl27NHbsWEnSjBkz9K9//UvPPfec3nrrLbVv3155eXn66quvJEnLli1Tp06dNHbsWI0ZM6ba+ywqKtKAAQPUvXt3bdy4Ufn5+Ro9erQmTJigJUuWqLS0VDfffLPGjBmjN998UyUlJfriiy9c7/WOO+5Q586dtXDhQlmtVm3ZskUhISG1Og6onSlTpmj48OHq1q2b0tPT9dJLLyknJ0fjxo3zWg0FBZKaOselvrn1zV7bLwCYiduBp3Xr1tqyZYuOHDmijIwMjRw5UqtXrz5n6GndurVat27tep6enq59+/Zp7ty5VQaeadOmacqUKa7nhYWFNb441Oq6iMeghQeooOhkkaLmRHl9v8emHav1PUZmz56thx9+WCNHjpQkNW/eXE8++aSmTp2qGTNmKCcnR4mJibr++usVEhKi1NRUXXHFFZKkhg0bymq1Kjo6WomJidXe5xtvvKETJ07o1VdfVWSks/4XX3xRgwcP1h//+EeFhISooKBAN9xwg1q0aCFJatu2rWv9nJwc/f73v1ebNm0kyavXMuLcbr31Vh0+fFizZs1Sbm6uLr30Un344Ydq2rSp12r45UiZlOxs6eNmowDgGW53aQsNDVXLli3VrVs3zZkzR506ddLzzz9f7fW7d++unTt3VrmMzWZzjQRXPtWUK/BYygg8QIDYtGmTZs2apaioKNc0ZswY5ebmqqioSLfccotOnDih5s2ba8yYMXrnnXcqdXerie3bt6tTp06usCNJV111lcrKypSdna2GDRtq1KhR6t+/vwYPHqznn3++0nVIU6ZM0ejRo3X99dfr6aef1q5du2pVD+rGvffeqz179shut2vTpk1VfhnnCblH8yTbUVkMq9pe1PbCKwAA3FbroWAMw5Ddbq/28ps3b1ZSUlJtd1ttdGkDzi0iJELHph278IIe2G9tlZWV6YknntCQIUPOei0sLEwpKSnKzs7WypUr9cknn+jee+/VM888o9WrV9e4G1lVo3eVz1+8eLEmTZqk5cuXa+nSpXrssce0cuVKde/eXTNnztTtt9+uDz74QB999JFmzJiht956S7/61a9qVA8CQ97xA1K0FG1JZHQ2APAQtz5dH3nkEQ0cOFApKSk6evSo3nrrLWVmZmr58uWSnF3R9u/fr1dffVWSNH/+fKWlpal9+/YqKSnR66+/royMDGVkZNT9OzkPWniAc7NYLLXuWuYrXbp0UXZ2tlq2bHneZcLDw3XjjTfqxhtv1H333ac2bdpo27Zt6tKli0JDQ+VwONzaZ7t27fTKK6/o+PHjrlae//3vfwoKClKrVq1cy3Xu3FmdO3fWtGnTlJ6ern/84x/q3t3ZValVq1Zq1aqV7r//ft12221avHgxgcfkDtkPSJLirMk+rgQAApdbgeenn37S8OHDlZubq9jYWHXs2FHLly9X3759JTmHkc3JyXEtX1JSogcffFD79+9XeHi42rdvrw8++ECDBg2q23dRhdOBh2t4gEAxffp03XDDDUpJSdEtt9yioKAgbd26Vdu2bdNTTz2lJUuWyOFw6Morr1RERIRee+01hYeHu67NSEtL05o1azRs2DDZbDbFx8dfcJ933HGHZsyYoZEjR2rmzJk6ePCgJk6cqOHDh6tx48bavXu3XnrpJd14441KTk5Wdna2vvvuO40YMUInTpzQ73//e/36179Ws2bN9OOPP2rjxo0aOnSopw8V/FxBmTPwNAol8ACAp7gVeF5++eUqX1+yZEml51OnTtXUqVPdLqouBVdo4aFLGxAY+vfvr/fff1+zZs3Sn/70J4WEhKhNmzYaPdp5D5O4uDg9/fTTmjJlihwOhzp06KD33ntPjRo1kiTNmjVLv/3tb9WiRQvZ7XYZ1fg2JCIiQh9//LF+97vf6fLLL1dERISGDh2qZ5991vX6jh079Morr+jw4cNKSkrShAkT9Nvf/lalpaU6fPiwRowYoZ9++knx8fEaMmRInY1GifrreJAz8DQM9V5XbwAwG4tRnTO9jxUWFio2NlYFBQVuD2Cw/acf1G5RC6kkUkemHlNsrIeKBPxYcXGxdu/erWbNmiksLMzX5cDPVPX3UZvP30BXF8cm+Z7fKTd1gYZc9Igy7p1dxxUCQGBy9/O3xjcerS+sFoalBgD4pxI5Bw6JCvX+EPEAYBYBH3iCrXRpAwD4p5MWZ+CJthF4AMBTAj7wBFlOD0tNCw8AwJ+UBhF4AMDTAj7wMCw1AMBfOU4FnpgwAg8AeIqJAo9BlzYAgF9xWJ2BJzacwAMAnhLwgScoiC5tQLl6MCgjfIC/C98pCz4uSYoNr583AQaA+iDwA4+FG48CISEhkqSioiIfVwJ/VP53Uf53Au8pC3a28MRF0MIDAJ7i1o1H6yNX4JHkcBiSLL4rBvARq9WquLg45efnS3LeJNNi4d+C2RmGoaKiIuXn5ysuLk5Wq9XXJZmKYUgKdQaeBpEEHgDwFHMFnrIySZzQYU6JiYmS5Ao9QLm4uDjX3we85+RJwxV4aOEBAM8J+MBjqdCi4zAIPDAvi8WipKQkJSQk6OTJk74uB34iJCSElh0fKT55UgpySJKiwsJ9XA0ABK6ADzxnt/AA5ma1WvkPLuAHTpSc/uIhItTmw0oAILCZZ9AClV/DAwCA7xWXlLh+jrCF+rASAAhsAR94Kl6YXWbQwgMA8A9FFQJPaDCtrgDgKQEfeOjSBgDwR8UnTwWe0lBZrYyaCACeYrLAQ5c2AIB/KC6/hscRKkaJBwDPCfjAU3GUtjJaeAAAfsLVwlPGDV8BwJMCPvBUauHhGh4AgJ84UR54HAxYAACeZKrAU+og8AAA/EPJqfthWcoIPADgSaYKPIbBNTwAAP9Q3sJD4AEAzwr4wFNxWGpGaQMA+At7aXng4RoeAPCkgA88kiTDGXoIPAAAf1FMCw8AeIVJAo/zbZYxLDUAwE+UlJ66hscg8ACAJ5kk8Jxq4WGUNgCAnyjv0hZECw8AeJQ5As+pt+lglDYAgJ9wXcNjcA0PAHiSOQLPqS5ttPAAACRp9uzZ6tGjhyIiIhQXF+eTGlwtPHRpAwCPMkXgscjZpY1hqQEAklRSUqJbbrlF48eP910NDuc1PAQeAPCsYF8X4BWnWni48SgAQJKeeOIJSdKSJUt8VkOJq4WHLm0A4EmmCjxldGkDANSQ3W6X3W53PS8sLKzd9hx0aQMAbzBFl7bTgYcubQCAmpkzZ45iY2NdU0pKSq22d7K8S5sIPADgSaYIPOXX8DBKGwAErpkzZ8pisVQ5ZWVl1Xj706ZNU0FBgWvat29freotH7TASgsPAHgUXdoAAAFhwoQJGjZsWJXLpKWl1Xj7NptNNputxuuf6WTZqS5tFq7hAQBPMkfgEV3aACDQxcfHKz4+3tdlVNtJR6kkKUhWH1cCAIHNJIHnVJe2Mlp4AABSTk6Ofv75Z+Xk5MjhcGjLli2SpJYtWyoqKsorNTjKHJIkq4XAAwCeZIrAYym/8SiBBwAgafr06XrllVdczzt37ixJWrVqlXr16uWVGkpPBZ4gAg8AeJRbgxYsXLhQHTt2VExMjGJiYpSenq6PPvqoynVWr16trl27KiwsTM2bN9eiRYtqVXCNcA0PAKCCJUuWyDCMsyZvhR3p9JdwtPAAgGe5FXiaNGmip59+WllZWcrKytJ1112nm266Sd988805l9+9e7cGDRqka665Rps3b9YjjzyiSZMmKSMjo06Kr67yUdrKyriGBwDgH1wtPFzDAwAe5VaXtsGDB1d6Pnv2bC1cuFDr169X+/btz1p+0aJFSk1N1fz58yVJbdu2VVZWlubOnauhQ4fWvGq3nerSRgsPAMBPuK7hCSLwAIAn1fg+PA6HQ2+99ZaOHz+u9PT0cy6zbt069evXr9K8/v37KysrSydPnjzvtu12uwoLCytNtWGhSxsAwM84uIYHALzC7cCzbds2RUVFyWazady4cXrnnXfUrl27cy6bl5enxo0bV5rXuHFjlZaW6tChQ+fdR13fzVp0aQMA+BmHwShtAOANbgee1q1ba8uWLVq/fr3Gjx+vkSNH6ttvvz3v8haLpdJz49S9cM6cX1Fd3826fNACurQBAPwFo7QBgHe4PSx1aGioWrZsKUnq1q2bNm7cqOeff15//etfz1o2MTFReXl5lebl5+crODhYjRo1Ou8+6vpu1pbyG48yLDUAwE+UnWrhCeYaHgDwqBpfw1POMAzZ7fZzvpaenq6VK1dWmrdixQp169ZNISEhtd21G7iGBwDgX7iGBwC8w63A88gjj+izzz7Tnj17tG3bNj366KPKzMzUHXfcIcnZFW3EiBGu5ceNG6e9e/dqypQp2r59u/7+97/r5Zdf1oMPPli37+ICLAbX8AAA/AvX8ACAd7jVpe2nn37S8OHDlZubq9jYWHXs2FHLly9X3759JUm5ubnKyclxLd+sWTN9+OGHuv/++/XnP/9ZycnJWrBggZeHpJZo4QEA+BtX4KFLGwB4lFuB5+WXX67y9SVLlpw1r2fPnvryyy/dKqquWbgPDwDAz5QZDslCCw8AeFqtr+GpF+jSBgDwM2XiGh4A8AZTBB4LXdoAAH6mfJS2IBF4AMCTCDwAAPhAeQsPXdoAwLNMEXgkZ5c2B/fhAQD4CVcLD4EHADzKFIGnvIXHMLiGBwDgH7iGBwC8wxyBx6BLGwDAv5RxHx4A8ApTBB4xLDUAwM8YtPAAgFeYIvBYTl3DQ5c2AIC/YNACAPAOkwQeurQBAPwLgxYAgHeYIvC4urQxShsAwE/QwgMA3mGKwFPepY0WHgCAvzAIPADgFSYJPAxLDQDwLwxLDQDeYY7AYzBKGwDAv7haeIIIPADgSaYIPKJLGwDAz3ANDwB4hykCD13aAAD+hsADAN5hqsDDKG0AAH/BjUcBwDtMFXjo0gYA8BdcwwMA3mGSwMM1PAAA/1JG4AEArzBH4LEwShsAwL+4WnhE4AEATzJH4Cnv0sY1PAAAP2FYaOEBAG8wReAJUrAkyWE4fFwJAMDX9uzZo3vuuUfNmjVTeHi4WrRooRkzZqikpMSrddClDQC8I9jXBXhD0KnuAqVlpT6uBADgazt27FBZWZn++te/qmXLlvr66681ZswYHT9+XHPnzvVaHeVd2oIJPADgUeYIPKeG/HSU0cIDAGY3YMAADRgwwPW8efPmys7O1sKFC30SeBiWGgA8yxyBRwQeAMD5FRQUqGHDhlUuY7fbZbfbXc8LCwtrtc/ya3ho4QEAzzLFNTyW8sDDNTwAgDPs2rVLL7zwgsaNG1flcnPmzFFsbKxrSklJqdV+uQ8PAHiHKQKP1ULgAYBAN3PmTFksliqnrKysSuscOHBAAwYM0C233KLRo0dXuf1p06apoKDANe3bt69W9bpGaaNLGwB4FF3aAAABYcKECRo2bFiVy6Slpbl+PnDggHr37q309HS99NJLF9y+zWaTzWarbZkutPAAgHeYI/DQwgMAAS8+Pl7x8fHVWnb//v3q3bu3unbtqsWLFysoyPsdHgyL895wVh/sGwDMxByB51QLTxmBBwBM78CBA+rVq5dSU1M1d+5cHTx40PVaYmKiFysxJBF4AMDTzBF4LNx4FADgtGLFCn3//ff6/vvv1aRJk0qvGYbhtToMOVt4giwWr+0TAMzIFF8rMWgBAKDcqFGjZBjGOSevstDCAwDeYIpP2fJreOjSBgDwBxXDFYEHADzLFJ+yp0dpK/VxJQAASGVGmetnC13aAMCjTBF4XF3aRAsPAMD3DJ1u4QmmhQcAPMqtT9k5c+bo8ssvV3R0tBISEnTzzTcrOzu7ynUyMzPPefO3HTt21Kpwd9ClDQDgTyq28AQF0cIDAJ7kVuBZvXq17rvvPq1fv14rV65UaWmp+vXrp+PHj19w3ezsbOXm5rqmSy65pMZFu4vAAwDwJ1zDAwDe49aw1MuXL6/0fPHixUpISNCmTZt07bXXVrluQkKC4uLi3C6wLjBKGwDAn1Rq4eEaHgDwqFp9rVRQUCBJatiw4QWX7dy5s5KSktSnTx+tWrWqymXtdrsKCwsrTbVhpYUHAOBHKgYeq5UWHgDwpBp/yhqGoSlTpujqq6/WpZdeet7lkpKS9NJLLykjI0PLli1T69at1adPH61Zs+a868yZM0exsbGuKSUlpaZlSpKsp248WsagBQAAP1Bx0AK6tAGAZ7nVpa2iCRMmaOvWrfr888+rXK5169Zq3bq163l6err27dunuXPnnrcb3LRp0zRlyhTX88LCwlqFHmsQLTwAAP9BlzYA8J4afa00ceJE/ec//9GqVavUpEkTt9fv3r27du7ced7XbTabYmJiKk214Rq0gBYeAIAfYNACAPAet1p4DMPQxIkT9c477ygzM1PNmjWr0U43b96spKSkGq1bE1aLVTJo4QEA+IfK1/DQwgMAnuRW4Lnvvvv0j3/8Q//+978VHR2tvLw8SVJsbKzCw8MlObuj7d+/X6+++qokaf78+UpLS1P79u1VUlKi119/XRkZGcrIyKjjt3J+1iCr5KCFBwDgHyoFHlp4AMCj3Ao8CxculCT16tWr0vzFixdr1KhRkqTc3Fzl5OS4XispKdGDDz6o/fv3Kzw8XO3bt9cHH3ygQYMG1a5yN5wepa3Ua/sEAOB8Kg1aYCHwAIAnud2l7UKWLFlS6fnUqVM1depUt4qqa65BC2jhAQD4Abq0AYD3mOJrJQIPAMCfuL5ANCyiRxsAeJYpPmaDuQ8PAMCPuFp4DIsYlRoAPMsUgaf8Gh6DUdoAAH7gdOAJooUHADzMFB+zVitd2gAA/sM1aAGBBwA8zhQfs8HlLTwEHgCAHzg9aAHX8ACAp5niY5ZBCwAA/uT0oAVBXMMDAB5misATHEQLDwDAf1QctIAWHgDwLFN8zNLCAwDwJwxaAADeY4qPWSstPAAAP1Jx0AK6tAGAZ5ki8JR3aSuzlPq4EgAAGLQAALzJFB+zwUHOG4/SwgMA8AcVBy0g8ACAZ5niYzbYSpc2AID/qDhoAV3aAMCzzBF4yq/hsRB4AAC+x41HAcB7TPExy6AFAAB/wihtAOA9pviYDbWGSJLKLCd9XAkAAAxaAADeZIqP2ZBTgccg8AAAJN14441KTU1VWFiYkpKSNHz4cB04cMBr+684aAHX8ACAZ5kj8ASdCjxBBB4AgNS7d2+9/fbbys7OVkZGhnbt2qVf//rXXtt/xUELaOEBAM8K9nUB3mALDpUklVlKfFwJAMAf3H///a6fmzZtqocfflg333yzTp48qZCQEI/vn0ELAMB7TBF4XF3aaOEBAJzh559/1htvvKEePXpUGXbsdrvsdrvreWFhYY33WXHQArq0AYBnmeJ7pVCu4QEAnOGhhx5SZGSkGjVqpJycHP373/+ucvk5c+YoNjbWNaWkpNR43wxaAADeY4qP2dDgU9/YBTlOXygKAAgoM2fOlMViqXLKyspyLf/73/9emzdv1ooVK2S1WjVixIgqzxHTpk1TQUGBa9q3b1+Na604aAGBBwA8yxRd2spbeCTpZNlJhVpDfVgNAMATJkyYoGHDhlW5TFpamuvn+Ph4xcfHq1WrVmrbtq1SUlK0fv16paenn3Ndm80mm81WJ7UyaAEAeI9JAs/pgFPiKCHwAEAAKg8wNVHe4lLxGh1PqjhoAdfwAIBnmSLw2IIrtPA4uI4HAMzsiy++0BdffKGrr75aDRo00A8//KDp06erRYsW523dqWsVBy2ghQcAPMsUH7Mh1tO57mQZgQcAzCw8PFzLli1Tnz591Lp1a91999269NJLtXr16jrrsnYhDFoAAN5jihaekBCL5AiWrKW08ACAyXXo0EGffvqpT2uoOGgBXdoAwLNM8b1ScLAkh/O6nRIHNx8FAPhWxUELCDwA4FnmCTxlzut46NIGAPA1Bi0AAO8xT+BxnAo8dGkDAPhYxUELCDwA4FnmCTy08AAA/ETFQQsIPADgWeYJPLTwAAD8BIMWAID3mCjwMGgBAMA/MGgBAHiPeQIPXdoAAH6CQQsAwHvME3hOdWkrKSXwAAB8y1F2etACAIBnufVJO2fOHF1++eWKjo5WQkKCbr75ZmVnZ19wvdWrV6tr164KCwtT8+bNtWjRohoXXBMVW3jsBB4AgI+5Ag+DFgCAx7kVeFavXq377rtP69ev18qVK1VaWqp+/frp+PHj511n9+7dGjRokK655hpt3rxZjzzyiCZNmqSMjIxaF19dFVt4ik8SeAAAvsWgBQDgPcHuLLx8+fJKzxcvXqyEhARt2rRJ11577TnXWbRokVJTUzV//nxJUtu2bZWVlaW5c+dq6NChNavaTRUHLSguYdACAIBvne7SRgsPAHharToPFxQUSJIaNmx43mXWrVunfv36VZrXv39/ZWVl6aSXWlsqBp6ik3av7BMAgPMpo4UHALzGrRaeigzD0JQpU3T11Vfr0ksvPe9yeXl5aty4caV5jRs3VmlpqQ4dOqSkpKSz1rHb7bLbTweTwsLCmpYp6VTgKQ2TJJ0oIfAAAHzr9LDUBB4A8LQat/BMmDBBW7du1ZtvvnnBZS1nfJqX910+c365OXPmKDY21jWlpKTUtExJUlCQpNJwSVJRyYlabQsAgNpi0AIA8J4aBZ6JEyfqP//5j1atWqUmTZpUuWxiYqLy8vIqzcvPz1dwcLAaNWp0znWmTZumgoIC17Rv376alFmJxeFs4Sk6WVzrbQEAUBt0aQMA73GrS5thGJo4caLeeecdZWZmqlmzZhdcJz09Xe+9916leStWrFC3bt0UEhJyznVsNptsNps7pV1QUFmYHJJOlBB4AAC+VcagBQDgNW618Nx33316/fXX9Y9//EPR0dHKy8tTXl6eTpw43U1s2rRpGjFihOv5uHHjtHfvXk2ZMkXbt2/X3//+d7388st68MEH6+5dVENQmbNL24mTdGkDAPgWLTwA4D1uBZ6FCxeqoKBAvXr1UlJSkmtaunSpa5nc3Fzl5OS4njdr1kwffvihMjMzddlll+nJJ5/UggULvDYkdbmgslODFtClDQDgYwxaAADe43aXtgtZsmTJWfN69uypL7/80p1d1TmrcSrwlBJ4AAC+xaAFAOA9tboPT31iPdWlrbiULm0AAN8yRJc2APAW8wSeUy08xbTwAAB8rOKgBQAAzzJf4HEQeAAAvsWgBQDgPaYJPMEGXdoAAP6BQQsAwHvME3jkbOGx08IDAPAxBi0AAO8h8AAA4GUGXdoAwGtMFHicXdrsDrq0AQB8y2GcHrSAwAMAnmWawBOiSEnSCcdxH1cCADA7WngAwHtME3hCjWhJUpGj0MeVAADMznUND4EHADzONIHHphhJ0gnHUR9XAgAwO9cobQxaAAAeZ6LA42zhKTFOqLSs1MfVAADMjPvwAID3mCbwhFujXT8ftdPKAwCQ7Ha7LrvsMlksFm3ZssVr+y0rOz1oAQDAs0wTeMJCQqVSmySp0M51PAAAaerUqUpOTvb6fst0uoUHAOBZpvmktdkk2Z3X8RwtoYUHAMzuo48+0ooVKzR37lyv79twXcNjmtMwAPhMsK8L8JbQUEn2aCnyIC08AGByP/30k8aMGaN3331XERER1VrHbrfLbre7nhcW1vxc4qBLGwB4jWm+WnIGnlMtPFzDAwCmZRiGRo0apXHjxqlbt27VXm/OnDmKjY11TSkpKTWu4dZWd0kLv1LQ6lk13gYAoHpME3gqdmmjhQcAAs/MmTNlsViqnLKysvTCCy+osLBQ06ZNc2v706ZNU0FBgWvat29fjWttGHaR9FNHBR1NrfE2AADVY64ubUedI7VxDQ8ABJ4JEyZo2LBhVS6Tlpamp556SuvXr5fNZqv0Wrdu3XTHHXfolVdeOee6NpvtrHVqqnxUaoakBgDPM1fgoYUHAAJWfHy84uPjL7jcggUL9NRTT7meHzhwQP3799fSpUt15ZVXerJEFwIPAHiPaQKPzSapOE6S9MuJX3xaCwDAd1JTK3cji4qKkiS1aNFCTZo08UoNBB4A8B7TXMMTGirp+EWSpINFB31bDADA1Ag8AOA9pmnhCQ2VVOTs6nCo6JBviwEA+I20tDQZ5QnESwg8AOA9pmnhsdnkCjy08AAAfInAAwDeY5rA42zhcXZpo4UHAOBLBB4A8B6TBR66tAEAfI/AAwDeY67Ac/x0C4+3+2sDAFCOwAMA3mOawGOzSTrRSJJUWlaqAnuBbwsCAJgegQcAPM80gSc0VFJpmKylzvstHDzOwAUAAN+gkwEAeI+5Ao+k4BLuxQMA8C26tAGA95gm8NhszkdrcYIkWngAAL5D4AEA7zFN4AkLcz4GFSVKkvKO5fmwGgCAmRF4AMB7TBN4IiKcj8ZRZ+DJPZbrw2oAAGZG4AEA7zFd4HEU0sIDAPAtAg8AeI9pAk9kpPOx9BcCDwDAtwg8AOA9pgk85S08pUcIPAAA3yLwAID3uB141qxZo8GDBys5OVkWi0XvvvtulctnZmbKYrGcNe3YsaOmNddIeeDRMQIPAMC3CDwA4D3B7q5w/PhxderUSXfddZeGDh1a7fWys7MVExPjen7RRRe5u+taCQ8/9UOFwGMYhiycbQAAXkbgAQDvcTvwDBw4UAMHDnR7RwkJCYqLi3N7vboSFOQMPSdOBR67w64Ce4HiwnxXEwDAnAg8AOA9XruGp3PnzkpKSlKfPn20atWqKpe12+0qLCysNNWFiAhJpWGKCYmTJOUeZWhqAID3EXgAwHs8HniSkpL00ksvKSMjQ8uWLVPr1q3Vp08frVmz5rzrzJkzR7Gxsa4pJSWlTmopH6mtYSjX8QAAfI/AAwCe53aXNne1bt1arVu3dj1PT0/Xvn37NHfuXF177bXnXGfatGmaMmWK63lhYWGdhJ7ygQviQhIl7SDwAAB8oryFBwDgeT4Zlrp79+7auXPneV+32WyKiYmpNNWF8sATG0QLDwDAd+jSBgDe45PAs3nzZiUlJXl9v+WBJ0oEHgCA7xB4AMB73O7SduzYMX3//feu57t379aWLVvUsGFDpaamatq0adq/f79effVVSdL8+fOVlpam9u3bq6SkRK+//royMjKUkZFRd++imsqv4Yk0TgWe4wQeAID3EXgAwHvcDjxZWVnq3bu363n5tTYjR47UkiVLlJubq5ycHNfrJSUlevDBB7V//36Fh4erffv2+uCDDzRo0KA6KN895S08YQ5aeAAAvkPgAQDvcTvw9OrVS0YVV1suWbKk0vOpU6dq6tSpbhfmCeWBx1biDDwMSw0A8AUCDwB4j0+u4fGV8rEPLEW08AAAfIfAAwDeY6rAExfnfCwrcA6YcKjokE46TvquIACAKRF4AMB7TBl47L/EKyQoRIYMHTh6wKc1AQDMh8ADAN5jysBTcCRIKbHOG5nmFOScfwUAADyAwAMA3mPKwHPkiJQamyqJwAMA8D4CDwB4D4GHwAMAppOWliaLxVJpevjhh722fwIPAHiP28NS12eVAk+MM/DsK9zns3oAAL4za9YsjRkzxvU8KirK6zUQeADA80wVeBo0cD7SwgMAiI6OVmJiok/2XcXt7AAAdcyUXdoKC6WLowk8AGBmf/zjH9WoUSNddtllmj17tkpKSqpc3m63q7CwsNJUU3RpAwDvMVULT2zs6Z8bWgk8AGBWv/vd79SlSxc1aNBAX3zxhaZNm6bdu3frb3/723nXmTNnjp544ok62T+BBwC8x1QtPKGhUnkX7TC7c1jqAnuBCooLfFgVAKAuzJw586yBCM6csrKyJEn333+/evbsqY4dO2r06NFatGiRXn75ZR0+fPi82582bZoKCgpc0759Nb8GlMADAN5jqhYeSUpIkI4dk4qORKlheEP9fOJn7TmyR50SO/m6NABALUyYMEHDhg2rcpm0tLRzzu/evbsk6fvvv1ejRo3OuYzNZpPNZqtVjeUIPADgPaYMPD/8IOXnSy0bttQX+7/Q9z9/T+ABgHouPj5e8fHxNVp38+bNkqSkpKS6LOm8CDwA4D2mDDySM/BcEn+Jvtj/hXb+vNO3RQEAvGbdunVav369evfurdjYWG3cuFH333+/brzxRqWmpnqlBgIPAHiPuQNPq0skSTsPE3gAwCxsNpuWLl2qJ554Qna7XU2bNtWYMWM0depUr9VA4AEA7zF14One6FTgoYUHAEyjS5cuWr9+vU9rIPAAgPeYapQ26YwWnoYEHgCA9xF4AMB7zB14TrXw5B3L01H7UR9WBQAwEwIPAHiPqQNPXFicGkc2liR9e/BbH1YFADATAg8AeI+pA48kdWzcUZK09aetPqoIAGBWBB4A8DzTBp5DhySHg8ADAPC+8hYeAIDnmS7wNGrk/EbNMKTDh6VOjZ03HN2aT+ABAHgHXdoAwHtMF3iCg52hR3J2a6vYwmPwlRsAwAsIPADgPaYLPJLU2DlOgXJzpTbxbRQSFKIjxUe0+8hu3xYGADAFAg8AeI8pA8/FFzsf9++XbME2dUnqIklat2+dD6sCAJgFgQcAvMf0gUeS0pukS5LW7lvro4oAAGZC4AEA7zFl4ElOdj4eOOB87JHSQ5K07kdaeAAAnkfgAQDvMWXgOauFJ8XZwvPVT1+poLjAR1UBAMyCwAMA3hPs6wJ84czA0ySmiVo3aq3sw9n65IdPNLTdUN8VBwAIeAQewPccDodOnjzp6zJwDiEhIbJarXW2PQLPKQNbDlT24Wx9uPNDAg8AwKMIPIDvGIahvLw8HTlyxNeloApxcXFKTEyUpQ4+KE0deH76SSotdd6bZ9AlgzR/w3x99P1HKjPKFGQxZW8/AIAXEHgA3ykPOwkJCYqIiKiT/1Cj7hiGoaKiIuXn50uSkpKSar1NUwaehARnyCktlfLypCZNpGubXqu4sDjlHsvVqt2r1Kd5H1+XCQAIUAQewDccDocr7DQqvxM9/E54eLgkKT8/XwkJCbXu3mbKZoygoNOtPDk5zkdbsE3D2g+TJC35aolvCgMAmAqBB/Cu8mt2IiIifFwJLqT8d1QX11mZMvBIUvPmzsddu07PG3XZKElSxrcZOlJ8xOs1AQDMobyFB4Bv0I3N/9Xl78jtwLNmzRoNHjxYycnJslgsevfddy+4zurVq9W1a1eFhYWpefPmWrRoUU1qrVMtWjgfKwaeKy6+Qh0SOuhE6Qkt3LjQN4UBAAIeXdoAwHvcDjzHjx9Xp06d9OKLL1Zr+d27d2vQoEG65pprtHnzZj3yyCOaNGmSMjIy3C62LrVs6Xz8/vvT8ywWi6ZeNVWSNH/DfJ04ecIHlQEAAh2BB4A/6NWrlyZPnuzrMjzO7cAzcOBAPfXUUxoyZEi1ll+0aJFSU1M1f/58tW3bVqNHj9bdd9+tuXPnul1sXTpXC48kDbt0mNLi0pR/PF8vflG9UAcAgDsIPADcYbFYqpxGjRpVo+0uW7ZMTz75ZK1qGzVqlKuO4OBgpaamavz48frll19cy/z888+aOHGiWrdurYiICKWmpmrSpEkqKCio1b6ry+PX8Kxbt079+vWrNK9///7Kysry6c2eygNPxRYeSQoOCtaMnjMkSU+sfkI/Fv7o5coAAIGOwAPAHbm5ua5p/vz5iomJqTTv+eefr7R8df+P3bBhQ0VHR9e6vgEDBig3N1d79uzR3/72N7333nu69957Xa8fOHBABw4c0Ny5c7Vt2zYtWbJEy5cv1z333FPrfVeHxwNPXl6eGjduXGle48aNVVpaqkOHDp1zHbvdrsLCwkpTXSsPPIcOSWdufkSnEeqR0kPHTx7X7Rm3q8RRUuf7BwCYF4EH8B+GIR0/7pupugOYJCYmuqbY2FhZLBbX8+LiYsXFxentt99Wr169FBYWptdff12HDx/WbbfdpiZNmigiIkIdOnTQm2++WWm7Z3ZpS0tL0x/+8Afdfffdio6OVmpqql566aUL1mez2ZSYmKgmTZqoX79+uvXWW7VixQrX65deeqkyMjI0ePBgtWjRQtddd51mz56t9957T6WlpdU7CLXglVHazhxlwTj12z3f6Atz5sxRbGysa0pJSanzmmJipPIctmNH5deCLEH6+41/V4wtRp/lfKYR74wg9AAA6gyBB/AfRUVSVJRvpqKiunsfDz30kCZNmqTt27erf//+Ki4uVteuXfX+++/r66+/1tixYzV8+HBt2LChyu3MmzdP3bp10+bNm3Xvvfdq/Pjx2nHmf5ar8MMPP2j58uUKCQmpcrmCggLFxMQoONjztwX1eOBJTExUXl5epXn5+fkKDg4+7w2fpk2bpoKCAte0b98+j9TWoYPzcdu2s19rHd9abw59U8FBwVr6zVINfGOgDhw94JE6AADmQuABUNcmT56sIUOGqFmzZkpOTtbFF1+sBx98UJdddpmaN2+uiRMnqn///vrnP/9Z5XYGDRqke++9Vy1bttRDDz2k+Ph4ZWZmVrnO+++/r6ioKIWHh6tFixb69ttv9dBDD513+cOHD+vJJ5/Ub3/725q8Vbd5PFKlp6frvffeqzRvxYoV6tat23mTn81mk81m83Rp6thR+uQTaevWc78+6JJBeu+29zT07aH6dPenav+X9nog/QGN7jJaiVGJHq8PABCYCDyA/4iIkI4d892+60q3bt0qPXc4HHr66ae1dOlS7d+/X3a7XXa7XZGRkVVup2PHjq6fy7vO5efnV7lO7969tXDhQhUVFelvf/ubvvvuO02cOPGcyxYWFur//u//1K5dO82YMaOa76523A48x44d0/cVrvTfvXu3tmzZooYNGyo1NVXTpk3T/v379eqrr0qSxo0bpxdffFFTpkzRmDFjtG7dOr388stn9SH0hfLf5/kCjyQNaDlAWWOydOc7d+rL3C/1+KrHNX3VdHVv0l09UnqoQ0IHpcSmKCkqSZGhkYoIiVBYcJgsssiQ84xW3oWvzCir1STprG2WPz/XPF8sU99VfI/1Gb+PuufpY+rp95reJF2RoVWf5OA9BB7Af1gs0gUyQL1wZpCZN2+ennvuOc2fP18dOnRQZGSkJk+erJKSqi/TOLNBwmKxqKys7IL7bnnqni8LFixQ79699cQTT5w1AtzRo0c1YMAARUVF6Z133rlgt7e64nbgycrKUu/evV3Pp0yZIkkaOXKklixZotzcXOXk5Lheb9asmT788EPdf//9+vOf/6zk5GQtWLBAQ4cOrYPya6di4DGM85942l7UVl+M/kJLv1mqBRsWaMP+DVr34zqt+3Gd94oFgFrInpCtVo1a+boMnELgAeBpn332mW666SbdeeedkqSysjLt3LlTbdu29fi+Z8yYoYEDB2r8+PFKTk6W5GzZ6d+/v2w2m/7zn/8oLCzM43WUczvw9OrVq8pvOpcsWXLWvJ49e+rLL790d1ce17atZLVKP/8s7dsnpaaef1lrkFW3d7hdt3e4XXuP7NXqvau1/sf12vnzTu0r2Kf84/kqOlkku8PuVg1BlqBqTRZZXIM8WHTqsZrPa7JO+fOarlOfnW8wjfrGjL8PwzD86vfnT7+DUGuor0tABQQeAJ7WsmVLZWRkaO3atWrQoIGeffZZ5eXleSXw9OrVS+3bt9cf/vAHvfjiizp69Kj69eunoqIivf7665VGYb7oootktVo9Wo/nh0XwY2FhUqdO0pdfSuvWVR14Kmoa11Qj4kZoRKcRZ73mKHOouLTY9bxiGLBYLLJarKdDDGc6ADA1TgMAPOXxxx/X7t271b9/f0VERGjs2LG6+eabvXazzylTpuiuu+7SQw89pF27drlGhyvv+lZu9+7dSktL82gtFqMedPYvLCxUbGysa/i6ujRxovTii9KkSdIZ92wCANPz5Oevr33wwQeaNWuWtm7dqsjISF177bVatmxZtdevzbFZvFi6+25p0CDpgw/crRxATRUXF2v37t1q1qyZV7tUwX1V/a7c/fz1yn14/NlVVzkf1671bR0AAO/JyMjQ8OHDddddd+mrr77S//73P91+++1e2z9d2gDAe0zdpU2SevRwPm7e7LzjbSCM0gEAOL/S0lL97ne/0zPPPKN77rnHNb9169Zeq4HAAwDeY/oWnpQU6eKLJYeDVh4AMIMvv/xS+/fvV1BQkDp37qykpCQNHDhQ33zzTZXr2e1214W2FS+4rQkCDwB4j+kDj8Ui9e3r/Hn5ct/WAgDwvB9++EGSNHPmTD322GN6//331aBBA/Xs2VM///zzedebM2eOYmNjXVNKSkqNayDwAID3mD7wSM6LRiXpo498WwcAoOZmzpwpi8VS5ZSVleW6gd6jjz6qoUOHqmvXrlq8eLEsFov++c9/nnf706ZNU0FBgWvat29fjWsl8ACA95j+Gh7J2cJjtUrbt0t79kgeHhkPAOABEyZM0LBhw6pcJi0tTUePHpUktWvXzjXfZrOpefPmlW6cfSabzSabzVYntRJ4AMB7CDyS4uKcgxd89pm0bJk0ZYqvKwIAuCs+Pl7x8fEXXK5r166y2WzKzs7W1VdfLUk6efKk9uzZo6ZNm3q6TEkEHgDwJrq0nXLbbc7H117zbR0AAM+KiYnRuHHjNGPGDK1YsULZ2dkaP368JOmWW27xSg0EHgDwHlp4Trn1Vul3v5O2bJG2bpU6dvR1RQAAT3nmmWcUHBys4cOH68SJE7ryyiv16aefqkGDBl7ZP4EHALyHFp5TGjaUBg92/rxokW9rAQB4VkhIiObOnauffvpJhYWFWrlypdq3b++1/RN4AMB7CDwVTJzofFy8WMrP920tAIDAReAB4I4LjUA5atSoGm87LS1N8+fPr9Zy5fsLDw9XmzZt9Mwzz8go/0CT9NVXX+m2225TSkqKwsPD1bZtWz3//PM1rq2u0KWtgp49pcsvlzZulObNk/74R19XBAAIZAQeANWRm5vr+nnp0qWaPn26srOzXfPCw8O9UsesWbM0ZswYFRcX65NPPtH48eMVExOj3/72t5KkTZs26aKLLtLrr7+ulJQUrV27VmPHjpXVatWECRO8UuO50MJTgcUiPf648+f586Vdu3xaDgAgQFX4QhSAjxmGoeMlx30yGdX8MEhMTHRNsbGxslgsleatWbNGXbt2VVhYmJo3b64nnnhCpaWlrvVnzpyp1NRU2Ww2JScna9KkSZKkXr16ae/evbr//vtdrTdViY6OVmJiotLS0jR69Gh17NhRK1ascL1+9913a8GCBerZs6eaN2+uO++8U3fddZeWLVtWg99M3aGF5ww33OC8L8/KldL48dLy5VIQsRAAUIfo0gb4j6KTRYqaE+WTfR+bdkyRoZG12sbHH3+sO++8UwsWLNA111yjXbt2aezYsZKkGTNm6F//+peee+45vfXWW2rfvr3y8vL01VdfSZKWLVumTp06aezYsRozZky192kYhlavXq3t27frkksuqXLZgoICNWzYsOZvsA7wX/kzWCzSggVSeLgz9NCtDQBQ1wg8AOrK7Nmz9fDDD2vkyJFq3ry5+vbtqyeffFJ//etfJUk5OTlKTEzU9ddfr9TUVF1xxRWucNOwYUNZrVZXy01iYmKV+3rooYcUFRUlm82m3r17yzAMV2vRuaxbt05vv/22q8ubr9DCcw5t2jhDz5gx0iOPSImJ0l13+boqAECgIPAA/iMiJELHph3z2b5ra9OmTdq4caNmz57tmudwOFRcXKyioiLdcsstmj9/vpo3b64BAwZo0KBBGjx4sIKD3Y8Bv//97zVq1CgdPHhQjz76qK677jr16NHjnMt+8803uummmzR9+nT17du3xu+vLhB4zuOee6Tt26Vnn5XuvlvKyZEee0yyWn1dGQCgviPwAP7DYrHUuluZL5WVlemJJ57QkCFDznotLCxMKSkpys7O1sqVK/XJJ5/o3nvv1TPPPKPVq1crJCTErX3Fx8erZcuWatmypTIyMtSyZUt1795d119/faXlvv32W1133XUaM2aMHnvssVq9v7pAl7bzsFikuXOlyZOdz2fOlK68Uvrvf7nYFABQOwQeAHWlS5cuys7OdgWRilPQqQvRw8PDdeONN2rBggXKzMzUunXrtG3bNklSaGioHA6H2/tt0KCBJk6cqAcffLDS4AvffPONevfurZEjR1ZqdfIlAk8VLBbpueekJUukmBhp0ybp+uul9u2drT2rV0tHjvi6SgBAfUPgAVBXpk+frldffVUzZ87UN998o+3bt2vp0qWulpUlS5bo5Zdf1tdff60ffvhBr732msLDw9W0aVNJzvvrrFmzRvv379ehQ4fc2vd9992n7OxsZWRkSDoddvr27aspU6YoLy9PeXl5OnjwYN2+aTfRpa0aRo6UBgyQZs2SXnnF2dVt9mznJEnJydLFF0tJSdJFF0mRkVJExOkpJMQ50ltQkLNL3Ll+tljOfeKr7jx/WRaAf+rbV4qO9nUVKEfgAVBX+vfvr/fff1+zZs3Sn/70J4WEhKhNmzYaPXq0JCkuLk5PP/20pkyZIofDoQ4dOui9995To0aNJDnvrfPb3/5WLVq0kN1ur/ZQ2ZJ00UUXafjw4Zo5c6aGDBmif/7znzp48KDeeOMNvfHGG67lmjZtqj179tTp+3aHxXDnXflIYWGhYmNjVVBQoJiYGJ/WUlAgvfuu9NFH0vr10t69Pi0HAKolO1tq1cr99fzp89ff1ObYPPOMNHWqNGKE84s0AN5RXFys3bt3q1mzZgoLC/N1OahCVb8rdz9/aeFxU2yss8Vn5Ejn84IC538kcnOlvDzp4EHpxAmpqOj0VFoqORxSWdnp6cznZWVn7+tcUfR88dQflgXgv7x0E25U08UXS1ddVbMQCgBwD4GnlmJjpSuu8HUVAID65PbbnRMAwPMYtAAAAABAwCLwAAAAAAhYBB4AAACYSj0Ys8v06vJ3ROABAACAKYSEhEiSioqKfFwJLqT8d1T+O6sNBi0AAACAKVitVsXFxSk/P1+SFBERIQs3xPIrhmGoqKhI+fn5iouLk9VqrfU2CTwAAAAwjcTERElyhR74p7i4ONfvqrYIPAAAADANi8WipKQkJSQk6OTJk74uB+cQEhJSJy075Qg8AAAAMB2r1Vqn/6mG/2LQAgAAAAABi8ADAAAAIGAReAAAAAAErHpxDU/5jYcKCwt9XAkAmEv55y436Tsb5yYA8A13z031IvAcPXpUkpSSkuLjSgDAnI4eParY2Fhfl+FXODcBgG9V99xkMerB13ZlZWU6cOCAoqOja3RzqMLCQqWkpGjfvn2KiYnxQIWBjeNXOxy/2uH41U5tj59hGDp69KiSk5MVFEQv6Io4N/kWx692OH61w/GrHW+fm+pFC09QUJCaNGlS6+3ExMTwR1kLHL/a4fjVDsevdmpz/GjZOTfOTf6B41c7HL/a4fjVjrfOTXxdBwAAACBgEXgAAAAABCxTBB6bzaYZM2bIZrP5upR6ieNXOxy/2uH41Q7Hz3/xu6kdjl/tcPxqh+NXO94+fvVi0AIAAAAAqAlTtPAAAAAAMCcCDwAAAICAReABAAAAELAIPAAAAAACVsAHnr/85S9q1qyZwsLC1LVrV3322We+LskvzJw5UxaLpdKUmJjoet0wDM2cOVPJyckKDw9Xr1699M0331Taht1u18SJExUfH6/IyEjdeOON+vHHH739VrxizZo1Gjx4sJKTk2WxWPTuu+9Wer2ujtcvv/yi4cOHKzY2VrGxsRo+fLiOHDni4XfneRc6fqNGjTrr77F79+6VljHz8ZszZ44uv/xyRUdHKyEhQTfffLOys7MrLcPfYP3CuelsnJfcx7mpdjg31Vx9Oy8FdOBZunSpJk+erEcffVSbN2/WNddco4EDByonJ8fXpfmF9u3bKzc31zVt27bN9dqf/vQnPfvss3rxxRe1ceNGJSYmqm/fvjp69KhrmcmTJ+udd97RW2+9pc8//1zHjh3TDTfcIIfD4Yu341HHjx9Xp06d9OKLL57z9bo6Xrfffru2bNmi5cuXa/ny5dqyZYuGDx/u8ffnaRc6fpI0YMCASn+PH374YaXXzXz8Vq9erfvuu0/r16/XypUrVVpaqn79+un48eOuZfgbrD84N50f5yX3cG6qHc5NNVfvzktGALviiiuMcePGVZrXpk0b4+GHH/ZRRf5jxowZRqdOnc75WllZmZGYmGg8/fTTrnnFxcVGbGyssWjRIsMwDOPIkSNGSEiI8dZbb7mW2b9/vxEUFGQsX77co7X7miTjnXfecT2vq+P17bffGpKM9evXu5ZZt26dIcnYsWOHh9+V95x5/AzDMEaOHGncdNNN512H41dZfn6+IclYvXq1YRj8DdY3nJvOjfNS7XBuqh3OTbXj7+elgG3hKSkp0aZNm9SvX79K8/v166e1a9f6qCr/snPnTiUnJ6tZs2YaNmyYfvjhB0nS7t27lZeXV+nY2Ww29ezZ03XsNm3apJMnT1ZaJjk5WZdeeqnpjm9dHa9169YpNjZWV155pWuZ7t27KzY21hTHNDMzUwkJCWrVqpXGjBmj/Px812scv8oKCgokSQ0bNpTE32B9wrmpapyX6g6fC3WDc1P1+Pt5KWADz6FDh+RwONS4ceNK8xs3bqy8vDwfVeU/rrzySr366qv6+OOP9f/+3/9TXl6eevToocOHD7uOT1XHLi8vT6GhoWrQoMF5lzGLujpeeXl5SkhIOGv7CQkJAX9MBw4cqDfeeEOffvqp5s2bp40bN+q6666T3W6XxPGryDAMTZkyRVdffbUuvfRSSfwN1iecm86P81Ld4nOh9jg3VU99OC8FV//t1E8Wi6XSc8MwzppnRgMHDnT93KFDB6Wnp6tFixZ65ZVXXBfk1eTYmfn41sXxOtfyZjimt956q+vnSy+9VN26dVPTpk31wQcfaMiQIeddz4zHb8KECdq6das+//zzs17jb7D+4Nx0Ns5LnsHnQs1xbqqe+nBeCtgWnvj4eFmt1rPSX35+/llpE1JkZKQ6dOignTt3ukbFqerYJSYmqqSkRL/88st5lzGLujpeiYmJ+umnn87a/sGDB013TJOSktS0aVPt3LlTEsev3MSJE/Wf//xHq1atUpMmTVzz+RusPzg3VR/npdrhc6HucW46W305LwVs4AkNDVXXrl21cuXKSvNXrlypHj16+Kgq/2W327V9+3YlJSWpWbNmSkxMrHTsSkpKtHr1atex69q1q0JCQiotk5ubq6+//tp0x7eujld6eroKCgr0xRdfuJbZsGGDCgoKTHdMDx8+rH379ikpKUkSx88wDE2YMEHLli3Tp59+qmbNmlV6nb/B+oNzU/VxXqodPhfqHuem0+rdeanawxvUQ2+99ZYREhJivPzyy8a3335rTJ482YiMjDT27Nnj69J87oEHHjAyMzONH374wVi/fr1xww03GNHR0a5j8/TTTxuxsbHGsmXLjG3bthm33XabkZSUZBQWFrq2MW7cOKNJkybGJ598Ynz55ZfGddddZ3Tq1MkoLS311dvymKNHjxqbN282Nm/ebEgynn32WWPz5s3G3r17DcOou+M1YMAAo2PHjsa6deuMdevWGR06dDBuuOEGr7/fulbV8Tt69KjxwAMPGGvXrjV2795trFq1ykhPTzcuvvhijt8p48ePN2JjY43MzEwjNzfXNRUVFbmW4W+w/uDcdG6cl9zHual2ODfVXH07LwV04DEMw/jzn/9sNG3a1AgNDTW6dOniGi7P7G699VYjKSnJCAkJMZKTk40hQ4YY33zzjev1srIyY8aMGUZiYqJhs9mMa6+91ti2bVulbZw4ccKYMGGC0bBhQyM8PNy44YYbjJycHG+/Fa9YtWqVIemsaeTIkYZh1N3xOnz4sHHHHXcY0dHRRnR0tHHHHXcYv/zyi5fepedUdfyKioqMfv36GRdddJEREhJipKamGiNHjjzr2Jj5+J3r2EkyFi9e7FqGv8H6hXPT2TgvuY9zU+1wbqq5+nZespwqGgAAAAACTsBewwMAAAAABB4AAAAAAYvAAwAAACBgEXgAAAAABCwCDwAAAICAReABAAAAELAIPAAAAAACFoEHOI+0tDTNnz/f12XU2pIlSxQXF+frMgAAdYBzE+C+YF8XANSVXr166bLLLquzE8HGjRsVGRlZJ9sCAJgT5ybA9wg8MBXDMORwOBQcfOE//YsuusgLFQEAzI5zE+BZdGlDQBg1apRWr16t559/XhaLRRaLRXv27FFmZqYsFos+/vhjdevWTTabTZ999pl27dqlm266SY0bN1ZUVJQuv/xyffLJJ5W2eWa3AYvFor/97W/61a9+pYiICF1yySX6z3/+U2VdJSUlmjp1qi6++GJFRkbqyiuvVGZmpuv18ib9d999V61atVJYWJj69u2rffv2VdrOwoUL1aJFC4WGhqp169Z67bXXKr1+5MgRjR07Vo0bN1ZYWJguvfRSvf/++5WW+fjjj9W2bVtFRUVpwIABys3NdeMIAwDcxbmJcxP8hAEEgCNHjhjp6enGmDFjjNzcXCM3N9coLS01Vq1aZUgyOnbsaKxYscL4/vvvjUOHDhlbtmwxFi1aZGzdutX47rvvjEcffdQICwsz9u7d69pm06ZNjeeee871XJLRpEkT4x//+Iexc+dOY9KkSUZUVJRx+PDh89Z1++23Gz169DDWrFljfP/998Yzzzxj2Gw247vvvjMMwzAWL15shISEGN26dTPWrl1rZGVlGVdccYXRo0cP1zaWLVtmhISEGH/+85+N7OxsY968eYbVajU+/fRTwzAMw+FwGN27dzfat29vrFixwti1a5fx3nvvGR9++GGlfVx//fXGxo0bjU2bNhlt27Y1br/99rr8FQAAzsC5iXMT/AOBBwGjZ8+exu9+97tK88pPKu++++4F12/Xrp3xwgsvuJ6f66Ty2GOPuZ4fO3bMsFgsxkcffXTO7X3//feGxWIx9u/fX2l+nz59jGnTphmG4fzAl2SsX7/e9fr27dsNScaGDRsMwzCMHj16GGPGjKm0jVtuucUYNGiQYRiG8fHHHxtBQUFGdnb2Oeso38f333/vmvfnP//ZaNy48XmPBQCgbnBu4twE36NLG0yhW7dulZ4fP35cU6dOVbt27RQXF6eoqCjt2LFDOTk5VW6nY8eOrp8jIyMVHR2t/Pz8cy775ZdfyjAMtWrVSlFRUa5p9erV2rVrl2u54ODgSvW1adNGcXFx2r59uyRp+/btuuqqqypt+6qrrnK9vmXLFjVp0kStWrU6b90RERFq0aKF63lSUtJ56wYAeAfnJs5N8A4GLYApnDmize9//3t9/PHHmjt3rlq2bKnw8HD9+te/VklJSZXbCQkJqfTcYrGorKzsnMuWlZXJarVq06ZNslqtlV6Lioo6aztnqjjvzNcNw3DNCw8Pr7Lm89VtGMYF1wMAeA7nJs5N8A5aeBAwQkND5XA4qrXsZ599plGjRulXv/qVOnTooMTERO3Zs6dO6+ncubMcDofy8/PVsmXLSlNiYqJrudLSUmVlZbmeZ2dn68iRI2rTpo0kqW3btvr8888rbXvt2rVq27atJOc3ez/++KO+++67Oq0fAFB7nJs4N8H3aOFBwEhLS9OGDRu0Z88eRUVFqWHDhuddtmXLllq2bJkGDx4si8Wixx9//LzfhtVUq1atdMcdd2jEiBGaN2+eOnfurEOHDunTTz9Vhw4dNGjQIEnOb7gmTpyoBQsWKCQkRBMmTFD37t11xRVXSHJ+4/eb3/xGXbp0UZ8+ffTee+9p2bJlrpF7evbsqWuvvVZDhw7Vs88+q5YtW2rHjh2yWCwaMGBAnb4nAIB7ODdxboLv0cKDgPHggw/KarWqXbt2uuiii6rs8/zcc8+pQYMG6tGjhwYPHqz+/furS5cudV7T4sWLNWLECD3wwANq3bq1brzxRm3YsEEpKSmuZSIiIvTQQw/p9ttvV3p6usLDw/XWW2+5Xr/55pv1/PPP65lnnlH79u3117/+VYsXL1avXr1cy2RkZOjyyy/Xbbfdpnbt2mnq1KnV/kYRAOA5nJs4N8H3LAadJQGfWbJkiSZPnqwjR474uhQAACRxbkLgoYUHAAAAQMAi8AAAAAAIWHRpAwAAABCwaOEBAAAAELAIPAAAAAACFoEHAAAAQMAi8AAAAAAIWAQeAAAAAAGLwAMAAAAgYBF4AAAAAAQsAg8AAACAgEXgAQAAABCw/j+ZM7exSSlw1AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_model.model(train_x)\n",
    "    test_preds = dynamics_model.model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.simple import Simple\n",
    "\n",
    "# Hyperparams\n",
    "learningRate = 0.0001\n",
    "weight_decay = 0.00001\n",
    "optim_eps = 1e-8\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "eval_epoch_freq = 1\n",
    "in_size = train_x.shape[-1]\n",
    "out_size = train_y.shape[-1]\n",
    "device = \"cpu\"\n",
    "NN_model = Simple(\n",
    "    in_size,\n",
    "    out_size,\n",
    "    device,\n",
    "    num_layers=2,\n",
    "    hid_size=100,\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "    NN_model.parameters(),\n",
    "    lr=learningRate,\n",
    "    weight_decay=weight_decay,\n",
    "    eps=optim_eps,\n",
    ")\n",
    "# torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 5.084537029266357, R2 -0.35017168521881104\n",
      "Eval loss 5.101757526397705, R2 -0.34853339195251465\n",
      "epoch 1, loss 4.560205459594727, R2 -0.1813887506723404\n",
      "Eval loss 4.5604753494262695, R2 -0.17539280652999878\n",
      "epoch 2, loss 3.709505081176758, R2 0.00689114211127162\n",
      "Eval loss 3.7030155658721924, R2 0.018820814788341522\n",
      "epoch 3, loss 3.2644011974334717, R2 0.09219035506248474\n",
      "Eval loss 3.3018875122070312, R2 0.09712818264961243\n",
      "epoch 4, loss 3.147446393966675, R2 0.11825559288263321\n",
      "Eval loss 3.207768440246582, R2 0.11668994277715683\n",
      "epoch 5, loss 3.0691137313842773, R2 0.13322535157203674\n",
      "Eval loss 3.1312477588653564, R2 0.1318073719739914\n",
      "epoch 6, loss 2.986293315887451, R2 0.1494521051645279\n",
      "Eval loss 3.0499651432037354, R2 0.14798718690872192\n",
      "epoch 7, loss 2.894711494445801, R2 0.16709300875663757\n",
      "Eval loss 2.960143566131592, R2 0.1654273122549057\n",
      "epoch 8, loss 2.7952778339385986, R2 0.1864708811044693\n",
      "Eval loss 2.8612163066864014, R2 0.18521390855312347\n",
      "epoch 9, loss 2.6898558139801025, R2 0.20747973024845123\n",
      "Eval loss 2.7581300735473633, R2 0.20611706376075745\n",
      "epoch 10, loss 2.5802106857299805, R2 0.23122316598892212\n",
      "Eval loss 2.649510622024536, R2 0.23002900183200836\n",
      "epoch 11, loss 2.4673779010772705, R2 0.25639864802360535\n",
      "Eval loss 2.538316488265991, R2 0.25508758425712585\n",
      "epoch 12, loss 2.352557897567749, R2 0.2819962799549103\n",
      "Eval loss 2.425096273422241, R2 0.28046461939811707\n",
      "epoch 13, loss 2.2370450496673584, R2 0.30712053179740906\n",
      "Eval loss 2.311392307281494, R2 0.3051970899105072\n",
      "epoch 14, loss 2.1202681064605713, R2 0.3324419856071472\n",
      "Eval loss 2.1961774826049805, R2 0.3303031921386719\n",
      "epoch 15, loss 2.0033156871795654, R2 0.3578978180885315\n",
      "Eval loss 2.0811991691589355, R2 0.35506224632263184\n",
      "epoch 16, loss 1.88866126537323, R2 0.3828373849391937\n",
      "Eval loss 1.968503713607788, R2 0.37932834029197693\n",
      "epoch 17, loss 1.7790701389312744, R2 0.406717985868454\n",
      "Eval loss 1.8606743812561035, R2 0.4025222063064575\n",
      "epoch 18, loss 1.6756558418273926, R2 0.42923295497894287\n",
      "Eval loss 1.758486270904541, R2 0.4244026839733124\n",
      "epoch 19, loss 1.5787427425384521, R2 0.4505406618118286\n",
      "Eval loss 1.6621365547180176, R2 0.4452371597290039\n",
      "epoch 20, loss 1.4884896278381348, R2 0.4707970917224884\n",
      "Eval loss 1.5716981887817383, R2 0.4651724100112915\n",
      "epoch 21, loss 1.4049174785614014, R2 0.49007436633110046\n",
      "Eval loss 1.4871667623519897, R2 0.48424434661865234\n",
      "epoch 22, loss 1.3282674551010132, R2 0.5083236694335938\n",
      "Eval loss 1.4087761640548706, R2 0.5024963021278381\n",
      "epoch 23, loss 1.2586932182312012, R2 0.5254606008529663\n",
      "Eval loss 1.3368927240371704, R2 0.5198166370391846\n",
      "epoch 24, loss 1.1962165832519531, R2 0.5413904786109924\n",
      "Eval loss 1.271645188331604, R2 0.5361077785491943\n",
      "epoch 25, loss 1.1407041549682617, R2 0.5560182929039001\n",
      "Eval loss 1.2129992246627808, R2 0.5512500405311584\n",
      "epoch 26, loss 1.091849446296692, R2 0.5693175792694092\n",
      "Eval loss 1.1608140468597412, R2 0.5651830434799194\n",
      "epoch 27, loss 1.0492362976074219, R2 0.581325352191925\n",
      "Eval loss 1.1148563623428345, R2 0.5778842568397522\n",
      "epoch 28, loss 1.0123645067214966, R2 0.5921310782432556\n",
      "Eval loss 1.0747803449630737, R2 0.5893803238868713\n",
      "epoch 29, loss 0.9807354211807251, R2 0.6018247604370117\n",
      "Eval loss 1.040203332901001, R2 0.599713921546936\n",
      "epoch 30, loss 0.9538806080818176, R2 0.6104834079742432\n",
      "Eval loss 1.0107423067092896, R2 0.6089363098144531\n",
      "epoch 31, loss 0.931358814239502, R2 0.6181802153587341\n",
      "Eval loss 0.9859046339988708, R2 0.6171324253082275\n",
      "epoch 32, loss 0.9126355051994324, R2 0.625009298324585\n",
      "Eval loss 0.9651562571525574, R2 0.6243929862976074\n",
      "epoch 33, loss 0.8971784710884094, R2 0.6310735940933228\n",
      "Eval loss 0.9479002356529236, R2 0.6308495998382568\n",
      "epoch 34, loss 0.8844226002693176, R2 0.6364753842353821\n",
      "Eval loss 0.9334802031517029, R2 0.6366352438926697\n",
      "epoch 35, loss 0.8738373517990112, R2 0.6413170099258423\n",
      "Eval loss 0.9212787747383118, R2 0.6418774127960205\n",
      "epoch 36, loss 0.8649380207061768, R2 0.6456977725028992\n",
      "Eval loss 0.9108126759529114, R2 0.6466720104217529\n",
      "epoch 37, loss 0.8573553562164307, R2 0.6496858596801758\n",
      "Eval loss 0.90171217918396, R2 0.6510820388793945\n",
      "epoch 38, loss 0.8508023023605347, R2 0.6533300876617432\n",
      "Eval loss 0.8936733603477478, R2 0.6551592350006104\n",
      "epoch 39, loss 0.8450659513473511, R2 0.6566713452339172\n",
      "Eval loss 0.8864725828170776, R2 0.6589483618736267\n",
      "epoch 40, loss 0.8399957418441772, R2 0.6597296595573425\n",
      "Eval loss 0.8799671530723572, R2 0.6624661087989807\n",
      "epoch 41, loss 0.8354834914207458, R2 0.6625204682350159\n",
      "Eval loss 0.8740501403808594, R2 0.665729820728302\n",
      "epoch 42, loss 0.8314517140388489, R2 0.6650621294975281\n",
      "Eval loss 0.8686403632164001, R2 0.6687566637992859\n",
      "epoch 43, loss 0.8278369903564453, R2 0.6673728227615356\n",
      "Eval loss 0.8637017011642456, R2 0.6715531945228577\n",
      "epoch 44, loss 0.8245943784713745, R2 0.6694698929786682\n",
      "Eval loss 0.8591809272766113, R2 0.67413330078125\n",
      "epoch 45, loss 0.8216872215270996, R2 0.6713694334030151\n",
      "Eval loss 0.8550633788108826, R2 0.676501989364624\n",
      "epoch 46, loss 0.8190909028053284, R2 0.6730836629867554\n",
      "Eval loss 0.8513212203979492, R2 0.6786717772483826\n",
      "epoch 47, loss 0.8167744874954224, R2 0.6746264100074768\n",
      "Eval loss 0.8479408621788025, R2 0.6806450486183167\n",
      "epoch 48, loss 0.8147177696228027, R2 0.6760032773017883\n",
      "Eval loss 0.844918966293335, R2 0.6824226975440979\n",
      "epoch 49, loss 0.8128959536552429, R2 0.6772286891937256\n",
      "Eval loss 0.8422029614448547, R2 0.6840338110923767\n",
      "epoch 50, loss 0.8112974166870117, R2 0.678306519985199\n",
      "Eval loss 0.8397673964500427, R2 0.6854812502861023\n",
      "epoch 51, loss 0.8099042773246765, R2 0.6792430281639099\n",
      "Eval loss 0.8376345634460449, R2 0.6867546439170837\n",
      "epoch 52, loss 0.8086942434310913, R2 0.6800482273101807\n",
      "Eval loss 0.8357480764389038, R2 0.6878687143325806\n",
      "epoch 53, loss 0.8076481819152832, R2 0.6807330846786499\n",
      "Eval loss 0.8341148495674133, R2 0.688828706741333\n",
      "epoch 54, loss 0.8067436218261719, R2 0.6813127994537354\n",
      "Eval loss 0.8326800465583801, R2 0.6896539926528931\n",
      "epoch 55, loss 0.8059622049331665, R2 0.681800127029419\n",
      "Eval loss 0.8314375281333923, R2 0.6903569102287292\n",
      "epoch 56, loss 0.8052847981452942, R2 0.6822084784507751\n",
      "Eval loss 0.8303694725036621, R2 0.6909514665603638\n",
      "epoch 57, loss 0.8046892881393433, R2 0.6825586557388306\n",
      "Eval loss 0.8294623494148254, R2 0.6914503574371338\n",
      "epoch 58, loss 0.8041654825210571, R2 0.6828558444976807\n",
      "Eval loss 0.8286598920822144, R2 0.691871702671051\n",
      "epoch 59, loss 0.8037006258964539, R2 0.6831112504005432\n",
      "Eval loss 0.8279722332954407, R2 0.6922271251678467\n",
      "epoch 60, loss 0.8032819628715515, R2 0.6833344101905823\n",
      "Eval loss 0.827373743057251, R2 0.6925344467163086\n",
      "epoch 61, loss 0.8028998374938965, R2 0.6835334300994873\n",
      "Eval loss 0.8268473148345947, R2 0.6927990317344666\n",
      "epoch 62, loss 0.8025485873222351, R2 0.6837108731269836\n",
      "Eval loss 0.8263879418373108, R2 0.6930269002914429\n",
      "epoch 63, loss 0.8022276163101196, R2 0.6838697791099548\n",
      "Eval loss 0.8259795904159546, R2 0.6932252049446106\n",
      "epoch 64, loss 0.80193030834198, R2 0.6840140223503113\n",
      "Eval loss 0.8256165385246277, R2 0.6934028267860413\n",
      "epoch 65, loss 0.8016502857208252, R2 0.6841509938240051\n",
      "Eval loss 0.825285792350769, R2 0.6935612559318542\n",
      "epoch 66, loss 0.8013841509819031, R2 0.6842797994613647\n",
      "Eval loss 0.8249883055686951, R2 0.6937039494514465\n",
      "epoch 67, loss 0.801131010055542, R2 0.6844018697738647\n",
      "Eval loss 0.8247144818305969, R2 0.6938368082046509\n",
      "epoch 68, loss 0.8008893132209778, R2 0.68450528383255\n",
      "Eval loss 0.8244622945785522, R2 0.6939602494239807\n",
      "epoch 69, loss 0.800656795501709, R2 0.6846298575401306\n",
      "Eval loss 0.8242288827896118, R2 0.6940756440162659\n",
      "epoch 70, loss 0.8004330396652222, R2 0.6847383975982666\n",
      "Eval loss 0.8240092396736145, R2 0.6941824555397034\n",
      "epoch 71, loss 0.8002180457115173, R2 0.6848430633544922\n",
      "Eval loss 0.8238032460212708, R2 0.6942834258079529\n",
      "epoch 72, loss 0.8000113368034363, R2 0.684943437576294\n",
      "Eval loss 0.8236106038093567, R2 0.6943762898445129\n",
      "epoch 73, loss 0.7998135685920715, R2 0.6850375533103943\n",
      "Eval loss 0.8234264850616455, R2 0.6944596171379089\n",
      "epoch 74, loss 0.7996228933334351, R2 0.6851330399513245\n",
      "Eval loss 0.8232539296150208, R2 0.6945505738258362\n",
      "epoch 75, loss 0.7994394302368164, R2 0.6852236390113831\n",
      "Eval loss 0.8230921626091003, R2 0.6946313381195068\n",
      "epoch 76, loss 0.7992624044418335, R2 0.6853107810020447\n",
      "Eval loss 0.8229331374168396, R2 0.6947144865989685\n",
      "epoch 77, loss 0.7990907430648804, R2 0.6853967308998108\n",
      "Eval loss 0.8227853178977966, R2 0.6947886943817139\n",
      "epoch 78, loss 0.798922598361969, R2 0.6854824423789978\n",
      "Eval loss 0.8226464986801147, R2 0.6948635578155518\n",
      "epoch 79, loss 0.7987577319145203, R2 0.6855667233467102\n",
      "Eval loss 0.8225069046020508, R2 0.6949326992034912\n",
      "epoch 80, loss 0.7985991835594177, R2 0.6856439113616943\n",
      "Eval loss 0.8223769664764404, R2 0.6950041055679321\n",
      "epoch 81, loss 0.7984464764595032, R2 0.6857244968414307\n",
      "Eval loss 0.8222475051879883, R2 0.6950700283050537\n",
      "epoch 82, loss 0.7982980012893677, R2 0.6857953667640686\n",
      "Eval loss 0.8221321105957031, R2 0.6951344013214111\n",
      "epoch 83, loss 0.7981519103050232, R2 0.6858741641044617\n",
      "Eval loss 0.8220229148864746, R2 0.695195734500885\n",
      "epoch 84, loss 0.7980095744132996, R2 0.6859474182128906\n",
      "Eval loss 0.821915864944458, R2 0.6952556371688843\n",
      "epoch 85, loss 0.7978706359863281, R2 0.6860204339027405\n",
      "Eval loss 0.8218008875846863, R2 0.6953192353248596\n",
      "epoch 86, loss 0.7977367043495178, R2 0.6860899329185486\n",
      "Eval loss 0.8216924667358398, R2 0.6953760981559753\n",
      "epoch 87, loss 0.7976058125495911, R2 0.6861574053764343\n",
      "Eval loss 0.8215941190719604, R2 0.6954322457313538\n",
      "epoch 88, loss 0.7974777817726135, R2 0.6862225532531738\n",
      "Eval loss 0.8215129375457764, R2 0.6954757571220398\n",
      "epoch 89, loss 0.7973540425300598, R2 0.6862843036651611\n",
      "Eval loss 0.8214120864868164, R2 0.6955332159996033\n",
      "epoch 90, loss 0.797232449054718, R2 0.6863502264022827\n",
      "Eval loss 0.8213300704956055, R2 0.6955799460411072\n",
      "epoch 91, loss 0.7971129417419434, R2 0.6864116787910461\n",
      "Eval loss 0.8212599158287048, R2 0.6956212520599365\n",
      "epoch 92, loss 0.7969968318939209, R2 0.6864630579948425\n",
      "Eval loss 0.8211764693260193, R2 0.6956655383110046\n",
      "epoch 93, loss 0.7968811988830566, R2 0.6865294575691223\n",
      "Eval loss 0.8211173415184021, R2 0.6957072615623474\n",
      "epoch 94, loss 0.7967681884765625, R2 0.6865861415863037\n",
      "Eval loss 0.8210436701774597, R2 0.6957509517669678\n",
      "epoch 95, loss 0.7966594696044922, R2 0.6866323351860046\n",
      "Eval loss 0.8209852576255798, R2 0.6957688927650452\n",
      "epoch 96, loss 0.7965512871742249, R2 0.6866933703422546\n",
      "Eval loss 0.8208976984024048, R2 0.6958324909210205\n",
      "epoch 97, loss 0.7964451313018799, R2 0.6867444515228271\n",
      "Eval loss 0.8208137154579163, R2 0.6958823800086975\n",
      "epoch 98, loss 0.7963393330574036, R2 0.6867970824241638\n",
      "Eval loss 0.8207620978355408, R2 0.69591224193573\n",
      "epoch 99, loss 0.7962363362312317, R2 0.6868471503257751\n",
      "Eval loss 0.8206801414489746, R2 0.695962131023407\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    k = 0\n",
    "    l = batch_size\n",
    "    batch_loss = []\n",
    "    while l < train_x.shape[0]:\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = NN_model(train_x)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, train_y)\n",
    "        batch_loss.append(loss.item())\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update k,l\n",
    "        k = l\n",
    "        l = min(l + batch_size, train_x.shape[0])\n",
    "\n",
    "    # Append train loss\n",
    "    train_losses.append(np.mean(batch_loss))\n",
    "\n",
    "    # Compute metric\n",
    "    train_metric = metric(outputs, train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print(\"epoch {}, loss {}, R2 {}\".format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch % eval_epoch_freq == 0:\n",
    "        with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "            preds = NN_model(test_x)\n",
    "            test_loss = criterion(preds, test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            # Compute metric\n",
    "            test_metric = metric(preds, test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print(\"Eval loss {}, R2 {}\".format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAAFzCAYAAADYPF2rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABujElEQVR4nO3dd3gU5d7G8e9m0ztJIAkQem+iIAiIoiDYEMWOUiwUaQKiiChFUDgqVQU7HCuooK+FAwSVoigdQQSk14TQkhBC2u68fyxZCQRIIMnsbu7Pdc21u7PPzNw7hJ388jwzYzEMw0BERERERMSFeZkdQERERERE5FJUuIiIiIiIiMtT4SIiIiIiIi5PhYuIiIiIiLg8FS4iIiIiIuLyVLiIiIiIiIjLU+EiIiIiIiIuT4WLiIiIiIi4PO+S3qDdbufQoUOEhIRgsVhKevMiIqWWYRicPHmS8uXL4+Wlv1vl0nFJRMQ8hTk2lXjhcujQIeLi4kp6syIicsb+/fupWLGi2TFcho5LIiLmK8ixqcQLl5CQEMARLjQ0tKQ3LyJSaqWmphIXF+f8HhYHHZdERMxTmGNTiRcuud3woaGhOkCIiJhAw6Hy0nFJRMR8BTk2aZCziIiIiIi4PBUuIiIiIiLi8lS4iIiIiIiIyyvxc1xEpGgYhkFOTg42m83sKOIirFYr3t7eOodFREQ8kgoXETeUlZVFQkIC6enpZkcRFxMYGEhsbCy+vr5mRxERESlSKlxE3Izdbmf37t1YrVbKly+Pr6+v/sIuGIZBVlYWR44cYffu3dSsWVM3mRQREY+iwkXEzWRlZWG324mLiyMwMNDsOOJCAgIC8PHxYe/evWRlZeHv7292JBERkSKjP8eJuCn9NV3yo58LERHxVDrCiYiIiIiIy3OroWL79sHatRAdDS1bmp1GRERERDydYRgYGBiGgd2wY+B4tBv28+Zd7HV+68mdd/b6zl723OUu9pi7DJDv++d+loK8Zzfs560vv31SObwyTcs3LfZ/C7cqXD79FEaMgG7dVLiICLRp04bGjRszZcqUArXfs2cPVatWZf369TRu3LjYci1ZsoSbbrqJEydOEB4eXmzbEREpTlm2LNKz0zmVdYrTOac5nX2a0zmnycjJcE6ZOZlk2bLItDkes2xZZNuyHY/2bLJt2WTbs8mx55w32Qzbv8/tNuc8m92W59Fu2PPMsxv2PPNzf+kvzHR2sXDee+cUH3Jp3a/qzqy7ZxX7dtyqcKle3fG4a5e5OUSkcC511bPu3bsza9asQq933rx5+Pj4FLh9XFwcCQkJREVFFXpbIiLuxjAMUjJTOJx2mCPpRziafpSj6Uc5ln6MExknOH76OMkZyaRkppCSkUJqZippWWmczDpJWlYaOfYcsz+CR/CyeGHBgsVicT6eO8/L4pXv8/za5h5T85t/qUfgou+du97c50C+68nNWiuyVonsS7cqXKpVczzu3GluDhEpnISEBOfzOXPmMHLkSLZt2+acFxAQkKd9dnZ2gQqSiIiIQuWwWq3ExMQUahkREVdkGAZJp5LYk7yHvSl72Zu8l/2p+zl48iAHUw+SkJZAYloiWbasK96W1WIl0CeQAJ8AArwD8Pf2d05+3n74Wf3wtfrmmXy8fPD28sFq8cGKN1aLD154Y8UHC1a88MYLb+dzi2HFYjiec+a58xEvsFuxYAXDCwwv5/vYvcCwYtjPnu+FYVjOzHO0MQwL2K0YhheG3eJo45zv5Zyfux4ML+z2M+/ZvbA721nOtLM6Hs9sx7GcBbtzHbnPLRgG2O04H3On/OYbRt7557YpyLzLfZ07L+ec146ftwsvYxhw7H6g9RX/qF2SWxYuCQmQng66EqyI4wvDrPtQBgZCQW4hc3axEBYWhsVicc7bs2cPsbGxzJkzh+nTp/PHH38wY8YM7rrrLvr378/y5cs5fvw41atX54UXXuDhhx92ruvcoWJVqlShV69e7Nixg6+++ooyZcrw4osv0qtXL+e2zh4qljuka/HixQwbNoy///6bxo0bM3PmTGrXru3czrhx45g2bRqnT5/mwQcfJCoqigULFrBhw4YC76u5c+cycuRIduzYQWxsLAMGDOCZZ55xvj99+nQmT57M/v37CQsLo3Xr1nz99dcAfP3114wZM4YdO3YQGBjI1Vdfzf/93/8RFBRU4O2LiPsxDIPDpw6z9ehW/jn2D/8c+4ftx7ez68Qudp3YRXp2wb78Q3xDifQrR6hPFMFekQQSha+9DL62CLyzw/HKDsOSFQYZodgzQ7CfDsaeEUxOehC2jCCyM3zJzISsLJyPp7MhNRuyz5pychxT7vMzp1tIKXDsWMlsx60Kl4gICAuDlBTYswfq1TM7kYj50tMhONicbaelQVH97jxs2DAmTpzIzJkz8fPzIyMjgyZNmjBs2DBCQ0P58ccf6dq1K9WqVaN58+YXXM/EiRMZO3YsL7zwAl9//TVPPfUUN9xwA3Xq1LngMiNGjGDixImULVuWPn368Pjjj/Pbb78B8Nlnn/HKK68wffp0WrVqxezZs5k4cSJVq1Yt8Gdbu3YtDzzwAKNHj+bBBx9kxYoV9O3bl8jISHr06MGaNWsYOHAgn3zyCS1btuT48eMsX74ccPRWPfzww7z22mvcc889nDx5kuXLlztPwBQRz5CSkcLGwxvZeHgjm5I28VfSX/x95G9OZJy44DIWLJSxViDUqIx/ZmWsaXHYUyqQdawCpw+XJy0xltRD0ZzM8edkCX6WgrBaLzx5eZ3/3GL5d97Zz89ue+783NfnPj97utD8/JbPfcxvfkHeO7vNucuc2+5Cy+Q+L8j6zm57qeUvNF2qTe77JTUC260KF4vF0euyfr1juJgKFxHPMWjQIDp37pxn3tChQ53PBwwYwIIFC/jqq68uWrjcfvvt9O3bF3AUQ5MnT2bJkiUXLVxeeeUVbrzxRgCef/557rjjDjIyMvD39+fNN9/kiSee4LHHHgNg5MiRLFq0iLS0tAJ/tkmTJtG2bVteeuklAGrVqsXff//N66+/To8ePdi3bx9BQUHceeedhISEULlyZa6++mrAUbjk5OTQuXNnKleuDEDDhg0LvG0RcT0nM0+y5tAaVh1cxdqEtaxLWMfOE/mPg7cYXgTnVMUntTY5h2tyan9NbEk14EQ1jJTKHLf5crwA2/Tzg9DQf6fgYMcUFOSYAgMdU0AA+Pvnnfz8HJOvb97Jxyfv5O397+PZk9Wa97luNyWXy60KF/i3cNEJ+iIOgYGOng+ztl1UmjbNexlFm83GhAkTmDNnDgcPHiQzM5PMzMxLDo9q1KiR83nukLSkpKQCLxMbGwtAUlISlSpVYtu2bc5CKFezZs34+eefC/S5ALZs2UKnTp3yzGvVqhVTpkzBZrNxyy23ULlyZapVq8att97Krbfeyj333ENgYCBXXXUVbdu2pWHDhnTo0IH27dtz3333UaZMmQJvX0TMYxgGu07s4rf9v/Hrvl9ZsX8Ffx/523lZ2bN5n4rDfugq7AmNIKkBHKmPcawWJ3P8z2sbFQWx9SA21nGbiOhoKFvWMUVFQWQklCnjmMLDHQWIiLtzq8Il6VQSvjW2Qblwdu3SXxxFwNET6QmnOpxbkEycOJHJkyczZcoUGjZsSFBQEIMGDSIr6+Inmp57Ur/FYsFuv/jlLM9eJvdqKWcvc+5V0Qo7TMswjIuuIyQkhHXr1rFkyRIWLVrEyJEjGT16NKtXryY8PJz4+HhWrFjBokWLePPNNxkxYgQrV64s1HA1ESkZhmGw88ROftn9C0v2LmHJniUcOnno/IbJleBgMzh0LSRcAwlXk3M6EnD0WlSpAlWvgqpVoXJlqFTJMVWsCOXLO3pAREobtypcPlr/EV8EDIeW3dm1a5bZcUSkGC1fvpxOnTrx6KOPAo5CYvv27dStW7dEc9SuXZtVq1bRtWtX57w1a9YUah316tXj119/zTNvxYoV1KpVC6vVCoC3tzft2rWjXbt2jBo1ivDwcH7++Wc6d+6MxWKhVatWtGrVipEjR1K5cmW++eYbhgwZcuUfUESuWHJGMvE741m0cxELd8Sz/+TevA1yfCGhCexrBftbwYHrIC2G2FjHsPc6t0GdOlCrFtSsCXFxjmFVIpKXW/23iAk+c2Wi4ER2rjU3i4gUrxo1ajB37lxWrFhBmTJlmDRpEomJiSVeuAwYMICePXvStGlTWrZsyZw5c9i4cSPVci9zWADPPPMM1157LWPHjuXBBx/k999/56233mL69OkA/PDDD+zatYsbbriBMmXKMH/+fOx2O7Vr12blypX89NNPtG/fnnLlyrFy5UqOHDlS4vtBRPLadnQb/7ft//h283xWJvyKHdu/b9p8YH8L2HMT7GmD9+HmNKwTwNVXQ+OO0KgRNGjgGM4lIgXntoXL7t2Oa0nrBC8Rz/TSSy+xe/duOnToQGBgIL169eLuu+8mJSWlRHM88sgj7Nq1i6FDh5KRkcEDDzxAjx49WLVqVYHXcc011/Dll18ycuRIxo4dS2xsLC+//DI9evQAIDw8nHnz5jF69GgyMjKoWbMmX3zxBfXr12fLli0sW7aMKVOmkJqaSuXKlZk4cSK33XZbMX1iEcmPYRisTVjLF3/O5cuN33IgY2veBkfqwo4OsOsWqltvpFWzIJrfD82aQcOGGtolUhQsRiEGa48ePZoxY8bkmRcdHU1iYmKBN5iamkpYWBgpKSmEhoYWPCnwZ+KfNH63MaSVgzcOc/CgY5ynSGmSkZHB7t27qVq1Kv4629IUt9xyCzExMXzyySdmRznPxX4+ruT715Npv8iFGIbBn4f/5J1fZ/P1li85Zt/975s2H9h9M/zTkXo+t9GhWTVuuAFatoRy5czLLOJuCvMdXOgel/r167N48WLn69zx2SXB2eMSdAS8cti501uFi4gUq/T0dN555x06dOiA1Wrliy++YPHixcTHx5sdTUSKyaGTh5i0+DM+3fgJh9n07xtZgbD9DqKOdubO2rfR8Z4w2rRx3GdORIpfoQsXb2/vPHfBLklRgVF4WbywY4fAI+zaFUvr1qZEEZFSwmKxMH/+fMaNG0dmZia1a9dm7ty5tGvXzuxoIlKEcuw5fPz7/3j95/fYapsPljNXFszxhR13UDfnIR5tfgf3jg2iVi3HFR1FpGQVunDZvn075cuXx8/Pj+bNm/Pqq69e9CTV3Hsv5EpNTb28pIDVy0q5oHIkpiVCcCK7dsVe9rpERAoiICAgTy+ziHiW/cePMOSzd/k+4R0y/Q46ZlrAsr8V9XO60bv1/XQZXEa9KiIuoFCFS/Pmzfn444+pVasWhw8fZty4cbRs2ZLNmzcTeYFLY4wfP/6882KuRExwzFmFS5GtVkREREqR+as389y3E9ns9Tl4Z4IfcCqKise60+fanvQdWBvd51XEtRSqcDn7KjYNGzakRYsWVK9enf/+978XvJ/A8OHD87yXmppKXFzcZcaF2OBYNrABQhLYufOyVyMiIiKljGHAez+uZvRPr5IY/i34Oub7HGlKx7JPM77P/dSqpst/ibiqK7occlBQEA0bNmT79u0XbOPn54dfEV4D8OxLIu/6p8hWKyIiIh7KMODteet48ZcXSCm7EMIBw0L0iXsY2vIZBo1ogbe3TloRcXVXVLhkZmayZcsWWpfgGfJnFy6HD8OpUxAUVGKbFxERETdhGDDrux0Mnf8ix8vPgbKA3UqtjEeZdO8w7mimG7mKuJNCFS5Dhw6lY8eOVKpUiaSkJMaNG0dqairdu3cvrnznyS1cfMokkg3s2uW4sZOIiIhIrlUb0njw7XHsiZ0E5bMBqJPdhf92H0uzmhe+qJCIuK5C3Xf+wIEDPPzww9SuXZvOnTvj6+vLH3/8QeXKlYsr33ligx1XEvOLTADQCfoiUiAWi4Vvv/3W7BgiUsyOHjVoP+hrmn9clz0V/wPWbCpnd2DxfevZMu4zFS0ibqxQPS6zZ88urhwFdvZQMVDhIuIOLJe44UH37t2ZNWvWZa27SpUqDBo0iEGDBl3W8iLiGQwD3vnkMIN+6k1Wtf8DIDCrCpNumUqvGzte8ntIRFxfoXpcXEFu4ZLlq8JFxF0kJCQ4pylTphAaGppn3tSpU82OKC5k+vTpVK1aFX9/f5o0acLy5csv2j4zM5MRI0ZQuXJl/Pz8qF69Oh999FEJpRVXsG8fXNP1a/purk9Wtf/DYvehW+WXODr6b3q3uUtFi4iHcN/CxZIGvmm6JLKIG4iJiXFOYWFhWCyWPPOWLVtGkyZN8Pf3p1q1aowZM4acnBzn8qNHj6ZSpUr4+flRvnx5Bg4cCECbNm3Yu3cvgwcPxmKxFOqXk02bNnHzzTcTEBBAZGQkvXr1Ii0tzfn+kiVLaNasGUFBQYSHh9OqVSv27t0LwJ9//slNN91ESEgIoaGhNGnShDVr1hTR3ird5syZw6BBgxgxYgTr16+ndevW3Hbbbezbt++CyzzwwAP89NNPfPjhh2zbto0vvviCOnXqlGBqMdPHs09RY2gPNtS8HwKPEcNVrHpyNf/t8TIBPgFmxxORInRFVxUzQ4hfCEE+QZzKPnXmJpQ1zI4kYirDMEjPTjdl24E+gVf8l8yFCxfy6KOPMm3aNFq3bs3OnTvp1asXAKNGjeLrr79m8uTJzJ49m/r165OYmMiff/4JwLx587jqqqvo1asXPXv2LPA209PTufXWW7nuuutYvXo1SUlJPPnkk/Tv359Zs2aRk5PD3XffTc+ePfniiy/Iyspi1apVzs/6yCOPcPXVVzNjxgysVisbNmzAx8fnivaDOEyaNIknnniCJ598EoApU6awcOFCZsyYwfjx489rv2DBApYuXcquXbuIOHNr8ypVqpRkZDFJWhp0H7KNeT73Qv3NYHjRp/4LTO38Er5WX7PjiUgxcLvCBRy9LjtP7DxzSWQVLlK6pWenEzw+2JRtpw1PI8j3yq5H/sorr/D88887r05YrVo1xo4dy3PPPceoUaPYt28fMTExtGvXDh8fHypVqkSzZs0AiIiIwGq1EhISQkxMTIG3+dlnn3H69Gk+/vhjgs5cT/2tt96iY8eO/Oc//8HHx4eUlBTuvPNOqlevDkDduv9eNnXfvn08++yzzr/q16xZ84r2gThkZWWxdu1ann/++Tzz27dvz4oVK/Jd5rvvvqNp06a89tprfPLJJwQFBXHXXXcxduxYAgLy/2t7ZmYmmZmZztepqalF9yGkRGzdCm0HfMWhax8HvzSCjGi+fXQ27Wq0MTuaiBQjtxsqBnlP0E9OBpvN1DgicgXWrl3Lyy+/THBwsHPq2bMnCQkJpKenc//993P69GmqVatGz549+eabb/IMI7scW7Zs4aqrrnIWLQCtWrXCbrezbds2IiIi6NGjBx06dKBjx45MnTqVhIQEZ9shQ4bw5JNP0q5dOyZMmMBOjVktEkePHsVmsxEdHZ1nfnR0NImJifkus2vXLn799Vf++usvvvnmG6ZMmcLXX39Nv379Lrid8ePHExYW5pzi4uKK9HNI8fq//zO4asA4Dl3/APil0Ti8DTuGblDRIlIKuG2PC+C8slhyMkRGmpdHxEyBPoGkDU+7dMNi2vaVstvtjBkzhs6dO5/3nr+/P3FxcWzbto34+HgWL15M3759ef3111m6dOllD88yDOOCQ9xy58+cOZOBAweyYMEC5syZw4svvkh8fDzXXXcdo0ePpkuXLvz444/873//Y9SoUcyePZt77rnnsvJIXuf+21zs38tut2OxWPjss88ICwsDHMPN7rvvPt5+++18e12GDx/OkCFDnK9TU1NVvLgBw4BRL2cxdkMvuP6/APRuNJi3Or2Gt5db/jojIoXklv/TcwsX34gEsoDjx1W4SOllsViueLiWma655hq2bdtGjRoXHvYZEBDAXXfdxV133UW/fv2oU6cOmzZt4pprrsHX1xdbIbtd69Wrx3//+19OnTrl7HX57bff8PLyolatWs52V199NVdffTXDhw+nRYsWfP7551x33XUA1KpVi1q1ajF48GAefvhhZs6cqcLlCkVFRWG1Ws/rXUlKSjqvFyZXbGwsFSpUcBYt4BjWZxgGBw4cyHcYn5+fH35+fkUbXopVdjb06J3K5/ZO0HgJFsPKm7e9Rb/mfcyOJiIlyC2HiuXehNInwnFwO37czDQiciVGjhzJxx9/zOjRo9m8eTNbtmxx9nAAzJo1iw8//JC//vqLXbt28cknnxAQEOC88W2VKlVYtmwZBw8e5OjRowXa5iOPPIK/vz/du3fnr7/+4pdffmHAgAF07dqV6Ohodu/ezfDhw/n999/Zu3cvixYt4p9//qFu3bqcPn2a/v37s2TJEvbu3ctvv/3G6tWr85wDI5fH19eXJk2aEB8fn2d+fHw8LVu2zHeZVq1acejQoTxXhPvnn3/w8vKiYsWKxZpXSkZaGtza+Rife7eFqkvwt4Twv0d/VNEiUgq5ZeGS2+NiDVXhIuLuOnTowA8//EB8fDzXXnst1113HZMmTXIWJuHh4bz//vu0atWKRo0a8dNPP/H9998Teaab9eWXX2bPnj1Ur16dsmXLFmibgYGBLFy4kOPHj3Pttddy33330bZtW9566y3n+1u3buXee++lVq1a9OrVi/79+9O7d2+sVivHjh2jW7du1KpViwceeIDbbruNMWPGFM8OKmWGDBnCBx98wEcffcSWLVsYPHgw+/bto08fxy+pw4cPp1u3bs72Xbp0ITIykscee4y///6bZcuW8eyzz/L4449f8OR8cR9Hj8L1tybyc1wbqLCGUO8oVvRcSocaHcyOJiImsBiGYZTkBlNTUwkLCyMlJYXQ0NDLWseP//zInV/cScjJazg5cS2ffgqPPFLEQUVcVEZGBrt373beoE/kbBf7+SiK79+SMH36dF577TUSEhJo0KABkydP5oYbbgCgR48e7NmzhyVLljjbb926lQEDBvDbb78RGRnJAw88wLhx4wpcuLjLfiltjhyBG+48wNbmN0HkDsr6lWfJE/HUK1vP7GgiUoQK8x3s1ue45Pg7elyOHTMzjYiIFKW+ffvSt2/ffN+bNWvWefPq1Klz3vAycW9JSXDj7UlsbdEOIndQIagKy574iWplqpkdTURM5JaFS2yI4xyXDO/DYLFx/LjV5EQiIiJSFJKS4IYOx9l23S0QtY3YwDh+e3IJlcMrmx1NREzmlue4lA0siwULhsUGgcd0jouIiIgHSE2FdnecZFvT2yBmI1H+0Sx9/CcVLSICuGnh4mP1ISowyvEiOFGFi4iIiJvLyIC77slmU917oeIqwn0j+OWxxdSMPP+S1iJSOrll4QJ5b0KpwkVERMR92WzwaFeDpcFPQfV4/K2BLOq2gAblGpgdTURciNsWLrnnuRCcoMJFSqUSviCguAn9XIg7GjQI5h4eD9d8iBdefPXAHK6tcK3ZsUTExbht4aIeFymtfHx8AEhPTzc5ibii3J+L3J8TEVc3fTq8teQLaDsCgDdvf5M7a91pcioRcUVueVUxgJigfwuXY1vMzSJSkqxWK+Hh4SQlJQGOmyVaLBaTU4nZDMMgPT2dpKQkwsPDsVp1tUVxfT//DANeXQuPPQbAMy2eoe+1+V8KW0TEfQuXs3pcTpwAux283Lb/SKRwYmIcP/+5xYtIrvDwcOfPh4gr27EDOnc9gv3Be8A7kztr3slrt7xmdiwRcWHuX7iEJGAYkJICZcqYm0mkpFgsFmJjYylXrhzZ2dlmxxEX4ePjo54WcQtpadCxUzYptzwAYfupGVGLTzt/ipdFf4EUkQtz28Il9+R8S2gCBnD8uAoXKX2sVqt+URURt2IY0Ls3bI17DqouIcgnmG8f+oYw/zCzo4mIi3PbP23k3sfFEuA4M18n6IuIiLi+996Dz9fPhRZTAPjkno+pV7aeuaFExC24beES5uf4y4zhlwIYKlxERERc3Lp1MOCl3dDpCQCea/kc99S9x+RUIuIu3LdwOdOlbHhlg89pFS4iIiIuLCUF7nswi+xOD4F/Ci0qtmDczePMjiUibsRtC5dg3+B/T+LzS+HYMXPziIiIyIX16we7q70AFVcR7leGL+79Ah+r7jckIgXntoWLl8WLUL9Qxwv/FPW4iIiIuKgvvoDPVv4PWk4EYNbdM6kcXtnkVCLibty2cIF/z3PBT4WLiIiIK9q3D3oPPgqdHgdgQLMBdKrTyeRUIuKO3LpwCfcPdzzxT1bhIiIi4mJsNujazeBkm14Qkki9qHr8p91/zI4lIm7KrQsX5zXfNVRMRETE5UyeDMtS/gt1v8HHy4dPO39KgE+A2bFExE25d+GioWIiIiIuads2GPH6brhtIABj2ozh6tirTU4lIu7MrQsXDRUTERFxPTYbPPa4nazbHge/k7SKa8VzrZ4zO5aIuDm3LlycPS7+uhyyiIiIq5g2DX7PeheqLiHAO5D/3v1frF5Ws2OJiJtz78LFP+9QMbvd3DwiIiKl3Y4dMPw/e+AWRw/LhHbjqR5R3dxQIuIR3LtwOavHxW6HkyfNzSMiIlKaGQY88aRBZvue4JdG60qt6d+sv9mxRMRDuHXhknuOi1dgMoDOcxERETHRzJmw7OQHUH0xflZ/PrzrQ7wsbv2rhoi4ELf+NskdKuYdnAKocBERETFLUhIMGXUI2g8F4NW2r1AzsqbJqUTEk7h34XJmqJhXgAoXERERMz3zDKS0HAj+qVxbvhlPN3/a7Egi4mHcunDJHSpm+CUDKlxERETMEB8Pn67+P6g3F6vFmw/uel9XERORIufWhUvuUDG7j6PHRZdEFhERKVkZGdB7YCrc0Q+AZ1sOpVF0I5NTiYgncu/C5cxQsRxrKljs6nEREREpYa+/DrurvgihB6kaVp2RN440O5KIeCi3LlycQ8UsdvBNU+EiIiJSgvbsgXEfrYFmbwHw3l3vEOATYG4oEfFYbl24+Hv74+Pl43hx5iaUIiIiUjIGD7GRdUtfsBg80vAR2lVrZ3YkEfFgbl24WCwW53ku+KtwERERKSkLF8K3+9+HCqsJ9g7ljfZvmB1JRDycWxcu8O95LupxERERKRmZmfDU0CPQ9gUAXmk3lpjgGJNTiYinc/vCJfc8F/yTVbiIiIiUgDffhN3Vh0HACRqWbUzfa/uaHUlESgG3L1w0VExERKTkHDkCoz74Ha6eCcC7Hafj7eVtcioRKQ3cv3A5a6jYsWNgGObmERER8WQjR9lJv2EgAD2ueowWcS1MTiQipYXbFy5nDxXLyYG0NFPjiIiIeKy//oJ3//gvVFhDoDWECe3Gmx1JREoRty9ccntcrEEpABw7ZmYaERERz2QY8PRzqRhthwMw5uaRRAdHm5xKREoT9y9czpzj4hfqKFx0nouIiEjRW7AAfs4ZB8GHqRJSk4HNB5odSURKGbcvXHKHivmEqMdFRESkONhsMGjsdrhuCgBv3TkZX6uvuaFEpNRx+8Ll36FiyYAKFxERkaL22WfwT6XnwJpNu8q3cUetO8yOJCKl0BUVLuPHj8disTBo0KAiilN4Z18OGVS4iIiIFKXMTHju7eVQ91sseDH1jjfMjiQipdRlFy6rV6/mvffeo1GjRkWZp9Bye1wMXxUuIiIiRW36dIPDjYYC8PhVPalXtp7JiUSktLqswiUtLY1HHnmE999/nzJlyhR1pkLJPcclxzsZUOEiIuLupk+fTtWqVfH396dJkyYsX768QMv99ttveHt707hx4+INWIqkpMDIL7+EiqvwswQxrt1osyOJSCl2WYVLv379uOOOO2jXrt0l22ZmZpKamppnKkq5Q8WyLLqqmIiIu5szZw6DBg1ixIgRrF+/ntatW3Pbbbexb9++iy6XkpJCt27daNu2bQklLR0mvJFJWnPH5Y+Htx5GTHCMyYlEpDQrdOEye/Zs1q1bx/jxBbvp1Pjx4wkLC3NOcXFxhQ55MblDxTJJA68c9biIiLixSZMm8cQTT/Dkk09St25dpkyZQlxcHDNmzLjocr1796ZLly60aKG7uBeVo0dh0vK3ocxuynjHMrTVELMjiUgpV6jCZf/+/Tz99NN8+umn+Pv7F2iZ4cOHk5KS4pz2799/WUEvxHlyPoBfqgoXERE3lZWVxdq1a2nfvn2e+e3bt2fFihUXXG7mzJns3LmTUaNGFWg7xT0SwFO88kYqWc1fBeC128YS5BtkciIRKe28C9N47dq1JCUl0aRJE+c8m83GsmXLeOutt8jMzMRqteZZxs/PDz8/v6JJmw9fqy8B3gGczjkNfikcOxZRbNsSEZHic/ToUWw2G9HRee/GHh0dTWJiYr7LbN++neeff57ly5fj7V2wQ9r48eMZM2bMFef1ZElJ8Pa6ydDqGBX8atOjcXezI4mIFK7HpW3btmzatIkNGzY4p6ZNm/LII4+wYcOG84qWknL2JZHV4yIi4t4sFkue14ZhnDcPHH8469KlC2PGjKFWrVoFXn9xjwTwBC+/fozsphMBmHjny3h7FervnCIixaJQ30QhISE0aNAgz7ygoCAiIyPPm1+Swv3DSUxLBL8UkhMhJwcK+Ic3ERFxEVFRUVit1vN6V5KSks7rhQE4efIka9asYf369fTv3x8Au92OYRh4e3uzaNEibr755vOWK+6RAO7u8GF4d/N/oPlJqgZcxf317zM7kogIcIU3oHQVuSfo458MwIkT5mUREZHL4+vrS5MmTYiPj88zPz4+npYtW57XPjQ09LxRAH369KF27dps2LCB5s2bl1R0j/LSa4fIueYtAKZ1egUvi0f8qiAiHuCK+yWWLFlSBDGuTO5QsYAyKZzGcS+XsmXNzSQiIoU3ZMgQunbtStOmTWnRogXvvfce+/bto0+fPoBjmNfBgwf5+OOP8fLyOq+3v1y5cvj7+5s6CsCdHT4MH21/BZqcpm5wC+6odbvZkUREnDxiQFXuTSgDIxyFi+7lIiLinh588EGOHTvGyy+/TEJCAg0aNGD+/PlUrlwZgISEhEve00Uu3+jJ+7A1fh+At+55Jd9zi0REzOIRhUvuUDG/sGQAnaAvIuLG+vbtS9++ffN9b9asWRdddvTo0YwePbroQ5UCx47BB9smQONsGoXcxM3VbjI7kohIHh4xcDW3cPENSQFUuIiIiBTW2Gn7yWn4AQBT7ynYPXFEREqSZxQuZ85xsQapcBERESmslBSYsWkCWLOpH9SGNlVvNDuSiMh5PKJwyT3HxeKvwkVERKSwxr99gKz6jt6WaZ3V2yIirskjCpfcoWJ2v2RAhYuIiEhBnToFU9dOAO8s6vjfyM3V2pgdSUQkX55RuJwZKmbzVo+LiIhIYbzx3kEy6jmuJPbmveptERHX5RGFS+5QsSyLChcREZGCysqC1397HbyzqOnbmrbV25gdSUTkgjyicMkdKpZBMqD7uIiIiBTEW7MOc6rOuwBMvucl3bdFRFyaZxQuZ4aKpdvU4yIiIlIQNhuM+2ki+GRQyas5t9duZ3YkEZGL8ojCJXeoWKY9A6yZKlxEREQuYeaco5yoMR2AiZ3U2yIirs8jCpcQ35B/X/inkJEB6enm5REREXFlhgEjvp8CvqeINa7h3oa3mx1JROSSPKJwsXpZiQiIcDwPPQxouJiIiMiFfP1DMklV3gRgwu0vqrdFRNyCRxQuAFXCqwAQErcHUOEiIiJyIUO/mgb+qUTaGvDotZ3MjiMiUiAeU7hUDa8KgF/sbkCFi4iISH4W/pLGvtipAIy9ZQReFo/5VUBEPJzHfFvlFi7WSBUuIiIiFzLw43cg8DhhOTXp1ep+s+OIiBSYxxQuuUPF7KGOwkX3chEREcnrt1Wn+SdyIgDDWz+P1ctqciIRkYLzmMKlahlHj0tW4B5APS4iIiLn6v/BRxCSSGB2JQa3fdTsOCIiheI5hcuZoWJpPrsBQ4WLiIjIWTZuzmZD0GsADL72OXytviYnEhEpHI8pXCqHVwYgy5IKASdUuIiIiJzlqemfQvg+/LKjGXH742bHEREpNI8pXAJ9AokOina8CN+jwkVEROSM7TtzWGF9FYBeDZ8hwCfA5EQiIoXnMYUL/HueC+G7VbiIiIic0eetORC5A5/sSF695ymz44iIXBbPKlzOnOdCGRUuIiIiAAcO2vk55xUAutcaTLBvsMmJREQuj0cVLrmXRFaPi4iIiMNT0+ZC1Bas2eG88UB/s+OIiFw2jypcnD0u4Xs4cQLsdnPziIiImCnpiJ0f08YB8GCVgYT5h5mcSETk8nlW4VLm36FihgHJyabGERERMVX/N7/DKLcRr5xgpnV52uw4IiJXxKMKl3+Hiu1B93IREZHSLCXFYN5RR2/L3bEDiAyMMDmRiMiV8ajCpVJYJSxYwOc0BCWpcBERkVLrmenx2KLXYskJZHrXwWbHERG5Yh5VuPhafakYWtHxQlcWExGRUiojAz7e47hvyy0RvYgOKWtyIhGRK+dRhQvkvbLYvn2mRhERETHFS+/9Rnb5pWDz4Z1uz5gdR0SkSHhc4fLvTSj3sGqVuVlERERKWk4OvL1xPAAtArtTNbKiyYlERIqG5xUuZ92E8o8/zM0iIiJS0l7/5E9Ox/0Idi/e6fqc2XFERIqMxxUuZw8V27oVTpwwNY6IiEiJMQz4z28TAGhofYBGFWqanEhEpOh4XOGS2+PiXXY3AKtXm5lGRESk5Lz39XZSKnwJwPSHh5ucRkSkaHle4XLmHBd7yD6w2DRcTERESgXDgJcWTgAvO9Vtd3B9zUZmRxIRKVIeV7hUCKmAt5c3dks2hBxS4SIiIqXCF/P3cqT8xwBMu+9Fk9OIiBQ9jytcrF5WKoVVcrw4c4K+YZibSUREpLg993+vgzWHilk3c3uj68yOIyJS5DyucAGoGeE4GdFaeyEnTsD27SYHEhERKUbfL0ngYPQHALx+l3pbRMQzeWTh0rtJbwCM5pM1XExERDzeoNmTwDuTcpkteLBZG7PjiIgUC48sXO6uczct41pit56GNqNZudLsRCIiIsVjyapj7IqcAcC49i9isVhMTiQiUjw8snCxWCy81u41x4urP+SXv/42N5CIiEgx6T1rIvieIjyjMU/eeJvZcUREio1HFi4ArSq1on2lTuBlZ0uF4aSnm51IRESkaP30+1H+CX8TgLFtR6m3RUQ8mscWLgBT7pwAdivU/o6PflpudhwREZEi1fu/b4BfGmUyrqZf205mxxERKVYeXbjULVuHKieeAGDs+j5k5mSanEhERKRo/G9ZEjsj3wJgfPsx6m0REY/n0YULQK/qr0BaOZKMvxmxeLTZcURE5BKmT59O1apV8ff3p0mTJixffuEe83nz5nHLLbdQtmxZQkNDadGiBQsXLizBtObp+9nr4HuKqMym9Gpzp9lxRESKnccXLk/3jCJq5TsATFr5GqsOrjI5kYiIXMicOXMYNGgQI0aMYP369bRu3ZrbbruNffv25dt+2bJl3HLLLcyfP5+1a9dy00030bFjR9avX1/CyUvW9z8fZk/ZtwGYcJt6W0SkdLAYRsneVz41NZWwsDBSUlIIDQ0tkW1++SU8OOcRaPQ5NcLqsqn/Ovy9/Utk2yIirsKM79/Cat68Oddccw0zZsxwzqtbty53330348ePL9A66tevz4MPPsjIkSML1N4d9svZDAMqPPE0CZWnUS6rOYnjflfhIiJuqzDfwR7f4wJw//3QMmUapEWzI2ULo5eMNjuSiIicIysri7Vr19K+ffs889u3b8+KFSsKtA673c7JkyeJiIi4YJvMzExSU1PzTO7kjf/+TUKco7dlaqdxKlpEpNQoFYWLxQJvvxGJ5Yd3AXhjxRtsSNxgbigREcnj6NGj2Gw2oqOj88yPjo4mMTGxQOuYOHEip06d4oEHHrhgm/HjxxMWFuac4uLirih3SUpLM3jpt8HgZaOupRMPNWtndiQRkRJTKgoXgMaNodeNnWDz/dgMG09+1xOb3WZ2LBEROce5PQiGYRSoV+GLL75g9OjRzJkzh3Llyl2w3fDhw0lJSXFO+/fvv+LMJaXXGz+SWXER2Hz58sk3zI4jIlKiSk3hAjBuHJT5YypkhLE2YQ1vr37b7EgiInJGVFQUVqv1vN6VpKSk83phzjVnzhyeeOIJvvzyS9q1u3gvhJ+fH6GhoXkmd7BjdxazTwwGoFO5wTQoX8PkRCIiJatUFS5RUTDt1VhYPAGA4YtHsD/Fff7SJiLiyXx9fWnSpAnx8fF55sfHx9OyZcsLLvfFF1/Qo0cPPv/8c+64447ijmma+16fhhGxA5/MGD5+coTZcURESlyhCpcZM2bQqFEj51+oWrRowf/+97/iylYsHnkE2kf2gn0tSc9Jo//8AWZHEhGRM4YMGcIHH3zARx99xJYtWxg8eDD79u2jT58+gGOYV7du3Zztv/jiC7p168bEiRO57rrrSExMJDExkZSUFLM+QrH47PsD/Bk+BoARzccT6h9iciIRkZJXqMKlYsWKTJgwgTVr1rBmzRpuvvlmOnXqxObNm4srX5GzWODdd7zwj38PbD5898//sWDHArNjiYgI8OCDDzJlyhRefvllGjduzLJly5g/fz6VK1cGICEhIc89Xd59911ycnLo168fsbGxzunpp5826yMUudOnofe8IeCXRmxOC166q9ulFxIR8UBXfB+XiIgIXn/9dZ544okCtXeV6+VPmQKD/zcUWk6kZpm6/N1/I95e3qblEREpbq7y/etqXH2/PPziQmb73Ap2K791X0vLaleZHUlEpMiUyH1cbDYbs2fP5tSpU7Ro0eJyV2OaAQPgqpQX4VQU209s4d0175odSUREJI91GzOYfbIfAB3LDVTRIiKlWqELl02bNhEcHIyfnx99+vThm2++oV69ehds76o3+rJaYcbkcPhlLAAvLB7JidMnzA0lIiJyht0Od0+cABE78c8qz6dPjjY7koiIqQpduNSuXZsNGzbwxx9/8NRTT9G9e3f+/vvvC7Z35Rt9tWgBj9Z7Eg43IDX7OGOWvmx2JBEREQCen/QX+yuNB2Bi+8mE+rneMDYRkZJ0xee4tGvXjurVq/Puu/kPtcrMzCQzM9P5OjU1lbi4OJcZS5yQANVuiSfj/vZY8ebv/pupFVnL7FgiIkXO1c/lMIsr7pc//8ri6unNMKL/pIHf7Wwc9kOBbsIpIuJuSuQcl1yGYeQpTM7l6jf6io2Fl7vfAtvuxEYOLywaZXYkEREpxbKz4dYJozGi/8QnO5JF/T5U0SIiQiELlxdeeIHly5ezZ88eNm3axIgRI1iyZAmPPPJIceUrEU8/DZV3jgNg7j+z2Xh4o8mJRESktOrz6m8kVv8PAG/f+h6xITEmJxIRcQ2FKlwOHz5M165dqV27Nm3btmXlypUsWLCAW265pbjylQhfX5j8/FXw14MAPPe/kSYnEhGR0uiXFSf56EQ38LJzQ2h3el7f2exIIiIu44rPcSksVxxLDGAY0LjdVjZeXx+87Kx8ciXNKjQzO5aISJFx1e9fs7nKfjlxwiDu6a6cqv4ZgVmVOPjiRsIDwkzLIyJSEkr0HBdPYbHA1JfqwJ+OOxIP+eFFkxOJiEhpYRjQ7tmPOFX9M7Bbmffo5ypaRETOocLlLG3awA2MBJsPvyXGs3TPUrMjiYhIKTBi2ibWxfQHoF/dcXSo28rkRCIirkeFyzmmjq4K654EYOgPo80NIyIiHu/XVWmM3/EA+GRQx/tWpj34nNmRRERckgqXczRuDHdFDAebD2uOLVGvi4iIFJvDhw06vN0Torbin1WepU9/jJdFh2YRkfzo2zEfr78YB+ufAGDoD2NMTiMiIp4oOxtaPjOJ9Gqzwe7N3C6zKRdc1uxYIiIuS4VLPmrVgs5lc3tdfmH53uVmRxIREQ9z/7DF7KruGBY2oslkbq/f2uREIiKuTYXLBYx/vhKsfxyAZ75Xr4uIiBSd19/fzf/5PAhedm6O6MHYjv3MjiQi4vJUuFxArVpwz5lel9XHfuK3fb+ZHUlERDzAqXQ7L6zrAoHHKU9TfnxqBhaLxexYIiIuT4XLRUx4vjJs6AHA0O9fNjeMiIh4hJ7vvE9OzB9YskJY3n8u/t7+ZkcSEXELKlwuolYtuDtqONit/HF0EWsPrTU7koiIuLFDKYeZc/R5ADqHv0K1yEomJxIRcR8qXC5hwrCqsOlhAIb/OMHkNCIi4s4e/OgZ7H7JWA834aM+fc2OIyLiVlS4XELt2tAheBgA8Qfnsu3oNpMTiYiIO1q88yd+Tf0MDAuPl3uH0BCr2ZFERNyKCpcCmDCkAWzrCBaDlxa8bnYcERFxM4Zh8MTcAQBY1/Xj1f5NTU4kIuJ+VLgUQOPG0MI2HIC5Oz7mQOoBcwOJiIhbOXzqMPtObwHDwhNVxxIVZXYiERH3o8KlgF4f2AL23Ijdks3L8ZPMjiMiIm5kw4Ezw4xPVGXY0+GmZhERcVcqXAqoVStokOzodZm16T2Onz5uciIREXEXv23bCoBPam2qVTM5jIiIm1LhUgiv924PCY3Jtpxi4rIZZscRERE3sWG/o8cl0qhjchIREfelwqUQOnSwUPngswBM/WMap7NPm5xIRETcwfYTjh6XSoG1TU4iIuK+VLgUgsUCLz90PyRX5hRJfLj2Y7MjiYiIGziU5ehxqVtOPS4iIpdLhUshPfyAD2W2DgFg7OI3sNltJicSERFXlpGTwUnrbgCaVlGPi4jI5VLhUkg+PvB8hycgPYIk2w7m/f2t2ZFERMSF7Ti+AywGZITRtE602XFERNyWCpfL0PfJIPw39QPghfn/wTAMkxOJiIir2njIcX4LR2tTs6bF3DAiIm5MhctlCA6G3lf3h2x/dpxezdI9y8yOJCIiLuqPHY7zW3xS6xARYXIYERE3psLlMj0/oBxem3oA8OL8ieaGERERl5Xb41LOqzYWdbiIiFw2FS6XKSYG7i0/GAwLvx39nq1Ht5odSUREXNCuFEePS9UQXVFMRORKqHC5AmMG1oJtdwEwasEkk9OIiIirMQyDxBzHH7bqx+iKYiIiV0KFyxWoWxdaGEMBmLvjYw6nHTY5kYiIuJLEtESyvU6C3Ytrq9cwO46IiFtT4XKFxvVqBQeaY7Nk8vqyt82OIyIiLsQ5jPhENerW9DM3jIiIm1PhcoVuuslClYRnAJi++m3Ss9NNTiQiIq7ir0TH+S0cq03NmuZmERFxdypcrpDFAi8/fA+cqMppjvP+6llmRxIRERexarejx8U3tQ5RUSaHERFxcypcisBDD3gTtnUQAK/+Mhmb3WZuIBERcQl/n+lxifHWpZBFRK6UCpci4OMDz7Z9HE6Hk5Szg++2fW92JBERcQF70hw9LtXDdSlkEZErpcKliPTrGYzPxj4AvPS/N0xOIyIiZjudfZrj9r0AXFVBl0IWEblSKlyKSHg4dK01AGw+bD75GysPrDQ7koiImGj78e1gMeB0GRpVL2t2HBERt6fCpQiNGFgeNnUBYNTCiSanERERMyWmJTqepFakZk2d4CIicqVUuBShatWgbeAQABbtn8vuE7tNTiQi4n6mT59O1apV8ff3p0mTJixfvvyi7ZcuXUqTJk3w9/enWrVqvPPOOyWU9OKOnEx2PDldhhq696SIyBVT4VLExvRtBDvaY1jsjP9litlxRETcypw5cxg0aBAjRoxg/fr1tG7dmttuu419+/bl23737t3cfvvttG7dmvXr1/PCCy8wcOBA5s6dW8LJz7fzYDIA1pxwoqPNzSIi4glUuBSxli2h1jHHDSn/u+kjkjOSzQ0kIuJGJk2axBNPPMGTTz5J3bp1mTJlCnFxccyYMSPf9u+88w6VKlViypQp1K1blyeffJLHH3+cN94w/yIpB44mAxDsHa5LIYuIFAEVLkXMYoHRXW+BpPpkkcY7qz4wO5KIiFvIyspi7dq1tG/fPs/89u3bs2LFinyX+f33389r36FDB9asWUN2dna+y2RmZpKamppnKg5H05IBCPQKL5b1i4iUNipcisF991kos9Vxrsvry6aRbcv/4CkiIv86evQoNpuN6HPGVUVHR5OYmJjvMomJifm2z8nJ4ejRo/kuM378eMLCwpxTXFxc0XyAc5w4nQw4elxEROTKqXApBj4+8Ez7LpBWjuO2/Xz9t/ljrUVE3IXlnHFVhmGcN+9S7fObn2v48OGkpKQ4p/37919h4vzlDhUO9Q0vlvWLiJQ2KlyKSd9e/vhs6AvAmEWTnAdSERHJX1RUFFar9bzelaSkpPN6VXLFxMTk297b25vIyMh8l/Hz8yM0NDTPVBxOZicDEO4fXizrFxEpbVS4FJMyZeDROk9Bjh/b0lazYn/+47NFRMTB19eXJk2aEB8fn2d+fHw8LVu2zHeZFi1anNd+0aJFNG3aFB8fn2LLWhBptmQAIgLCTc0hIuIpVLgUo+EDy8HGrgC8HD/J5DQiIq5vyJAhfPDBB3z00Uds2bKFwYMHs2/fPvr06QM4hnl169bN2b5Pnz7s3buXIUOGsGXLFj766CM+/PBDhg4datZHcDptTwYgKiTc1BwiIp7C2+wAnqxmTWjjN5glfED8/m/ZdWIX1cpUMzuWiIjLevDBBzl27Bgvv/wyCQkJNGjQgPnz51O5cmUAEhIS8tzTpWrVqsyfP5/Bgwfz9ttvU758eaZNm8a9995r1kdwyrSkAFAuNNzcICIiHsJilPDJF6mpqYSFhZGSklJs44pdyS+/wM0f3gY1F9DrqoG8e/dUsyOJSClV2r5/C6q49ov1pSDs3um8U2cXvR+sWmTrFRHxJIX5DtZQsWLWpg1UO+y4NPKsPz/UDSlFREqBLFsWdu90AMpHhJsbRkTEQ6hwKWYWC4x4uB0cbkAWp3h39ftmRxIRkWKWkpHifF6hrHq3RESKggqXEtCli4WQv3RDShGR0sLZu54ZQmQZq6lZREQ8hQqXEuDvDwNu6gJp0RzLOcDXf39tdiQRESlGh1OTHU8ywilTxtQoIiIeQ4VLCRnwlB9ea/sBMPYn3ZBSRMSTHTia7HiSEU5wsKlRREQ8hgqXEhITA/dV7gPZ/mxJWcOyvcvMjiQiIsUk4UQyAN454XjpSCsiUiQK9XU6fvx4rr32WkJCQihXrhx3330327ZtK65sHmfYgLKwoQcAr/wy0dwwIiJSbA4nJwPgaw83NYeIiCcpVOGydOlS+vXrxx9//EF8fDw5OTm0b9+eU6dOFVc+j3LNNdA0ZzAYFuL3fc+WI1vMjiQiIsXgyMlkAPwJNzWHiIgnKVThsmDBAnr06EH9+vW56qqrmDlzJvv27WPt2rXFlc/jjOhTC7Z2AuC15ZNMTiMiIsXh6KlkAAK9wk3NISLiSa5o5G1KiuM69REREUUSpjTo2BHK7xkKwKebPiYxLdHkRCIiUtROnE4GINg73NQcIiKe5LILF8MwGDJkCNdffz0NGjS4YLvMzExSU1PzTKWZ1QrDurSC/S3IIYs3V75ldiQRESliKZnJAIT5hZuaQ0TEk1x24dK/f382btzIF198cdF248ePJywszDnFxcVd7iY9xmOPQcB6R6/LtN+ncypL5wiJiHiSk9nJAIT7h5uaQ0TEk1xW4TJgwAC+++47fvnlFypWrHjRtsOHDyclJcU57d+//7KCepKQEOhzUyc4VoM02wk+WPeB2ZFERKQIpdscQ6kjg8LNDSIi4kEKVbgYhkH//v2ZN28eP//8M1WrVr3kMn5+foSGhuaZBJ4eYMXyxzMAjF/2Blm2LJMTiYhIUTltJAMQFRxuag4REU9SqMKlX79+fPrpp3z++eeEhISQmJhIYmIip0+fLq58HqtyZehcrQecjOHw6QN8tvEzsyOJiEgRybQkA1AuNNzUHCIinqRQhcuMGTNISUmhTZs2xMbGOqc5c+YUVz6PNuwZf/h9CADjlk7AZreZnEhERIpCtjUZgNgy4abmEBHxJIUeKpbf1KNHj2KK59muvRZa+feB0+HsSvmHb7Z+Y3YkERG5Qtm2bOzejouuVIgMNzeMiIgHuaL7uMiVe35wCKwcCMDYpa9iGIbJiURE5EqkZKY4n1eI0nmdIiJFRYWLyW6/HWqeGAhZgWxMWs/CnQvNjiQiIlfgeHqy40lmCFER3qZmERHxJCpcTOblBc8PjIS1vQF4eclY9bqIiLixQ8eTHU8ywggPNzOJiIhnUeHiAh55BMrtGAo5fvx+cAWLdy02O5KIiFymA0eTAbBkhuPvb24WERFPosLFBfj5weAny8MaR6/L6CWj1esiIuKmEpKTAfC2hZuaQ0TE06hwcRFPPQUhG5+HbH9WHFCvi4iIuzp8pnDxs4ebmkNExNOocHERYWEw8LFY57kuo5aMUq+LiIgbOpKWDIC/JdzUHCIinkaFiwsZNAgC1g6DbH9+P/A78bvizY4kIiKFdOxUMgBB1nBTc4iIeBoVLi4kKgr6PBoLa/oAMPKXkep1ERFxM8mnkwEI9g43NYeIiKdR4eJihg4Fn1XDIDuAlQdX8v0/35sdSURECiH3BpRhfuHmBhER8TAqXFxM+fLwxIMx8McgAF746QVsdpu5oUREpMDScpIBKOMfbmoOERFPo8LFBT33HFj/eA5Ol2Hzkc18uvFTsyOJiEgBnbIlAxAZHG5qDhERT6PCxQVVrQpPPBIOvz4PwMglI8nMyTQ3lIiIFEiGkQxAWRUuIiJFSoWLi3rxRfBZ3x9Sy7MvZR/vrHnH7EgiIlIAmV7JAJQLCzc1h4iIp1Hh4qLi4qD344GwdBQA45aPIzUz1eRUIiJyKTnWZADKlwk3NYeIiKdR4eLCXngB/LY8BkdrcTT9KK8uf9XsSCIichE59hzsPmkAlI8MNzeMiIiHUeHiwmJjoV8fH4h/HYDJf0xm94ndJqcSEZELSclIcT6PKxtmYhIREc+jwsXFDRsGgQc6wq6bybJl8fxPz5sdSURELuBwarLjSWYwURHepmYREfE0KlxcXLlyMPQZCyycBIaFLzd/yW/7fjM7loiI5OPgsWTHk4xwwtThIiJSpFS4uIGhQ6GccRWsewKAwQsHYzfsJqcSEZFzHTyaDIBXdhheOsKKiBQpfa26gZAQGDMG+GUslqxgVh9azSd/fmJ2LBEROUdCcjIAPrZwU3OIiHgiFS5u4oknoHaFGIylLwHwbPyznDh9wuRUIiJytqSUZAD8jHBTc4iIeCIVLm7CxwcmTAD+GITlaF2OpB/hxZ9fNDuWiIic5UhaMgD+lnBTc4iIeCIVLm6kUye4voUvxg/TAZixZgZrDq0xOZWIiOQ6dsrREx5sDTc3iIiIB1Lh4kYsFpg6FSx728DGLhgY9P2xLza7zexoIiICnMg4DkCod6TJSUREPI8KFzdzzTXQsyew6A28skNZfWg17619z+xYIiJX7MSJE3Tt2pWwsDDCwsLo2rUryWdOds9PdnY2w4YNo2HDhgQFBVG+fHm6devGoUOHSi70OVIyHT0uZQIiTMsgIuKpVLi4oVdegTI+sdgXjwVg2OJh7E/Zb3IqEZEr06VLFzZs2MCCBQtYsGABGzZsoGvXrhdsn56ezrp163jppZdYt24d8+bN459//uGuu+4qwdR5peY4elyiAlW4iIgUNd3W1w1FRcHYsdB/QD+sV83mZPnf6fNjH354+AcsFovZ8URECm3Lli0sWLCAP/74g+bNmwPw/vvv06JFC7Zt20bt2rXPWyYsLIz4+Pg88958802aNWvGvn37qFSpUolkP1u63VG4lAtR4SIiUtTU4+KmeveGRg2t2OZ9iJfhy/zt8/ls02dmxxIRuSy///47YWFhzqIF4LrrriMsLIwVK1YUeD0pKSlYLBbCw8OLIeWlZVgchUtMeBlTti8i4slUuLgpb2+YPh04Whf7z6MAeHrB0xxOO2xuMBGRy5CYmEi5cuXOm1+uXDkSExMLtI6MjAyef/55unTpQmho6AXbZWZmkpqammcqKlnejsKlYqR6XEREipqGirmxVq3gqadgxrvP4nv1VxyP2EDf+X35+v6vNWRMRFzC6NGjGTNmzEXbrF69GiDf7y3DMAr0fZadnc1DDz2E3W5n+vTpF207fvz4S2a6HDa7DbtvMgCVyqpwESlJNpuN7Oxss2NIPnx8fLBarUWyLhUubm78ePjuOx8OfjkTr97XMm/LPGZumMnjVz9udjQREfr3789DDz100TZVqlRh48aNHD58fo/xkSNHiI6Ovujy2dnZPPDAA+zevZuff/75or0tAMOHD2fIkCHO16mpqcTFxV10mYI4cTrZ+bxytIaKiZQEwzBITEy86BUIxXzh4eHExMRc8R/WVbi4ubAwePttuPvuxhg/j4O2zzPwfwNpXak1NSNrmh1PREq5qKgooqKiLtmuRYsWpKSksGrVKpo1awbAypUrSUlJoWXLlhdcLrdo2b59O7/88guRkZe+f4qfnx9+fn4F/xAFdPC441LIZIYQU9anyNcvIufLLVrKlStHYGCgRpy4GMMwSE9PJykpCYDY2NgrWp8KFw/QqRPcdx98PXcowY0WkFZ2CY/Me4TfHv8NH6sOniLi+urWrcutt95Kz549effddwHo1asXd955Z54ritWpU4fx48dzzz33kJOTw3333ce6dev44YcfsNlszvNhIiIi8PX1LdHPsDfJcX4LpyMIDCzRTYuUSjabzVm0FOSPFmKOgIAAAJKSkihXrtwVDRvTyfke4s03ITLCStonH+NvlGH1odWMWjLK7FgiIgX22Wef0bBhQ9q3b0/79u1p1KgRn3zySZ4227ZtIyUlBYADBw7w3XffceDAARo3bkxsbKxzKsyVyIrK/qOOwsU7OwL90Vek+OWe0xKovxS4vNx/oys9D0k9Lh4iJgbeew/uvTeOjK/fhfsfYMKvE2hdqTW31bzN7HgiIpcUERHBp59+etE2hmE4n1epUiXPa7MdPO4oXPzsOr9FpCRpeJjrK6p/I/W4eJDOneGxx4DN9xO8tTcGBo/Me4RdJ3aZHU1ExOMdTnUULgHoimIiIsVBhYuHmToVqlaFtK+mEpXRnBMZJ7j3y3tJz043O5qIiEc7kuYoXIKsKlxEpGS1adOGQYMGmR2j2Klw8TAhIfDJJ+Bl+HF0+teEeJVlQ+IGnvrxKZcaUiEi4mmOpTsKlzAfFS4ikj+LxXLRqUePHpe13nnz5jF27NgrytajRw9nDm9vbypVqsRTTz3FiRMnnG2OHz/OgAEDqF27NoGBgVSqVImBAwc6zz0sbipcPFCrVvDyy0BqRTI+mYMXXnz858dM/H2i2dFERDxWcqbj4B7ur8JFRPKXkJDgnKZMmUJoaGieeVOnTs3TvqAns0dERBASEnLF+W699VYSEhLYs2cPH3zwAd9//z19+/Z1vn/o0CEOHTrEG2+8waZNm5g1axYLFizgiSeeuOJtF4QKFw81fDh07AjZ228ibJWjYHk2/lm+2vyVyclERDzTyWxHj0tUoAoXEbMYBpw6VfJTQQe1xMTEOKewsDAsFovzdUZGBuHh4Xz55Ze0adMGf39/Pv30U44dO8bDDz9MxYoVCQwMpGHDhnzxxRd51nvuULEqVarw6quv8vjjjxMSEkKlSpV47733LpnPz8+PmJgYKlasSPv27XnwwQdZtGiR8/0GDRowd+5cOnbsSPXq1bn55pt55ZVX+P7778nJySnYTrgCKlw8lJcXfPwxVKsGJ+Y/TeXEAQB0/aYrv+771eR0IiKeJ83uKFzKhapwETFLejoEB5f8lF6EpxIPGzaMgQMHsmXLFjp06EBGRgZNmjThhx9+4K+//qJXr1507dqVlStXXnQ9EydOpGnTpqxfv56+ffvy1FNPsXXr1gLn2LVrFwsWLMDH5+L3BExJSSE0NBRv7+K/WLEKFw8WHg5z54K/v4W9706menYnMm2ZdJrdia1HC/6DKyIil5ZhcRQuMWG6HLKIXL5BgwbRuXNnqlatSvny5alQoQJDhw6lcePGVKtWjQEDBtChQwe++urio2huv/12+vbtS40aNRg2bBhRUVEsWbLkosv88MMPBAcHExAQQPXq1fn7778ZNmzYBdsfO3aMsWPH0rt378v5qIWm+7h4uMaN4aOPoEsXKztf+5zKo25i7+lVtP24LUt7LKVGRA2zI4qIeIQsq6NwKR+hHhcRswQGQlqaOdstKk2bNs3z2mazMWHCBObMmcPBgwfJzMwkMzOToKCgi66nUaNGzue5Q9KSkpIuusxNN93EjBkzSE9P54MPPuCff/5hwIAB+bZNTU3ljjvuoF69eowaVTI3PVePSynw8MPwyitAdiD7JvxAJf/6HDp5iJv+e5Pu8SIiUgQMw8Dm4yhcKpdV4SJiFosFgoJKfirKe2CeW5BMnDiRyZMn89xzz/Hzzz+zYcMGOnToQFZW1kXXc+4QL4vFgt1uv+S2a9SoQaNGjZg2bRqZmZmMGTPmvHYnT57k1ltvJTg4mG+++eaSw8mKigqXUmL4cHjySTDSynLkjZ+oElSHA6kHuOm/N7E3ea/Z8URE3NrJzDSwOk5MrRKtwkVEis7y5cvp1KkTjz76KFdddRXVqlVj+/btJbLtUaNG8cYbb3Do0CHnvNTUVNq3b4+vry/fffcd/v7+JZIFVLiUGhYLTJ8OHTrA6aPRHJ/8M5WCarEvZR/Xz7yezUmbzY4oIuK2Dh4/c5+DHD/Klw0wN4yIeJQaNWoQHx/PihUr2LJlC7179yYxMbFEtt2mTRvq16/Pq6++Cjh6Wtq3b8+pU6f48MMPSU1NJTExkcTERGw2W7HnUeFSivj4wNdfO+7zknoolpSpP1M1uC4HUg9w/czrdbUxEZHLtDvRMUyM0xEEBRXhmBERKfVeeuklrrnmGjp06ECbNm2IiYnh7rvvLrHtDxkyhPfff5/9+/ezdu1aVq5cyaZNm6hRowaxsbHOaf/+/cWexWKU8O3UU1NTCQsLc146TUpeaqqj5+WPP6BMhWNUHnYXG46vwM/qx+f3fk7nup3NjigixUDfv/kriv3y7qKf6fN7W7yP1yd76l9FnFBE8pORkcHu3bupWrVqiQ5XksK72L9VYb6D1eNSCoWGwoIFcO21cOJgJLvHLKZV1F1k2jK598t7GfnLSGz24u/uExHxFIdOOHpcfG26FLKISHFR4VJKhYXBokXQsiWkHAtg1TNzuS3Ccbm7scvGcsfnd3As/ZjJKUVE3ENCsqNwCUAn5ouIFBcVLqVYeDgsXgx33w3Zmd4seHoajwR8QoB3AAt3LqTJe01Yumep2TFFRFzekTRH4RLkpcJFRKS4qHAp5QICHCfs9+0LhgGfDXuUm3b+QbXw6uxN2Uub/7Zh0IJBpGenmx1VRMRlHUt3FC4hPipcRESKS6ELl2XLltGxY0fKly+PxWLh22+/LYZYUpKsVnjrLZg0yfF8/sxG+M5cx/3VngRg6sqpNH6nMYt3LTY5qYiIazqR4bgcchl/FS4iIsWl0IXLqVOnuOqqq3jrrbeKI4+YxGKBwYPhl18gNha2/hnK//q+T/8y8ykfUp7tx7dzyye3cPfsu9lxfIfZcUVEXMrJbEePS2SgChcRkeJS6MLltttuY9y4cXTurEvmeqLWrWHdOmjTBtLS4K2nb6Pmos30qDsQq8XK/237P+q9XY+B/xvI/pTiv163iIg7SLM7CpeywbqqmIhIcSn2c1wyMzNJTU3NM4lri4lxnLQ/aRL4+8PSheF8/cRUBvtvon21W8m2Z/PmqjepPq06vb7vxfZj282OLCJiqtM4CpeYMPW4iIgUl2IvXMaPH09YWJhziouLK+5NShGwWh1DxzZuhFatHL0vbwyry84x/2NMtZ9oU7kN2fZs3l/3PrXeqkX7T9ozb8s8sm3ZZkcXESlxWVZH4VIhQoWLiEhxKfbCZfjw4aSkpDin/fs1vMid1KwJy5bBzJmOnpidO2FUt5s5/c4vTKi1nNtr3I4FC/G74rn3y3upOLki/X7sx9I9S3UTSxEpNXJ8HIVLXJQKFxGR4lLshYufnx+hoaF5JnEvXl7Qowds3w4vvugYPrZyJTzf5XoOvPYjb1TaybPXDadcUDmSTiUxfc102vy3DRUnV+Tx/3ucrzZ/xYnTJ8z+GCIixSIjOxN8HJeMrxKtwkVELsxisVx06tGjx2Wvu0qVKkyZMqVA7XK3FxAQQJ06dXj99dcxDMPZ5s8//+Thhx8mLi6OgIAA6taty9SpUy87W1HxNjuAuI/gYBg7Fvr3d5z/Mn26YyjZM49XJSLiVR7tPoZ6d/7EypNf8s3Wb0hMS2TmhpnM3DATL4sXjWMa07pSa1pXak3LuJbEhsSa/ZFERK7YgWNn/jBj96JyjP44JyIXlpCQ4Hw+Z84cRo4cybZt25zzAgICSiTHyy+/TM+ePcnIyGDx4sU89dRThIaG0rt3bwDWrl1L2bJl+fTTT4mLi2PFihX06tULq9VK//79SyRjfizG2eVVAaSlpbFjh+NyuFdffTWTJk3ipptuIiIigkqVKl1y+dTUVMLCwkhJSVHvi5s7ftxRvLz3Hpw9ArBBA7jnviwq3bCULVkLWbDzf/x95O/zli8fUp6m5ZtyTcw1NIpuRMPohlQrUw0vi+6LKlIc9P2bvyvdL4v//Jtbvq0P6ZHYJxzFYimGkCJynoyMDHbv3k3VqlXx9/cHwDAMU26aHegTiKWQ//lnzZrFoEGDSE5Ods77/vvvGT16NJs3b6Z8+fJ0796dESNG4O3t6GsYPXo0H330EYcPHyYyMpL77ruPadOm0aZNG5YuXZpn/Rf6Fb9KlSoMGjSIQYMGOec1adKEKlWqMHfu3Avm7devH1u2bOHnn38u1OeE/P+tchXmO7jQPS5r1qzhpptucr4eMmQIAN27d2fWrFmFXZ24sYgIx9Cx4cNhwQJHATN/Pvz1F/z1ly9wC5Ur30L79m8w8KaDUGk5G5OX8+v+X/kr6S8OnTzEd9u+47tt3znXGegTSO3I2tSJqkPdqLrUiKjhnMoE6DKjIuJ69iY5zm+xZpdR0SJisvTsdILHB5f4dtOGpxHkG3RF61i4cCGPPvoo06ZNo3Xr1uzcuZNevXoBMGrUKL7++msmT57M7NmzqV+/PomJifz5558AzJs3j6uuuopevXrRs2fPAm/TMAyWLl3Kli1bqFmz5kXbpqSkEGHyBUgKXbi0adPmghWclE5WK9xxh2M6cQL+7//gq68cl1Teuxfefx/ef78C8BDVqz9Ey5bQ7dpTBFTdQGrwarYmb2BT0iY2J20mPTud9YnrWZ+4/rzthPuHUyW8ClXDq1I5rDKVwipRKawScWFxVAipQHRwNN5eGv0oIiXr4HFH4eJr0/ktInL5XnnlFZ5//nm6d+8OQLVq1Rg7dizPPfcco0aNYt++fcTExNCuXTt8fHyoVKkSzZo1AyAiIgKr1UpISAgxMTGX3NawYcN48cUXycrKIjs7G39/fwYOHHjB9r///jtffvklP/74Y9F82Muk3/KkSJUp4ziRv0cPOHXKcUWyRYscRczmzY6rku3cCZ98EgS0AlpRtSrUrQs31s0hovouLGW3cipgKwnZW9l5Ygc7ju8gIS2B5IxkNiRuYEPihny37WXxIiY4htjgWOdjdHA00UHRlAsqR7mgcpQNKktUYBRRgVEqckSkSCSmOAoXf0OFi4jZAn0CSRueZsp2r9TatWtZvXo1r7zyinOezWYjIyOD9PR07r//fqZMmUK1atW49dZbuf322+nYsaNzGFlhPPvss/To0YMjR44wYsQIbr75Zlq2bJlv282bN9OpUydGjhzJLbfcctmfryjoNzcpNkFBcNttjgkgOdlxNbIVK2DdOtiwAQ4cgN27HdP8+d5ArTPTXfj5QaVKUK8StKtyiqAKe7BG7iYneDfpvvtINvZxJHM/B07uJ+FkAjbDxqGThzh08lCB8oX6hRIZEElEQARlAspQxt8xhfuHE+YfRphfGKF+oYT6hRLiF0KIbwghfiEE+wYT5BNEkG8Qvlbf4tl5IuI2kk46CpcgLxUuImazWCxXPGTLLHa7nTFjxtC5c+fz3vP39ycuLo5t27YRHx/P4sWL6du3L6+//jpLly7Fx8enUNuKioqiRo0a1KhRg7lz51KjRg2uu+462rVrl6fd33//zc0330zPnj158cUXr+jzFQUVLlJiwsOhQwfHlOvoUUdPzJYtjmnbNkePzJ49kJnpuATz9u0AQUD9M9O/LBaIioJa5WyEV0giKPoQPhEJWEISsQclkOV7mAyvw6RxmJP2I6RkHeVE5jEMDFIzU0nNTGV38u7L/kzeXt4E+QQR4BNAoE8gAd4BBPgE4O/t75z8rH74efs5Hq1++Fp98bX64mP1wcfLx/nc28sbHy/H49mT1cvqeLRYsXpZz3v0snhhtZx5PPP67MmCJe9riyXP/Iu9tlgsRf4IXHSeiLs5espRuIR6q3ARkct3zTXXsG3bNmrUqHHBNgEBAdx1113cdddd9OvXjzp16rBp0yauueYafH19sdkKfw+9MmXKMGDAAIYOHcr69eudx+LNmzdz880307179zy9QGZS4SKmioqCG290TGfLyXFcqWzfPsd5Mnv3wsGD/04JCZCUBHY7HDkCR45YYXMsUIBLLFtsEHCCoKjjBEQew7/MMXxCT+AdfAJL4Aks/ikYvinYfFOweaeS43WSbK9UMkklm1Nk2E9iI8eR055DSmYKKZkpRb9zSjELZ4qZM8VN7vPc985+nl+7Sy1z7vLntj37dUGWyW+5S7U7d35Bt1MuqBy/P/E74jqSM0+AL4T5qXARkcs3cuRI7rzzTuLi4rj//vvx8vJi48aNbNq0iXHjxjFr1ixsNhvNmzcnMDCQTz75hICAACpXrgw4rha2bNkyHnroIfz8/IiKiirwtvv168d//vMf5s6dy3333cfmzZu56aabaN++PUOGDCExMREAq9VK2bJli+XzF4QKF3FJ3t5QtapjuhCbDY4dg8TE3OLFMR079u90/LjjggEnTkBKimO4WlaWFdKjOLUvilP7LjOgNQt8ToHvKcejz2nwPv3vo3fGWVOm49GaCdZsvHyz8PLJxMs7G4t3NhZrFhZrDlizsVizwZqDxSsHvHLAmuMotLxywMt25vmZR4sNw2I/89yOceYRiw0wnO8ZGIDx7/PctmfmO+c5X9vBcuYxT5uSuyiHI8s5l3LUNUEASE47bXYEOUdK1nHwhchAFS4icvk6dOjADz/8wMsvv8xrr72Gj48PderU4cknnwQgPDycCRMmMGTIEGw2Gw0bNuT7778nMjIScNybpXfv3lSvXp3MzMxCXUyrbNmydO3aldGjR9O5c2e++uorjhw5wmeffcZnn33mbFe5cmX27NlTpJ+7MAp9H5crpfsIiNkyMhwFzMmTkJrqmE6dgrQ0x7xTp/6d0tMd0+nTjseMDMfz06cdQ9kyMx3zsrIcU2am4zE729Fr5HnOKmAs9rNeF/TxzDrg/PfPnneh5+fNOydTvgXWhd7PZzu58t3ORZbJb5v5vpff/ItsJ592kWV8OLqxKZdD37/5u9L9Um5IB46ELeKx8P/y0dPdiiGhiOTnYvcGEddi2n1cRNydvz/ExDim4mS3O4qX7Ox/J5vt33m5z3NyHM9zX9vt/7622Ryvc+ed/dww8s4/+7Vh/Ps69zG/5+e2u9g8x2vLmQkMw+uSy5w7wcXfv9gy+S17ofVdbJkLvX+xeVf6/sWWyVXQ9Zz5w5q4kPY+L7NqwxO0eayZ2VFERDyaCheRYuLlBb6+jklEPNen/2kONDc7hoiIx/MyO4CIiIiIiMilqHARERERERGXp8JFRERERNxWCV9nSi5DUf0bqXAREREREbeTe7f49PR0k5PIpeT+G+X+m10unZwvIiIiIm7HarUSHh5OUlISAIGBgefd2FfMZRgG6enpJCUlER4ejtVqvaL1qXAREREREbcUc+beBrnFi7im8PBw57/VlVDhIiIiIiJuyWKxEBsbS7ly5cjOzjY7juTDx8fnintacqlwERERl3DixAkGDhzId999B8Bdd93Fm2++SXh4eIGW7927N++99x6TJ09m0KBBxRdURFyO1Wotsl+OxXXp5HwREXEJXbp0YcOGDSxYsIAFCxawYcMGunbtWqBlv/32W1auXEn58uWLOaWIiJhFPS4iImK6LVu2sGDBAv744w+aN3fchf7999+nRYsWbNu2jdq1a19w2YMHD9K/f38WLlzIHXfcUVKRRUSkhKnHRURETPf7778TFhbmLFoArrvuOsLCwlixYsUFl7Pb7XTt2pVnn32W+vXrF2hbmZmZpKam5plERMT1lXiPS+4NaHSgEBEpWbnfu654s7bExETKlSt33vxy5cqRmJh4weX+85//4O3tzcCBAwu8rfHjxzNmzJjz5uu4JCJS8gpzbCrxwuXkyZMAxMXFlfSmRUQEx/dwWFhYiWxr9OjR+RYJZ1u9ejVAvvdfMAzjgvdlWLt2LVOnTmXdunWFunfD8OHDGTJkiPP1wYMHqVevno5LIiImKsixqcQLl/Lly7N//35CQkIu6yZBqampxMXFsX//fkJDQ4shoWsr7Z8ftA9A+6C0f364vH1gGAYnT54s0RPY+/fvz0MPPXTRNlWqVGHjxo0cPnz4vPeOHDlCdHR0vsstX76cpKQkKlWq5Jxns9l45plnmDJlCnv27Ml3OT8/P/z8/Jyvg4ODdVy6QtoH2gel/fOD9gEU/7GpxAsXLy8vKlaseMXrCQ0NLbU/FKDPD9oHoH1Q2j8/FH4flFRPS66oqCiioqIu2a5FixakpKSwatUqmjVrBsDKlStJSUmhZcuW+S7TtWtX2rVrl2dehw4d6Nq1K4899liBM+q4VHS0D7QPSvvnB+0DKL5jk64qJiIipqtbty633norPXv25N133wWgV69e3HnnnXmuKFanTh3Gjx/PPffcQ2RkJJGRkXnW4+PjQ0xMzEWvQiYiIu5JVxUTERGX8Nlnn9GwYUPat29P+/btadSoEZ988kmeNtu2bSMlJcWkhCIiYia363Hx8/Nj1KhRecYnlyal/fOD9gFoH5T2zw+euQ8iIiL49NNPL9rmUledudB5LcXJE/8tCkv7QPugtH9+0D6A4t8HFsMVr4spIiIiIiJyFg0VExERERERl6fCRUREREREXJ4KFxERERERcXkqXERERERExOW5VeEyffp0qlatir+/P02aNGH58uVmRyoW48eP59prryUkJIRy5cpx9913s23btjxtDMNg9OjRlC9fnoCAANq0acPmzZtNSlz8xo8fj8ViYdCgQc55pWEfHDx4kEcffZTIyEgCAwNp3Lgxa9eudb7v6fsgJyeHF198kapVqxIQEEC1atV4+eWXsdvtzjaetA+WLVtGx44dKV++PBaLhW+//TbP+wX5rJmZmQwYMICoqCiCgoK46667OHDgQAl+itJHx6Z/edL/x4LQsan0HZtK23EJXOzYZLiJ2bNnGz4+Psb7779v/P3338bTTz9tBAUFGXv37jU7WpHr0KGDMXPmTOOvv/4yNmzYYNxxxx1GpUqVjLS0NGebCRMmGCEhIcbcuXONTZs2GQ8++KARGxtrpKammpi8eKxatcqoUqWK0ahRI+Ppp592zvf0fXD8+HGjcuXKRo8ePYyVK1cau3fvNhYvXmzs2LHD2cbT98G4ceOMyMhI44cffjB2795tfPXVV0ZwcLAxZcoUZxtP2gfz5883RowYYcydO9cAjG+++SbP+wX5rH369DEqVKhgxMfHG+vWrTNuuukm46qrrjJycnJK+NOUDjo26dikY1PpOjaVtuOSYbjWscltCpdmzZoZffr0yTOvTp06xvPPP29SopKTlJRkAMbSpUsNwzAMu91uxMTEGBMmTHC2ycjIMMLCwox33nnHrJjF4uTJk0bNmjWN+Ph448Ybb3QeHErDPhg2bJhx/fXXX/D90rAP7rjjDuPxxx/PM69z587Go48+ahiGZ++Dcw8OBfmsycnJho+PjzF79mxnm4MHDxpeXl7GggULSix7aaJjk45NOjbl5en7oDQflwzD/GOTWwwVy8rKYu3atbRv3z7P/Pbt27NixQqTUpWc3LtER0REALB7924SExPz7A8/Pz9uvPFGj9sf/fr144477qBdu3Z55peGffDdd9/RtGlT7r//fsqVK8fVV1/N+++/73y/NOyD66+/np9++ol//vkHgD///JNff/2V22+/HSgd+yBXQT7r2rVryc7OztOmfPnyNGjQwOP2hyvQsUnHJh2bSt+xScelvEr62ORdNLGL19GjR7HZbERHR+eZHx0dTWJiokmpSoZhGAwZMoTrr7+eBg0aADg/c377Y+/evSWesbjMnj2bdevWsXr16vPeKw37YNeuXcyYMYMhQ4bwwgsvsGrVKgYOHIifnx/dunUrFftg2LBhpKSkUKdOHaxWKzabjVdeeYWHH34YKB0/B7kK8lkTExPx9fWlTJky57Xx9O9KM+jYpGPTuUrDPijtxyYdl/Iq6WOTWxQuuSwWS57XhmGcN8/T9O/fn40bN/Lrr7+e954n74/9+/fz9NNPs2jRIvz9/S/YzpP3gd1up2nTprz66qsAXH311WzevJkZM2bQrVs3ZztP3gdz5szh008/5fPPP6d+/fps2LCBQYMGUb58ebp37+5s58n74FyX81k9eX+4gtL085dLxyYdm0rrsUnHpfyV1LHJLYaKRUVFYbVaz6vKkpKSzqvwPMmAAQP47rvv+OWXX6hYsaJzfkxMDIBH74+1a9eSlJREkyZN8Pb2xtvbm6VLlzJt2jS8vb2dn9OT90FsbCz16tXLM69u3brs27cPKB0/B88++yzPP/88Dz30EA0bNqRr164MHjyY8ePHA6VjH+QqyGeNiYkhKyuLEydOXLCNFB0dm3Rs0rHJoTQdm3Rcyqukj01uUbj4+vrSpEkT4uPj88yPj4+nZcuWJqUqPoZh0L9/f+bNm8fPP/9M1apV87xftWpVYmJi8uyPrKwsli5d6jH7o23btmzatIkNGzY4p6ZNm/LII4+wYcMGqlWr5vH7oFWrVuddavSff/6hcuXKQOn4OUhPT8fLK+/XlNVqdV52sjTsg1wF+axNmjTBx8cnT5uEhAT++usvj9sfrkDHJh2bdGxyKE3HJh2X8irxY1OhTuU3Ue4lJz/88EPj77//NgYNGmQEBQUZe/bsMTtakXvqqaeMsLAwY8mSJUZCQoJzSk9Pd7aZMGGCERYWZsybN8/YtGmT8fDDD7v1pfYK4uwrtxiG5++DVatWGd7e3sYrr7xibN++3fjss8+MwMBA49NPP3W28fR90L17d6NChQrOy07OmzfPiIqKMp577jlnG0/aBydPnjTWr19vrF+/3gCMSZMmGevXr3deWrcgn7VPnz5GxYoVjcWLFxvr1q0zbr75Zl0OuRjp2KRjk45NpevYVNqOS4bhWscmtylcDMMw3n77baNy5cqGr6+vcc011zgvwehpgHynmTNnOtvY7XZj1KhRRkxMjOHn52fccMMNxqZNm8wLXQLOPTiUhn3w/fffGw0aNDD8/PyMOnXqGO+9916e9z19H6SmphpPP/20UalSJcPf39+oVq2aMWLECCMzM9PZxpP2wS+//JLv//3u3bsbhlGwz3r69Gmjf//+RkREhBEQEGDceeedxr59+0z4NKWHjk0znW086f9jQenYVLqOTaXtuGQYrnVsshiGYRSuj0ZERERERKRkucU5LiIiIiIiUrqpcBEREREREZenwkVERERERFyeChcREREREXF5KlxERERERMTlqXARERERERGXp8JFRERERERcngoXKRWqVKnClClTzI5xxWbNmkV4eLjZMUREpAjo2CRSON5mBxDJT5s2bWjcuHGRfaGvXr2aoKCgIlmXiIiUTjo2iZhLhYu4LcMwsNlseHtf+se4bNmyJZBIRERKOx2bRIqPhoqJy+nRowdLly5l6tSpWCwWLBYLe/bsYcmSJVgsFhYuXEjTpk3x8/Nj+fLl7Ny5k06dOhEdHU1wcDDXXnstixcvzrPOc7vjLRYLH3zwAffccw+BgYHUrFmT77777qK5srKyeO6556hQoQJBQUE0b96cJUuWON/P7Sr/9ttvqVWrFv7+/txyyy3s378/z3pmzJhB9erV8fX1pXbt2nzyySd53k9OTqZXr15ER0fj7+9PgwYN+OGHH/K0WbhwIXXr1iU4OJhbb72VhISEQuxhEREpLB2bdGwSF2CIuJjk5GSjRYsWRs+ePY2EhAQjISHByMnJMX755RcDMBo1amQsWrTI2LFjh3H06FFjw4YNxjvvvGNs3LjR+Oeff4wRI0YY/v7+xt69e53rrFy5sjF58mTna8CoWLGi8fnnnxvbt283Bg4caAQHBxvHjh27YK4uXboYLVu2NJYtW2bs2LHDeP311w0/Pz/jn3/+MQzDMGbOnGn4+PgYTZs2NVasWGGsWbPGaNasmdGyZUvnOubNm2f4+PgYb7/9trFt2zZj4sSJhtVqNX7++WfDMAzDZrMZ1113nVG/fn1j0aJFxs6dO43vv//emD9/fp5ttGvXzli9erWxdu1ao27dukaXLl2K8p9ARETOoWOTjk1iPhUu4pJuvPFG4+mnn84zL/fg8O23315y+Xr16hlvvvmm83V+B4cXX3zR+TotLc2wWCzG//73v3zXt2PHDsNisRgHDx7MM79t27bG8OHDDcNwfHEDxh9//OF8f8uWLQZgrFy50jAMw2jZsqXRs2fPPOu4//77jdtvv90wDMNYuHCh4eXlZWzbti3fHLnb2LFjh3Pe22+/bURHR19wX4iISNHQsUnHJjGXhoqJ22natGme16dOneK5556jXr16hIeHExwczNatW9m3b99F19OoUSPn86CgIEJCQkhKSsq37bp16zAMg1q1ahEcHOycli5dys6dO53tvL298+SrU6cO4eHhbNmyBYAtW7bQqlWrPOtu1aqV8/0NGzZQsWJFatWqdcHcgYGBVK9e3fk6Njb2grlFRKRk6NikY5MUP52cL27n3CuwPPvssyxcuJA33niDGjVqEBAQwH333UdWVtZF1+Pj45PntcViwW6359vWbrdjtVpZu3YtVqs1z3vBwcHnredcZ887933DMJzzAgICLpr5QrkNw7jkciIiUnx0bNKxSYqfelzEJfn6+mKz2QrUdvny5fTo0YN77rmHhg0bEhMTw549e4o0z9VXX43NZiMpKYkaNWrkmWJiYpztcnJyWLNmjfP1tm3bSE5Opk6dOgDUrVuXX3/9Nc+6V6xYQd26dQHHX9oOHDjAP//8U6T5RUTkyunYpGOTmEs9LuKSqlSpwsqVK9mzZw/BwcFERERcsG2NGjWYN28eHTt2xGKx8NJLL13wr1OXq1atWjzyyCN069aNiRMncvXVV3P06FF+/vlnGjZsyO233w44/uI0YMAApk2bho+PD/379+e6666jWbNmgOMvcA888ADXXHMNbdu25fvvv2fevHnOK83ceOON3HDDDdx7771MmjSJGjVqsHXrViwWC7feemuRfiYRESkcHZt0bBJzqcdFXNLQoUOxWq3Uq1ePsmXLXnRM8OTJkylTpgwtW7akY8eOdOjQgWuuuabIM82cOZNu3brxzDPPULt2be666y5WrlxJXFycs01gYCDDhg2jS5cutGjRgoCAAGbPnu18/+6772bq1Km8/vrr1K9fn3fffZeZM2fSpk0bZ5u5c+dy7bXX8vDDD1OvXj2ee+65Av+FT0REio+OTTo2ibkshgYgihSJWbNmMWjQIJKTk82OIiIiAujYJJ5FPS4iIiIiIuLyVLiIiIiIiIjL01AxERERERFxeepxERERERERl6fCRUREREREXJ4KFxERERERcXkqXERERERExOWpcBEREREREZenwkVERERERFyeChcREREREXF5KlxERERERMTlqXARERERERGX9/9HfeeNG7sYLwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = NN_model(train_x)\n",
    "    test_preds = NN_model(test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 44]) torch.Size([800, 44]) torch.Size([200, 44]) torch.Size([200, 44])\n"
     ]
    }
   ],
   "source": [
    "from src.model.gaussian_process import MultiOutputGP\n",
    "\n",
    "# Hyperparams\n",
    "learningRate = 0.1\n",
    "weight_decay = 0\n",
    "optim_eps = 1e-8\n",
    "mean = \"Linear\"\n",
    "kernel = \"Linear\"\n",
    "\n",
    "epochs = 50\n",
    "eval_epoch_freq = 1\n",
    "in_size = train_x.shape[-1]\n",
    "out_size = train_y.shape[-1]\n",
    "device = \"cpu\"\n",
    "GP_model = MultiOutputGP(in_size, out_size, device, mean, kernel)\n",
    "mll = gpytorch.mlls.SumMarginalLogLikelihood(GP_model.likelihood, GP_model.gp)\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.Adam(\n",
    "    GP_model.parameters(),\n",
    "    lr=learningRate,\n",
    "    weight_decay=weight_decay,\n",
    "    eps=optim_eps,\n",
    ")\n",
    "# torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "dataset_size_gp = 1000\n",
    "gp_idx_split = int(test_split_ratio * dataset_size_gp)\n",
    "\n",
    "train_x_gp = train_x[gp_idx_split:dataset_size_gp, ...]\n",
    "train_y_gp = train_y[gp_idx_split:dataset_size_gp, ...]\n",
    "test_x_gp = test_x[:gp_idx_split, ...]\n",
    "test_y_gp = test_y[:gp_idx_split, ...]\n",
    "\n",
    "print(train_x_gp.shape, train_y_gp.shape, test_x_gp.shape, test_y_gp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.6525566577911377, R2 -442.6127014160156\n",
      "Eval loss 1.409150242805481, R2 0.5499072670936584\n",
      "epoch 1, loss 1.6070736646652222, R2 -370.25836181640625\n",
      "Eval loss 1.382038950920105, R2 0.5787969827651978\n",
      "epoch 2, loss 1.5723155736923218, R2 -303.2740173339844\n",
      "Eval loss 1.360946536064148, R2 0.6033591628074646\n",
      "epoch 3, loss 1.5456608533859253, R2 -246.81480407714844\n",
      "Eval loss 1.344666838645935, R2 0.6236982345581055\n",
      "epoch 4, loss 1.5247894525527954, R2 -199.2003936767578\n",
      "Eval loss 1.3321117162704468, R2 0.6399360299110413\n",
      "epoch 5, loss 1.508100986480713, R2 -158.91722106933594\n",
      "Eval loss 1.322338581085205, R2 0.6522696614265442\n",
      "epoch 6, loss 1.4944539070129395, R2 -124.80152130126953\n",
      "Eval loss 1.3146113157272339, R2 0.6609153151512146\n",
      "epoch 7, loss 1.483010172843933, R2 -96.12910461425781\n",
      "Eval loss 1.3082785606384277, R2 0.6662331819534302\n",
      "epoch 8, loss 1.4730991125106812, R2 -72.44343566894531\n",
      "Eval loss 1.3028409481048584, R2 0.6687151193618774\n",
      "epoch 9, loss 1.4642326831817627, R2 -53.42706298828125\n",
      "Eval loss 1.2981257438659668, R2 0.6689431667327881\n",
      "epoch 10, loss 1.4561302661895752, R2 -38.80351257324219\n",
      "Eval loss 1.2939013242721558, R2 0.6678144335746765\n",
      "epoch 11, loss 1.4485750198364258, R2 -28.196636199951172\n",
      "Eval loss 1.2900516986846924, R2 0.6662544012069702\n",
      "epoch 12, loss 1.4413737058639526, R2 -20.99057388305664\n",
      "Eval loss 1.286475658416748, R2 0.6651242971420288\n",
      "epoch 13, loss 1.4344271421432495, R2 -16.310503005981445\n",
      "Eval loss 1.283228874206543, R2 0.6649078130722046\n",
      "epoch 14, loss 1.427641749382019, R2 -13.175434112548828\n",
      "Eval loss 1.2802759408950806, R2 0.6657324433326721\n",
      "epoch 15, loss 1.4209481477737427, R2 -10.78738784790039\n",
      "Eval loss 1.2776182889938354, R2 0.6673374772071838\n",
      "epoch 16, loss 1.4142484664916992, R2 -8.736515998840332\n",
      "Eval loss 1.2751903533935547, R2 0.6693050861358643\n",
      "epoch 17, loss 1.4075227975845337, R2 -6.967267990112305\n",
      "Eval loss 1.272889494895935, R2 0.6711816787719727\n",
      "epoch 18, loss 1.4007548093795776, R2 -5.549572467803955\n",
      "Eval loss 1.2707120180130005, R2 0.6725630164146423\n",
      "epoch 19, loss 1.393979549407959, R2 -4.473941802978516\n",
      "Eval loss 1.268707513809204, R2 0.6731948852539062\n",
      "epoch 20, loss 1.3872807025909424, R2 -3.6485016345977783\n",
      "Eval loss 1.2669175863265991, R2 0.6731020212173462\n",
      "epoch 21, loss 1.3806785345077515, R2 -2.973177194595337\n",
      "Eval loss 1.2653746604919434, R2 0.6725804805755615\n",
      "epoch 22, loss 1.3741627931594849, R2 -2.4591078758239746\n",
      "Eval loss 1.2639826536178589, R2 0.6721362471580505\n",
      "epoch 23, loss 1.3676977157592773, R2 -2.154120922088623\n",
      "Eval loss 1.2626146078109741, R2 0.6721873879432678\n",
      "epoch 24, loss 1.3612651824951172, R2 -1.9979298114776611\n",
      "Eval loss 1.261228322982788, R2 0.6727672219276428\n",
      "epoch 25, loss 1.354844570159912, R2 -1.7037159204483032\n",
      "Eval loss 1.2599142789840698, R2 0.6735835671424866\n",
      "epoch 26, loss 1.3484059572219849, R2 -1.0864590406417847\n",
      "Eval loss 1.2586781978607178, R2 0.6742873787879944\n",
      "epoch 27, loss 1.3419257402420044, R2 -0.47039273381233215\n",
      "Eval loss 1.2575381994247437, R2 0.6746689677238464\n",
      "epoch 28, loss 1.3353824615478516, R2 -0.05845621973276138\n",
      "Eval loss 1.2564582824707031, R2 0.6746845841407776\n",
      "epoch 29, loss 1.3287948369979858, R2 0.18494068086147308\n",
      "Eval loss 1.2554266452789307, R2 0.674572765827179\n",
      "epoch 30, loss 1.3222228288650513, R2 0.23573073744773865\n",
      "Eval loss 1.254416823387146, R2 0.6747360825538635\n",
      "epoch 31, loss 1.3155977725982666, R2 0.1660180687904358\n",
      "Eval loss 1.253571629524231, R2 0.6749628186225891\n",
      "epoch 32, loss 1.3088501691818237, R2 0.10655531287193298\n",
      "Eval loss 1.2529793977737427, R2 0.6750012040138245\n",
      "epoch 33, loss 1.3021467924118042, R2 0.2974998354911804\n",
      "Eval loss 1.2524980306625366, R2 0.6748231649398804\n",
      "epoch 34, loss 1.295647144317627, R2 0.4608432650566101\n",
      "Eval loss 1.252460241317749, R2 0.674228847026825\n",
      "epoch 35, loss 1.2893348932266235, R2 0.4980684220790863\n",
      "Eval loss 1.252162218093872, R2 0.6742336750030518\n",
      "epoch 36, loss 1.2831858396530151, R2 0.45995962619781494\n",
      "Eval loss 1.2513391971588135, R2 0.674871027469635\n",
      "epoch 37, loss 1.2770949602127075, R2 0.5116732120513916\n",
      "Eval loss 1.2512761354446411, R2 0.6749265789985657\n",
      "epoch 38, loss 1.271125078201294, R2 0.5405891537666321\n",
      "Eval loss 1.251261830329895, R2 0.6750602126121521\n",
      "epoch 39, loss 1.2655284404754639, R2 0.5929330587387085\n",
      "Eval loss 1.2510515451431274, R2 0.6751936674118042\n",
      "epoch 40, loss 1.260231614112854, R2 0.6038808226585388\n",
      "Eval loss 1.2507433891296387, R2 0.6751765012741089\n",
      "epoch 41, loss 1.2550263404846191, R2 0.6279163956642151\n",
      "Eval loss 1.2508158683776855, R2 0.6751207113265991\n",
      "epoch 42, loss 1.2504777908325195, R2 0.6115236878395081\n",
      "Eval loss 1.2514760494232178, R2 0.6747332215309143\n",
      "epoch 43, loss 1.245993733406067, R2 0.6322929859161377\n",
      "Eval loss 1.2519159317016602, R2 0.6747274994850159\n",
      "epoch 44, loss 1.2418501377105713, R2 0.6314993500709534\n",
      "Eval loss 1.2524785995483398, R2 0.674299955368042\n",
      "epoch 45, loss 1.2381314039230347, R2 0.6267807483673096\n",
      "Eval loss 1.2531484365463257, R2 0.6738986372947693\n",
      "epoch 46, loss 1.2347809076309204, R2 0.6231033205986023\n",
      "Eval loss 1.2525051832199097, R2 0.6748554706573486\n",
      "epoch 47, loss 1.2315723896026611, R2 0.5648758411407471\n",
      "Eval loss 1.254289150238037, R2 0.6740771532058716\n",
      "epoch 48, loss 1.2297993898391724, R2 0.570385754108429\n",
      "Eval loss 1.2563202381134033, R2 0.6724655628204346\n",
      "epoch 49, loss 1.2301493883132935, R2 0.5751101970672607\n",
      "Eval loss 1.2551751136779785, R2 0.673427164554596\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    # Set training mode REQUIRED FOR GP\n",
    "    GP_model.gp.train()\n",
    "\n",
    "    # Set the training data\n",
    "    if epoch == 0:\n",
    "        GP_model.set_train_data(train_x_gp, train_y_gp)\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = GP_model.forward()\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = -mll(outputs, GP_model.gp.train_targets)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute metric\n",
    "    train_pred_output = GP_model.likelihood(*outputs)\n",
    "    train_pred_mean = torch.cat(\n",
    "        [pred.mean.unsqueeze(-1) for pred in train_pred_output], axis=-1\n",
    "    )\n",
    "    train_metric = metric(train_pred_mean, train_y_gp)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print(\"epoch {}, loss {}, R2 {}\".format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch % eval_epoch_freq == 0:\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            GP_model.gp.eval()\n",
    "            GP_model.likelihood.eval()\n",
    "            preds = GP_model.forward(test_x_gp)\n",
    "            test_loss = -mll(\n",
    "                preds, [test_y_gp[..., i] for i in range(len(GP_model.gp.models))]\n",
    "            )\n",
    "            test_losses.append(test_loss.item())\n",
    "            # Compute metric\n",
    "            test_pred_output = GP_model.likelihood(*preds)\n",
    "            test_pred_mean = torch.cat(\n",
    "                [pred.mean.unsqueeze(-1) for pred in test_pred_output], axis=-1\n",
    "            )\n",
    "            test_metric = metric(test_pred_mean, test_y_gp)\n",
    "            test_metrics.append(test_metric)\n",
    "            print(\"Eval loss {}, R2 {}\".format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theauv/miniconda3/envs/pdm_env/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = GP_model.likelihood(*GP_model(train_x_gp))\n",
    "    test_preds = GP_model.likelihood(*GP_model(test_x_gp))\n",
    "    train_preds = torch.cat([pred.mean.unsqueeze(-1) for pred in train_preds], axis=-1)\n",
    "    test_preds = torch.cat([pred.mean.unsqueeze(-1) for pred in test_preds], axis=-1)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y_gp, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y_gp, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y_gp, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y_gp, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (200,)\n",
      "Lower train: -1.7761917114257812+-0.013998170383274555\n",
      "Upper train: 1.7761917114257812+-0.013998174108564854\n",
      "Lower test: -1.7772729396820068+-0.013850781135261059\n",
      "Lower test: 1.7772729396820068+-0.013850778341293335\n"
     ]
    }
   ],
   "source": [
    "# For a given output look at the confidence interval for each sample of the training and test sets\n",
    "\n",
    "gp_id = 0\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_pred = GP_model.likelihood.likelihoods[gp_id](\n",
    "        GP_model.gp.models[gp_id](train_x_gp)\n",
    "    )\n",
    "    test_pred = GP_model.likelihood.likelihoods[gp_id](\n",
    "        GP_model.gp.models[gp_id](test_x_gp)\n",
    "    )\n",
    "\n",
    "    train_lower, train_upper = train_pred.confidence_region()\n",
    "    mean_lower_train = np.mean(train_lower.numpy() - train_pred.mean.numpy())\n",
    "    std_lower_train = np.std(train_lower.numpy() - train_pred.mean.numpy())\n",
    "    mean_upper_train = np.mean(train_upper.numpy() - train_pred.mean.numpy())\n",
    "    std_upper_train = np.std(train_upper.numpy() - train_pred.mean.numpy())\n",
    "    test_lower, test_upper = test_pred.confidence_region()\n",
    "    mean_lower_test = np.mean(test_lower.numpy() - test_pred.mean.numpy())\n",
    "    std_lower_test = np.std(test_lower.numpy() - test_pred.mean.numpy())\n",
    "    mean_upper_test = np.mean(test_upper.numpy() - test_pred.mean.numpy())\n",
    "    std_upper_test = np.std(test_upper.numpy() - test_pred.mean.numpy())\n",
    "\n",
    "    print(train_pred.mean.numpy().shape, test_pred.mean.numpy().shape)\n",
    "    print(f\"Lower train: {mean_lower_train}+-{std_lower_train}\")\n",
    "    print(f\"Upper train: {mean_upper_train}+-{std_upper_train}\")\n",
    "    print(f\"Lower test: {mean_lower_test}+-{std_lower_test}\")\n",
    "    print(f\"Lower test: {mean_upper_test}+-{std_upper_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark: next_obs=obs (only dynamics, can't fake predict reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 43]) torch.Size([8000, 43])\n",
      "Train MSE: 1.071226716041565, Train R2: 0.545248806476593\n",
      "Test MSE: 1.049034833908081, Test R2: 0.5838119387626648\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "\n",
    "train_fake_preds = train_x[:, :-1]\n",
    "train_target = train_y[:, :-1]\n",
    "test_fake_preds = test_x[:, :-1]\n",
    "test_target = test_y[:, :-1]\n",
    "print(train_fake_preds.shape, train_target.shape)\n",
    "print(\n",
    "    f\"Train MSE: {criterion(train_fake_preds, train_target)}, Train R2: {metric(train_fake_preds, train_target)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test MSE: {criterion(test_fake_preds, test_target)}, Test R2: {metric(test_fake_preds, test_target)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = train_fake_preds\n",
    "    test_preds = test_fake_preds\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - train_y[:, :-1], axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - train_y[:, :-1], axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - test_y[:, :-1], axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - test_y[:, :-1], axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model: Reward and dynamics separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 44]) torch.Size([8000, 43]) torch.Size([2000, 44]) torch.Size([2000, 43])\n"
     ]
    }
   ],
   "source": [
    "dynamics_train_x = train_x\n",
    "dynamics_train_y = train_y[:, :-1]\n",
    "dynamics_test_x = test_x\n",
    "dynamics_test_y = test_y[:, :-1]\n",
    "\n",
    "print(\n",
    "    dynamics_train_x.shape,\n",
    "    dynamics_train_y.shape,\n",
    "    dynamics_test_x.shape,\n",
    "    dynamics_test_y.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01\n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size = dynamics_train_x.shape[-1]\n",
    "out_size = dynamics_train_y.shape[-1]\n",
    "device = \"cpu\"\n",
    "dynamics_lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(dynamics_lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 7.579585075378418, R2 -1.2256653308868408\n",
      "Eval loss 7.2239251136779785, R2 -1.0983457565307617\n",
      "epoch 1, loss 7.25657844543457, R2 -1.1373687982559204\n",
      "Eval loss 6.935920238494873, R2 -1.0201694965362549\n",
      "epoch 2, loss 6.962957859039307, R2 -1.0571080446243286\n",
      "Eval loss 6.674193859100342, R2 -0.9491091966629028\n",
      "epoch 3, loss 6.695937633514404, R2 -0.9841214418411255\n",
      "Eval loss 6.436247825622559, R2 -0.8844888210296631\n",
      "epoch 4, loss 6.452997207641602, R2 -0.917719841003418\n",
      "Eval loss 6.21981954574585, R2 -0.8256970047950745\n",
      "epoch 5, loss 6.231855392456055, R2 -0.8572792410850525\n",
      "Eval loss 6.022862434387207, R2 -0.7721801400184631\n",
      "epoch 6, loss 6.0304484367370605, R2 -0.8022348284721375\n",
      "Eval loss 5.843524932861328, R2 -0.7234376072883606\n",
      "epoch 7, loss 5.846907138824463, R2 -0.7520757913589478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss 5.680130958557129, R2 -0.67901611328125\n",
      "epoch 8, loss 5.679541110992432, R2 -0.7063394784927368\n",
      "Eval loss 5.531164646148682, R2 -0.6385055184364319\n",
      "epoch 9, loss 5.526818752288818, R2 -0.6646074056625366\n",
      "Eval loss 5.395254611968994, R2 -0.6015348434448242\n",
      "epoch 10, loss 5.387355327606201, R2 -0.6265006065368652\n",
      "Eval loss 5.2711591720581055, R2 -0.5677680373191833\n",
      "epoch 11, loss 5.259896755218506, R2 -0.5916762351989746\n",
      "Eval loss 5.1577558517456055, R2 -0.5369014143943787\n",
      "epoch 12, loss 5.143309593200684, R2 -0.5598240494728088\n",
      "Eval loss 5.054027080535889, R2 -0.5086598992347717\n",
      "epoch 13, loss 5.03656530380249, R2 -0.5306631326675415\n",
      "Eval loss 4.9590559005737305, R2 -0.4827946126461029\n",
      "epoch 14, loss 4.938735485076904, R2 -0.5039392709732056\n",
      "Eval loss 4.872008800506592, R2 -0.4590804874897003\n",
      "epoch 15, loss 4.8489789962768555, R2 -0.4794224500656128\n",
      "Eval loss 4.792133331298828, R2 -0.4373137354850769\n",
      "epoch 16, loss 4.766534328460693, R2 -0.45690447092056274\n",
      "Eval loss 4.71875, R2 -0.4173099994659424\n",
      "epoch 17, loss 4.6907124519348145, R2 -0.43619680404663086\n",
      "Eval loss 4.651241779327393, R2 -0.39890244603157043\n",
      "epoch 18, loss 4.620889663696289, R2 -0.4171290397644043\n",
      "Eval loss 4.589051246643066, R2 -0.38194018602371216\n",
      "epoch 19, loss 4.556501388549805, R2 -0.39954665303230286\n",
      "Eval loss 4.531675338745117, R2 -0.36628642678260803\n",
      "epoch 20, loss 4.497036933898926, R2 -0.3833100199699402\n",
      "Eval loss 4.478656768798828, R2 -0.351817786693573\n",
      "epoch 21, loss 4.442033767700195, R2 -0.36829277873039246\n",
      "Eval loss 4.429583549499512, R2 -0.33842235803604126\n",
      "epoch 22, loss 4.391074180603027, R2 -0.3543802797794342\n",
      "Eval loss 4.384083271026611, R2 -0.3259989321231842\n",
      "epoch 23, loss 4.343778133392334, R2 -0.3414691090583801\n",
      "Eval loss 4.3418169021606445, R2 -0.31445592641830444\n",
      "epoch 24, loss 4.299804210662842, R2 -0.32946544885635376\n",
      "Eval loss 4.302478790283203, R2 -0.30371055006980896\n",
      "epoch 25, loss 4.258841037750244, R2 -0.31828442215919495\n",
      "Eval loss 4.2657952308654785, R2 -0.2936878204345703\n",
      "epoch 26, loss 4.220608234405518, R2 -0.3078491985797882\n",
      "Eval loss 4.231513023376465, R2 -0.2843199372291565\n",
      "epoch 27, loss 4.184851169586182, R2 -0.2980903685092926\n",
      "Eval loss 4.199408531188965, R2 -0.27554550766944885\n",
      "epoch 28, loss 4.151340961456299, R2 -0.28894495964050293\n",
      "Eval loss 4.169275760650635, R2 -0.2673090696334839\n",
      "epoch 29, loss 4.11986780166626, R2 -0.2803560197353363\n",
      "Eval loss 4.14093017578125, R2 -0.2595602869987488\n",
      "epoch 30, loss 4.090244293212891, R2 -0.272271990776062\n",
      "Eval loss 4.114205837249756, R2 -0.2522537410259247\n",
      "epoch 31, loss 4.0622992515563965, R2 -0.2646462321281433\n",
      "Eval loss 4.088949680328369, R2 -0.24534842371940613\n",
      "epoch 32, loss 4.035878658294678, R2 -0.25743648409843445\n",
      "Eval loss 4.0650248527526855, R2 -0.23880696296691895\n",
      "epoch 33, loss 4.010842323303223, R2 -0.2506045401096344\n",
      "Eval loss 4.042308807373047, R2 -0.2325957715511322\n",
      "epoch 34, loss 3.9870636463165283, R2 -0.24411581456661224\n",
      "Eval loss 4.020689487457275, R2 -0.22668446600437164\n",
      "epoch 35, loss 3.964428663253784, R2 -0.23793897032737732\n",
      "Eval loss 4.000064849853516, R2 -0.2210453897714615\n",
      "epoch 36, loss 3.942833185195923, R2 -0.23204563558101654\n",
      "Eval loss 3.980343818664551, R2 -0.21565364301204681\n",
      "epoch 37, loss 3.9221837520599365, R2 -0.2264101505279541\n",
      "Eval loss 3.9614434242248535, R2 -0.21048665046691895\n",
      "epoch 38, loss 3.902395009994507, R2 -0.22100929915905\n",
      "Eval loss 3.943288803100586, R2 -0.20552411675453186\n",
      "epoch 39, loss 3.883390188217163, R2 -0.2158220410346985\n",
      "Eval loss 3.9258124828338623, R2 -0.20074747502803802\n",
      "epoch 40, loss 3.8650996685028076, R2 -0.21082931756973267\n",
      "Eval loss 3.9089531898498535, R2 -0.19614000618457794\n",
      "epoch 41, loss 3.8474600315093994, R2 -0.20601385831832886\n",
      "Eval loss 3.892655611038208, R2 -0.1916866898536682\n",
      "epoch 42, loss 3.830414295196533, R2 -0.20136015117168427\n",
      "Eval loss 3.8768692016601562, R2 -0.18737384676933289\n",
      "epoch 43, loss 3.8139114379882812, R2 -0.19685396552085876\n",
      "Eval loss 3.8615493774414062, R2 -0.1831890195608139\n",
      "epoch 44, loss 3.7979040145874023, R2 -0.1924825757741928\n",
      "Eval loss 3.8466548919677734, R2 -0.17912116646766663\n",
      "epoch 45, loss 3.7823498249053955, R2 -0.1882343888282776\n",
      "Eval loss 3.832148551940918, R2 -0.17516006529331207\n",
      "epoch 46, loss 3.7672107219696045, R2 -0.18409886956214905\n",
      "Eval loss 3.817997932434082, R2 -0.17129653692245483\n",
      "epoch 47, loss 3.7524514198303223, R2 -0.1800665557384491\n",
      "Eval loss 3.804171562194824, R2 -0.1675223708152771\n",
      "epoch 48, loss 3.7380406856536865, R2 -0.1761288344860077\n",
      "Eval loss 3.790642499923706, R2 -0.16383005678653717\n",
      "epoch 49, loss 3.723950147628784, R2 -0.17227789759635925\n",
      "Eval loss 3.777385950088501, R2 -0.16021279990673065\n",
      "epoch 50, loss 3.7101540565490723, R2 -0.1685066819190979\n",
      "Eval loss 3.7643799781799316, R2 -0.1566644310951233\n",
      "epoch 51, loss 3.6966285705566406, R2 -0.16480885446071625\n",
      "Eval loss 3.7516043186187744, R2 -0.15317942202091217\n",
      "epoch 52, loss 3.6833529472351074, R2 -0.1611785888671875\n",
      "Eval loss 3.7390406131744385, R2 -0.14975282549858093\n",
      "epoch 53, loss 3.6703073978424072, R2 -0.15761061012744904\n",
      "Eval loss 3.7266721725463867, R2 -0.1463799923658371\n",
      "epoch 54, loss 3.6574759483337402, R2 -0.1541002094745636\n",
      "Eval loss 3.7144837379455566, R2 -0.14305685460567474\n",
      "epoch 55, loss 3.644840955734253, R2 -0.15064305067062378\n",
      "Eval loss 3.702462911605835, R2 -0.1397797167301178\n",
      "epoch 56, loss 3.6323890686035156, R2 -0.1472352296113968\n",
      "Eval loss 3.690595865249634, R2 -0.13654513657093048\n",
      "epoch 57, loss 3.620107650756836, R2 -0.14387327432632446\n",
      "Eval loss 3.6788735389709473, R2 -0.13335011899471283\n",
      "epoch 58, loss 3.6079843044281006, R2 -0.14055386185646057\n",
      "Eval loss 3.6672842502593994, R2 -0.13019192218780518\n",
      "epoch 59, loss 3.596008777618408, R2 -0.13727420568466187\n",
      "Eval loss 3.6558196544647217, R2 -0.12706802785396576\n",
      "epoch 60, loss 3.5841715335845947, R2 -0.13403160870075226\n",
      "Eval loss 3.644472360610962, R2 -0.12397616356611252\n",
      "epoch 61, loss 3.572463035583496, R2 -0.13082362711429596\n",
      "Eval loss 3.6332333087921143, R2 -0.12091429531574249\n",
      "epoch 62, loss 3.5608763694763184, R2 -0.12764817476272583\n",
      "Eval loss 3.6220974922180176, R2 -0.11788053065538406\n",
      "epoch 63, loss 3.549403429031372, R2 -0.12450329959392548\n",
      "Eval loss 3.611058235168457, R2 -0.11487326771020889\n",
      "epoch 64, loss 3.5380377769470215, R2 -0.12138710170984268\n",
      "Eval loss 3.600109815597534, R2 -0.11189094930887222\n",
      "epoch 65, loss 3.5267744064331055, R2 -0.11829806119203568\n",
      "Eval loss 3.589246988296509, R2 -0.10893215984106064\n",
      "epoch 66, loss 3.5156068801879883, R2 -0.11523467302322388\n",
      "Eval loss 3.5784661769866943, R2 -0.10599570721387863\n",
      "epoch 67, loss 3.504530429840088, R2 -0.11219558864831924\n",
      "Eval loss 3.567763328552246, R2 -0.10308042913675308\n",
      "epoch 68, loss 3.4935414791107178, R2 -0.10917963087558746\n",
      "Eval loss 3.5571343898773193, R2 -0.1001853197813034\n",
      "epoch 69, loss 3.4826347827911377, R2 -0.10618563741445541\n",
      "Eval loss 3.5465755462646484, R2 -0.0973094254732132\n",
      "epoch 70, loss 3.4718070030212402, R2 -0.10321266949176788\n",
      "Eval loss 3.5360844135284424, R2 -0.09445192664861679\n",
      "epoch 71, loss 3.461055278778076, R2 -0.10025973618030548\n",
      "Eval loss 3.525658369064331, R2 -0.09161202609539032\n",
      "epoch 72, loss 3.450376033782959, R2 -0.09732603281736374\n",
      "Eval loss 3.5152950286865234, R2 -0.08878907561302185\n",
      "epoch 73, loss 3.4397666454315186, R2 -0.09441082924604416\n",
      "Eval loss 3.504991292953491, R2 -0.08598239719867706\n",
      "epoch 74, loss 3.429224967956543, R2 -0.09151344001293182\n",
      "Eval loss 3.494745969772339, R2 -0.08319142460823059\n",
      "epoch 75, loss 3.418747901916504, R2 -0.08863315731287003\n",
      "Eval loss 3.4845573902130127, R2 -0.08041566610336304\n",
      "epoch 76, loss 3.4083340167999268, R2 -0.08576950430870056\n",
      "Eval loss 3.4744229316711426, R2 -0.07765459269285202\n",
      "epoch 77, loss 3.3979804515838623, R2 -0.08292188495397568\n",
      "Eval loss 3.464341640472412, R2 -0.0749078243970871\n",
      "epoch 78, loss 3.3876867294311523, R2 -0.08008985966444016\n",
      "Eval loss 3.454312562942505, R2 -0.07217497378587723\n",
      "epoch 79, loss 3.3774502277374268, R2 -0.07727300375699997\n",
      "Eval loss 3.444333553314209, R2 -0.06945563852787018\n",
      "epoch 80, loss 3.367269515991211, R2 -0.07447083294391632\n",
      "Eval loss 3.434404134750366, R2 -0.06674947589635849\n",
      "epoch 81, loss 3.3571438789367676, R2 -0.07168304920196533\n",
      "Eval loss 3.424522876739502, R2 -0.06405624002218246\n",
      "epoch 82, loss 3.347071886062622, R2 -0.06890930235385895\n",
      "Eval loss 3.414689064025879, R2 -0.06137562170624733\n",
      "epoch 83, loss 3.3370518684387207, R2 -0.0661492869257927\n",
      "Eval loss 3.4049012660980225, R2 -0.058707404881715775\n",
      "epoch 84, loss 3.327082872390747, R2 -0.0634026899933815\n",
      "Eval loss 3.3951592445373535, R2 -0.05605130270123482\n",
      "epoch 85, loss 3.317164182662964, R2 -0.06066925451159477\n",
      "Eval loss 3.3854622840881348, R2 -0.05340714752674103\n",
      "epoch 86, loss 3.307295322418213, R2 -0.057948749512434006\n",
      "Eval loss 3.375809907913208, R2 -0.05077475309371948\n",
      "epoch 87, loss 3.2974746227264404, R2 -0.05524098128080368\n",
      "Eval loss 3.3662009239196777, R2 -0.04815387725830078\n",
      "epoch 88, loss 3.2877016067504883, R2 -0.05254567414522171\n",
      "Eval loss 3.356635093688965, R2 -0.045544419437646866\n",
      "epoch 89, loss 3.27797532081604, R2 -0.049862682819366455\n",
      "Eval loss 3.347111225128174, R2 -0.042946189641952515\n",
      "epoch 90, loss 3.268296241760254, R2 -0.04719183221459389\n",
      "Eval loss 3.337630033493042, R2 -0.04035905748605728\n",
      "epoch 91, loss 3.258661985397339, R2 -0.044532909989356995\n",
      "Eval loss 3.328190326690674, R2 -0.03778289631009102\n",
      "epoch 92, loss 3.2490735054016113, R2 -0.04188580438494682\n",
      "Eval loss 3.3187918663024902, R2 -0.03521757200360298\n",
      "epoch 93, loss 3.2395291328430176, R2 -0.03925034776329994\n",
      "Eval loss 3.309434175491333, R2 -0.032662976533174515\n",
      "epoch 94, loss 3.2300286293029785, R2 -0.0366264171898365\n",
      "Eval loss 3.3001163005828857, R2 -0.030119020491838455\n",
      "epoch 95, loss 3.2205727100372314, R2 -0.034013908356428146\n",
      "Eval loss 3.2908387184143066, R2 -0.0275855902582407\n",
      "epoch 96, loss 3.2111592292785645, R2 -0.03141266852617264\n",
      "Eval loss 3.2816009521484375, R2 -0.025062602013349533\n",
      "epoch 97, loss 3.2017884254455566, R2 -0.02882262133061886\n",
      "Eval loss 3.2724030017852783, R2 -0.022549906745553017\n",
      "epoch 98, loss 3.192460060119629, R2 -0.02624361775815487\n",
      "Eval loss 3.2632434368133545, R2 -0.020047491416335106\n",
      "epoch 99, loss 3.183173656463623, R2 -0.02367556095123291\n",
      "Eval loss 3.2541234493255615, R2 -0.017555266618728638\n",
      "epoch 100, loss 3.173928737640381, R2 -0.02111838385462761\n",
      "Eval loss 3.2450413703918457, R2 -0.015073106624186039\n",
      "epoch 101, loss 3.1647253036499023, R2 -0.01857198216021061\n",
      "Eval loss 3.2359979152679443, R2 -0.012601037509739399\n",
      "epoch 102, loss 3.1555628776550293, R2 -0.016036290675401688\n",
      "Eval loss 3.2269928455352783, R2 -0.010138923302292824\n",
      "epoch 103, loss 3.1464412212371826, R2 -0.01351122371852398\n",
      "Eval loss 3.2180254459381104, R2 -0.007686687167733908\n",
      "epoch 104, loss 3.1373589038848877, R2 -0.010996639728546143\n",
      "Eval loss 3.2090952396392822, R2 -0.005244296509772539\n",
      "epoch 105, loss 3.1283175945281982, R2 -0.00849253498017788\n",
      "Eval loss 3.200202703475952, R2 -0.00281168962828815\n",
      "epoch 106, loss 3.1193153858184814, R2 -0.0059988247230648994\n",
      "Eval loss 3.191347360610962, R2 -0.000388824671972543\n",
      "epoch 107, loss 3.1103532314300537, R2 -0.0035154472570866346\n",
      "Eval loss 3.1825296878814697, R2 0.0020244205370545387\n",
      "epoch 108, loss 3.1014299392700195, R2 -0.001042281510308385\n",
      "Eval loss 3.17374849319458, R2 0.004428031854331493\n",
      "epoch 109, loss 3.092545747756958, R2 0.0014206281630322337\n",
      "Eval loss 3.165003538131714, R2 0.006822055205702782\n",
      "epoch 110, loss 3.083699941635132, R2 0.0038734786212444305\n",
      "Eval loss 3.1562957763671875, R2 0.009206618182361126\n",
      "epoch 111, loss 3.0748932361602783, R2 0.006316227838397026\n",
      "Eval loss 3.1476235389709473, R2 0.01158169936388731\n",
      "epoch 112, loss 3.066124200820923, R2 0.008749001659452915\n",
      "Eval loss 3.1389882564544678, R2 0.013947349973022938\n",
      "epoch 113, loss 3.0573935508728027, R2 0.011171801015734673\n",
      "Eval loss 3.130388021469116, R2 0.016303643584251404\n",
      "epoch 114, loss 3.0487005710601807, R2 0.013584724627435207\n",
      "Eval loss 3.121824026107788, R2 0.018650619313120842\n",
      "epoch 115, loss 3.0400452613830566, R2 0.015987783670425415\n",
      "Eval loss 3.113295793533325, R2 0.020988326519727707\n",
      "epoch 116, loss 3.0314273834228516, R2 0.018381057307124138\n",
      "Eval loss 3.1048030853271484, R2 0.023316804319620132\n",
      "epoch 117, loss 3.0228466987609863, R2 0.020764613524079323\n",
      "Eval loss 3.0963451862335205, R2 0.02563607506453991\n",
      "epoch 118, loss 3.014302968978882, R2 0.0231384988874197\n",
      "Eval loss 3.087923049926758, R2 0.027946215122938156\n",
      "epoch 119, loss 3.005796194076538, R2 0.02550273761153221\n",
      "Eval loss 3.0795352458953857, R2 0.030247261747717857\n",
      "epoch 120, loss 2.997325897216797, R2 0.027857378125190735\n",
      "Eval loss 3.0711824893951416, R2 0.0325392447412014\n",
      "epoch 121, loss 2.988892078399658, R2 0.030202500522136688\n",
      "Eval loss 3.0628645420074463, R2 0.03482219949364662\n",
      "epoch 122, loss 2.980494260787964, R2 0.03253812715411186\n",
      "Eval loss 3.0545806884765625, R2 0.03709616884589195\n",
      "epoch 123, loss 2.972132921218872, R2 0.03486430272459984\n",
      "Eval loss 3.0463316440582275, R2 0.03936120495200157\n",
      "epoch 124, loss 2.9638073444366455, R2 0.037181101739406586\n",
      "Eval loss 3.038116931915283, R2 0.04161733761429787\n",
      "epoch 125, loss 2.955517292022705, R2 0.03948851674795151\n",
      "Eval loss 3.029935598373413, R2 0.043864618986845016\n",
      "epoch 126, loss 2.94726300239563, R2 0.04178665578365326\n",
      "Eval loss 3.0217888355255127, R2 0.04610306769609451\n",
      "epoch 127, loss 2.9390435218811035, R2 0.04407552257180214\n",
      "Eval loss 3.0136759281158447, R2 0.048332735896110535\n",
      "epoch 128, loss 2.9308595657348633, R2 0.04635516554117203\n",
      "Eval loss 3.0055959224700928, R2 0.05055365338921547\n",
      "epoch 129, loss 2.922710657119751, R2 0.048625648021698\n",
      "Eval loss 2.9975497722625732, R2 0.052765872329473495\n",
      "epoch 130, loss 2.9145967960357666, R2 0.05088699609041214\n",
      "Eval loss 2.989537000656128, R2 0.054969411343336105\n",
      "epoch 131, loss 2.906517267227173, R2 0.053139254450798035\n",
      "Eval loss 2.9815573692321777, R2 0.05716431885957718\n",
      "epoch 132, loss 2.898472309112549, R2 0.05538245290517807\n",
      "Eval loss 2.9736108779907227, R2 0.05935063585639\n",
      "epoch 133, loss 2.8904619216918945, R2 0.057616643607616425\n",
      "Eval loss 2.9656970500946045, R2 0.061528418213129044\n",
      "epoch 134, loss 2.8824851512908936, R2 0.059841908514499664\n",
      "Eval loss 2.9578163623809814, R2 0.06369765847921371\n",
      "epoch 135, loss 2.874542713165283, R2 0.062058232724666595\n",
      "Eval loss 2.949967861175537, R2 0.0658584013581276\n",
      "epoch 136, loss 2.8666341304779053, R2 0.0642656609416008\n",
      "Eval loss 2.9421517848968506, R2 0.06801068782806396\n",
      "epoch 137, loss 2.8587591648101807, R2 0.06646426767110825\n",
      "Eval loss 2.934368371963501, R2 0.07015460729598999\n",
      "epoch 138, loss 2.8509175777435303, R2 0.06865406036376953\n",
      "Eval loss 2.926616907119751, R2 0.07229010760784149\n",
      "epoch 139, loss 2.843109369277954, R2 0.07083509862422943\n",
      "Eval loss 2.918897867202759, R2 0.07441727817058563\n",
      "epoch 140, loss 2.8353347778320312, R2 0.07300739735364914\n",
      "Eval loss 2.911210298538208, R2 0.076536163687706\n",
      "epoch 141, loss 2.8275930881500244, R2 0.07517103850841522\n",
      "Eval loss 2.903554916381836, R2 0.07864676415920258\n",
      "epoch 142, loss 2.8198840618133545, R2 0.07732603698968887\n",
      "Eval loss 2.895930528640747, R2 0.08074913918972015\n",
      "epoch 143, loss 2.8122079372406006, R2 0.07947242259979248\n",
      "Eval loss 2.888338327407837, R2 0.08284330368041992\n",
      "epoch 144, loss 2.8045642375946045, R2 0.08161023259162903\n",
      "Eval loss 2.880777359008789, R2 0.08492930233478546\n",
      "epoch 145, loss 2.7969532012939453, R2 0.08373953402042389\n",
      "Eval loss 2.8732473850250244, R2 0.08700718730688095\n",
      "epoch 146, loss 2.789374351501465, R2 0.08586032688617706\n",
      "Eval loss 2.865748643875122, R2 0.08907696604728699\n",
      "epoch 147, loss 2.781827688217163, R2 0.08797265589237213\n",
      "Eval loss 2.8582804203033447, R2 0.09113867580890656\n",
      "epoch 148, loss 2.77431321144104, R2 0.09007660299539566\n",
      "Eval loss 2.850843667984009, R2 0.09319235384464264\n",
      "epoch 149, loss 2.7668302059173584, R2 0.09217215329408646\n",
      "Eval loss 2.843437433242798, R2 0.0952380895614624\n",
      "epoch 150, loss 2.7593791484832764, R2 0.09425939619541168\n",
      "Eval loss 2.836061477661133, R2 0.09727584570646286\n",
      "epoch 151, loss 2.7519595623016357, R2 0.09633829444646835\n",
      "Eval loss 2.828716278076172, R2 0.0993056520819664\n",
      "epoch 152, loss 2.7445714473724365, R2 0.09840895235538483\n",
      "Eval loss 2.8214008808135986, R2 0.10132758319377899\n",
      "epoch 153, loss 2.7372148036956787, R2 0.1004713624715805\n",
      "Eval loss 2.8141160011291504, R2 0.10334164649248123\n",
      "epoch 154, loss 2.729888916015625, R2 0.10252561420202255\n",
      "Eval loss 2.806861162185669, R2 0.10534790903329849\n",
      "epoch 155, loss 2.7225940227508545, R2 0.10457168519496918\n",
      "Eval loss 2.799635887145996, R2 0.10734639316797256\n",
      "epoch 156, loss 2.7153303623199463, R2 0.10660963505506516\n",
      "Eval loss 2.79244065284729, R2 0.10933706164360046\n",
      "epoch 157, loss 2.708096742630005, R2 0.10863948613405228\n",
      "Eval loss 2.7852749824523926, R2 0.11132007092237473\n",
      "epoch 158, loss 2.700894355773926, R2 0.11066130548715591\n",
      "Eval loss 2.7781388759613037, R2 0.11329533159732819\n",
      "epoch 159, loss 2.6937220096588135, R2 0.11267509311437607\n",
      "Eval loss 2.7710323333740234, R2 0.1152629405260086\n",
      "epoch 160, loss 2.686579942703247, R2 0.1146809309720993\n",
      "Eval loss 2.7639546394348145, R2 0.11722303181886673\n",
      "epoch 161, loss 2.6794681549072266, R2 0.11667880415916443\n",
      "Eval loss 2.756906270980835, R2 0.11917543411254883\n",
      "epoch 162, loss 2.6723861694335938, R2 0.1186688095331192\n",
      "Eval loss 2.7498865127563477, R2 0.12112029641866684\n",
      "epoch 163, loss 2.6653342247009277, R2 0.12065088003873825\n",
      "Eval loss 2.74289608001709, R2 0.12305761128664017\n",
      "epoch 164, loss 2.6583120822906494, R2 0.12262514233589172\n",
      "Eval loss 2.735934257507324, R2 0.12498743832111359\n",
      "epoch 165, loss 2.6513195037841797, R2 0.12459160387516022\n",
      "Eval loss 2.729001045227051, R2 0.12690982222557068\n",
      "epoch 166, loss 2.6443560123443604, R2 0.12655030190944672\n",
      "Eval loss 2.7220959663391113, R2 0.12882477045059204\n",
      "epoch 167, loss 2.6374220848083496, R2 0.12850123643875122\n",
      "Eval loss 2.715219497680664, R2 0.13073231279850006\n",
      "epoch 168, loss 2.6305174827575684, R2 0.1304444670677185\n",
      "Eval loss 2.70837140083313, R2 0.13263244926929474\n",
      "epoch 169, loss 2.6236419677734375, R2 0.13238006830215454\n",
      "Eval loss 2.7015511989593506, R2 0.13452531397342682\n",
      "epoch 170, loss 2.616795063018799, R2 0.13430801033973694\n",
      "Eval loss 2.694758892059326, R2 0.13641083240509033\n",
      "epoch 171, loss 2.6099770069122314, R2 0.13622833788394928\n",
      "Eval loss 2.6879944801330566, R2 0.13828909397125244\n",
      "epoch 172, loss 2.6031875610351562, R2 0.13814112544059753\n",
      "Eval loss 2.681257963180542, R2 0.14016012847423553\n",
      "epoch 173, loss 2.5964267253875732, R2 0.14004632830619812\n",
      "Eval loss 2.674548625946045, R2 0.14202389121055603\n",
      "epoch 174, loss 2.5896944999694824, R2 0.1419440656900406\n",
      "Eval loss 2.667867422103882, R2 0.14388051629066467\n",
      "epoch 175, loss 2.5829906463623047, R2 0.14383433759212494\n",
      "Eval loss 2.6612131595611572, R2 0.14572998881340027\n",
      "epoch 176, loss 2.5763142108917236, R2 0.14571714401245117\n",
      "Eval loss 2.65458607673645, R2 0.147572323679924\n",
      "epoch 177, loss 2.5696663856506348, R2 0.14759257435798645\n",
      "Eval loss 2.647986650466919, R2 0.14940756559371948\n",
      "epoch 178, loss 2.5630462169647217, R2 0.14946064352989197\n",
      "Eval loss 2.641413688659668, R2 0.15123577415943146\n",
      "epoch 179, loss 2.5564539432525635, R2 0.15132135152816772\n",
      "Eval loss 2.6348676681518555, R2 0.15305691957473755\n",
      "epoch 180, loss 2.54988956451416, R2 0.1531747281551361\n",
      "Eval loss 2.6283485889434814, R2 0.15487109124660492\n",
      "epoch 181, loss 2.5433521270751953, R2 0.15502086281776428\n",
      "Eval loss 2.621856212615967, R2 0.15667830407619476\n",
      "epoch 182, loss 2.5368423461914062, R2 0.15685975551605225\n",
      "Eval loss 2.615389823913574, R2 0.1584785133600235\n",
      "epoch 183, loss 2.5303597450256348, R2 0.15869140625\n",
      "Eval loss 2.60895037651062, R2 0.16027188301086426\n",
      "epoch 184, loss 2.523904323577881, R2 0.1605158895254135\n",
      "Eval loss 2.602537155151367, R2 0.16205833852291107\n",
      "epoch 185, loss 2.5174760818481445, R2 0.16233325004577637\n",
      "Eval loss 2.5961501598358154, R2 0.16383793950080872\n",
      "epoch 186, loss 2.5110745429992676, R2 0.1641434282064438\n",
      "Eval loss 2.5897886753082275, R2 0.16561073064804077\n",
      "epoch 187, loss 2.50469970703125, R2 0.1659465730190277\n",
      "Eval loss 2.583453416824341, R2 0.16737674176692963\n",
      "epoch 188, loss 2.4983513355255127, R2 0.16774266958236694\n",
      "Eval loss 2.5771443843841553, R2 0.1691359430551529\n",
      "epoch 189, loss 2.492029905319214, R2 0.1695316880941391\n",
      "Eval loss 2.5708606243133545, R2 0.17088843882083893\n",
      "epoch 190, loss 2.485734701156616, R2 0.17131373286247253\n",
      "Eval loss 2.564602851867676, R2 0.17263418436050415\n",
      "epoch 191, loss 2.479465961456299, R2 0.17308881878852844\n",
      "Eval loss 2.558370351791382, R2 0.1743733137845993\n",
      "epoch 192, loss 2.4732232093811035, R2 0.17485696077346802\n",
      "Eval loss 2.5521631240844727, R2 0.17610575258731842\n",
      "epoch 193, loss 2.4670064449310303, R2 0.17661818861961365\n",
      "Eval loss 2.5459811687469482, R2 0.17783156037330627\n",
      "epoch 194, loss 2.460815668106079, R2 0.1783725619316101\n",
      "Eval loss 2.5398247241973877, R2 0.17955079674720764\n",
      "epoch 195, loss 2.454650640487671, R2 0.1801200658082962\n",
      "Eval loss 2.5336928367614746, R2 0.18126347661018372\n",
      "epoch 196, loss 2.4485113620758057, R2 0.1818607598543167\n",
      "Eval loss 2.5275862216949463, R2 0.1829695999622345\n",
      "epoch 197, loss 2.4423978328704834, R2 0.18359465897083282\n",
      "Eval loss 2.5215041637420654, R2 0.18466922640800476\n",
      "epoch 198, loss 2.436309576034546, R2 0.18532182276248932\n",
      "Eval loss 2.515446901321411, R2 0.1863623559474945\n",
      "epoch 199, loss 2.4302468299865723, R2 0.187042236328125\n",
      "Eval loss 2.5094144344329834, R2 0.1880490481853485\n",
      "epoch 200, loss 2.424208879470825, R2 0.18875597417354584\n",
      "Eval loss 2.503406286239624, R2 0.18972928822040558\n",
      "epoch 201, loss 2.418196678161621, R2 0.19046302139759064\n",
      "Eval loss 2.497422695159912, R2 0.1914031207561493\n",
      "epoch 202, loss 2.4122090339660645, R2 0.19216342270374298\n",
      "Eval loss 2.4914631843566895, R2 0.19307059049606323\n",
      "epoch 203, loss 2.4062464237213135, R2 0.19385723769664764\n",
      "Eval loss 2.485527515411377, R2 0.1947317272424698\n",
      "epoch 204, loss 2.400308609008789, R2 0.19554446637630463\n",
      "Eval loss 2.479616403579712, R2 0.19638653099536896\n",
      "epoch 205, loss 2.394395351409912, R2 0.19722507894039154\n",
      "Eval loss 2.473729372024536, R2 0.19803504645824432\n",
      "epoch 206, loss 2.3885068893432617, R2 0.1988992542028427\n",
      "Eval loss 2.4678659439086914, R2 0.19967728853225708\n",
      "epoch 207, loss 2.3826425075531006, R2 0.20056688785552979\n",
      "Eval loss 2.4620261192321777, R2 0.2013133317232132\n",
      "epoch 208, loss 2.376802682876587, R2 0.20222800970077515\n",
      "Eval loss 2.456210136413574, R2 0.2029430866241455\n",
      "epoch 209, loss 2.3709871768951416, R2 0.20388275384902954\n",
      "Eval loss 2.4504175186157227, R2 0.20456668734550476\n",
      "epoch 210, loss 2.3651957511901855, R2 0.2055310606956482\n",
      "Eval loss 2.444648504257202, R2 0.20618414878845215\n",
      "epoch 211, loss 2.3594284057617188, R2 0.20717297494411469\n",
      "Eval loss 2.4389026165008545, R2 0.2077954262495041\n",
      "epoch 212, loss 2.353684663772583, R2 0.2088085263967514\n",
      "Eval loss 2.433180332183838, R2 0.20940063893795013\n",
      "epoch 213, loss 2.3479650020599365, R2 0.21043772995471954\n",
      "Eval loss 2.427480936050415, R2 0.21099980175495148\n",
      "epoch 214, loss 2.342268943786621, R2 0.21206064522266388\n",
      "Eval loss 2.421804428100586, R2 0.21259285509586334\n",
      "epoch 215, loss 2.336596727371216, R2 0.2136772722005844\n",
      "Eval loss 2.4161510467529297, R2 0.2141799032688141\n",
      "epoch 216, loss 2.3309476375579834, R2 0.21528765559196472\n",
      "Eval loss 2.410520315170288, R2 0.21576091647148132\n",
      "epoch 217, loss 2.325321912765503, R2 0.2168918401002884\n",
      "Eval loss 2.4049124717712402, R2 0.21733593940734863\n",
      "epoch 218, loss 2.3197195529937744, R2 0.21848978102207184\n",
      "Eval loss 2.399327039718628, R2 0.21890507638454437\n",
      "epoch 219, loss 2.3141403198242188, R2 0.22008159756660461\n",
      "Eval loss 2.3937642574310303, R2 0.220468208193779\n",
      "epoch 220, loss 2.308584213256836, R2 0.22166723012924194\n",
      "Eval loss 2.388223886489868, R2 0.22202545404434204\n",
      "epoch 221, loss 2.303050994873047, R2 0.2232467234134674\n",
      "Eval loss 2.3827059268951416, R2 0.22357682883739471\n",
      "epoch 222, loss 2.2975406646728516, R2 0.22482015192508698\n",
      "Eval loss 2.3772101402282715, R2 0.2251223474740982\n",
      "epoch 223, loss 2.292052984237671, R2 0.22638753056526184\n",
      "Eval loss 2.3717362880706787, R2 0.22666200995445251\n",
      "epoch 224, loss 2.286588191986084, R2 0.2279488742351532\n",
      "Eval loss 2.3662846088409424, R2 0.228195920586586\n",
      "epoch 225, loss 2.2811458110809326, R2 0.22950418293476105\n",
      "Eval loss 2.3608551025390625, R2 0.22972401976585388\n",
      "epoch 226, loss 2.2757256031036377, R2 0.23105348646640778\n",
      "Eval loss 2.3554470539093018, R2 0.23124632239341736\n",
      "epoch 227, loss 2.2703282833099365, R2 0.23259684443473816\n",
      "Eval loss 2.3500607013702393, R2 0.23276293277740479\n",
      "epoch 228, loss 2.2649528980255127, R2 0.2341342717409134\n",
      "Eval loss 2.344696521759033, R2 0.23427386581897736\n",
      "epoch 229, loss 2.259599447250366, R2 0.23566573858261108\n",
      "Eval loss 2.339353561401367, R2 0.2357790470123291\n",
      "epoch 230, loss 2.2542686462402344, R2 0.23719137907028198\n",
      "Eval loss 2.334031820297241, R2 0.23727858066558838\n",
      "epoch 231, loss 2.2489593029022217, R2 0.2387111634016037\n",
      "Eval loss 2.3287315368652344, R2 0.23877252638339996\n",
      "epoch 232, loss 2.2436718940734863, R2 0.24022507667541504\n",
      "Eval loss 2.3234527111053467, R2 0.24026080965995789\n",
      "epoch 233, loss 2.238406181335449, R2 0.24173322319984436\n",
      "Eval loss 2.318195104598999, R2 0.24174350500106812\n",
      "epoch 234, loss 2.2331621646881104, R2 0.2432355284690857\n",
      "Eval loss 2.3129584789276123, R2 0.24322068691253662\n",
      "epoch 235, loss 2.2279398441314697, R2 0.2447320818901062\n",
      "Eval loss 2.3077428340911865, R2 0.24469226598739624\n",
      "epoch 236, loss 2.2227389812469482, R2 0.24622291326522827\n",
      "Eval loss 2.3025479316711426, R2 0.24615836143493652\n",
      "epoch 237, loss 2.217559337615967, R2 0.2477080374956131\n",
      "Eval loss 2.2973740100860596, R2 0.24761894345283508\n",
      "epoch 238, loss 2.2124011516571045, R2 0.24918745458126068\n",
      "Eval loss 2.2922208309173584, R2 0.2490740269422531\n",
      "epoch 239, loss 2.207263708114624, R2 0.2506612241268158\n",
      "Eval loss 2.287088394165039, R2 0.2505236864089966\n",
      "epoch 240, loss 2.202147960662842, R2 0.25212931632995605\n",
      "Eval loss 2.2819762229919434, R2 0.2519679367542267\n",
      "epoch 241, loss 2.1970529556274414, R2 0.2535918354988098\n",
      "Eval loss 2.2768845558166504, R2 0.25340673327445984\n",
      "epoch 242, loss 2.1919784545898438, R2 0.2550487220287323\n",
      "Eval loss 2.271813154220581, R2 0.2548401951789856\n",
      "epoch 243, loss 2.186925172805786, R2 0.2565000653266907\n",
      "Eval loss 2.2667622566223145, R2 0.25626829266548157\n",
      "epoch 244, loss 2.1818923950195312, R2 0.25794586539268494\n",
      "Eval loss 2.2617313861846924, R2 0.25769099593162537\n",
      "epoch 245, loss 2.176880359649658, R2 0.25938618183135986\n",
      "Eval loss 2.256720542907715, R2 0.25910842418670654\n",
      "epoch 246, loss 2.171888828277588, R2 0.2608209252357483\n",
      "Eval loss 2.251729965209961, R2 0.2605205774307251\n",
      "epoch 247, loss 2.1669178009033203, R2 0.26225024461746216\n",
      "Eval loss 2.2467591762542725, R2 0.26192739605903625\n",
      "epoch 248, loss 2.1619670391082764, R2 0.2636740803718567\n",
      "Eval loss 2.2418081760406494, R2 0.26332902908325195\n",
      "epoch 249, loss 2.157036781311035, R2 0.26509249210357666\n",
      "Eval loss 2.2368767261505127, R2 0.2647254168987274\n",
      "epoch 250, loss 2.1521260738372803, R2 0.26650553941726685\n",
      "Eval loss 2.2319653034210205, R2 0.26611658930778503\n",
      "epoch 251, loss 2.147235870361328, R2 0.26791316270828247\n",
      "Eval loss 2.2270731925964355, R2 0.2675025761127472\n",
      "epoch 252, loss 2.1423654556274414, R2 0.2693154215812683\n",
      "Eval loss 2.222200632095337, R2 0.26888343691825867\n",
      "epoch 253, loss 2.1375153064727783, R2 0.27071237564086914\n",
      "Eval loss 2.2173473834991455, R2 0.2702591121196747\n",
      "epoch 254, loss 2.1326847076416016, R2 0.2721039950847626\n",
      "Eval loss 2.2125134468078613, R2 0.2716297209262848\n",
      "epoch 255, loss 2.127873659133911, R2 0.273490309715271\n",
      "Eval loss 2.2076988220214844, R2 0.2729952037334442\n",
      "epoch 256, loss 2.123082399368286, R2 0.2748713791370392\n",
      "Eval loss 2.2029037475585938, R2 0.27435559034347534\n",
      "epoch 257, loss 2.1183109283447266, R2 0.27624720335006714\n",
      "Eval loss 2.198127031326294, R2 0.27571094036102295\n",
      "epoch 258, loss 2.113558530807495, R2 0.27761781215667725\n",
      "Eval loss 2.1933696269989014, R2 0.27706125378608704\n",
      "epoch 259, loss 2.10882568359375, R2 0.2789832055568695\n",
      "Eval loss 2.188631057739258, R2 0.2784065306186676\n",
      "epoch 260, loss 2.104112148284912, R2 0.2803434133529663\n",
      "Eval loss 2.183911085128784, R2 0.27974680066108704\n",
      "epoch 261, loss 2.0994174480438232, R2 0.2816984951496124\n",
      "Eval loss 2.1792099475860596, R2 0.2810821831226349\n",
      "epoch 262, loss 2.0947422981262207, R2 0.2830484211444855\n",
      "Eval loss 2.174527883529663, R2 0.2824125587940216\n",
      "epoch 263, loss 2.0900862216949463, R2 0.28439319133758545\n",
      "Eval loss 2.1698641777038574, R2 0.28373798727989197\n",
      "epoch 264, loss 2.085448741912842, R2 0.2857329249382019\n",
      "Eval loss 2.1652188301086426, R2 0.28505855798721313\n",
      "epoch 265, loss 2.0808303356170654, R2 0.2870675325393677\n",
      "Eval loss 2.1605918407440186, R2 0.28637418150901794\n",
      "epoch 266, loss 2.076230764389038, R2 0.2883971333503723\n",
      "Eval loss 2.1559832096099854, R2 0.28768494725227356\n",
      "epoch 267, loss 2.0716497898101807, R2 0.28972169756889343\n",
      "Eval loss 2.151393175125122, R2 0.28899088501930237\n",
      "epoch 268, loss 2.067087173461914, R2 0.2910412549972534\n",
      "Eval loss 2.1468207836151123, R2 0.29029199481010437\n",
      "epoch 269, loss 2.0625436305999756, R2 0.29235580563545227\n",
      "Eval loss 2.1422669887542725, R2 0.29158830642700195\n",
      "epoch 270, loss 2.0580179691314697, R2 0.29366540908813477\n",
      "Eval loss 2.137730836868286, R2 0.29287976026535034\n",
      "epoch 271, loss 2.053510904312134, R2 0.2949700355529785\n",
      "Eval loss 2.1332125663757324, R2 0.29416653513908386\n",
      "epoch 272, loss 2.0490224361419678, R2 0.29626980423927307\n",
      "Eval loss 2.1287126541137695, R2 0.29544851183891296\n",
      "epoch 273, loss 2.0445516109466553, R2 0.29756462574005127\n",
      "Eval loss 2.124229907989502, R2 0.2967257499694824\n",
      "epoch 274, loss 2.040099620819092, R2 0.2988545596599579\n",
      "Eval loss 2.119765281677246, R2 0.297998309135437\n",
      "epoch 275, loss 2.0356650352478027, R2 0.3001396358013153\n",
      "Eval loss 2.1153182983398438, R2 0.29926615953445435\n",
      "epoch 276, loss 2.0312485694885254, R2 0.3014198839664459\n",
      "Eval loss 2.1108884811401367, R2 0.3005293607711792\n",
      "epoch 277, loss 2.0268502235412598, R2 0.30269530415534973\n",
      "Eval loss 2.106476306915283, R2 0.3017878532409668\n",
      "epoch 278, loss 2.0224690437316895, R2 0.30396589636802673\n",
      "Eval loss 2.102081775665283, R2 0.3030417859554291\n",
      "epoch 279, loss 2.01810622215271, R2 0.3052317500114441\n",
      "Eval loss 2.0977041721343994, R2 0.3042910397052765\n",
      "epoch 280, loss 2.013760805130005, R2 0.30649280548095703\n",
      "Eval loss 2.093344211578369, R2 0.30553576350212097\n",
      "epoch 281, loss 2.0094330310821533, R2 0.3077491521835327\n",
      "Eval loss 2.089001178741455, R2 0.30677586793899536\n",
      "epoch 282, loss 2.0051229000091553, R2 0.30900076031684875\n",
      "Eval loss 2.0846753120422363, R2 0.3080114424228668\n",
      "epoch 283, loss 2.0008299350738525, R2 0.31024765968322754\n",
      "Eval loss 2.080366611480713, R2 0.30924245715141296\n",
      "epoch 284, loss 1.9965543746948242, R2 0.31148990988731384\n",
      "Eval loss 2.0760746002197266, R2 0.31046897172927856\n",
      "epoch 285, loss 1.9922960996627808, R2 0.3127274811267853\n",
      "Eval loss 2.0717997550964355, R2 0.3116909861564636\n",
      "epoch 286, loss 1.9880551099777222, R2 0.31396040320396423\n",
      "Eval loss 2.0675415992736816, R2 0.312908411026001\n",
      "epoch 287, loss 1.9838310480117798, R2 0.3151887059211731\n",
      "Eval loss 2.0632998943328857, R2 0.3141215145587921\n",
      "epoch 288, loss 1.9796239137649536, R2 0.31641238927841187\n",
      "Eval loss 2.059075117111206, R2 0.3153301179409027\n",
      "epoch 289, loss 1.9754341840744019, R2 0.3176315426826477\n",
      "Eval loss 2.0548665523529053, R2 0.3165343105792999\n",
      "epoch 290, loss 1.9712610244750977, R2 0.31884607672691345\n",
      "Eval loss 2.0506751537323, R2 0.3177340626716614\n",
      "epoch 291, loss 1.9671050310134888, R2 0.32005611062049866\n",
      "Eval loss 2.046499729156494, R2 0.3189294636249542\n",
      "epoch 292, loss 1.962965488433838, R2 0.32126158475875854\n",
      "Eval loss 2.0423409938812256, R2 0.3201204538345337\n",
      "epoch 293, loss 1.9588426351547241, R2 0.3224625587463379\n",
      "Eval loss 2.038198471069336, R2 0.32130709290504456\n",
      "epoch 294, loss 1.9547362327575684, R2 0.3236590325832367\n",
      "Eval loss 2.034071922302246, R2 0.3224894404411316\n",
      "epoch 295, loss 1.9506464004516602, R2 0.32485100626945496\n",
      "Eval loss 2.0299620628356934, R2 0.32366740703582764\n",
      "epoch 296, loss 1.94657301902771, R2 0.3260386288166046\n",
      "Eval loss 2.0258677005767822, R2 0.32484111189842224\n",
      "epoch 297, loss 1.9425162076950073, R2 0.32722169160842896\n",
      "Eval loss 2.02178955078125, R2 0.32601049542427063\n",
      "epoch 298, loss 1.9384756088256836, R2 0.3284004330635071\n",
      "Eval loss 2.0177276134490967, R2 0.32717567682266235\n",
      "epoch 299, loss 1.9344513416290283, R2 0.3295747637748718\n",
      "Eval loss 2.013681411743164, R2 0.32833656668663025\n",
      "epoch 300, loss 1.9304429292678833, R2 0.3307446837425232\n",
      "Eval loss 2.0096511840820312, R2 0.3294932246208191\n",
      "epoch 301, loss 1.9264506101608276, R2 0.33191028237342834\n",
      "Eval loss 2.00563645362854, R2 0.33064568042755127\n",
      "epoch 302, loss 1.9224746227264404, R2 0.3330715000629425\n",
      "Eval loss 2.0016374588012695, R2 0.3317939341068268\n",
      "epoch 303, loss 1.918514370918274, R2 0.33422842621803284\n",
      "Eval loss 1.9976540803909302, R2 0.33293798565864563\n",
      "epoch 304, loss 1.9145700931549072, R2 0.33538103103637695\n",
      "Eval loss 1.9936864376068115, R2 0.334077924489975\n",
      "epoch 305, loss 1.9106416702270508, R2 0.33652934432029724\n",
      "Eval loss 1.9897340536117554, R2 0.33521369099617004\n",
      "epoch 306, loss 1.906728982925415, R2 0.3376733958721161\n",
      "Eval loss 1.9857970476150513, R2 0.3363453149795532\n",
      "epoch 307, loss 1.9028319120407104, R2 0.3388131856918335\n",
      "Eval loss 1.9818757772445679, R2 0.3374728858470917\n",
      "epoch 308, loss 1.8989505767822266, R2 0.33994874358177185\n",
      "Eval loss 1.9779695272445679, R2 0.33859631419181824\n",
      "epoch 309, loss 1.8950846195220947, R2 0.34108009934425354\n",
      "Eval loss 1.9740785360336304, R2 0.3397156298160553\n",
      "epoch 310, loss 1.891234040260315, R2 0.34220725297927856\n",
      "Eval loss 1.9702024459838867, R2 0.34083092212677\n",
      "epoch 311, loss 1.8873988389968872, R2 0.3433302044868469\n",
      "Eval loss 1.9663417339324951, R2 0.34194216132164\n",
      "epoch 312, loss 1.8835792541503906, R2 0.3444490134716034\n",
      "Eval loss 1.962496042251587, R2 0.34304943680763245\n",
      "epoch 313, loss 1.8797749280929565, R2 0.345563679933548\n",
      "Eval loss 1.958665370941162, R2 0.344152569770813\n",
      "epoch 314, loss 1.8759855031967163, R2 0.3466741740703583\n",
      "Eval loss 1.954849362373352, R2 0.34525179862976074\n",
      "epoch 315, loss 1.8722114562988281, R2 0.34778058528900146\n",
      "Eval loss 1.9510483741760254, R2 0.34634706377983093\n",
      "epoch 316, loss 1.8684524297714233, R2 0.34888288378715515\n",
      "Eval loss 1.9472620487213135, R2 0.3474383056163788\n",
      "epoch 317, loss 1.8647081851959229, R2 0.3499811589717865\n",
      "Eval loss 1.9434905052185059, R2 0.34852564334869385\n",
      "epoch 318, loss 1.8609793186187744, R2 0.35107532143592834\n",
      "Eval loss 1.9397335052490234, R2 0.34960904717445374\n",
      "epoch 319, loss 1.8572648763656616, R2 0.35216543078422546\n",
      "Eval loss 1.9359912872314453, R2 0.35068854689598083\n",
      "epoch 320, loss 1.8535654544830322, R2 0.353251576423645\n",
      "Eval loss 1.9322634935379028, R2 0.35176408290863037\n",
      "epoch 321, loss 1.8498808145523071, R2 0.3543336093425751\n",
      "Eval loss 1.9285497665405273, R2 0.3528357744216919\n",
      "epoch 322, loss 1.8462107181549072, R2 0.35541173815727234\n",
      "Eval loss 1.9248510599136353, R2 0.3539036512374878\n",
      "epoch 323, loss 1.8425556421279907, R2 0.35648584365844727\n",
      "Eval loss 1.9211664199829102, R2 0.3549675941467285\n",
      "epoch 324, loss 1.8389146327972412, R2 0.35755598545074463\n",
      "Eval loss 1.9174959659576416, R2 0.3560277223587036\n",
      "epoch 325, loss 1.835288166999817, R2 0.3586221933364868\n",
      "Eval loss 1.9138396978378296, R2 0.3570840656757355\n",
      "epoch 326, loss 1.8316762447357178, R2 0.35968443751335144\n",
      "Eval loss 1.9101976156234741, R2 0.3581365644931793\n",
      "epoch 327, loss 1.8280786275863647, R2 0.36074280738830566\n",
      "Eval loss 1.9065697193145752, R2 0.35918524861335754\n",
      "epoch 328, loss 1.8244954347610474, R2 0.3617972731590271\n",
      "Eval loss 1.9029560089111328, R2 0.3602302372455597\n",
      "epoch 329, loss 1.8209264278411865, R2 0.3628478944301605\n",
      "Eval loss 1.8993561267852783, R2 0.3612714409828186\n",
      "epoch 330, loss 1.8173716068267822, R2 0.36389458179473877\n",
      "Eval loss 1.8957703113555908, R2 0.3623088598251343\n",
      "epoch 331, loss 1.813830852508545, R2 0.3649374544620514\n",
      "Eval loss 1.892198085784912, R2 0.3633425831794739\n",
      "epoch 332, loss 1.8103041648864746, R2 0.365976482629776\n",
      "Eval loss 1.8886399269104004, R2 0.3643725514411926\n",
      "epoch 333, loss 1.8067914247512817, R2 0.3670116662979126\n",
      "Eval loss 1.885095238685608, R2 0.3653988838195801\n",
      "epoch 334, loss 1.8032927513122559, R2 0.36804309487342834\n",
      "Eval loss 1.8815642595291138, R2 0.36642155051231384\n",
      "epoch 335, loss 1.7998077869415283, R2 0.36907070875167847\n",
      "Eval loss 1.8780468702316284, R2 0.36744049191474915\n",
      "epoch 336, loss 1.7963366508483887, R2 0.3700945973396301\n",
      "Eval loss 1.8745431900024414, R2 0.36845576763153076\n",
      "epoch 337, loss 1.792879343032837, R2 0.37111470103263855\n",
      "Eval loss 1.8710530996322632, R2 0.36946743726730347\n",
      "epoch 338, loss 1.789435625076294, R2 0.3721310794353485\n",
      "Eval loss 1.8675761222839355, R2 0.3704754710197449\n",
      "epoch 339, loss 1.7860058546066284, R2 0.37314367294311523\n",
      "Eval loss 1.8641126155853271, R2 0.37147992849349976\n",
      "epoch 340, loss 1.7825894355773926, R2 0.37415260076522827\n",
      "Eval loss 1.860662817955017, R2 0.37248069047927856\n",
      "epoch 341, loss 1.7791863679885864, R2 0.37515783309936523\n",
      "Eval loss 1.857226014137268, R2 0.3734779357910156\n",
      "epoch 342, loss 1.7757970094680786, R2 0.3761593997478485\n",
      "Eval loss 1.8538025617599487, R2 0.37447166442871094\n",
      "epoch 343, loss 1.7724207639694214, R2 0.3771573007106781\n",
      "Eval loss 1.8503921031951904, R2 0.37546178698539734\n",
      "epoch 344, loss 1.769058108329773, R2 0.3781515061855316\n",
      "Eval loss 1.8469948768615723, R2 0.376448392868042\n",
      "epoch 345, loss 1.7657088041305542, R2 0.3791421055793762\n",
      "Eval loss 1.843610405921936, R2 0.3774314820766449\n",
      "epoch 346, loss 1.7623724937438965, R2 0.3801291286945343\n",
      "Eval loss 1.8402390480041504, R2 0.37841105461120605\n",
      "epoch 347, loss 1.759049415588379, R2 0.3811125159263611\n",
      "Eval loss 1.8368808031082153, R2 0.37938711047172546\n",
      "epoch 348, loss 1.755739450454712, R2 0.38209232687950134\n",
      "Eval loss 1.8335354328155518, R2 0.3803596794605255\n",
      "epoch 349, loss 1.752442717552185, R2 0.3830684721469879\n",
      "Eval loss 1.8302031755447388, R2 0.38132885098457336\n",
      "epoch 350, loss 1.7491587400436401, R2 0.3840411901473999\n",
      "Eval loss 1.8268831968307495, R2 0.3822944462299347\n",
      "epoch 351, loss 1.745888113975525, R2 0.385010302066803\n",
      "Eval loss 1.8235763311386108, R2 0.3832567036151886\n",
      "epoch 352, loss 1.7426297664642334, R2 0.3859758675098419\n",
      "Eval loss 1.820281982421875, R2 0.38421550393104553\n",
      "epoch 353, loss 1.739384651184082, R2 0.3869379162788391\n",
      "Eval loss 1.8170000314712524, R2 0.38517090678215027\n",
      "epoch 354, loss 1.7361522912979126, R2 0.38789647817611694\n",
      "Eval loss 1.8137307167053223, R2 0.3861229121685028\n",
      "epoch 355, loss 1.732932448387146, R2 0.3888515532016754\n",
      "Eval loss 1.810474157333374, R2 0.38707152009010315\n",
      "epoch 356, loss 1.7297255992889404, R2 0.3898031413555145\n",
      "Eval loss 1.8072298765182495, R2 0.3880167305469513\n",
      "epoch 357, loss 1.7265310287475586, R2 0.39075127243995667\n",
      "Eval loss 1.8039982318878174, R2 0.3889586329460144\n",
      "epoch 358, loss 1.7233492136001587, R2 0.3916960060596466\n",
      "Eval loss 1.8007787466049194, R2 0.3898971378803253\n",
      "epoch 359, loss 1.7201799154281616, R2 0.3926372528076172\n",
      "Eval loss 1.7975714206695557, R2 0.3908323645591736\n",
      "epoch 360, loss 1.7170228958129883, R2 0.39357510209083557\n",
      "Eval loss 1.7943764925003052, R2 0.39176422357559204\n",
      "epoch 361, loss 1.7138786315917969, R2 0.39450955390930176\n",
      "Eval loss 1.7911938428878784, R2 0.39269283413887024\n",
      "epoch 362, loss 1.71074640750885, R2 0.39544060826301575\n",
      "Eval loss 1.7880232334136963, R2 0.3936181664466858\n",
      "epoch 363, loss 1.707626461982727, R2 0.3963682949542999\n",
      "Eval loss 1.7848647832870483, R2 0.39454013109207153\n",
      "epoch 364, loss 1.7045186758041382, R2 0.3972926139831543\n",
      "Eval loss 1.781718373298645, R2 0.3954589366912842\n",
      "epoch 365, loss 1.7014232873916626, R2 0.39821359515190125\n",
      "Eval loss 1.7785838842391968, R2 0.396374374628067\n",
      "epoch 366, loss 1.6983400583267212, R2 0.3991312086582184\n",
      "Eval loss 1.7754614353179932, R2 0.39728671312332153\n",
      "epoch 367, loss 1.6952685117721558, R2 0.4000455141067505\n",
      "Eval loss 1.7723506689071655, R2 0.39819571375846863\n",
      "epoch 368, loss 1.6922094821929932, R2 0.40095654129981995\n",
      "Eval loss 1.7692521810531616, R2 0.3991015553474426\n",
      "epoch 369, loss 1.689162254333496, R2 0.4018642008304596\n",
      "Eval loss 1.766165018081665, R2 0.40000417828559875\n",
      "epoch 370, loss 1.686126947402954, R2 0.40276864171028137\n",
      "Eval loss 1.7630901336669922, R2 0.4009036719799042\n",
      "epoch 371, loss 1.6831035614013672, R2 0.4036698043346405\n",
      "Eval loss 1.7600263357162476, R2 0.40179985761642456\n",
      "epoch 372, loss 1.6800919771194458, R2 0.404567688703537\n",
      "Eval loss 1.7569745779037476, R2 0.4026930034160614\n",
      "epoch 373, loss 1.6770920753479004, R2 0.4054623544216156\n",
      "Eval loss 1.7539342641830444, R2 0.40358299016952515\n",
      "epoch 374, loss 1.6741039752960205, R2 0.40635380148887634\n",
      "Eval loss 1.7509055137634277, R2 0.4044697880744934\n",
      "epoch 375, loss 1.6711275577545166, R2 0.4072420001029968\n",
      "Eval loss 1.747888445854187, R2 0.40535345673561096\n",
      "epoch 376, loss 1.6681628227233887, R2 0.4081270396709442\n",
      "Eval loss 1.744882583618164, R2 0.4062340259552002\n",
      "epoch 377, loss 1.6652095317840576, R2 0.40900886058807373\n",
      "Eval loss 1.7418882846832275, R2 0.4071115553379059\n",
      "epoch 378, loss 1.662267804145813, R2 0.40988749265670776\n",
      "Eval loss 1.7389051914215088, R2 0.4079858958721161\n",
      "epoch 379, loss 1.6593375205993652, R2 0.4107629656791687\n",
      "Eval loss 1.735933542251587, R2 0.4088572561740875\n",
      "epoch 380, loss 1.6564185619354248, R2 0.4116353392601013\n",
      "Eval loss 1.7329730987548828, R2 0.409725546836853\n",
      "epoch 381, loss 1.6535112857818604, R2 0.41250449419021606\n",
      "Eval loss 1.730023980140686, R2 0.4105907380580902\n",
      "epoch 382, loss 1.6506152153015137, R2 0.4133705198764801\n",
      "Eval loss 1.7270859479904175, R2 0.41145288944244385\n",
      "epoch 383, loss 1.6477303504943848, R2 0.414233535528183\n",
      "Eval loss 1.7241591215133667, R2 0.4123120903968811\n",
      "epoch 384, loss 1.6448566913604736, R2 0.41509345173835754\n",
      "Eval loss 1.721243143081665, R2 0.41316816210746765\n",
      "epoch 385, loss 1.6419942378997803, R2 0.41595014929771423\n",
      "Eval loss 1.7183382511138916, R2 0.41402125358581543\n",
      "epoch 386, loss 1.6391431093215942, R2 0.41680383682250977\n",
      "Eval loss 1.715444564819336, R2 0.414871484041214\n",
      "epoch 387, loss 1.6363028287887573, R2 0.41765448451042175\n",
      "Eval loss 1.7125617265701294, R2 0.41571861505508423\n",
      "epoch 388, loss 1.6334736347198486, R2 0.4185020923614502\n",
      "Eval loss 1.709689736366272, R2 0.4165628254413605\n",
      "epoch 389, loss 1.6306555271148682, R2 0.41934657096862793\n",
      "Eval loss 1.7068283557891846, R2 0.41740405559539795\n",
      "epoch 390, loss 1.6278481483459473, R2 0.4201880991458893\n",
      "Eval loss 1.703978180885315, R2 0.41824230551719666\n",
      "epoch 391, loss 1.6250519752502441, R2 0.4210266172885895\n",
      "Eval loss 1.7011384963989258, R2 0.41907772421836853\n",
      "epoch 392, loss 1.6222665309906006, R2 0.4218621253967285\n",
      "Eval loss 1.6983095407485962, R2 0.41991016268730164\n",
      "epoch 393, loss 1.6194918155670166, R2 0.4226946234703064\n",
      "Eval loss 1.6954914331436157, R2 0.42073968052864075\n",
      "epoch 394, loss 1.6167278289794922, R2 0.4235242009162903\n",
      "Eval loss 1.6926838159561157, R2 0.421566367149353\n",
      "epoch 395, loss 1.6139745712280273, R2 0.424350768327713\n",
      "Eval loss 1.6898868083953857, R2 0.42239007353782654\n",
      "epoch 396, loss 1.6112321615219116, R2 0.425174355506897\n",
      "Eval loss 1.6871002912521362, R2 0.4232109487056732\n",
      "epoch 397, loss 1.608500361442566, R2 0.4259950518608093\n",
      "Eval loss 1.6843242645263672, R2 0.4240289628505707\n",
      "epoch 398, loss 1.6057790517807007, R2 0.4268128275871277\n",
      "Eval loss 1.6815588474273682, R2 0.4248441755771637\n",
      "epoch 399, loss 1.6030683517456055, R2 0.4276275932788849\n",
      "Eval loss 1.6788034439086914, R2 0.42565643787384033\n",
      "epoch 400, loss 1.6003681421279907, R2 0.42843952775001526\n",
      "Eval loss 1.6760588884353638, R2 0.4264659583568573\n",
      "epoch 401, loss 1.5976784229278564, R2 0.42924854159355164\n",
      "Eval loss 1.6733243465423584, R2 0.4272726774215698\n",
      "epoch 402, loss 1.5949989557266235, R2 0.4300546944141388\n",
      "Eval loss 1.6706000566482544, R2 0.42807653546333313\n",
      "epoch 403, loss 1.592329978942871, R2 0.43085789680480957\n",
      "Eval loss 1.6678862571716309, R2 0.4288776218891144\n",
      "epoch 404, loss 1.5896711349487305, R2 0.4316583573818207\n",
      "Eval loss 1.6651824712753296, R2 0.4296759366989136\n",
      "epoch 405, loss 1.5870229005813599, R2 0.4324558973312378\n",
      "Eval loss 1.6624886989593506, R2 0.4304714500904083\n",
      "epoch 406, loss 1.5843844413757324, R2 0.43325063586235046\n",
      "Eval loss 1.659805178642273, R2 0.43126416206359863\n",
      "epoch 407, loss 1.581756591796875, R2 0.4340425431728363\n",
      "Eval loss 1.6571316719055176, R2 0.43205422163009644\n",
      "epoch 408, loss 1.5791383981704712, R2 0.4348316490650177\n",
      "Eval loss 1.6544684171676636, R2 0.4328414499759674\n",
      "epoch 409, loss 1.5765306949615479, R2 0.43561792373657227\n",
      "Eval loss 1.6518148183822632, R2 0.4336259663105011\n",
      "epoch 410, loss 1.5739330053329468, R2 0.4364013671875\n",
      "Eval loss 1.6491713523864746, R2 0.4344078004360199\n",
      "epoch 411, loss 1.5713452100753784, R2 0.43718209862709045\n",
      "Eval loss 1.6465376615524292, R2 0.43518686294555664\n",
      "epoch 412, loss 1.5687674283981323, R2 0.43796002864837646\n",
      "Eval loss 1.6439138650894165, R2 0.43596330285072327\n",
      "epoch 413, loss 1.566199541091919, R2 0.4387352168560028\n",
      "Eval loss 1.6412999629974365, R2 0.43673697113990784\n",
      "epoch 414, loss 1.5636416673660278, R2 0.4395076334476471\n",
      "Eval loss 1.6386958360671997, R2 0.43750807642936707\n",
      "epoch 415, loss 1.5610933303833008, R2 0.4402773082256317\n",
      "Eval loss 1.6361013650894165, R2 0.43827641010284424\n",
      "epoch 416, loss 1.558555245399475, R2 0.44104424118995667\n",
      "Eval loss 1.6335166692733765, R2 0.4390421211719513\n",
      "epoch 417, loss 1.556026577949524, R2 0.4418085217475891\n",
      "Eval loss 1.6309415102005005, R2 0.43980517983436584\n",
      "epoch 418, loss 1.5535080432891846, R2 0.4425700902938843\n",
      "Eval loss 1.6283761262893677, R2 0.4405655860900879\n",
      "epoch 419, loss 1.5509986877441406, R2 0.4433288872241974\n",
      "Eval loss 1.6258201599121094, R2 0.44132348895072937\n",
      "epoch 420, loss 1.5484992265701294, R2 0.4440850615501404\n",
      "Eval loss 1.6232736110687256, R2 0.4420786201953888\n",
      "epoch 421, loss 1.5460094213485718, R2 0.44483858346939087\n",
      "Eval loss 1.6207369565963745, R2 0.44283121824264526\n",
      "epoch 422, loss 1.5435290336608887, R2 0.44558942317962646\n",
      "Eval loss 1.6182092428207397, R2 0.4435811936855316\n",
      "epoch 423, loss 1.5410581827163696, R2 0.44633758068084717\n",
      "Eval loss 1.6156915426254272, R2 0.4443286061286926\n",
      "epoch 424, loss 1.5385968685150146, R2 0.447083055973053\n",
      "Eval loss 1.613182783126831, R2 0.4450734555721283\n",
      "epoch 425, loss 1.5361448526382446, R2 0.4478260278701782\n",
      "Eval loss 1.610683560371399, R2 0.445815771818161\n",
      "epoch 426, loss 1.5337023735046387, R2 0.4485663175582886\n",
      "Eval loss 1.608193278312683, R2 0.4465554356575012\n",
      "epoch 427, loss 1.5312693119049072, R2 0.4493040144443512\n",
      "Eval loss 1.6057125329971313, R2 0.44729262590408325\n",
      "epoch 428, loss 1.528845191001892, R2 0.4500390887260437\n",
      "Eval loss 1.6032413244247437, R2 0.44802728295326233\n",
      "epoch 429, loss 1.526430606842041, R2 0.4507715106010437\n",
      "Eval loss 1.6007790565490723, R2 0.44875937700271606\n",
      "epoch 430, loss 1.5240254402160645, R2 0.4515014886856079\n",
      "Eval loss 1.5983257293701172, R2 0.44948896765708923\n",
      "epoch 431, loss 1.5216293334960938, R2 0.4522288143634796\n",
      "Eval loss 1.5958819389343262, R2 0.4502160847187042\n",
      "epoch 432, loss 1.5192424058914185, R2 0.45295360684394836\n",
      "Eval loss 1.5934466123580933, R2 0.45094063878059387\n",
      "epoch 433, loss 1.5168644189834595, R2 0.4536758363246918\n",
      "Eval loss 1.5910207033157349, R2 0.4516627788543701\n",
      "epoch 434, loss 1.514495849609375, R2 0.4543955624103546\n",
      "Eval loss 1.5886039733886719, R2 0.4523823857307434\n",
      "epoch 435, loss 1.5121361017227173, R2 0.4551127254962921\n",
      "Eval loss 1.5861958265304565, R2 0.4530995786190033\n",
      "epoch 436, loss 1.5097852945327759, R2 0.45582741498947144\n",
      "Eval loss 1.5837968587875366, R2 0.4538142681121826\n",
      "epoch 437, loss 1.5074435472488403, R2 0.4565395414829254\n",
      "Eval loss 1.581406593322754, R2 0.45452654361724854\n",
      "epoch 438, loss 1.5051108598709106, R2 0.45724916458129883\n",
      "Eval loss 1.5790250301361084, R2 0.45523640513420105\n",
      "epoch 439, loss 1.5027868747711182, R2 0.45795634388923645\n",
      "Eval loss 1.5766526460647583, R2 0.4559437930583954\n",
      "epoch 440, loss 1.500472068786621, R2 0.4586610198020935\n",
      "Eval loss 1.5742888450622559, R2 0.4566487669944763\n",
      "epoch 441, loss 1.4981657266616821, R2 0.4593632221221924\n",
      "Eval loss 1.5719338655471802, R2 0.45735129714012146\n",
      "epoch 442, loss 1.495868444442749, R2 0.4600629508495331\n",
      "Eval loss 1.5695877075195312, R2 0.45805150270462036\n",
      "epoch 443, loss 1.4935797452926636, R2 0.46076029539108276\n",
      "Eval loss 1.5672500133514404, R2 0.45874929428100586\n",
      "epoch 444, loss 1.4912997484207153, R2 0.4614551365375519\n",
      "Eval loss 1.5649213790893555, R2 0.45944470167160034\n",
      "epoch 445, loss 1.4890285730361938, R2 0.46214765310287476\n",
      "Eval loss 1.5626006126403809, R2 0.4601376950740814\n",
      "epoch 446, loss 1.4867658615112305, R2 0.4628375768661499\n",
      "Eval loss 1.560288906097412, R2 0.46082839369773865\n",
      "epoch 447, loss 1.4845118522644043, R2 0.46352529525756836\n",
      "Eval loss 1.557985782623291, R2 0.46151667833328247\n",
      "epoch 448, loss 1.4822663068771362, R2 0.4642104208469391\n",
      "Eval loss 1.5556907653808594, R2 0.46220263838768005\n",
      "epoch 449, loss 1.4800292253494263, R2 0.46489325165748596\n",
      "Eval loss 1.553404450416565, R2 0.462886244058609\n",
      "epoch 450, loss 1.4778008460998535, R2 0.4655736982822418\n",
      "Eval loss 1.5511268377304077, R2 0.4635675549507141\n",
      "epoch 451, loss 1.4755809307098389, R2 0.4662517309188843\n",
      "Eval loss 1.5488572120666504, R2 0.464246541261673\n",
      "epoch 452, loss 1.4733693599700928, R2 0.4669274091720581\n",
      "Eval loss 1.5465961694717407, R2 0.4649232029914856\n",
      "epoch 453, loss 1.4711662530899048, R2 0.4676007926464081\n",
      "Eval loss 1.54434335231781, R2 0.46559759974479675\n",
      "epoch 454, loss 1.4689714908599854, R2 0.46827173233032227\n",
      "Eval loss 1.542098879814148, R2 0.46626970171928406\n",
      "epoch 455, loss 1.4667848348617554, R2 0.4689404368400574\n",
      "Eval loss 1.5398626327514648, R2 0.46693944931030273\n",
      "epoch 456, loss 1.4646068811416626, R2 0.46960675716400146\n",
      "Eval loss 1.5376346111297607, R2 0.4676070213317871\n",
      "epoch 457, loss 1.4624367952346802, R2 0.47027072310447693\n",
      "Eval loss 1.5354145765304565, R2 0.46827226877212524\n",
      "epoch 458, loss 1.4602751731872559, R2 0.4709324538707733\n",
      "Eval loss 1.533202886581421, R2 0.4689352810382843\n",
      "epoch 459, loss 1.4581215381622314, R2 0.47159186005592346\n",
      "Eval loss 1.5309993028640747, R2 0.4695959687232971\n",
      "epoch 460, loss 1.4559762477874756, R2 0.47224897146224976\n",
      "Eval loss 1.528803825378418, R2 0.4702545404434204\n",
      "epoch 461, loss 1.45383882522583, R2 0.4729038178920746\n",
      "Eval loss 1.5266164541244507, R2 0.470910906791687\n",
      "epoch 462, loss 1.4517097473144531, R2 0.47355639934539795\n",
      "Eval loss 1.5244371891021729, R2 0.4715648889541626\n",
      "epoch 463, loss 1.4495885372161865, R2 0.4742066562175751\n",
      "Eval loss 1.5222656726837158, R2 0.47221675515174866\n",
      "epoch 464, loss 1.4474754333496094, R2 0.4748547673225403\n",
      "Eval loss 1.5201021432876587, R2 0.47286632657051086\n",
      "epoch 465, loss 1.4453703165054321, R2 0.47550055384635925\n",
      "Eval loss 1.517946720123291, R2 0.4735139012336731\n",
      "epoch 466, loss 1.4432730674743652, R2 0.47614404559135437\n",
      "Eval loss 1.5157989263534546, R2 0.4741591513156891\n",
      "epoch 467, loss 1.4411838054656982, R2 0.4767853915691376\n",
      "Eval loss 1.5136593580245972, R2 0.4748022258281708\n",
      "epoch 468, loss 1.4391025304794312, R2 0.4774245023727417\n",
      "Eval loss 1.5115272998809814, R2 0.47544315457344055\n",
      "epoch 469, loss 1.4370288848876953, R2 0.47806140780448914\n",
      "Eval loss 1.509403109550476, R2 0.47608184814453125\n",
      "epoch 470, loss 1.4349629878997803, R2 0.47869616746902466\n",
      "Eval loss 1.5072869062423706, R2 0.4767184555530548\n",
      "epoch 471, loss 1.4329051971435547, R2 0.47932863235473633\n",
      "Eval loss 1.5051780939102173, R2 0.47735294699668884\n",
      "epoch 472, loss 1.4308547973632812, R2 0.4799589216709137\n",
      "Eval loss 1.5030772686004639, R2 0.4779852628707886\n",
      "epoch 473, loss 1.4288125038146973, R2 0.48058709502220154\n",
      "Eval loss 1.5009838342666626, R2 0.4786154329776764\n",
      "epoch 474, loss 1.4267776012420654, R2 0.4812130630016327\n",
      "Eval loss 1.4988983869552612, R2 0.4792434871196747\n",
      "epoch 475, loss 1.4247503280639648, R2 0.48183685541152954\n",
      "Eval loss 1.496820330619812, R2 0.47986945509910583\n",
      "epoch 476, loss 1.4227309226989746, R2 0.4824585020542145\n",
      "Eval loss 1.494749903678894, R2 0.4804932773113251\n",
      "epoch 477, loss 1.420719027519226, R2 0.48307809233665466\n",
      "Eval loss 1.4926869869232178, R2 0.48111504316329956\n",
      "epoch 478, loss 1.4187146425247192, R2 0.4836954176425934\n",
      "Eval loss 1.4906315803527832, R2 0.4817346930503845\n",
      "epoch 479, loss 1.4167178869247437, R2 0.48431068658828735\n",
      "Eval loss 1.4885838031768799, R2 0.48235228657722473\n",
      "epoch 480, loss 1.4147285223007202, R2 0.484923779964447\n",
      "Eval loss 1.4865432977676392, R2 0.4829677641391754\n",
      "epoch 481, loss 1.412746548652649, R2 0.4855348467826843\n",
      "Eval loss 1.4845103025436401, R2 0.4835812449455261\n",
      "epoch 482, loss 1.4107722043991089, R2 0.48614373803138733\n",
      "Eval loss 1.4824846982955933, R2 0.4841926097869873\n",
      "epoch 483, loss 1.408805251121521, R2 0.4867505729198456\n",
      "Eval loss 1.480466604232788, R2 0.4848019480705261\n",
      "epoch 484, loss 1.4068454504013062, R2 0.4873552918434143\n",
      "Eval loss 1.4784555435180664, R2 0.4854092299938202\n",
      "epoch 485, loss 1.404893159866333, R2 0.48795801401138306\n",
      "Eval loss 1.4764519929885864, R2 0.4860144853591919\n",
      "epoch 486, loss 1.402948260307312, R2 0.4885586202144623\n",
      "Eval loss 1.474455714225769, R2 0.48661768436431885\n",
      "epoch 487, loss 1.401010513305664, R2 0.489157110452652\n",
      "Eval loss 1.4724665880203247, R2 0.4872188866138458\n",
      "epoch 488, loss 1.3990799188613892, R2 0.4897536337375641\n",
      "Eval loss 1.470484733581543, R2 0.48781806230545044\n",
      "epoch 489, loss 1.397156834602356, R2 0.49034807085990906\n",
      "Eval loss 1.4685101509094238, R2 0.48841530084609985\n",
      "epoch 490, loss 1.3952407836914062, R2 0.4909404516220093\n",
      "Eval loss 1.4665427207946777, R2 0.4890104830265045\n",
      "epoch 491, loss 1.3933318853378296, R2 0.4915308356285095\n",
      "Eval loss 1.4645824432373047, R2 0.4896036982536316\n",
      "epoch 492, loss 1.3914300203323364, R2 0.4921192228794098\n",
      "Eval loss 1.4626291990280151, R2 0.4901949465274811\n",
      "epoch 493, loss 1.3895354270935059, R2 0.49270549416542053\n",
      "Eval loss 1.460682988166809, R2 0.4907841682434082\n",
      "epoch 494, loss 1.387648105621338, R2 0.49328985810279846\n",
      "Eval loss 1.4587438106536865, R2 0.4913715124130249\n",
      "epoch 495, loss 1.3857675790786743, R2 0.4938722252845764\n",
      "Eval loss 1.4568116664886475, R2 0.49195683002471924\n",
      "epoch 496, loss 1.3838938474655151, R2 0.49445250630378723\n",
      "Eval loss 1.4548866748809814, R2 0.49254024028778076\n",
      "epoch 497, loss 1.3820276260375977, R2 0.49503087997436523\n",
      "Eval loss 1.4529683589935303, R2 0.4931216537952423\n",
      "epoch 498, loss 1.3801679611206055, R2 0.49560728669166565\n",
      "Eval loss 1.4510571956634521, R2 0.49370113015174866\n",
      "epoch 499, loss 1.3783154487609863, R2 0.4961816966533661\n",
      "Eval loss 1.449152946472168, R2 0.4942786991596222\n",
      "epoch 500, loss 1.3764697313308716, R2 0.49675416946411133\n",
      "Eval loss 1.4472554922103882, R2 0.4948543608188629\n",
      "epoch 501, loss 1.3746309280395508, R2 0.4973246455192566\n",
      "Eval loss 1.4453648328781128, R2 0.4954281151294708\n",
      "epoch 502, loss 1.3727989196777344, R2 0.4978931248188019\n",
      "Eval loss 1.4434810876846313, R2 0.4959999918937683\n",
      "epoch 503, loss 1.370973825454712, R2 0.49845975637435913\n",
      "Eval loss 1.4416041374206543, R2 0.49656984210014343\n",
      "epoch 504, loss 1.3691555261611938, R2 0.49902451038360596\n",
      "Eval loss 1.439733862876892, R2 0.4971379041671753\n",
      "epoch 505, loss 1.367343783378601, R2 0.4995872676372528\n",
      "Eval loss 1.4378703832626343, R2 0.4977041184902191\n",
      "epoch 506, loss 1.3655388355255127, R2 0.5001481175422668\n",
      "Eval loss 1.4360138177871704, R2 0.49826839566230774\n",
      "epoch 507, loss 1.3637406826019287, R2 0.5007070302963257\n",
      "Eval loss 1.4341638088226318, R2 0.49883076548576355\n",
      "epoch 508, loss 1.3619493246078491, R2 0.5012640953063965\n",
      "Eval loss 1.4323203563690186, R2 0.4993913471698761\n",
      "epoch 509, loss 1.3601646423339844, R2 0.5018192529678345\n",
      "Eval loss 1.4304836988449097, R2 0.49994996190071106\n",
      "epoch 510, loss 1.3583863973617554, R2 0.5023724436759949\n",
      "Eval loss 1.428653359413147, R2 0.5005068182945251\n",
      "epoch 511, loss 1.356614589691162, R2 0.5029239058494568\n",
      "Eval loss 1.4268298149108887, R2 0.5010617971420288\n",
      "epoch 512, loss 1.3548496961593628, R2 0.5034733414649963\n",
      "Eval loss 1.4250127077102661, R2 0.5016149282455444\n",
      "epoch 513, loss 1.3530912399291992, R2 0.5040209293365479\n",
      "Eval loss 1.423202395439148, R2 0.5021663308143616\n",
      "epoch 514, loss 1.351339340209961, R2 0.5045667886734009\n",
      "Eval loss 1.421398401260376, R2 0.5027158260345459\n",
      "epoch 515, loss 1.3495938777923584, R2 0.5051107406616211\n",
      "Eval loss 1.4196008443832397, R2 0.503263533115387\n",
      "epoch 516, loss 1.3478549718856812, R2 0.5056528449058533\n",
      "Eval loss 1.4178097248077393, R2 0.50380939245224\n",
      "epoch 517, loss 1.34612238407135, R2 0.5061931014060974\n",
      "Eval loss 1.416025161743164, R2 0.5043534636497498\n",
      "epoch 518, loss 1.3443961143493652, R2 0.5067315101623535\n",
      "Eval loss 1.4142470359802246, R2 0.504895806312561\n",
      "epoch 519, loss 1.3426764011383057, R2 0.5072681307792664\n",
      "Eval loss 1.4124751091003418, R2 0.5054361820220947\n",
      "epoch 520, loss 1.3409631252288818, R2 0.5078029036521912\n",
      "Eval loss 1.4107096195220947, R2 0.5059749484062195\n",
      "epoch 521, loss 1.3392560482025146, R2 0.5083358883857727\n",
      "Eval loss 1.4089504480361938, R2 0.506511926651001\n",
      "epoch 522, loss 1.3375552892684937, R2 0.5088670253753662\n",
      "Eval loss 1.4071974754333496, R2 0.5070470571517944\n",
      "epoch 523, loss 1.3358608484268188, R2 0.509396493434906\n",
      "Eval loss 1.4054509401321411, R2 0.5075804591178894\n",
      "epoch 524, loss 1.3341724872589111, R2 0.5099239945411682\n",
      "Eval loss 1.4037106037139893, R2 0.5081121921539307\n",
      "epoch 525, loss 1.3324905633926392, R2 0.5104498267173767\n",
      "Eval loss 1.4019763469696045, R2 0.5086420178413391\n",
      "epoch 526, loss 1.3308148384094238, R2 0.5109739303588867\n",
      "Eval loss 1.4002485275268555, R2 0.5091702342033386\n",
      "epoch 527, loss 1.3291451930999756, R2 0.5114961862564087\n",
      "Eval loss 1.398526668548584, R2 0.5096966624259949\n",
      "epoch 528, loss 1.3274818658828735, R2 0.5120167136192322\n",
      "Eval loss 1.3968110084533691, R2 0.5102213025093079\n",
      "epoch 529, loss 1.325824499130249, R2 0.5125355124473572\n",
      "Eval loss 1.395101547241211, R2 0.5107443332672119\n",
      "epoch 530, loss 1.3241733312606812, R2 0.5130524635314941\n",
      "Eval loss 1.3933982849121094, R2 0.5112655758857727\n",
      "epoch 531, loss 1.3225281238555908, R2 0.5135677456855774\n",
      "Eval loss 1.3917008638381958, R2 0.511785089969635\n",
      "epoch 532, loss 1.3208891153335571, R2 0.5140813589096069\n",
      "Eval loss 1.3900096416473389, R2 0.5123029947280884\n",
      "epoch 533, loss 1.319256067276001, R2 0.5145931243896484\n",
      "Eval loss 1.38832426071167, R2 0.5128191709518433\n",
      "epoch 534, loss 1.317629098892212, R2 0.515103280544281\n",
      "Eval loss 1.3866451978683472, R2 0.5133335590362549\n",
      "epoch 535, loss 1.3160080909729004, R2 0.5156115889549255\n",
      "Eval loss 1.3849718570709229, R2 0.5138463377952576\n",
      "epoch 536, loss 1.3143930435180664, R2 0.5161182284355164\n",
      "Eval loss 1.3833045959472656, R2 0.5143575072288513\n",
      "epoch 537, loss 1.3127838373184204, R2 0.516623318195343\n",
      "Eval loss 1.3816431760787964, R2 0.5148668885231018\n",
      "epoch 538, loss 1.311180591583252, R2 0.5171265006065369\n",
      "Eval loss 1.3799875974655151, R2 0.5153747200965881\n",
      "epoch 539, loss 1.3095831871032715, R2 0.5176280736923218\n",
      "Eval loss 1.3783382177352905, R2 0.5158807635307312\n",
      "epoch 540, loss 1.307991862297058, R2 0.5181280374526978\n",
      "Eval loss 1.3766945600509644, R2 0.5163852572441101\n",
      "epoch 541, loss 1.3064061403274536, R2 0.5186262726783752\n",
      "Eval loss 1.375056505203247, R2 0.5168880224227905\n",
      "epoch 542, loss 1.304826259613037, R2 0.519122838973999\n",
      "Eval loss 1.3734244108200073, R2 0.517389178276062\n",
      "epoch 543, loss 1.3032522201538086, R2 0.5196177363395691\n",
      "Eval loss 1.3717981576919556, R2 0.5178887248039246\n",
      "epoch 544, loss 1.301684021949768, R2 0.5201109647750854\n",
      "Eval loss 1.3701777458190918, R2 0.5183866620063782\n",
      "epoch 545, loss 1.300121545791626, R2 0.5206025838851929\n",
      "Eval loss 1.368562936782837, R2 0.5188828706741333\n",
      "epoch 546, loss 1.2985646724700928, R2 0.5210925340652466\n",
      "Eval loss 1.3669540882110596, R2 0.5193775296211243\n",
      "epoch 547, loss 1.2970134019851685, R2 0.5215808153152466\n",
      "Eval loss 1.365350604057312, R2 0.5198706388473511\n",
      "epoch 548, loss 1.2954682111740112, R2 0.5220674872398376\n",
      "Eval loss 1.3637529611587524, R2 0.5203620791435242\n",
      "epoch 549, loss 1.2939285039901733, R2 0.522552490234375\n",
      "Eval loss 1.3621609210968018, R2 0.5208519101142883\n",
      "epoch 550, loss 1.2923942804336548, R2 0.523036003112793\n",
      "Eval loss 1.360574722290039, R2 0.5213401317596436\n",
      "epoch 551, loss 1.2908657789230347, R2 0.5235177874565125\n",
      "Eval loss 1.3589938879013062, R2 0.5218268632888794\n",
      "epoch 552, loss 1.2893427610397339, R2 0.5239980220794678\n",
      "Eval loss 1.3574187755584717, R2 0.5223119258880615\n",
      "epoch 553, loss 1.287825345993042, R2 0.5244766473770142\n",
      "Eval loss 1.3558491468429565, R2 0.5227953791618347\n",
      "epoch 554, loss 1.2863134145736694, R2 0.5249536633491516\n",
      "Eval loss 1.3542852401733398, R2 0.5232773423194885\n",
      "epoch 555, loss 1.2848072052001953, R2 0.5254290103912354\n",
      "Eval loss 1.352726697921753, R2 0.5237577557563782\n",
      "epoch 556, loss 1.283306360244751, R2 0.5259029269218445\n",
      "Eval loss 1.3511736392974854, R2 0.5242366194725037\n",
      "epoch 557, loss 1.281810998916626, R2 0.5263751745223999\n",
      "Eval loss 1.349626064300537, R2 0.5247138142585754\n",
      "epoch 558, loss 1.2803211212158203, R2 0.5268459320068359\n",
      "Eval loss 1.3480840921401978, R2 0.5251895785331726\n",
      "epoch 559, loss 1.2788366079330444, R2 0.5273150205612183\n",
      "Eval loss 1.3465473651885986, R2 0.5256637930870056\n",
      "epoch 560, loss 1.277357578277588, R2 0.5277826189994812\n",
      "Eval loss 1.3450162410736084, R2 0.5261364579200745\n",
      "epoch 561, loss 1.275883674621582, R2 0.5282486081123352\n",
      "Eval loss 1.3434903621673584, R2 0.5266075730323792\n",
      "epoch 562, loss 1.274415373802185, R2 0.5287131071090698\n",
      "Eval loss 1.3419699668884277, R2 0.5270771980285645\n",
      "epoch 563, loss 1.2729524374008179, R2 0.5291760563850403\n",
      "Eval loss 1.3404549360275269, R2 0.5275452136993408\n",
      "epoch 564, loss 1.2714948654174805, R2 0.5296374559402466\n",
      "Eval loss 1.3389451503753662, R2 0.5280117988586426\n",
      "epoch 565, loss 1.2700425386428833, R2 0.5300973057746887\n",
      "Eval loss 1.3374407291412354, R2 0.528476893901825\n",
      "epoch 566, loss 1.2685953378677368, R2 0.5305556654930115\n",
      "Eval loss 1.3359415531158447, R2 0.5289403796195984\n",
      "epoch 567, loss 1.2671535015106201, R2 0.5310124158859253\n",
      "Eval loss 1.3344476222991943, R2 0.5294024348258972\n",
      "epoch 568, loss 1.2657169103622437, R2 0.5314677357673645\n",
      "Eval loss 1.3329589366912842, R2 0.5298631191253662\n",
      "epoch 569, loss 1.2642854452133179, R2 0.5319215655326843\n",
      "Eval loss 1.3314756155014038, R2 0.5303221344947815\n",
      "epoch 570, loss 1.2628593444824219, R2 0.53237384557724\n",
      "Eval loss 1.3299973011016846, R2 0.5307797789573669\n",
      "epoch 571, loss 1.2614383697509766, R2 0.532824695110321\n",
      "Eval loss 1.3285243511199951, R2 0.5312359929084778\n",
      "epoch 572, loss 1.2600224018096924, R2 0.5332740545272827\n",
      "Eval loss 1.3270565271377563, R2 0.5316905975341797\n",
      "epoch 573, loss 1.2586116790771484, R2 0.5337218642234802\n",
      "Eval loss 1.3255937099456787, R2 0.5321438312530518\n",
      "epoch 574, loss 1.2572060823440552, R2 0.5341681838035583\n",
      "Eval loss 1.3241361379623413, R2 0.5325956344604492\n",
      "epoch 575, loss 1.255805492401123, R2 0.5346130728721619\n",
      "Eval loss 1.3226836919784546, R2 0.5330458879470825\n",
      "epoch 576, loss 1.2544100284576416, R2 0.535056471824646\n",
      "Eval loss 1.321236252784729, R2 0.5334947109222412\n",
      "epoch 577, loss 1.2530195713043213, R2 0.5354984402656555\n",
      "Eval loss 1.319793939590454, R2 0.5339422225952148\n",
      "epoch 578, loss 1.2516342401504517, R2 0.5359388589859009\n",
      "Eval loss 1.3183566331863403, R2 0.5343880653381348\n",
      "epoch 579, loss 1.2502537965774536, R2 0.5363779664039612\n",
      "Eval loss 1.3169244527816772, R2 0.5348327159881592\n",
      "epoch 580, loss 1.2488784790039062, R2 0.5368155837059021\n",
      "Eval loss 1.3154972791671753, R2 0.5352757573127747\n",
      "epoch 581, loss 1.2475080490112305, R2 0.5372516512870789\n",
      "Eval loss 1.314074993133545, R2 0.5357174873352051\n",
      "epoch 582, loss 1.2461426258087158, R2 0.5376864075660706\n",
      "Eval loss 1.3126577138900757, R2 0.5361577868461609\n",
      "epoch 583, loss 1.2447822093963623, R2 0.5381197333335876\n",
      "Eval loss 1.311245322227478, R2 0.5365965962409973\n",
      "epoch 584, loss 1.2434264421463013, R2 0.5385515689849854\n",
      "Eval loss 1.3098379373550415, R2 0.5370340943336487\n",
      "epoch 585, loss 1.242075800895691, R2 0.5389819741249084\n",
      "Eval loss 1.308435320854187, R2 0.5374701619148254\n",
      "epoch 586, loss 1.240729808807373, R2 0.5394109487533569\n",
      "Eval loss 1.3070378303527832, R2 0.5379048585891724\n",
      "epoch 587, loss 1.2393888235092163, R2 0.5398386716842651\n",
      "Eval loss 1.3056448698043823, R2 0.5383381843566895\n",
      "epoch 588, loss 1.2380527257919312, R2 0.5402648448944092\n",
      "Eval loss 1.3042571544647217, R2 0.5387700796127319\n",
      "epoch 589, loss 1.2367212772369385, R2 0.5406897068023682\n",
      "Eval loss 1.3028740882873535, R2 0.5392005443572998\n",
      "epoch 590, loss 1.2353945970535278, R2 0.5411131381988525\n",
      "Eval loss 1.3014957904815674, R2 0.5396297574043274\n",
      "epoch 591, loss 1.2340728044509888, R2 0.5415351390838623\n",
      "Eval loss 1.3001223802566528, R2 0.5400574803352356\n",
      "epoch 592, loss 1.2327557802200317, R2 0.5419557690620422\n",
      "Eval loss 1.2987537384033203, R2 0.5404838919639587\n",
      "epoch 593, loss 1.2314435243606567, R2 0.5423750281333923\n",
      "Eval loss 1.2973898649215698, R2 0.5409089922904968\n",
      "epoch 594, loss 1.2301359176635742, R2 0.5427929162979126\n",
      "Eval loss 1.2960307598114014, R2 0.5413326621055603\n",
      "epoch 595, loss 1.2288329601287842, R2 0.5432094931602478\n",
      "Eval loss 1.2946761846542358, R2 0.5417550802230835\n",
      "epoch 596, loss 1.2275347709655762, R2 0.5436246395111084\n",
      "Eval loss 1.293326497077942, R2 0.5421760082244873\n",
      "epoch 597, loss 1.2262412309646606, R2 0.5440384745597839\n",
      "Eval loss 1.2919814586639404, R2 0.5425956845283508\n",
      "epoch 598, loss 1.2249523401260376, R2 0.5444509387016296\n",
      "Eval loss 1.290640950202942, R2 0.5430140495300293\n",
      "epoch 599, loss 1.2236679792404175, R2 0.5448619723320007\n",
      "Eval loss 1.2893054485321045, R2 0.5434310436248779\n",
      "epoch 600, loss 1.2223883867263794, R2 0.5452718138694763\n",
      "Eval loss 1.287974238395691, R2 0.5438467264175415\n",
      "epoch 601, loss 1.2211132049560547, R2 0.5456802248954773\n",
      "Eval loss 1.2866476774215698, R2 0.54426109790802\n",
      "epoch 602, loss 1.219842791557312, R2 0.5460873246192932\n",
      "Eval loss 1.2853258848190308, R2 0.5446741580963135\n",
      "epoch 603, loss 1.2185766696929932, R2 0.5464931130409241\n",
      "Eval loss 1.284008502960205, R2 0.5450859069824219\n",
      "epoch 604, loss 1.2173153162002563, R2 0.5468974709510803\n",
      "Eval loss 1.2826956510543823, R2 0.5454962849617004\n",
      "epoch 605, loss 1.216058373451233, R2 0.5473006367683411\n",
      "Eval loss 1.2813875675201416, R2 0.5459054708480835\n",
      "epoch 606, loss 1.2148059606552124, R2 0.5477024912834167\n",
      "Eval loss 1.2800838947296143, R2 0.5463132262229919\n",
      "epoch 607, loss 1.2135581970214844, R2 0.5481029748916626\n",
      "Eval loss 1.2787847518920898, R2 0.5467197895050049\n",
      "epoch 608, loss 1.212314486503601, R2 0.5485022068023682\n",
      "Eval loss 1.2774900197982788, R2 0.5471250414848328\n",
      "epoch 609, loss 1.2110754251480103, R2 0.5489001274108887\n",
      "Eval loss 1.2761998176574707, R2 0.5475290417671204\n",
      "epoch 610, loss 1.2098408937454224, R2 0.5492967367172241\n",
      "Eval loss 1.2749139070510864, R2 0.5479317307472229\n",
      "epoch 611, loss 1.2086106538772583, R2 0.5496920347213745\n",
      "Eval loss 1.2736327648162842, R2 0.5483331680297852\n",
      "epoch 612, loss 1.2073848247528076, R2 0.5500860810279846\n",
      "Eval loss 1.2723557949066162, R2 0.5487332344055176\n",
      "epoch 613, loss 1.2061632871627808, R2 0.5504788160324097\n",
      "Eval loss 1.2710832357406616, R2 0.5491321682929993\n",
      "epoch 614, loss 1.2049463987350464, R2 0.5508702993392944\n",
      "Eval loss 1.26981520652771, R2 0.5495297908782959\n",
      "epoch 615, loss 1.2037334442138672, R2 0.5512605309486389\n",
      "Eval loss 1.2685514688491821, R2 0.5499261617660522\n",
      "epoch 616, loss 1.2025251388549805, R2 0.5516495108604431\n",
      "Eval loss 1.2672920227050781, R2 0.5503213405609131\n",
      "epoch 617, loss 1.201321005821228, R2 0.5520371794700623\n",
      "Eval loss 1.2660369873046875, R2 0.5507151484489441\n",
      "epoch 618, loss 1.200121283531189, R2 0.5524235367774963\n",
      "Eval loss 1.2647861242294312, R2 0.5511077642440796\n",
      "epoch 619, loss 1.1989257335662842, R2 0.5528088212013245\n",
      "Eval loss 1.2635397911071777, R2 0.5514991879463196\n",
      "epoch 620, loss 1.1977344751358032, R2 0.553192675113678\n",
      "Eval loss 1.2622976303100586, R2 0.5518893599510193\n",
      "epoch 621, loss 1.196547269821167, R2 0.5535754561424255\n",
      "Eval loss 1.2610596418380737, R2 0.5522782802581787\n",
      "epoch 622, loss 1.1953644752502441, R2 0.553956925868988\n",
      "Eval loss 1.2598261833190918, R2 0.5526659488677979\n",
      "epoch 623, loss 1.1941858530044556, R2 0.5543370842933655\n",
      "Eval loss 1.258596658706665, R2 0.5530524849891663\n",
      "epoch 624, loss 1.1930114030838013, R2 0.5547160506248474\n",
      "Eval loss 1.2573715448379517, R2 0.5534377694129944\n",
      "epoch 625, loss 1.1918411254882812, R2 0.5550937652587891\n",
      "Eval loss 1.256150484085083, R2 0.5538218021392822\n",
      "epoch 626, loss 1.1906750202178955, R2 0.5554704070091248\n",
      "Eval loss 1.2549338340759277, R2 0.5542047023773193\n",
      "epoch 627, loss 1.189513087272644, R2 0.5558456778526306\n",
      "Eval loss 1.2537212371826172, R2 0.5545863509178162\n",
      "epoch 628, loss 1.1883552074432373, R2 0.5562198758125305\n",
      "Eval loss 1.2525126934051514, R2 0.5549668669700623\n",
      "epoch 629, loss 1.1872013807296753, R2 0.5565927624702454\n",
      "Eval loss 1.2513084411621094, R2 0.5553461313247681\n",
      "epoch 630, loss 1.186051607131958, R2 0.5569644570350647\n",
      "Eval loss 1.2501081228256226, R2 0.5557241439819336\n",
      "epoch 631, loss 1.1849061250686646, R2 0.5573349595069885\n",
      "Eval loss 1.2489120960235596, R2 0.5561010837554932\n",
      "epoch 632, loss 1.1837646961212158, R2 0.5577042698860168\n",
      "Eval loss 1.2477200031280518, R2 0.5564768314361572\n",
      "epoch 633, loss 1.1826272010803223, R2 0.5580724477767944\n",
      "Eval loss 1.2465322017669678, R2 0.5568513870239258\n",
      "epoch 634, loss 1.1814936399459839, R2 0.5584393739700317\n",
      "Eval loss 1.245348334312439, R2 0.557224690914154\n",
      "epoch 635, loss 1.1803641319274902, R2 0.5588052272796631\n",
      "Eval loss 1.2441686391830444, R2 0.5575969815254211\n",
      "epoch 636, loss 1.1792386770248413, R2 0.5591697692871094\n",
      "Eval loss 1.2429927587509155, R2 0.557968020439148\n",
      "epoch 637, loss 1.178117275238037, R2 0.5595331788063049\n",
      "Eval loss 1.2418209314346313, R2 0.5583378672599792\n",
      "epoch 638, loss 1.176999807357788, R2 0.5598954558372498\n",
      "Eval loss 1.2406532764434814, R2 0.5587065815925598\n",
      "epoch 639, loss 1.1758861541748047, R2 0.5602565407752991\n",
      "Eval loss 1.2394894361495972, R2 0.5590741634368896\n",
      "epoch 640, loss 1.1747766733169556, R2 0.5606164932250977\n",
      "Eval loss 1.238329529762268, R2 0.5594406127929688\n",
      "epoch 641, loss 1.1736708879470825, R2 0.560975193977356\n",
      "Eval loss 1.2371737957000732, R2 0.5598058700561523\n",
      "epoch 642, loss 1.1725690364837646, R2 0.5613328218460083\n",
      "Eval loss 1.2360217571258545, R2 0.5601701140403748\n",
      "epoch 643, loss 1.171471118927002, R2 0.5616893172264099\n",
      "Eval loss 1.23487389087677, R2 0.5605330467224121\n",
      "epoch 644, loss 1.1703771352767944, R2 0.5620446801185608\n",
      "Eval loss 1.2337298393249512, R2 0.5608949065208435\n",
      "epoch 645, loss 1.169286847114563, R2 0.5623988509178162\n",
      "Eval loss 1.232589602470398, R2 0.561255693435669\n",
      "epoch 646, loss 1.1682006120681763, R2 0.5627519488334656\n",
      "Eval loss 1.2314532995224, R2 0.5616153478622437\n",
      "epoch 647, loss 1.167117953300476, R2 0.5631038546562195\n",
      "Eval loss 1.230320692062378, R2 0.5619738101959229\n",
      "epoch 648, loss 1.1660393476486206, R2 0.5634546875953674\n",
      "Eval loss 1.2291922569274902, R2 0.5623311996459961\n",
      "epoch 649, loss 1.1649644374847412, R2 0.5638043880462646\n",
      "Eval loss 1.228067398071289, R2 0.5626875162124634\n",
      "epoch 650, loss 1.163893222808838, R2 0.5641530156135559\n",
      "Eval loss 1.226946473121643, R2 0.5630427002906799\n",
      "epoch 651, loss 1.1628257036209106, R2 0.5645005106925964\n",
      "Eval loss 1.2258293628692627, R2 0.5633967518806458\n",
      "epoch 652, loss 1.1617622375488281, R2 0.5648468136787415\n",
      "Eval loss 1.2247159481048584, R2 0.5637497305870056\n",
      "epoch 653, loss 1.1607023477554321, R2 0.5651920437812805\n",
      "Eval loss 1.2236064672470093, R2 0.5641016364097595\n",
      "epoch 654, loss 1.1596461534500122, R2 0.5655361413955688\n",
      "Eval loss 1.2225005626678467, R2 0.5644524693489075\n",
      "epoch 655, loss 1.158593773841858, R2 0.565879225730896\n",
      "Eval loss 1.2213985919952393, R2 0.5648021101951599\n",
      "epoch 656, loss 1.1575449705123901, R2 0.5662211775779724\n",
      "Eval loss 1.2203001976013184, R2 0.565150797367096\n",
      "epoch 657, loss 1.1564998626708984, R2 0.5665620565414429\n",
      "Eval loss 1.2192054986953735, R2 0.5654983520507812\n",
      "epoch 658, loss 1.1554584503173828, R2 0.5669018030166626\n",
      "Eval loss 1.2181146144866943, R2 0.5658447742462158\n",
      "epoch 659, loss 1.1544207334518433, R2 0.5672405362129211\n",
      "Eval loss 1.2170274257659912, R2 0.5661901831626892\n",
      "epoch 660, loss 1.1533865928649902, R2 0.567578136920929\n",
      "Eval loss 1.2159438133239746, R2 0.5665344595909119\n",
      "epoch 661, loss 1.1523560285568237, R2 0.5679147243499756\n",
      "Eval loss 1.2148640155792236, R2 0.5668778419494629\n",
      "epoch 662, loss 1.1513290405273438, R2 0.5682501792907715\n",
      "Eval loss 1.2137877941131592, R2 0.5672200322151184\n",
      "epoch 663, loss 1.1503057479858398, R2 0.5685845613479614\n",
      "Eval loss 1.2127151489257812, R2 0.567561149597168\n",
      "epoch 664, loss 1.149286150932312, R2 0.568917989730835\n",
      "Eval loss 1.2116461992263794, R2 0.5679012537002563\n",
      "epoch 665, loss 1.1482700109481812, R2 0.5692501664161682\n",
      "Eval loss 1.2105807065963745, R2 0.5682403445243835\n",
      "epoch 666, loss 1.1472572088241577, R2 0.5695815086364746\n",
      "Eval loss 1.2095190286636353, R2 0.56857830286026\n",
      "epoch 667, loss 1.1462483406066895, R2 0.5699116587638855\n",
      "Eval loss 1.2084609270095825, R2 0.5689152479171753\n",
      "epoch 668, loss 1.145242691040039, R2 0.5702407956123352\n",
      "Eval loss 1.2074062824249268, R2 0.5692511796951294\n",
      "epoch 669, loss 1.1442407369613647, R2 0.5705689191818237\n",
      "Eval loss 1.2063552141189575, R2 0.5695860981941223\n",
      "epoch 670, loss 1.1432422399520874, R2 0.5708960294723511\n",
      "Eval loss 1.2053076028823853, R2 0.569920003414154\n",
      "epoch 671, loss 1.1422470808029175, R2 0.5712219476699829\n",
      "Eval loss 1.2042635679244995, R2 0.5702527761459351\n",
      "epoch 672, loss 1.141255497932434, R2 0.5715469717979431\n",
      "Eval loss 1.2032231092453003, R2 0.5705845355987549\n",
      "epoch 673, loss 1.1402674913406372, R2 0.5718709230422974\n",
      "Eval loss 1.2021859884262085, R2 0.5709154009819031\n",
      "epoch 674, loss 1.1392827033996582, R2 0.5721939206123352\n",
      "Eval loss 1.2011525630950928, R2 0.5712451338768005\n",
      "epoch 675, loss 1.1383014917373657, R2 0.5725157260894775\n",
      "Eval loss 1.200122594833374, R2 0.5715739130973816\n",
      "epoch 676, loss 1.1373236179351807, R2 0.572836697101593\n",
      "Eval loss 1.1990959644317627, R2 0.5719016790390015\n",
      "epoch 677, loss 1.1363492012023926, R2 0.5731565952301025\n",
      "Eval loss 1.198072910308838, R2 0.5722284317016602\n",
      "epoch 678, loss 1.135378122329712, R2 0.5734754204750061\n",
      "Eval loss 1.1970531940460205, R2 0.5725542306900024\n",
      "epoch 679, loss 1.1344103813171387, R2 0.573793351650238\n",
      "Eval loss 1.1960369348526, R2 0.572878897190094\n",
      "epoch 680, loss 1.133446216583252, R2 0.574110209941864\n",
      "Eval loss 1.195024013519287, R2 0.5732026696205139\n",
      "epoch 681, loss 1.132485270500183, R2 0.5744260549545288\n",
      "Eval loss 1.194014549255371, R2 0.5735254287719727\n",
      "epoch 682, loss 1.1315276622772217, R2 0.5747408866882324\n",
      "Eval loss 1.1930084228515625, R2 0.573847234249115\n",
      "epoch 683, loss 1.1305733919143677, R2 0.5750548243522644\n",
      "Eval loss 1.1920058727264404, R2 0.5741679668426514\n",
      "epoch 684, loss 1.129622459411621, R2 0.5753676295280457\n",
      "Eval loss 1.1910063028335571, R2 0.5744878053665161\n",
      "epoch 685, loss 1.1286747455596924, R2 0.5756796002388\n",
      "Eval loss 1.1900103092193604, R2 0.5748066306114197\n",
      "epoch 686, loss 1.127730369567871, R2 0.5759904980659485\n",
      "Eval loss 1.1890177726745605, R2 0.5751245021820068\n",
      "epoch 687, loss 1.1267892122268677, R2 0.5763004422187805\n",
      "Eval loss 1.188028335571289, R2 0.5754413604736328\n",
      "epoch 688, loss 1.1258515119552612, R2 0.5766094326972961\n",
      "Eval loss 1.1870423555374146, R2 0.5757573246955872\n",
      "epoch 689, loss 1.124916911125183, R2 0.5769174695014954\n",
      "Eval loss 1.186059594154358, R2 0.5760722160339355\n",
      "epoch 690, loss 1.1239855289459229, R2 0.5772244930267334\n",
      "Eval loss 1.1850801706314087, R2 0.5763862133026123\n",
      "epoch 691, loss 1.1230573654174805, R2 0.5775305032730103\n",
      "Eval loss 1.1841039657592773, R2 0.5766992568969727\n",
      "epoch 692, loss 1.122132420539856, R2 0.5778356790542603\n",
      "Eval loss 1.1831308603286743, R2 0.5770113468170166\n",
      "epoch 693, loss 1.1212108135223389, R2 0.5781398415565491\n",
      "Eval loss 1.1821612119674683, R2 0.5773224234580994\n",
      "epoch 694, loss 1.1202921867370605, R2 0.5784429907798767\n",
      "Eval loss 1.1811946630477905, R2 0.5776326656341553\n",
      "epoch 695, loss 1.1193767786026, R2 0.5787453055381775\n",
      "Eval loss 1.1802315711975098, R2 0.57794189453125\n",
      "epoch 696, loss 1.118464708328247, R2 0.5790465474128723\n",
      "Eval loss 1.1792714595794678, R2 0.5782501697540283\n",
      "epoch 697, loss 1.1175557374954224, R2 0.5793469548225403\n",
      "Eval loss 1.1783145666122437, R2 0.578557550907135\n",
      "epoch 698, loss 1.1166497468948364, R2 0.5796463489532471\n",
      "Eval loss 1.177361011505127, R2 0.5788639783859253\n",
      "epoch 699, loss 1.1157468557357788, R2 0.5799448490142822\n",
      "Eval loss 1.176410436630249, R2 0.5791694521903992\n",
      "epoch 700, loss 1.1148474216461182, R2 0.580242395401001\n",
      "Eval loss 1.175463080406189, R2 0.5794740319252014\n",
      "epoch 701, loss 1.1139507293701172, R2 0.5805389881134033\n",
      "Eval loss 1.1745189428329468, R2 0.579777717590332\n",
      "epoch 702, loss 1.113057255744934, R2 0.5808347463607788\n",
      "Eval loss 1.173577904701233, R2 0.5800804495811462\n",
      "epoch 703, loss 1.1121670007705688, R2 0.5811294913291931\n",
      "Eval loss 1.1726398468017578, R2 0.580382227897644\n",
      "epoch 704, loss 1.1112794876098633, R2 0.5814234018325806\n",
      "Eval loss 1.1717051267623901, R2 0.5806831121444702\n",
      "epoch 705, loss 1.1103953123092651, R2 0.5817162990570068\n",
      "Eval loss 1.1707735061645508, R2 0.5809831023216248\n",
      "epoch 706, loss 1.1095141172409058, R2 0.5820083022117615\n",
      "Eval loss 1.1698448657989502, R2 0.5812822580337524\n",
      "epoch 707, loss 1.1086360216140747, R2 0.5822994112968445\n",
      "Eval loss 1.168919324874878, R2 0.5815803408622742\n",
      "epoch 708, loss 1.1077606678009033, R2 0.5825895667076111\n",
      "Eval loss 1.167996883392334, R2 0.5818777084350586\n",
      "epoch 709, loss 1.1068886518478394, R2 0.5828788876533508\n",
      "Eval loss 1.1670775413513184, R2 0.5821740031242371\n",
      "epoch 710, loss 1.1060194969177246, R2 0.583167314529419\n",
      "Eval loss 1.166161060333252, R2 0.5824694633483887\n",
      "epoch 711, loss 1.1051533222198486, R2 0.5834548473358154\n",
      "Eval loss 1.1652477979660034, R2 0.5827640295028687\n",
      "epoch 712, loss 1.1042900085449219, R2 0.5837413668632507\n",
      "Eval loss 1.1643375158309937, R2 0.583057701587677\n",
      "epoch 713, loss 1.103429913520813, R2 0.584027111530304\n",
      "Eval loss 1.1634302139282227, R2 0.5833505392074585\n",
      "epoch 714, loss 1.1025726795196533, R2 0.5843119621276855\n",
      "Eval loss 1.16252601146698, R2 0.5836424827575684\n",
      "epoch 715, loss 1.1017181873321533, R2 0.5845959186553955\n",
      "Eval loss 1.1616246700286865, R2 0.5839335322380066\n",
      "epoch 716, loss 1.1008667945861816, R2 0.5848790407180786\n",
      "Eval loss 1.1607263088226318, R2 0.5842235684394836\n",
      "epoch 717, loss 1.1000183820724487, R2 0.5851611495018005\n",
      "Eval loss 1.1598308086395264, R2 0.5845129489898682\n",
      "epoch 718, loss 1.0991727113723755, R2 0.5854424834251404\n",
      "Eval loss 1.1589386463165283, R2 0.5848013162612915\n",
      "epoch 719, loss 1.0983299016952515, R2 0.5857229232788086\n",
      "Eval loss 1.1580491065979004, R2 0.585088849067688\n",
      "epoch 720, loss 1.0974901914596558, R2 0.5860024094581604\n",
      "Eval loss 1.1571625471115112, R2 0.5853756070137024\n",
      "epoch 721, loss 1.0966532230377197, R2 0.5862811207771301\n",
      "Eval loss 1.1562788486480713, R2 0.5856614112854004\n",
      "epoch 722, loss 1.0958192348480225, R2 0.586558997631073\n",
      "Eval loss 1.1553981304168701, R2 0.5859463214874268\n",
      "epoch 723, loss 1.0949878692626953, R2 0.5868359804153442\n",
      "Eval loss 1.1545205116271973, R2 0.586230456829071\n",
      "epoch 724, loss 1.094159483909607, R2 0.5871121287345886\n",
      "Eval loss 1.1536455154418945, R2 0.5865137577056885\n",
      "epoch 725, loss 1.0933340787887573, R2 0.5873873233795166\n",
      "Eval loss 1.1527734994888306, R2 0.5867961645126343\n",
      "epoch 726, loss 1.0925112962722778, R2 0.5876617431640625\n",
      "Eval loss 1.1519043445587158, R2 0.5870776772499084\n",
      "epoch 727, loss 1.0916913747787476, R2 0.5879352688789368\n",
      "Eval loss 1.1510381698608398, R2 0.5873584151268005\n",
      "epoch 728, loss 1.0908743143081665, R2 0.5882080793380737\n",
      "Eval loss 1.150174617767334, R2 0.5876383185386658\n",
      "epoch 729, loss 1.0900599956512451, R2 0.5884799361228943\n",
      "Eval loss 1.1493139266967773, R2 0.5879173874855042\n",
      "epoch 730, loss 1.0892484188079834, R2 0.588750958442688\n",
      "Eval loss 1.1484562158584595, R2 0.5881955027580261\n",
      "epoch 731, loss 1.088439702987671, R2 0.5890211462974548\n",
      "Eval loss 1.1476012468338013, R2 0.5884729027748108\n",
      "epoch 732, loss 1.087633490562439, R2 0.5892905592918396\n",
      "Eval loss 1.1467491388320923, R2 0.5887494087219238\n",
      "epoch 733, loss 1.0868302583694458, R2 0.5895590782165527\n",
      "Eval loss 1.1458996534347534, R2 0.58902508020401\n",
      "epoch 734, loss 1.0860296487808228, R2 0.5898268222808838\n",
      "Eval loss 1.1450531482696533, R2 0.5893000364303589\n",
      "epoch 735, loss 1.085231900215149, R2 0.5900936722755432\n",
      "Eval loss 1.1442092657089233, R2 0.5895740985870361\n",
      "epoch 736, loss 1.0844367742538452, R2 0.5903596878051758\n",
      "Eval loss 1.1433682441711426, R2 0.5898472666740417\n",
      "epoch 737, loss 1.0836443901062012, R2 0.590624988079071\n",
      "Eval loss 1.1425299644470215, R2 0.5901197195053101\n",
      "epoch 738, loss 1.0828546285629272, R2 0.5908893942832947\n",
      "Eval loss 1.14169442653656, R2 0.5903913378715515\n",
      "epoch 739, loss 1.082067608833313, R2 0.5911529660224915\n",
      "Eval loss 1.1408616304397583, R2 0.5906621217727661\n",
      "epoch 740, loss 1.0812832117080688, R2 0.5914158225059509\n",
      "Eval loss 1.1400315761566162, R2 0.5909321904182434\n",
      "epoch 741, loss 1.080501675605774, R2 0.5916778445243835\n",
      "Eval loss 1.1392042636871338, R2 0.5912013649940491\n",
      "epoch 742, loss 1.07972252368927, R2 0.5919389724731445\n",
      "Eval loss 1.138379454612732, R2 0.5914697647094727\n",
      "epoch 743, loss 1.0789461135864258, R2 0.5921993851661682\n",
      "Eval loss 1.1375573873519897, R2 0.5917373299598694\n",
      "epoch 744, loss 1.0781723260879517, R2 0.592458963394165\n",
      "Eval loss 1.1367381811141968, R2 0.592004120349884\n",
      "epoch 745, loss 1.0774012804031372, R2 0.5927177667617798\n",
      "Eval loss 1.1359214782714844, R2 0.5922701358795166\n",
      "epoch 746, loss 1.0766326189041138, R2 0.5929757952690125\n",
      "Eval loss 1.1351075172424316, R2 0.5925353169441223\n",
      "epoch 747, loss 1.07586669921875, R2 0.593233048915863\n",
      "Eval loss 1.1342962980270386, R2 0.5927997827529907\n",
      "epoch 748, loss 1.0751034021377563, R2 0.5934894680976868\n",
      "Eval loss 1.1334877014160156, R2 0.5930634140968323\n",
      "epoch 749, loss 1.0743426084518433, R2 0.5937450528144836\n",
      "Eval loss 1.1326817274093628, R2 0.593326210975647\n",
      "epoch 750, loss 1.0735844373703003, R2 0.5939999222755432\n",
      "Eval loss 1.1318782567977905, R2 0.5935883522033691\n",
      "epoch 751, loss 1.0728288888931274, R2 0.5942539572715759\n",
      "Eval loss 1.131077527999878, R2 0.5938495993614197\n",
      "epoch 752, loss 1.0720757246017456, R2 0.5945072770118713\n",
      "Eval loss 1.130279302597046, R2 0.5941100716590881\n",
      "epoch 753, loss 1.071325421333313, R2 0.5947598218917847\n",
      "Eval loss 1.129483699798584, R2 0.5943699479103088\n",
      "epoch 754, loss 1.0705772638320923, R2 0.5950115323066711\n",
      "Eval loss 1.1286907196044922, R2 0.5946289300918579\n",
      "epoch 755, loss 1.0698319673538208, R2 0.5952625870704651\n",
      "Eval loss 1.127900242805481, R2 0.5948870778083801\n",
      "epoch 756, loss 1.0690890550613403, R2 0.5955127477645874\n",
      "Eval loss 1.1271125078201294, R2 0.595144510269165\n",
      "epoch 757, loss 1.0683485269546509, R2 0.5957622528076172\n",
      "Eval loss 1.1263271570205688, R2 0.5954011678695679\n",
      "epoch 758, loss 1.0676106214523315, R2 0.5960109829902649\n",
      "Eval loss 1.1255444288253784, R2 0.5956571102142334\n",
      "epoch 759, loss 1.0668752193450928, R2 0.5962588787078857\n",
      "Eval loss 1.1247642040252686, R2 0.5959123373031616\n",
      "epoch 760, loss 1.066142201423645, R2 0.5965060591697693\n",
      "Eval loss 1.1239864826202393, R2 0.596166729927063\n",
      "epoch 761, loss 1.065411925315857, R2 0.596752405166626\n",
      "Eval loss 1.1232112646102905, R2 0.5964203476905823\n",
      "epoch 762, loss 1.0646836757659912, R2 0.5969981551170349\n",
      "Eval loss 1.1224387884140015, R2 0.5966733694076538\n",
      "epoch 763, loss 1.0639581680297852, R2 0.597243070602417\n",
      "Eval loss 1.1216684579849243, R2 0.5969254970550537\n",
      "epoch 764, loss 1.0632350444793701, R2 0.597487211227417\n",
      "Eval loss 1.1209009885787964, R2 0.5971769094467163\n",
      "epoch 765, loss 1.0625141859054565, R2 0.5977306962013245\n",
      "Eval loss 1.12013578414917, R2 0.5974276065826416\n",
      "epoch 766, loss 1.061795949935913, R2 0.5979734063148499\n",
      "Eval loss 1.119373083114624, R2 0.5976775288581848\n",
      "epoch 767, loss 1.0610800981521606, R2 0.5982153415679932\n",
      "Eval loss 1.1186128854751587, R2 0.5979267358779907\n",
      "epoch 768, loss 1.0603666305541992, R2 0.5984565615653992\n",
      "Eval loss 1.1178550720214844, R2 0.5981752276420593\n",
      "epoch 769, loss 1.0596556663513184, R2 0.5986970663070679\n",
      "Eval loss 1.1170997619628906, R2 0.5984230041503906\n",
      "epoch 770, loss 1.058946967124939, R2 0.5989368557929993\n",
      "Eval loss 1.116346836090088, R2 0.5986700057983398\n",
      "epoch 771, loss 1.0582407712936401, R2 0.5991758704185486\n",
      "Eval loss 1.1155964136123657, R2 0.5989162921905518\n",
      "epoch 772, loss 1.0575367212295532, R2 0.5994142293930054\n",
      "Eval loss 1.1148483753204346, R2 0.5991618037223816\n",
      "epoch 773, loss 1.0568351745605469, R2 0.5996517539024353\n",
      "Eval loss 1.114102840423584, R2 0.5994067192077637\n",
      "epoch 774, loss 1.056135892868042, R2 0.5998886227607727\n",
      "Eval loss 1.1133595705032349, R2 0.5996508598327637\n",
      "epoch 775, loss 1.0554389953613281, R2 0.6001248359680176\n",
      "Eval loss 1.1126186847686768, R2 0.5998942255973816\n",
      "epoch 776, loss 1.0547446012496948, R2 0.6003602743148804\n",
      "Eval loss 1.1118803024291992, R2 0.600136935710907\n",
      "epoch 777, loss 1.0540523529052734, R2 0.6005949974060059\n",
      "Eval loss 1.1111443042755127, R2 0.6003788709640503\n",
      "epoch 778, loss 1.0533623695373535, R2 0.600829005241394\n",
      "Eval loss 1.1104105710983276, R2 0.6006202101707458\n",
      "epoch 779, loss 1.0526747703552246, R2 0.6010623574256897\n",
      "Eval loss 1.1096793413162231, R2 0.6008607149124146\n",
      "epoch 780, loss 1.0519895553588867, R2 0.6012949347496033\n",
      "Eval loss 1.1089502573013306, R2 0.6011006236076355\n",
      "epoch 781, loss 1.0513064861297607, R2 0.6015268564224243\n",
      "Eval loss 1.1082236766815186, R2 0.6013398170471191\n",
      "epoch 782, loss 1.0506259202957153, R2 0.6017580032348633\n",
      "Eval loss 1.1074994802474976, R2 0.6015781760215759\n",
      "epoch 783, loss 1.0499472618103027, R2 0.6019884943962097\n",
      "Eval loss 1.1067774295806885, R2 0.6018161177635193\n",
      "epoch 784, loss 1.0492712259292603, R2 0.6022183299064636\n",
      "Eval loss 1.1060577630996704, R2 0.6020531058311462\n",
      "epoch 785, loss 1.0485972166061401, R2 0.6024474501609802\n",
      "Eval loss 1.1053403615951538, R2 0.6022894978523254\n",
      "epoch 786, loss 1.0479257106781006, R2 0.6026758551597595\n",
      "Eval loss 1.1046253442764282, R2 0.6025251746177673\n",
      "epoch 787, loss 1.0472562313079834, R2 0.6029036045074463\n",
      "Eval loss 1.103912591934204, R2 0.6027601957321167\n",
      "epoch 788, loss 1.0465890169143677, R2 0.6031306385993958\n",
      "Eval loss 1.103202223777771, R2 0.6029945015907288\n",
      "epoch 789, loss 1.0459240674972534, R2 0.6033570170402527\n",
      "Eval loss 1.1024940013885498, R2 0.6032280325889587\n",
      "epoch 790, loss 1.045261263847351, R2 0.6035826802253723\n",
      "Eval loss 1.1017881631851196, R2 0.603460967540741\n",
      "epoch 791, loss 1.0446007251739502, R2 0.6038076877593994\n",
      "Eval loss 1.1010844707489014, R2 0.6036933064460754\n",
      "epoch 792, loss 1.0439424514770508, R2 0.604032039642334\n",
      "Eval loss 1.1003830432891846, R2 0.6039249300956726\n",
      "epoch 793, loss 1.0432862043380737, R2 0.6042556166648865\n",
      "Eval loss 1.0996838808059692, R2 0.6041558384895325\n",
      "epoch 794, loss 1.0426322221755981, R2 0.6044787168502808\n",
      "Eval loss 1.0989868640899658, R2 0.6043860912322998\n",
      "epoch 795, loss 1.0419803857803345, R2 0.604701042175293\n",
      "Eval loss 1.0982922315597534, R2 0.6046156883239746\n",
      "epoch 796, loss 1.0413308143615723, R2 0.6049226522445679\n",
      "Eval loss 1.097599744796753, R2 0.6048446297645569\n",
      "epoch 797, loss 1.0406832695007324, R2 0.6051435470581055\n",
      "Eval loss 1.096909523010254, R2 0.6050727963447571\n",
      "epoch 798, loss 1.040037989616394, R2 0.6053639650344849\n",
      "Eval loss 1.0962213277816772, R2 0.6053003668785095\n",
      "epoch 799, loss 1.0393948554992676, R2 0.6055836081504822\n",
      "Eval loss 1.0955355167388916, R2 0.6055272817611694\n",
      "epoch 800, loss 1.038753867149353, R2 0.605802595615387\n",
      "Eval loss 1.0948518514633179, R2 0.6057535409927368\n",
      "epoch 801, loss 1.0381147861480713, R2 0.6060209274291992\n",
      "Eval loss 1.0941704511642456, R2 0.6059792041778564\n",
      "epoch 802, loss 1.037477970123291, R2 0.6062386631965637\n",
      "Eval loss 1.0934909582138062, R2 0.6062042117118835\n",
      "epoch 803, loss 1.0368434190750122, R2 0.6064556241035461\n",
      "Eval loss 1.0928137302398682, R2 0.6064283847808838\n",
      "epoch 804, loss 1.0362107753753662, R2 0.6066721081733704\n",
      "Eval loss 1.0921387672424316, R2 0.6066520810127258\n",
      "epoch 805, loss 1.0355802774429321, R2 0.6068877577781677\n",
      "Eval loss 1.0914658308029175, R2 0.6068751215934753\n",
      "epoch 806, loss 1.0349518060684204, R2 0.6071028709411621\n",
      "Eval loss 1.0907952785491943, R2 0.6070975065231323\n",
      "epoch 807, loss 1.0343254804611206, R2 0.6073173880577087\n",
      "Eval loss 1.090126633644104, R2 0.6073191165924072\n",
      "epoch 808, loss 1.0337011814117432, R2 0.6075311303138733\n",
      "Eval loss 1.089460015296936, R2 0.6075401902198792\n",
      "epoch 809, loss 1.0330791473388672, R2 0.6077443361282349\n",
      "Eval loss 1.088795781135559, R2 0.6077607274055481\n",
      "epoch 810, loss 1.0324589014053345, R2 0.6079568862915039\n",
      "Eval loss 1.0881335735321045, R2 0.607980489730835\n",
      "epoch 811, loss 1.0318408012390137, R2 0.6081687808036804\n",
      "Eval loss 1.0874733924865723, R2 0.6081996560096741\n",
      "epoch 812, loss 1.0312248468399048, R2 0.6083800196647644\n",
      "Eval loss 1.086815357208252, R2 0.6084181070327759\n",
      "epoch 813, loss 1.0306106805801392, R2 0.6085907220840454\n",
      "Eval loss 1.086159348487854, R2 0.6086360812187195\n",
      "epoch 814, loss 1.029998779296875, R2 0.6088007092475891\n",
      "Eval loss 1.085505485534668, R2 0.6088533401489258\n",
      "epoch 815, loss 1.0293887853622437, R2 0.6090101003646851\n",
      "Eval loss 1.0848536491394043, R2 0.6090699434280396\n",
      "epoch 816, loss 1.0287809371948242, R2 0.6092188358306885\n",
      "Eval loss 1.084203839302063, R2 0.6092860102653503\n",
      "epoch 817, loss 1.0281749963760376, R2 0.6094269752502441\n",
      "Eval loss 1.0835561752319336, R2 0.6095014214515686\n",
      "epoch 818, loss 1.0275710821151733, R2 0.6096344590187073\n",
      "Eval loss 1.0829106569290161, R2 0.6097161769866943\n",
      "epoch 819, loss 1.0269689559936523, R2 0.6098414659500122\n",
      "Eval loss 1.082266926765442, R2 0.6099303364753723\n",
      "epoch 820, loss 1.0263689756393433, R2 0.6100478172302246\n",
      "Eval loss 1.0816253423690796, R2 0.6101438999176025\n",
      "epoch 821, loss 1.0257710218429565, R2 0.6102535128593445\n",
      "Eval loss 1.0809857845306396, R2 0.610356867313385\n",
      "epoch 822, loss 1.0251750946044922, R2 0.610458493232727\n",
      "Eval loss 1.0803483724594116, R2 0.610569179058075\n",
      "epoch 823, loss 1.024580955505371, R2 0.6106628775596619\n",
      "Eval loss 1.0797127485275269, R2 0.6107808351516724\n",
      "epoch 824, loss 1.0239888429641724, R2 0.6108667850494385\n",
      "Eval loss 1.0790793895721436, R2 0.6109919548034668\n",
      "epoch 825, loss 1.0233986377716064, R2 0.6110700964927673\n",
      "Eval loss 1.0784478187561035, R2 0.6112024784088135\n",
      "epoch 826, loss 1.0228103399276733, R2 0.6112726926803589\n",
      "Eval loss 1.0778182744979858, R2 0.6114123463630676\n",
      "epoch 827, loss 1.0222240686416626, R2 0.6114747524261475\n",
      "Eval loss 1.0771907567977905, R2 0.6116216778755188\n",
      "epoch 828, loss 1.0216397047042847, R2 0.6116762161254883\n",
      "Eval loss 1.0765652656555176, R2 0.6118302941322327\n",
      "epoch 829, loss 1.0210572481155396, R2 0.6118770837783813\n",
      "Eval loss 1.0759416818618774, R2 0.6120384335517883\n",
      "epoch 830, loss 1.0204766988754272, R2 0.6120772957801819\n",
      "Eval loss 1.0753201246261597, R2 0.6122459173202515\n",
      "epoch 831, loss 1.0198979377746582, R2 0.6122769117355347\n",
      "Eval loss 1.0747004747390747, R2 0.6124528646469116\n",
      "epoch 832, loss 1.0193212032318115, R2 0.612476110458374\n",
      "Eval loss 1.0740827322006226, R2 0.6126590967178345\n",
      "epoch 833, loss 1.0187464952468872, R2 0.6126745939254761\n",
      "Eval loss 1.0734670162200928, R2 0.6128648519515991\n",
      "epoch 834, loss 1.018173336982727, R2 0.6128724217414856\n",
      "Eval loss 1.0728530883789062, R2 0.613070011138916\n",
      "epoch 835, loss 1.0176023244857788, R2 0.6130697727203369\n",
      "Eval loss 1.072241187095642, R2 0.6132745146751404\n",
      "epoch 836, loss 1.0170331001281738, R2 0.6132664680480957\n",
      "Eval loss 1.0716311931610107, R2 0.6134784817695618\n",
      "epoch 837, loss 1.016465663909912, R2 0.6134626865386963\n",
      "Eval loss 1.0710232257843018, R2 0.6136818528175354\n",
      "epoch 838, loss 1.0159001350402832, R2 0.6136581897735596\n",
      "Eval loss 1.0704171657562256, R2 0.6138846278190613\n",
      "epoch 839, loss 1.015336513519287, R2 0.6138532757759094\n",
      "Eval loss 1.0698126554489136, R2 0.6140868663787842\n",
      "epoch 840, loss 1.0147746801376343, R2 0.6140477061271667\n",
      "Eval loss 1.069210410118103, R2 0.6142885684967041\n",
      "epoch 841, loss 1.0142147541046143, R2 0.6142414808273315\n",
      "Eval loss 1.0686099529266357, R2 0.6144896149635315\n",
      "epoch 842, loss 1.013656497001648, R2 0.6144347786903381\n",
      "Eval loss 1.0680114030838013, R2 0.6146901249885559\n",
      "epoch 843, loss 1.0131001472473145, R2 0.6146275401115417\n",
      "Eval loss 1.06741464138031, R2 0.6148900985717773\n",
      "epoch 844, loss 1.0125455856323242, R2 0.6148197054862976\n",
      "Eval loss 1.0668197870254517, R2 0.6150893568992615\n",
      "epoch 845, loss 1.0119929313659668, R2 0.6150112748146057\n",
      "Eval loss 1.0662267208099365, R2 0.6152881979942322\n",
      "epoch 846, loss 1.0114420652389526, R2 0.6152023077011108\n",
      "Eval loss 1.0656355619430542, R2 0.6154863834381104\n",
      "epoch 847, loss 1.0108928680419922, R2 0.6153927445411682\n",
      "Eval loss 1.0650463104248047, R2 0.6156841516494751\n",
      "epoch 848, loss 1.0103455781936646, R2 0.6155826449394226\n",
      "Eval loss 1.064458966255188, R2 0.6158811450004578\n",
      "epoch 849, loss 1.0097999572753906, R2 0.6157718896865845\n",
      "Eval loss 1.0638734102249146, R2 0.616077721118927\n",
      "epoch 850, loss 1.00925612449646, R2 0.6159606575965881\n",
      "Eval loss 1.0632895231246948, R2 0.6162737607955933\n",
      "epoch 851, loss 1.008714199066162, R2 0.6161489486694336\n",
      "Eval loss 1.062707543373108, R2 0.616469144821167\n",
      "epoch 852, loss 1.0081738233566284, R2 0.6163365840911865\n",
      "Eval loss 1.0621273517608643, R2 0.6166640520095825\n",
      "epoch 853, loss 1.007635474205017, R2 0.6165237426757812\n",
      "Eval loss 1.0615490674972534, R2 0.6168583035469055\n",
      "epoch 854, loss 1.00709867477417, R2 0.6167103052139282\n",
      "Eval loss 1.0609725713729858, R2 0.6170520782470703\n",
      "epoch 855, loss 1.0065637826919556, R2 0.616896390914917\n",
      "Eval loss 1.060397744178772, R2 0.6172453165054321\n",
      "epoch 856, loss 1.0060304403305054, R2 0.6170818209648132\n",
      "Eval loss 1.059824824333191, R2 0.6174379587173462\n",
      "epoch 857, loss 1.0054988861083984, R2 0.6172667145729065\n",
      "Eval loss 1.0592536926269531, R2 0.617630124092102\n",
      "epoch 858, loss 1.0049691200256348, R2 0.6174511313438416\n",
      "Eval loss 1.0586843490600586, R2 0.6178216934204102\n",
      "epoch 859, loss 1.0044410228729248, R2 0.6176349520683289\n",
      "Eval loss 1.0581166744232178, R2 0.6180127859115601\n",
      "epoch 860, loss 1.003914713859558, R2 0.6178184151649475\n",
      "Eval loss 1.0575506687164307, R2 0.6182032823562622\n",
      "epoch 861, loss 1.0033899545669556, R2 0.6180011630058289\n",
      "Eval loss 1.056986689567566, R2 0.6183931827545166\n",
      "epoch 862, loss 1.0028669834136963, R2 0.618183434009552\n",
      "Eval loss 1.0564243793487549, R2 0.6185826659202576\n",
      "epoch 863, loss 1.0023458003997803, R2 0.6183651685714722\n",
      "Eval loss 1.055863618850708, R2 0.6187715530395508\n",
      "epoch 864, loss 1.0018261671066284, R2 0.6185463070869446\n",
      "Eval loss 1.0553048849105835, R2 0.618959903717041\n",
      "epoch 865, loss 1.0013083219528198, R2 0.6187269687652588\n",
      "Eval loss 1.0547477006912231, R2 0.6191478371620178\n",
      "epoch 866, loss 1.0007920265197754, R2 0.6189071536064148\n",
      "Eval loss 1.0541924238204956, R2 0.6193350553512573\n",
      "epoch 867, loss 1.0002774000167847, R2 0.6190868020057678\n",
      "Eval loss 1.0536385774612427, R2 0.6195218563079834\n",
      "epoch 868, loss 0.9997645616531372, R2 0.6192658543586731\n",
      "Eval loss 1.0530866384506226, R2 0.6197081208229065\n",
      "epoch 869, loss 0.9992533326148987, R2 0.6194444894790649\n",
      "Eval loss 1.0525364875793457, R2 0.6198938488960266\n",
      "epoch 870, loss 0.9987438321113586, R2 0.6196224689483643\n",
      "Eval loss 1.051987886428833, R2 0.620078980922699\n",
      "epoch 871, loss 0.998235821723938, R2 0.6197999715805054\n",
      "Eval loss 1.051440954208374, R2 0.6202636957168579\n",
      "epoch 872, loss 0.997729480266571, R2 0.6199769973754883\n",
      "Eval loss 1.0508958101272583, R2 0.6204478740692139\n",
      "epoch 873, loss 0.997224748134613, R2 0.6201534867286682\n",
      "Eval loss 1.0503524541854858, R2 0.6206315159797668\n",
      "epoch 874, loss 0.9967218637466431, R2 0.6203294992446899\n",
      "Eval loss 1.049810528755188, R2 0.6208146810531616\n",
      "epoch 875, loss 0.9962204098701477, R2 0.6205049753189087\n",
      "Eval loss 1.049270510673523, R2 0.6209972500801086\n",
      "epoch 876, loss 0.9957205653190613, R2 0.6206799149513245\n",
      "Eval loss 1.0487319231033325, R2 0.6211794018745422\n",
      "epoch 877, loss 0.9952223896980286, R2 0.620854377746582\n",
      "Eval loss 1.048195242881775, R2 0.6213609576225281\n",
      "epoch 878, loss 0.9947258234024048, R2 0.6210283637046814\n",
      "Eval loss 1.0476601123809814, R2 0.6215420365333557\n",
      "epoch 879, loss 0.9942307472229004, R2 0.6212018132209778\n",
      "Eval loss 1.0471265316009521, R2 0.6217226386070251\n",
      "epoch 880, loss 0.9937372803688049, R2 0.6213747262954712\n",
      "Eval loss 1.0465946197509766, R2 0.6219027042388916\n",
      "epoch 881, loss 0.993245542049408, R2 0.6215471625328064\n",
      "Eval loss 1.0460644960403442, R2 0.6220822334289551\n",
      "epoch 882, loss 0.9927552938461304, R2 0.6217191219329834\n",
      "Eval loss 1.045535922050476, R2 0.6222612857818604\n",
      "epoch 883, loss 0.9922667145729065, R2 0.621890664100647\n",
      "Eval loss 1.045008897781372, R2 0.6224399209022522\n",
      "epoch 884, loss 0.991779625415802, R2 0.622061550617218\n",
      "Eval loss 1.0444836616516113, R2 0.6226179599761963\n",
      "epoch 885, loss 0.9912940859794617, R2 0.6222319602966309\n",
      "Eval loss 1.0439599752426147, R2 0.6227955222129822\n",
      "epoch 886, loss 0.9908100366592407, R2 0.6224019527435303\n",
      "Eval loss 1.0434378385543823, R2 0.6229726076126099\n",
      "epoch 887, loss 0.9903275966644287, R2 0.6225714683532715\n",
      "Eval loss 1.0429173707962036, R2 0.6231492757797241\n",
      "epoch 888, loss 0.9898468255996704, R2 0.6227404475212097\n",
      "Eval loss 1.042398452758789, R2 0.6233252882957458\n",
      "epoch 889, loss 0.9893674850463867, R2 0.6229089498519897\n",
      "Eval loss 1.0418812036514282, R2 0.6235008239746094\n",
      "epoch 890, loss 0.9888896942138672, R2 0.6230769157409668\n",
      "Eval loss 1.041365385055542, R2 0.6236759424209595\n",
      "epoch 891, loss 0.9884133338928223, R2 0.6232444643974304\n",
      "Eval loss 1.040851354598999, R2 0.6238505840301514\n",
      "epoch 892, loss 0.9879387021064758, R2 0.6234114170074463\n",
      "Eval loss 1.0403387546539307, R2 0.6240246891975403\n",
      "epoch 893, loss 0.987465500831604, R2 0.6235780119895935\n",
      "Eval loss 1.0398277044296265, R2 0.624198317527771\n",
      "epoch 894, loss 0.9869938492774963, R2 0.6237440705299377\n",
      "Eval loss 1.039318323135376, R2 0.6243715286254883\n",
      "epoch 895, loss 0.9865235090255737, R2 0.6239096522331238\n",
      "Eval loss 1.0388103723526, R2 0.6245442032814026\n",
      "epoch 896, loss 0.9860548973083496, R2 0.6240748167037964\n",
      "Eval loss 1.038304090499878, R2 0.6247164011001587\n",
      "epoch 897, loss 0.9855876564979553, R2 0.624239444732666\n",
      "Eval loss 1.0377994775772095, R2 0.6248880624771118\n",
      "epoch 898, loss 0.9851219058036804, R2 0.6244035959243774\n",
      "Eval loss 1.037296175956726, R2 0.6250593662261963\n",
      "epoch 899, loss 0.9846578240394592, R2 0.6245673894882202\n",
      "Eval loss 1.0367945432662964, R2 0.6252301335334778\n",
      "epoch 900, loss 0.9841950535774231, R2 0.6247305870056152\n",
      "Eval loss 1.0362943410873413, R2 0.6254004240036011\n",
      "epoch 901, loss 0.9837337136268616, R2 0.6248933672904968\n",
      "Eval loss 1.03579580783844, R2 0.6255702376365662\n",
      "epoch 902, loss 0.983273983001709, R2 0.6250556111335754\n",
      "Eval loss 1.0352987051010132, R2 0.625739574432373\n",
      "epoch 903, loss 0.9828157424926758, R2 0.6252174377441406\n",
      "Eval loss 1.0348031520843506, R2 0.6259084343910217\n",
      "epoch 904, loss 0.9823588132858276, R2 0.6253787875175476\n",
      "Eval loss 1.0343090295791626, R2 0.626076877117157\n",
      "epoch 905, loss 0.9819034337997437, R2 0.6255397200584412\n",
      "Eval loss 1.0338164567947388, R2 0.626244843006134\n",
      "epoch 906, loss 0.9814494848251343, R2 0.6257001757621765\n",
      "Eval loss 1.033325433731079, R2 0.6264123320579529\n",
      "epoch 907, loss 0.9809969067573547, R2 0.6258601546287537\n",
      "Eval loss 1.032835841178894, R2 0.6265792846679688\n",
      "epoch 908, loss 0.9805458784103394, R2 0.6260196566581726\n",
      "Eval loss 1.0323477983474731, R2 0.626745879650116\n",
      "epoch 909, loss 0.9800962805747986, R2 0.6261786818504333\n",
      "Eval loss 1.0318611860275269, R2 0.626911997795105\n",
      "epoch 910, loss 0.9796480536460876, R2 0.6263372898101807\n",
      "Eval loss 1.0313761234283447, R2 0.6270776391029358\n",
      "epoch 911, loss 0.9792013168334961, R2 0.6264954805374146\n",
      "Eval loss 1.0308924913406372, R2 0.6272428631782532\n",
      "epoch 912, loss 0.9787560105323792, R2 0.6266531944274902\n",
      "Eval loss 1.0304102897644043, R2 0.6274074912071228\n",
      "epoch 913, loss 0.9783119559288025, R2 0.6268104314804077\n",
      "Eval loss 1.029929518699646, R2 0.6275717616081238\n",
      "epoch 914, loss 0.9778695702552795, R2 0.626967191696167\n",
      "Eval loss 1.0294504165649414, R2 0.6277356147766113\n",
      "epoch 915, loss 0.9774283170700073, R2 0.6271235942840576\n",
      "Eval loss 1.0289725065231323, R2 0.6278989911079407\n",
      "epoch 916, loss 0.9769885540008545, R2 0.6272794604301453\n",
      "Eval loss 1.028496265411377, R2 0.6280618906021118\n",
      "epoch 917, loss 0.9765503406524658, R2 0.6274349093437195\n",
      "Eval loss 1.0280213356018066, R2 0.6282243132591248\n",
      "epoch 918, loss 0.9761133790016174, R2 0.627590000629425\n",
      "Eval loss 1.0275479555130005, R2 0.6283863186836243\n",
      "epoch 919, loss 0.9756777882575989, R2 0.6277444958686829\n",
      "Eval loss 1.0270758867263794, R2 0.6285479068756104\n",
      "epoch 920, loss 0.9752435684204102, R2 0.6278985738754272\n",
      "Eval loss 1.0266053676605225, R2 0.6287090182304382\n",
      "epoch 921, loss 0.9748108386993408, R2 0.6280523538589478\n",
      "Eval loss 1.0261362791061401, R2 0.6288697123527527\n",
      "epoch 922, loss 0.9743793606758118, R2 0.6282055974006653\n",
      "Eval loss 1.0256683826446533, R2 0.6290299892425537\n",
      "epoch 923, loss 0.9739493131637573, R2 0.6283584237098694\n",
      "Eval loss 1.0252022743225098, R2 0.6291897892951965\n",
      "epoch 924, loss 0.9735206365585327, R2 0.6285108327865601\n",
      "Eval loss 1.0247372388839722, R2 0.6293491721153259\n",
      "epoch 925, loss 0.9730932116508484, R2 0.6286627054214478\n",
      "Eval loss 1.0242737531661987, R2 0.6295080184936523\n",
      "epoch 926, loss 0.9726673364639282, R2 0.6288142800331116\n",
      "Eval loss 1.0238116979599, R2 0.6296665072441101\n",
      "epoch 927, loss 0.9722426533699036, R2 0.6289653182029724\n",
      "Eval loss 1.0233510732650757, R2 0.6298246383666992\n",
      "epoch 928, loss 0.9718192219734192, R2 0.6291160583496094\n",
      "Eval loss 1.022891640663147, R2 0.6299821734428406\n",
      "epoch 929, loss 0.9713971614837646, R2 0.6292662024497986\n",
      "Eval loss 1.0224336385726929, R2 0.6301394701004028\n",
      "epoch 930, loss 0.9709766507148743, R2 0.6294160485267639\n",
      "Eval loss 1.0219773054122925, R2 0.6302961707115173\n",
      "epoch 931, loss 0.9705573320388794, R2 0.629565417766571\n",
      "Eval loss 1.0215219259262085, R2 0.6304525136947632\n",
      "epoch 932, loss 0.9701393246650696, R2 0.6297143697738647\n",
      "Eval loss 1.0210683345794678, R2 0.6306083798408508\n",
      "epoch 933, loss 0.9697227478027344, R2 0.6298628449440002\n",
      "Eval loss 1.0206159353256226, R2 0.6307638883590698\n",
      "epoch 934, loss 0.9693072438240051, R2 0.6300109624862671\n",
      "Eval loss 1.0201647281646729, R2 0.6309189200401306\n",
      "epoch 935, loss 0.9688932299613953, R2 0.6301587224006653\n",
      "Eval loss 1.0197150707244873, R2 0.631073534488678\n",
      "epoch 936, loss 0.9684803485870361, R2 0.6303060054779053\n",
      "Eval loss 1.0192666053771973, R2 0.6312277317047119\n",
      "epoch 937, loss 0.9680689573287964, R2 0.6304528117179871\n",
      "Eval loss 1.0188196897506714, R2 0.6313815116882324\n",
      "epoch 938, loss 0.9676588177680969, R2 0.630599319934845\n",
      "Eval loss 1.0183740854263306, R2 0.6315348148345947\n",
      "epoch 939, loss 0.9672499895095825, R2 0.6307452321052551\n",
      "Eval loss 1.0179296731948853, R2 0.6316878199577332\n",
      "epoch 940, loss 0.9668422937393188, R2 0.6308909058570862\n",
      "Eval loss 1.0174866914749146, R2 0.6318403482437134\n",
      "epoch 941, loss 0.9664361476898193, R2 0.631036102771759\n",
      "Eval loss 1.017045021057129, R2 0.6319924592971802\n",
      "epoch 942, loss 0.9660310745239258, R2 0.6311808228492737\n",
      "Eval loss 1.0166047811508179, R2 0.6321441531181335\n",
      "epoch 943, loss 0.9656273722648621, R2 0.6313252449035645\n",
      "Eval loss 1.0161657333374023, R2 0.6322954297065735\n",
      "epoch 944, loss 0.9652247428894043, R2 0.631469190120697\n",
      "Eval loss 1.0157279968261719, R2 0.6324462294578552\n",
      "epoch 945, loss 0.9648236036300659, R2 0.6316128373146057\n",
      "Eval loss 1.0152915716171265, R2 0.6325967311859131\n",
      "epoch 946, loss 0.9644235968589783, R2 0.6317559480667114\n",
      "Eval loss 1.0148566961288452, R2 0.6327468156814575\n",
      "epoch 947, loss 0.9640249013900757, R2 0.6318987011909485\n",
      "Eval loss 1.0144230127334595, R2 0.6328964233398438\n",
      "epoch 948, loss 0.9636273384094238, R2 0.6320410370826721\n",
      "Eval loss 1.0139904022216797, R2 0.6330457329750061\n",
      "epoch 949, loss 0.9632312655448914, R2 0.6321830153465271\n",
      "Eval loss 1.0135592222213745, R2 0.6331945061683655\n",
      "epoch 950, loss 0.9628362059593201, R2 0.6323245763778687\n",
      "Eval loss 1.0131292343139648, R2 0.6333429217338562\n",
      "epoch 951, loss 0.9624423980712891, R2 0.632465660572052\n",
      "Eval loss 1.0127005577087402, R2 0.6334909200668335\n",
      "epoch 952, loss 0.9620499610900879, R2 0.6326064467430115\n",
      "Eval loss 1.0122733116149902, R2 0.6336386203765869\n",
      "epoch 953, loss 0.9616585969924927, R2 0.6327467560768127\n",
      "Eval loss 1.0118472576141357, R2 0.6337857842445374\n",
      "epoch 954, loss 0.9612685441970825, R2 0.6328868269920349\n",
      "Eval loss 1.0114226341247559, R2 0.6339326500892639\n",
      "epoch 955, loss 0.9608797430992126, R2 0.6330263614654541\n",
      "Eval loss 1.010999083518982, R2 0.634079098701477\n",
      "epoch 956, loss 0.9604920148849487, R2 0.6331655383110046\n",
      "Eval loss 1.0105767250061035, R2 0.634225070476532\n",
      "epoch 957, loss 0.9601057171821594, R2 0.6333042979240417\n",
      "Eval loss 1.0101559162139893, R2 0.634370744228363\n",
      "epoch 958, loss 0.9597203731536865, R2 0.6334426999092102\n",
      "Eval loss 1.0097360610961914, R2 0.6345158815383911\n",
      "epoch 959, loss 0.9593364596366882, R2 0.63358074426651\n",
      "Eval loss 1.0093176364898682, R2 0.6346607804298401\n",
      "epoch 960, loss 0.9589536786079407, R2 0.6337183117866516\n",
      "Eval loss 1.0089002847671509, R2 0.6348052024841309\n",
      "epoch 961, loss 0.9585720300674438, R2 0.6338555812835693\n",
      "Eval loss 1.0084843635559082, R2 0.6349493265151978\n",
      "epoch 962, loss 0.9581915736198425, R2 0.6339924335479736\n",
      "Eval loss 1.008069634437561, R2 0.6350929737091064\n",
      "epoch 963, loss 0.9578123092651367, R2 0.6341288685798645\n",
      "Eval loss 1.007656216621399, R2 0.6352362036705017\n",
      "epoch 964, loss 0.9574342370033264, R2 0.6342649459838867\n",
      "Eval loss 1.0072438716888428, R2 0.6353791356086731\n",
      "epoch 965, loss 0.9570574164390564, R2 0.6344007253646851\n",
      "Eval loss 1.0068327188491821, R2 0.6355217099189758\n",
      "epoch 966, loss 0.9566816091537476, R2 0.6345359683036804\n",
      "Eval loss 1.006422996520996, R2 0.6356638073921204\n",
      "epoch 967, loss 0.9563071131706238, R2 0.6346709132194519\n",
      "Eval loss 1.006014347076416, R2 0.635805606842041\n",
      "epoch 968, loss 0.955933690071106, R2 0.6348055005073547\n",
      "Eval loss 1.0056068897247314, R2 0.6359469294548035\n",
      "epoch 969, loss 0.9555615782737732, R2 0.6349396109580994\n",
      "Eval loss 1.005200743675232, R2 0.6360878944396973\n",
      "epoch 970, loss 0.9551904201507568, R2 0.6350734829902649\n",
      "Eval loss 1.004795789718628, R2 0.6362285017967224\n",
      "epoch 971, loss 0.9548205137252808, R2 0.6352069973945618\n",
      "Eval loss 1.0043919086456299, R2 0.6363687515258789\n",
      "epoch 972, loss 0.9544517397880554, R2 0.6353400349617004\n",
      "Eval loss 1.003989338874817, R2 0.6365085244178772\n",
      "epoch 973, loss 0.9540840983390808, R2 0.6354727149009705\n",
      "Eval loss 1.0035879611968994, R2 0.6366480588912964\n",
      "epoch 974, loss 0.9537177681922913, R2 0.635604977607727\n",
      "Eval loss 1.003187656402588, R2 0.6367871165275574\n",
      "epoch 975, loss 0.9533523917198181, R2 0.6357370018959045\n",
      "Eval loss 1.0027886629104614, R2 0.6369258165359497\n",
      "epoch 976, loss 0.9529882073402405, R2 0.6358685493469238\n",
      "Eval loss 1.0023908615112305, R2 0.6370642185211182\n",
      "epoch 977, loss 0.9526251554489136, R2 0.6359997987747192\n",
      "Eval loss 1.0019941329956055, R2 0.6372022032737732\n",
      "epoch 978, loss 0.9522630572319031, R2 0.636130690574646\n",
      "Eval loss 1.0015987157821655, R2 0.6373398303985596\n",
      "epoch 979, loss 0.9519022703170776, R2 0.6362611055374146\n",
      "Eval loss 1.0012043714523315, R2 0.6374770402908325\n",
      "epoch 980, loss 0.9515424966812134, R2 0.636391282081604\n",
      "Eval loss 1.0008111000061035, R2 0.6376139521598816\n",
      "epoch 981, loss 0.9511839747428894, R2 0.63652104139328\n",
      "Eval loss 1.0004191398620605, R2 0.6377503871917725\n",
      "epoch 982, loss 0.9508264660835266, R2 0.6366504430770874\n",
      "Eval loss 1.000028371810913, R2 0.6378865242004395\n",
      "epoch 983, loss 0.9504700899124146, R2 0.6367795467376709\n",
      "Eval loss 0.9996386170387268, R2 0.6380223035812378\n",
      "epoch 984, loss 0.9501148462295532, R2 0.6369081735610962\n",
      "Eval loss 0.9992501735687256, R2 0.6381577253341675\n",
      "epoch 985, loss 0.9497605562210083, R2 0.6370365023612976\n",
      "Eval loss 0.998862624168396, R2 0.6382927298545837\n",
      "epoch 986, loss 0.9494075179100037, R2 0.6371644735336304\n",
      "Eval loss 0.9984763860702515, R2 0.6384274959564209\n",
      "epoch 987, loss 0.9490554928779602, R2 0.6372921466827393\n",
      "Eval loss 0.9980912804603577, R2 0.6385618448257446\n",
      "epoch 988, loss 0.9487046003341675, R2 0.6374193429946899\n",
      "Eval loss 0.9977073073387146, R2 0.6386957168579102\n",
      "epoch 989, loss 0.9483547210693359, R2 0.6375463008880615\n",
      "Eval loss 0.9973244667053223, R2 0.6388294100761414\n",
      "epoch 990, loss 0.9480059742927551, R2 0.6376728415489197\n",
      "Eval loss 0.9969426989555359, R2 0.6389626264572144\n",
      "epoch 991, loss 0.9476582407951355, R2 0.6377990245819092\n",
      "Eval loss 0.996562123298645, R2 0.6390955448150635\n",
      "epoch 992, loss 0.9473115801811218, R2 0.63792484998703\n",
      "Eval loss 0.9961826205253601, R2 0.6392280459403992\n",
      "epoch 993, loss 0.9469662308692932, R2 0.6380504369735718\n",
      "Eval loss 0.9958041310310364, R2 0.639360249042511\n",
      "epoch 994, loss 0.9466216564178467, R2 0.6381755471229553\n",
      "Eval loss 0.9954269528388977, R2 0.6394920945167542\n",
      "epoch 995, loss 0.9462781548500061, R2 0.638300359249115\n",
      "Eval loss 0.9950507879257202, R2 0.6396236419677734\n",
      "epoch 996, loss 0.945935845375061, R2 0.6384248733520508\n",
      "Eval loss 0.9946756958961487, R2 0.6397547125816345\n",
      "epoch 997, loss 0.9455944895744324, R2 0.6385489702224731\n",
      "Eval loss 0.9943017959594727, R2 0.6398854851722717\n",
      "epoch 998, loss 0.945254385471344, R2 0.6386726498603821\n",
      "Eval loss 0.9939289689064026, R2 0.6400159597396851\n",
      "epoch 999, loss 0.9449151754379272, R2 0.6387961506843567\n",
      "Eval loss 0.9935570359230042, R2 0.6401460766792297\n",
      "epoch 1000, loss 0.9445768594741821, R2 0.6389192342758179\n",
      "Eval loss 0.9931864142417908, R2 0.640275776386261\n",
      "epoch 1001, loss 0.9442398548126221, R2 0.6390419602394104\n",
      "Eval loss 0.9928167462348938, R2 0.6404052376747131\n",
      "epoch 1002, loss 0.9439036846160889, R2 0.6391644477844238\n",
      "Eval loss 0.9924483895301819, R2 0.6405342817306519\n",
      "epoch 1003, loss 0.9435685873031616, R2 0.6392865180969238\n",
      "Eval loss 0.9920808672904968, R2 0.6406629681587219\n",
      "epoch 1004, loss 0.9432345628738403, R2 0.6394082307815552\n",
      "Eval loss 0.991714596748352, R2 0.6407913565635681\n",
      "epoch 1005, loss 0.9429015517234802, R2 0.6395297050476074\n",
      "Eval loss 0.9913492798805237, R2 0.6409194469451904\n",
      "epoch 1006, loss 0.9425694942474365, R2 0.6396507620811462\n",
      "Eval loss 0.9909849166870117, R2 0.6410470604896545\n",
      "epoch 1007, loss 0.9422385692596436, R2 0.6397714614868164\n",
      "Eval loss 0.99062180519104, R2 0.6411744952201843\n",
      "epoch 1008, loss 0.941908597946167, R2 0.6398918628692627\n",
      "Eval loss 0.9902597069740295, R2 0.6413015127182007\n",
      "epoch 1009, loss 0.9415796399116516, R2 0.6400119662284851\n",
      "Eval loss 0.9898988008499146, R2 0.6414281725883484\n",
      "epoch 1010, loss 0.9412516355514526, R2 0.6401317715644836\n",
      "Eval loss 0.9895386695861816, R2 0.6415545344352722\n",
      "epoch 1011, loss 0.9409247040748596, R2 0.6402512192726135\n",
      "Eval loss 0.9891796708106995, R2 0.6416805386543274\n",
      "epoch 1012, loss 0.940598726272583, R2 0.6403701901435852\n",
      "Eval loss 0.9888218641281128, R2 0.6418062448501587\n",
      "epoch 1013, loss 0.9402738213539124, R2 0.6404889822006226\n",
      "Eval loss 0.9884650111198425, R2 0.6419315338134766\n",
      "epoch 1014, loss 0.9399498701095581, R2 0.6406074166297913\n",
      "Eval loss 0.9881091713905334, R2 0.6420566439628601\n",
      "epoch 1015, loss 0.9396268129348755, R2 0.6407254934310913\n",
      "Eval loss 0.9877544641494751, R2 0.6421813368797302\n",
      "epoch 1016, loss 0.9393048882484436, R2 0.6408433318138123\n",
      "Eval loss 0.9874005913734436, R2 0.6423056721687317\n",
      "epoch 1017, loss 0.9389838576316833, R2 0.6409607529640198\n",
      "Eval loss 0.9870479702949524, R2 0.6424296498298645\n",
      "epoch 1018, loss 0.938663899898529, R2 0.6410778760910034\n",
      "Eval loss 0.9866962432861328, R2 0.6425533890724182\n",
      "epoch 1019, loss 0.9383448362350464, R2 0.6411947011947632\n",
      "Eval loss 0.9863455891609192, R2 0.6426767110824585\n",
      "epoch 1020, loss 0.9380267262458801, R2 0.6413112282752991\n",
      "Eval loss 0.9859958291053772, R2 0.6427997946739197\n",
      "epoch 1021, loss 0.9377096891403198, R2 0.6414273977279663\n",
      "Eval loss 0.9856471419334412, R2 0.6429225206375122\n",
      "epoch 1022, loss 0.9373935461044312, R2 0.6415433287620544\n",
      "Eval loss 0.9852995872497559, R2 0.6430450081825256\n",
      "epoch 1023, loss 0.9370782375335693, R2 0.6416588425636292\n",
      "Eval loss 0.9849529266357422, R2 0.6431670784950256\n",
      "epoch 1024, loss 0.9367640018463135, R2 0.64177405834198\n",
      "Eval loss 0.9846073985099792, R2 0.643288791179657\n",
      "epoch 1025, loss 0.9364507794380188, R2 0.6418889760971069\n",
      "Eval loss 0.9842627048492432, R2 0.6434102058410645\n",
      "epoch 1026, loss 0.9361385107040405, R2 0.64200359582901\n",
      "Eval loss 0.9839191436767578, R2 0.6435313820838928\n",
      "epoch 1027, loss 0.9358271956443787, R2 0.6421178579330444\n",
      "Eval loss 0.9835764765739441, R2 0.6436522006988525\n",
      "epoch 1028, loss 0.9355167150497437, R2 0.6422318816184998\n",
      "Eval loss 0.9832348227500916, R2 0.6437726020812988\n",
      "epoch 1029, loss 0.9352073073387146, R2 0.6423454880714417\n",
      "Eval loss 0.982894241809845, R2 0.643892765045166\n",
      "epoch 1030, loss 0.9348987340927124, R2 0.6424589157104492\n",
      "Eval loss 0.9825544953346252, R2 0.6440126895904541\n",
      "epoch 1031, loss 0.9345909953117371, R2 0.6425718665122986\n",
      "Eval loss 0.9822158217430115, R2 0.6441321969032288\n",
      "epoch 1032, loss 0.934284508228302, R2 0.6426845788955688\n",
      "Eval loss 0.9818781614303589, R2 0.6442514657974243\n",
      "epoch 1033, loss 0.9339787364006042, R2 0.64279705286026\n",
      "Eval loss 0.9815413951873779, R2 0.6443703174591064\n",
      "epoch 1034, loss 0.9336738586425781, R2 0.6429091095924377\n",
      "Eval loss 0.9812056422233582, R2 0.6444889903068542\n",
      "epoch 1035, loss 0.9333699941635132, R2 0.6430209279060364\n",
      "Eval loss 0.9808708429336548, R2 0.6446073055267334\n",
      "epoch 1036, loss 0.9330670237541199, R2 0.6431325078010559\n",
      "Eval loss 0.9805370569229126, R2 0.6447252631187439\n",
      "epoch 1037, loss 0.932765007019043, R2 0.6432437300682068\n",
      "Eval loss 0.9802042245864868, R2 0.6448429226875305\n",
      "epoch 1038, loss 0.9324638247489929, R2 0.6433545351028442\n",
      "Eval loss 0.9798722863197327, R2 0.644960343837738\n",
      "epoch 1039, loss 0.9321637153625488, R2 0.6434651613235474\n",
      "Eval loss 0.9795412421226501, R2 0.6450773477554321\n",
      "epoch 1040, loss 0.9318642616271973, R2 0.6435754299163818\n",
      "Eval loss 0.9792113304138184, R2 0.6451941132545471\n",
      "epoch 1041, loss 0.9315659403800964, R2 0.643685519695282\n",
      "Eval loss 0.9788822531700134, R2 0.6453105807304382\n",
      "epoch 1042, loss 0.9312684535980225, R2 0.6437951922416687\n",
      "Eval loss 0.9785541296005249, R2 0.6454267501831055\n",
      "epoch 1043, loss 0.9309718608856201, R2 0.6439046263694763\n",
      "Eval loss 0.9782270193099976, R2 0.645542562007904\n",
      "epoch 1044, loss 0.9306761622428894, R2 0.6440137624740601\n",
      "Eval loss 0.9779008030891418, R2 0.6456580758094788\n",
      "epoch 1045, loss 0.9303812384605408, R2 0.6441225409507751\n",
      "Eval loss 0.9775754809379578, R2 0.6457733511924744\n",
      "epoch 1046, loss 0.9300873875617981, R2 0.6442310214042664\n",
      "Eval loss 0.9772511720657349, R2 0.6458882689476013\n",
      "epoch 1047, loss 0.9297943115234375, R2 0.6443392634391785\n",
      "Eval loss 0.9769278764724731, R2 0.6460029482841492\n",
      "epoch 1048, loss 0.9295021891593933, R2 0.6444472670555115\n",
      "Eval loss 0.9766053557395935, R2 0.6461173295974731\n",
      "epoch 1049, loss 0.9292109608650208, R2 0.6445547342300415\n",
      "Eval loss 0.9762837290763855, R2 0.6462313532829285\n",
      "epoch 1050, loss 0.9289204478263855, R2 0.6446621417999268\n",
      "Eval loss 0.9759631156921387, R2 0.6463450789451599\n",
      "epoch 1051, loss 0.9286310076713562, R2 0.6447691917419434\n",
      "Eval loss 0.9756434559822083, R2 0.6464585065841675\n",
      "epoch 1052, loss 0.9283422827720642, R2 0.6448760032653809\n",
      "Eval loss 0.9753245115280151, R2 0.646571695804596\n",
      "epoch 1053, loss 0.9280545115470886, R2 0.6449824571609497\n",
      "Eval loss 0.9750065207481384, R2 0.6466845870018005\n",
      "epoch 1054, loss 0.9277676343917847, R2 0.6450886130332947\n",
      "Eval loss 0.9746896624565125, R2 0.6467971205711365\n",
      "epoch 1055, loss 0.927481472492218, R2 0.6451945304870605\n",
      "Eval loss 0.9743736386299133, R2 0.6469094157218933\n",
      "epoch 1056, loss 0.9271963834762573, R2 0.6453001499176025\n",
      "Eval loss 0.9740585088729858, R2 0.647021472454071\n",
      "epoch 1057, loss 0.9269120693206787, R2 0.6454054713249207\n",
      "Eval loss 0.9737442135810852, R2 0.6471331119537354\n",
      "epoch 1058, loss 0.9266285300254822, R2 0.6455104947090149\n",
      "Eval loss 0.9734307527542114, R2 0.6472445130348206\n",
      "epoch 1059, loss 0.926345944404602, R2 0.6456152200698853\n",
      "Eval loss 0.9731183648109436, R2 0.6473556160926819\n",
      "epoch 1060, loss 0.926064133644104, R2 0.6457197070121765\n",
      "Eval loss 0.9728066921234131, R2 0.6474664807319641\n",
      "epoch 1061, loss 0.9257832765579224, R2 0.6458239555358887\n",
      "Eval loss 0.972495973110199, R2 0.6475770473480225\n",
      "epoch 1062, loss 0.9255030751228333, R2 0.6459278464317322\n",
      "Eval loss 0.9721862077713013, R2 0.6476872563362122\n",
      "epoch 1063, loss 0.9252238273620605, R2 0.6460314989089966\n",
      "Eval loss 0.9718772768974304, R2 0.6477972269058228\n",
      "epoch 1064, loss 0.9249454736709595, R2 0.6461348533630371\n",
      "Eval loss 0.9715692400932312, R2 0.6479068994522095\n",
      "epoch 1065, loss 0.9246678948402405, R2 0.6462379097938538\n",
      "Eval loss 0.9712619781494141, R2 0.6480163335800171\n",
      "epoch 1066, loss 0.9243910908699036, R2 0.6463406682014465\n",
      "Eval loss 0.9709557890892029, R2 0.6481254696846008\n",
      "epoch 1067, loss 0.9241151809692383, R2 0.6464431881904602\n",
      "Eval loss 0.9706502556800842, R2 0.6482343673706055\n",
      "epoch 1068, loss 0.9238401055335999, R2 0.6465454697608948\n",
      "Eval loss 0.9703457355499268, R2 0.6483428478240967\n",
      "epoch 1069, loss 0.9235658645629883, R2 0.6466473937034607\n",
      "Eval loss 0.9700419902801514, R2 0.6484511494636536\n",
      "epoch 1070, loss 0.923292338848114, R2 0.6467490792274475\n",
      "Eval loss 0.9697392582893372, R2 0.6485591530799866\n",
      "epoch 1071, loss 0.9230198264122009, R2 0.6468505859375\n",
      "Eval loss 0.969437301158905, R2 0.6486669182777405\n",
      "epoch 1072, loss 0.922747790813446, R2 0.6469516754150391\n",
      "Eval loss 0.9691361784934998, R2 0.6487743258476257\n",
      "epoch 1073, loss 0.9224769473075867, R2 0.6470525860786438\n",
      "Eval loss 0.9688359498977661, R2 0.6488814949989319\n",
      "epoch 1074, loss 0.9222066402435303, R2 0.6471531987190247\n",
      "Eval loss 0.9685364365577698, R2 0.6489883661270142\n",
      "epoch 1075, loss 0.9219373464584351, R2 0.6472535133361816\n",
      "Eval loss 0.9682379364967346, R2 0.6490949988365173\n",
      "epoch 1076, loss 0.9216685891151428, R2 0.6473536491394043\n",
      "Eval loss 0.9679402112960815, R2 0.6492013335227966\n",
      "epoch 1077, loss 0.9214009642601013, R2 0.6474533677101135\n",
      "Eval loss 0.9676433205604553, R2 0.649307370185852\n",
      "epoch 1078, loss 0.9211338758468628, R2 0.6475529074668884\n",
      "Eval loss 0.967347264289856, R2 0.6494132280349731\n",
      "epoch 1079, loss 0.9208677411079407, R2 0.6476522088050842\n",
      "Eval loss 0.967052161693573, R2 0.6495187282562256\n",
      "epoch 1080, loss 0.9206023812294006, R2 0.6477512121200562\n",
      "Eval loss 0.9667577147483826, R2 0.6496240496635437\n",
      "epoch 1081, loss 0.9203377366065979, R2 0.647849977016449\n",
      "Eval loss 0.9664642214775085, R2 0.6497290134429932\n",
      "epoch 1082, loss 0.9200738668441772, R2 0.6479483842849731\n",
      "Eval loss 0.9661715030670166, R2 0.6498336791992188\n",
      "epoch 1083, loss 0.9198108911514282, R2 0.6480465531349182\n",
      "Eval loss 0.9658797383308411, R2 0.64993816614151\n",
      "epoch 1084, loss 0.9195486307144165, R2 0.648144543170929\n",
      "Eval loss 0.9655886888504028, R2 0.6500422954559326\n",
      "epoch 1085, loss 0.9192870855331421, R2 0.6482422351837158\n",
      "Eval loss 0.9652985334396362, R2 0.6501462459564209\n",
      "epoch 1086, loss 0.9190263152122498, R2 0.6483396291732788\n",
      "Eval loss 0.9650089740753174, R2 0.6502498388290405\n",
      "epoch 1087, loss 0.918766438961029, R2 0.6484367847442627\n",
      "Eval loss 0.9647204875946045, R2 0.6503532528877258\n",
      "epoch 1088, loss 0.9185072779655457, R2 0.6485337018966675\n",
      "Eval loss 0.9644326567649841, R2 0.6504563689231873\n",
      "epoch 1089, loss 0.9182490110397339, R2 0.6486303210258484\n",
      "Eval loss 0.9641457200050354, R2 0.6505591869354248\n",
      "epoch 1090, loss 0.9179912805557251, R2 0.648726761341095\n",
      "Eval loss 0.9638595581054688, R2 0.6506617665290833\n",
      "epoch 1091, loss 0.9177343845367432, R2 0.6488228440284729\n",
      "Eval loss 0.9635742902755737, R2 0.6507641077041626\n",
      "epoch 1092, loss 0.9174783825874329, R2 0.6489187479019165\n",
      "Eval loss 0.9632896780967712, R2 0.6508661508560181\n",
      "epoch 1093, loss 0.9172230362892151, R2 0.6490143537521362\n",
      "Eval loss 0.9630059003829956, R2 0.6509680151939392\n",
      "epoch 1094, loss 0.9169684052467346, R2 0.6491096615791321\n",
      "Eval loss 0.9627229571342468, R2 0.6510695815086365\n",
      "epoch 1095, loss 0.9167146682739258, R2 0.6492047905921936\n",
      "Eval loss 0.9624407887458801, R2 0.6511707901954651\n",
      "epoch 1096, loss 0.9164615869522095, R2 0.6492996215820312\n",
      "Eval loss 0.9621595144271851, R2 0.6512718796730042\n",
      "epoch 1097, loss 0.9162092804908752, R2 0.6493942141532898\n",
      "Eval loss 0.9618788957595825, R2 0.6513726711273193\n",
      "epoch 1098, loss 0.9159576892852783, R2 0.6494886875152588\n",
      "Eval loss 0.9615991115570068, R2 0.6514731645584106\n",
      "epoch 1099, loss 0.9157067537307739, R2 0.6495826840400696\n",
      "Eval loss 0.9613201022148132, R2 0.6515734195709229\n",
      "epoch 1100, loss 0.9154567718505859, R2 0.6496766209602356\n",
      "Eval loss 0.9610419869422913, R2 0.6516733765602112\n",
      "epoch 1101, loss 0.9152075052261353, R2 0.6497701406478882\n",
      "Eval loss 0.9607645273208618, R2 0.6517730951309204\n",
      "epoch 1102, loss 0.9149587750434875, R2 0.6498635411262512\n",
      "Eval loss 0.9604879021644592, R2 0.6518725752830505\n",
      "epoch 1103, loss 0.9147108197212219, R2 0.6499566435813904\n",
      "Eval loss 0.9602120518684387, R2 0.6519718766212463\n",
      "epoch 1104, loss 0.9144637584686279, R2 0.6500494480133057\n",
      "Eval loss 0.9599369764328003, R2 0.6520708799362183\n",
      "epoch 1105, loss 0.9142172932624817, R2 0.6501421332359314\n",
      "Eval loss 0.9596626162528992, R2 0.6521695852279663\n",
      "epoch 1106, loss 0.9139716625213623, R2 0.6502344608306885\n",
      "Eval loss 0.9593889713287354, R2 0.6522680521011353\n",
      "epoch 1107, loss 0.9137266278266907, R2 0.6503266096115112\n",
      "Eval loss 0.9591161608695984, R2 0.6523663401603699\n",
      "epoch 1108, loss 0.9134824872016907, R2 0.6504184603691101\n",
      "Eval loss 0.9588441848754883, R2 0.6524643301963806\n",
      "epoch 1109, loss 0.9132389426231384, R2 0.6505100131034851\n",
      "Eval loss 0.9585729241371155, R2 0.6525620222091675\n",
      "epoch 1110, loss 0.9129961729049683, R2 0.6506015062332153\n",
      "Eval loss 0.9583024382591248, R2 0.65265953540802\n",
      "epoch 1111, loss 0.9127539992332458, R2 0.6506925821304321\n",
      "Eval loss 0.9580326080322266, R2 0.6527567505836487\n",
      "epoch 1112, loss 0.9125127196311951, R2 0.6507834792137146\n",
      "Eval loss 0.9577636122703552, R2 0.652853786945343\n",
      "epoch 1113, loss 0.9122719764709473, R2 0.6508741974830627\n",
      "Eval loss 0.957495391368866, R2 0.6529505252838135\n",
      "epoch 1114, loss 0.9120319485664368, R2 0.650964617729187\n",
      "Eval loss 0.9572278261184692, R2 0.6530470252037048\n",
      "epoch 1115, loss 0.9117926955223083, R2 0.6510547995567322\n",
      "Eval loss 0.9569610357284546, R2 0.6531432867050171\n",
      "epoch 1116, loss 0.911554217338562, R2 0.6511447429656982\n",
      "Eval loss 0.9566949605941772, R2 0.6532393097877502\n",
      "epoch 1117, loss 0.9113163352012634, R2 0.6512344479560852\n",
      "Eval loss 0.956429660320282, R2 0.6533351540565491\n",
      "epoch 1118, loss 0.9110792279243469, R2 0.6513239145278931\n",
      "Eval loss 0.9561651349067688, R2 0.6534306406974792\n",
      "epoch 1119, loss 0.9108426570892334, R2 0.6514131426811218\n",
      "Eval loss 0.9559013247489929, R2 0.6535259485244751\n",
      "epoch 1120, loss 0.9106070399284363, R2 0.6515021324157715\n",
      "Eval loss 0.9556383490562439, R2 0.6536210179328918\n",
      "epoch 1121, loss 0.9103718996047974, R2 0.6515909433364868\n",
      "Eval loss 0.9553760290145874, R2 0.6537157893180847\n",
      "epoch 1122, loss 0.9101375341415405, R2 0.651679515838623\n",
      "Eval loss 0.9551143646240234, R2 0.6538103818893433\n",
      "epoch 1123, loss 0.9099038243293762, R2 0.6517677903175354\n",
      "Eval loss 0.9548534750938416, R2 0.6539047360420227\n",
      "epoch 1124, loss 0.9096707701683044, R2 0.6518558263778687\n",
      "Eval loss 0.954593300819397, R2 0.6539987921714783\n",
      "epoch 1125, loss 0.9094384908676147, R2 0.6519437432289124\n",
      "Eval loss 0.9543337821960449, R2 0.6540926694869995\n",
      "epoch 1126, loss 0.9092068672180176, R2 0.6520312428474426\n",
      "Eval loss 0.9540752172470093, R2 0.6541862487792969\n",
      "epoch 1127, loss 0.9089758396148682, R2 0.6521186232566833\n",
      "Eval loss 0.9538172483444214, R2 0.6542796492576599\n",
      "epoch 1128, loss 0.908745527267456, R2 0.652205765247345\n",
      "Eval loss 0.953559935092926, R2 0.6543728113174438\n",
      "epoch 1129, loss 0.908515989780426, R2 0.6522926688194275\n",
      "Eval loss 0.9533032178878784, R2 0.6544656753540039\n",
      "epoch 1130, loss 0.9082870483398438, R2 0.6523793935775757\n",
      "Eval loss 0.9530473947525024, R2 0.6545584201812744\n",
      "epoch 1131, loss 0.9080587029457092, R2 0.6524658799171448\n",
      "Eval loss 0.9527923464775085, R2 0.6546508073806763\n",
      "epoch 1132, loss 0.9078310132026672, R2 0.6525521278381348\n",
      "Eval loss 0.9525377750396729, R2 0.6547431349754333\n",
      "epoch 1133, loss 0.9076040983200073, R2 0.6526380777359009\n",
      "Eval loss 0.9522841572761536, R2 0.654835045337677\n",
      "epoch 1134, loss 0.9073778986930847, R2 0.6527238488197327\n",
      "Eval loss 0.9520310759544373, R2 0.6549268364906311\n",
      "epoch 1135, loss 0.9071522355079651, R2 0.6528093814849854\n",
      "Eval loss 0.9517785906791687, R2 0.6550183892250061\n",
      "epoch 1136, loss 0.9069273471832275, R2 0.6528947949409485\n",
      "Eval loss 0.9515270590782166, R2 0.6551096439361572\n",
      "epoch 1137, loss 0.9067029356956482, R2 0.652979850769043\n",
      "Eval loss 0.9512760639190674, R2 0.655200719833374\n",
      "epoch 1138, loss 0.9064792990684509, R2 0.6530647873878479\n",
      "Eval loss 0.9510257840156555, R2 0.6552915573120117\n",
      "epoch 1139, loss 0.9062562584877014, R2 0.653149425983429\n",
      "Eval loss 0.9507763385772705, R2 0.6553821563720703\n",
      "epoch 1140, loss 0.906033992767334, R2 0.6532338261604309\n",
      "Eval loss 0.9505274295806885, R2 0.6554725766181946\n",
      "epoch 1141, loss 0.90581214427948, R2 0.6533181071281433\n",
      "Eval loss 0.950279176235199, R2 0.6555626392364502\n",
      "epoch 1142, loss 0.9055911898612976, R2 0.6534020900726318\n",
      "Eval loss 0.9500316381454468, R2 0.655652642250061\n",
      "epoch 1143, loss 0.9053707122802734, R2 0.6534858345985413\n",
      "Eval loss 0.9497848749160767, R2 0.6557424068450928\n",
      "epoch 1144, loss 0.9051510095596313, R2 0.6535694003105164\n",
      "Eval loss 0.9495387077331543, R2 0.6558318138122559\n",
      "epoch 1145, loss 0.9049316644668579, R2 0.6536527872085571\n",
      "Eval loss 0.9492932558059692, R2 0.6559211015701294\n",
      "epoch 1146, loss 0.9047132730484009, R2 0.653735876083374\n",
      "Eval loss 0.9490483999252319, R2 0.656010091304779\n",
      "epoch 1147, loss 0.9044954776763916, R2 0.6538188457489014\n",
      "Eval loss 0.9488043189048767, R2 0.6560989022254944\n",
      "epoch 1148, loss 0.9042781591415405, R2 0.6539014577865601\n",
      "Eval loss 0.948560893535614, R2 0.6561875343322754\n",
      "epoch 1149, loss 0.904061496257782, R2 0.6539838910102844\n",
      "Eval loss 0.9483180642127991, R2 0.6562758684158325\n",
      "epoch 1150, loss 0.9038455486297607, R2 0.6540662050247192\n",
      "Eval loss 0.9480759501457214, R2 0.6563640236854553\n",
      "epoch 1151, loss 0.903630256652832, R2 0.6541482210159302\n",
      "Eval loss 0.9478344917297363, R2 0.6564520001411438\n",
      "epoch 1152, loss 0.9034155011177063, R2 0.6542300581932068\n",
      "Eval loss 0.9475937485694885, R2 0.6565396785736084\n",
      "epoch 1153, loss 0.9032014012336731, R2 0.6543116569519043\n",
      "Eval loss 0.9473534822463989, R2 0.6566271781921387\n",
      "epoch 1154, loss 0.9029878377914429, R2 0.6543931365013123\n",
      "Eval loss 0.947114109992981, R2 0.6567144393920898\n",
      "epoch 1155, loss 0.9027750492095947, R2 0.6544743180274963\n",
      "Eval loss 0.9468753337860107, R2 0.6568015217781067\n",
      "epoch 1156, loss 0.9025627970695496, R2 0.6545552611351013\n",
      "Eval loss 0.946636974811554, R2 0.6568883657455444\n",
      "epoch 1157, loss 0.9023510813713074, R2 0.654636025428772\n",
      "Eval loss 0.9463995099067688, R2 0.6569749712944031\n",
      "epoch 1158, loss 0.9021400809288025, R2 0.6547165513038635\n",
      "Eval loss 0.9461628198623657, R2 0.6570614576339722\n",
      "epoch 1159, loss 0.9019296765327454, R2 0.6547969579696655\n",
      "Eval loss 0.9459264278411865, R2 0.6571475863456726\n",
      "epoch 1160, loss 0.901719868183136, R2 0.6548770070075989\n",
      "Eval loss 0.945690929889679, R2 0.6572335958480835\n",
      "epoch 1161, loss 0.9015106558799744, R2 0.6549569964408875\n",
      "Eval loss 0.9454560279846191, R2 0.6573193669319153\n",
      "epoch 1162, loss 0.9013020396232605, R2 0.6550367474555969\n",
      "Eval loss 0.9452217221260071, R2 0.657404899597168\n",
      "epoch 1163, loss 0.9010941386222839, R2 0.6551162600517273\n",
      "Eval loss 0.9449880719184875, R2 0.6574902534484863\n",
      "epoch 1164, loss 0.9008868336677551, R2 0.6551955938339233\n",
      "Eval loss 0.9447550177574158, R2 0.6575753688812256\n",
      "epoch 1165, loss 0.900679886341095, R2 0.6552747488021851\n",
      "Eval loss 0.9445227384567261, R2 0.6576603651046753\n",
      "epoch 1166, loss 0.9004736542701721, R2 0.6553536057472229\n",
      "Eval loss 0.9442910552024841, R2 0.6577450037002563\n",
      "epoch 1167, loss 0.9002681970596313, R2 0.6554322838783264\n",
      "Eval loss 0.9440599679946899, R2 0.6578295230865479\n",
      "epoch 1168, loss 0.9000630378723145, R2 0.6555107235908508\n",
      "Eval loss 0.9438294172286987, R2 0.6579138040542603\n",
      "epoch 1169, loss 0.8998586535453796, R2 0.6555891036987305\n",
      "Eval loss 0.9435995817184448, R2 0.6579979062080383\n",
      "epoch 1170, loss 0.8996548056602478, R2 0.6556671261787415\n",
      "Eval loss 0.9433702826499939, R2 0.6580817699432373\n",
      "epoch 1171, loss 0.899451494216919, R2 0.6557450294494629\n",
      "Eval loss 0.9431416988372803, R2 0.658165454864502\n",
      "epoch 1172, loss 0.8992488980293274, R2 0.65582275390625\n",
      "Eval loss 0.9429137110710144, R2 0.6582489609718323\n",
      "epoch 1173, loss 0.8990468978881836, R2 0.6559001803398132\n",
      "Eval loss 0.9426864385604858, R2 0.6583322286605835\n",
      "epoch 1174, loss 0.8988453149795532, R2 0.6559774875640869\n",
      "Eval loss 0.9424596428871155, R2 0.6584152579307556\n",
      "epoch 1175, loss 0.8986444473266602, R2 0.6560545563697815\n",
      "Eval loss 0.9422335624694824, R2 0.6584980487823486\n",
      "epoch 1176, loss 0.8984440565109253, R2 0.6561314463615417\n",
      "Eval loss 0.9420080184936523, R2 0.6585807204246521\n",
      "epoch 1177, loss 0.8982443809509277, R2 0.6562080979347229\n",
      "Eval loss 0.94178307056427, R2 0.6586632132530212\n",
      "epoch 1178, loss 0.8980451226234436, R2 0.6562846302986145\n",
      "Eval loss 0.941558837890625, R2 0.6587454080581665\n",
      "epoch 1179, loss 0.8978464603424072, R2 0.6563608646392822\n",
      "Eval loss 0.9413352012634277, R2 0.6588274240493774\n",
      "epoch 1180, loss 0.8976484537124634, R2 0.6564369797706604\n",
      "Eval loss 0.9411121010780334, R2 0.658909261226654\n",
      "epoch 1181, loss 0.8974509239196777, R2 0.6565129160881042\n",
      "Eval loss 0.9408895373344421, R2 0.6589908599853516\n",
      "epoch 1182, loss 0.8972539901733398, R2 0.6565886735916138\n",
      "Eval loss 0.9406676888465881, R2 0.6590723395347595\n",
      "epoch 1183, loss 0.8970577120780945, R2 0.6566641330718994\n",
      "Eval loss 0.9404464960098267, R2 0.6591535806655884\n",
      "epoch 1184, loss 0.8968618512153625, R2 0.6567394137382507\n",
      "Eval loss 0.9402258396148682, R2 0.6592345237731934\n",
      "epoch 1185, loss 0.8966665863990784, R2 0.6568145155906677\n",
      "Eval loss 0.9400058388710022, R2 0.6593154072761536\n",
      "epoch 1186, loss 0.8964719176292419, R2 0.6568893790245056\n",
      "Eval loss 0.9397862553596497, R2 0.6593959927558899\n",
      "epoch 1187, loss 0.8962777853012085, R2 0.6569641828536987\n",
      "Eval loss 0.9395673871040344, R2 0.6594764590263367\n",
      "epoch 1188, loss 0.8960843086242676, R2 0.657038688659668\n",
      "Eval loss 0.9393491148948669, R2 0.6595567464828491\n",
      "epoch 1189, loss 0.8958912491798401, R2 0.6571130752563477\n",
      "Eval loss 0.9391313791275024, R2 0.6596367359161377\n",
      "epoch 1190, loss 0.8956987857818604, R2 0.657187283039093\n",
      "Eval loss 0.9389142394065857, R2 0.6597166061401367\n",
      "epoch 1191, loss 0.8955069184303284, R2 0.6572611331939697\n",
      "Eval loss 0.9386976957321167, R2 0.6597962379455566\n",
      "epoch 1192, loss 0.8953155875205994, R2 0.6573349833488464\n",
      "Eval loss 0.9384818077087402, R2 0.6598756313323975\n",
      "epoch 1193, loss 0.8951247334480286, R2 0.657408595085144\n",
      "Eval loss 0.9382665157318115, R2 0.6599549055099487\n",
      "epoch 1194, loss 0.8949344754219055, R2 0.6574819684028625\n",
      "Eval loss 0.9380515813827515, R2 0.6600340604782104\n",
      "epoch 1195, loss 0.8947447538375854, R2 0.6575551629066467\n",
      "Eval loss 0.9378376007080078, R2 0.6601128578186035\n",
      "epoch 1196, loss 0.8945555686950684, R2 0.6576281785964966\n",
      "Eval loss 0.937623918056488, R2 0.660191535949707\n",
      "epoch 1197, loss 0.8943669199943542, R2 0.6577010154724121\n",
      "Eval loss 0.9374107718467712, R2 0.660270094871521\n",
      "epoch 1198, loss 0.8941788077354431, R2 0.6577736735343933\n",
      "Eval loss 0.9371984004974365, R2 0.6603482961654663\n",
      "epoch 1199, loss 0.8939912915229797, R2 0.6578461527824402\n",
      "Eval loss 0.9369863867759705, R2 0.6604264378547668\n",
      "epoch 1200, loss 0.8938042521476746, R2 0.6579183340072632\n",
      "Eval loss 0.9367750883102417, R2 0.6605042815208435\n",
      "epoch 1201, loss 0.8936177492141724, R2 0.6579905152320862\n",
      "Eval loss 0.9365643262863159, R2 0.6605820655822754\n",
      "epoch 1202, loss 0.8934317827224731, R2 0.6580623388290405\n",
      "Eval loss 0.9363541007041931, R2 0.6606595516204834\n",
      "epoch 1203, loss 0.8932462930679321, R2 0.6581341028213501\n",
      "Eval loss 0.9361444115638733, R2 0.6607369184494019\n",
      "epoch 1204, loss 0.8930613994598389, R2 0.6582056283950806\n",
      "Eval loss 0.9359354376792908, R2 0.6608140468597412\n",
      "epoch 1205, loss 0.8928769826889038, R2 0.6582770943641663\n",
      "Eval loss 0.9357268214225769, R2 0.6608909964561462\n",
      "epoch 1206, loss 0.8926931619644165, R2 0.6583482027053833\n",
      "Eval loss 0.9355189204216003, R2 0.6609678268432617\n",
      "epoch 1207, loss 0.8925098180770874, R2 0.6584191918373108\n",
      "Eval loss 0.9353116154670715, R2 0.6610443592071533\n",
      "epoch 1208, loss 0.892326831817627, R2 0.658490002155304\n",
      "Eval loss 0.9351046681404114, R2 0.6611207723617554\n",
      "epoch 1209, loss 0.8921446204185486, R2 0.658560574054718\n",
      "Eval loss 0.9348984360694885, R2 0.6611970663070679\n",
      "epoch 1210, loss 0.8919627666473389, R2 0.6586310863494873\n",
      "Eval loss 0.9346925616264343, R2 0.6612730026245117\n",
      "epoch 1211, loss 0.8917816281318665, R2 0.6587013602256775\n",
      "Eval loss 0.9344874620437622, R2 0.6613489389419556\n",
      "epoch 1212, loss 0.8916008472442627, R2 0.6587714552879333\n",
      "Eval loss 0.9342828989028931, R2 0.6614245176315308\n",
      "epoch 1213, loss 0.8914206027984619, R2 0.6588413119316101\n",
      "Eval loss 0.9340786933898926, R2 0.661500096321106\n",
      "epoch 1214, loss 0.8912408351898193, R2 0.6589109897613525\n",
      "Eval loss 0.9338752031326294, R2 0.6615753173828125\n",
      "epoch 1215, loss 0.8910616040229797, R2 0.6589805483818054\n",
      "Eval loss 0.9336722493171692, R2 0.6616504788398743\n",
      "epoch 1216, loss 0.8908829092979431, R2 0.6590499877929688\n",
      "Eval loss 0.9334696531295776, R2 0.6617254018783569\n",
      "epoch 1217, loss 0.8907046914100647, R2 0.6591192483901978\n",
      "Eval loss 0.9332678318023682, R2 0.6618001461029053\n",
      "epoch 1218, loss 0.8905270099639893, R2 0.6591882705688477\n",
      "Eval loss 0.9330664277076721, R2 0.6618747115135193\n",
      "epoch 1219, loss 0.8903497457504272, R2 0.6592570543289185\n",
      "Eval loss 0.932865560054779, R2 0.6619491577148438\n",
      "epoch 1220, loss 0.8901729583740234, R2 0.6593257188796997\n",
      "Eval loss 0.932665228843689, R2 0.6620233058929443\n",
      "epoch 1221, loss 0.8899968266487122, R2 0.6593942046165466\n",
      "Eval loss 0.9324654936790466, R2 0.6620973348617554\n",
      "epoch 1222, loss 0.8898210525512695, R2 0.659462571144104\n",
      "Eval loss 0.9322661757469177, R2 0.6621711850166321\n",
      "epoch 1223, loss 0.8896458745002747, R2 0.6595306992530823\n",
      "Eval loss 0.9320675134658813, R2 0.6622448563575745\n",
      "epoch 1224, loss 0.8894711136817932, R2 0.659598708152771\n",
      "Eval loss 0.931869387626648, R2 0.6623183488845825\n",
      "epoch 1225, loss 0.8892968893051147, R2 0.6596665382385254\n",
      "Eval loss 0.9316716194152832, R2 0.6623916625976562\n",
      "epoch 1226, loss 0.889123260974884, R2 0.6597341299057007\n",
      "Eval loss 0.9314745664596558, R2 0.6624648571014404\n",
      "epoch 1227, loss 0.8889499306678772, R2 0.6598016023635864\n",
      "Eval loss 0.9312779903411865, R2 0.6625377535820007\n",
      "epoch 1228, loss 0.8887771368026733, R2 0.6598688364028931\n",
      "Eval loss 0.9310817718505859, R2 0.6626105904579163\n",
      "epoch 1229, loss 0.8886048197746277, R2 0.6599359512329102\n",
      "Eval loss 0.9308862686157227, R2 0.6626831889152527\n",
      "epoch 1230, loss 0.8884331583976746, R2 0.6600029468536377\n",
      "Eval loss 0.930691123008728, R2 0.6627556681632996\n",
      "epoch 1231, loss 0.8882617950439453, R2 0.6600697636604309\n",
      "Eval loss 0.9304967522621155, R2 0.6628278493881226\n",
      "epoch 1232, loss 0.8880910277366638, R2 0.660136342048645\n",
      "Eval loss 0.9303026795387268, R2 0.662899911403656\n",
      "epoch 1233, loss 0.887920618057251, R2 0.6602028012275696\n",
      "Eval loss 0.9301092028617859, R2 0.6629718542098999\n",
      "epoch 1234, loss 0.8877507448196411, R2 0.6602690815925598\n",
      "Eval loss 0.929916262626648, R2 0.6630435585975647\n",
      "epoch 1235, loss 0.8875814080238342, R2 0.6603351831436157\n",
      "Eval loss 0.9297236800193787, R2 0.6631151437759399\n",
      "epoch 1236, loss 0.887412428855896, R2 0.6604011654853821\n",
      "Eval loss 0.9295316338539124, R2 0.6631865501403809\n",
      "epoch 1237, loss 0.8872439861297607, R2 0.6604669094085693\n",
      "Eval loss 0.9293403029441833, R2 0.6632577776908875\n",
      "epoch 1238, loss 0.8870761394500732, R2 0.660532534122467\n",
      "Eval loss 0.9291492700576782, R2 0.6633288860321045\n",
      "epoch 1239, loss 0.8869085907936096, R2 0.6605979204177856\n",
      "Eval loss 0.9289587736129761, R2 0.6633996963500977\n",
      "epoch 1240, loss 0.8867416381835938, R2 0.6606631875038147\n",
      "Eval loss 0.9287689924240112, R2 0.6634703874588013\n",
      "epoch 1241, loss 0.8865750432014465, R2 0.660728394985199\n",
      "Eval loss 0.9285793900489807, R2 0.6635410189628601\n",
      "epoch 1242, loss 0.8864089846611023, R2 0.6607932448387146\n",
      "Eval loss 0.9283904433250427, R2 0.6636112928390503\n",
      "epoch 1243, loss 0.8862433433532715, R2 0.6608580946922302\n",
      "Eval loss 0.9282020330429077, R2 0.6636815071105957\n",
      "epoch 1244, loss 0.8860781192779541, R2 0.6609227657318115\n",
      "Eval loss 0.9280139803886414, R2 0.6637515425682068\n",
      "epoch 1245, loss 0.8859134912490845, R2 0.660987138748169\n",
      "Eval loss 0.9278266429901123, R2 0.6638214588165283\n",
      "epoch 1246, loss 0.8857492804527283, R2 0.6610514521598816\n",
      "Eval loss 0.9276397228240967, R2 0.663891077041626\n",
      "epoch 1247, loss 0.8855855464935303, R2 0.6611155271530151\n",
      "Eval loss 0.9274532198905945, R2 0.6639606356620789\n",
      "epoch 1248, loss 0.8854221701622009, R2 0.6611795425415039\n",
      "Eval loss 0.9272672533988953, R2 0.6640300154685974\n",
      "epoch 1249, loss 0.8852592706680298, R2 0.6612433791160583\n",
      "Eval loss 0.9270817637443542, R2 0.6640991568565369\n",
      "epoch 1250, loss 0.8850968480110168, R2 0.6613069772720337\n",
      "Eval loss 0.9268968105316162, R2 0.6641682386398315\n",
      "epoch 1251, loss 0.8849349617958069, R2 0.6613705158233643\n",
      "Eval loss 0.9267122149467468, R2 0.6642371416091919\n",
      "epoch 1252, loss 0.8847734332084656, R2 0.6614338159561157\n",
      "Eval loss 0.9265281558036804, R2 0.6643058657646179\n",
      "epoch 1253, loss 0.8846124410629272, R2 0.6614970564842224\n",
      "Eval loss 0.926344633102417, R2 0.6643743515014648\n",
      "epoch 1254, loss 0.8844518661499023, R2 0.6615599989891052\n",
      "Eval loss 0.9261615872383118, R2 0.6644427180290222\n",
      "epoch 1255, loss 0.8842915892601013, R2 0.6616228818893433\n",
      "Eval loss 0.9259790182113647, R2 0.66451096534729\n",
      "epoch 1256, loss 0.8841319680213928, R2 0.6616855263710022\n",
      "Eval loss 0.9257969856262207, R2 0.6645789742469788\n",
      "epoch 1257, loss 0.8839726448059082, R2 0.6617481112480164\n",
      "Eval loss 0.9256152510643005, R2 0.6646468043327332\n",
      "epoch 1258, loss 0.8838137984275818, R2 0.6618105173110962\n",
      "Eval loss 0.9254340529441833, R2 0.6647145748138428\n",
      "epoch 1259, loss 0.8836555480957031, R2 0.6618727445602417\n",
      "Eval loss 0.9252534508705139, R2 0.6647821664810181\n",
      "epoch 1260, loss 0.8834976553916931, R2 0.6619347929954529\n",
      "Eval loss 0.9250732064247131, R2 0.6648496389389038\n",
      "epoch 1261, loss 0.8833400011062622, R2 0.6619966626167297\n",
      "Eval loss 0.9248935580253601, R2 0.6649168729782104\n",
      "epoch 1262, loss 0.883182942867279, R2 0.6620584726333618\n",
      "Eval loss 0.924714207649231, R2 0.664983868598938\n",
      "epoch 1263, loss 0.8830264210700989, R2 0.6621200442314148\n",
      "Eval loss 0.9245354533195496, R2 0.665050745010376\n",
      "epoch 1264, loss 0.8828702569007874, R2 0.6621814966201782\n",
      "Eval loss 0.9243571758270264, R2 0.6651175618171692\n",
      "epoch 1265, loss 0.8827145099639893, R2 0.6622428297996521\n",
      "Eval loss 0.9241793155670166, R2 0.6651841402053833\n",
      "epoch 1266, loss 0.8825592398643494, R2 0.6623039841651917\n",
      "Eval loss 0.9240019917488098, R2 0.6652505993843079\n",
      "epoch 1267, loss 0.8824042677879333, R2 0.6623649001121521\n",
      "Eval loss 0.9238250255584717, R2 0.6653168797492981\n",
      "epoch 1268, loss 0.8822498321533203, R2 0.6624257564544678\n",
      "Eval loss 0.9236484169960022, R2 0.665382981300354\n",
      "epoch 1269, loss 0.8820959329605103, R2 0.6624864339828491\n",
      "Eval loss 0.9234724640846252, R2 0.6654490232467651\n",
      "epoch 1270, loss 0.8819422125816345, R2 0.6625469326972961\n",
      "Eval loss 0.9232970476150513, R2 0.6655147671699524\n",
      "epoch 1271, loss 0.8817890882492065, R2 0.6626074314117432\n",
      "Eval loss 0.9231219291687012, R2 0.6655804514884949\n",
      "epoch 1272, loss 0.881636381149292, R2 0.6626676321029663\n",
      "Eval loss 0.9229472279548645, R2 0.665645956993103\n",
      "epoch 1273, loss 0.8814840316772461, R2 0.6627275943756104\n",
      "Eval loss 0.9227730631828308, R2 0.6657114028930664\n",
      "epoch 1274, loss 0.8813320398330688, R2 0.6627876162528992\n",
      "Eval loss 0.9225993156433105, R2 0.6657764911651611\n",
      "epoch 1275, loss 0.8811805844306946, R2 0.6628474593162537\n",
      "Eval loss 0.9224260449409485, R2 0.6658415794372559\n",
      "epoch 1276, loss 0.8810295462608337, R2 0.662907063961029\n",
      "Eval loss 0.9222532510757446, R2 0.6659064888954163\n",
      "epoch 1277, loss 0.8808788061141968, R2 0.6629664897918701\n",
      "Eval loss 0.9220808744430542, R2 0.6659711599349976\n",
      "epoch 1278, loss 0.8807287216186523, R2 0.6630258560180664\n",
      "Eval loss 0.9219088554382324, R2 0.6660358309745789\n",
      "epoch 1279, loss 0.880578875541687, R2 0.6630850434303284\n",
      "Eval loss 0.9217374920845032, R2 0.6661001443862915\n",
      "epoch 1280, loss 0.8804295063018799, R2 0.663144052028656\n",
      "Eval loss 0.9215664863586426, R2 0.6661644577980042\n",
      "epoch 1281, loss 0.8802804350852966, R2 0.6632030010223389\n",
      "Eval loss 0.9213957786560059, R2 0.6662285327911377\n",
      "epoch 1282, loss 0.8801319003105164, R2 0.6632616519927979\n",
      "Eval loss 0.9212256669998169, R2 0.6662925481796265\n",
      "epoch 1283, loss 0.8799838423728943, R2 0.6633203029632568\n",
      "Eval loss 0.9210559725761414, R2 0.6663564443588257\n",
      "epoch 1284, loss 0.8798359632492065, R2 0.6633787155151367\n",
      "Eval loss 0.9208866953849792, R2 0.6664201021194458\n",
      "epoch 1285, loss 0.879688560962677, R2 0.6634371280670166\n",
      "Eval loss 0.9207178354263306, R2 0.6664835810661316\n",
      "epoch 1286, loss 0.8795416951179504, R2 0.6634952425956726\n",
      "Eval loss 0.9205493927001953, R2 0.6665470004081726\n",
      "epoch 1287, loss 0.8793951869010925, R2 0.6635532975196838\n",
      "Eval loss 0.9203815460205078, R2 0.6666101217269897\n",
      "epoch 1288, loss 0.879249095916748, R2 0.6636111736297607\n",
      "Eval loss 0.9202139973640442, R2 0.6666732430458069\n",
      "epoch 1289, loss 0.8791033029556274, R2 0.6636688709259033\n",
      "Eval loss 0.920046865940094, R2 0.6667361855506897\n",
      "epoch 1290, loss 0.8789580464363098, R2 0.6637265682220459\n",
      "Eval loss 0.9198801517486572, R2 0.6667989492416382\n",
      "epoch 1291, loss 0.8788130283355713, R2 0.6637839674949646\n",
      "Eval loss 0.9197140336036682, R2 0.6668615937232971\n",
      "epoch 1292, loss 0.8786686062812805, R2 0.663841187953949\n",
      "Eval loss 0.9195482134819031, R2 0.6669241189956665\n",
      "epoch 1293, loss 0.8785243630409241, R2 0.6638984084129333\n",
      "Eval loss 0.9193829298019409, R2 0.6669864058494568\n",
      "epoch 1294, loss 0.8783807158470154, R2 0.6639554500579834\n",
      "Eval loss 0.9192179441452026, R2 0.6670486330986023\n",
      "epoch 1295, loss 0.8782373070716858, R2 0.6640123128890991\n",
      "Eval loss 0.9190534353256226, R2 0.6671106219291687\n",
      "epoch 1296, loss 0.8780943155288696, R2 0.6640691757202148\n",
      "Eval loss 0.9188892841339111, R2 0.6671725511550903\n",
      "epoch 1297, loss 0.8779518604278564, R2 0.6641256809234619\n",
      "Eval loss 0.9187256693840027, R2 0.6672343015670776\n",
      "epoch 1298, loss 0.8778097033500671, R2 0.6641820669174194\n",
      "Eval loss 0.9185624718666077, R2 0.6672958731651306\n",
      "epoch 1299, loss 0.8776679039001465, R2 0.664238452911377\n",
      "Eval loss 0.9183995127677917, R2 0.667357325553894\n",
      "epoch 1300, loss 0.8775265216827393, R2 0.6642946004867554\n",
      "Eval loss 0.9182372093200684, R2 0.6674186587333679\n",
      "epoch 1301, loss 0.8773855566978455, R2 0.6643506288528442\n",
      "Eval loss 0.9180752038955688, R2 0.6674798727035522\n",
      "epoch 1302, loss 0.8772448897361755, R2 0.6644065976142883\n",
      "Eval loss 0.9179136753082275, R2 0.6675407886505127\n",
      "epoch 1303, loss 0.877104640007019, R2 0.6644623279571533\n",
      "Eval loss 0.9177525639533997, R2 0.6676017642021179\n",
      "epoch 1304, loss 0.8769647479057312, R2 0.6645179390907288\n",
      "Eval loss 0.9175917506217957, R2 0.6676624417304993\n",
      "epoch 1305, loss 0.8768253922462463, R2 0.6645734310150146\n",
      "Eval loss 0.9174314141273499, R2 0.6677230596542358\n",
      "epoch 1306, loss 0.8766863942146301, R2 0.6646288633346558\n",
      "Eval loss 0.9172714352607727, R2 0.6677834987640381\n",
      "epoch 1307, loss 0.8765476942062378, R2 0.664683997631073\n",
      "Eval loss 0.9171119928359985, R2 0.6678438782691956\n",
      "epoch 1308, loss 0.8764093518257141, R2 0.6647391319274902\n",
      "Eval loss 0.9169529676437378, R2 0.6679039597511292\n",
      "epoch 1309, loss 0.8762713670730591, R2 0.6647940278053284\n",
      "Eval loss 0.9167941212654114, R2 0.6679641008377075\n",
      "epoch 1310, loss 0.8761337995529175, R2 0.664848804473877\n",
      "Eval loss 0.9166359305381775, R2 0.6680240035057068\n",
      "epoch 1311, loss 0.8759967088699341, R2 0.6649035215377808\n",
      "Eval loss 0.9164780378341675, R2 0.6680837869644165\n",
      "epoch 1312, loss 0.8758598566055298, R2 0.6649580597877502\n",
      "Eval loss 0.9163205623626709, R2 0.6681434512138367\n",
      "epoch 1313, loss 0.8757234811782837, R2 0.6650124192237854\n",
      "Eval loss 0.9161635041236877, R2 0.6682027578353882\n",
      "epoch 1314, loss 0.8755874037742615, R2 0.6650667190551758\n",
      "Eval loss 0.9160069227218628, R2 0.668262243270874\n",
      "epoch 1315, loss 0.8754517436027527, R2 0.6651209592819214\n",
      "Eval loss 0.9158506393432617, R2 0.6683213710784912\n",
      "epoch 1316, loss 0.875316321849823, R2 0.6651749014854431\n",
      "Eval loss 0.9156947731971741, R2 0.6683804988861084\n",
      "epoch 1317, loss 0.8751814961433411, R2 0.6652287244796753\n",
      "Eval loss 0.9155392646789551, R2 0.6684393286705017\n",
      "epoch 1318, loss 0.8750468492507935, R2 0.6652825474739075\n",
      "Eval loss 0.9153841733932495, R2 0.6684980988502502\n",
      "epoch 1319, loss 0.8749126195907593, R2 0.6653361320495605\n",
      "Eval loss 0.9152294397354126, R2 0.668556809425354\n",
      "epoch 1320, loss 0.8747788071632385, R2 0.6653895974159241\n",
      "Eval loss 0.9150752425193787, R2 0.6686153411865234\n",
      "epoch 1321, loss 0.8746452331542969, R2 0.665442943572998\n",
      "Eval loss 0.9149213433265686, R2 0.6686736941337585\n",
      "epoch 1322, loss 0.8745121955871582, R2 0.6654961705207825\n",
      "Eval loss 0.9147678017616272, R2 0.6687319278717041\n",
      "epoch 1323, loss 0.8743793368339539, R2 0.6655492782592773\n",
      "Eval loss 0.914614737033844, R2 0.6687900424003601\n",
      "epoch 1324, loss 0.8742468953132629, R2 0.6656021475791931\n",
      "Eval loss 0.9144620299339294, R2 0.6688480377197266\n",
      "epoch 1325, loss 0.8741149306297302, R2 0.6656548976898193\n",
      "Eval loss 0.9143096804618835, R2 0.6689058542251587\n",
      "epoch 1326, loss 0.8739832639694214, R2 0.6657077074050903\n",
      "Eval loss 0.9141576886177063, R2 0.6689635515213013\n",
      "epoch 1327, loss 0.8738519549369812, R2 0.6657602787017822\n",
      "Eval loss 0.9140061736106873, R2 0.6690210700035095\n",
      "epoch 1328, loss 0.8737209439277649, R2 0.6658126711845398\n",
      "Eval loss 0.9138550162315369, R2 0.6690785884857178\n",
      "epoch 1329, loss 0.8735902905464172, R2 0.6658650040626526\n",
      "Eval loss 0.9137042164802551, R2 0.6691358685493469\n",
      "epoch 1330, loss 0.8734601140022278, R2 0.665917158126831\n",
      "Eval loss 0.9135539531707764, R2 0.6691930294036865\n",
      "epoch 1331, loss 0.8733302354812622, R2 0.6659692525863647\n",
      "Eval loss 0.9134038686752319, R2 0.6692501306533813\n",
      "epoch 1332, loss 0.8732006549835205, R2 0.6660211086273193\n",
      "Eval loss 0.9132542014122009, R2 0.6693069934844971\n",
      "epoch 1333, loss 0.8730714917182922, R2 0.6660729050636292\n",
      "Eval loss 0.9131050109863281, R2 0.669363796710968\n",
      "epoch 1334, loss 0.8729425668716431, R2 0.6661246418952942\n",
      "Eval loss 0.9129562377929688, R2 0.6694204807281494\n",
      "epoch 1335, loss 0.8728141188621521, R2 0.6661762595176697\n",
      "Eval loss 0.9128075838088989, R2 0.6694769263267517\n",
      "epoch 1336, loss 0.8726858496665955, R2 0.6662275791168213\n",
      "Eval loss 0.9126595258712769, R2 0.669533371925354\n",
      "epoch 1337, loss 0.8725581169128418, R2 0.6662788987159729\n",
      "Eval loss 0.9125118255615234, R2 0.6695895791053772\n",
      "epoch 1338, loss 0.872430682182312, R2 0.666330099105835\n",
      "Eval loss 0.9123643636703491, R2 0.6696457266807556\n",
      "epoch 1339, loss 0.8723036050796509, R2 0.6663811206817627\n",
      "Eval loss 0.9122173190116882, R2 0.6697016954421997\n",
      "epoch 1340, loss 0.8721767663955688, R2 0.6664320230484009\n",
      "Eval loss 0.9120707511901855, R2 0.669757604598999\n",
      "epoch 1341, loss 0.8720503449440002, R2 0.6664828658103943\n",
      "Eval loss 0.9119244813919067, R2 0.669813334941864\n",
      "epoch 1342, loss 0.8719242215156555, R2 0.6665334701538086\n",
      "Eval loss 0.9117786884307861, R2 0.6698688864707947\n",
      "epoch 1343, loss 0.8717985153198242, R2 0.6665840744972229\n",
      "Eval loss 0.9116331934928894, R2 0.6699243783950806\n",
      "epoch 1344, loss 0.8716731667518616, R2 0.6666344404220581\n",
      "Eval loss 0.9114879369735718, R2 0.6699797511100769\n",
      "epoch 1345, loss 0.8715481758117676, R2 0.6666847467422485\n",
      "Eval loss 0.9113432168960571, R2 0.6700350046157837\n",
      "epoch 1346, loss 0.8714233040809631, R2 0.6667349338531494\n",
      "Eval loss 0.9111986756324768, R2 0.6700901389122009\n",
      "epoch 1347, loss 0.8712989687919617, R2 0.6667850613594055\n",
      "Eval loss 0.911054790019989, R2 0.6701450943946838\n",
      "epoch 1348, loss 0.8711748719215393, R2 0.6668350100517273\n",
      "Eval loss 0.910910964012146, R2 0.6701998710632324\n",
      "epoch 1349, loss 0.8710511326789856, R2 0.6668847799301147\n",
      "Eval loss 0.9107676148414612, R2 0.6702545881271362\n",
      "epoch 1350, loss 0.8709276914596558, R2 0.6669344902038574\n",
      "Eval loss 0.9106248021125793, R2 0.6703091859817505\n",
      "epoch 1351, loss 0.8708046674728394, R2 0.6669840216636658\n",
      "Eval loss 0.9104821681976318, R2 0.6703636050224304\n",
      "epoch 1352, loss 0.8706819415092468, R2 0.6670335531234741\n",
      "Eval loss 0.9103399515151978, R2 0.6704180240631104\n",
      "epoch 1353, loss 0.870559573173523, R2 0.6670827865600586\n",
      "Eval loss 0.9101980328559875, R2 0.670472264289856\n",
      "epoch 1354, loss 0.870437502861023, R2 0.6671320199966431\n",
      "Eval loss 0.9100565314292908, R2 0.670526385307312\n",
      "epoch 1355, loss 0.8703157901763916, R2 0.667181134223938\n",
      "Eval loss 0.9099153280258179, R2 0.6705803275108337\n",
      "epoch 1356, loss 0.8701943159103394, R2 0.6672301292419434\n",
      "Eval loss 0.9097745418548584, R2 0.6706340909004211\n",
      "epoch 1357, loss 0.8700731992721558, R2 0.6672789454460144\n",
      "Eval loss 0.9096341133117676, R2 0.6706878542900085\n",
      "epoch 1358, loss 0.8699524998664856, R2 0.6673277020454407\n",
      "Eval loss 0.9094938039779663, R2 0.6707414388656616\n",
      "epoch 1359, loss 0.8698320984840393, R2 0.6673763990402222\n",
      "Eval loss 0.9093540906906128, R2 0.6707949638366699\n",
      "epoch 1360, loss 0.8697120547294617, R2 0.6674248576164246\n",
      "Eval loss 0.9092147350311279, R2 0.6708482503890991\n",
      "epoch 1361, loss 0.8695921301841736, R2 0.6674732565879822\n",
      "Eval loss 0.9090756773948669, R2 0.6709014773368835\n",
      "epoch 1362, loss 0.8694727420806885, R2 0.6675214767456055\n",
      "Eval loss 0.9089369773864746, R2 0.670954704284668\n",
      "epoch 1363, loss 0.8693535327911377, R2 0.667569637298584\n",
      "Eval loss 0.9087985754013062, R2 0.6710076332092285\n",
      "epoch 1364, loss 0.8692347407341003, R2 0.6676177382469177\n",
      "Eval loss 0.9086605906486511, R2 0.6710605025291443\n",
      "epoch 1365, loss 0.8691163063049316, R2 0.6676655411720276\n",
      "Eval loss 0.9085229635238647, R2 0.6711131930351257\n",
      "epoch 1366, loss 0.868998110294342, R2 0.6677134037017822\n",
      "Eval loss 0.9083855152130127, R2 0.6711658835411072\n",
      "epoch 1367, loss 0.8688800930976868, R2 0.6677610278129578\n",
      "Eval loss 0.9082486629486084, R2 0.6712183952331543\n",
      "epoch 1368, loss 0.8687625527381897, R2 0.6678085923194885\n",
      "Eval loss 0.908112108707428, R2 0.6712707281112671\n",
      "epoch 1369, loss 0.8686453700065613, R2 0.6678560376167297\n",
      "Eval loss 0.9079756736755371, R2 0.6713230013847351\n",
      "epoch 1370, loss 0.8685283660888672, R2 0.667903482913971\n",
      "Eval loss 0.9078397750854492, R2 0.6713751554489136\n",
      "epoch 1371, loss 0.8684117197990417, R2 0.6679506897926331\n",
      "Eval loss 0.9077041149139404, R2 0.6714271903038025\n",
      "epoch 1372, loss 0.868295431137085, R2 0.6679977774620056\n",
      "Eval loss 0.9075688719749451, R2 0.6714791059494019\n",
      "epoch 1373, loss 0.868179440498352, R2 0.6680446863174438\n",
      "Eval loss 0.9074338674545288, R2 0.6715309023857117\n",
      "epoch 1374, loss 0.868063747882843, R2 0.6680916547775269\n",
      "Eval loss 0.9072992205619812, R2 0.6715825796127319\n",
      "epoch 1375, loss 0.8679484128952026, R2 0.6681384444236755\n",
      "Eval loss 0.907164990901947, R2 0.6716341376304626\n",
      "epoch 1376, loss 0.8678333163261414, R2 0.6681851148605347\n",
      "Eval loss 0.9070310592651367, R2 0.6716855764389038\n",
      "epoch 1377, loss 0.8677185773849487, R2 0.6682316660881042\n",
      "Eval loss 0.9068974256515503, R2 0.6717368960380554\n",
      "epoch 1378, loss 0.8676040768623352, R2 0.6682780981063843\n",
      "Eval loss 0.9067641496658325, R2 0.6717880964279175\n",
      "epoch 1379, loss 0.8674899935722351, R2 0.6683244705200195\n",
      "Eval loss 0.906631350517273, R2 0.67183917760849\n",
      "epoch 1380, loss 0.8673760890960693, R2 0.6683706641197205\n",
      "Eval loss 0.9064986109733582, R2 0.6718901991844177\n",
      "epoch 1381, loss 0.8672625422477722, R2 0.6684167385101318\n",
      "Eval loss 0.9063663482666016, R2 0.6719409823417664\n",
      "epoch 1382, loss 0.8671493530273438, R2 0.6684626936912537\n",
      "Eval loss 0.9062344431877136, R2 0.671991765499115\n",
      "epoch 1383, loss 0.8670363426208496, R2 0.6685086488723755\n",
      "Eval loss 0.9061028361320496, R2 0.6720424294471741\n",
      "epoch 1384, loss 0.8669236898422241, R2 0.6685543656349182\n",
      "Eval loss 0.9059714674949646, R2 0.6720929741859436\n",
      "epoch 1385, loss 0.8668113946914673, R2 0.6686000823974609\n",
      "Eval loss 0.9058405756950378, R2 0.672143280506134\n",
      "epoch 1386, loss 0.8666993379592896, R2 0.6686456203460693\n",
      "Eval loss 0.9057098627090454, R2 0.6721936464309692\n",
      "epoch 1387, loss 0.8665875792503357, R2 0.6686910390853882\n",
      "Eval loss 0.9055795669555664, R2 0.6722438335418701\n",
      "epoch 1388, loss 0.8664761185646057, R2 0.6687363386154175\n",
      "Eval loss 0.9054495096206665, R2 0.6722938418388367\n",
      "epoch 1389, loss 0.8663650155067444, R2 0.6687816381454468\n",
      "Eval loss 0.90531986951828, R2 0.6723437905311584\n",
      "epoch 1390, loss 0.8662541508674622, R2 0.668826699256897\n",
      "Eval loss 0.9051905870437622, R2 0.6723936200141907\n",
      "epoch 1391, loss 0.866143524646759, R2 0.6688717007637024\n",
      "Eval loss 0.9050616025924683, R2 0.6724433302879333\n",
      "epoch 1392, loss 0.8660332560539246, R2 0.6689165830612183\n",
      "Eval loss 0.9049327969551086, R2 0.6724929213523865\n",
      "epoch 1393, loss 0.8659233450889587, R2 0.6689613461494446\n",
      "Eval loss 0.9048044085502625, R2 0.67254239320755\n",
      "epoch 1394, loss 0.8658136129379272, R2 0.6690061092376709\n",
      "Eval loss 0.9046764373779297, R2 0.6725917458534241\n",
      "epoch 1395, loss 0.8657042384147644, R2 0.6690506339073181\n",
      "Eval loss 0.9045485258102417, R2 0.6726410388946533\n",
      "epoch 1396, loss 0.8655950427055359, R2 0.6690951585769653\n",
      "Eval loss 0.9044211506843567, R2 0.6726902723312378\n",
      "epoch 1397, loss 0.8654862642288208, R2 0.6691395044326782\n",
      "Eval loss 0.904293954372406, R2 0.6727392673492432\n",
      "epoch 1398, loss 0.8653777241706848, R2 0.6691837906837463\n",
      "Eval loss 0.9041671752929688, R2 0.6727882027626038\n",
      "epoch 1399, loss 0.8652695417404175, R2 0.6692278981208801\n",
      "Eval loss 0.9040406942367554, R2 0.6728370189666748\n",
      "epoch 1400, loss 0.8651615977287292, R2 0.6692719459533691\n",
      "Eval loss 0.9039145112037659, R2 0.6728857755661011\n",
      "epoch 1401, loss 0.8650539517402649, R2 0.6693159341812134\n",
      "Eval loss 0.9037886261940002, R2 0.6729344129562378\n",
      "epoch 1402, loss 0.8649464845657349, R2 0.6693596839904785\n",
      "Eval loss 0.903663158416748, R2 0.6729828715324402\n",
      "epoch 1403, loss 0.8648393750190735, R2 0.6694034934043884\n",
      "Eval loss 0.9035378098487854, R2 0.6730312705039978\n",
      "epoch 1404, loss 0.864732563495636, R2 0.6694470643997192\n",
      "Eval loss 0.9034128189086914, R2 0.6730795502662659\n",
      "epoch 1405, loss 0.8646261096000671, R2 0.66949063539505\n",
      "Eval loss 0.9032881259918213, R2 0.6731277108192444\n",
      "epoch 1406, loss 0.8645197749137878, R2 0.6695340871810913\n",
      "Eval loss 0.9031639099121094, R2 0.6731757521629333\n",
      "epoch 1407, loss 0.8644137978553772, R2 0.669577419757843\n",
      "Eval loss 0.903039813041687, R2 0.6732236742973328\n",
      "epoch 1408, loss 0.8643081188201904, R2 0.6696205735206604\n",
      "Eval loss 0.9029160737991333, R2 0.6732715964317322\n",
      "epoch 1409, loss 0.8642027378082275, R2 0.6696637272834778\n",
      "Eval loss 0.9027926921844482, R2 0.6733193397521973\n",
      "epoch 1410, loss 0.864097535610199, R2 0.6697067022323608\n",
      "Eval loss 0.9026694893836975, R2 0.6733669638633728\n",
      "epoch 1411, loss 0.8639927506446838, R2 0.6697496175765991\n",
      "Eval loss 0.9025467038154602, R2 0.6734144687652588\n",
      "epoch 1412, loss 0.8638880848884583, R2 0.6697924137115479\n",
      "Eval loss 0.9024242162704468, R2 0.6734619140625\n",
      "epoch 1413, loss 0.8637837767601013, R2 0.6698351502418518\n",
      "Eval loss 0.9023019671440125, R2 0.6735091805458069\n",
      "epoch 1414, loss 0.8636797666549683, R2 0.6698777675628662\n",
      "Eval loss 0.9021801352500916, R2 0.6735564470291138\n",
      "epoch 1415, loss 0.8635760545730591, R2 0.6699202060699463\n",
      "Eval loss 0.902058482170105, R2 0.6736035346984863\n",
      "epoch 1416, loss 0.863472580909729, R2 0.6699626445770264\n",
      "Eval loss 0.9019371271133423, R2 0.6736505031585693\n",
      "epoch 1417, loss 0.8633692860603333, R2 0.6700049042701721\n",
      "Eval loss 0.9018161296844482, R2 0.6736973524093628\n",
      "epoch 1418, loss 0.8632663488388062, R2 0.6700471043586731\n",
      "Eval loss 0.9016954302787781, R2 0.6737442016601562\n",
      "epoch 1419, loss 0.8631637096405029, R2 0.6700891852378845\n",
      "Eval loss 0.901574969291687, R2 0.6737908720970154\n",
      "epoch 1420, loss 0.863061249256134, R2 0.670131266117096\n",
      "Eval loss 0.9014549255371094, R2 0.6738374829292297\n",
      "epoch 1421, loss 0.8629591464996338, R2 0.6701731085777283\n",
      "Eval loss 0.9013350009918213, R2 0.6738839149475098\n",
      "epoch 1422, loss 0.8628572821617126, R2 0.6702148914337158\n",
      "Eval loss 0.9012154936790466, R2 0.6739303469657898\n",
      "epoch 1423, loss 0.8627556562423706, R2 0.6702566146850586\n",
      "Eval loss 0.9010962843894958, R2 0.6739766001701355\n",
      "epoch 1424, loss 0.8626542687416077, R2 0.6702982187271118\n",
      "Eval loss 0.9009773135185242, R2 0.6740227341651917\n",
      "epoch 1425, loss 0.8625532388687134, R2 0.6703397035598755\n",
      "Eval loss 0.9008585810661316, R2 0.6740688681602478\n",
      "epoch 1426, loss 0.862452507019043, R2 0.6703811287879944\n",
      "Eval loss 0.9007402062416077, R2 0.6741148233413696\n",
      "epoch 1427, loss 0.8623519539833069, R2 0.6704224348068237\n",
      "Eval loss 0.9006220698356628, R2 0.6741605997085571\n",
      "epoch 1428, loss 0.8622516393661499, R2 0.6704636216163635\n",
      "Eval loss 0.9005042910575867, R2 0.6742063760757446\n",
      "epoch 1429, loss 0.8621516823768616, R2 0.6705047488212585\n",
      "Eval loss 0.9003868103027344, R2 0.6742519736289978\n",
      "epoch 1430, loss 0.8620518445968628, R2 0.670545756816864\n",
      "Eval loss 0.900269627571106, R2 0.674297571182251\n",
      "epoch 1431, loss 0.8619524240493774, R2 0.6705865859985352\n",
      "Eval loss 0.9001526236534119, R2 0.6743430495262146\n",
      "epoch 1432, loss 0.8618531823158264, R2 0.6706275343894958\n",
      "Eval loss 0.9000359773635864, R2 0.6743883490562439\n",
      "epoch 1433, loss 0.8617541790008545, R2 0.6706682443618774\n",
      "Eval loss 0.8999195098876953, R2 0.6744336485862732\n",
      "epoch 1434, loss 0.8616555333137512, R2 0.6707088351249695\n",
      "Eval loss 0.8998034000396729, R2 0.6744787693023682\n",
      "epoch 1435, loss 0.8615570664405823, R2 0.6707494258880615\n",
      "Eval loss 0.8996877074241638, R2 0.6745237708091736\n",
      "epoch 1436, loss 0.8614587783813477, R2 0.6707897782325745\n",
      "Eval loss 0.8995720148086548, R2 0.674568772315979\n",
      "epoch 1437, loss 0.8613608479499817, R2 0.6708301901817322\n",
      "Eval loss 0.8994568586349487, R2 0.6746135950088501\n",
      "epoch 1438, loss 0.8612631559371948, R2 0.6708704233169556\n",
      "Eval loss 0.8993418216705322, R2 0.6746584177017212\n",
      "epoch 1439, loss 0.8611657619476318, R2 0.6709104776382446\n",
      "Eval loss 0.8992272019386292, R2 0.6747030019760132\n",
      "epoch 1440, loss 0.861068606376648, R2 0.6709505915641785\n",
      "Eval loss 0.8991128206253052, R2 0.6747475266456604\n",
      "epoch 1441, loss 0.8609716296195984, R2 0.670990526676178\n",
      "Eval loss 0.8989987373352051, R2 0.6747919917106628\n",
      "epoch 1442, loss 0.8608750104904175, R2 0.6710303425788879\n",
      "Eval loss 0.8988847136497498, R2 0.674836277961731\n",
      "epoch 1443, loss 0.8607785105705261, R2 0.6710701584815979\n",
      "Eval loss 0.8987712860107422, R2 0.6748805642127991\n",
      "epoch 1444, loss 0.8606824278831482, R2 0.6711098551750183\n",
      "Eval loss 0.8986578583717346, R2 0.6749247312545776\n",
      "epoch 1445, loss 0.8605864644050598, R2 0.6711494326591492\n",
      "Eval loss 0.8985448479652405, R2 0.6749687790870667\n",
      "epoch 1446, loss 0.8604907989501953, R2 0.6711889505386353\n",
      "Eval loss 0.8984320759773254, R2 0.6750127077102661\n",
      "epoch 1447, loss 0.8603953719139099, R2 0.6712283492088318\n",
      "Eval loss 0.8983194828033447, R2 0.6750566363334656\n",
      "epoch 1448, loss 0.8603002429008484, R2 0.6712676882743835\n",
      "Eval loss 0.8982073068618774, R2 0.6751003861427307\n",
      "epoch 1449, loss 0.8602052927017212, R2 0.671306848526001\n",
      "Eval loss 0.8980953693389893, R2 0.6751441359519958\n",
      "epoch 1450, loss 0.8601106405258179, R2 0.6713460087776184\n",
      "Eval loss 0.8979836702346802, R2 0.6751876473426819\n",
      "epoch 1451, loss 0.8600161671638489, R2 0.6713849902153015\n",
      "Eval loss 0.897872269153595, R2 0.6752311587333679\n",
      "epoch 1452, loss 0.8599219918251038, R2 0.6714239120483398\n",
      "Eval loss 0.8977611064910889, R2 0.6752744913101196\n",
      "epoch 1453, loss 0.8598280549049377, R2 0.6714627742767334\n",
      "Eval loss 0.8976502418518066, R2 0.6753178238868713\n",
      "epoch 1454, loss 0.8597343564033508, R2 0.6715015769004822\n",
      "Eval loss 0.8975396752357483, R2 0.6753610372543335\n",
      "epoch 1455, loss 0.8596407771110535, R2 0.6715402603149414\n",
      "Eval loss 0.897429347038269, R2 0.6754040122032166\n",
      "epoch 1456, loss 0.8595476746559143, R2 0.6715787649154663\n",
      "Eval loss 0.8973193168640137, R2 0.6754470467567444\n",
      "epoch 1457, loss 0.8594547510147095, R2 0.6716172695159912\n",
      "Eval loss 0.8972094655036926, R2 0.6754899621009827\n",
      "epoch 1458, loss 0.8593619465827942, R2 0.6716556549072266\n",
      "Eval loss 0.8970999121665955, R2 0.6755327582359314\n",
      "epoch 1459, loss 0.8592694401741028, R2 0.6716939210891724\n",
      "Eval loss 0.8969906568527222, R2 0.6755754947662354\n",
      "epoch 1460, loss 0.8591772317886353, R2 0.6717321872711182\n",
      "Eval loss 0.8968816995620728, R2 0.6756181120872498\n",
      "epoch 1461, loss 0.859085202217102, R2 0.6717703342437744\n",
      "Eval loss 0.8967729210853577, R2 0.6756606698036194\n",
      "epoch 1462, loss 0.8589934706687927, R2 0.6718083024024963\n",
      "Eval loss 0.8966644406318665, R2 0.6757031083106995\n",
      "epoch 1463, loss 0.8589019179344177, R2 0.6718462705612183\n",
      "Eval loss 0.8965561389923096, R2 0.67574542760849\n",
      "epoch 1464, loss 0.8588106632232666, R2 0.6718841195106506\n",
      "Eval loss 0.8964481949806213, R2 0.6757876873016357\n",
      "epoch 1465, loss 0.8587194681167603, R2 0.6719219088554382\n",
      "Eval loss 0.8963404893875122, R2 0.6758297681808472\n",
      "epoch 1466, loss 0.8586287498474121, R2 0.6719595789909363\n",
      "Eval loss 0.8962330222129822, R2 0.6758719086647034\n",
      "epoch 1467, loss 0.8585380911827087, R2 0.6719971299171448\n",
      "Eval loss 0.8961257934570312, R2 0.6759138703346252\n",
      "epoch 1468, loss 0.858447790145874, R2 0.6720346808433533\n",
      "Eval loss 0.896018922328949, R2 0.6759557723999023\n",
      "epoch 1469, loss 0.8583576679229736, R2 0.6720721125602722\n",
      "Eval loss 0.895912230014801, R2 0.6759975552558899\n",
      "epoch 1470, loss 0.8582677841186523, R2 0.6721094846725464\n",
      "Eval loss 0.8958057761192322, R2 0.6760392189025879\n",
      "epoch 1471, loss 0.8581780791282654, R2 0.6721466183662415\n",
      "Eval loss 0.8956995010375977, R2 0.6760808825492859\n",
      "epoch 1472, loss 0.8580886721611023, R2 0.6721838116645813\n",
      "Eval loss 0.8955936431884766, R2 0.6761223077774048\n",
      "epoch 1473, loss 0.8579994440078735, R2 0.6722208261489868\n",
      "Eval loss 0.8954880237579346, R2 0.6761637330055237\n",
      "epoch 1474, loss 0.8579104542732239, R2 0.6722579002380371\n",
      "Eval loss 0.8953826427459717, R2 0.6762050986289978\n",
      "epoch 1475, loss 0.8578217625617981, R2 0.6722946763038635\n",
      "Eval loss 0.8952774405479431, R2 0.6762464046478271\n",
      "epoch 1476, loss 0.8577331900596619, R2 0.6723315119743347\n",
      "Eval loss 0.8951725363731384, R2 0.6762874722480774\n",
      "epoch 1477, loss 0.8576449751853943, R2 0.6723682880401611\n",
      "Eval loss 0.8950677514076233, R2 0.6763285398483276\n",
      "epoch 1478, loss 0.8575568795204163, R2 0.6724050045013428\n",
      "Eval loss 0.8949635028839111, R2 0.6763694882392883\n",
      "epoch 1479, loss 0.8574690222740173, R2 0.6724415421485901\n",
      "Eval loss 0.894859254360199, R2 0.676410436630249\n",
      "epoch 1480, loss 0.8573814630508423, R2 0.6724779605865479\n",
      "Eval loss 0.8947552442550659, R2 0.6764511466026306\n",
      "epoch 1481, loss 0.8572940826416016, R2 0.6725143790245056\n",
      "Eval loss 0.8946517109870911, R2 0.676491916179657\n",
      "epoch 1482, loss 0.8572068214416504, R2 0.672550618648529\n",
      "Eval loss 0.894548237323761, R2 0.676532506942749\n",
      "epoch 1483, loss 0.8571199178695679, R2 0.6725868582725525\n",
      "Eval loss 0.8944450616836548, R2 0.6765730381011963\n",
      "epoch 1484, loss 0.8570332527160645, R2 0.6726229786872864\n",
      "Eval loss 0.8943421244621277, R2 0.676613450050354\n",
      "epoch 1485, loss 0.8569467663764954, R2 0.6726589798927307\n",
      "Eval loss 0.8942394852638245, R2 0.6766538619995117\n",
      "epoch 1486, loss 0.8568604588508606, R2 0.672694981098175\n",
      "Eval loss 0.8941370844841003, R2 0.6766940951347351\n",
      "epoch 1487, loss 0.8567744493484497, R2 0.6727308630943298\n",
      "Eval loss 0.8940348029136658, R2 0.676734209060669\n",
      "epoch 1488, loss 0.8566886782646179, R2 0.6727666854858398\n",
      "Eval loss 0.8939329385757446, R2 0.6767743229866028\n",
      "epoch 1489, loss 0.8566030263900757, R2 0.6728023886680603\n",
      "Eval loss 0.893831193447113, R2 0.6768143177032471\n",
      "epoch 1490, loss 0.8565176129341125, R2 0.6728379726409912\n",
      "Eval loss 0.8937296271324158, R2 0.6768543124198914\n",
      "epoch 1491, loss 0.8564324975013733, R2 0.6728735566139221\n",
      "Eval loss 0.8936284780502319, R2 0.6768940687179565\n",
      "epoch 1492, loss 0.8563475608825684, R2 0.6729090213775635\n",
      "Eval loss 0.8935274481773376, R2 0.6769338250160217\n",
      "epoch 1493, loss 0.8562628030776978, R2 0.6729444265365601\n",
      "Eval loss 0.893426775932312, R2 0.6769734025001526\n",
      "epoch 1494, loss 0.856178343296051, R2 0.6729796528816223\n",
      "Eval loss 0.8933262228965759, R2 0.6770130395889282\n",
      "epoch 1495, loss 0.8560941219329834, R2 0.6730148792266846\n",
      "Eval loss 0.893225908279419, R2 0.6770525574684143\n",
      "epoch 1496, loss 0.8560099005699158, R2 0.673050045967102\n",
      "Eval loss 0.8931258916854858, R2 0.6770919561386108\n",
      "epoch 1497, loss 0.8559260368347168, R2 0.6730850338935852\n",
      "Eval loss 0.8930261731147766, R2 0.6771312355995178\n",
      "epoch 1498, loss 0.8558424115180969, R2 0.6731200218200684\n",
      "Eval loss 0.8929265737533569, R2 0.6771703958511353\n",
      "epoch 1499, loss 0.8557589054107666, R2 0.6731549501419067\n",
      "Eval loss 0.8928272128105164, R2 0.6772096157073975\n",
      "epoch 1500, loss 0.8556756973266602, R2 0.6731897592544556\n",
      "Eval loss 0.8927280902862549, R2 0.6772486567497253\n",
      "epoch 1501, loss 0.8555927276611328, R2 0.6732245683670044\n",
      "Eval loss 0.8926292657852173, R2 0.6772876977920532\n",
      "epoch 1502, loss 0.855509877204895, R2 0.6732591390609741\n",
      "Eval loss 0.892530620098114, R2 0.6773264408111572\n",
      "epoch 1503, loss 0.8554273247718811, R2 0.6732937097549438\n",
      "Eval loss 0.8924322128295898, R2 0.6773653030395508\n",
      "epoch 1504, loss 0.8553449511528015, R2 0.6733282208442688\n",
      "Eval loss 0.8923341035842896, R2 0.67740398645401\n",
      "epoch 1505, loss 0.855262815952301, R2 0.673362672328949\n",
      "Eval loss 0.8922361731529236, R2 0.6774426102638245\n",
      "epoch 1506, loss 0.8551807999610901, R2 0.6733970046043396\n",
      "Eval loss 0.8921384215354919, R2 0.6774812340736389\n",
      "epoch 1507, loss 0.8550990223884583, R2 0.6734312176704407\n",
      "Eval loss 0.8920409679412842, R2 0.6775196194648743\n",
      "epoch 1508, loss 0.8550174236297607, R2 0.6734654903411865\n",
      "Eval loss 0.8919438719749451, R2 0.6775580644607544\n",
      "epoch 1509, loss 0.8549362421035767, R2 0.6734995245933533\n",
      "Eval loss 0.8918468356132507, R2 0.677596390247345\n",
      "epoch 1510, loss 0.8548550009727478, R2 0.67353355884552\n",
      "Eval loss 0.8917499780654907, R2 0.677634596824646\n",
      "epoch 1511, loss 0.8547741770744324, R2 0.6735674738883972\n",
      "Eval loss 0.8916535377502441, R2 0.6776727437973022\n",
      "epoch 1512, loss 0.8546934127807617, R2 0.6736013293266296\n",
      "Eval loss 0.8915570378303528, R2 0.6777108311653137\n",
      "epoch 1513, loss 0.8546129465103149, R2 0.6736350655555725\n",
      "Eval loss 0.8914609551429749, R2 0.6777487397193909\n",
      "epoch 1514, loss 0.8545325398445129, R2 0.6736688017845154\n",
      "Eval loss 0.8913651704788208, R2 0.677786648273468\n",
      "epoch 1515, loss 0.8544524908065796, R2 0.6737024188041687\n",
      "Eval loss 0.8912694454193115, R2 0.6778244376182556\n",
      "epoch 1516, loss 0.8543725609779358, R2 0.6737359762191772\n",
      "Eval loss 0.8911740779876709, R2 0.6778622269630432\n",
      "epoch 1517, loss 0.8542928695678711, R2 0.6737694144248962\n",
      "Eval loss 0.8910788297653198, R2 0.6778998374938965\n",
      "epoch 1518, loss 0.8542134165763855, R2 0.6738028526306152\n",
      "Eval loss 0.8909839391708374, R2 0.677937388420105\n",
      "epoch 1519, loss 0.8541340827941895, R2 0.6738361716270447\n",
      "Eval loss 0.8908891677856445, R2 0.6779749393463135\n",
      "epoch 1520, loss 0.8540549874305725, R2 0.6738694310188293\n",
      "Eval loss 0.8907946944236755, R2 0.6780122518539429\n",
      "epoch 1521, loss 0.8539760112762451, R2 0.6739025712013245\n",
      "Eval loss 0.8907003998756409, R2 0.6780495643615723\n",
      "epoch 1522, loss 0.8538973331451416, R2 0.6739356517791748\n",
      "Eval loss 0.8906062841415405, R2 0.6780868172645569\n",
      "epoch 1523, loss 0.8538188338279724, R2 0.6739686727523804\n",
      "Eval loss 0.8905123472213745, R2 0.678123950958252\n",
      "epoch 1524, loss 0.8537406325340271, R2 0.6740015745162964\n",
      "Eval loss 0.8904187083244324, R2 0.678161084651947\n",
      "epoch 1525, loss 0.8536624908447266, R2 0.6740344762802124\n",
      "Eval loss 0.8903251886367798, R2 0.6781980991363525\n",
      "epoch 1526, loss 0.8535845875740051, R2 0.6740672588348389\n",
      "Eval loss 0.8902320861816406, R2 0.6782350540161133\n",
      "epoch 1527, loss 0.8535069227218628, R2 0.6740999221801758\n",
      "Eval loss 0.890139102935791, R2 0.6782718300819397\n",
      "epoch 1528, loss 0.8534294366836548, R2 0.6741325855255127\n",
      "Eval loss 0.8900463581085205, R2 0.6783085465431213\n",
      "epoch 1529, loss 0.8533521294593811, R2 0.6741651296615601\n",
      "Eval loss 0.8899537324905396, R2 0.6783453226089478\n",
      "epoch 1530, loss 0.8532750010490417, R2 0.6741976141929626\n",
      "Eval loss 0.8898613452911377, R2 0.6783819198608398\n",
      "epoch 1531, loss 0.8531981110572815, R2 0.6742300987243652\n",
      "Eval loss 0.8897693753242493, R2 0.6784184575080872\n",
      "epoch 1532, loss 0.8531213402748108, R2 0.6742624044418335\n",
      "Eval loss 0.8896773457527161, R2 0.6784549355506897\n",
      "epoch 1533, loss 0.8530448079109192, R2 0.6742945909500122\n",
      "Eval loss 0.889585554599762, R2 0.6784912943840027\n",
      "epoch 1534, loss 0.8529684543609619, R2 0.6743268966674805\n",
      "Eval loss 0.8894941806793213, R2 0.6785275936126709\n",
      "epoch 1535, loss 0.852892279624939, R2 0.6743589639663696\n",
      "Eval loss 0.8894028663635254, R2 0.6785638332366943\n",
      "epoch 1536, loss 0.8528163433074951, R2 0.6743910908699036\n",
      "Eval loss 0.8893119692802429, R2 0.6785999536514282\n",
      "epoch 1537, loss 0.8527406454086304, R2 0.6744230389595032\n",
      "Eval loss 0.8892210125923157, R2 0.6786360144615173\n",
      "epoch 1538, loss 0.8526650667190552, R2 0.6744548678398132\n",
      "Eval loss 0.8891303539276123, R2 0.6786720156669617\n",
      "epoch 1539, loss 0.8525896668434143, R2 0.6744866967201233\n",
      "Eval loss 0.8890399932861328, R2 0.6787078976631165\n",
      "epoch 1540, loss 0.8525145053863525, R2 0.6745185256004333\n",
      "Eval loss 0.8889497518539429, R2 0.6787437796592712\n",
      "epoch 1541, loss 0.8524395227432251, R2 0.6745501756668091\n",
      "Eval loss 0.888859748840332, R2 0.6787795424461365\n",
      "epoch 1542, loss 0.8523646593093872, R2 0.6745817065238953\n",
      "Eval loss 0.8887698650360107, R2 0.6788151860237122\n",
      "epoch 1543, loss 0.8522899746894836, R2 0.6746132373809814\n",
      "Eval loss 0.8886803984642029, R2 0.6788507699966431\n",
      "epoch 1544, loss 0.852215588092804, R2 0.6746446490287781\n",
      "Eval loss 0.8885910511016846, R2 0.678886353969574\n",
      "epoch 1545, loss 0.8521413803100586, R2 0.6746761202812195\n",
      "Eval loss 0.8885018229484558, R2 0.6789217591285706\n",
      "epoch 1546, loss 0.852067232131958, R2 0.6747074723243713\n",
      "Eval loss 0.8884127736091614, R2 0.6789571642875671\n",
      "epoch 1547, loss 0.8519933819770813, R2 0.6747387051582336\n",
      "Eval loss 0.8883240222930908, R2 0.6789924502372742\n",
      "epoch 1548, loss 0.8519197106361389, R2 0.674769937992096\n",
      "Eval loss 0.8882354497909546, R2 0.6790277361869812\n",
      "epoch 1549, loss 0.8518462181091309, R2 0.6748009324073792\n",
      "Eval loss 0.8881471753120422, R2 0.6790628433227539\n",
      "epoch 1550, loss 0.8517729043960571, R2 0.6748319864273071\n",
      "Eval loss 0.8880590200424194, R2 0.6790979504585266\n",
      "epoch 1551, loss 0.8516996502876282, R2 0.6748629212379456\n",
      "Eval loss 0.8879711031913757, R2 0.679132878780365\n",
      "epoch 1552, loss 0.8516268134117126, R2 0.6748939156532288\n",
      "Eval loss 0.8878833651542664, R2 0.6791678667068481\n",
      "epoch 1553, loss 0.8515539765357971, R2 0.6749247312545776\n",
      "Eval loss 0.8877958059310913, R2 0.679202675819397\n",
      "epoch 1554, loss 0.8514813780784607, R2 0.674955427646637\n",
      "Eval loss 0.8877083659172058, R2 0.679237425327301\n",
      "epoch 1555, loss 0.8514089584350586, R2 0.6749861240386963\n",
      "Eval loss 0.8876213431358337, R2 0.6792722344398499\n",
      "epoch 1556, loss 0.851336658000946, R2 0.6750167012214661\n",
      "Eval loss 0.8875343203544617, R2 0.6793068647384644\n",
      "epoch 1557, loss 0.8512647151947021, R2 0.6750473380088806\n",
      "Eval loss 0.8874476552009583, R2 0.6793414354324341\n",
      "epoch 1558, loss 0.851192831993103, R2 0.6750777363777161\n",
      "Eval loss 0.8873611688613892, R2 0.6793758869171143\n",
      "epoch 1559, loss 0.851121187210083, R2 0.6751081943511963\n",
      "Eval loss 0.8872748613357544, R2 0.6794103384017944\n",
      "epoch 1560, loss 0.8510496020317078, R2 0.6751384735107422\n",
      "Eval loss 0.8871886730194092, R2 0.6794446706771851\n",
      "epoch 1561, loss 0.8509781956672668, R2 0.6751687526702881\n",
      "Eval loss 0.8871027231216431, R2 0.6794789433479309\n",
      "epoch 1562, loss 0.8509071469306946, R2 0.6751989126205444\n",
      "Eval loss 0.887017011642456, R2 0.6795130968093872\n",
      "epoch 1563, loss 0.8508360981941223, R2 0.6752290725708008\n",
      "Eval loss 0.8869314789772034, R2 0.6795473098754883\n",
      "epoch 1564, loss 0.8507653474807739, R2 0.6752591729164124\n",
      "Eval loss 0.886846125125885, R2 0.679581344127655\n",
      "epoch 1565, loss 0.8506946563720703, R2 0.6752891540527344\n",
      "Eval loss 0.8867610096931458, R2 0.6796151399612427\n",
      "epoch 1566, loss 0.8506242632865906, R2 0.6753190755844116\n",
      "Eval loss 0.8866760730743408, R2 0.6796492338180542\n",
      "epoch 1567, loss 0.8505539894104004, R2 0.6753488779067993\n",
      "Eval loss 0.886591374874115, R2 0.6796830296516418\n",
      "epoch 1568, loss 0.8504838347434998, R2 0.6753787994384766\n",
      "Eval loss 0.8865067362785339, R2 0.6797167658805847\n",
      "epoch 1569, loss 0.8504140377044678, R2 0.6754084825515747\n",
      "Eval loss 0.8864223957061768, R2 0.6797505021095276\n",
      "epoch 1570, loss 0.850344181060791, R2 0.6754381656646729\n",
      "Eval loss 0.8863382339477539, R2 0.6797840595245361\n",
      "epoch 1571, loss 0.8502746820449829, R2 0.6754676699638367\n",
      "Eval loss 0.8862542510032654, R2 0.6798176765441895\n",
      "epoch 1572, loss 0.8502053022384644, R2 0.6754972338676453\n",
      "Eval loss 0.886170506477356, R2 0.6798511147499084\n",
      "epoch 1573, loss 0.8501361012458801, R2 0.6755267381668091\n",
      "Eval loss 0.8860868215560913, R2 0.6798845529556274\n",
      "epoch 1574, loss 0.8500669598579407, R2 0.6755560636520386\n",
      "Eval loss 0.8860034346580505, R2 0.6799178719520569\n",
      "epoch 1575, loss 0.8499981164932251, R2 0.6755853891372681\n",
      "Eval loss 0.8859202265739441, R2 0.6799511313438416\n",
      "epoch 1576, loss 0.8499293327331543, R2 0.6756146550178528\n",
      "Eval loss 0.885837197303772, R2 0.6799843311309814\n",
      "epoch 1577, loss 0.8498609066009521, R2 0.6756438612937927\n",
      "Eval loss 0.8857543468475342, R2 0.6800175309181213\n",
      "epoch 1578, loss 0.8497925400733948, R2 0.6756729483604431\n",
      "Eval loss 0.8856717944145203, R2 0.6800506114959717\n",
      "epoch 1579, loss 0.8497241735458374, R2 0.6757020354270935\n",
      "Eval loss 0.8855891823768616, R2 0.6800835728645325\n",
      "epoch 1580, loss 0.8496562242507935, R2 0.6757310628890991\n",
      "Eval loss 0.8855069875717163, R2 0.6801164746284485\n",
      "epoch 1581, loss 0.8495883941650391, R2 0.6757599115371704\n",
      "Eval loss 0.8854249715805054, R2 0.6801493167877197\n",
      "epoch 1582, loss 0.8495206832885742, R2 0.6757888197898865\n",
      "Eval loss 0.8853430151939392, R2 0.6801820993423462\n",
      "epoch 1583, loss 0.8494531512260437, R2 0.6758175492286682\n",
      "Eval loss 0.8852614164352417, R2 0.6802148222923279\n",
      "epoch 1584, loss 0.8493857979774475, R2 0.67584627866745\n",
      "Eval loss 0.885179877281189, R2 0.68024742603302\n",
      "epoch 1585, loss 0.8493186831474304, R2 0.6758750081062317\n",
      "Eval loss 0.8850984573364258, R2 0.6802800297737122\n",
      "epoch 1586, loss 0.8492515683174133, R2 0.6759035587310791\n",
      "Eval loss 0.8850173354148865, R2 0.6803126335144043\n",
      "epoch 1587, loss 0.8491847515106201, R2 0.6759321689605713\n",
      "Eval loss 0.8849363923072815, R2 0.6803450584411621\n",
      "epoch 1588, loss 0.8491179943084717, R2 0.6759606003761292\n",
      "Eval loss 0.8848556280136108, R2 0.6803773641586304\n",
      "epoch 1589, loss 0.8490514159202576, R2 0.6759890913963318\n",
      "Eval loss 0.8847751021385193, R2 0.6804097294807434\n",
      "epoch 1590, loss 0.8489850759506226, R2 0.6760173439979553\n",
      "Eval loss 0.8846945762634277, R2 0.6804419755935669\n",
      "epoch 1591, loss 0.8489189743995667, R2 0.6760455965995789\n",
      "Eval loss 0.8846143484115601, R2 0.6804741024971008\n",
      "epoch 1592, loss 0.848852813243866, R2 0.6760738492012024\n",
      "Eval loss 0.8845343589782715, R2 0.68050616979599\n",
      "epoch 1593, loss 0.8487870693206787, R2 0.6761019825935364\n",
      "Eval loss 0.884454607963562, R2 0.6805382370948792\n",
      "epoch 1594, loss 0.8487212657928467, R2 0.6761300563812256\n",
      "Eval loss 0.8843747973442078, R2 0.6805701851844788\n",
      "epoch 1595, loss 0.8486557006835938, R2 0.6761581301689148\n",
      "Eval loss 0.8842953443527222, R2 0.6806021928787231\n",
      "epoch 1596, loss 0.8485903739929199, R2 0.6761861443519592\n",
      "Eval loss 0.8842160105705261, R2 0.6806339621543884\n",
      "epoch 1597, loss 0.8485250473022461, R2 0.6762139797210693\n",
      "Eval loss 0.8841369152069092, R2 0.6806657314300537\n",
      "epoch 1598, loss 0.8484600186347961, R2 0.6762418746948242\n",
      "Eval loss 0.8840579390525818, R2 0.6806974411010742\n",
      "epoch 1599, loss 0.8483951687812805, R2 0.6762695908546448\n",
      "Eval loss 0.883979082107544, R2 0.6807291507720947\n",
      "epoch 1600, loss 0.8483303785324097, R2 0.6762973666191101\n",
      "Eval loss 0.8839006423950195, R2 0.6807606816291809\n",
      "epoch 1601, loss 0.8482658267021179, R2 0.6763250827789307\n",
      "Eval loss 0.8838221430778503, R2 0.6807922124862671\n",
      "epoch 1602, loss 0.8482013940811157, R2 0.6763526201248169\n",
      "Eval loss 0.8837438225746155, R2 0.6808236241340637\n",
      "epoch 1603, loss 0.8481371998786926, R2 0.6763801574707031\n",
      "Eval loss 0.8836658596992493, R2 0.6808550357818604\n",
      "epoch 1604, loss 0.8480730652809143, R2 0.6764076352119446\n",
      "Eval loss 0.8835879564285278, R2 0.6808862686157227\n",
      "epoch 1605, loss 0.8480091094970703, R2 0.6764350533485413\n",
      "Eval loss 0.883510172367096, R2 0.6809175610542297\n",
      "epoch 1606, loss 0.8479453325271606, R2 0.6764624118804932\n",
      "Eval loss 0.8834325671195984, R2 0.6809487342834473\n",
      "epoch 1607, loss 0.8478816151618958, R2 0.6764896512031555\n",
      "Eval loss 0.8833553791046143, R2 0.6809799075126648\n",
      "epoch 1608, loss 0.84781813621521, R2 0.6765168905258179\n",
      "Eval loss 0.8832780718803406, R2 0.681010901927948\n",
      "epoch 1609, loss 0.8477548360824585, R2 0.6765440702438354\n",
      "Eval loss 0.8832011222839355, R2 0.6810418963432312\n",
      "epoch 1610, loss 0.8476915955543518, R2 0.6765711307525635\n",
      "Eval loss 0.8831242918968201, R2 0.6810728311538696\n",
      "epoch 1611, loss 0.847628653049469, R2 0.6765981912612915\n",
      "Eval loss 0.8830475807189941, R2 0.6811036467552185\n",
      "epoch 1612, loss 0.847565770149231, R2 0.6766251921653748\n",
      "Eval loss 0.8829711079597473, R2 0.6811344623565674\n",
      "epoch 1613, loss 0.8475030660629272, R2 0.6766520738601685\n",
      "Eval loss 0.8828948140144348, R2 0.6811652779579163\n",
      "epoch 1614, loss 0.8474404811859131, R2 0.6766789555549622\n",
      "Eval loss 0.8828186988830566, R2 0.6811959147453308\n",
      "epoch 1615, loss 0.8473781943321228, R2 0.6767057180404663\n",
      "Eval loss 0.882742702960968, R2 0.6812264323234558\n",
      "epoch 1616, loss 0.8473158478736877, R2 0.6767325401306152\n",
      "Eval loss 0.8826667666435242, R2 0.6812569499015808\n",
      "epoch 1617, loss 0.8472537994384766, R2 0.6767591834068298\n",
      "Eval loss 0.882591187953949, R2 0.6812874674797058\n",
      "epoch 1618, loss 0.8471918702125549, R2 0.6767858266830444\n",
      "Eval loss 0.8825157880783081, R2 0.6813178658485413\n",
      "epoch 1619, loss 0.8471300601959229, R2 0.6768124103546143\n",
      "Eval loss 0.8824403882026672, R2 0.6813482046127319\n",
      "epoch 1620, loss 0.8470684289932251, R2 0.6768388748168945\n",
      "Eval loss 0.8823652863502502, R2 0.6813784837722778\n",
      "epoch 1621, loss 0.8470069169998169, R2 0.67686527967453\n",
      "Eval loss 0.8822903633117676, R2 0.6814087629318237\n",
      "epoch 1622, loss 0.8469454646110535, R2 0.6768916845321655\n",
      "Eval loss 0.8822155594825745, R2 0.6814388632774353\n",
      "epoch 1623, loss 0.8468844294548035, R2 0.6769180297851562\n",
      "Eval loss 0.8821408748626709, R2 0.6814689636230469\n",
      "epoch 1624, loss 0.8468233346939087, R2 0.6769443154335022\n",
      "Eval loss 0.8820664882659912, R2 0.6814990043640137\n",
      "epoch 1625, loss 0.8467623591423035, R2 0.6769704818725586\n",
      "Eval loss 0.8819921612739563, R2 0.6815290451049805\n",
      "epoch 1626, loss 0.8467016816139221, R2 0.676996648311615\n",
      "Eval loss 0.8819180727005005, R2 0.6815589666366577\n",
      "epoch 1627, loss 0.8466410636901855, R2 0.6770227551460266\n",
      "Eval loss 0.8818441033363342, R2 0.6815887093544006\n",
      "epoch 1628, loss 0.8465806841850281, R2 0.6770487427711487\n",
      "Eval loss 0.8817703723907471, R2 0.6816185712814331\n",
      "epoch 1629, loss 0.8465203642845154, R2 0.6770747900009155\n",
      "Eval loss 0.8816967010498047, R2 0.681648313999176\n",
      "epoch 1630, loss 0.846460223197937, R2 0.6771007180213928\n",
      "Eval loss 0.8816232085227966, R2 0.6816779971122742\n",
      "epoch 1631, loss 0.846400260925293, R2 0.6771265268325806\n",
      "Eval loss 0.8815498948097229, R2 0.6817075610160828\n",
      "epoch 1632, loss 0.8463402986526489, R2 0.6771523952484131\n",
      "Eval loss 0.8814768195152283, R2 0.6817371249198914\n",
      "epoch 1633, loss 0.8462806344032288, R2 0.6771780252456665\n",
      "Eval loss 0.8814038634300232, R2 0.6817665696144104\n",
      "epoch 1634, loss 0.8462210893630981, R2 0.6772037744522095\n",
      "Eval loss 0.8813309669494629, R2 0.6817960143089294\n",
      "epoch 1635, loss 0.8461616039276123, R2 0.6772294640541077\n",
      "Eval loss 0.8812583684921265, R2 0.6818253993988037\n",
      "epoch 1636, loss 0.8461024761199951, R2 0.6772549748420715\n",
      "Eval loss 0.8811858892440796, R2 0.6818546652793884\n",
      "epoch 1637, loss 0.8460432291030884, R2 0.6772804856300354\n",
      "Eval loss 0.8811135292053223, R2 0.6818839311599731\n",
      "epoch 1638, loss 0.8459842801094055, R2 0.6773059964179993\n",
      "Eval loss 0.881041407585144, R2 0.6819131374359131\n",
      "epoch 1639, loss 0.845925509929657, R2 0.6773313283920288\n",
      "Eval loss 0.8809694051742554, R2 0.6819422245025635\n",
      "epoch 1640, loss 0.8458667397499084, R2 0.6773567199707031\n",
      "Eval loss 0.8808976411819458, R2 0.6819713115692139\n",
      "epoch 1641, loss 0.8458081483840942, R2 0.6773820519447327\n",
      "Eval loss 0.8808258771896362, R2 0.6820003390312195\n",
      "epoch 1642, loss 0.8457497358322144, R2 0.6774072647094727\n",
      "Eval loss 0.8807543516159058, R2 0.6820292472839355\n",
      "epoch 1643, loss 0.8456915020942688, R2 0.6774324178695679\n",
      "Eval loss 0.8806829452514648, R2 0.6820581555366516\n",
      "epoch 1644, loss 0.8456332683563232, R2 0.6774575710296631\n",
      "Eval loss 0.8806117177009583, R2 0.6820869445800781\n",
      "epoch 1645, loss 0.8455753922462463, R2 0.6774826645851135\n",
      "Eval loss 0.880540668964386, R2 0.6821157336235046\n",
      "epoch 1646, loss 0.8455174565315247, R2 0.6775076389312744\n",
      "Eval loss 0.8804698586463928, R2 0.6821444034576416\n",
      "epoch 1647, loss 0.8454597592353821, R2 0.6775326132774353\n",
      "Eval loss 0.8803991675376892, R2 0.6821730136871338\n",
      "epoch 1648, loss 0.8454022407531738, R2 0.6775574684143066\n",
      "Eval loss 0.8803284764289856, R2 0.682201623916626\n",
      "epoch 1649, loss 0.8453448414802551, R2 0.6775823831558228\n",
      "Eval loss 0.8802582025527954, R2 0.6822302341461182\n",
      "epoch 1650, loss 0.8452874422073364, R2 0.6776071786880493\n",
      "Eval loss 0.8801878690719604, R2 0.682258665561676\n",
      "epoch 1651, loss 0.8452302813529968, R2 0.6776319146156311\n",
      "Eval loss 0.8801178336143494, R2 0.6822870373725891\n",
      "epoch 1652, loss 0.8451732993125916, R2 0.6776565909385681\n",
      "Eval loss 0.8800478577613831, R2 0.6823154091835022\n",
      "epoch 1653, loss 0.845116376876831, R2 0.6776812076568604\n",
      "Eval loss 0.8799780011177063, R2 0.6823437213897705\n",
      "epoch 1654, loss 0.8450597524642944, R2 0.6777058243751526\n",
      "Eval loss 0.8799084424972534, R2 0.6823719143867493\n",
      "epoch 1655, loss 0.845003068447113, R2 0.6777303218841553\n",
      "Eval loss 0.8798389434814453, R2 0.6824001669883728\n",
      "epoch 1656, loss 0.844946563243866, R2 0.6777548789978027\n",
      "Eval loss 0.8797696232795715, R2 0.6824283003807068\n",
      "epoch 1657, loss 0.8448902368545532, R2 0.6777791976928711\n",
      "Eval loss 0.8797004222869873, R2 0.682456374168396\n",
      "epoch 1658, loss 0.8448341488838196, R2 0.6778035163879395\n",
      "Eval loss 0.8796313405036926, R2 0.6824843287467957\n",
      "epoch 1659, loss 0.8447780013084412, R2 0.6778278946876526\n",
      "Eval loss 0.879562497138977, R2 0.6825123429298401\n",
      "epoch 1660, loss 0.8447220921516418, R2 0.677852213382721\n",
      "Eval loss 0.8794938325881958, R2 0.6825401782989502\n",
      "epoch 1661, loss 0.8446663618087769, R2 0.677876353263855\n",
      "Eval loss 0.8794252276420593, R2 0.6825680732727051\n",
      "epoch 1662, loss 0.8446106314659119, R2 0.6779004335403442\n",
      "Eval loss 0.879356861114502, R2 0.6825958490371704\n",
      "epoch 1663, loss 0.8445552587509155, R2 0.677924633026123\n",
      "Eval loss 0.8792884945869446, R2 0.682623565196991\n",
      "epoch 1664, loss 0.8444996476173401, R2 0.6779486536979675\n",
      "Eval loss 0.8792204856872559, R2 0.6826512813568115\n",
      "epoch 1665, loss 0.8444445133209229, R2 0.6779726147651672\n",
      "Eval loss 0.8791524171829224, R2 0.6826788187026978\n",
      "epoch 1666, loss 0.8443893790245056, R2 0.6779965162277222\n",
      "Eval loss 0.8790846467018127, R2 0.6827064156532288\n",
      "epoch 1667, loss 0.8443344831466675, R2 0.6780204772949219\n",
      "Eval loss 0.8790170550346375, R2 0.682733952999115\n",
      "epoch 1668, loss 0.8442795276641846, R2 0.678044319152832\n",
      "Eval loss 0.8789494633674622, R2 0.6827613711357117\n",
      "epoch 1669, loss 0.844224750995636, R2 0.6780680418014526\n",
      "Eval loss 0.8788821697235107, R2 0.6827887892723083\n",
      "epoch 1670, loss 0.8441702127456665, R2 0.678091824054718\n",
      "Eval loss 0.8788148760795593, R2 0.6828160285949707\n",
      "epoch 1671, loss 0.8441157341003418, R2 0.6781154274940491\n",
      "Eval loss 0.878747820854187, R2 0.6828433871269226\n",
      "epoch 1672, loss 0.8440614938735962, R2 0.6781390905380249\n",
      "Eval loss 0.8786808848381042, R2 0.6828705668449402\n",
      "epoch 1673, loss 0.8440072536468506, R2 0.678162693977356\n",
      "Eval loss 0.8786141276359558, R2 0.6828978061676025\n",
      "epoch 1674, loss 0.8439531326293945, R2 0.6781861782073975\n",
      "Eval loss 0.8785474896430969, R2 0.6829248666763306\n",
      "epoch 1675, loss 0.8438992500305176, R2 0.678209662437439\n",
      "Eval loss 0.8784809112548828, R2 0.6829519271850586\n",
      "epoch 1676, loss 0.8438453674316406, R2 0.6782330274581909\n",
      "Eval loss 0.8784146308898926, R2 0.6829789876937866\n",
      "epoch 1677, loss 0.8437917828559875, R2 0.6782564520835876\n",
      "Eval loss 0.8783484697341919, R2 0.6830059289932251\n",
      "epoch 1678, loss 0.8437381982803345, R2 0.6782797574996948\n",
      "Eval loss 0.8782824277877808, R2 0.683032751083374\n",
      "epoch 1679, loss 0.8436847925186157, R2 0.6783030033111572\n",
      "Eval loss 0.878216564655304, R2 0.6830596327781677\n",
      "epoch 1680, loss 0.8436315655708313, R2 0.6783262491226196\n",
      "Eval loss 0.8781508207321167, R2 0.6830863952636719\n",
      "epoch 1681, loss 0.8435782790184021, R2 0.6783494353294373\n",
      "Eval loss 0.878085196018219, R2 0.6831130981445312\n",
      "epoch 1682, loss 0.843525230884552, R2 0.6783725023269653\n",
      "Eval loss 0.8780196905136108, R2 0.6831398606300354\n",
      "epoch 1683, loss 0.8434723615646362, R2 0.6783955097198486\n",
      "Eval loss 0.8779544234275818, R2 0.6831664443016052\n",
      "epoch 1684, loss 0.84341961145401, R2 0.6784185171127319\n",
      "Eval loss 0.8778891563415527, R2 0.683193027973175\n",
      "epoch 1685, loss 0.8433669209480286, R2 0.6784414052963257\n",
      "Eval loss 0.8778241276741028, R2 0.6832194924354553\n",
      "epoch 1686, loss 0.8433144092559814, R2 0.6784644722938538\n",
      "Eval loss 0.8777591586112976, R2 0.6832460165023804\n",
      "epoch 1687, loss 0.8432620167732239, R2 0.6784873604774475\n",
      "Eval loss 0.8776944279670715, R2 0.6832723617553711\n",
      "epoch 1688, loss 0.8432096838951111, R2 0.6785101294517517\n",
      "Eval loss 0.8776298761367798, R2 0.6832987666130066\n",
      "epoch 1689, loss 0.8431575298309326, R2 0.6785328388214111\n",
      "Eval loss 0.8775653839111328, R2 0.6833250522613525\n",
      "epoch 1690, loss 0.8431055545806885, R2 0.6785555481910706\n",
      "Eval loss 0.8775010704994202, R2 0.6833512783050537\n",
      "epoch 1691, loss 0.8430535793304443, R2 0.6785781979560852\n",
      "Eval loss 0.8774369359016418, R2 0.6833774447441101\n",
      "epoch 1692, loss 0.8430018424987793, R2 0.6786008477210999\n",
      "Eval loss 0.8773728013038635, R2 0.6834036707878113\n",
      "epoch 1693, loss 0.8429502248764038, R2 0.678623378276825\n",
      "Eval loss 0.8773088455200195, R2 0.6834296584129333\n",
      "epoch 1694, loss 0.8428986072540283, R2 0.67864590883255\n",
      "Eval loss 0.8772450685501099, R2 0.683455765247345\n",
      "epoch 1695, loss 0.8428472280502319, R2 0.6786683797836304\n",
      "Eval loss 0.8771815299987793, R2 0.6834818124771118\n",
      "epoch 1696, loss 0.8427959680557251, R2 0.6786907911300659\n",
      "Eval loss 0.8771179914474487, R2 0.6835076808929443\n",
      "epoch 1697, loss 0.8427447080612183, R2 0.6787131428718567\n",
      "Eval loss 0.8770546913146973, R2 0.6835335493087769\n",
      "epoch 1698, loss 0.8426937460899353, R2 0.6787354946136475\n",
      "Eval loss 0.8769914507865906, R2 0.6835594177246094\n",
      "epoch 1699, loss 0.8426427841186523, R2 0.6787577867507935\n",
      "Eval loss 0.8769282102584839, R2 0.6835851669311523\n",
      "epoch 1700, loss 0.8425919413566589, R2 0.6787799596786499\n",
      "Eval loss 0.8768653869628906, R2 0.6836109161376953\n",
      "epoch 1701, loss 0.8425412178039551, R2 0.6788021326065063\n",
      "Eval loss 0.8768025040626526, R2 0.6836365461349487\n",
      "epoch 1702, loss 0.8424907326698303, R2 0.678824245929718\n",
      "Eval loss 0.8767397999763489, R2 0.6836622357368469\n",
      "epoch 1703, loss 0.8424403071403503, R2 0.6788462996482849\n",
      "Eval loss 0.8766773343086243, R2 0.6836877465248108\n",
      "epoch 1704, loss 0.8423898816108704, R2 0.678868293762207\n",
      "Eval loss 0.8766149878501892, R2 0.6837133169174194\n",
      "epoch 1705, loss 0.8423397541046143, R2 0.6788902878761292\n",
      "Eval loss 0.8765525817871094, R2 0.6837387681007385\n",
      "epoch 1706, loss 0.8422896862030029, R2 0.6789122223854065\n",
      "Eval loss 0.8764905333518982, R2 0.6837641596794128\n",
      "epoch 1707, loss 0.8422396183013916, R2 0.6789340972900391\n",
      "Eval loss 0.876428484916687, R2 0.6837896108627319\n",
      "epoch 1708, loss 0.8421898484230042, R2 0.6789559721946716\n",
      "Eval loss 0.8763666152954102, R2 0.6838148236274719\n",
      "epoch 1709, loss 0.8421400785446167, R2 0.6789777278900146\n",
      "Eval loss 0.8763049840927124, R2 0.6838400959968567\n",
      "epoch 1710, loss 0.8420904874801636, R2 0.6789994835853577\n",
      "Eval loss 0.8762433528900146, R2 0.6838653683662415\n",
      "epoch 1711, loss 0.8420410752296448, R2 0.6790212392807007\n",
      "Eval loss 0.8761818408966064, R2 0.6838905215263367\n",
      "epoch 1712, loss 0.841991662979126, R2 0.6790428161621094\n",
      "Eval loss 0.8761204481124878, R2 0.6839156746864319\n",
      "epoch 1713, loss 0.8419424295425415, R2 0.6790643930435181\n",
      "Eval loss 0.8760592937469482, R2 0.6839407086372375\n",
      "epoch 1714, loss 0.8418932557106018, R2 0.6790859699249268\n",
      "Eval loss 0.8759981989860535, R2 0.6839656829833984\n",
      "epoch 1715, loss 0.8418442010879517, R2 0.6791075468063354\n",
      "Eval loss 0.8759374022483826, R2 0.6839907169342041\n",
      "epoch 1716, loss 0.8417954444885254, R2 0.6791290044784546\n",
      "Eval loss 0.8758764266967773, R2 0.6840155720710754\n",
      "epoch 1717, loss 0.8417465686798096, R2 0.679150402545929\n",
      "Eval loss 0.8758158683776855, R2 0.6840404272079468\n",
      "epoch 1718, loss 0.8416978716850281, R2 0.6791717410087585\n",
      "Eval loss 0.8757553696632385, R2 0.6840652823448181\n",
      "epoch 1719, loss 0.8416492342948914, R2 0.6791930794715881\n",
      "Eval loss 0.8756949305534363, R2 0.6840900778770447\n",
      "epoch 1720, loss 0.8416008353233337, R2 0.6792144179344177\n",
      "Eval loss 0.8756346106529236, R2 0.6841147541999817\n",
      "epoch 1721, loss 0.8415524959564209, R2 0.679235577583313\n",
      "Eval loss 0.8755744695663452, R2 0.6841394305229187\n",
      "epoch 1722, loss 0.8415043354034424, R2 0.679256796836853\n",
      "Eval loss 0.8755144476890564, R2 0.6841641068458557\n",
      "epoch 1723, loss 0.8414561152458191, R2 0.6792778968811035\n",
      "Eval loss 0.8754546046257019, R2 0.6841886639595032\n",
      "epoch 1724, loss 0.8414080739021301, R2 0.679298996925354\n",
      "Eval loss 0.8753948211669922, R2 0.6842131018638611\n",
      "epoch 1725, loss 0.8413602709770203, R2 0.6793200373649597\n",
      "Eval loss 0.8753350973129272, R2 0.6842376589775085\n",
      "epoch 1726, loss 0.8413125276565552, R2 0.6793410181999207\n",
      "Eval loss 0.8752756118774414, R2 0.6842620968818665\n",
      "epoch 1727, loss 0.8412649035453796, R2 0.6793619990348816\n",
      "Eval loss 0.8752163052558899, R2 0.6842864751815796\n",
      "epoch 1728, loss 0.8412172794342041, R2 0.6793829202651978\n",
      "Eval loss 0.8751569986343384, R2 0.684310793876648\n",
      "epoch 1729, loss 0.8411698937416077, R2 0.6794037818908691\n",
      "Eval loss 0.875097930431366, R2 0.6843349933624268\n",
      "epoch 1730, loss 0.8411226272583008, R2 0.6794245839118958\n",
      "Eval loss 0.8750388622283936, R2 0.6843592524528503\n",
      "epoch 1731, loss 0.8410754203796387, R2 0.6794453263282776\n",
      "Eval loss 0.8749800324440002, R2 0.6843833923339844\n",
      "epoch 1732, loss 0.8410283327102661, R2 0.6794661283493042\n",
      "Eval loss 0.8749212622642517, R2 0.6844075918197632\n",
      "epoch 1733, loss 0.8409813046455383, R2 0.6794868111610413\n",
      "Eval loss 0.8748626708984375, R2 0.6844315528869629\n",
      "epoch 1734, loss 0.8409343957901001, R2 0.6795074343681335\n",
      "Eval loss 0.8748041391372681, R2 0.6844556927680969\n",
      "epoch 1735, loss 0.840887725353241, R2 0.6795280575752258\n",
      "Eval loss 0.8747457265853882, R2 0.6844797730445862\n",
      "epoch 1736, loss 0.8408409953117371, R2 0.6795485615730286\n",
      "Eval loss 0.8746874928474426, R2 0.6845036149024963\n",
      "epoch 1737, loss 0.8407945036888123, R2 0.6795690655708313\n",
      "Eval loss 0.8746293783187866, R2 0.6845275163650513\n",
      "epoch 1738, loss 0.8407480120658875, R2 0.6795895099639893\n",
      "Eval loss 0.8745713829994202, R2 0.6845513582229614\n",
      "epoch 1739, loss 0.8407018780708313, R2 0.6796099543571472\n",
      "Eval loss 0.8745136260986328, R2 0.6845752000808716\n",
      "epoch 1740, loss 0.840655505657196, R2 0.6796303391456604\n",
      "Eval loss 0.8744558691978455, R2 0.684598982334137\n",
      "epoch 1741, loss 0.8406093716621399, R2 0.6796506643295288\n",
      "Eval loss 0.8743982315063477, R2 0.6846226453781128\n",
      "epoch 1742, loss 0.8405634164810181, R2 0.6796709895133972\n",
      "Eval loss 0.8743406534194946, R2 0.6846462488174438\n",
      "epoch 1743, loss 0.840517520904541, R2 0.6796912550926208\n",
      "Eval loss 0.8742832541465759, R2 0.6846698522567749\n",
      "epoch 1744, loss 0.8404716849327087, R2 0.6797114014625549\n",
      "Eval loss 0.8742260336875916, R2 0.684693455696106\n",
      "epoch 1745, loss 0.8404260277748108, R2 0.679731547832489\n",
      "Eval loss 0.874168872833252, R2 0.6847169995307922\n",
      "epoch 1746, loss 0.8403804302215576, R2 0.6797517538070679\n",
      "Eval loss 0.8741118311882019, R2 0.6847404837608337\n",
      "epoch 1747, loss 0.8403350114822388, R2 0.6797717809677124\n",
      "Eval loss 0.874055027961731, R2 0.6847638487815857\n",
      "epoch 1748, loss 0.8402897119522095, R2 0.6797918081283569\n",
      "Eval loss 0.8739981651306152, R2 0.6847872138023376\n",
      "epoch 1749, loss 0.840244472026825, R2 0.6798117756843567\n",
      "Eval loss 0.8739416599273682, R2 0.6848106384277344\n",
      "epoch 1750, loss 0.8401992917060852, R2 0.6798316836357117\n",
      "Eval loss 0.8738849759101868, R2 0.6848339438438416\n",
      "epoch 1751, loss 0.840154230594635, R2 0.6798515915870667\n",
      "Eval loss 0.873828649520874, R2 0.6848571300506592\n",
      "epoch 1752, loss 0.8401093482971191, R2 0.6798714399337769\n",
      "Eval loss 0.8737723231315613, R2 0.6848803758621216\n",
      "epoch 1753, loss 0.840064525604248, R2 0.6798913478851318\n",
      "Eval loss 0.8737162351608276, R2 0.6849035024642944\n",
      "epoch 1754, loss 0.840019702911377, R2 0.6799110174179077\n",
      "Eval loss 0.873660147190094, R2 0.6849266290664673\n",
      "epoch 1755, loss 0.839975118637085, R2 0.6799307465553284\n",
      "Eval loss 0.8736041188240051, R2 0.6849496960639954\n",
      "epoch 1756, loss 0.8399305939674377, R2 0.6799505352973938\n",
      "Eval loss 0.8735484480857849, R2 0.6849727034568787\n",
      "epoch 1757, loss 0.8398861885070801, R2 0.6799702048301697\n",
      "Eval loss 0.8734927177429199, R2 0.6849956512451172\n",
      "epoch 1758, loss 0.8398419618606567, R2 0.679989755153656\n",
      "Eval loss 0.8734371662139893, R2 0.6850184798240662\n",
      "epoch 1759, loss 0.8397976756095886, R2 0.6800092458724976\n",
      "Eval loss 0.8733815550804138, R2 0.6850414872169495\n",
      "epoch 1760, loss 0.8397536277770996, R2 0.6800287961959839\n",
      "Eval loss 0.873326301574707, R2 0.6850642561912537\n",
      "epoch 1761, loss 0.8397095799446106, R2 0.6800482273101807\n",
      "Eval loss 0.8732710480690002, R2 0.6850870251655579\n",
      "epoch 1762, loss 0.8396657109260559, R2 0.6800677180290222\n",
      "Eval loss 0.8732160329818726, R2 0.6851098537445068\n",
      "epoch 1763, loss 0.839621901512146, R2 0.6800870895385742\n",
      "Eval loss 0.8731609582901001, R2 0.6851325631141663\n",
      "epoch 1764, loss 0.8395781517028809, R2 0.680106520652771\n",
      "Eval loss 0.8731061220169067, R2 0.6851551532745361\n",
      "epoch 1765, loss 0.8395346403121948, R2 0.6801258325576782\n",
      "Eval loss 0.8730514049530029, R2 0.6851778030395508\n",
      "epoch 1766, loss 0.839491069316864, R2 0.6801450252532959\n",
      "Eval loss 0.8729966282844543, R2 0.6852003335952759\n",
      "epoch 1767, loss 0.8394477367401123, R2 0.6801642775535583\n",
      "Eval loss 0.8729422092437744, R2 0.685222864151001\n",
      "epoch 1768, loss 0.8394044041633606, R2 0.680183470249176\n",
      "Eval loss 0.8728877902030945, R2 0.6852453947067261\n",
      "epoch 1769, loss 0.8393613696098328, R2 0.6802026033401489\n",
      "Eval loss 0.8728334903717041, R2 0.6852678060531616\n",
      "epoch 1770, loss 0.8393182158470154, R2 0.6802217364311218\n",
      "Eval loss 0.8727792501449585, R2 0.6852902173995972\n",
      "epoch 1771, loss 0.8392752408981323, R2 0.68024080991745\n",
      "Eval loss 0.8727253079414368, R2 0.6853125095367432\n",
      "epoch 1772, loss 0.8392323851585388, R2 0.6802597641944885\n",
      "Eval loss 0.8726713061332703, R2 0.6853348612785339\n",
      "epoch 1773, loss 0.8391895890235901, R2 0.6802787780761719\n",
      "Eval loss 0.8726173639297485, R2 0.6853570938110352\n",
      "epoch 1774, loss 0.8391469120979309, R2 0.6802977919578552\n",
      "Eval loss 0.8725637793540955, R2 0.6853792667388916\n",
      "epoch 1775, loss 0.8391042947769165, R2 0.6803166270256042\n",
      "Eval loss 0.8725100755691528, R2 0.6854014992713928\n",
      "epoch 1776, loss 0.8390617966651917, R2 0.6803355813026428\n",
      "Eval loss 0.8724565505981445, R2 0.6854236721992493\n",
      "epoch 1777, loss 0.8390194177627563, R2 0.6803543567657471\n",
      "Eval loss 0.8724031448364258, R2 0.6854457259178162\n",
      "epoch 1778, loss 0.8389770984649658, R2 0.6803731918334961\n",
      "Eval loss 0.8723499178886414, R2 0.6854677200317383\n",
      "epoch 1779, loss 0.8389349579811096, R2 0.6803919076919556\n",
      "Eval loss 0.8722966909408569, R2 0.6854897141456604\n",
      "epoch 1780, loss 0.8388928174972534, R2 0.6804105043411255\n",
      "Eval loss 0.8722436428070068, R2 0.6855117678642273\n",
      "epoch 1781, loss 0.8388508558273315, R2 0.680429220199585\n",
      "Eval loss 0.8721907734870911, R2 0.6855335831642151\n",
      "epoch 1782, loss 0.8388089537620544, R2 0.6804478764533997\n",
      "Eval loss 0.8721379041671753, R2 0.6855555772781372\n",
      "epoch 1783, loss 0.8387671709060669, R2 0.6804664134979248\n",
      "Eval loss 0.8720852136611938, R2 0.685577392578125\n",
      "epoch 1784, loss 0.8387254476547241, R2 0.6804850101470947\n",
      "Eval loss 0.872032642364502, R2 0.6855990886688232\n",
      "epoch 1785, loss 0.8386838436126709, R2 0.6805035471916199\n",
      "Eval loss 0.8719801306724548, R2 0.6856208443641663\n",
      "epoch 1786, loss 0.8386422395706177, R2 0.6805220246315002\n",
      "Eval loss 0.8719276785850525, R2 0.6856425404548645\n",
      "epoch 1787, loss 0.8386008143424988, R2 0.6805403828620911\n",
      "Eval loss 0.8718754649162292, R2 0.6856642365455627\n",
      "epoch 1788, loss 0.8385595679283142, R2 0.6805588006973267\n",
      "Eval loss 0.8718231916427612, R2 0.6856858730316162\n",
      "epoch 1789, loss 0.8385183215141296, R2 0.6805771589279175\n",
      "Eval loss 0.8717712759971619, R2 0.6857073307037354\n",
      "epoch 1790, loss 0.8384771943092346, R2 0.6805954575538635\n",
      "Eval loss 0.8717193007469177, R2 0.6857289671897888\n",
      "epoch 1791, loss 0.8384362459182739, R2 0.6806138157844543\n",
      "Eval loss 0.8716673254966736, R2 0.6857504844665527\n",
      "epoch 1792, loss 0.8383951783180237, R2 0.6806319355964661\n",
      "Eval loss 0.8716157078742981, R2 0.6857718825340271\n",
      "epoch 1793, loss 0.8383542895317078, R2 0.6806502342224121\n",
      "Eval loss 0.8715641498565674, R2 0.6857933402061462\n",
      "epoch 1794, loss 0.8383135795593262, R2 0.680668294429779\n",
      "Eval loss 0.8715125322341919, R2 0.6858146786689758\n",
      "epoch 1795, loss 0.8382729887962341, R2 0.6806864142417908\n",
      "Eval loss 0.8714610934257507, R2 0.6858360171318054\n",
      "epoch 1796, loss 0.8382323980331421, R2 0.6807045340538025\n",
      "Eval loss 0.8714098930358887, R2 0.6858572363853455\n",
      "epoch 1797, loss 0.8381918668746948, R2 0.6807225942611694\n",
      "Eval loss 0.8713586330413818, R2 0.6858785152435303\n",
      "epoch 1798, loss 0.8381515145301819, R2 0.6807405352592468\n",
      "Eval loss 0.8713076114654541, R2 0.6858997344970703\n",
      "epoch 1799, loss 0.838111162185669, R2 0.680758535861969\n",
      "Eval loss 0.8712565302848816, R2 0.6859208941459656\n",
      "epoch 1800, loss 0.8380710482597351, R2 0.6807764172554016\n",
      "Eval loss 0.8712056875228882, R2 0.6859419941902161\n",
      "epoch 1801, loss 0.8380310535430908, R2 0.680794358253479\n",
      "Eval loss 0.8711549639701843, R2 0.6859630942344666\n",
      "epoch 1802, loss 0.837990939617157, R2 0.6808121204376221\n",
      "Eval loss 0.8711043000221252, R2 0.6859841346740723\n",
      "epoch 1803, loss 0.8379510641098022, R2 0.6808299422264099\n",
      "Eval loss 0.8710535764694214, R2 0.6860051155090332\n",
      "epoch 1804, loss 0.8379111289978027, R2 0.6808477640151978\n",
      "Eval loss 0.871003270149231, R2 0.6860261559486389\n",
      "epoch 1805, loss 0.8378714323043823, R2 0.6808655261993408\n",
      "Eval loss 0.870952844619751, R2 0.6860470175743103\n",
      "epoch 1806, loss 0.8378317356109619, R2 0.6808832287788391\n",
      "Eval loss 0.8709025979042053, R2 0.6860678791999817\n",
      "epoch 1807, loss 0.8377923369407654, R2 0.6809009313583374\n",
      "Eval loss 0.8708524703979492, R2 0.6860887408256531\n",
      "epoch 1808, loss 0.8377528190612793, R2 0.6809185147285461\n",
      "Eval loss 0.8708025217056274, R2 0.6861094832420349\n",
      "epoch 1809, loss 0.8377134799957275, R2 0.6809360384941101\n",
      "Eval loss 0.8707525730133057, R2 0.6861302852630615\n",
      "epoch 1810, loss 0.8376742601394653, R2 0.6809536218643188\n",
      "Eval loss 0.8707026839256287, R2 0.6861510276794434\n",
      "epoch 1811, loss 0.8376349806785583, R2 0.6809712052345276\n",
      "Eval loss 0.870652973651886, R2 0.6861717104911804\n",
      "epoch 1812, loss 0.8375958204269409, R2 0.6809886693954468\n",
      "Eval loss 0.8706033825874329, R2 0.6861923336982727\n",
      "epoch 1813, loss 0.8375568389892578, R2 0.681006133556366\n",
      "Eval loss 0.8705537915229797, R2 0.686212956905365\n",
      "epoch 1814, loss 0.8375179767608643, R2 0.6810234189033508\n",
      "Eval loss 0.8705043792724609, R2 0.6862334609031677\n",
      "epoch 1815, loss 0.8374791145324707, R2 0.6810408234596252\n",
      "Eval loss 0.8704550266265869, R2 0.6862540245056152\n",
      "epoch 1816, loss 0.8374404311180115, R2 0.6810582876205444\n",
      "Eval loss 0.870405912399292, R2 0.686274528503418\n",
      "epoch 1817, loss 0.8374016880989075, R2 0.6810754537582397\n",
      "Eval loss 0.8703566789627075, R2 0.6862949132919312\n",
      "epoch 1818, loss 0.8373631834983826, R2 0.6810927987098694\n",
      "Eval loss 0.8703076839447021, R2 0.6863152980804443\n",
      "epoch 1819, loss 0.8373247385025024, R2 0.6811100244522095\n",
      "Eval loss 0.8702588081359863, R2 0.6863356232643127\n",
      "epoch 1820, loss 0.8372863531112671, R2 0.68112713098526\n",
      "Eval loss 0.8702100515365601, R2 0.6863559484481812\n",
      "epoch 1821, loss 0.8372480273246765, R2 0.6811442971229553\n",
      "Eval loss 0.8701613545417786, R2 0.6863762736320496\n",
      "epoch 1822, loss 0.8372098207473755, R2 0.6811614632606506\n",
      "Eval loss 0.8701127171516418, R2 0.6863964796066284\n",
      "epoch 1823, loss 0.8371716737747192, R2 0.6811785101890564\n",
      "Eval loss 0.8700641393661499, R2 0.6864166855812073\n",
      "epoch 1824, loss 0.8371336460113525, R2 0.6811956167221069\n",
      "Eval loss 0.8700157999992371, R2 0.6864369511604309\n",
      "epoch 1825, loss 0.8370956778526306, R2 0.6812124848365784\n",
      "Eval loss 0.8699674606323242, R2 0.6864569783210754\n",
      "epoch 1826, loss 0.8370577692985535, R2 0.6812295317649841\n",
      "Eval loss 0.8699193596839905, R2 0.6864771246910095\n",
      "epoch 1827, loss 0.8370199799537659, R2 0.6812464594841003\n",
      "Eval loss 0.869871199131012, R2 0.686497151851654\n",
      "epoch 1828, loss 0.8369823694229126, R2 0.6812633872032166\n",
      "Eval loss 0.869823157787323, R2 0.6865171194076538\n",
      "epoch 1829, loss 0.8369447588920593, R2 0.6812801957130432\n",
      "Eval loss 0.8697751760482788, R2 0.6865371465682983\n",
      "epoch 1830, loss 0.836907148361206, R2 0.6812970042228699\n",
      "Eval loss 0.8697274923324585, R2 0.6865571141242981\n",
      "epoch 1831, loss 0.8368697166442871, R2 0.6813138723373413\n",
      "Eval loss 0.8696796894073486, R2 0.6865769624710083\n",
      "epoch 1832, loss 0.8368324041366577, R2 0.681330680847168\n",
      "Eval loss 0.8696320652961731, R2 0.6865968704223633\n",
      "epoch 1833, loss 0.8367952108383179, R2 0.6813473105430603\n",
      "Eval loss 0.8695845603942871, R2 0.6866166591644287\n",
      "epoch 1834, loss 0.836758017539978, R2 0.6813639998435974\n",
      "Eval loss 0.8695370554924011, R2 0.6866363883018494\n",
      "epoch 1835, loss 0.8367209434509277, R2 0.6813806891441345\n",
      "Eval loss 0.869489848613739, R2 0.6866561770439148\n",
      "epoch 1836, loss 0.8366837501525879, R2 0.6813972592353821\n",
      "Eval loss 0.8694425821304321, R2 0.6866758465766907\n",
      "epoch 1837, loss 0.8366469740867615, R2 0.6814138889312744\n",
      "Eval loss 0.8693955540657043, R2 0.6866955161094666\n",
      "epoch 1838, loss 0.8366100788116455, R2 0.681430459022522\n",
      "Eval loss 0.8693484663963318, R2 0.6867152452468872\n",
      "epoch 1839, loss 0.8365732431411743, R2 0.6814469695091248\n",
      "Eval loss 0.8693014979362488, R2 0.6867347955703735\n",
      "epoch 1840, loss 0.836536705493927, R2 0.6814634203910828\n",
      "Eval loss 0.8692547082901001, R2 0.6867542862892151\n",
      "epoch 1841, loss 0.8364999890327454, R2 0.6814798712730408\n",
      "Eval loss 0.8692078590393066, R2 0.6867738962173462\n",
      "epoch 1842, loss 0.836463451385498, R2 0.681496262550354\n",
      "Eval loss 0.8691613078117371, R2 0.686793327331543\n",
      "epoch 1843, loss 0.8364270329475403, R2 0.6815125942230225\n",
      "Eval loss 0.8691146373748779, R2 0.6868128776550293\n",
      "epoch 1844, loss 0.8363907933235168, R2 0.6815290451049805\n",
      "Eval loss 0.8690683245658875, R2 0.6868321895599365\n",
      "epoch 1845, loss 0.8363544940948486, R2 0.6815453171730042\n",
      "Eval loss 0.8690218925476074, R2 0.6868515610694885\n",
      "epoch 1846, loss 0.83631831407547, R2 0.6815615296363831\n",
      "Eval loss 0.8689756393432617, R2 0.6868708729743958\n",
      "epoch 1847, loss 0.8362821340560913, R2 0.6815778613090515\n",
      "Eval loss 0.868929386138916, R2 0.686890184879303\n",
      "epoch 1848, loss 0.8362461924552917, R2 0.6815940737724304\n",
      "Eval loss 0.8688833713531494, R2 0.6869094967842102\n",
      "epoch 1849, loss 0.8362101912498474, R2 0.6816101670265198\n",
      "Eval loss 0.8688374161720276, R2 0.6869286894798279\n",
      "epoch 1850, loss 0.8361744284629822, R2 0.6816263198852539\n",
      "Eval loss 0.8687915205955505, R2 0.6869478225708008\n",
      "epoch 1851, loss 0.8361386060714722, R2 0.681642472743988\n",
      "Eval loss 0.868745744228363, R2 0.6869670748710632\n",
      "epoch 1852, loss 0.8361029028892517, R2 0.6816585659980774\n",
      "Eval loss 0.8687000274658203, R2 0.6869861483573914\n",
      "epoch 1853, loss 0.8360673785209656, R2 0.6816746592521667\n",
      "Eval loss 0.8686543703079224, R2 0.6870052218437195\n",
      "epoch 1854, loss 0.8360317945480347, R2 0.6816905736923218\n",
      "Eval loss 0.868608832359314, R2 0.6870242953300476\n",
      "epoch 1855, loss 0.8359962701797485, R2 0.6817066073417664\n",
      "Eval loss 0.8685634136199951, R2 0.6870432496070862\n",
      "epoch 1856, loss 0.8359609246253967, R2 0.6817225217819214\n",
      "Eval loss 0.868518054485321, R2 0.6870622634887695\n",
      "epoch 1857, loss 0.8359256982803345, R2 0.6817384362220764\n",
      "Eval loss 0.8684729337692261, R2 0.6870811581611633\n",
      "epoch 1858, loss 0.8358904719352722, R2 0.6817542910575867\n",
      "Eval loss 0.8684276938438416, R2 0.6871000528335571\n",
      "epoch 1859, loss 0.8358553647994995, R2 0.6817701458930969\n",
      "Eval loss 0.8683826327323914, R2 0.6871189475059509\n",
      "epoch 1860, loss 0.8358203172683716, R2 0.6817858815193176\n",
      "Eval loss 0.8683377504348755, R2 0.6871377825737\n",
      "epoch 1861, loss 0.8357852697372437, R2 0.6818016171455383\n",
      "Eval loss 0.8682926893234253, R2 0.6871565580368042\n",
      "epoch 1862, loss 0.8357503414154053, R2 0.6818174123764038\n",
      "Eval loss 0.8682481050491333, R2 0.6871753334999084\n",
      "epoch 1863, loss 0.8357155919075012, R2 0.6818330883979797\n",
      "Eval loss 0.8682034015655518, R2 0.6871940493583679\n",
      "epoch 1864, loss 0.8356807827949524, R2 0.6818488240242004\n",
      "Eval loss 0.8681586980819702, R2 0.6872127652168274\n",
      "epoch 1865, loss 0.8356461524963379, R2 0.6818644404411316\n",
      "Eval loss 0.868114173412323, R2 0.6872313618659973\n",
      "epoch 1866, loss 0.8356115818023682, R2 0.6818801164627075\n",
      "Eval loss 0.8680697679519653, R2 0.687250018119812\n",
      "epoch 1867, loss 0.8355770111083984, R2 0.6818956732749939\n",
      "Eval loss 0.8680254220962524, R2 0.6872685551643372\n",
      "epoch 1868, loss 0.8355426788330078, R2 0.6819112300872803\n",
      "Eval loss 0.8679813146591187, R2 0.6872870922088623\n",
      "epoch 1869, loss 0.8355082869529724, R2 0.6819267272949219\n",
      "Eval loss 0.8679371476173401, R2 0.6873056888580322\n",
      "epoch 1870, loss 0.8354740142822266, R2 0.6819421648979187\n",
      "Eval loss 0.8678929805755615, R2 0.6873241066932678\n",
      "epoch 1871, loss 0.8354398608207703, R2 0.6819576025009155\n",
      "Eval loss 0.8678489923477173, R2 0.6873425245285034\n",
      "epoch 1872, loss 0.835405707359314, R2 0.6819730401039124\n",
      "Eval loss 0.8678051233291626, R2 0.687360942363739\n",
      "epoch 1873, loss 0.8353715538978577, R2 0.6819884181022644\n",
      "Eval loss 0.8677613735198975, R2 0.6873793601989746\n",
      "epoch 1874, loss 0.83533775806427, R2 0.6820037961006165\n",
      "Eval loss 0.8677176833152771, R2 0.6873976588249207\n",
      "epoch 1875, loss 0.8353038430213928, R2 0.6820191740989685\n",
      "Eval loss 0.8676740527153015, R2 0.6874158978462219\n",
      "epoch 1876, loss 0.8352699875831604, R2 0.6820344924926758\n",
      "Eval loss 0.8676304221153259, R2 0.687434196472168\n",
      "epoch 1877, loss 0.8352361917495728, R2 0.6820496916770935\n",
      "Eval loss 0.8675870299339294, R2 0.6874524354934692\n",
      "epoch 1878, loss 0.8352025747299194, R2 0.6820648312568665\n",
      "Eval loss 0.8675436973571777, R2 0.6874706149101257\n",
      "epoch 1879, loss 0.8351690769195557, R2 0.6820800304412842\n",
      "Eval loss 0.867500364780426, R2 0.6874887943267822\n",
      "epoch 1880, loss 0.8351355195045471, R2 0.6820952892303467\n",
      "Eval loss 0.8674573302268982, R2 0.687506914138794\n",
      "epoch 1881, loss 0.8351020812988281, R2 0.6821103692054749\n",
      "Eval loss 0.8674142360687256, R2 0.6875250339508057\n",
      "epoch 1882, loss 0.8350687623023987, R2 0.6821255087852478\n",
      "Eval loss 0.8673712015151978, R2 0.6875430941581726\n",
      "epoch 1883, loss 0.8350354433059692, R2 0.682140588760376\n",
      "Eval loss 0.8673282265663147, R2 0.6875610947608948\n",
      "epoch 1884, loss 0.8350021839141846, R2 0.6821556091308594\n",
      "Eval loss 0.867285430431366, R2 0.6875791549682617\n",
      "epoch 1885, loss 0.8349690437316895, R2 0.682170569896698\n",
      "Eval loss 0.8672425746917725, R2 0.6875970959663391\n",
      "epoch 1886, loss 0.8349359631538391, R2 0.6821855902671814\n",
      "Eval loss 0.8671999573707581, R2 0.6876150369644165\n",
      "epoch 1887, loss 0.8349030017852783, R2 0.6822006106376648\n",
      "Eval loss 0.8671573400497437, R2 0.6876329183578491\n",
      "epoch 1888, loss 0.8348701000213623, R2 0.6822154521942139\n",
      "Eval loss 0.8671149015426636, R2 0.687650740146637\n",
      "epoch 1889, loss 0.8348371982574463, R2 0.6822303533554077\n",
      "Eval loss 0.8670724630355835, R2 0.6876686215400696\n",
      "epoch 1890, loss 0.8348045349121094, R2 0.6822451949119568\n",
      "Eval loss 0.867030143737793, R2 0.6876864433288574\n",
      "epoch 1891, loss 0.8347718119621277, R2 0.6822600364685059\n",
      "Eval loss 0.866987943649292, R2 0.6877041459083557\n",
      "epoch 1892, loss 0.8347392082214355, R2 0.6822748184204102\n",
      "Eval loss 0.8669458627700806, R2 0.687721848487854\n",
      "epoch 1893, loss 0.8347066044807434, R2 0.6822895407676697\n",
      "Eval loss 0.8669037222862244, R2 0.6877395510673523\n",
      "epoch 1894, loss 0.8346741199493408, R2 0.682304322719574\n",
      "Eval loss 0.8668617606163025, R2 0.6877572536468506\n",
      "epoch 1895, loss 0.834641695022583, R2 0.6823190450668335\n",
      "Eval loss 0.8668198585510254, R2 0.6877748370170593\n",
      "epoch 1896, loss 0.8346093893051147, R2 0.6823336482048035\n",
      "Eval loss 0.8667779564857483, R2 0.6877924203872681\n",
      "epoch 1897, loss 0.8345771431922913, R2 0.6823483109474182\n",
      "Eval loss 0.8667362928390503, R2 0.6878100037574768\n",
      "epoch 1898, loss 0.8345448970794678, R2 0.6823628544807434\n",
      "Eval loss 0.8666945695877075, R2 0.6878275871276855\n",
      "epoch 1899, loss 0.8345128297805786, R2 0.6823774576187134\n",
      "Eval loss 0.8666530847549438, R2 0.68784499168396\n",
      "epoch 1900, loss 0.8344807624816895, R2 0.6823921203613281\n",
      "Eval loss 0.8666115403175354, R2 0.6878624558448792\n",
      "epoch 1901, loss 0.8344487547874451, R2 0.6824066042900085\n",
      "Eval loss 0.866570234298706, R2 0.6878799200057983\n",
      "epoch 1902, loss 0.8344168066978455, R2 0.6824210286140442\n",
      "Eval loss 0.8665289878845215, R2 0.6878973245620728\n",
      "epoch 1903, loss 0.8343849778175354, R2 0.6824355125427246\n",
      "Eval loss 0.8664876222610474, R2 0.6879147291183472\n",
      "epoch 1904, loss 0.8343533873558044, R2 0.6824499368667603\n",
      "Eval loss 0.8664463758468628, R2 0.687932014465332\n",
      "epoch 1905, loss 0.8343215584754944, R2 0.6824643611907959\n",
      "Eval loss 0.8664054274559021, R2 0.6879492998123169\n",
      "epoch 1906, loss 0.8342899680137634, R2 0.6824787259101868\n",
      "Eval loss 0.8663643598556519, R2 0.6879665851593018\n",
      "epoch 1907, loss 0.8342583775520325, R2 0.6824930906295776\n",
      "Eval loss 0.8663234710693359, R2 0.6879838705062866\n",
      "epoch 1908, loss 0.8342269062995911, R2 0.682507336139679\n",
      "Eval loss 0.8662827014923096, R2 0.6880010366439819\n",
      "epoch 1909, loss 0.8341954946517944, R2 0.6825217604637146\n",
      "Eval loss 0.866241991519928, R2 0.6880182027816772\n",
      "epoch 1910, loss 0.8341641426086426, R2 0.6825359463691711\n",
      "Eval loss 0.8662012815475464, R2 0.6880353689193726\n",
      "epoch 1911, loss 0.8341329097747803, R2 0.6825501918792725\n",
      "Eval loss 0.8661606907844543, R2 0.6880525350570679\n",
      "epoch 1912, loss 0.8341017365455627, R2 0.682564377784729\n",
      "Eval loss 0.8661202788352966, R2 0.6880695223808289\n",
      "epoch 1913, loss 0.8340705037117004, R2 0.6825785636901855\n",
      "Eval loss 0.8660798668861389, R2 0.6880865693092346\n",
      "epoch 1914, loss 0.8340395092964172, R2 0.6825926303863525\n",
      "Eval loss 0.8660394549369812, R2 0.6881036162376404\n",
      "epoch 1915, loss 0.8340083360671997, R2 0.6826067566871643\n",
      "Eval loss 0.8659991025924683, R2 0.6881205439567566\n",
      "epoch 1916, loss 0.8339775800704956, R2 0.6826209425926208\n",
      "Eval loss 0.8659589290618896, R2 0.6881375908851624\n",
      "epoch 1917, loss 0.8339467644691467, R2 0.6826349496841431\n",
      "Eval loss 0.8659188747406006, R2 0.6881544589996338\n",
      "epoch 1918, loss 0.8339158892631531, R2 0.6826490163803101\n",
      "Eval loss 0.8658788204193115, R2 0.6881713271141052\n",
      "epoch 1919, loss 0.8338851928710938, R2 0.6826629042625427\n",
      "Eval loss 0.8658389449119568, R2 0.6881881356239319\n",
      "epoch 1920, loss 0.8338545560836792, R2 0.6826769113540649\n",
      "Eval loss 0.865799069404602, R2 0.6882050037384033\n",
      "epoch 1921, loss 0.8338239192962646, R2 0.6826907992362976\n",
      "Eval loss 0.8657592535018921, R2 0.68822181224823\n",
      "epoch 1922, loss 0.8337934017181396, R2 0.682704746723175\n",
      "Eval loss 0.8657195568084717, R2 0.6882385611534119\n",
      "epoch 1923, loss 0.8337628841400146, R2 0.6827185750007629\n",
      "Eval loss 0.8656799793243408, R2 0.6882553696632385\n",
      "epoch 1924, loss 0.833732545375824, R2 0.6827324628829956\n",
      "Eval loss 0.8656404614448547, R2 0.6882720589637756\n",
      "epoch 1925, loss 0.8337022066116333, R2 0.6827462911605835\n",
      "Eval loss 0.8656009435653687, R2 0.688288688659668\n",
      "epoch 1926, loss 0.8336719870567322, R2 0.6827601790428162\n",
      "Eval loss 0.8655614852905273, R2 0.6883053183555603\n",
      "epoch 1927, loss 0.8336418867111206, R2 0.6827738285064697\n",
      "Eval loss 0.8655221462249756, R2 0.6883219480514526\n",
      "epoch 1928, loss 0.8336117267608643, R2 0.6827875971794128\n",
      "Eval loss 0.8654829859733582, R2 0.6883385181427002\n",
      "epoch 1929, loss 0.8335816860198975, R2 0.6828013062477112\n",
      "Eval loss 0.865443766117096, R2 0.6883550882339478\n",
      "epoch 1930, loss 0.8335515856742859, R2 0.6828149557113647\n",
      "Eval loss 0.8654047846794128, R2 0.6883715391159058\n",
      "epoch 1931, loss 0.8335217237472534, R2 0.6828287243843079\n",
      "Eval loss 0.865365743637085, R2 0.6883880496025085\n",
      "epoch 1932, loss 0.8334918022155762, R2 0.6828423142433167\n",
      "Eval loss 0.8653268814086914, R2 0.6884045004844666\n",
      "epoch 1933, loss 0.8334619998931885, R2 0.6828559041023254\n",
      "Eval loss 0.8652878999710083, R2 0.6884210109710693\n",
      "epoch 1934, loss 0.8334324359893799, R2 0.6828694939613342\n",
      "Eval loss 0.8652492761611938, R2 0.6884373426437378\n",
      "epoch 1935, loss 0.833402693271637, R2 0.6828829646110535\n",
      "Eval loss 0.8652104139328003, R2 0.6884536743164062\n",
      "epoch 1936, loss 0.8333731889724731, R2 0.6828965544700623\n",
      "Eval loss 0.8651718497276306, R2 0.6884700655937195\n",
      "epoch 1937, loss 0.8333435654640198, R2 0.6829100251197815\n",
      "Eval loss 0.8651333451271057, R2 0.6884863376617432\n",
      "epoch 1938, loss 0.8333141207695007, R2 0.6829234957695007\n",
      "Eval loss 0.8650948405265808, R2 0.6885026693344116\n",
      "epoch 1939, loss 0.8332846760749817, R2 0.6829369068145752\n",
      "Eval loss 0.8650563955307007, R2 0.6885188817977905\n",
      "epoch 1940, loss 0.8332554697990417, R2 0.6829503178596497\n",
      "Eval loss 0.8650181889533997, R2 0.6885350346565247\n",
      "epoch 1941, loss 0.833226203918457, R2 0.6829637289047241\n",
      "Eval loss 0.8649799227714539, R2 0.6885512471199036\n",
      "epoch 1942, loss 0.8331969380378723, R2 0.6829770803451538\n",
      "Eval loss 0.8649416565895081, R2 0.6885673403739929\n",
      "epoch 1943, loss 0.8331676721572876, R2 0.6829904317855835\n",
      "Eval loss 0.8649036884307861, R2 0.6885835528373718\n",
      "epoch 1944, loss 0.8331387042999268, R2 0.6830036640167236\n",
      "Eval loss 0.8648655414581299, R2 0.6885996460914612\n",
      "epoch 1945, loss 0.8331097364425659, R2 0.6830169558525085\n",
      "Eval loss 0.8648277521133423, R2 0.6886156797409058\n",
      "epoch 1946, loss 0.8330807685852051, R2 0.6830302476882935\n",
      "Eval loss 0.8647897839546204, R2 0.6886317133903503\n",
      "epoch 1947, loss 0.8330518007278442, R2 0.6830434799194336\n",
      "Eval loss 0.8647519946098328, R2 0.6886478066444397\n",
      "epoch 1948, loss 0.8330230712890625, R2 0.6830565929412842\n",
      "Eval loss 0.8647143244743347, R2 0.6886637210845947\n",
      "epoch 1949, loss 0.8329941630363464, R2 0.6830698251724243\n",
      "Eval loss 0.8646765947341919, R2 0.6886796951293945\n",
      "epoch 1950, loss 0.8329654932022095, R2 0.6830828785896301\n",
      "Eval loss 0.8646391034126282, R2 0.6886956095695496\n",
      "epoch 1951, loss 0.8329369425773621, R2 0.6830959916114807\n",
      "Eval loss 0.8646015524864197, R2 0.688711404800415\n",
      "epoch 1952, loss 0.8329084515571594, R2 0.6831091046333313\n",
      "Eval loss 0.8645641207695007, R2 0.6887273788452148\n",
      "epoch 1953, loss 0.832879900932312, R2 0.6831221580505371\n",
      "Eval loss 0.8645268678665161, R2 0.6887431740760803\n",
      "epoch 1954, loss 0.8328514099121094, R2 0.6831352114677429\n",
      "Eval loss 0.8644894361495972, R2 0.6887589693069458\n",
      "epoch 1955, loss 0.8328230381011963, R2 0.6831481456756592\n",
      "Eval loss 0.8644522428512573, R2 0.6887747049331665\n",
      "epoch 1956, loss 0.8327946662902832, R2 0.683161199092865\n",
      "Eval loss 0.864415168762207, R2 0.6887904405593872\n",
      "epoch 1957, loss 0.8327664136886597, R2 0.6831741333007812\n",
      "Eval loss 0.8643780946731567, R2 0.6888062357902527\n",
      "epoch 1958, loss 0.8327381610870361, R2 0.6831870675086975\n",
      "Eval loss 0.864341139793396, R2 0.6888218522071838\n",
      "epoch 1959, loss 0.8327101469039917, R2 0.6832000613212585\n",
      "Eval loss 0.8643041253089905, R2 0.6888375282287598\n",
      "epoch 1960, loss 0.8326820731163025, R2 0.6832128167152405\n",
      "Eval loss 0.8642672896385193, R2 0.6888532638549805\n",
      "epoch 1961, loss 0.8326540589332581, R2 0.683225691318512\n",
      "Eval loss 0.8642305731773376, R2 0.6888687610626221\n",
      "epoch 1962, loss 0.8326261043548584, R2 0.6832384467124939\n",
      "Eval loss 0.864193856716156, R2 0.6888843178749084\n",
      "epoch 1963, loss 0.832598090171814, R2 0.6832513213157654\n",
      "Eval loss 0.8641572594642639, R2 0.6888999342918396\n",
      "epoch 1964, loss 0.8325703144073486, R2 0.6832640767097473\n",
      "Eval loss 0.8641207218170166, R2 0.6889154314994812\n",
      "epoch 1965, loss 0.8325425982475281, R2 0.6832767724990845\n",
      "Eval loss 0.8640841245651245, R2 0.6889309287071228\n",
      "epoch 1966, loss 0.8325148820877075, R2 0.6832895278930664\n",
      "Eval loss 0.8640477657318115, R2 0.6889463663101196\n",
      "epoch 1967, loss 0.832487165927887, R2 0.6833022236824036\n",
      "Eval loss 0.8640113472938538, R2 0.6889618039131165\n",
      "epoch 1968, loss 0.8324596881866455, R2 0.683314859867096\n",
      "Eval loss 0.8639751076698303, R2 0.6889772415161133\n",
      "epoch 1969, loss 0.8324320316314697, R2 0.6833274960517883\n",
      "Eval loss 0.8639389276504517, R2 0.6889925003051758\n",
      "epoch 1970, loss 0.832404613494873, R2 0.6833401322364807\n",
      "Eval loss 0.8639028072357178, R2 0.6890079379081726\n",
      "epoch 1971, loss 0.8323771953582764, R2 0.6833528280258179\n",
      "Eval loss 0.8638666272163391, R2 0.6890231966972351\n",
      "epoch 1972, loss 0.832349956035614, R2 0.6833653450012207\n",
      "Eval loss 0.8638306856155396, R2 0.6890384554862976\n",
      "epoch 1973, loss 0.8323224782943726, R2 0.6833778619766235\n",
      "Eval loss 0.8637946844100952, R2 0.6890537142753601\n",
      "epoch 1974, loss 0.8322953581809998, R2 0.6833904385566711\n",
      "Eval loss 0.86375892162323, R2 0.6890690326690674\n",
      "epoch 1975, loss 0.8322681784629822, R2 0.6834028959274292\n",
      "Eval loss 0.86372309923172, R2 0.6890842914581299\n",
      "epoch 1976, loss 0.8322409987449646, R2 0.6834153532981873\n",
      "Eval loss 0.8636873364448547, R2 0.6890994906425476\n",
      "epoch 1977, loss 0.8322139978408813, R2 0.6834278106689453\n",
      "Eval loss 0.8636518120765686, R2 0.6891145706176758\n",
      "epoch 1978, loss 0.8321870565414429, R2 0.6834401488304138\n",
      "Eval loss 0.8636161088943481, R2 0.6891297101974487\n",
      "epoch 1979, loss 0.8321600556373596, R2 0.6834526062011719\n",
      "Eval loss 0.8635806441307068, R2 0.6891447901725769\n",
      "epoch 1980, loss 0.8321331739425659, R2 0.6834649443626404\n",
      "Eval loss 0.8635452389717102, R2 0.6891598105430603\n",
      "epoch 1981, loss 0.8321062922477722, R2 0.6834772825241089\n",
      "Eval loss 0.8635098338127136, R2 0.6891749501228333\n",
      "epoch 1982, loss 0.8320795893669128, R2 0.6834896206855774\n",
      "Eval loss 0.8634745478630066, R2 0.6891899704933167\n",
      "epoch 1983, loss 0.8320528864860535, R2 0.6835018396377563\n",
      "Eval loss 0.8634393215179443, R2 0.6892049908638\n",
      "epoch 1984, loss 0.8320261836051941, R2 0.6835141181945801\n",
      "Eval loss 0.8634040951728821, R2 0.6892198920249939\n",
      "epoch 1985, loss 0.831999659538269, R2 0.6835263967514038\n",
      "Eval loss 0.8633689880371094, R2 0.6892349123954773\n",
      "epoch 1986, loss 0.831973135471344, R2 0.683538556098938\n",
      "Eval loss 0.8633340001106262, R2 0.6892497539520264\n",
      "epoch 1987, loss 0.831946611404419, R2 0.6835507750511169\n",
      "Eval loss 0.8632990717887878, R2 0.6892645359039307\n",
      "epoch 1988, loss 0.8319201469421387, R2 0.6835629940032959\n",
      "Eval loss 0.8632641434669495, R2 0.6892794966697693\n",
      "epoch 1989, loss 0.831893801689148, R2 0.6835750937461853\n",
      "Eval loss 0.8632292747497559, R2 0.6892943382263184\n",
      "epoch 1990, loss 0.8318675756454468, R2 0.6835872530937195\n",
      "Eval loss 0.863194465637207, R2 0.6893091201782227\n",
      "epoch 1991, loss 0.831841230392456, R2 0.6835993528366089\n",
      "Eval loss 0.8631597757339478, R2 0.6893238425254822\n",
      "epoch 1992, loss 0.8318150639533997, R2 0.6836113333702087\n",
      "Eval loss 0.863125205039978, R2 0.6893386840820312\n",
      "epoch 1993, loss 0.831788957118988, R2 0.6836234927177429\n",
      "Eval loss 0.8630906343460083, R2 0.6893534064292908\n",
      "epoch 1994, loss 0.8317629098892212, R2 0.6836354732513428\n",
      "Eval loss 0.8630561232566833, R2 0.6893680691719055\n",
      "epoch 1995, loss 0.8317368030548096, R2 0.6836475133895874\n",
      "Eval loss 0.8630216121673584, R2 0.6893827319145203\n",
      "epoch 1996, loss 0.8317108750343323, R2 0.6836594343185425\n",
      "Eval loss 0.8629873991012573, R2 0.689397394657135\n",
      "epoch 1997, loss 0.831684947013855, R2 0.6836713552474976\n",
      "Eval loss 0.8629530072212219, R2 0.6894119381904602\n",
      "epoch 1998, loss 0.8316591382026672, R2 0.6836833357810974\n",
      "Eval loss 0.8629187941551208, R2 0.689426600933075\n",
      "epoch 1999, loss 0.8316333293914795, R2 0.6836952567100525\n",
      "Eval loss 0.8628846406936646, R2 0.6894411444664001\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(dynamics_train_x.shape[0])\n",
    "    dynamics_train_x = dynamics_train_x[idx, :]\n",
    "    dynamics_train_y = dynamics_train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    dynamics_test_x = dynamics_test_x[idx, :]\n",
    "    dynamics_test_y = dynamics_test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = dynamics_lr_model(dynamics_train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, dynamics_train_y)\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute metric\n",
    "    train_metric = metric(outputs, dynamics_train_y)\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print(\"epoch {}, loss {}, R2 {}\".format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch % eval_epoch_freq == 0:\n",
    "        with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "            preds = dynamics_lr_model(dynamics_test_x)\n",
    "            test_loss = criterion(preds, dynamics_test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "            # Compute metric\n",
    "            test_metric = metric(preds, dynamics_test_y)\n",
    "            test_metrics.append(test_metric)\n",
    "            print(\"Eval loss {}, R2 {}\".format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_lr_model(dynamics_train_x)\n",
    "    test_preds = dynamics_lr_model(dynamics_test_x)\n",
    "\n",
    "relative_train_preds_mean = torch.mean(train_preds - dynamics_train_y, axis=0)\n",
    "relative_train_preds_std = torch.std(train_preds - dynamics_train_y, axis=0)\n",
    "relative_test_preds_mean = torch.mean(test_preds - dynamics_test_y, axis=0)\n",
    "relative_test_preds_std = torch.std(test_preds - dynamics_test_y, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Colors and labels\n",
    "if len(relative_train_preds_mean) < num_centroids:\n",
    "    raise ValueError(\"Predicted outputs are weird\")\n",
    "if len(relative_train_preds_mean) == num_centroids:\n",
    "    color = \"green\"\n",
    "    label = \"centroids\"\n",
    "else:\n",
    "    colors = [\"green\"] * num_centroids\n",
    "    colors.append(\"red\")\n",
    "    label = [\"centroids\"] * num_centroids\n",
    "    label.append(\"reward\")\n",
    "\n",
    "# Train bar plots\n",
    "x = torch.arange(0, len(relative_train_preds_mean), 1)\n",
    "ax[0].bar(x, relative_train_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_train_preds_mean, relative_train_preds_std, colors\n",
    "):\n",
    "    ax[0].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "    # error_kw=dict(ecolor=colors, lw=1, capsize=1, capthick=2, alpha=0.5),\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "ax[1].bar(x, relative_test_preds_mean, color=colors, label=label)\n",
    "for pos, y, err, color in zip(\n",
    "    x, relative_test_preds_mean, relative_test_preds_std, colors\n",
    "):\n",
    "    ax[1].errorbar(pos, y, err, lw=2, capsize=2, capthick=1, color=color, alpha=0.3)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value for each centroid (and reward if learned)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 44]) torch.Size([800, 1]) torch.Size([200, 44]) torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "reward_train_x = train_x\n",
    "reward_train_y = train_y[:, -1][..., None]\n",
    "reward_test_x = test_x\n",
    "reward_test_y = test_y[:, -1][..., None]\n",
    "\n",
    "print(\n",
    "    reward_train_x.shape, reward_train_y.shape, reward_test_x.shape, reward_test_y.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.linear_regression import LinearRegression\n",
    "\n",
    "learningRate = 0.05\n",
    "epochs = 2000\n",
    "eval_epoch_freq = 1\n",
    "in_size = reward_train_x.shape[-1]\n",
    "out_size = reward_train_y.shape[-1]\n",
    "device = \"cpu\"\n",
    "reward_lr_model = LinearRegression(in_size, out_size, device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "metric = r2_score\n",
    "optimizer = torch.optim.SGD(reward_lr_model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.09437771141529083, R2 -2.828277587890625\n",
      "Eval loss 0.1160796731710434, R2 -2.738022804260254\n",
      "epoch 1, loss 0.08736592531204224, R2 -2.5438544750213623\n",
      "Eval loss 0.11290904879570007, R2 -2.6359217166900635\n",
      "epoch 2, loss 0.08277229219675064, R2 -2.3575215339660645\n",
      "Eval loss 0.11074014008045197, R2 -2.566077947616577\n",
      "epoch 3, loss 0.0796939879655838, R2 -2.2326550483703613\n",
      "Eval loss 0.10917405784130096, R2 -2.515645742416382\n",
      "epoch 4, loss 0.07755535840988159, R2 -2.1459062099456787\n",
      "Eval loss 0.10797608643770218, R2 -2.4770681858062744\n",
      "epoch 5, loss 0.0760030448436737, R2 -2.0829405784606934\n",
      "Eval loss 0.10700736194849014, R2 -2.445873260498047\n",
      "epoch 6, loss 0.07482103258371353, R2 -2.03499174118042\n",
      "Eval loss 0.10618427395820618, R2 -2.419368028640747\n",
      "epoch 7, loss 0.07387606799602509, R2 -1.996659517288208\n",
      "Eval loss 0.10545536130666733, R2 -2.3958964347839355\n",
      "epoch 8, loss 0.0730847418308258, R2 -1.964561939239502\n",
      "Eval loss 0.10478820651769638, R2 -2.3744125366210938\n",
      "epoch 9, loss 0.07239405810832977, R2 -1.9365453720092773\n",
      "Eval loss 0.10416198521852493, R2 -2.354245901107788\n",
      "epoch 10, loss 0.07176990807056427, R2 -1.911229133605957\n",
      "Eval loss 0.10356300324201584, R2 -2.33495831489563\n",
      "epoch 11, loss 0.0711900070309639, R2 -1.8877050876617432\n",
      "Eval loss 0.10298217833042145, R2 -2.3162543773651123\n",
      "epoch 12, loss 0.07063968479633331, R2 -1.8653833866119385\n",
      "Eval loss 0.1024133563041687, R2 -2.2979371547698975\n",
      "epoch 13, loss 0.07010917365550995, R2 -1.8438630104064941\n",
      "Eval loss 0.10185235738754272, R2 -2.279871702194214\n",
      "epoch 14, loss 0.06959199160337448, R2 -1.8228821754455566\n",
      "Eval loss 0.1012963280081749, R2 -2.261967420578003\n",
      "epoch 15, loss 0.0690837875008583, R2 -1.8022675514221191\n",
      "Eval loss 0.10074325650930405, R2 -2.2441561222076416\n",
      "epoch 16, loss 0.06858167052268982, R2 -1.7819032669067383\n",
      "Eval loss 0.10019176453351974, R2 -2.226396083831787\n",
      "epoch 17, loss 0.06808371841907501, R2 -1.7617037296295166\n",
      "Eval loss 0.09964091330766678, R2 -2.2086572647094727\n",
      "epoch 18, loss 0.0675887018442154, R2 -1.741624116897583\n",
      "Eval loss 0.09909002482891083, R2 -2.190918445587158\n",
      "epoch 19, loss 0.0670958012342453, R2 -1.721630573272705\n",
      "Eval loss 0.09853863716125488, R2 -2.173161506652832\n",
      "epoch 20, loss 0.06660455465316772, R2 -1.7017037868499756\n",
      "Eval loss 0.09798648953437805, R2 -2.155381202697754\n",
      "epoch 21, loss 0.06611467152833939, R2 -1.6818304061889648\n",
      "Eval loss 0.09743330627679825, R2 -2.137568473815918\n",
      "epoch 22, loss 0.06562601774930954, R2 -1.6620123386383057\n",
      "Eval loss 0.0968790352344513, R2 -2.1197197437286377\n",
      "epoch 23, loss 0.06513854116201401, R2 -1.642235517501831\n",
      "Eval loss 0.09632354974746704, R2 -2.1018316745758057\n",
      "epoch 24, loss 0.06465226411819458, R2 -1.6225125789642334\n",
      "Eval loss 0.09576684981584549, R2 -2.083904981613159\n",
      "epoch 25, loss 0.06416726857423782, R2 -1.6028416156768799\n",
      "Eval loss 0.09520888328552246, R2 -2.0659379959106445\n",
      "epoch 26, loss 0.06368362158536911, R2 -1.5832209587097168\n",
      "Eval loss 0.09464970231056213, R2 -2.047929525375366\n",
      "epoch 27, loss 0.0632014349102974, R2 -1.563661813735962\n",
      "Eval loss 0.0940893292427063, R2 -2.0298831462860107\n",
      "epoch 28, loss 0.06272081285715103, R2 -1.5441663265228271\n",
      "Eval loss 0.09352780133485794, R2 -2.0118026733398438\n",
      "epoch 29, loss 0.06224188208580017, R2 -1.5247392654418945\n",
      "Eval loss 0.09296516329050064, R2 -1.9936842918395996\n",
      "epoch 30, loss 0.06176476180553436, R2 -1.5053837299346924\n",
      "Eval loss 0.09240152686834335, R2 -1.9755339622497559\n",
      "epoch 31, loss 0.06128956004977226, R2 -1.486109733581543\n",
      "Eval loss 0.09183689206838608, R2 -1.9573523998260498\n",
      "epoch 32, loss 0.06081640347838402, R2 -1.4669170379638672\n",
      "Eval loss 0.09127139300107956, R2 -1.939141035079956\n",
      "epoch 33, loss 0.060345396399497986, R2 -1.4478106498718262\n",
      "Eval loss 0.09070508927106857, R2 -1.9209048748016357\n",
      "epoch 34, loss 0.05987665802240372, R2 -1.4287986755371094\n",
      "Eval loss 0.09013807028532028, R2 -1.9026448726654053\n",
      "epoch 35, loss 0.05941028892993927, R2 -1.409881353378296\n",
      "Eval loss 0.08957041800022125, R2 -1.8843660354614258\n",
      "epoch 36, loss 0.0589464008808136, R2 -1.3910624980926514\n",
      "Eval loss 0.08900224417448044, R2 -1.8660688400268555\n",
      "epoch 37, loss 0.05848509818315506, R2 -1.3723523616790771\n",
      "Eval loss 0.0884336456656456, R2 -1.847759485244751\n",
      "epoch 38, loss 0.05802648141980171, R2 -1.3537492752075195\n",
      "Eval loss 0.08786472678184509, R2 -1.8294389247894287\n",
      "epoch 39, loss 0.05757063254714012, R2 -1.3352577686309814\n",
      "Eval loss 0.08729556947946548, R2 -1.8111109733581543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, loss 0.057117655873298645, R2 -1.3168835639953613\n",
      "Eval loss 0.08672632277011871, R2 -1.7927799224853516\n",
      "epoch 41, loss 0.05666763335466385, R2 -1.2986290454864502\n",
      "Eval loss 0.08615706115961075, R2 -1.7744483947753906\n",
      "epoch 42, loss 0.0562206469476223, R2 -1.2804961204528809\n",
      "Eval loss 0.08558791875839233, R2 -1.7561209201812744\n",
      "epoch 43, loss 0.05577678233385086, R2 -1.2624931335449219\n",
      "Eval loss 0.08501900732517242, R2 -1.7378005981445312\n",
      "epoch 44, loss 0.055336106568574905, R2 -1.2446179389953613\n",
      "Eval loss 0.08445043861865997, R2 -1.7194905281066895\n",
      "epoch 45, loss 0.054898716509342194, R2 -1.2268757820129395\n",
      "Eval loss 0.08388233929872513, R2 -1.7011973857879639\n",
      "epoch 46, loss 0.05446465313434601, R2 -1.2092688083648682\n",
      "Eval loss 0.08331482857465744, R2 -1.682922124862671\n",
      "epoch 47, loss 0.05403400957584381, R2 -1.1918003559112549\n",
      "Eval loss 0.08274804800748825, R2 -1.6646699905395508\n",
      "epoch 48, loss 0.05360683053731918, R2 -1.1744725704193115\n",
      "Eval loss 0.08218209445476532, R2 -1.6464455127716064\n",
      "epoch 49, loss 0.053183186799287796, R2 -1.1572883129119873\n",
      "Eval loss 0.08161710947751999, R2 -1.6282517910003662\n",
      "epoch 50, loss 0.052763134241104126, R2 -1.1402502059936523\n",
      "Eval loss 0.08105321228504181, R2 -1.610093116760254\n",
      "epoch 51, loss 0.052346717566251755, R2 -1.123356580734253\n",
      "Eval loss 0.08049052953720093, R2 -1.5919735431671143\n",
      "epoch 52, loss 0.05193399265408516, R2 -1.1066160202026367\n",
      "Eval loss 0.07992921024560928, R2 -1.5738985538482666\n",
      "epoch 53, loss 0.051525015383958817, R2 -1.0900273323059082\n",
      "Eval loss 0.07936936616897583, R2 -1.5558695793151855\n",
      "epoch 54, loss 0.05111980810761452, R2 -1.0735890865325928\n",
      "Eval loss 0.07881110906600952, R2 -1.5378925800323486\n",
      "epoch 55, loss 0.05071843042969704, R2 -1.057309627532959\n",
      "Eval loss 0.07825460284948349, R2 -1.5199716091156006\n",
      "epoch 56, loss 0.05032090097665787, R2 -1.0411851406097412\n",
      "Eval loss 0.07769995182752609, R2 -1.5021107196807861\n",
      "epoch 57, loss 0.04992727190256119, R2 -1.0252182483673096\n",
      "Eval loss 0.07714729756116867, R2 -1.48431396484375\n",
      "epoch 58, loss 0.049537550657987595, R2 -1.009408950805664\n",
      "Eval loss 0.07659676671028137, R2 -1.466588020324707\n",
      "epoch 59, loss 0.049151789397001266, R2 -0.9937620162963867\n",
      "Eval loss 0.07604847848415375, R2 -1.4489295482635498\n",
      "epoch 60, loss 0.0487699881196022, R2 -0.9782733917236328\n",
      "Eval loss 0.07550256699323654, R2 -1.4313499927520752\n",
      "epoch 61, loss 0.04839218780398369, R2 -0.9629484415054321\n",
      "Eval loss 0.07495914399623871, R2 -1.4138505458831787\n",
      "epoch 62, loss 0.04801838845014572, R2 -0.9477875232696533\n",
      "Eval loss 0.0744183361530304, R2 -1.396435260772705\n",
      "epoch 63, loss 0.04764861986041069, R2 -0.932786226272583\n",
      "Eval loss 0.07388026267290115, R2 -1.3791074752807617\n",
      "epoch 64, loss 0.047282878309488297, R2 -0.917951226234436\n",
      "Eval loss 0.07334505021572113, R2 -1.3618724346160889\n",
      "epoch 65, loss 0.04692118242383003, R2 -0.9032803773880005\n",
      "Eval loss 0.07281281799077988, R2 -1.3447339534759521\n",
      "epoch 66, loss 0.046563535928726196, R2 -0.8887729644775391\n",
      "Eval loss 0.07228367030620575, R2 -1.327692985534668\n",
      "epoch 67, loss 0.046209950000047684, R2 -0.8744289875030518\n",
      "Eval loss 0.07175770401954651, R2 -1.3107593059539795\n",
      "epoch 68, loss 0.0458604134619236, R2 -0.8602520227432251\n",
      "Eval loss 0.07123507559299469, R2 -1.2939274311065674\n",
      "epoch 69, loss 0.04551493749022484, R2 -0.8462398052215576\n",
      "Eval loss 0.07071582973003387, R2 -1.2772059440612793\n",
      "epoch 70, loss 0.04517350718379021, R2 -0.8323887586593628\n",
      "Eval loss 0.0702001228928566, R2 -1.2605996131896973\n",
      "epoch 71, loss 0.04483611881732941, R2 -0.8187025785446167\n",
      "Eval loss 0.06968803703784943, R2 -1.2441091537475586\n",
      "epoch 72, loss 0.04450276866555214, R2 -0.8051806688308716\n",
      "Eval loss 0.06917965412139893, R2 -1.2277376651763916\n",
      "epoch 73, loss 0.04417344182729721, R2 -0.7918227910995483\n",
      "Eval loss 0.06867507845163345, R2 -1.2114899158477783\n",
      "epoch 74, loss 0.043848127126693726, R2 -0.7786270380020142\n",
      "Eval loss 0.06817442178726196, R2 -1.1953673362731934\n",
      "epoch 75, loss 0.04352681338787079, R2 -0.7655912637710571\n",
      "Eval loss 0.06767774373292923, R2 -1.1793735027313232\n",
      "epoch 76, loss 0.04320948198437691, R2 -0.7527213096618652\n",
      "Eval loss 0.06718514859676361, R2 -1.1635093688964844\n",
      "epoch 77, loss 0.04289611801505089, R2 -0.7400109767913818\n",
      "Eval loss 0.06669671833515167, R2 -1.1477820873260498\n",
      "epoch 78, loss 0.04258670657873154, R2 -0.7274595499038696\n",
      "Eval loss 0.06621251255273819, R2 -1.1321890354156494\n",
      "epoch 79, loss 0.04228121414780617, R2 -0.7150676250457764\n",
      "Eval loss 0.06573262810707092, R2 -1.1167359352111816\n",
      "epoch 80, loss 0.04197963699698448, R2 -0.7028334140777588\n",
      "Eval loss 0.06525713205337524, R2 -1.1014249324798584\n",
      "epoch 81, loss 0.04168194904923439, R2 -0.6907601356506348\n",
      "Eval loss 0.06478608399629593, R2 -1.0862574577331543\n",
      "epoch 82, loss 0.041388124227523804, R2 -0.6788417100906372\n",
      "Eval loss 0.06431957334280014, R2 -1.071232795715332\n",
      "epoch 83, loss 0.04109815135598183, R2 -0.6670787334442139\n",
      "Eval loss 0.06385764479637146, R2 -1.0563578605651855\n",
      "epoch 84, loss 0.04081199690699577, R2 -0.6554713249206543\n",
      "Eval loss 0.06340036541223526, R2 -1.041632890701294\n",
      "epoch 85, loss 0.04052964225411415, R2 -0.6440187692642212\n",
      "Eval loss 0.06294779479503632, R2 -1.0270578861236572\n",
      "epoch 86, loss 0.04025106504559517, R2 -0.6327180862426758\n",
      "Eval loss 0.06249998137354851, R2 -1.0126378536224365\n",
      "epoch 87, loss 0.03997624292969704, R2 -0.6215710639953613\n",
      "Eval loss 0.06205698847770691, R2 -0.9983726739883423\n",
      "epoch 88, loss 0.03970515727996826, R2 -0.6105742454528809\n",
      "Eval loss 0.0616188608109951, R2 -0.9842640161514282\n",
      "epoch 89, loss 0.039437782019376755, R2 -0.5997285842895508\n",
      "Eval loss 0.06118565425276756, R2 -0.9703137874603271\n",
      "epoch 90, loss 0.03917410597205162, R2 -0.5890324115753174\n",
      "Eval loss 0.060757409781217575, R2 -0.9565232992172241\n",
      "epoch 91, loss 0.03891409561038017, R2 -0.5784868001937866\n",
      "Eval loss 0.060334157198667526, R2 -0.9428936243057251\n",
      "epoch 92, loss 0.038657743483781815, R2 -0.5680875778198242\n",
      "Eval loss 0.05991596356034279, R2 -0.9294267892837524\n",
      "epoch 93, loss 0.038405030965805054, R2 -0.5578367710113525\n",
      "Eval loss 0.05950284004211426, R2 -0.9161227941513062\n",
      "epoch 94, loss 0.038155943155288696, R2 -0.5477334260940552\n",
      "Eval loss 0.05909484997391701, R2 -0.902985692024231\n",
      "epoch 95, loss 0.03791045770049095, R2 -0.5377745628356934\n",
      "Eval loss 0.05869201198220253, R2 -0.8900128602981567\n",
      "epoch 96, loss 0.03766857087612152, R2 -0.5279639959335327\n",
      "Eval loss 0.05829436331987381, R2 -0.8772077560424805\n",
      "epoch 97, loss 0.037430260330438614, R2 -0.5182967185974121\n",
      "Eval loss 0.057901933789253235, R2 -0.8645724058151245\n",
      "epoch 98, loss 0.03719552606344223, R2 -0.5087757110595703\n",
      "Eval loss 0.05751475319266319, R2 -0.852102518081665\n",
      "epoch 99, loss 0.03696434944868088, R2 -0.49939846992492676\n",
      "Eval loss 0.05713283643126488, R2 -0.83980393409729\n",
      "epoch 100, loss 0.036736730486154556, R2 -0.4901624917984009\n",
      "Eval loss 0.05675622075796127, R2 -0.8276760578155518\n",
      "epoch 101, loss 0.03651265427470207, R2 -0.4810750484466553\n",
      "Eval loss 0.05638493224978447, R2 -0.815719723701477\n",
      "epoch 102, loss 0.036292120814323425, R2 -0.47213006019592285\n",
      "Eval loss 0.05601897090673447, R2 -0.8039339780807495\n",
      "epoch 103, loss 0.03607512265443802, R2 -0.46332788467407227\n",
      "Eval loss 0.055658359080553055, R2 -0.7923237085342407\n",
      "epoch 104, loss 0.035861656069755554, R2 -0.45466840267181396\n",
      "Eval loss 0.05530313029885292, R2 -0.7808834314346313\n",
      "epoch 105, loss 0.03565172106027603, R2 -0.44615328311920166\n",
      "Eval loss 0.05495326220989227, R2 -0.7696168422698975\n",
      "epoch 106, loss 0.03544530272483826, R2 -0.43778014183044434\n",
      "Eval loss 0.0546087846159935, R2 -0.7585239410400391\n",
      "epoch 107, loss 0.03524240851402283, R2 -0.4295501708984375\n",
      "Eval loss 0.0542696937918663, R2 -0.747606635093689\n",
      "epoch 108, loss 0.03504302725195885, R2 -0.42146313190460205\n",
      "Eval loss 0.05393599346280098, R2 -0.736858606338501\n",
      "epoch 109, loss 0.034847162663936615, R2 -0.4135183095932007\n",
      "Eval loss 0.05360767990350723, R2 -0.7262866497039795\n",
      "epoch 110, loss 0.034654803574085236, R2 -0.4057149887084961\n",
      "Eval loss 0.053284745663404465, R2 -0.715888500213623\n",
      "epoch 111, loss 0.03446594625711441, R2 -0.39805424213409424\n",
      "Eval loss 0.05296717956662178, R2 -0.7056622505187988\n",
      "epoch 112, loss 0.03428058326244354, R2 -0.39053523540496826\n",
      "Eval loss 0.05265498161315918, R2 -0.6956071853637695\n",
      "epoch 113, loss 0.03409869968891144, R2 -0.3831568956375122\n",
      "Eval loss 0.05234811455011368, R2 -0.6857253313064575\n",
      "epoch 114, loss 0.0339202918112278, R2 -0.37592124938964844\n",
      "Eval loss 0.05204656720161438, R2 -0.6760143041610718\n",
      "epoch 115, loss 0.03374534100294113, R2 -0.3688241243362427\n",
      "Eval loss 0.0517503060400486, R2 -0.6664745807647705\n",
      "epoch 116, loss 0.03357383608818054, R2 -0.3618673086166382\n",
      "Eval loss 0.051459308713674545, R2 -0.6571042537689209\n",
      "epoch 117, loss 0.03340575471520424, R2 -0.35504984855651855\n",
      "Eval loss 0.051173534244298935, R2 -0.6479007005691528\n",
      "epoch 118, loss 0.03324107080698013, R2 -0.3483692407608032\n",
      "Eval loss 0.050892945379018784, R2 -0.638866662979126\n",
      "epoch 119, loss 0.03307976573705673, R2 -0.3418257236480713\n",
      "Eval loss 0.05061749368906021, R2 -0.6299960613250732\n",
      "epoch 120, loss 0.03292180970311165, R2 -0.3354189395904541\n",
      "Eval loss 0.05034714564681053, R2 -0.6212897300720215\n",
      "epoch 121, loss 0.032767172902822495, R2 -0.32914626598358154\n",
      "Eval loss 0.05008183792233467, R2 -0.6127462387084961\n",
      "epoch 122, loss 0.032615818083286285, R2 -0.32300686836242676\n",
      "Eval loss 0.04982151463627815, R2 -0.604363203048706\n",
      "epoch 123, loss 0.03246770799160004, R2 -0.3169980049133301\n",
      "Eval loss 0.0495661161839962, R2 -0.5961388349533081\n",
      "epoch 124, loss 0.032322801649570465, R2 -0.31112051010131836\n",
      "Eval loss 0.04931558668613434, R2 -0.5880712270736694\n",
      "epoch 125, loss 0.03218105435371399, R2 -0.30537140369415283\n",
      "Eval loss 0.04906986281275749, R2 -0.5801578760147095\n",
      "epoch 126, loss 0.03204242140054703, R2 -0.29974842071533203\n",
      "Eval loss 0.0488288439810276, R2 -0.5723971128463745\n",
      "epoch 127, loss 0.0319068469107151, R2 -0.294248104095459\n",
      "Eval loss 0.04859250411391258, R2 -0.5647873878479004\n",
      "epoch 128, loss 0.031774286180734634, R2 -0.28887152671813965\n",
      "Eval loss 0.04836072772741318, R2 -0.5573232173919678\n",
      "epoch 129, loss 0.031644679605960846, R2 -0.2836141586303711\n",
      "Eval loss 0.04813345894217491, R2 -0.5500041246414185\n",
      "epoch 130, loss 0.03151797130703926, R2 -0.2784750461578369\n",
      "Eval loss 0.04791061580181122, R2 -0.5428279638290405\n",
      "epoch 131, loss 0.031394101679325104, R2 -0.27344846725463867\n",
      "Eval loss 0.047692108899354935, R2 -0.5357916355133057\n",
      "epoch 132, loss 0.0312730111181736, R2 -0.26853859424591064\n",
      "Eval loss 0.04747786000370979, R2 -0.5288923978805542\n",
      "epoch 133, loss 0.031154634431004524, R2 -0.2637368440628052\n",
      "Eval loss 0.04726777970790863, R2 -0.5221272706985474\n",
      "epoch 134, loss 0.031038912013173103, R2 -0.25904130935668945\n",
      "Eval loss 0.04706178605556488, R2 -0.5154938697814941\n",
      "epoch 135, loss 0.030925774946808815, R2 -0.25445258617401123\n",
      "Eval loss 0.046859804540872574, R2 -0.5089895725250244\n",
      "epoch 136, loss 0.030815158039331436, R2 -0.24996602535247803\n",
      "Eval loss 0.04666171967983246, R2 -0.5026108026504517\n",
      "epoch 137, loss 0.03070700168609619, R2 -0.24557888507843018\n",
      "Eval loss 0.046467460691928864, R2 -0.4963552951812744\n",
      "epoch 138, loss 0.03060123696923256, R2 -0.2412881851196289\n",
      "Eval loss 0.04627694934606552, R2 -0.49022042751312256\n",
      "epoch 139, loss 0.030497800558805466, R2 -0.23709344863891602\n",
      "Eval loss 0.04609008878469467, R2 -0.4842034578323364\n",
      "epoch 140, loss 0.03039662539958954, R2 -0.23298895359039307\n",
      "Eval loss 0.04590679332613945, R2 -0.47830045223236084\n",
      "epoch 141, loss 0.030297646299004555, R2 -0.2289741039276123\n",
      "Eval loss 0.045726969838142395, R2 -0.4725116491317749\n",
      "epoch 142, loss 0.03020080365240574, R2 -0.2250458002090454\n",
      "Eval loss 0.045550551265478134, R2 -0.46682918071746826\n",
      "epoch 143, loss 0.03010602481663227, R2 -0.2212008237838745\n",
      "Eval loss 0.045377425849437714, R2 -0.4612537622451782\n",
      "epoch 144, loss 0.030013252049684525, R2 -0.2174379825592041\n",
      "Eval loss 0.04520753398537636, R2 -0.45578277111053467\n",
      "epoch 145, loss 0.029922431334853172, R2 -0.2137540578842163\n",
      "Eval loss 0.0450407937169075, R2 -0.45041346549987793\n",
      "epoch 146, loss 0.029833490028977394, R2 -0.2101449966430664\n",
      "Eval loss 0.04487710818648338, R2 -0.4451420307159424\n",
      "epoch 147, loss 0.02974637970328331, R2 -0.20661282539367676\n",
      "Eval loss 0.04471640661358833, R2 -0.43996739387512207\n",
      "epoch 148, loss 0.029661040753126144, R2 -0.20315122604370117\n",
      "Eval loss 0.04455861449241638, R2 -0.4348866939544678\n",
      "epoch 149, loss 0.029577407985925674, R2 -0.19975876808166504\n",
      "Eval loss 0.04440365359187126, R2 -0.42989611625671387\n",
      "epoch 150, loss 0.029495442286133766, R2 -0.19643354415893555\n",
      "Eval loss 0.044251441955566406, R2 -0.4249945878982544\n",
      "epoch 151, loss 0.02941507287323475, R2 -0.19317352771759033\n",
      "Eval loss 0.044101908802986145, R2 -0.42017972469329834\n",
      "epoch 152, loss 0.029336264356970787, R2 -0.18997764587402344\n",
      "Eval loss 0.043954987078905106, R2 -0.41544806957244873\n",
      "epoch 153, loss 0.029258955270051956, R2 -0.18684124946594238\n",
      "Eval loss 0.04381061717867851, R2 -0.4107985496520996\n",
      "epoch 154, loss 0.029183100908994675, R2 -0.1837635040283203\n",
      "Eval loss 0.04366869851946831, R2 -0.4062298536300659\n",
      "epoch 155, loss 0.02910865843296051, R2 -0.18074464797973633\n",
      "Eval loss 0.0435292012989521, R2 -0.4017367362976074\n",
      "epoch 156, loss 0.029035573825240135, R2 -0.1777801513671875\n",
      "Eval loss 0.043392032384872437, R2 -0.3973209857940674\n",
      "epoch 157, loss 0.028963806107640266, R2 -0.17486906051635742\n",
      "Eval loss 0.043257150799036026, R2 -0.39297664165496826\n",
      "epoch 158, loss 0.02889331988990307, R2 -0.17200994491577148\n",
      "Eval loss 0.0431244894862175, R2 -0.38870418071746826\n",
      "epoch 159, loss 0.028824063017964363, R2 -0.16919958591461182\n",
      "Eval loss 0.042993973940610886, R2 -0.3845013380050659\n",
      "epoch 160, loss 0.028756000101566315, R2 -0.16643881797790527\n",
      "Eval loss 0.0428655631840229, R2 -0.38036656379699707\n",
      "epoch 161, loss 0.028689097613096237, R2 -0.16372597217559814\n",
      "Eval loss 0.04273919016122818, R2 -0.3762967586517334\n",
      "epoch 162, loss 0.02862331084907055, R2 -0.1610574722290039\n",
      "Eval loss 0.04261481389403343, R2 -0.3722914457321167\n",
      "epoch 163, loss 0.028558610007166862, R2 -0.15843284130096436\n",
      "Eval loss 0.04249236732721329, R2 -0.36834967136383057\n",
      "epoch 164, loss 0.02849496155977249, R2 -0.1558516025543213\n",
      "Eval loss 0.042371805757284164, R2 -0.36446619033813477\n",
      "epoch 165, loss 0.028432324528694153, R2 -0.15330982208251953\n",
      "Eval loss 0.04225306957960129, R2 -0.36064374446868896\n",
      "epoch 166, loss 0.028370676562190056, R2 -0.150809645652771\n",
      "Eval loss 0.04213612526655197, R2 -0.3568766117095947\n",
      "epoch 167, loss 0.028309978544712067, R2 -0.14834630489349365\n",
      "Eval loss 0.04202091693878174, R2 -0.35316669940948486\n",
      "epoch 168, loss 0.028250209987163544, R2 -0.14592182636260986\n",
      "Eval loss 0.04190739989280701, R2 -0.3495112657546997\n",
      "epoch 169, loss 0.0281913373619318, R2 -0.14353418350219727\n",
      "Eval loss 0.04179553687572479, R2 -0.345908522605896\n",
      "epoch 170, loss 0.0281333327293396, R2 -0.1411813497543335\n",
      "Eval loss 0.04168527573347092, R2 -0.3423583507537842\n",
      "epoch 171, loss 0.028076166287064552, R2 -0.1388634443283081\n",
      "Eval loss 0.041576579213142395, R2 -0.3388584852218628\n",
      "epoch 172, loss 0.028019823133945465, R2 -0.13657844066619873\n",
      "Eval loss 0.041469406336545944, R2 -0.3354068994522095\n",
      "epoch 173, loss 0.0279642753303051, R2 -0.13432466983795166\n",
      "Eval loss 0.04136371985077858, R2 -0.33200347423553467\n",
      "epoch 174, loss 0.027909493073821068, R2 -0.13210296630859375\n",
      "Eval loss 0.04125947877764702, R2 -0.32864630222320557\n",
      "epoch 175, loss 0.027855457738041878, R2 -0.12990987300872803\n",
      "Eval loss 0.041156649589538574, R2 -0.3253345489501953\n",
      "epoch 176, loss 0.027802150696516037, R2 -0.12774884700775146\n",
      "Eval loss 0.04105519875884056, R2 -0.32206881046295166\n",
      "epoch 177, loss 0.027749544009566307, R2 -0.12561440467834473\n",
      "Eval loss 0.0409550815820694, R2 -0.3188447952270508\n",
      "epoch 178, loss 0.027697624638676643, R2 -0.12350761890411377\n",
      "Eval loss 0.04085627570748329, R2 -0.31566309928894043\n",
      "epoch 179, loss 0.027646372094750404, R2 -0.1214299201965332\n",
      "Eval loss 0.04075874388217926, R2 -0.31252193450927734\n",
      "epoch 180, loss 0.02759576588869095, R2 -0.11937713623046875\n",
      "Eval loss 0.040662456303834915, R2 -0.309421181678772\n",
      "epoch 181, loss 0.027545785531401634, R2 -0.1173485517501831\n",
      "Eval loss 0.04056738317012787, R2 -0.30636096000671387\n",
      "epoch 182, loss 0.027496416121721268, R2 -0.11534678936004639\n",
      "Eval loss 0.04047349840402603, R2 -0.30333757400512695\n",
      "epoch 183, loss 0.027447642758488655, R2 -0.11336839199066162\n",
      "Eval loss 0.040380772203207016, R2 -0.3003500699996948\n",
      "epoch 184, loss 0.027399446815252304, R2 -0.11141252517700195\n",
      "Eval loss 0.04028916358947754, R2 -0.2974003553390503\n",
      "epoch 185, loss 0.02735181339085102, R2 -0.10947990417480469\n",
      "Eval loss 0.0401986688375473, R2 -0.2944866418838501\n",
      "epoch 186, loss 0.02730473130941391, R2 -0.10757136344909668\n",
      "Eval loss 0.04010924696922302, R2 -0.2916066646575928\n",
      "epoch 187, loss 0.02725818008184433, R2 -0.1056830883026123\n",
      "Eval loss 0.04002087190747261, R2 -0.2887611389160156\n",
      "epoch 188, loss 0.027212144806981087, R2 -0.10381495952606201\n",
      "Eval loss 0.03993352875113487, R2 -0.28594815731048584\n",
      "epoch 189, loss 0.027166619896888733, R2 -0.10196948051452637\n",
      "Eval loss 0.03984718397259712, R2 -0.28316760063171387\n",
      "epoch 190, loss 0.027121586725115776, R2 -0.1001424789428711\n",
      "Eval loss 0.039761826395988464, R2 -0.28041934967041016\n",
      "epoch 191, loss 0.027077030390501022, R2 -0.09833431243896484\n",
      "Eval loss 0.03967742249369621, R2 -0.2776998281478882\n",
      "epoch 192, loss 0.027032949030399323, R2 -0.09654700756072998\n",
      "Eval loss 0.039593957364559174, R2 -0.2750135660171509\n",
      "epoch 193, loss 0.026989324018359184, R2 -0.09477746486663818\n",
      "Eval loss 0.039511408656835556, R2 -0.2723557949066162\n",
      "epoch 194, loss 0.026946144178509712, R2 -0.09302544593811035\n",
      "Eval loss 0.03942975774407387, R2 -0.2697252035140991\n",
      "epoch 195, loss 0.026903396472334862, R2 -0.09129190444946289\n",
      "Eval loss 0.03934897482395172, R2 -0.2671254873275757\n",
      "epoch 196, loss 0.026861073449254036, R2 -0.08957600593566895\n",
      "Eval loss 0.039269059896469116, R2 -0.26455187797546387\n",
      "epoch 197, loss 0.026819167658686638, R2 -0.08787572383880615\n",
      "Eval loss 0.03918997570872307, R2 -0.26200413703918457\n",
      "epoch 198, loss 0.026777660474181175, R2 -0.08619201183319092\n",
      "Eval loss 0.03911171853542328, R2 -0.25948405265808105\n",
      "epoch 199, loss 0.0267365500330925, R2 -0.08452320098876953\n",
      "Eval loss 0.03903426229953766, R2 -0.2569897174835205\n",
      "epoch 200, loss 0.026695823296904564, R2 -0.08287084102630615\n",
      "Eval loss 0.038957592099905014, R2 -0.25452160835266113\n",
      "epoch 201, loss 0.02665547840297222, R2 -0.08123552799224854\n",
      "Eval loss 0.03888169676065445, R2 -0.25207674503326416\n",
      "epoch 202, loss 0.02661549486219883, R2 -0.07961320877075195\n",
      "Eval loss 0.038806553930044174, R2 -0.24965739250183105\n",
      "epoch 203, loss 0.026575878262519836, R2 -0.07800579071044922\n",
      "Eval loss 0.038732144981622696, R2 -0.2472609281539917\n",
      "epoch 204, loss 0.0265366081148386, R2 -0.07641291618347168\n",
      "Eval loss 0.03865846246480942, R2 -0.24488818645477295\n",
      "epoch 205, loss 0.02649768628180027, R2 -0.07483494281768799\n",
      "Eval loss 0.038585495203733444, R2 -0.2425391674041748\n",
      "epoch 206, loss 0.026459097862243652, R2 -0.07327008247375488\n",
      "Eval loss 0.03851321339607239, R2 -0.24021124839782715\n",
      "epoch 207, loss 0.0264208372682333, R2 -0.0717172622680664\n",
      "Eval loss 0.03844162076711655, R2 -0.2379053831100464\n",
      "epoch 208, loss 0.026382897049188614, R2 -0.07017874717712402\n",
      "Eval loss 0.03837069123983383, R2 -0.23562097549438477\n",
      "epoch 209, loss 0.026345275342464447, R2 -0.06865262985229492\n",
      "Eval loss 0.038300417363643646, R2 -0.2333592176437378\n",
      "epoch 210, loss 0.026307959109544754, R2 -0.06713902950286865\n",
      "Eval loss 0.038230787962675095, R2 -0.2311164140701294\n",
      "epoch 211, loss 0.026270942762494087, R2 -0.06563746929168701\n",
      "Eval loss 0.038161784410476685, R2 -0.22889411449432373\n",
      "epoch 212, loss 0.026234231889247894, R2 -0.06414830684661865\n",
      "Eval loss 0.03809340298175812, R2 -0.22669243812561035\n",
      "epoch 213, loss 0.026197800412774086, R2 -0.06267094612121582\n",
      "Eval loss 0.0380256287753582, R2 -0.22450947761535645\n",
      "epoch 214, loss 0.02616165578365326, R2 -0.06120443344116211\n",
      "Eval loss 0.03795844689011574, R2 -0.22234606742858887\n",
      "epoch 215, loss 0.026125788688659668, R2 -0.059749484062194824\n",
      "Eval loss 0.037891849875450134, R2 -0.2202014923095703\n",
      "epoch 216, loss 0.026090197265148163, R2 -0.05830585956573486\n",
      "Eval loss 0.037825822830200195, R2 -0.21807539463043213\n",
      "epoch 217, loss 0.02605486661195755, R2 -0.056871891021728516\n",
      "Eval loss 0.03776036202907562, R2 -0.21596693992614746\n",
      "epoch 218, loss 0.026019804179668427, R2 -0.055450439453125\n",
      "Eval loss 0.03769545257091522, R2 -0.21387720108032227\n",
      "epoch 219, loss 0.0259849913418293, R2 -0.054038286209106445\n",
      "Eval loss 0.0376310832798481, R2 -0.21180391311645508\n",
      "epoch 220, loss 0.02595043182373047, R2 -0.05263650417327881\n",
      "Eval loss 0.037567250430583954, R2 -0.2097487449645996\n",
      "epoch 221, loss 0.025916125625371933, R2 -0.051244497299194336\n",
      "Eval loss 0.037503939121961594, R2 -0.20771002769470215\n",
      "epoch 222, loss 0.0258820578455925, R2 -0.04986298084259033\n",
      "Eval loss 0.03744114190340042, R2 -0.2056877613067627\n",
      "epoch 223, loss 0.025848226621747017, R2 -0.04849064350128174\n",
      "Eval loss 0.03737885132431984, R2 -0.20368218421936035\n",
      "epoch 224, loss 0.02581462822854519, R2 -0.04712784290313721\n",
      "Eval loss 0.037317052483558655, R2 -0.2016918659210205\n",
      "epoch 225, loss 0.025781257078051567, R2 -0.04577493667602539\n",
      "Eval loss 0.037255749106407166, R2 -0.19971811771392822\n",
      "epoch 226, loss 0.02574811689555645, R2 -0.044429898262023926\n",
      "Eval loss 0.03719491884112358, R2 -0.19775891304016113\n",
      "epoch 227, loss 0.025715194642543793, R2 -0.04309403896331787\n",
      "Eval loss 0.0371345654129982, R2 -0.1958153247833252\n",
      "epoch 228, loss 0.025682486593723297, R2 -0.04176819324493408\n",
      "Eval loss 0.03707467392086983, R2 -0.19388675689697266\n",
      "epoch 229, loss 0.025649992749094963, R2 -0.040450096130371094\n",
      "Eval loss 0.03701523691415787, R2 -0.19197237491607666\n",
      "epoch 230, loss 0.025617709383368492, R2 -0.03914010524749756\n",
      "Eval loss 0.03695625066757202, R2 -0.19007325172424316\n",
      "epoch 231, loss 0.025585629045963287, R2 -0.03783762454986572\n",
      "Eval loss 0.036897704005241394, R2 -0.18818867206573486\n",
      "epoch 232, loss 0.025553753599524498, R2 -0.0365450382232666\n",
      "Eval loss 0.03683959320187569, R2 -0.1863166093826294\n",
      "epoch 233, loss 0.025522079318761826, R2 -0.03526103496551514\n",
      "Eval loss 0.036781907081604004, R2 -0.18445897102355957\n",
      "epoch 234, loss 0.025490596890449524, R2 -0.03398323059082031\n",
      "Eval loss 0.03672464191913605, R2 -0.1826152801513672\n",
      "epoch 235, loss 0.025459308177232742, R2 -0.03271484375\n",
      "Eval loss 0.03666779398918152, R2 -0.18078386783599854\n",
      "epoch 236, loss 0.025428207591176033, R2 -0.03145325183868408\n",
      "Eval loss 0.036611348390579224, R2 -0.17896664142608643\n",
      "epoch 237, loss 0.025397291406989098, R2 -0.030199170112609863\n",
      "Eval loss 0.03655530512332916, R2 -0.17716193199157715\n",
      "epoch 238, loss 0.025366559624671936, R2 -0.028952598571777344\n",
      "Eval loss 0.03649965673685074, R2 -0.17536985874176025\n",
      "epoch 239, loss 0.02533600851893425, R2 -0.027712583541870117\n",
      "Eval loss 0.03644439950585365, R2 -0.17359042167663574\n",
      "epoch 240, loss 0.02530563436448574, R2 -0.026480913162231445\n",
      "Eval loss 0.03638952225446701, R2 -0.17182326316833496\n",
      "epoch 241, loss 0.02527543157339096, R2 -0.02525651454925537\n",
      "Eval loss 0.03633502498269081, R2 -0.17006981372833252\n",
      "epoch 242, loss 0.02524540387094021, R2 -0.02403736114501953\n",
      "Eval loss 0.03628089278936386, R2 -0.16832518577575684\n",
      "epoch 243, loss 0.025215543806552887, R2 -0.022826552391052246\n",
      "Eval loss 0.03622712939977646, R2 -0.1665949821472168\n",
      "epoch 244, loss 0.025185851380228996, R2 -0.021622538566589355\n",
      "Eval loss 0.03617372736334801, R2 -0.16487419605255127\n",
      "epoch 245, loss 0.02515632100403309, R2 -0.02042388916015625\n",
      "Eval loss 0.03612067550420761, R2 -0.16316592693328857\n",
      "epoch 246, loss 0.025126952677965164, R2 -0.01923346519470215\n",
      "Eval loss 0.036067984998226166, R2 -0.16146910190582275\n",
      "epoch 247, loss 0.025097744539380074, R2 -0.018047809600830078\n",
      "Eval loss 0.03601562976837158, R2 -0.159784197807312\n",
      "epoch 248, loss 0.02506869100034237, R2 -0.016868948936462402\n",
      "Eval loss 0.03596361726522446, R2 -0.15810823440551758\n",
      "epoch 249, loss 0.02503979206085205, R2 -0.015697836875915527\n",
      "Eval loss 0.03591194003820419, R2 -0.15644454956054688\n",
      "epoch 250, loss 0.02501104772090912, R2 -0.014531970024108887\n",
      "Eval loss 0.035860590636730194, R2 -0.15479052066802979\n",
      "epoch 251, loss 0.024982446804642677, R2 -0.013371825218200684\n",
      "Eval loss 0.03580956906080246, R2 -0.15314757823944092\n",
      "epoch 252, loss 0.02495400235056877, R2 -0.012217879295349121\n",
      "Eval loss 0.03575886785984039, R2 -0.15151488780975342\n",
      "epoch 253, loss 0.024925697594881058, R2 -0.011068940162658691\n",
      "Eval loss 0.035708483308553696, R2 -0.14989233016967773\n",
      "epoch 254, loss 0.024897539988160133, R2 -0.009927988052368164\n",
      "Eval loss 0.03565841540694237, R2 -0.14827966690063477\n",
      "epoch 255, loss 0.024869520217180252, R2 -0.008791923522949219\n",
      "Eval loss 0.035608645528554916, R2 -0.14667737483978271\n",
      "epoch 256, loss 0.024841642007231712, R2 -0.007660269737243652\n",
      "Eval loss 0.03555918484926224, R2 -0.14508461952209473\n",
      "epoch 257, loss 0.024813899770379066, R2 -0.00653529167175293\n",
      "Eval loss 0.035510025918483734, R2 -0.14350199699401855\n",
      "epoch 258, loss 0.024786299094557762, R2 -0.00541532039642334\n",
      "Eval loss 0.03546116128563881, R2 -0.1419275999069214\n",
      "epoch 259, loss 0.024758830666542053, R2 -0.004300355911254883\n",
      "Eval loss 0.035412587225437164, R2 -0.1403648853302002\n",
      "epoch 260, loss 0.02473149262368679, R2 -0.0031914710998535156\n",
      "Eval loss 0.0353643000125885, R2 -0.13880884647369385\n",
      "epoch 261, loss 0.024704284965991974, R2 -0.002087831497192383\n",
      "Eval loss 0.03531630337238312, R2 -0.13726365566253662\n",
      "epoch 262, loss 0.024677209556102753, R2 -0.0009895563125610352\n",
      "Eval loss 0.03526858240365982, R2 -0.1357264518737793\n",
      "epoch 263, loss 0.02465026080608368, R2 0.00010317564010620117\n",
      "Eval loss 0.03522113338112831, R2 -0.13419854640960693\n",
      "epoch 264, loss 0.024623436853289604, R2 0.0011920332908630371\n",
      "Eval loss 0.035173967480659485, R2 -0.13267970085144043\n",
      "epoch 265, loss 0.02459673583507538, R2 0.0022739768028259277\n",
      "Eval loss 0.03512706607580185, R2 -0.13116967678070068\n",
      "epoch 266, loss 0.024570155888795853, R2 0.0033521652221679688\n",
      "Eval loss 0.035080429166555405, R2 -0.1296672821044922\n",
      "epoch 267, loss 0.024543702602386475, R2 0.004425168037414551\n",
      "Eval loss 0.03503406420350075, R2 -0.12817513942718506\n",
      "epoch 268, loss 0.02451736479997635, R2 0.005493521690368652\n",
      "Eval loss 0.03498794883489609, R2 -0.12668955326080322\n",
      "epoch 269, loss 0.024491148069500923, R2 0.006556987762451172\n",
      "Eval loss 0.03494209423661232, R2 -0.12521326541900635\n",
      "epoch 270, loss 0.02446504309773445, R2 0.007616221904754639\n",
      "Eval loss 0.03489649295806885, R2 -0.12374448776245117\n",
      "epoch 271, loss 0.02443905919790268, R2 0.008669912815093994\n",
      "Eval loss 0.03485114499926567, R2 -0.12228405475616455\n",
      "epoch 272, loss 0.024413185194134712, R2 0.009719431400299072\n",
      "Eval loss 0.034806035459041595, R2 -0.12083160877227783\n",
      "epoch 273, loss 0.024387426674365997, R2 0.010764658451080322\n",
      "Eval loss 0.034761179238557816, R2 -0.11938810348510742\n",
      "epoch 274, loss 0.024361779913306236, R2 0.01180422306060791\n",
      "Eval loss 0.03471655771136284, R2 -0.1179502010345459\n",
      "epoch 275, loss 0.02433624304831028, R2 0.012841224670410156\n",
      "Eval loss 0.03467217832803726, R2 -0.11652243137359619\n",
      "epoch 276, loss 0.02431081235408783, R2 0.013871967792510986\n",
      "Eval loss 0.03462803736329079, R2 -0.11509990692138672\n",
      "epoch 277, loss 0.024285493418574333, R2 0.01489943265914917\n",
      "Eval loss 0.03458411991596222, R2 -0.11368536949157715\n",
      "epoch 278, loss 0.024260276928544044, R2 0.015921831130981445\n",
      "Eval loss 0.03454044461250305, R2 -0.11227917671203613\n",
      "epoch 279, loss 0.024235164746642113, R2 0.01694047451019287\n",
      "Eval loss 0.034496985375881195, R2 -0.11087942123413086\n",
      "epoch 280, loss 0.024210160598158836, R2 0.01795482635498047\n",
      "Eval loss 0.03445375710725784, R2 -0.10948741436004639\n",
      "epoch 281, loss 0.02418525703251362, R2 0.01896458864212036\n",
      "Eval loss 0.0344107486307621, R2 -0.10810244083404541\n",
      "epoch 282, loss 0.02416045591235161, R2 0.019970953464508057\n",
      "Eval loss 0.03436795994639397, R2 -0.10672545433044434\n",
      "epoch 283, loss 0.024135757237672806, R2 0.020972847938537598\n",
      "Eval loss 0.03432539105415344, R2 -0.1053537130355835\n",
      "epoch 284, loss 0.024111159145832062, R2 0.021970629692077637\n",
      "Eval loss 0.03428303450345993, R2 -0.10398972034454346\n",
      "epoch 285, loss 0.02408665604889393, R2 0.02296525239944458\n",
      "Eval loss 0.03424089029431343, R2 -0.10263252258300781\n",
      "epoch 286, loss 0.024062251672148705, R2 0.023954451084136963\n",
      "Eval loss 0.034198954701423645, R2 -0.10128223896026611\n",
      "epoch 287, loss 0.02403794601559639, R2 0.0249403715133667\n",
      "Eval loss 0.03415722772479057, R2 -0.0999380350112915\n",
      "epoch 288, loss 0.024013733491301537, R2 0.02592325210571289\n",
      "Eval loss 0.034115709364414215, R2 -0.09860146045684814\n",
      "epoch 289, loss 0.023989615961909294, R2 0.026900827884674072\n",
      "Eval loss 0.034074388444423676, R2 -0.09727060794830322\n",
      "epoch 290, loss 0.023965591564774513, R2 0.02787637710571289\n",
      "Eval loss 0.03403326869010925, R2 -0.09594666957855225\n",
      "epoch 291, loss 0.023941662162542343, R2 0.028845608234405518\n",
      "Eval loss 0.033992353826761246, R2 -0.09462916851043701\n",
      "epoch 292, loss 0.023917825892567635, R2 0.02981323003768921\n",
      "Eval loss 0.03395162895321846, R2 -0.09331774711608887\n",
      "epoch 293, loss 0.02389407716691494, R2 0.030776560306549072\n",
      "Eval loss 0.033911097794771194, R2 -0.09201228618621826\n",
      "epoch 294, loss 0.023870419710874557, R2 0.031736910343170166\n",
      "Eval loss 0.03387076407670975, R2 -0.0907137393951416\n",
      "epoch 295, loss 0.023846853524446487, R2 0.032691001892089844\n",
      "Eval loss 0.03383062034845352, R2 -0.08942091464996338\n",
      "epoch 296, loss 0.023823373019695282, R2 0.033644139766693115\n",
      "Eval loss 0.03379065915942192, R2 -0.0881345272064209\n",
      "epoch 297, loss 0.02379998192191124, R2 0.03459370136260986\n",
      "Eval loss 0.03375089168548584, R2 -0.0868535041809082\n",
      "epoch 298, loss 0.023776674643158913, R2 0.03553915023803711\n",
      "Eval loss 0.033711303025484085, R2 -0.0855783224105835\n",
      "epoch 299, loss 0.0237534549087286, R2 0.03648030757904053\n",
      "Eval loss 0.03367190062999725, R2 -0.08431017398834229\n",
      "epoch 300, loss 0.02373032085597515, R2 0.03741943836212158\n",
      "Eval loss 0.03363267332315445, R2 -0.08304667472839355\n",
      "epoch 301, loss 0.023707270622253418, R2 0.03835481405258179\n",
      "Eval loss 0.03359363228082657, R2 -0.08179044723510742\n",
      "epoch 302, loss 0.0236843042075634, R2 0.03928530216217041\n",
      "Eval loss 0.03355475887656212, R2 -0.08053767681121826\n",
      "epoch 303, loss 0.02366141974925995, R2 0.04021388292312622\n",
      "Eval loss 0.033516064286231995, R2 -0.07929158210754395\n",
      "epoch 304, loss 0.023638620972633362, R2 0.04113835096359253\n",
      "Eval loss 0.0334775410592556, R2 -0.07805109024047852\n",
      "epoch 305, loss 0.023615898564457893, R2 0.042059242725372314\n",
      "Eval loss 0.03343919664621353, R2 -0.07681584358215332\n",
      "epoch 306, loss 0.02359325811266899, R2 0.042977988719940186\n",
      "Eval loss 0.033401019871234894, R2 -0.07558691501617432\n",
      "epoch 307, loss 0.023570699617266655, R2 0.04389452934265137\n",
      "Eval loss 0.03336300700902939, R2 -0.0743628740310669\n",
      "epoch 308, loss 0.023548219352960587, R2 0.04480534791946411\n",
      "Eval loss 0.03332516551017761, R2 -0.07314419746398926\n",
      "epoch 309, loss 0.023525817319750786, R2 0.04571330547332764\n",
      "Eval loss 0.03328748792409897, R2 -0.0719308853149414\n",
      "epoch 310, loss 0.023503493517637253, R2 0.04661917686462402\n",
      "Eval loss 0.03324997052550316, R2 -0.07072246074676514\n",
      "epoch 311, loss 0.023481249809265137, R2 0.04752182960510254\n",
      "Eval loss 0.03321261703968048, R2 -0.0695199966430664\n",
      "epoch 312, loss 0.02345907688140869, R2 0.048421263694763184\n",
      "Eval loss 0.03317542374134064, R2 -0.06832218170166016\n",
      "epoch 313, loss 0.023436982184648514, R2 0.04931747913360596\n",
      "Eval loss 0.03313838690519333, R2 -0.06712949275970459\n",
      "epoch 314, loss 0.023414963856339455, R2 0.05021059513092041\n",
      "Eval loss 0.033101510256528854, R2 -0.06594264507293701\n",
      "epoch 315, loss 0.023393020033836365, R2 0.0511014461517334\n",
      "Eval loss 0.03306478634476662, R2 -0.06475937366485596\n",
      "epoch 316, loss 0.023371152579784393, R2 0.05198770761489868\n",
      "Eval loss 0.033028215169906616, R2 -0.0635824203491211\n",
      "epoch 317, loss 0.023349354043602943, R2 0.05287271738052368\n",
      "Eval loss 0.03299180045723915, R2 -0.062409162521362305\n",
      "epoch 318, loss 0.023327631875872612, R2 0.0537530779838562\n",
      "Eval loss 0.03295553848147392, R2 -0.06124138832092285\n",
      "epoch 319, loss 0.023305978626012802, R2 0.05463212728500366\n",
      "Eval loss 0.032919418066740036, R2 -0.06007814407348633\n",
      "epoch 320, loss 0.02328440174460411, R2 0.05550664663314819\n",
      "Eval loss 0.032883454114198685, R2 -0.05892050266265869\n",
      "epoch 321, loss 0.02326289378106594, R2 0.056379079818725586\n",
      "Eval loss 0.03284763544797897, R2 -0.05776667594909668\n",
      "epoch 322, loss 0.02324145846068859, R2 0.05724823474884033\n",
      "Eval loss 0.0328119620680809, R2 -0.05661797523498535\n",
      "epoch 323, loss 0.023220090195536613, R2 0.05811494588851929\n",
      "Eval loss 0.03277643024921417, R2 -0.05547380447387695\n",
      "epoch 324, loss 0.023198792710900307, R2 0.0589788556098938\n",
      "Eval loss 0.03274104744195938, R2 -0.054334044456481934\n",
      "epoch 325, loss 0.02317756414413452, R2 0.05984067916870117\n",
      "Eval loss 0.03270580619573593, R2 -0.0531994104385376\n",
      "epoch 326, loss 0.023156406357884407, R2 0.060699284076690674\n",
      "Eval loss 0.032670699059963226, R2 -0.05206894874572754\n",
      "epoch 327, loss 0.023135313764214516, R2 0.061554133892059326\n",
      "Eval loss 0.03263573348522186, R2 -0.05094337463378906\n",
      "epoch 328, loss 0.023114291951060295, R2 0.062406837940216064\n",
      "Eval loss 0.03260090947151184, R2 -0.04982161521911621\n",
      "epoch 329, loss 0.023093335330486298, R2 0.06325656175613403\n",
      "Eval loss 0.03256622329354286, R2 -0.0487055778503418\n",
      "epoch 330, loss 0.023072443902492523, R2 0.06410437822341919\n",
      "Eval loss 0.03253167122602463, R2 -0.0475916862487793\n",
      "epoch 331, loss 0.02305162139236927, R2 0.06494933366775513\n",
      "Eval loss 0.03249724954366684, R2 -0.04648315906524658\n",
      "epoch 332, loss 0.02303086221218109, R2 0.06579101085662842\n",
      "Eval loss 0.032462965697050095, R2 -0.04537975788116455\n",
      "epoch 333, loss 0.023010168224573135, R2 0.06663048267364502\n",
      "Eval loss 0.032428812235593796, R2 -0.044280052185058594\n",
      "epoch 334, loss 0.022989537566900253, R2 0.06746768951416016\n",
      "Eval loss 0.03239478915929794, R2 -0.04318439960479736\n",
      "epoch 335, loss 0.022968975827097893, R2 0.06830096244812012\n",
      "Eval loss 0.032360900193452835, R2 -0.04209268093109131\n",
      "epoch 336, loss 0.022948475554585457, R2 0.0691329836845398\n",
      "Eval loss 0.03232713043689728, R2 -0.04100501537322998\n",
      "epoch 337, loss 0.022928036749362946, R2 0.06996268033981323\n",
      "Eval loss 0.03229350224137306, R2 -0.039922356605529785\n",
      "epoch 338, loss 0.022907664999365807, R2 0.07078945636749268\n",
      "Eval loss 0.0322599932551384, R2 -0.03884422779083252\n",
      "epoch 339, loss 0.022887350991368294, R2 0.07161307334899902\n",
      "Eval loss 0.03222660720348358, R2 -0.03776848316192627\n",
      "epoch 340, loss 0.022867102175951004, R2 0.0724344253540039\n",
      "Eval loss 0.032193347811698914, R2 -0.03669750690460205\n",
      "epoch 341, loss 0.02284691296517849, R2 0.07325226068496704\n",
      "Eval loss 0.03216021507978439, R2 -0.035630226135253906\n",
      "epoch 342, loss 0.0228267852216959, R2 0.074069082736969\n",
      "Eval loss 0.032127201557159424, R2 -0.03456676006317139\n",
      "epoch 343, loss 0.022806722670793533, R2 0.07488358020782471\n",
      "Eval loss 0.032094310969114304, R2 -0.03350794315338135\n",
      "epoch 344, loss 0.022786717861890793, R2 0.0756940245628357\n",
      "Eval loss 0.032061539590358734, R2 -0.032453060150146484\n",
      "epoch 345, loss 0.02276677079498768, R2 0.0765034556388855\n",
      "Eval loss 0.032028891146183014, R2 -0.031401872634887695\n",
      "epoch 346, loss 0.022746887058019638, R2 0.07731002569198608\n",
      "Eval loss 0.03199635446071625, R2 -0.030353546142578125\n",
      "epoch 347, loss 0.022727057337760925, R2 0.07811439037322998\n",
      "Eval loss 0.03196394070982933, R2 -0.029309391975402832\n",
      "epoch 348, loss 0.022707290947437286, R2 0.07891619205474854\n",
      "Eval loss 0.031931642442941666, R2 -0.02827000617980957\n",
      "epoch 349, loss 0.022687582299113274, R2 0.07971525192260742\n",
      "Eval loss 0.03189946338534355, R2 -0.027233123779296875\n",
      "epoch 350, loss 0.022667933255434036, R2 0.08051341772079468\n",
      "Eval loss 0.03186739236116409, R2 -0.02620065212249756\n",
      "epoch 351, loss 0.022648340091109276, R2 0.08130747079849243\n",
      "Eval loss 0.03183543682098389, R2 -0.025171637535095215\n",
      "epoch 352, loss 0.022628802806138992, R2 0.08209991455078125\n",
      "Eval loss 0.03180360049009323, R2 -0.02414679527282715\n",
      "epoch 353, loss 0.022609325125813484, R2 0.08289003372192383\n",
      "Eval loss 0.03177186846733093, R2 -0.02312469482421875\n",
      "epoch 354, loss 0.022589903324842453, R2 0.08367747068405151\n",
      "Eval loss 0.031740251928567886, R2 -0.022106528282165527\n",
      "epoch 355, loss 0.022570539265871048, R2 0.08446401357650757\n",
      "Eval loss 0.031708747148513794, R2 -0.02109229564666748\n",
      "epoch 356, loss 0.02255123108625412, R2 0.08524686098098755\n",
      "Eval loss 0.03167735040187836, R2 -0.0200808048248291\n",
      "epoch 357, loss 0.02253197692334652, R2 0.08602821826934814\n",
      "Eval loss 0.031646061688661575, R2 -0.0190737247467041\n",
      "epoch 358, loss 0.022512778639793396, R2 0.08680659532546997\n",
      "Eval loss 0.03161488100886345, R2 -0.01806926727294922\n",
      "epoch 359, loss 0.0224936343729496, R2 0.08758354187011719\n",
      "Eval loss 0.03158380463719368, R2 -0.01706850528717041\n",
      "epoch 360, loss 0.022474544122815132, R2 0.08835721015930176\n",
      "Eval loss 0.031552840024232864, R2 -0.016071081161499023\n",
      "epoch 361, loss 0.02245551161468029, R2 0.08912956714630127\n",
      "Eval loss 0.03152197599411011, R2 -0.015077471733093262\n",
      "epoch 362, loss 0.022436531260609627, R2 0.08989983797073364\n",
      "Eval loss 0.031491223722696304, R2 -0.014088153839111328\n",
      "epoch 363, loss 0.02241760492324829, R2 0.09066647291183472\n",
      "Eval loss 0.03146056830883026, R2 -0.013100028038024902\n",
      "epoch 364, loss 0.022398732602596283, R2 0.09143239259719849\n",
      "Eval loss 0.031430020928382874, R2 -0.012116312980651855\n",
      "epoch 365, loss 0.022379912436008453, R2 0.09219610691070557\n",
      "Eval loss 0.031399570405483246, R2 -0.011135578155517578\n",
      "epoch 366, loss 0.022361144423484802, R2 0.0929570198059082\n",
      "Eval loss 0.031369224190711975, R2 -0.010158658027648926\n",
      "epoch 367, loss 0.022342432290315628, R2 0.09371614456176758\n",
      "Eval loss 0.03133898228406906, R2 -0.009184718132019043\n",
      "epoch 368, loss 0.022323770448565483, R2 0.09447306394577026\n",
      "Eval loss 0.03130883350968361, R2 -0.008214235305786133\n",
      "epoch 369, loss 0.022305158898234367, R2 0.09522801637649536\n",
      "Eval loss 0.031278789043426514, R2 -0.00724637508392334\n",
      "epoch 370, loss 0.02228660136461258, R2 0.09598147869110107\n",
      "Eval loss 0.03124884143471718, R2 -0.0062819719314575195\n",
      "epoch 371, loss 0.02226809225976467, R2 0.09673190116882324\n",
      "Eval loss 0.0312189944088459, R2 -0.0053217411041259766\n",
      "epoch 372, loss 0.02224963717162609, R2 0.0974808931350708\n",
      "Eval loss 0.031189244240522385, R2 -0.004362821578979492\n",
      "epoch 373, loss 0.02223123051226139, R2 0.09822750091552734\n",
      "Eval loss 0.03115958720445633, R2 -0.0034078359603881836\n",
      "epoch 374, loss 0.022212877869606018, R2 0.09897160530090332\n",
      "Eval loss 0.031130027025938034, R2 -0.002455592155456543\n",
      "epoch 375, loss 0.022194571793079376, R2 0.09971380233764648\n",
      "Eval loss 0.031100565567612648, R2 -0.0015075206756591797\n",
      "epoch 376, loss 0.022176316007971764, R2 0.10045468807220459\n",
      "Eval loss 0.031071200966835022, R2 -0.0005615949630737305\n",
      "epoch 377, loss 0.02215811237692833, R2 0.10119235515594482\n",
      "Eval loss 0.03104192018508911, R2 0.0003813505172729492\n",
      "epoch 378, loss 0.022139955312013626, R2 0.10192924737930298\n",
      "Eval loss 0.03101273998618126, R2 0.0013210177421569824\n",
      "epoch 379, loss 0.02212185226380825, R2 0.10266357660293579\n",
      "Eval loss 0.03098365291953087, R2 0.002257704734802246\n",
      "epoch 380, loss 0.022103793919086456, R2 0.10339605808258057\n",
      "Eval loss 0.03095465712249279, R2 0.0031914114952087402\n",
      "epoch 381, loss 0.02208578586578369, R2 0.10412651300430298\n",
      "Eval loss 0.030925750732421875, R2 0.004122257232666016\n",
      "epoch 382, loss 0.022067826241254807, R2 0.10485506057739258\n",
      "Eval loss 0.030896933749318123, R2 0.005050241947174072\n",
      "epoch 383, loss 0.022049913182854652, R2 0.10558199882507324\n",
      "Eval loss 0.03086821362376213, R2 0.005975127220153809\n",
      "epoch 384, loss 0.02203204855322838, R2 0.10630559921264648\n",
      "Eval loss 0.030839579179883003, R2 0.006896853446960449\n",
      "epoch 385, loss 0.022014234215021133, R2 0.10702961683273315\n",
      "Eval loss 0.03081103414297104, R2 0.007816433906555176\n",
      "epoch 386, loss 0.02199646271765232, R2 0.10774946212768555\n",
      "Eval loss 0.03078257478773594, R2 0.008732497692108154\n",
      "epoch 387, loss 0.021978741511702538, R2 0.10846829414367676\n",
      "Eval loss 0.030754202976822853, R2 0.009646475315093994\n",
      "epoch 388, loss 0.021961065009236336, R2 0.10918527841567993\n",
      "Eval loss 0.03072591871023178, R2 0.010557293891906738\n",
      "epoch 389, loss 0.021943440660834312, R2 0.10990017652511597\n",
      "Eval loss 0.03069772571325302, R2 0.011465191841125488\n",
      "epoch 390, loss 0.021925857290625572, R2 0.11061340570449829\n",
      "Eval loss 0.03066961281001568, R2 0.012370467185974121\n",
      "epoch 391, loss 0.021908320486545563, R2 0.11132508516311646\n",
      "Eval loss 0.030641591176390648, R2 0.01327282190322876\n",
      "epoch 392, loss 0.021890833973884583, R2 0.11203515529632568\n",
      "Eval loss 0.030613651499152184, R2 0.014172554016113281\n",
      "epoch 393, loss 0.021873388439416885, R2 0.11274206638336182\n",
      "Eval loss 0.030585797503590584, R2 0.015069842338562012\n",
      "epoch 394, loss 0.02185598760843277, R2 0.11344891786575317\n",
      "Eval loss 0.0305580236017704, R2 0.01596391201019287\n",
      "epoch 395, loss 0.021838637068867683, R2 0.11415141820907593\n",
      "Eval loss 0.03053033910691738, R2 0.016854524612426758\n",
      "epoch 396, loss 0.02182132750749588, R2 0.1148538589477539\n",
      "Eval loss 0.03050272911787033, R2 0.017744481563568115\n",
      "epoch 397, loss 0.021804066374897957, R2 0.11555439233779907\n",
      "Eval loss 0.030475201085209846, R2 0.0186312198638916\n",
      "epoch 398, loss 0.021786846220493317, R2 0.11625248193740845\n",
      "Eval loss 0.030447764322161674, R2 0.019514501094818115\n",
      "epoch 399, loss 0.021769674494862556, R2 0.11694872379302979\n",
      "Eval loss 0.030420400202274323, R2 0.020394742488861084\n",
      "epoch 400, loss 0.02175254374742508, R2 0.11764359474182129\n",
      "Eval loss 0.030393125489354134, R2 0.02127397060394287\n",
      "epoch 401, loss 0.021735457703471184, R2 0.11833697557449341\n",
      "Eval loss 0.030365921556949615, R2 0.022150278091430664\n",
      "epoch 402, loss 0.02171841636300087, R2 0.11902892589569092\n",
      "Eval loss 0.03033880516886711, R2 0.023023247718811035\n",
      "epoch 403, loss 0.021701419726014137, R2 0.11971735954284668\n",
      "Eval loss 0.030311763286590576, R2 0.02389371395111084\n",
      "epoch 404, loss 0.021684465929865837, R2 0.12040543556213379\n",
      "Eval loss 0.030284803360700607, R2 0.024762213230133057\n",
      "epoch 405, loss 0.02166755497455597, R2 0.12109136581420898\n",
      "Eval loss 0.030257921665906906, R2 0.025627851486206055\n",
      "epoch 406, loss 0.021650683134794235, R2 0.1217760443687439\n",
      "Eval loss 0.030231116339564323, R2 0.026490747928619385\n",
      "epoch 407, loss 0.02163386158645153, R2 0.12245810031890869\n",
      "Eval loss 0.03020438179373741, R2 0.02735191583633423\n",
      "epoch 408, loss 0.02161707729101181, R2 0.12314033508300781\n",
      "Eval loss 0.030177731066942215, R2 0.028209269046783447\n",
      "epoch 409, loss 0.02160033769905567, R2 0.12381798028945923\n",
      "Eval loss 0.030151156708598137, R2 0.02906590700149536\n",
      "epoch 410, loss 0.021583640947937965, R2 0.12449592351913452\n",
      "Eval loss 0.030124658718705177, R2 0.029919147491455078\n",
      "epoch 411, loss 0.021566981449723244, R2 0.12517094612121582\n",
      "Eval loss 0.030098235234618187, R2 0.030770063400268555\n",
      "epoch 412, loss 0.021550368517637253, R2 0.1258448362350464\n",
      "Eval loss 0.030071886256337166, R2 0.031618595123291016\n",
      "epoch 413, loss 0.021533796563744545, R2 0.12651705741882324\n",
      "Eval loss 0.030045606195926666, R2 0.03246486186981201\n",
      "epoch 414, loss 0.02151726745069027, R2 0.1271875500679016\n",
      "Eval loss 0.03001941181719303, R2 0.03330838680267334\n",
      "epoch 415, loss 0.02150077559053898, R2 0.12785649299621582\n",
      "Eval loss 0.02999328449368477, R2 0.03414946794509888\n",
      "epoch 416, loss 0.02148432657122612, R2 0.128523051738739\n",
      "Eval loss 0.029967231675982475, R2 0.034988999366760254\n",
      "epoch 417, loss 0.021467922255396843, R2 0.129189133644104\n",
      "Eval loss 0.029941249638795853, R2 0.03582543134689331\n",
      "epoch 418, loss 0.0214515570551157, R2 0.1298530101776123\n",
      "Eval loss 0.0299153421074152, R2 0.03666025400161743\n",
      "epoch 419, loss 0.02143522910773754, R2 0.13051527738571167\n",
      "Eval loss 0.029889505356550217, R2 0.03749138116836548\n",
      "epoch 420, loss 0.021418940275907516, R2 0.13117599487304688\n",
      "Eval loss 0.029863741248846054, R2 0.03832101821899414\n",
      "epoch 421, loss 0.021402694284915924, R2 0.13183563947677612\n",
      "Eval loss 0.029838047921657562, R2 0.03914874792098999\n",
      "epoch 422, loss 0.021386489272117615, R2 0.13249331712722778\n",
      "Eval loss 0.02981242723762989, R2 0.03997379541397095\n",
      "epoch 423, loss 0.02137032523751259, R2 0.13314765691757202\n",
      "Eval loss 0.02978687360882759, R2 0.04079693555831909\n",
      "epoch 424, loss 0.021354198455810547, R2 0.13380217552185059\n",
      "Eval loss 0.029761390760540962, R2 0.04161721467971802\n",
      "epoch 425, loss 0.021338114514946938, R2 0.13445454835891724\n",
      "Eval loss 0.029735976830124855, R2 0.04243558645248413\n",
      "epoch 426, loss 0.021322065964341164, R2 0.13510555028915405\n",
      "Eval loss 0.02971063368022442, R2 0.04325169324874878\n",
      "epoch 427, loss 0.021306060254573822, R2 0.13575482368469238\n",
      "Eval loss 0.029685353860259056, R2 0.04406547546386719\n",
      "epoch 428, loss 0.021290089935064316, R2 0.13640260696411133\n",
      "Eval loss 0.029660150408744812, R2 0.04487651586532593\n",
      "epoch 429, loss 0.021274160593748093, R2 0.13704973459243774\n",
      "Eval loss 0.02963501214981079, R2 0.04568690061569214\n",
      "epoch 430, loss 0.021258268505334854, R2 0.13769304752349854\n",
      "Eval loss 0.02960994280874729, R2 0.046494483947753906\n",
      "epoch 431, loss 0.02124241553246975, R2 0.1383364200592041\n",
      "Eval loss 0.029584936797618866, R2 0.047298550605773926\n",
      "epoch 432, loss 0.02122660167515278, R2 0.13897722959518433\n",
      "Eval loss 0.02956000156700611, R2 0.048102736473083496\n",
      "epoch 433, loss 0.021210823208093643, R2 0.1396186351776123\n",
      "Eval loss 0.02953513152897358, R2 0.048903584480285645\n",
      "epoch 434, loss 0.02119508758187294, R2 0.1402565836906433\n",
      "Eval loss 0.02951032482087612, R2 0.04970186948776245\n",
      "epoch 435, loss 0.021179385483264923, R2 0.14089351892471313\n",
      "Eval loss 0.029485588893294334, R2 0.05049782991409302\n",
      "epoch 436, loss 0.02116372436285019, R2 0.14152848720550537\n",
      "Eval loss 0.029460914433002472, R2 0.05129355192184448\n",
      "epoch 437, loss 0.02114810049533844, R2 0.14216220378875732\n",
      "Eval loss 0.029436307027935982, R2 0.05208534002304077\n",
      "epoch 438, loss 0.021132512018084526, R2 0.14279448986053467\n",
      "Eval loss 0.029411766678094864, R2 0.05287593603134155\n",
      "epoch 439, loss 0.021116962656378746, R2 0.14342522621154785\n",
      "Eval loss 0.02938728593289852, R2 0.05366426706314087\n",
      "epoch 440, loss 0.0211014486849308, R2 0.14405488967895508\n",
      "Eval loss 0.02936287224292755, R2 0.05445045232772827\n",
      "epoch 441, loss 0.02108597196638584, R2 0.14468294382095337\n",
      "Eval loss 0.029338520020246506, R2 0.05523461103439331\n",
      "epoch 442, loss 0.021070534363389015, R2 0.14530915021896362\n",
      "Eval loss 0.029314231127500534, R2 0.056016743183135986\n",
      "epoch 443, loss 0.021055130288004875, R2 0.14593303203582764\n",
      "Eval loss 0.029290009289979935, R2 0.05679678916931152\n",
      "epoch 444, loss 0.02103976532816887, R2 0.1465563178062439\n",
      "Eval loss 0.02926584705710411, R2 0.057573676109313965\n",
      "epoch 445, loss 0.021024437621235847, R2 0.147178053855896\n",
      "Eval loss 0.02924174815416336, R2 0.05835086107254028\n",
      "epoch 446, loss 0.02100914530456066, R2 0.14779871702194214\n",
      "Eval loss 0.029217710718512535, R2 0.05912494659423828\n",
      "epoch 447, loss 0.02099388651549816, R2 0.14841794967651367\n",
      "Eval loss 0.029193736612796783, R2 0.05989694595336914\n",
      "epoch 448, loss 0.020978664979338646, R2 0.1490350365638733\n",
      "Eval loss 0.029169822111725807, R2 0.060666799545288086\n",
      "epoch 449, loss 0.020963480696082115, R2 0.1496516466140747\n",
      "Eval loss 0.029145970940589905, R2 0.061435163021087646\n",
      "epoch 450, loss 0.02094833366572857, R2 0.15026640892028809\n",
      "Eval loss 0.029122179374098778, R2 0.06220155954360962\n",
      "epoch 451, loss 0.02093321830034256, R2 0.15087854862213135\n",
      "Eval loss 0.029098449274897575, R2 0.06296545267105103\n",
      "epoch 452, loss 0.020918138325214386, R2 0.1514902114868164\n",
      "Eval loss 0.029074776917696, R2 0.0637277364730835\n",
      "epoch 453, loss 0.020903093740344048, R2 0.1521008014678955\n",
      "Eval loss 0.029051166027784348, R2 0.0644877552986145\n",
      "epoch 454, loss 0.020888088271021843, R2 0.15271013975143433\n",
      "Eval loss 0.02902761474251747, R2 0.06524646282196045\n",
      "epoch 455, loss 0.020873112604022026, R2 0.15331757068634033\n",
      "Eval loss 0.02900412119925022, R2 0.06600326299667358\n",
      "epoch 456, loss 0.020858176052570343, R2 0.1539231538772583\n",
      "Eval loss 0.028980689123272896, R2 0.06675755977630615\n",
      "epoch 457, loss 0.020843273028731346, R2 0.15452706813812256\n",
      "Eval loss 0.028957314789295197, R2 0.06751054525375366\n",
      "epoch 458, loss 0.020828403532505035, R2 0.15513014793395996\n",
      "Eval loss 0.028933994472026825, R2 0.06826120615005493\n",
      "epoch 459, loss 0.02081356756389141, R2 0.15573161840438843\n",
      "Eval loss 0.028910737484693527, R2 0.06900900602340698\n",
      "epoch 460, loss 0.02079876884818077, R2 0.15633225440979004\n",
      "Eval loss 0.028887538239359856, R2 0.06975603103637695\n",
      "epoch 461, loss 0.020784003660082817, R2 0.1569318175315857\n",
      "Eval loss 0.02886439859867096, R2 0.0705024003982544\n",
      "epoch 462, loss 0.02076927199959755, R2 0.15752941370010376\n",
      "Eval loss 0.028841309249401093, R2 0.07124590873718262\n",
      "epoch 463, loss 0.02075457014143467, R2 0.15812504291534424\n",
      "Eval loss 0.0288182832300663, R2 0.07198739051818848\n",
      "epoch 464, loss 0.020739907398819923, R2 0.15871983766555786\n",
      "Eval loss 0.028795313090085983, R2 0.0727270245552063\n",
      "epoch 465, loss 0.020725281909108162, R2 0.15931379795074463\n",
      "Eval loss 0.028772395104169846, R2 0.07346481084823608\n",
      "epoch 466, loss 0.02071068063378334, R2 0.1599050760269165\n",
      "Eval loss 0.028749536722898483, R2 0.07420027256011963\n",
      "epoch 467, loss 0.020696118474006653, R2 0.16049611568450928\n",
      "Eval loss 0.02872673235833645, R2 0.07493579387664795\n",
      "epoch 468, loss 0.020681586116552353, R2 0.16108590364456177\n",
      "Eval loss 0.02870398759841919, R2 0.07566797733306885\n",
      "epoch 469, loss 0.020667091012001038, R2 0.16167289018630981\n",
      "Eval loss 0.028681296855211258, R2 0.07639861106872559\n",
      "epoch 470, loss 0.02065262570977211, R2 0.16226094961166382\n",
      "Eval loss 0.028658660128712654, R2 0.07712650299072266\n",
      "epoch 471, loss 0.020638195797801018, R2 0.16284561157226562\n",
      "Eval loss 0.02863607555627823, R2 0.07785487174987793\n",
      "epoch 472, loss 0.020623797550797462, R2 0.16343027353286743\n",
      "Eval loss 0.02861355058848858, R2 0.07858020067214966\n",
      "epoch 473, loss 0.020609430968761444, R2 0.16401243209838867\n",
      "Eval loss 0.028591079637408257, R2 0.07930296659469604\n",
      "epoch 474, loss 0.02059509977698326, R2 0.16459405422210693\n",
      "Eval loss 0.028568657115101814, R2 0.08002591133117676\n",
      "epoch 475, loss 0.020580800250172615, R2 0.1651744842529297\n",
      "Eval loss 0.028546292334794998, R2 0.08074522018432617\n",
      "epoch 476, loss 0.020566530525684357, R2 0.16575294733047485\n",
      "Eval loss 0.02852398343384266, R2 0.08146446943283081\n",
      "epoch 477, loss 0.020552294328808784, R2 0.16633009910583496\n",
      "Eval loss 0.02850172482430935, R2 0.08218127489089966\n",
      "epoch 478, loss 0.020538093522191048, R2 0.1669064164161682\n",
      "Eval loss 0.028479518368840218, R2 0.08289551734924316\n",
      "epoch 479, loss 0.02052392065525055, R2 0.16748100519180298\n",
      "Eval loss 0.028457369655370712, R2 0.08360874652862549\n",
      "epoch 480, loss 0.020509781315922737, R2 0.16805386543273926\n",
      "Eval loss 0.028435267508029938, R2 0.08432132005691528\n",
      "epoch 481, loss 0.020495671778917313, R2 0.1686268448829651\n",
      "Eval loss 0.02841322496533394, R2 0.08503121137619019\n",
      "epoch 482, loss 0.020481595769524574, R2 0.16919845342636108\n",
      "Eval loss 0.028391225263476372, R2 0.0857396125793457\n",
      "epoch 483, loss 0.020467547699809074, R2 0.1697673201560974\n",
      "Eval loss 0.02836928516626358, R2 0.08644610643386841\n",
      "epoch 484, loss 0.02045353874564171, R2 0.1703365445137024\n",
      "Eval loss 0.02834739163517952, R2 0.08715087175369263\n",
      "epoch 485, loss 0.02043955586850643, R2 0.17090314626693726\n",
      "Eval loss 0.028325555846095085, R2 0.08785432577133179\n",
      "epoch 486, loss 0.02042560651898384, R2 0.17146867513656616\n",
      "Eval loss 0.02830376848578453, R2 0.08855587244033813\n",
      "epoch 487, loss 0.02041168510913849, R2 0.1720333695411682\n",
      "Eval loss 0.028282029554247856, R2 0.0892559289932251\n",
      "epoch 488, loss 0.020397797226905823, R2 0.17259705066680908\n",
      "Eval loss 0.02826034463942051, R2 0.0899541974067688\n",
      "epoch 489, loss 0.020383939146995544, R2 0.17315876483917236\n",
      "Eval loss 0.028238708153367043, R2 0.09065097570419312\n",
      "epoch 490, loss 0.020370114594697952, R2 0.17372089624404907\n",
      "Eval loss 0.028217125684022903, R2 0.09134602546691895\n",
      "epoch 491, loss 0.02035631611943245, R2 0.1742795705795288\n",
      "Eval loss 0.028195587918162346, R2 0.09203952550888062\n",
      "epoch 492, loss 0.020342551171779633, R2 0.17483800649642944\n",
      "Eval loss 0.028174104169011116, R2 0.09273052215576172\n",
      "epoch 493, loss 0.020328814163804054, R2 0.17539513111114502\n",
      "Eval loss 0.028152666985988617, R2 0.09342175722122192\n",
      "epoch 494, loss 0.020315110683441162, R2 0.17595070600509644\n",
      "Eval loss 0.028131278231739998, R2 0.09411048889160156\n",
      "epoch 495, loss 0.020301437005400658, R2 0.1765056848526001\n",
      "Eval loss 0.028109943494200706, R2 0.09479749202728271\n",
      "epoch 496, loss 0.020287789404392242, R2 0.17705923318862915\n",
      "Eval loss 0.028088655322790146, R2 0.09548217058181763\n",
      "epoch 497, loss 0.020274177193641663, R2 0.17761147022247314\n",
      "Eval loss 0.028067415580153465, R2 0.09616702795028687\n",
      "epoch 498, loss 0.020260591059923172, R2 0.17816251516342163\n",
      "Eval loss 0.028046224266290665, R2 0.09684938192367554\n",
      "epoch 499, loss 0.02024703472852707, R2 0.17871272563934326\n",
      "Eval loss 0.028025083243846893, R2 0.0975293517112732\n",
      "epoch 500, loss 0.020233511924743652, R2 0.17926162481307983\n",
      "Eval loss 0.028003990650177002, R2 0.0982094407081604\n",
      "epoch 501, loss 0.020220015197992325, R2 0.17980843782424927\n",
      "Eval loss 0.027982940897345543, R2 0.09888726472854614\n",
      "epoch 502, loss 0.020206548273563385, R2 0.18035465478897095\n",
      "Eval loss 0.027961941435933113, R2 0.09956353902816772\n",
      "epoch 503, loss 0.02019311487674713, R2 0.18089962005615234\n",
      "Eval loss 0.027940994128584862, R2 0.10023808479309082\n",
      "epoch 504, loss 0.020179705694317818, R2 0.18144351243972778\n",
      "Eval loss 0.027920091524720192, R2 0.10091114044189453\n",
      "epoch 505, loss 0.02016632817685604, R2 0.18198680877685547\n",
      "Eval loss 0.027899233624339104, R2 0.10158306360244751\n",
      "epoch 506, loss 0.020152978599071503, R2 0.18252766132354736\n",
      "Eval loss 0.027878422290086746, R2 0.10225296020507812\n",
      "epoch 507, loss 0.020139658823609352, R2 0.1830676794052124\n",
      "Eval loss 0.027857663109898567, R2 0.10292172431945801\n",
      "epoch 508, loss 0.02012636885046959, R2 0.18360769748687744\n",
      "Eval loss 0.02783694490790367, R2 0.10358864068984985\n",
      "epoch 509, loss 0.020113104954361916, R2 0.1841450333595276\n",
      "Eval loss 0.027816276997327805, R2 0.10425424575805664\n",
      "epoch 510, loss 0.02009987086057663, R2 0.18468248844146729\n",
      "Eval loss 0.02779565379023552, R2 0.10491836071014404\n",
      "epoch 511, loss 0.02008666843175888, R2 0.18521714210510254\n",
      "Eval loss 0.027775075286626816, R2 0.10558098554611206\n",
      "epoch 512, loss 0.02007349021732807, R2 0.18575167655944824\n",
      "Eval loss 0.02775454707443714, R2 0.10624206066131592\n",
      "epoch 513, loss 0.0200603436678648, R2 0.18628555536270142\n",
      "Eval loss 0.0277340579777956, R2 0.10690182447433472\n",
      "epoch 514, loss 0.020047223195433617, R2 0.18681710958480835\n",
      "Eval loss 0.02771361917257309, R2 0.10755980014801025\n",
      "epoch 515, loss 0.02003413252532482, R2 0.18734848499298096\n",
      "Eval loss 0.02769322320818901, R2 0.10821676254272461\n",
      "epoch 516, loss 0.020021069794893265, R2 0.18787866830825806\n",
      "Eval loss 0.02767287753522396, R2 0.10887199640274048\n",
      "epoch 517, loss 0.020008035004138947, R2 0.1884070634841919\n",
      "Eval loss 0.027652572840452194, R2 0.10952579975128174\n",
      "epoch 518, loss 0.019995026290416718, R2 0.18893438577651978\n",
      "Eval loss 0.02763231098651886, R2 0.11017835140228271\n",
      "epoch 519, loss 0.019982047379016876, R2 0.18946248292922974\n",
      "Eval loss 0.027612095698714256, R2 0.11082905530929565\n",
      "epoch 520, loss 0.019969096407294273, R2 0.18998688459396362\n",
      "Eval loss 0.027591925114393234, R2 0.11147856712341309\n",
      "epoch 521, loss 0.01995617337524891, R2 0.19051074981689453\n",
      "Eval loss 0.027571795508265495, R2 0.11212706565856934\n",
      "epoch 522, loss 0.019943278282880783, R2 0.19103354215621948\n",
      "Eval loss 0.027551712468266487, R2 0.11277377605438232\n",
      "epoch 523, loss 0.019930409267544746, R2 0.19155645370483398\n",
      "Eval loss 0.02753167226910591, R2 0.11341911554336548\n",
      "epoch 524, loss 0.019917568191885948, R2 0.1920766830444336\n",
      "Eval loss 0.027511680498719215, R2 0.11406320333480835\n",
      "epoch 525, loss 0.01990475319325924, R2 0.19259655475616455\n",
      "Eval loss 0.027491725981235504, R2 0.1147051453590393\n",
      "epoch 526, loss 0.019891967996954918, R2 0.1931154727935791\n",
      "Eval loss 0.027471818029880524, R2 0.11534655094146729\n",
      "epoch 527, loss 0.019879207015037537, R2 0.19363278150558472\n",
      "Eval loss 0.027451951056718826, R2 0.11598628759384155\n",
      "epoch 528, loss 0.019866475835442543, R2 0.19415009021759033\n",
      "Eval loss 0.02743212878704071, R2 0.11662459373474121\n",
      "epoch 529, loss 0.019853772595524788, R2 0.19466447830200195\n",
      "Eval loss 0.027412349358201027, R2 0.11726152896881104\n",
      "epoch 530, loss 0.019841093569993973, R2 0.1951788067817688\n",
      "Eval loss 0.027392616495490074, R2 0.11789703369140625\n",
      "epoch 531, loss 0.019828438758850098, R2 0.1956920623779297\n",
      "Eval loss 0.02737291529774666, R2 0.11853086948394775\n",
      "epoch 532, loss 0.01981581375002861, R2 0.19620448350906372\n",
      "Eval loss 0.02735326439142227, R2 0.11916422843933105\n",
      "epoch 533, loss 0.01980321668088436, R2 0.19671517610549927\n",
      "Eval loss 0.02733365260064602, R2 0.1197957992553711\n",
      "epoch 534, loss 0.019790643826127052, R2 0.19722485542297363\n",
      "Eval loss 0.027314085513353348, R2 0.12042587995529175\n",
      "epoch 535, loss 0.019778098911046982, R2 0.1977340579032898\n",
      "Eval loss 0.02729456126689911, R2 0.12105435132980347\n",
      "epoch 536, loss 0.019765580072999, R2 0.19824188947677612\n",
      "Eval loss 0.027275074273347855, R2 0.1216813325881958\n",
      "epoch 537, loss 0.01975308731198311, R2 0.19874858856201172\n",
      "Eval loss 0.02725563570857048, R2 0.12230813503265381\n",
      "epoch 538, loss 0.019740622490644455, R2 0.19925391674041748\n",
      "Eval loss 0.02723623253405094, R2 0.12293291091918945\n",
      "epoch 539, loss 0.019728180021047592, R2 0.19975954294204712\n",
      "Eval loss 0.027216870337724686, R2 0.12355643510818481\n",
      "epoch 540, loss 0.019715765491127968, R2 0.20026248693466187\n",
      "Eval loss 0.027197550982236862, R2 0.12417829036712646\n",
      "epoch 541, loss 0.01970338076353073, R2 0.20076584815979004\n",
      "Eval loss 0.02717827633023262, R2 0.12479901313781738\n",
      "epoch 542, loss 0.019691016525030136, R2 0.20126640796661377\n",
      "Eval loss 0.027159037068486214, R2 0.1254187822341919\n",
      "epoch 543, loss 0.01967868022620678, R2 0.20176678895950317\n",
      "Eval loss 0.02713984064757824, R2 0.12603700160980225\n",
      "epoch 544, loss 0.019666368141770363, R2 0.20226681232452393\n",
      "Eval loss 0.02712068147957325, R2 0.12665396928787231\n",
      "epoch 545, loss 0.019654085859656334, R2 0.20276445150375366\n",
      "Eval loss 0.027101567015051842, R2 0.127269446849823\n",
      "epoch 546, loss 0.019641825929284096, R2 0.20326173305511475\n",
      "Eval loss 0.027082491666078568, R2 0.12788373231887817\n",
      "epoch 547, loss 0.019629590213298798, R2 0.20375865697860718\n",
      "Eval loss 0.027063455432653427, R2 0.12849700450897217\n",
      "epoch 548, loss 0.019617382436990738, R2 0.2042529582977295\n",
      "Eval loss 0.02704445831477642, R2 0.1291084885597229\n",
      "epoch 549, loss 0.019605198875069618, R2 0.20474714040756226\n",
      "Eval loss 0.027025505900382996, R2 0.12971878051757812\n",
      "epoch 550, loss 0.019593041390180588, R2 0.20524060726165771\n",
      "Eval loss 0.027006585150957108, R2 0.13032805919647217\n",
      "epoch 551, loss 0.019580908119678497, R2 0.20573270320892334\n",
      "Eval loss 0.0269877091050148, R2 0.1309359073638916\n",
      "epoch 552, loss 0.019568800926208496, R2 0.20622354745864868\n",
      "Eval loss 0.02696887031197548, R2 0.13154256343841553\n",
      "epoch 553, loss 0.019556717947125435, R2 0.20671367645263672\n",
      "Eval loss 0.02695007249712944, R2 0.1321479082107544\n",
      "epoch 554, loss 0.019544661045074463, R2 0.20720309019088745\n",
      "Eval loss 0.026931311935186386, R2 0.13275229930877686\n",
      "epoch 555, loss 0.01953262649476528, R2 0.20769119262695312\n",
      "Eval loss 0.026912588626146317, R2 0.13335418701171875\n",
      "epoch 556, loss 0.01952061802148819, R2 0.2081783413887024\n",
      "Eval loss 0.02689390815794468, R2 0.13395649194717407\n",
      "epoch 557, loss 0.019508635625243187, R2 0.20866435766220093\n",
      "Eval loss 0.026875263080000877, R2 0.13455694913864136\n",
      "epoch 558, loss 0.019496677443385124, R2 0.20914971828460693\n",
      "Eval loss 0.02685665525496006, R2 0.13515585660934448\n",
      "epoch 559, loss 0.019484741613268852, R2 0.20963358879089355\n",
      "Eval loss 0.026838092133402824, R2 0.13575416803359985\n",
      "epoch 560, loss 0.01947283186018467, R2 0.2101166844367981\n",
      "Eval loss 0.026819562539458275, R2 0.1363508701324463\n",
      "epoch 561, loss 0.019460946321487427, R2 0.21059882640838623\n",
      "Eval loss 0.02680107206106186, R2 0.13694584369659424\n",
      "epoch 562, loss 0.019449086859822273, R2 0.21107983589172363\n",
      "Eval loss 0.02678261511027813, R2 0.13754040002822876\n",
      "epoch 563, loss 0.01943724974989891, R2 0.21156001091003418\n",
      "Eval loss 0.02676420472562313, R2 0.13813328742980957\n",
      "epoch 564, loss 0.019425436854362488, R2 0.21203917264938354\n",
      "Eval loss 0.02674582228064537, R2 0.13872522115707397\n",
      "epoch 565, loss 0.019413648173213005, R2 0.2125173807144165\n",
      "Eval loss 0.02672748640179634, R2 0.1393154263496399\n",
      "epoch 566, loss 0.019401881843805313, R2 0.21299469470977783\n",
      "Eval loss 0.02670918218791485, R2 0.13990509510040283\n",
      "epoch 567, loss 0.01939014345407486, R2 0.21347111463546753\n",
      "Eval loss 0.02669091522693634, R2 0.14049232006072998\n",
      "epoch 568, loss 0.019378429278731346, R2 0.2139466404914856\n",
      "Eval loss 0.026672685518860817, R2 0.14108037948608398\n",
      "epoch 569, loss 0.019366733729839325, R2 0.2144203782081604\n",
      "Eval loss 0.02665449120104313, R2 0.14166629314422607\n",
      "epoch 570, loss 0.019355066120624542, R2 0.21489369869232178\n",
      "Eval loss 0.026636343449354172, R2 0.14224988222122192\n",
      "epoch 571, loss 0.0193434190005064, R2 0.21536612510681152\n",
      "Eval loss 0.026618223637342453, R2 0.14283448457717896\n",
      "epoch 572, loss 0.01933179795742035, R2 0.2158374786376953\n",
      "Eval loss 0.02660014107823372, R2 0.14341646432876587\n",
      "epoch 573, loss 0.019320199266076088, R2 0.21630799770355225\n",
      "Eval loss 0.02658209763467312, R2 0.1439969539642334\n",
      "epoch 574, loss 0.019308624789118767, R2 0.21677744388580322\n",
      "Eval loss 0.026564087718725204, R2 0.1445775032043457\n",
      "epoch 575, loss 0.019297074526548386, R2 0.21724563837051392\n",
      "Eval loss 0.026546115055680275, R2 0.14515650272369385\n",
      "epoch 576, loss 0.019285546615719795, R2 0.21771389245986938\n",
      "Eval loss 0.02652817778289318, R2 0.14573389291763306\n",
      "epoch 577, loss 0.019274042919278145, R2 0.21817994117736816\n",
      "Eval loss 0.02651027962565422, R2 0.1463102102279663\n",
      "epoch 578, loss 0.019262561574578285, R2 0.21864652633666992\n",
      "Eval loss 0.026492414996027946, R2 0.14688575267791748\n",
      "epoch 579, loss 0.019251102581620216, R2 0.21911108493804932\n",
      "Eval loss 0.026474585756659508, R2 0.14745938777923584\n",
      "epoch 580, loss 0.01923966594040394, R2 0.2195746898651123\n",
      "Eval loss 0.026456791907548904, R2 0.14803260564804077\n",
      "epoch 581, loss 0.01922825537621975, R2 0.22003847360610962\n",
      "Eval loss 0.026439035311341286, R2 0.14860445261001587\n",
      "epoch 582, loss 0.019216863438487053, R2 0.22049957513809204\n",
      "Eval loss 0.026421314105391502, R2 0.14917510747909546\n",
      "epoch 583, loss 0.019205499440431595, R2 0.2209603190422058\n",
      "Eval loss 0.026403622701764107, R2 0.14974480867385864\n",
      "epoch 584, loss 0.01919415406882763, R2 0.2214207649230957\n",
      "Eval loss 0.026385972276329994, R2 0.1503126621246338\n",
      "epoch 585, loss 0.019182832911610603, R2 0.2218797206878662\n",
      "Eval loss 0.026368355378508568, R2 0.15088045597076416\n",
      "epoch 586, loss 0.01917153410613537, R2 0.22233837842941284\n",
      "Eval loss 0.026350773870944977, R2 0.15144634246826172\n",
      "epoch 587, loss 0.019160259515047073, R2 0.22279572486877441\n",
      "Eval loss 0.026333224028348923, R2 0.15201151371002197\n",
      "epoch 588, loss 0.01914900541305542, R2 0.22325187921524048\n",
      "Eval loss 0.026315713301301003, R2 0.15257567167282104\n",
      "epoch 589, loss 0.019137773662805557, R2 0.22370809316635132\n",
      "Eval loss 0.02629823423922062, R2 0.15313774347305298\n",
      "epoch 590, loss 0.019126566126942635, R2 0.22416269779205322\n",
      "Eval loss 0.026280788704752922, R2 0.15370029211044312\n",
      "epoch 591, loss 0.019115379080176353, R2 0.22461646795272827\n",
      "Eval loss 0.02626338042318821, R2 0.15426093339920044\n",
      "epoch 592, loss 0.019104214385151863, R2 0.22506940364837646\n",
      "Eval loss 0.026246005669236183, R2 0.15482038259506226\n",
      "epoch 593, loss 0.019093072041869164, R2 0.22552132606506348\n",
      "Eval loss 0.026228664442896843, R2 0.15537911653518677\n",
      "epoch 594, loss 0.019081952050328255, R2 0.22597205638885498\n",
      "Eval loss 0.02621135674417019, R2 0.15593618154525757\n",
      "epoch 595, loss 0.019070856273174286, R2 0.22642308473587036\n",
      "Eval loss 0.026194080710411072, R2 0.15649175643920898\n",
      "epoch 596, loss 0.01905977912247181, R2 0.22687208652496338\n",
      "Eval loss 0.02617684379220009, R2 0.1570475697517395\n",
      "epoch 597, loss 0.019048726186156273, R2 0.22732013463974\n",
      "Eval loss 0.02615964226424694, R2 0.15760129690170288\n",
      "epoch 598, loss 0.019037695601582527, R2 0.22776728868484497\n",
      "Eval loss 0.026142466813325882, R2 0.15815460681915283\n",
      "epoch 599, loss 0.019026685506105423, R2 0.22821450233459473\n",
      "Eval loss 0.026125328615307808, R2 0.15870648622512817\n",
      "epoch 600, loss 0.01901569589972496, R2 0.22866052389144897\n",
      "Eval loss 0.02610822394490242, R2 0.15925753116607666\n",
      "epoch 601, loss 0.019004730507731438, R2 0.22910469770431519\n",
      "Eval loss 0.02609114907681942, R2 0.15980714559555054\n",
      "epoch 602, loss 0.018993787467479706, R2 0.22954809665679932\n",
      "Eval loss 0.026074109598994255, R2 0.16035586595535278\n",
      "epoch 603, loss 0.018982863053679466, R2 0.2299911379814148\n",
      "Eval loss 0.026057105511426926, R2 0.1609034538269043\n",
      "epoch 604, loss 0.018971960991621017, R2 0.23043394088745117\n",
      "Eval loss 0.026040129363536835, R2 0.1614503264427185\n",
      "epoch 605, loss 0.01896107941865921, R2 0.23087447881698608\n",
      "Eval loss 0.02602319046854973, R2 0.16199558973312378\n",
      "epoch 606, loss 0.018950220197439194, R2 0.23131614923477173\n",
      "Eval loss 0.02600628323853016, R2 0.1625400185585022\n",
      "epoch 607, loss 0.018939383327960968, R2 0.23175513744354248\n",
      "Eval loss 0.025989407673478127, R2 0.16308343410491943\n",
      "epoch 608, loss 0.018928565084934235, R2 0.23219364881515503\n",
      "Eval loss 0.025972561910748482, R2 0.16362619400024414\n",
      "epoch 609, loss 0.01891777105629444, R2 0.23263239860534668\n",
      "Eval loss 0.02595575526356697, R2 0.1641671061515808\n",
      "epoch 610, loss 0.01890699565410614, R2 0.23306894302368164\n",
      "Eval loss 0.0259389728307724, R2 0.16470754146575928\n",
      "epoch 611, loss 0.01889624446630478, R2 0.2335047721862793\n",
      "Eval loss 0.025922229513525963, R2 0.16524595022201538\n",
      "epoch 612, loss 0.01888551190495491, R2 0.2339409589767456\n",
      "Eval loss 0.025905514135956764, R2 0.1657847762107849\n",
      "epoch 613, loss 0.018874799832701683, R2 0.23437488079071045\n",
      "Eval loss 0.0258888341486454, R2 0.16632217168807983\n",
      "epoch 614, loss 0.018864110112190247, R2 0.23480850458145142\n",
      "Eval loss 0.025872180238366127, R2 0.16685843467712402\n",
      "epoch 615, loss 0.01885344088077545, R2 0.23524123430252075\n",
      "Eval loss 0.025855565443634987, R2 0.1673935055732727\n",
      "epoch 616, loss 0.01884279027581215, R2 0.23567330837249756\n",
      "Eval loss 0.025838976725935936, R2 0.16792690753936768\n",
      "epoch 617, loss 0.018832162022590637, R2 0.23610436916351318\n",
      "Eval loss 0.025822419673204422, R2 0.16846108436584473\n",
      "epoch 618, loss 0.018821556121110916, R2 0.23653459548950195\n",
      "Eval loss 0.025805898010730743, R2 0.16899263858795166\n",
      "epoch 619, loss 0.018810970708727837, R2 0.23696428537368774\n",
      "Eval loss 0.025789406150579453, R2 0.1695241928100586\n",
      "epoch 620, loss 0.01880040392279625, R2 0.23739200830459595\n",
      "Eval loss 0.0257729459553957, R2 0.17005425691604614\n",
      "epoch 621, loss 0.018789859488606453, R2 0.23782038688659668\n",
      "Eval loss 0.025756515562534332, R2 0.1705830693244934\n",
      "epoch 622, loss 0.018779331818223, R2 0.2382473349571228\n",
      "Eval loss 0.025740118697285652, R2 0.17111140489578247\n",
      "epoch 623, loss 0.018768824636936188, R2 0.23867356777191162\n",
      "Eval loss 0.02572374790906906, R2 0.17163825035095215\n",
      "epoch 624, loss 0.018758343532681465, R2 0.2390996217727661\n",
      "Eval loss 0.025707412511110306, R2 0.17216408252716064\n",
      "epoch 625, loss 0.018747877329587936, R2 0.23952323198318481\n",
      "Eval loss 0.025691106915473938, R2 0.17268943786621094\n",
      "epoch 626, loss 0.0187374334782362, R2 0.2399469017982483\n",
      "Eval loss 0.02567482925951481, R2 0.17321383953094482\n",
      "epoch 627, loss 0.018727010115981102, R2 0.24037063121795654\n",
      "Eval loss 0.025658583268523216, R2 0.17373669147491455\n",
      "epoch 628, loss 0.018716607242822647, R2 0.24079227447509766\n",
      "Eval loss 0.02564236894249916, R2 0.17425835132598877\n",
      "epoch 629, loss 0.018706224858760834, R2 0.2412131428718567\n",
      "Eval loss 0.02562618814408779, R2 0.1747797131538391\n",
      "epoch 630, loss 0.018695859238505363, R2 0.2416333556175232\n",
      "Eval loss 0.025610031560063362, R2 0.17529994249343872\n",
      "epoch 631, loss 0.018685515969991684, R2 0.24205255508422852\n",
      "Eval loss 0.025593910366296768, R2 0.1758185625076294\n",
      "epoch 632, loss 0.018675193190574646, R2 0.2424716353416443\n",
      "Eval loss 0.025577817112207413, R2 0.1763375997543335\n",
      "epoch 633, loss 0.0186648890376091, R2 0.2428898811340332\n",
      "Eval loss 0.025561757385730743, R2 0.17685472965240479\n",
      "epoch 634, loss 0.018654603511095047, R2 0.24330711364746094\n",
      "Eval loss 0.025545721873641014, R2 0.17737114429473877\n",
      "epoch 635, loss 0.018644338473677635, R2 0.24372321367263794\n",
      "Eval loss 0.025529716163873672, R2 0.17788654565811157\n",
      "epoch 636, loss 0.018634093925356865, R2 0.2441384196281433\n",
      "Eval loss 0.025513743981719017, R2 0.17840009927749634\n",
      "epoch 637, loss 0.018623868003487587, R2 0.24455410242080688\n",
      "Eval loss 0.0254977997392416, R2 0.1789143681526184\n",
      "epoch 638, loss 0.0186136607080698, R2 0.244967520236969\n",
      "Eval loss 0.02548188529908657, R2 0.17942684888839722\n",
      "epoch 639, loss 0.018603475764393806, R2 0.24538099765777588\n",
      "Eval loss 0.02546600252389908, R2 0.1799379587173462\n",
      "epoch 640, loss 0.018593305721879005, R2 0.24579286575317383\n",
      "Eval loss 0.025450145825743675, R2 0.18044888973236084\n",
      "epoch 641, loss 0.018583159893751144, R2 0.24620592594146729\n",
      "Eval loss 0.02543431706726551, R2 0.1809585690498352\n",
      "epoch 642, loss 0.018573030829429626, R2 0.24661588668823242\n",
      "Eval loss 0.025418521836400032, R2 0.1814672350883484\n",
      "epoch 643, loss 0.0185629203915596, R2 0.247025728225708\n",
      "Eval loss 0.02540275640785694, R2 0.18197494745254517\n",
      "epoch 644, loss 0.018552832305431366, R2 0.24743527173995972\n",
      "Eval loss 0.02538701519370079, R2 0.1824818253517151\n",
      "epoch 645, loss 0.018542760983109474, R2 0.247844398021698\n",
      "Eval loss 0.025371307507157326, R2 0.18298763036727905\n",
      "epoch 646, loss 0.018532710149884224, R2 0.24825119972229004\n",
      "Eval loss 0.0253556277602911, R2 0.18349260091781616\n",
      "epoch 647, loss 0.018522674217820168, R2 0.24865853786468506\n",
      "Eval loss 0.025339975953102112, R2 0.18399661779403687\n",
      "epoch 648, loss 0.0185126643627882, R2 0.24906432628631592\n",
      "Eval loss 0.025324352085590363, R2 0.18449896574020386\n",
      "epoch 649, loss 0.018502667546272278, R2 0.24946951866149902\n",
      "Eval loss 0.025308756157755852, R2 0.1850011944770813\n",
      "epoch 650, loss 0.018492691218852997, R2 0.24987417459487915\n",
      "Eval loss 0.025293193757534027, R2 0.18550312519073486\n",
      "epoch 651, loss 0.018482737243175507, R2 0.2502779960632324\n",
      "Eval loss 0.025277655571699142, R2 0.1860024333000183\n",
      "epoch 652, loss 0.01847279816865921, R2 0.25068140029907227\n",
      "Eval loss 0.025262143462896347, R2 0.18650299310684204\n",
      "epoch 653, loss 0.018462879583239555, R2 0.2510834336280823\n",
      "Eval loss 0.025246668606996536, R2 0.18700158596038818\n",
      "epoch 654, loss 0.018452979624271393, R2 0.25148528814315796\n",
      "Eval loss 0.02523121051490307, R2 0.1874990463256836\n",
      "epoch 655, loss 0.018443096429109573, R2 0.25188618898391724\n",
      "Eval loss 0.025215793401002884, R2 0.1879955530166626\n",
      "epoch 656, loss 0.018433235585689545, R2 0.2522859573364258\n",
      "Eval loss 0.025200393050909042, R2 0.18849074840545654\n",
      "epoch 657, loss 0.01842339336872101, R2 0.25268518924713135\n",
      "Eval loss 0.025185024365782738, R2 0.1889858841896057\n",
      "epoch 658, loss 0.018413566052913666, R2 0.25308412313461304\n",
      "Eval loss 0.02516968734562397, R2 0.1894797682762146\n",
      "epoch 659, loss 0.018403757363557816, R2 0.253481924533844\n",
      "Eval loss 0.025154374539852142, R2 0.18997317552566528\n",
      "epoch 660, loss 0.018393969163298607, R2 0.25387895107269287\n",
      "Eval loss 0.025139091536402702, R2 0.19046550989151\n",
      "epoch 661, loss 0.018384195864200592, R2 0.2542750835418701\n",
      "Eval loss 0.025123832747340202, R2 0.19095689058303833\n",
      "epoch 662, loss 0.018374446779489517, R2 0.2546708583831787\n",
      "Eval loss 0.02510860748589039, R2 0.1914464235305786\n",
      "epoch 663, loss 0.018364710733294487, R2 0.25506579875946045\n",
      "Eval loss 0.025093404576182365, R2 0.19193673133850098\n",
      "epoch 664, loss 0.01835499331355095, R2 0.2554599642753601\n",
      "Eval loss 0.02507822960615158, R2 0.19242537021636963\n",
      "epoch 665, loss 0.018345296382904053, R2 0.2558535933494568\n",
      "Eval loss 0.025063082575798035, R2 0.19291317462921143\n",
      "epoch 666, loss 0.01833561807870865, R2 0.25624555349349976\n",
      "Eval loss 0.025047965347766876, R2 0.19340026378631592\n",
      "epoch 667, loss 0.018325956538319588, R2 0.2566378116607666\n",
      "Eval loss 0.025032872334122658, R2 0.19388550519943237\n",
      "epoch 668, loss 0.01831631176173687, R2 0.2570287585258484\n",
      "Eval loss 0.025017807260155678, R2 0.19437086582183838\n",
      "epoch 669, loss 0.018306687474250793, R2 0.2574194669723511\n",
      "Eval loss 0.025002770125865936, R2 0.19485563039779663\n",
      "epoch 670, loss 0.01829707808792591, R2 0.2578091621398926\n",
      "Eval loss 0.024987759068608284, R2 0.19533872604370117\n",
      "epoch 671, loss 0.01828749105334282, R2 0.25819867849349976\n",
      "Eval loss 0.02497277781367302, R2 0.19582122564315796\n",
      "epoch 672, loss 0.01827791891992092, R2 0.2585861086845398\n",
      "Eval loss 0.024957818910479546, R2 0.19630217552185059\n",
      "epoch 673, loss 0.018268363550305367, R2 0.25897395610809326\n",
      "Eval loss 0.02494288980960846, R2 0.1967836618423462\n",
      "epoch 674, loss 0.018258828669786453, R2 0.25936079025268555\n",
      "Eval loss 0.024927983060479164, R2 0.1972631812095642\n",
      "epoch 675, loss 0.018249310553073883, R2 0.25974738597869873\n",
      "Eval loss 0.024913109838962555, R2 0.19774258136749268\n",
      "epoch 676, loss 0.018239807337522507, R2 0.26013195514678955\n",
      "Eval loss 0.024898258969187737, R2 0.1982208490371704\n",
      "epoch 677, loss 0.018230324611067772, R2 0.2605166435241699\n",
      "Eval loss 0.024883434176445007, R2 0.1986982226371765\n",
      "epoch 678, loss 0.01822086051106453, R2 0.2609008550643921\n",
      "Eval loss 0.024868635460734367, R2 0.19917476177215576\n",
      "epoch 679, loss 0.01821141317486763, R2 0.26128464937210083\n",
      "Eval loss 0.024853866547346115, R2 0.19965040683746338\n",
      "epoch 680, loss 0.018201982602477074, R2 0.26166635751724243\n",
      "Eval loss 0.024839119985699654, R2 0.20012527704238892\n",
      "epoch 681, loss 0.01819256693124771, R2 0.2620481848716736\n",
      "Eval loss 0.02482440322637558, R2 0.20059943199157715\n",
      "epoch 682, loss 0.01818317361176014, R2 0.2624295949935913\n",
      "Eval loss 0.024809710681438446, R2 0.20107227563858032\n",
      "epoch 683, loss 0.018173791468143463, R2 0.26281100511550903\n",
      "Eval loss 0.0247950442135334, R2 0.2015441656112671\n",
      "epoch 684, loss 0.018164431676268578, R2 0.26318979263305664\n",
      "Eval loss 0.024780401960015297, R2 0.20201611518859863\n",
      "epoch 685, loss 0.018155086785554886, R2 0.26356881856918335\n",
      "Eval loss 0.02476578950881958, R2 0.20248669385910034\n",
      "epoch 686, loss 0.018145760521292686, R2 0.26394718885421753\n",
      "Eval loss 0.024751201272010803, R2 0.2029564380645752\n",
      "epoch 687, loss 0.01813645102083683, R2 0.26432478427886963\n",
      "Eval loss 0.024736637249588966, R2 0.20342493057250977\n",
      "epoch 688, loss 0.018127160146832466, R2 0.2647016644477844\n",
      "Eval loss 0.02472209930419922, R2 0.20389360189437866\n",
      "epoch 689, loss 0.018117882311344147, R2 0.26507794857025146\n",
      "Eval loss 0.02470759116113186, R2 0.20436078310012817\n",
      "epoch 690, loss 0.01810862496495247, R2 0.2654537558555603\n",
      "Eval loss 0.02469310350716114, R2 0.20482760667800903\n",
      "epoch 691, loss 0.018099384382367134, R2 0.26582860946655273\n",
      "Eval loss 0.024678640067577362, R2 0.20529305934906006\n",
      "epoch 692, loss 0.018090158700942993, R2 0.26620256900787354\n",
      "Eval loss 0.02466420643031597, R2 0.20575809478759766\n",
      "epoch 693, loss 0.018080949783325195, R2 0.2665761113166809\n",
      "Eval loss 0.02464979887008667, R2 0.2062218189239502\n",
      "epoch 694, loss 0.01807175949215889, R2 0.2669485807418823\n",
      "Eval loss 0.02463541179895401, R2 0.20668435096740723\n",
      "epoch 695, loss 0.018062585964798927, R2 0.26732122898101807\n",
      "Eval loss 0.02462105266749859, R2 0.2071475386619568\n",
      "epoch 696, loss 0.01805342733860016, R2 0.2676922082901001\n",
      "Eval loss 0.024606721475720406, R2 0.20760899782180786\n",
      "epoch 697, loss 0.018044287338852882, R2 0.2680637836456299\n",
      "Eval loss 0.024592410773038864, R2 0.2080698013305664\n",
      "epoch 698, loss 0.0180351659655571, R2 0.2684338688850403\n",
      "Eval loss 0.024578126147389412, R2 0.20852935314178467\n",
      "epoch 699, loss 0.018026059493422508, R2 0.26880210638046265\n",
      "Eval loss 0.0245638657361269, R2 0.20898908376693726\n",
      "epoch 700, loss 0.018016966059803963, R2 0.2691720128059387\n",
      "Eval loss 0.024549631401896477, R2 0.20944666862487793\n",
      "epoch 701, loss 0.018007894977927208, R2 0.269539475440979\n",
      "Eval loss 0.024535423144698143, R2 0.2099044919013977\n",
      "epoch 702, loss 0.017998836934566498, R2 0.2699068784713745\n",
      "Eval loss 0.0245212335139513, R2 0.21036189794540405\n",
      "epoch 703, loss 0.017989791929721832, R2 0.27027428150177\n",
      "Eval loss 0.024507077410817146, R2 0.21081781387329102\n",
      "epoch 704, loss 0.017980769276618958, R2 0.2706397771835327\n",
      "Eval loss 0.024492941796779633, R2 0.21127301454544067\n",
      "epoch 705, loss 0.017971761524677277, R2 0.27100515365600586\n",
      "Eval loss 0.02447882853448391, R2 0.21172767877578735\n",
      "epoch 706, loss 0.01796277053654194, R2 0.27137041091918945\n",
      "Eval loss 0.024464743211865425, R2 0.21218031644821167\n",
      "epoch 707, loss 0.017953794449567795, R2 0.2717336416244507\n",
      "Eval loss 0.02445068396627903, R2 0.21263426542282104\n",
      "epoch 708, loss 0.017944835126399994, R2 0.2720978856086731\n",
      "Eval loss 0.024436645209789276, R2 0.2130858302116394\n",
      "epoch 709, loss 0.017935892567038536, R2 0.27246010303497314\n",
      "Eval loss 0.024422630667686462, R2 0.2135370969772339\n",
      "epoch 710, loss 0.017926964908838272, R2 0.2728222608566284\n",
      "Eval loss 0.024408644065260887, R2 0.21398687362670898\n",
      "epoch 711, loss 0.01791805401444435, R2 0.2731837034225464\n",
      "Eval loss 0.024394676089286804, R2 0.2144368290901184\n",
      "epoch 712, loss 0.017909158021211624, R2 0.2735445499420166\n",
      "Eval loss 0.02438073605298996, R2 0.21488624811172485\n",
      "epoch 713, loss 0.01790028065443039, R2 0.2739046812057495\n",
      "Eval loss 0.024366820231080055, R2 0.21533435583114624\n",
      "epoch 714, loss 0.0178914163261652, R2 0.27426445484161377\n",
      "Eval loss 0.02435292676091194, R2 0.21578222513198853\n",
      "epoch 715, loss 0.0178825706243515, R2 0.27462297677993774\n",
      "Eval loss 0.02433905564248562, R2 0.216228187084198\n",
      "epoch 716, loss 0.017873739823698997, R2 0.2749817967414856\n",
      "Eval loss 0.024325210601091385, R2 0.21667402982711792\n",
      "epoch 717, loss 0.017864925786852837, R2 0.2753387689590454\n",
      "Eval loss 0.02431139163672924, R2 0.2171192765235901\n",
      "epoch 718, loss 0.01785612665116787, R2 0.27569544315338135\n",
      "Eval loss 0.024297595024108887, R2 0.21756380796432495\n",
      "epoch 719, loss 0.017847342416644096, R2 0.27605199813842773\n",
      "Eval loss 0.024283817037940025, R2 0.21800726652145386\n",
      "epoch 720, loss 0.017838576808571815, R2 0.27640753984451294\n",
      "Eval loss 0.02427006885409355, R2 0.21844995021820068\n",
      "epoch 721, loss 0.01782982610166073, R2 0.2767619490623474\n",
      "Eval loss 0.02425634302198887, R2 0.21889197826385498\n",
      "epoch 722, loss 0.017821088433265686, R2 0.2771175503730774\n",
      "Eval loss 0.024242637678980827, R2 0.21933335065841675\n",
      "epoch 723, loss 0.017812369391322136, R2 0.27747035026550293\n",
      "Eval loss 0.024228958413004875, R2 0.2197730541229248\n",
      "epoch 724, loss 0.01780366338789463, R2 0.2778242826461792\n",
      "Eval loss 0.024215299636125565, R2 0.2202134132385254\n",
      "epoch 725, loss 0.017794974148273468, R2 0.2781761884689331\n",
      "Eval loss 0.024201668798923492, R2 0.22065258026123047\n",
      "epoch 726, loss 0.01778630167245865, R2 0.27852797508239746\n",
      "Eval loss 0.024188056588172913, R2 0.22109097242355347\n",
      "epoch 727, loss 0.017777644097805023, R2 0.27887916564941406\n",
      "Eval loss 0.024174470454454422, R2 0.22152841091156006\n",
      "epoch 728, loss 0.017768999561667442, R2 0.27923041582107544\n",
      "Eval loss 0.024160906672477722, R2 0.22196495532989502\n",
      "epoch 729, loss 0.017760373651981354, R2 0.279579758644104\n",
      "Eval loss 0.024147367104887962, R2 0.2224012017250061\n",
      "epoch 730, loss 0.01775176078081131, R2 0.27992933988571167\n",
      "Eval loss 0.024133846163749695, R2 0.22283637523651123\n",
      "epoch 731, loss 0.01774316467344761, R2 0.2802780270576477\n",
      "Eval loss 0.024120353162288666, R2 0.2232709527015686\n",
      "epoch 732, loss 0.017734583467245102, R2 0.280626118183136\n",
      "Eval loss 0.024106884375214577, R2 0.22370511293411255\n",
      "epoch 733, loss 0.01772601529955864, R2 0.2809733748435974\n",
      "Eval loss 0.02409343235194683, R2 0.224138081073761\n",
      "epoch 734, loss 0.01771746762096882, R2 0.28131961822509766\n",
      "Eval loss 0.024080006405711174, R2 0.224570631980896\n",
      "epoch 735, loss 0.017708932980895042, R2 0.2816660404205322\n",
      "Eval loss 0.024066602811217308, R2 0.2250012755393982\n",
      "epoch 736, loss 0.01770041137933731, R2 0.28201204538345337\n",
      "Eval loss 0.024053221568465233, R2 0.22543299198150635\n",
      "epoch 737, loss 0.01769190840423107, R2 0.28235721588134766\n",
      "Eval loss 0.02403986267745495, R2 0.22586292028427124\n",
      "epoch 738, loss 0.017683416604995728, R2 0.2827010750770569\n",
      "Eval loss 0.024026528000831604, R2 0.22629255056381226\n",
      "epoch 739, loss 0.017674941569566727, R2 0.28304457664489746\n",
      "Eval loss 0.024013211950659752, R2 0.22672134637832642\n",
      "epoch 740, loss 0.01766648143529892, R2 0.2833888530731201\n",
      "Eval loss 0.02399992197751999, R2 0.22714906930923462\n",
      "epoch 741, loss 0.017658036202192307, R2 0.2837308645248413\n",
      "Eval loss 0.023986654356122017, R2 0.22757583856582642\n",
      "epoch 742, loss 0.017649607732892036, R2 0.28407251834869385\n",
      "Eval loss 0.023973412811756134, R2 0.22800296545028687\n",
      "epoch 743, loss 0.01764119230210781, R2 0.28441500663757324\n",
      "Eval loss 0.023960188031196594, R2 0.22842860221862793\n",
      "epoch 744, loss 0.01763279177248478, R2 0.2847554087638855\n",
      "Eval loss 0.023946983739733696, R2 0.2288540005683899\n",
      "epoch 745, loss 0.01762440614402294, R2 0.2850950360298157\n",
      "Eval loss 0.023933803662657738, R2 0.22927772998809814\n",
      "epoch 746, loss 0.017616039142012596, R2 0.28543418645858765\n",
      "Eval loss 0.02392064593732357, R2 0.2297021746635437\n",
      "epoch 747, loss 0.017607679590582848, R2 0.2857740521430969\n",
      "Eval loss 0.023907508701086044, R2 0.23012518882751465\n",
      "epoch 748, loss 0.01759933866560459, R2 0.28611207008361816\n",
      "Eval loss 0.02389439567923546, R2 0.2305477261543274\n",
      "epoch 749, loss 0.01759101264178753, R2 0.2864501476287842\n",
      "Eval loss 0.023881306871771812, R2 0.23096919059753418\n",
      "epoch 750, loss 0.01758270338177681, R2 0.28678637742996216\n",
      "Eval loss 0.02386823669075966, R2 0.2313898801803589\n",
      "epoch 751, loss 0.017574405297636986, R2 0.2871232032775879\n",
      "Eval loss 0.023855186998844147, R2 0.23181027173995972\n",
      "epoch 752, loss 0.017566122114658356, R2 0.2874588966369629\n",
      "Eval loss 0.023842161521315575, R2 0.2322293519973755\n",
      "epoch 753, loss 0.01755785383284092, R2 0.2877945899963379\n",
      "Eval loss 0.023829160258173943, R2 0.23264801502227783\n",
      "epoch 754, loss 0.017549602314829826, R2 0.28812962770462036\n",
      "Eval loss 0.023816177621483803, R2 0.23306649923324585\n",
      "epoch 755, loss 0.017541363835334778, R2 0.2884635329246521\n",
      "Eval loss 0.023803217336535454, R2 0.23348313570022583\n",
      "epoch 756, loss 0.017533136531710625, R2 0.2887977957725525\n",
      "Eval loss 0.023790277540683746, R2 0.2339003086090088\n",
      "epoch 757, loss 0.017524925991892815, R2 0.2891302704811096\n",
      "Eval loss 0.02377735823392868, R2 0.2343156337738037\n",
      "epoch 758, loss 0.017516732215881348, R2 0.2894626259803772\n",
      "Eval loss 0.023764463141560555, R2 0.23473185300827026\n",
      "epoch 759, loss 0.017508551478385925, R2 0.2897953391075134\n",
      "Eval loss 0.02375158853828907, R2 0.23514646291732788\n",
      "epoch 760, loss 0.0175003819167614, R2 0.29012614488601685\n",
      "Eval loss 0.023738736286759377, R2 0.23556005954742432\n",
      "epoch 761, loss 0.017492229118943214, R2 0.29045629501342773\n",
      "Eval loss 0.023725900799036026, R2 0.23597317934036255\n",
      "epoch 762, loss 0.017484093084931374, R2 0.2907871603965759\n",
      "Eval loss 0.023713087663054466, R2 0.2363860011100769\n",
      "epoch 763, loss 0.017475968226790428, R2 0.2911161780357361\n",
      "Eval loss 0.023700302466750145, R2 0.2367975115776062\n",
      "epoch 764, loss 0.017467858269810677, R2 0.2914445996284485\n",
      "Eval loss 0.023687532171607018, R2 0.23720872402191162\n",
      "epoch 765, loss 0.01745976135134697, R2 0.29177379608154297\n",
      "Eval loss 0.02367478609085083, R2 0.23761963844299316\n",
      "epoch 766, loss 0.017451681196689606, R2 0.29210132360458374\n",
      "Eval loss 0.023662058636546135, R2 0.23802924156188965\n",
      "epoch 767, loss 0.017443612217903137, R2 0.2924286723136902\n",
      "Eval loss 0.02364935353398323, R2 0.23843812942504883\n",
      "epoch 768, loss 0.017435558140277863, R2 0.2927553653717041\n",
      "Eval loss 0.02363666705787182, R2 0.23884689807891846\n",
      "epoch 769, loss 0.017427518963813782, R2 0.29308146238327026\n",
      "Eval loss 0.023624004796147346, R2 0.239254891872406\n",
      "epoch 770, loss 0.017419492825865746, R2 0.29340702295303345\n",
      "Eval loss 0.023611361160874367, R2 0.2396610975265503\n",
      "epoch 771, loss 0.017411481589078903, R2 0.293731689453125\n",
      "Eval loss 0.023598739877343178, R2 0.240068256855011\n",
      "epoch 772, loss 0.017403481528162956, R2 0.2940564751625061\n",
      "Eval loss 0.023586135357618332, R2 0.2404744029045105\n",
      "epoch 773, loss 0.01739550195634365, R2 0.29438072443008423\n",
      "Eval loss 0.023573555052280426, R2 0.24087947607040405\n",
      "epoch 774, loss 0.017387527972459793, R2 0.2947036027908325\n",
      "Eval loss 0.02356099896132946, R2 0.241283118724823\n",
      "epoch 775, loss 0.017379572615027428, R2 0.29502660036087036\n",
      "Eval loss 0.023548459634184837, R2 0.2416875958442688\n",
      "epoch 776, loss 0.017371630296111107, R2 0.2953487038612366\n",
      "Eval loss 0.023535940796136856, R2 0.24209052324295044\n",
      "epoch 777, loss 0.01736369915306568, R2 0.29567015171051025\n",
      "Eval loss 0.023523440584540367, R2 0.24249351024627686\n",
      "epoch 778, loss 0.0173557847738266, R2 0.2959917187690735\n",
      "Eval loss 0.023510964587330818, R2 0.24289411306381226\n",
      "epoch 779, loss 0.01734788343310356, R2 0.29631173610687256\n",
      "Eval loss 0.02349850721657276, R2 0.24329572916030884\n",
      "epoch 780, loss 0.017339996993541718, R2 0.29663193225860596\n",
      "Eval loss 0.023486068472266197, R2 0.24369651079177856\n",
      "epoch 781, loss 0.01733212172985077, R2 0.2969511151313782\n",
      "Eval loss 0.023473652079701424, R2 0.24409639835357666\n",
      "epoch 782, loss 0.017324261367321014, R2 0.29726994037628174\n",
      "Eval loss 0.023461250588297844, R2 0.244495689868927\n",
      "epoch 783, loss 0.017316414043307304, R2 0.2975887656211853\n",
      "Eval loss 0.023448877036571503, R2 0.2448941469192505\n",
      "epoch 784, loss 0.01730857975780964, R2 0.29790574312210083\n",
      "Eval loss 0.023436522111296654, R2 0.24529224634170532\n",
      "epoch 785, loss 0.017300760373473167, R2 0.29822295904159546\n",
      "Eval loss 0.023424185812473297, R2 0.2456885576248169\n",
      "epoch 786, loss 0.01729295402765274, R2 0.29853951930999756\n",
      "Eval loss 0.023411870002746582, R2 0.24608588218688965\n",
      "epoch 787, loss 0.017285160720348358, R2 0.2988559603691101\n",
      "Eval loss 0.02339957468211651, R2 0.24648183584213257\n",
      "epoch 788, loss 0.01727738231420517, R2 0.2991718053817749\n",
      "Eval loss 0.023387297987937927, R2 0.24687618017196655\n",
      "epoch 789, loss 0.017269613221287727, R2 0.29948657751083374\n",
      "Eval loss 0.023375041782855988, R2 0.2472718358039856\n",
      "epoch 790, loss 0.017261860892176628, R2 0.2998015880584717\n",
      "Eval loss 0.02336280792951584, R2 0.24766576290130615\n",
      "epoch 791, loss 0.017254121601581573, R2 0.30011558532714844\n",
      "Eval loss 0.023350587114691734, R2 0.24805933237075806\n",
      "epoch 792, loss 0.017246393486857414, R2 0.30042845010757446\n",
      "Eval loss 0.02333839237689972, R2 0.248451828956604\n",
      "epoch 793, loss 0.017238682135939598, R2 0.3007413148880005\n",
      "Eval loss 0.023326218128204346, R2 0.24884337186813354\n",
      "epoch 794, loss 0.017230980098247528, R2 0.3010542392730713\n",
      "Eval loss 0.023314058780670166, R2 0.2492351531982422\n",
      "epoch 795, loss 0.0172232948243618, R2 0.3013654351234436\n",
      "Eval loss 0.023301923647522926, R2 0.24962639808654785\n",
      "epoch 796, loss 0.01721562072634697, R2 0.30167728662490845\n",
      "Eval loss 0.02328980527818203, R2 0.2500166893005371\n",
      "epoch 797, loss 0.017207961529493332, R2 0.3019871711730957\n",
      "Eval loss 0.023277705535292625, R2 0.25040650367736816\n",
      "epoch 798, loss 0.01720031350851059, R2 0.3022977113723755\n",
      "Eval loss 0.023265624418854713, R2 0.25079554319381714\n",
      "epoch 799, loss 0.017192678526043892, R2 0.30260735750198364\n",
      "Eval loss 0.02325356937944889, R2 0.25118374824523926\n",
      "epoch 800, loss 0.01718505658209324, R2 0.3029162287712097\n",
      "Eval loss 0.023241527378559113, R2 0.25157105922698975\n",
      "epoch 801, loss 0.01717744767665863, R2 0.3032248616218567\n",
      "Eval loss 0.023229507729411125, R2 0.25195854902267456\n",
      "epoch 802, loss 0.017169851809740067, R2 0.30353355407714844\n",
      "Eval loss 0.02321750484406948, R2 0.25234419107437134\n",
      "epoch 803, loss 0.017162270843982697, R2 0.3038408160209656\n",
      "Eval loss 0.023205524310469627, R2 0.2527309060096741\n",
      "epoch 804, loss 0.017154701054096222, R2 0.3041478395462036\n",
      "Eval loss 0.023193562403321266, R2 0.2531156539916992\n",
      "epoch 805, loss 0.017147142440080643, R2 0.3044544458389282\n",
      "Eval loss 0.023181617259979248, R2 0.2535002827644348\n",
      "epoch 806, loss 0.017139598727226257, R2 0.3047603964805603\n",
      "Eval loss 0.02316969446837902, R2 0.2538837790489197\n",
      "epoch 807, loss 0.017132069915533066, R2 0.3050658106803894\n",
      "Eval loss 0.023157786577939987, R2 0.25426793098449707\n",
      "epoch 808, loss 0.01712455041706562, R2 0.30537086725234985\n",
      "Eval loss 0.023145904764533043, R2 0.2546505331993103\n",
      "epoch 809, loss 0.01711704395711422, R2 0.3056756258010864\n",
      "Eval loss 0.023134035989642143, R2 0.2550327777862549\n",
      "epoch 810, loss 0.017109554260969162, R2 0.3059794306755066\n",
      "Eval loss 0.023122189566493034, R2 0.2554142475128174\n",
      "epoch 811, loss 0.017102070152759552, R2 0.3062826991081238\n",
      "Eval loss 0.023110361769795418, R2 0.25579512119293213\n",
      "epoch 812, loss 0.017094604671001434, R2 0.30658555030822754\n",
      "Eval loss 0.023098550736904144, R2 0.25617480278015137\n",
      "epoch 813, loss 0.017087150365114212, R2 0.30688798427581787\n",
      "Eval loss 0.023086758330464363, R2 0.256555438041687\n",
      "epoch 814, loss 0.017079707235097885, R2 0.3071903586387634\n",
      "Eval loss 0.023074986413121223, R2 0.2569342851638794\n",
      "epoch 815, loss 0.017072279006242752, R2 0.30749112367630005\n",
      "Eval loss 0.023063233122229576, R2 0.2573127746582031\n",
      "epoch 816, loss 0.017064861953258514, R2 0.307792067527771\n",
      "Eval loss 0.02305149845778942, R2 0.25769108533859253\n",
      "epoch 817, loss 0.01705745793879032, R2 0.3080923557281494\n",
      "Eval loss 0.02303978055715561, R2 0.2580680251121521\n",
      "epoch 818, loss 0.017050065100193024, R2 0.3083924651145935\n",
      "Eval loss 0.023028085008263588, R2 0.2584450840950012\n",
      "epoch 819, loss 0.01704268530011177, R2 0.30869239568710327\n",
      "Eval loss 0.02301640436053276, R2 0.25882095098495483\n",
      "epoch 820, loss 0.017035316675901413, R2 0.3089901804924011\n",
      "Eval loss 0.023004747927188873, R2 0.25919610261917114\n",
      "epoch 821, loss 0.01702796295285225, R2 0.30928874015808105\n",
      "Eval loss 0.02299310266971588, R2 0.2595711350440979\n",
      "epoch 822, loss 0.01702062040567398, R2 0.3095865845680237\n",
      "Eval loss 0.02298148162662983, R2 0.2599446773529053\n",
      "epoch 823, loss 0.017013292759656906, R2 0.30988413095474243\n",
      "Eval loss 0.02296987548470497, R2 0.2603191137313843\n",
      "epoch 824, loss 0.01700597256422043, R2 0.31018078327178955\n",
      "Eval loss 0.022958287969231606, R2 0.2606920003890991\n",
      "epoch 825, loss 0.016998669132590294, R2 0.3104778528213501\n",
      "Eval loss 0.02294672094285488, R2 0.26106470823287964\n",
      "epoch 826, loss 0.016991375014185905, R2 0.3107725977897644\n",
      "Eval loss 0.0229351706802845, R2 0.261436402797699\n",
      "epoch 827, loss 0.01698409579694271, R2 0.31106871366500854\n",
      "Eval loss 0.02292363904416561, R2 0.26180845499038696\n",
      "epoch 828, loss 0.01697682775557041, R2 0.31136298179626465\n",
      "Eval loss 0.022912126034498215, R2 0.26217877864837646\n",
      "epoch 829, loss 0.016969570890069008, R2 0.3116573095321655\n",
      "Eval loss 0.02290063165128231, R2 0.26254868507385254\n",
      "epoch 830, loss 0.01696232706308365, R2 0.3119511604309082\n",
      "Eval loss 0.0228891521692276, R2 0.2629185914993286\n",
      "epoch 831, loss 0.016955094411969185, R2 0.31224507093429565\n",
      "Eval loss 0.02287769317626953, R2 0.2632875442504883\n",
      "epoch 832, loss 0.016947874799370766, R2 0.31253743171691895\n",
      "Eval loss 0.022866256535053253, R2 0.2636549472808838\n",
      "epoch 833, loss 0.01694066822528839, R2 0.31282925605773926\n",
      "Eval loss 0.02285483106970787, R2 0.2640237808227539\n",
      "epoch 834, loss 0.016933470964431763, R2 0.3131219744682312\n",
      "Eval loss 0.02284342609345913, R2 0.26439082622528076\n",
      "epoch 835, loss 0.01692628674209118, R2 0.3134133219718933\n",
      "Eval loss 0.02283203788101673, R2 0.26475775241851807\n",
      "epoch 836, loss 0.01691911555826664, R2 0.3137039542198181\n",
      "Eval loss 0.022820673882961273, R2 0.26512348651885986\n",
      "epoch 837, loss 0.016911955550312996, R2 0.3139944076538086\n",
      "Eval loss 0.02280931919813156, R2 0.2654893398284912\n",
      "epoch 838, loss 0.016904808580875397, R2 0.3142843246459961\n",
      "Eval loss 0.02279798686504364, R2 0.26585423946380615\n",
      "epoch 839, loss 0.016897672787308693, R2 0.3145737648010254\n",
      "Eval loss 0.022786669433116913, R2 0.26621806621551514\n",
      "epoch 840, loss 0.016890550032258034, R2 0.31486350297927856\n",
      "Eval loss 0.022775374352931976, R2 0.2665822505950928\n",
      "epoch 841, loss 0.01688343845307827, R2 0.3151512145996094\n",
      "Eval loss 0.022764094173908234, R2 0.2669457197189331\n",
      "epoch 842, loss 0.0168763380497694, R2 0.31543922424316406\n",
      "Eval loss 0.022752830758690834, R2 0.26730841398239136\n",
      "epoch 843, loss 0.01686924882233143, R2 0.31572669744491577\n",
      "Eval loss 0.022741587832570076, R2 0.2676704525947571\n",
      "epoch 844, loss 0.01686217077076435, R2 0.3160144090652466\n",
      "Eval loss 0.022730359807610512, R2 0.2680320143699646\n",
      "epoch 845, loss 0.016855107620358467, R2 0.31630009412765503\n",
      "Eval loss 0.02271915040910244, R2 0.26839303970336914\n",
      "epoch 846, loss 0.01684805564582348, R2 0.316586434841156\n",
      "Eval loss 0.02270795777440071, R2 0.26875340938568115\n",
      "epoch 847, loss 0.016841012984514236, R2 0.3168720602989197\n",
      "Eval loss 0.022696783766150475, R2 0.2691132426261902\n",
      "epoch 848, loss 0.01683398336172104, R2 0.3171572685241699\n",
      "Eval loss 0.02268562838435173, R2 0.26947247982025146\n",
      "epoch 849, loss 0.016826963052153587, R2 0.31744199991226196\n",
      "Eval loss 0.02267448604106903, R2 0.26983124017715454\n",
      "epoch 850, loss 0.01681995950639248, R2 0.3177260756492615\n",
      "Eval loss 0.022663366049528122, R2 0.2701893448829651\n",
      "epoch 851, loss 0.016812965273857117, R2 0.3180105686187744\n",
      "Eval loss 0.022652260959148407, R2 0.2705463171005249\n",
      "epoch 852, loss 0.01680598221719265, R2 0.31829309463500977\n",
      "Eval loss 0.022641172632575035, R2 0.2709040641784668\n",
      "epoch 853, loss 0.01679901033639908, R2 0.31857579946517944\n",
      "Eval loss 0.022630104795098305, R2 0.27126020193099976\n",
      "epoch 854, loss 0.016792049631476402, R2 0.31885766983032227\n",
      "Eval loss 0.022619051858782768, R2 0.2716163992881775\n",
      "epoch 855, loss 0.01678510382771492, R2 0.3191399574279785\n",
      "Eval loss 0.022608013823628426, R2 0.2719718813896179\n",
      "epoch 856, loss 0.016778163611888885, R2 0.3194214701652527\n",
      "Eval loss 0.022596996277570724, R2 0.27232640981674194\n",
      "epoch 857, loss 0.016771240159869194, R2 0.3197031021118164\n",
      "Eval loss 0.022585995495319366, R2 0.2726808786392212\n",
      "epoch 858, loss 0.0167643241584301, R2 0.3199828267097473\n",
      "Eval loss 0.022575009614229202, R2 0.2730346918106079\n",
      "epoch 859, loss 0.0167574230581522, R2 0.3202633261680603\n",
      "Eval loss 0.02256404422223568, R2 0.27338773012161255\n",
      "epoch 860, loss 0.016750529408454895, R2 0.3205423951148987\n",
      "Eval loss 0.02255309373140335, R2 0.2737395167350769\n",
      "epoch 861, loss 0.016743652522563934, R2 0.3208214044570923\n",
      "Eval loss 0.022542161867022514, R2 0.2740921974182129\n",
      "epoch 862, loss 0.01673678308725357, R2 0.32109999656677246\n",
      "Eval loss 0.02253124490380287, R2 0.2744441628456116\n",
      "epoch 863, loss 0.01672992669045925, R2 0.32137835025787354\n",
      "Eval loss 0.022520340979099274, R2 0.27479439973831177\n",
      "epoch 864, loss 0.01672307960689068, R2 0.32165586948394775\n",
      "Eval loss 0.022509463131427765, R2 0.27514517307281494\n",
      "epoch 865, loss 0.01671624556183815, R2 0.3219327926635742\n",
      "Eval loss 0.0224985983222723, R2 0.2754952311515808\n",
      "epoch 866, loss 0.016709420830011368, R2 0.322209894657135\n",
      "Eval loss 0.022487744688987732, R2 0.2758445143699646\n",
      "epoch 867, loss 0.01670261099934578, R2 0.32248616218566895\n",
      "Eval loss 0.022476913407444954, R2 0.27619266510009766\n",
      "epoch 868, loss 0.016695808619260788, R2 0.3227618336677551\n",
      "Eval loss 0.02246609702706337, R2 0.27654188871383667\n",
      "epoch 869, loss 0.01668901927769184, R2 0.3230379819869995\n",
      "Eval loss 0.022455301135778427, R2 0.2768886089324951\n",
      "epoch 870, loss 0.01668224111199379, R2 0.3233124017715454\n",
      "Eval loss 0.02244452014565468, R2 0.2772367000579834\n",
      "epoch 871, loss 0.016675474122166634, R2 0.32358717918395996\n",
      "Eval loss 0.022433752194046974, R2 0.27758294343948364\n",
      "epoch 872, loss 0.016668716445565224, R2 0.3238612413406372\n",
      "Eval loss 0.02242300473153591, R2 0.27792954444885254\n",
      "epoch 873, loss 0.01666196994483471, R2 0.3241351842880249\n",
      "Eval loss 0.022412272170186043, R2 0.2782749533653259\n",
      "epoch 874, loss 0.01665523648262024, R2 0.32440781593322754\n",
      "Eval loss 0.02240155264735222, R2 0.2786206007003784\n",
      "epoch 875, loss 0.016648512333631516, R2 0.3246805667877197\n",
      "Eval loss 0.022390857338905334, R2 0.2789647579193115\n",
      "epoch 876, loss 0.016641801223158836, R2 0.3249528408050537\n",
      "Eval loss 0.022380171343684196, R2 0.2793086767196655\n",
      "epoch 877, loss 0.016635097563266754, R2 0.32522469758987427\n",
      "Eval loss 0.0223695021122694, R2 0.27965247631073\n",
      "epoch 878, loss 0.016628408804535866, R2 0.32549601793289185\n",
      "Eval loss 0.022358857095241547, R2 0.27999502420425415\n",
      "epoch 879, loss 0.016621727496385574, R2 0.32576704025268555\n",
      "Eval loss 0.02234821952879429, R2 0.28033775091171265\n",
      "epoch 880, loss 0.016615059226751328, R2 0.32603776454925537\n",
      "Eval loss 0.022337600588798523, R2 0.28067970275878906\n",
      "epoch 881, loss 0.016608403995633125, R2 0.326307475566864\n",
      "Eval loss 0.0223269984126091, R2 0.2810213565826416\n",
      "epoch 882, loss 0.01660175621509552, R2 0.32657766342163086\n",
      "Eval loss 0.02231641858816147, R2 0.2813621163368225\n",
      "epoch 883, loss 0.01659512333571911, R2 0.32684624195098877\n",
      "Eval loss 0.02230585180222988, R2 0.28170233964920044\n",
      "epoch 884, loss 0.016588496044278145, R2 0.3271147608757019\n",
      "Eval loss 0.02229529805481434, R2 0.2820419669151306\n",
      "epoch 885, loss 0.016581883653998375, R2 0.32738322019577026\n",
      "Eval loss 0.02228476107120514, R2 0.2823806405067444\n",
      "epoch 886, loss 0.016575278714299202, R2 0.3276512026786804\n",
      "Eval loss 0.022274238988757133, R2 0.28272008895874023\n",
      "epoch 887, loss 0.016568684950470924, R2 0.3279183506965637\n",
      "Eval loss 0.02226373739540577, R2 0.28305763006210327\n",
      "epoch 888, loss 0.016562102362513542, R2 0.32818615436553955\n",
      "Eval loss 0.02225324884057045, R2 0.2833951711654663\n",
      "epoch 889, loss 0.016555532813072205, R2 0.32845157384872437\n",
      "Eval loss 0.022242775186896324, R2 0.28373265266418457\n",
      "epoch 890, loss 0.016548972576856613, R2 0.32871901988983154\n",
      "Eval loss 0.02223232015967369, R2 0.2840697765350342\n",
      "epoch 891, loss 0.01654241979122162, R2 0.32898402214050293\n",
      "Eval loss 0.022221878170967102, R2 0.2844054102897644\n",
      "epoch 892, loss 0.016535881906747818, R2 0.32924920320510864\n",
      "Eval loss 0.022211454808712006, R2 0.28474193811416626\n",
      "epoch 893, loss 0.016529353335499763, R2 0.3295140862464905\n",
      "Eval loss 0.022201046347618103, R2 0.2850770950317383\n",
      "epoch 894, loss 0.016522834077477455, R2 0.3297784924507141\n",
      "Eval loss 0.022190652787685394, R2 0.2854122519493103\n",
      "epoch 895, loss 0.016516325995326042, R2 0.33004218339920044\n",
      "Eval loss 0.02218027599155903, R2 0.28574591875076294\n",
      "epoch 896, loss 0.016509827226400375, R2 0.330305814743042\n",
      "Eval loss 0.022169912233948708, R2 0.2860797047615051\n",
      "epoch 897, loss 0.016503341495990753, R2 0.33056914806365967\n",
      "Eval loss 0.022159568965435028, R2 0.28641271591186523\n",
      "epoch 898, loss 0.016496865078806877, R2 0.3308318257331848\n",
      "Eval loss 0.022149238735437393, R2 0.28674542903900146\n",
      "epoch 899, loss 0.016490401700139046, R2 0.33109480142593384\n",
      "Eval loss 0.022138921543955803, R2 0.2870769500732422\n",
      "epoch 900, loss 0.01648394577205181, R2 0.3313564658164978\n",
      "Eval loss 0.022128626704216003, R2 0.28740912675857544\n",
      "epoch 901, loss 0.016477501019835472, R2 0.3316173553466797\n",
      "Eval loss 0.02211834490299225, R2 0.28774046897888184\n",
      "epoch 902, loss 0.01647106744349003, R2 0.3318790793418884\n",
      "Eval loss 0.022108078002929688, R2 0.2880708575248718\n",
      "epoch 903, loss 0.01646464318037033, R2 0.33213889598846436\n",
      "Eval loss 0.02209782414138317, R2 0.28840088844299316\n",
      "epoch 904, loss 0.01645822823047638, R2 0.3323994278907776\n",
      "Eval loss 0.022087588906288147, R2 0.2887309193611145\n",
      "epoch 905, loss 0.016451826319098473, R2 0.3326588273048401\n",
      "Eval loss 0.022077366709709167, R2 0.2890598177909851\n",
      "epoch 906, loss 0.016445433720946312, R2 0.33291810750961304\n",
      "Eval loss 0.02206716313958168, R2 0.2893877625465393\n",
      "epoch 907, loss 0.016439048573374748, R2 0.3331770896911621\n",
      "Eval loss 0.022056972607970238, R2 0.2897157073020935\n",
      "epoch 908, loss 0.01643267832696438, R2 0.3334354758262634\n",
      "Eval loss 0.02204679697751999, R2 0.29004424810409546\n",
      "epoch 909, loss 0.016426315531134605, R2 0.33369362354278564\n",
      "Eval loss 0.022036638110876083, R2 0.2903714179992676\n",
      "epoch 910, loss 0.016419963911175728, R2 0.3339512348175049\n",
      "Eval loss 0.02202649600803852, R2 0.2906973361968994\n",
      "epoch 911, loss 0.016413623467087746, R2 0.33420872688293457\n",
      "Eval loss 0.022016365081071854, R2 0.2910233736038208\n",
      "epoch 912, loss 0.01640729047358036, R2 0.3344650864601135\n",
      "Eval loss 0.022006254643201828, R2 0.2913498282432556\n",
      "epoch 913, loss 0.01640096865594387, R2 0.33472174406051636\n",
      "Eval loss 0.021996155381202698, R2 0.29167526960372925\n",
      "epoch 914, loss 0.016394658014178276, R2 0.33497774600982666\n",
      "Eval loss 0.02198607288300991, R2 0.2919995188713074\n",
      "epoch 915, loss 0.016388358548283577, R2 0.33523327112197876\n",
      "Eval loss 0.021976003423333168, R2 0.2923239469528198\n",
      "epoch 916, loss 0.016382066532969475, R2 0.3354882001876831\n",
      "Eval loss 0.021965952590107918, R2 0.29264765977859497\n",
      "epoch 917, loss 0.016375785693526268, R2 0.3357430100440979\n",
      "Eval loss 0.021955914795398712, R2 0.2929708957672119\n",
      "epoch 918, loss 0.016369516029953957, R2 0.3359981179237366\n",
      "Eval loss 0.0219458919018507, R2 0.29329341650009155\n",
      "epoch 919, loss 0.01636325567960739, R2 0.33625155687332153\n",
      "Eval loss 0.02193588763475418, R2 0.29361557960510254\n",
      "epoch 920, loss 0.016357004642486572, R2 0.33650535345077515\n",
      "Eval loss 0.021925892680883408, R2 0.2939378619194031\n",
      "epoch 921, loss 0.01635076478123665, R2 0.33675795793533325\n",
      "Eval loss 0.021915914490818977, R2 0.2942589521408081\n",
      "epoch 922, loss 0.016344532370567322, R2 0.33701103925704956\n",
      "Eval loss 0.02190595306456089, R2 0.29457974433898926\n",
      "epoch 923, loss 0.01633831299841404, R2 0.337263822555542\n",
      "Eval loss 0.021896004676818848, R2 0.294900119304657\n",
      "epoch 924, loss 0.016332101076841354, R2 0.33751553297042847\n",
      "Eval loss 0.021886073052883148, R2 0.29521995782852173\n",
      "epoch 925, loss 0.016325900331139565, R2 0.33776676654815674\n",
      "Eval loss 0.021876156330108643, R2 0.2955392599105835\n",
      "epoch 926, loss 0.01631971076130867, R2 0.33801841735839844\n",
      "Eval loss 0.02186625264585018, R2 0.2958579659461975\n",
      "epoch 927, loss 0.016313528642058372, R2 0.33826887607574463\n",
      "Eval loss 0.021856365725398064, R2 0.29617613554000854\n",
      "epoch 928, loss 0.01630735583603382, R2 0.33851975202560425\n",
      "Eval loss 0.02184649184346199, R2 0.2964944839477539\n",
      "epoch 929, loss 0.016301194205880165, R2 0.33876943588256836\n",
      "Eval loss 0.02183663286268711, R2 0.2968119978904724\n",
      "epoch 930, loss 0.016295043751597404, R2 0.33901864290237427\n",
      "Eval loss 0.021826792508363724, R2 0.29712891578674316\n",
      "epoch 931, loss 0.01628890261054039, R2 0.3392675518989563\n",
      "Eval loss 0.021816961467266083, R2 0.29744547605514526\n",
      "epoch 932, loss 0.01628277078270912, R2 0.339516282081604\n",
      "Eval loss 0.021807145327329636, R2 0.29776155948638916\n",
      "epoch 933, loss 0.0162766482681036, R2 0.3397645950317383\n",
      "Eval loss 0.02179734781384468, R2 0.2980775237083435\n",
      "epoch 934, loss 0.016270536929368973, R2 0.3400125503540039\n",
      "Eval loss 0.02178756147623062, R2 0.2983921766281128\n",
      "epoch 935, loss 0.016264433041214943, R2 0.340259850025177\n",
      "Eval loss 0.021777791902422905, R2 0.2987068295478821\n",
      "epoch 936, loss 0.01625834032893181, R2 0.340507447719574\n",
      "Eval loss 0.021768035367131233, R2 0.29902100563049316\n",
      "epoch 937, loss 0.01625225506722927, R2 0.34075379371643066\n",
      "Eval loss 0.021758293733000755, R2 0.29933470487594604\n",
      "epoch 938, loss 0.016246182844042778, R2 0.34100115299224854\n",
      "Eval loss 0.02174856886267662, R2 0.29964786767959595\n",
      "epoch 939, loss 0.01624011993408203, R2 0.3412468433380127\n",
      "Eval loss 0.02173885516822338, R2 0.29996126890182495\n",
      "epoch 940, loss 0.01623406447470188, R2 0.3414919376373291\n",
      "Eval loss 0.021729156374931335, R2 0.3002723455429077\n",
      "epoch 941, loss 0.016228020191192627, R2 0.34173762798309326\n",
      "Eval loss 0.021719474345445633, R2 0.30058497190475464\n",
      "epoch 942, loss 0.01622198522090912, R2 0.3419821858406067\n",
      "Eval loss 0.021709805354475975, R2 0.30089616775512695\n",
      "epoch 943, loss 0.016215957701206207, R2 0.34222692251205444\n",
      "Eval loss 0.021700149402022362, R2 0.30120712518692017\n",
      "epoch 944, loss 0.01620994135737419, R2 0.3424709439277649\n",
      "Eval loss 0.02169051021337509, R2 0.3015177249908447\n",
      "epoch 945, loss 0.01620393618941307, R2 0.342714786529541\n",
      "Eval loss 0.021680884063243866, R2 0.30182743072509766\n",
      "epoch 946, loss 0.016197938472032547, R2 0.34295761585235596\n",
      "Eval loss 0.021671270951628685, R2 0.30213743448257446\n",
      "epoch 947, loss 0.01619195193052292, R2 0.343200147151947\n",
      "Eval loss 0.021661672741174698, R2 0.3024458885192871\n",
      "epoch 948, loss 0.016185974702239037, R2 0.3434426784515381\n",
      "Eval loss 0.021652091294527054, R2 0.3027547001838684\n",
      "epoch 949, loss 0.016180003061890602, R2 0.3436848521232605\n",
      "Eval loss 0.021642522886395454, R2 0.3030627965927124\n",
      "epoch 950, loss 0.016174044460058212, R2 0.3439265489578247\n",
      "Eval loss 0.0216329675167799, R2 0.30337047576904297\n",
      "epoch 951, loss 0.016168097034096718, R2 0.3441680669784546\n",
      "Eval loss 0.02162342518568039, R2 0.3036777377128601\n",
      "epoch 952, loss 0.01616215519607067, R2 0.3444085717201233\n",
      "Eval loss 0.021613897755742073, R2 0.3039845824241638\n",
      "epoch 953, loss 0.01615622453391552, R2 0.34464967250823975\n",
      "Eval loss 0.02160438522696495, R2 0.3042904734611511\n",
      "epoch 954, loss 0.016150301322340965, R2 0.34488916397094727\n",
      "Eval loss 0.021594883874058723, R2 0.30459660291671753\n",
      "epoch 955, loss 0.016144389286637306, R2 0.34512919187545776\n",
      "Eval loss 0.021585403010249138, R2 0.30490219593048096\n",
      "epoch 956, loss 0.016138486564159393, R2 0.3453693985939026\n",
      "Eval loss 0.021575933322310448, R2 0.3052075505256653\n",
      "epoch 957, loss 0.016132591292262077, R2 0.3456079959869385\n",
      "Eval loss 0.021566476672887802, R2 0.30551183223724365\n",
      "epoch 958, loss 0.016126709058880806, R2 0.34584665298461914\n",
      "Eval loss 0.021557031199336052, R2 0.3058162331581116\n",
      "epoch 959, loss 0.01612083427608013, R2 0.34608471393585205\n",
      "Eval loss 0.021547604352235794, R2 0.3061193823814392\n",
      "epoch 960, loss 0.016114966943860054, R2 0.3463234305381775\n",
      "Eval loss 0.021538186818361282, R2 0.30642223358154297\n",
      "epoch 961, loss 0.016109110787510872, R2 0.346560537815094\n",
      "Eval loss 0.021528786048293114, R2 0.3067253828048706\n",
      "epoch 962, loss 0.016103262081742287, R2 0.34679847955703735\n",
      "Eval loss 0.02151939831674099, R2 0.3070274591445923\n",
      "epoch 963, loss 0.016097424551844597, R2 0.34703528881073\n",
      "Eval loss 0.02151002734899521, R2 0.3073294758796692\n",
      "epoch 964, loss 0.016091594472527504, R2 0.34727102518081665\n",
      "Eval loss 0.021500665694475174, R2 0.30763089656829834\n",
      "epoch 965, loss 0.016085775569081306, R2 0.3475070595741272\n",
      "Eval loss 0.021491320803761482, R2 0.3079318404197693\n",
      "epoch 966, loss 0.016079964116215706, R2 0.3477422595024109\n",
      "Eval loss 0.021481985226273537, R2 0.3082321882247925\n",
      "epoch 967, loss 0.01607416197657585, R2 0.3479781746864319\n",
      "Eval loss 0.021472668275237083, R2 0.3085324764251709\n",
      "epoch 968, loss 0.016068369150161743, R2 0.3482136130332947\n",
      "Eval loss 0.021463362500071526, R2 0.3088321089744568\n",
      "epoch 969, loss 0.01606258563697338, R2 0.34844791889190674\n",
      "Eval loss 0.02145407162606716, R2 0.3091315031051636\n",
      "epoch 970, loss 0.016056811437010765, R2 0.3486819267272949\n",
      "Eval loss 0.021444791927933693, R2 0.30942994356155396\n",
      "epoch 971, loss 0.016051046550273895, R2 0.3489155173301697\n",
      "Eval loss 0.021435530856251717, R2 0.30972862243652344\n",
      "epoch 972, loss 0.016045289114117622, R2 0.34914928674697876\n",
      "Eval loss 0.021426282823085785, R2 0.31002622842788696\n",
      "epoch 973, loss 0.016039540991187096, R2 0.34938299655914307\n",
      "Eval loss 0.02141704596579075, R2 0.3103238344192505\n",
      "epoch 974, loss 0.016033804044127464, R2 0.3496151566505432\n",
      "Eval loss 0.021407820284366608, R2 0.3106204867362976\n",
      "epoch 975, loss 0.01602807454764843, R2 0.34984761476516724\n",
      "Eval loss 0.02139861136674881, R2 0.3109166622161865\n",
      "epoch 976, loss 0.016022352501749992, R2 0.35007989406585693\n",
      "Eval loss 0.021389415487647057, R2 0.311212956905365\n",
      "epoch 977, loss 0.01601664163172245, R2 0.3503115773200989\n",
      "Eval loss 0.0213802307844162, R2 0.3115091323852539\n",
      "epoch 978, loss 0.016010938212275505, R2 0.35054266452789307\n",
      "Eval loss 0.021371060982346535, R2 0.3118044137954712\n",
      "epoch 979, loss 0.016005245968699455, R2 0.3507741093635559\n",
      "Eval loss 0.021361909806728363, R2 0.3120993971824646\n",
      "epoch 980, loss 0.015999561175704002, R2 0.3510047197341919\n",
      "Eval loss 0.02135276608169079, R2 0.3123936057090759\n",
      "epoch 981, loss 0.015993883833289146, R2 0.351234495639801\n",
      "Eval loss 0.02134363353252411, R2 0.3126870393753052\n",
      "epoch 982, loss 0.015988215804100037, R2 0.35146409273147583\n",
      "Eval loss 0.021334517747163773, R2 0.3129812479019165\n",
      "epoch 983, loss 0.015982557088136673, R2 0.3516939878463745\n",
      "Eval loss 0.02132541686296463, R2 0.3132745027542114\n",
      "epoch 984, loss 0.015976907685399055, R2 0.35192304849624634\n",
      "Eval loss 0.021316325291991234, R2 0.31356704235076904\n",
      "epoch 985, loss 0.015971267595887184, R2 0.3521518111228943\n",
      "Eval loss 0.02130724862217903, R2 0.31385934352874756\n",
      "epoch 986, loss 0.01596563495695591, R2 0.35238033533096313\n",
      "Eval loss 0.021298186853528023, R2 0.31415116786956787\n",
      "epoch 987, loss 0.01596001163125038, R2 0.35260868072509766\n",
      "Eval loss 0.02128913626074791, R2 0.31444239616394043\n",
      "epoch 988, loss 0.0159543976187706, R2 0.3528364300727844\n",
      "Eval loss 0.02128010056912899, R2 0.3147333860397339\n",
      "epoch 989, loss 0.015948791056871414, R2 0.35306358337402344\n",
      "Eval loss 0.021271076053380966, R2 0.3150239586830139\n",
      "epoch 990, loss 0.015943195670843124, R2 0.3532905578613281\n",
      "Eval loss 0.021262066438794136, R2 0.3153141140937805\n",
      "epoch 991, loss 0.01593760773539543, R2 0.35351723432540894\n",
      "Eval loss 0.0212530717253685, R2 0.31560397148132324\n",
      "epoch 992, loss 0.015932025387883186, R2 0.35374337434768677\n",
      "Eval loss 0.02124408446252346, R2 0.31589359045028687\n",
      "epoch 993, loss 0.015926456078886986, R2 0.35396963357925415\n",
      "Eval loss 0.021235113963484764, R2 0.31618207693099976\n",
      "epoch 994, loss 0.015920892357826233, R2 0.3541952967643738\n",
      "Eval loss 0.021226156502962112, R2 0.316470742225647\n",
      "epoch 995, loss 0.015915339812636375, R2 0.35442054271698\n",
      "Eval loss 0.021217210218310356, R2 0.31675881147384644\n",
      "epoch 996, loss 0.015909790992736816, R2 0.3546455502510071\n",
      "Eval loss 0.021208276972174644, R2 0.31704646348953247\n",
      "epoch 997, loss 0.015904255211353302, R2 0.354870080947876\n",
      "Eval loss 0.021199356764554977, R2 0.3173336982727051\n",
      "epoch 998, loss 0.015898728743195534, R2 0.3550940752029419\n",
      "Eval loss 0.021190447732806206, R2 0.31762057542800903\n",
      "epoch 999, loss 0.015893206000328064, R2 0.35531800985336304\n",
      "Eval loss 0.021181555464863777, R2 0.31790614128112793\n",
      "epoch 1000, loss 0.01588769629597664, R2 0.35554254055023193\n",
      "Eval loss 0.021172678098082542, R2 0.31819283962249756\n",
      "epoch 1001, loss 0.01588219404220581, R2 0.3557649850845337\n",
      "Eval loss 0.021163806319236755, R2 0.31847846508026123\n",
      "epoch 1002, loss 0.01587669923901558, R2 0.35598838329315186\n",
      "Eval loss 0.02115495130419731, R2 0.3187636137008667\n",
      "epoch 1003, loss 0.015871215611696243, R2 0.3562103509902954\n",
      "Eval loss 0.021146109327673912, R2 0.3190481662750244\n",
      "epoch 1004, loss 0.015865735709667206, R2 0.35643309354782104\n",
      "Eval loss 0.021137280389666557, R2 0.3193320631980896\n",
      "epoch 1005, loss 0.015860266983509064, R2 0.3566548824310303\n",
      "Eval loss 0.021128464490175247, R2 0.3196166157722473\n",
      "epoch 1006, loss 0.015854807570576668, R2 0.35687583684921265\n",
      "Eval loss 0.02111966162919998, R2 0.3199000954627991\n",
      "epoch 1007, loss 0.01584935560822487, R2 0.3570970296859741\n",
      "Eval loss 0.021110868081450462, R2 0.3201832175254822\n",
      "epoch 1008, loss 0.015843911096453667, R2 0.35731786489486694\n",
      "Eval loss 0.021102087572216988, R2 0.32046622037887573\n",
      "epoch 1009, loss 0.01583847776055336, R2 0.35753804445266724\n",
      "Eval loss 0.021093325689435005, R2 0.3207474946975708\n",
      "epoch 1010, loss 0.01583305187523365, R2 0.3577588200569153\n",
      "Eval loss 0.02108457125723362, R2 0.3210300803184509\n",
      "epoch 1011, loss 0.015827633440494537, R2 0.3579781651496887\n",
      "Eval loss 0.02107582986354828, R2 0.3213115334510803\n",
      "epoch 1012, loss 0.01582222431898117, R2 0.35819756984710693\n",
      "Eval loss 0.021067101508378983, R2 0.32159262895584106\n",
      "epoch 1013, loss 0.0158168226480484, R2 0.35841667652130127\n",
      "Eval loss 0.02105838805437088, R2 0.3218732476234436\n",
      "epoch 1014, loss 0.015811428427696228, R2 0.35863548517227173\n",
      "Eval loss 0.021049685776233673, R2 0.3221534490585327\n",
      "epoch 1015, loss 0.0158060435205698, R2 0.35885417461395264\n",
      "Eval loss 0.02104099467396736, R2 0.3224332928657532\n",
      "epoch 1016, loss 0.01580066606402397, R2 0.3590720295906067\n",
      "Eval loss 0.021032316610217094, R2 0.3227129578590393\n",
      "epoch 1017, loss 0.015795297920703888, R2 0.3592902421951294\n",
      "Eval loss 0.021023649722337723, R2 0.32299184799194336\n",
      "epoch 1018, loss 0.0157899372279644, R2 0.3595072031021118\n",
      "Eval loss 0.021014997735619545, R2 0.3232702612876892\n",
      "epoch 1019, loss 0.01578458771109581, R2 0.3597237467765808\n",
      "Eval loss 0.02100636065006256, R2 0.3235486149787903\n",
      "epoch 1020, loss 0.015779241919517517, R2 0.3599410653114319\n",
      "Eval loss 0.020997729152441025, R2 0.32382655143737793\n",
      "epoch 1021, loss 0.01577390730381012, R2 0.36015743017196655\n",
      "Eval loss 0.020989110693335533, R2 0.3241041302680969\n",
      "epoch 1022, loss 0.01576858013868332, R2 0.360373318195343\n",
      "Eval loss 0.020980510860681534, R2 0.3243810534477234\n",
      "epoch 1023, loss 0.015763260424137115, R2 0.3605896234512329\n",
      "Eval loss 0.020971916615962982, R2 0.3246577978134155\n",
      "epoch 1024, loss 0.015757950022816658, R2 0.3608047366142273\n",
      "Eval loss 0.020963339135050774, R2 0.3249340057373047\n",
      "epoch 1025, loss 0.015752645209431648, R2 0.36102062463760376\n",
      "Eval loss 0.02095477283000946, R2 0.3252098560333252\n",
      "epoch 1026, loss 0.015747351571917534, R2 0.3612349033355713\n",
      "Eval loss 0.020946219563484192, R2 0.3254852890968323\n",
      "epoch 1027, loss 0.015742067247629166, R2 0.3614490032196045\n",
      "Eval loss 0.02093767561018467, R2 0.3257606029510498\n",
      "epoch 1028, loss 0.015736786648631096, R2 0.36166316270828247\n",
      "Eval loss 0.02092914842069149, R2 0.3260346055030823\n",
      "epoch 1029, loss 0.01573151722550392, R2 0.36187744140625\n",
      "Eval loss 0.020920632407069206, R2 0.3263092637062073\n",
      "epoch 1030, loss 0.015726255252957344, R2 0.36209040880203247\n",
      "Eval loss 0.02091212570667267, R2 0.3265826106071472\n",
      "epoch 1031, loss 0.015721000730991364, R2 0.36230355501174927\n",
      "Eval loss 0.020903632044792175, R2 0.32685667276382446\n",
      "epoch 1032, loss 0.01571575365960598, R2 0.3625168800354004\n",
      "Eval loss 0.020895151421427727, R2 0.3271297812461853\n",
      "epoch 1033, loss 0.015710514038801193, R2 0.36272889375686646\n",
      "Eval loss 0.020886681973934174, R2 0.32740235328674316\n",
      "epoch 1034, loss 0.015705285593867302, R2 0.36294102668762207\n",
      "Eval loss 0.020878227427601814, R2 0.32767462730407715\n",
      "epoch 1035, loss 0.015700064599514008, R2 0.36315280199050903\n",
      "Eval loss 0.0208697821944952, R2 0.32794612646102905\n",
      "epoch 1036, loss 0.01569484733045101, R2 0.36336439847946167\n",
      "Eval loss 0.020861346274614334, R2 0.3282177448272705\n",
      "epoch 1037, loss 0.01568964123725891, R2 0.3635755777359009\n",
      "Eval loss 0.02085292898118496, R2 0.3284894824028015\n",
      "epoch 1038, loss 0.015684442594647408, R2 0.3637864589691162\n",
      "Eval loss 0.02084451913833618, R2 0.32876068353652954\n",
      "epoch 1039, loss 0.0156792514026165, R2 0.3639969825744629\n",
      "Eval loss 0.02083612233400345, R2 0.32903027534484863\n",
      "epoch 1040, loss 0.01567406952381134, R2 0.36420726776123047\n",
      "Eval loss 0.02082773856818676, R2 0.3293006420135498\n",
      "epoch 1041, loss 0.015668893232941628, R2 0.3644168972969055\n",
      "Eval loss 0.020819365978240967, R2 0.329570472240448\n",
      "epoch 1042, loss 0.01566372625529766, R2 0.36462652683258057\n",
      "Eval loss 0.02081100456416607, R2 0.3298395276069641\n",
      "epoch 1043, loss 0.01565856859087944, R2 0.3648364543914795\n",
      "Eval loss 0.020802658051252365, R2 0.33010828495025635\n",
      "epoch 1044, loss 0.015653416514396667, R2 0.3650447726249695\n",
      "Eval loss 0.02079431712627411, R2 0.33037686347961426\n",
      "epoch 1045, loss 0.01564827375113964, R2 0.36525362730026245\n",
      "Eval loss 0.020785992965102196, R2 0.33064496517181396\n",
      "epoch 1046, loss 0.015643136575818062, R2 0.3654627203941345\n",
      "Eval loss 0.02077767811715603, R2 0.330912709236145\n",
      "epoch 1047, loss 0.01563800871372223, R2 0.36566996574401855\n",
      "Eval loss 0.020769380033016205, R2 0.3311799168586731\n",
      "epoch 1048, loss 0.015632890164852142, R2 0.365877628326416\n",
      "Eval loss 0.020761089399456978, R2 0.33144688606262207\n",
      "epoch 1049, loss 0.015627777203917503, R2 0.3660850524902344\n",
      "Eval loss 0.020752809941768646, R2 0.33171355724334717\n",
      "epoch 1050, loss 0.015622672624886036, R2 0.3662922978401184\n",
      "Eval loss 0.02074454538524151, R2 0.33197903633117676\n",
      "epoch 1051, loss 0.01561757642775774, R2 0.36649876832962036\n",
      "Eval loss 0.020736288279294968, R2 0.33224552869796753\n",
      "epoch 1052, loss 0.015612487681210041, R2 0.36670541763305664\n",
      "Eval loss 0.02072804607450962, R2 0.33251070976257324\n",
      "epoch 1053, loss 0.015607405453920364, R2 0.36691129207611084\n",
      "Eval loss 0.02071981690824032, R2 0.33277595043182373\n",
      "epoch 1054, loss 0.01560233160853386, R2 0.36711692810058594\n",
      "Eval loss 0.02071159891784191, R2 0.3330399990081787\n",
      "epoch 1055, loss 0.015597266145050526, R2 0.3673228621482849\n",
      "Eval loss 0.02070339024066925, R2 0.3333049416542053\n",
      "epoch 1056, loss 0.015592208132147789, R2 0.3675277829170227\n",
      "Eval loss 0.020695190876722336, R2 0.33356839418411255\n",
      "epoch 1057, loss 0.015587158501148224, R2 0.3677324056625366\n",
      "Eval loss 0.020687008276581764, R2 0.33383268117904663\n",
      "epoch 1058, loss 0.015582115389406681, R2 0.367936909198761\n",
      "Eval loss 0.02067883498966694, R2 0.3340950608253479\n",
      "epoch 1059, loss 0.01557707879692316, R2 0.3681414723396301\n",
      "Eval loss 0.020670674741268158, R2 0.33435845375061035\n",
      "epoch 1060, loss 0.015572050586342812, R2 0.3683454394340515\n",
      "Eval loss 0.020662527531385422, R2 0.3346208333969116\n",
      "epoch 1061, loss 0.015567031688988209, R2 0.36854875087738037\n",
      "Eval loss 0.020654387772083282, R2 0.334882915019989\n",
      "epoch 1062, loss 0.015562020242214203, R2 0.36875230073928833\n",
      "Eval loss 0.02064625732600689, R2 0.3351447582244873\n",
      "epoch 1063, loss 0.01555701531469822, R2 0.3689560890197754\n",
      "Eval loss 0.02063814550638199, R2 0.33540594577789307\n",
      "epoch 1064, loss 0.015552017837762833, R2 0.3691588044166565\n",
      "Eval loss 0.020630039274692535, R2 0.3356669545173645\n",
      "epoch 1065, loss 0.015547027811408043, R2 0.36936020851135254\n",
      "Eval loss 0.020621947944164276, R2 0.3359275460243225\n",
      "epoch 1066, loss 0.015542047098279, R2 0.3695629835128784\n",
      "Eval loss 0.020613867789506912, R2 0.33618712425231934\n",
      "epoch 1067, loss 0.015537071041762829, R2 0.3697648048400879\n",
      "Eval loss 0.020605796948075294, R2 0.3364478349685669\n",
      "epoch 1068, loss 0.015532102435827255, R2 0.36996638774871826\n",
      "Eval loss 0.02059774100780487, R2 0.3367074728012085\n",
      "epoch 1069, loss 0.015527143143117428, R2 0.37016749382019043\n",
      "Eval loss 0.020589694380760193, R2 0.3369661569595337\n",
      "epoch 1070, loss 0.015522191300988197, R2 0.3703683614730835\n",
      "Eval loss 0.02058165706694126, R2 0.33722496032714844\n",
      "epoch 1071, loss 0.01551724411547184, R2 0.37056857347488403\n",
      "Eval loss 0.020573634654283524, R2 0.3374835252761841\n",
      "epoch 1072, loss 0.015512309037148952, R2 0.37076878547668457\n",
      "Eval loss 0.020565621554851532, R2 0.33774077892303467\n",
      "epoch 1073, loss 0.015507379546761513, R2 0.3709694743156433\n",
      "Eval loss 0.020557623356580734, R2 0.33799874782562256\n",
      "epoch 1074, loss 0.015502456575632095, R2 0.37116843461990356\n",
      "Eval loss 0.020549630746245384, R2 0.338256299495697\n",
      "epoch 1075, loss 0.015497541055083275, R2 0.37136805057525635\n",
      "Eval loss 0.02054165117442608, R2 0.3385133147239685\n",
      "epoch 1076, loss 0.0154926348477602, R2 0.3715665936470032\n",
      "Eval loss 0.02053368091583252, R2 0.3387695550918579\n",
      "epoch 1077, loss 0.015487734228372574, R2 0.3717656135559082\n",
      "Eval loss 0.020525727421045303, R2 0.3390260934829712\n",
      "epoch 1078, loss 0.015482839196920395, R2 0.3719639182090759\n",
      "Eval loss 0.020517781376838684, R2 0.33928197622299194\n",
      "epoch 1079, loss 0.015477955341339111, R2 0.37216299772262573\n",
      "Eval loss 0.02050984650850296, R2 0.33953750133514404\n",
      "epoch 1080, loss 0.015473075211048126, R2 0.3723604679107666\n",
      "Eval loss 0.02050192281603813, R2 0.3397926092147827\n",
      "epoch 1081, loss 0.015468204393982887, R2 0.3725575804710388\n",
      "Eval loss 0.020494012162089348, R2 0.34004735946655273\n",
      "epoch 1082, loss 0.015463341027498245, R2 0.3727550506591797\n",
      "Eval loss 0.02048611454665661, R2 0.3403016924858093\n",
      "epoch 1083, loss 0.0154584851115942, R2 0.37295228242874146\n",
      "Eval loss 0.020478222519159317, R2 0.34055501222610474\n",
      "epoch 1084, loss 0.015453636646270752, R2 0.37314897775650024\n",
      "Eval loss 0.02047034353017807, R2 0.34080958366394043\n",
      "epoch 1085, loss 0.015448794700205326, R2 0.37334561347961426\n",
      "Eval loss 0.02046247571706772, R2 0.3410629630088806\n",
      "epoch 1086, loss 0.015443959273397923, R2 0.37354129552841187\n",
      "Eval loss 0.02045462094247341, R2 0.3413160443305969\n",
      "epoch 1087, loss 0.01543913222849369, R2 0.37373727560043335\n",
      "Eval loss 0.020446771755814552, R2 0.34156858921051025\n",
      "epoch 1088, loss 0.01543431170284748, R2 0.3739326000213623\n",
      "Eval loss 0.020438937470316887, R2 0.3418208956718445\n",
      "epoch 1089, loss 0.015429500490427017, R2 0.37412822246551514\n",
      "Eval loss 0.020431116223335266, R2 0.3420727849006653\n",
      "epoch 1090, loss 0.015424693003296852, R2 0.374322772026062\n",
      "Eval loss 0.02042330428957939, R2 0.34232431650161743\n",
      "epoch 1091, loss 0.015419892966747284, R2 0.37451744079589844\n",
      "Eval loss 0.020415499806404114, R2 0.34257566928863525\n",
      "epoch 1092, loss 0.015415101312100887, R2 0.37471187114715576\n",
      "Eval loss 0.02040771022439003, R2 0.3428264856338501\n",
      "epoch 1093, loss 0.015410318039357662, R2 0.37490636110305786\n",
      "Eval loss 0.020399929955601692, R2 0.34307700395584106\n",
      "epoch 1094, loss 0.015405540354549885, R2 0.37510013580322266\n",
      "Eval loss 0.02039216086268425, R2 0.3433268070220947\n",
      "epoch 1095, loss 0.015400771982967854, R2 0.3752930760383606\n",
      "Eval loss 0.020384402945637703, R2 0.3435770273208618\n",
      "epoch 1096, loss 0.015396008267998695, R2 0.3754863142967224\n",
      "Eval loss 0.02037665620446205, R2 0.3438262939453125\n",
      "epoch 1097, loss 0.015391254797577858, R2 0.3756791353225708\n",
      "Eval loss 0.020368918776512146, R2 0.34407544136047363\n",
      "epoch 1098, loss 0.015386504121124744, R2 0.37587183713912964\n",
      "Eval loss 0.020361192524433136, R2 0.34432488679885864\n",
      "epoch 1099, loss 0.015381762757897377, R2 0.3760641813278198\n",
      "Eval loss 0.02035347744822502, R2 0.34457290172576904\n",
      "epoch 1100, loss 0.015377027913928032, R2 0.37625622749328613\n",
      "Eval loss 0.020345773547887802, R2 0.3448207974433899\n",
      "epoch 1101, loss 0.015372300520539284, R2 0.37644749879837036\n",
      "Eval loss 0.020338082686066628, R2 0.34506863355636597\n",
      "epoch 1102, loss 0.015367581509053707, R2 0.37663912773132324\n",
      "Eval loss 0.0203303974121809, R2 0.3453161120414734\n",
      "epoch 1103, loss 0.015362867154181004, R2 0.3768311142921448\n",
      "Eval loss 0.020322728902101517, R2 0.3455631136894226\n",
      "epoch 1104, loss 0.015358162112534046, R2 0.3770217299461365\n",
      "Eval loss 0.02031506597995758, R2 0.3458096385002136\n",
      "epoch 1105, loss 0.015353462658822536, R2 0.3772121071815491\n",
      "Eval loss 0.02030741423368454, R2 0.346055805683136\n",
      "epoch 1106, loss 0.015348770655691624, R2 0.3774024248123169\n",
      "Eval loss 0.020299773663282394, R2 0.3463022708892822\n",
      "epoch 1107, loss 0.015344083309173584, R2 0.3775925636291504\n",
      "Eval loss 0.020292144268751144, R2 0.34654778242111206\n",
      "epoch 1108, loss 0.015339404344558716, R2 0.37778234481811523\n",
      "Eval loss 0.02028452605009079, R2 0.3467932939529419\n",
      "epoch 1109, loss 0.01533473376184702, R2 0.3779718279838562\n",
      "Eval loss 0.02027691900730133, R2 0.347037672996521\n",
      "epoch 1110, loss 0.01533006876707077, R2 0.3781610131263733\n",
      "Eval loss 0.020269323140382767, R2 0.3472824692726135\n",
      "epoch 1111, loss 0.015325413085520267, R2 0.3783496618270874\n",
      "Eval loss 0.02026173286139965, R2 0.34752726554870605\n",
      "epoch 1112, loss 0.015320761129260063, R2 0.3785390853881836\n",
      "Eval loss 0.02025415748357773, R2 0.34777122735977173\n",
      "epoch 1113, loss 0.01531611941754818, R2 0.37872689962387085\n",
      "Eval loss 0.020246591418981552, R2 0.3480148911476135\n",
      "epoch 1114, loss 0.015311483293771744, R2 0.3789149522781372\n",
      "Eval loss 0.02023903839290142, R2 0.3482580780982971\n",
      "epoch 1115, loss 0.015306850895285606, R2 0.37910282611846924\n",
      "Eval loss 0.020231494680047035, R2 0.34850043058395386\n",
      "epoch 1116, loss 0.015302227810025215, R2 0.3792903423309326\n",
      "Eval loss 0.020223956555128098, R2 0.3487439751625061\n",
      "epoch 1117, loss 0.015297613106667995, R2 0.3794780373573303\n",
      "Eval loss 0.020216431468725204, R2 0.34898608922958374\n",
      "epoch 1118, loss 0.015293002128601074, R2 0.3796645998954773\n",
      "Eval loss 0.020208921283483505, R2 0.3492277264595032\n",
      "epoch 1119, loss 0.0152884004637599, R2 0.37985122203826904\n",
      "Eval loss 0.020201416686177254, R2 0.3494698405265808\n",
      "epoch 1120, loss 0.015283804386854172, R2 0.38003748655319214\n",
      "Eval loss 0.020193923264741898, R2 0.34971094131469727\n",
      "epoch 1121, loss 0.015279215760529041, R2 0.3802242875099182\n",
      "Eval loss 0.020186441019177437, R2 0.3499518632888794\n",
      "epoch 1122, loss 0.015274634584784508, R2 0.3804096579551697\n",
      "Eval loss 0.02017897181212902, R2 0.3501923680305481\n",
      "epoch 1123, loss 0.015270058065652847, R2 0.38059526681900024\n",
      "Eval loss 0.020171508193016052, R2 0.35043269395828247\n",
      "epoch 1124, loss 0.015265489928424358, R2 0.38078081607818604\n",
      "Eval loss 0.02016405761241913, R2 0.3506724238395691\n",
      "epoch 1125, loss 0.015260930173099041, R2 0.38096553087234497\n",
      "Eval loss 0.02015661634504795, R2 0.35091185569763184\n",
      "epoch 1126, loss 0.015256374143064022, R2 0.38115012645721436\n",
      "Eval loss 0.020149188116192818, R2 0.3511512875556946\n",
      "epoch 1127, loss 0.015251826494932175, R2 0.3813348412513733\n",
      "Eval loss 0.020141765475273132, R2 0.3513904809951782\n",
      "epoch 1128, loss 0.015247284434735775, R2 0.381519079208374\n",
      "Eval loss 0.02013435587286949, R2 0.35162848234176636\n",
      "epoch 1129, loss 0.015242750756442547, R2 0.3817034363746643\n",
      "Eval loss 0.020126957446336746, R2 0.35186755657196045\n",
      "epoch 1130, loss 0.015238221734762192, R2 0.38188666105270386\n",
      "Eval loss 0.020119570195674896, R2 0.3521052598953247\n",
      "epoch 1131, loss 0.015233701094985008, R2 0.38207000494003296\n",
      "Eval loss 0.020112190395593643, R2 0.3523429036140442\n",
      "epoch 1132, loss 0.015229186974465847, R2 0.38225340843200684\n",
      "Eval loss 0.020104819908738136, R2 0.35258036851882935\n",
      "epoch 1133, loss 0.015224678441882133, R2 0.38243603706359863\n",
      "Eval loss 0.020097462460398674, R2 0.3528171181678772\n",
      "epoch 1134, loss 0.015220178291201591, R2 0.3826183080673218\n",
      "Eval loss 0.020090114325284958, R2 0.3530537486076355\n",
      "epoch 1135, loss 0.015215682797133923, R2 0.3828006982803345\n",
      "Eval loss 0.02008277364075184, R2 0.3532901406288147\n",
      "epoch 1136, loss 0.015211193822324276, R2 0.3829835057258606\n",
      "Eval loss 0.020075447857379913, R2 0.35352545976638794\n",
      "epoch 1137, loss 0.015206712298095226, R2 0.38316500186920166\n",
      "Eval loss 0.020068131387233734, R2 0.3537617325782776\n",
      "epoch 1138, loss 0.015202236361801624, R2 0.3833463788032532\n",
      "Eval loss 0.020060820505023003, R2 0.3539971113204956\n",
      "epoch 1139, loss 0.015197769738733768, R2 0.38352757692337036\n",
      "Eval loss 0.020053520798683167, R2 0.3542323708534241\n",
      "epoch 1140, loss 0.015193307772278786, R2 0.38370829820632935\n",
      "Eval loss 0.020046234130859375, R2 0.3544667959213257\n",
      "epoch 1141, loss 0.01518885139375925, R2 0.3838895559310913\n",
      "Eval loss 0.02003895305097103, R2 0.3547006845474243\n",
      "epoch 1142, loss 0.015184405259788036, R2 0.38406920433044434\n",
      "Eval loss 0.02003168873488903, R2 0.3549346327781677\n",
      "epoch 1143, loss 0.015179961919784546, R2 0.38424938917160034\n",
      "Eval loss 0.020024430006742477, R2 0.35516834259033203\n",
      "epoch 1144, loss 0.015175526030361652, R2 0.38442981243133545\n",
      "Eval loss 0.02001718245446682, R2 0.3554023504257202\n",
      "epoch 1145, loss 0.015171097591519356, R2 0.3846094608306885\n",
      "Eval loss 0.020009944215416908, R2 0.355635404586792\n",
      "epoch 1146, loss 0.015166674740612507, R2 0.3847893476486206\n",
      "Eval loss 0.020002715289592743, R2 0.355868399143219\n",
      "epoch 1147, loss 0.015162259340286255, R2 0.38496798276901245\n",
      "Eval loss 0.019995499402284622, R2 0.35610055923461914\n",
      "epoch 1148, loss 0.015157848596572876, R2 0.3851468563079834\n",
      "Eval loss 0.0199882909655571, R2 0.3563327193260193\n",
      "epoch 1149, loss 0.015153447166085243, R2 0.3853253722190857\n",
      "Eval loss 0.01998109184205532, R2 0.3565645217895508\n",
      "epoch 1150, loss 0.015149051323533058, R2 0.38550347089767456\n",
      "Eval loss 0.019973905757069588, R2 0.3567959666252136\n",
      "epoch 1151, loss 0.01514466293156147, R2 0.38568150997161865\n",
      "Eval loss 0.01996672712266445, R2 0.35702717304229736\n",
      "epoch 1152, loss 0.015140277333557606, R2 0.3858596086502075\n",
      "Eval loss 0.01995955966413021, R2 0.3572579622268677\n",
      "epoch 1153, loss 0.015135901048779488, R2 0.3860369324684143\n",
      "Eval loss 0.019952401518821716, R2 0.3574880361557007\n",
      "epoch 1154, loss 0.015131530351936817, R2 0.3862146735191345\n",
      "Eval loss 0.01994525082409382, R2 0.35771888494491577\n",
      "epoch 1155, loss 0.015127165243029594, R2 0.38639146089553833\n",
      "Eval loss 0.019938113167881966, R2 0.35794776678085327\n",
      "epoch 1156, loss 0.015122806653380394, R2 0.38656824827194214\n",
      "Eval loss 0.01993098296225071, R2 0.35817819833755493\n",
      "epoch 1157, loss 0.015118453651666641, R2 0.3867446184158325\n",
      "Eval loss 0.01992386393249035, R2 0.3584076166152954\n",
      "epoch 1158, loss 0.015114109963178635, R2 0.3869210481643677\n",
      "Eval loss 0.019916754215955734, R2 0.3586365580558777\n",
      "epoch 1159, loss 0.015109769999980927, R2 0.3870972990989685\n",
      "Eval loss 0.019909651950001717, R2 0.35886502265930176\n",
      "epoch 1160, loss 0.015105437487363815, R2 0.38727259635925293\n",
      "Eval loss 0.019902562722563744, R2 0.3590930104255676\n",
      "epoch 1161, loss 0.015101112425327301, R2 0.3874483108520508\n",
      "Eval loss 0.019895480945706367, R2 0.3593209981918335\n",
      "epoch 1162, loss 0.015096791088581085, R2 0.3876240849494934\n",
      "Eval loss 0.019888410344719887, R2 0.3595488667488098\n",
      "epoch 1163, loss 0.015092478133738041, R2 0.387798547744751\n",
      "Eval loss 0.019881349056959152, R2 0.35977649688720703\n",
      "epoch 1164, loss 0.01508816983550787, R2 0.3879733085632324\n",
      "Eval loss 0.019874297082424164, R2 0.36000359058380127\n",
      "epoch 1165, loss 0.01508386805653572, R2 0.38814777135849\n",
      "Eval loss 0.01986725628376007, R2 0.3602302670478821\n",
      "epoch 1166, loss 0.015079574659466743, R2 0.3883219361305237\n",
      "Eval loss 0.019860222935676575, R2 0.36045676469802856\n",
      "epoch 1167, loss 0.015075284987688065, R2 0.38849568367004395\n",
      "Eval loss 0.019853202626109123, R2 0.3606829047203064\n",
      "epoch 1168, loss 0.015071004629135132, R2 0.38867002725601196\n",
      "Eval loss 0.01984618790447712, R2 0.360908567905426\n",
      "epoch 1169, loss 0.015066728927195072, R2 0.38884299993515015\n",
      "Eval loss 0.01983918622136116, R2 0.36113405227661133\n",
      "epoch 1170, loss 0.015062456950545311, R2 0.3890162706375122\n",
      "Eval loss 0.019832191988825798, R2 0.36135929822921753\n",
      "epoch 1171, loss 0.015058194287121296, R2 0.3891894221305847\n",
      "Eval loss 0.019825205206871033, R2 0.3615844249725342\n",
      "epoch 1172, loss 0.015053936280310154, R2 0.3893621563911438\n",
      "Eval loss 0.019818229600787163, R2 0.3618084788322449\n",
      "epoch 1173, loss 0.015049684792757034, R2 0.3895348310470581\n",
      "Eval loss 0.019811267033219337, R2 0.36203330755233765\n",
      "epoch 1174, loss 0.015045441687107086, R2 0.38970649242401123\n",
      "Eval loss 0.01980431005358696, R2 0.36225730180740356\n",
      "epoch 1175, loss 0.015041203238070011, R2 0.38987863063812256\n",
      "Eval loss 0.019797364249825478, R2 0.3624809980392456\n",
      "epoch 1176, loss 0.015036971308290958, R2 0.3900505304336548\n",
      "Eval loss 0.019790425896644592, R2 0.36270439624786377\n",
      "epoch 1177, loss 0.015032744035124779, R2 0.3902217745780945\n",
      "Eval loss 0.0197835024446249, R2 0.3629271984100342\n",
      "epoch 1178, loss 0.015028524212539196, R2 0.3903927206993103\n",
      "Eval loss 0.019776584580540657, R2 0.3631501793861389\n",
      "epoch 1179, loss 0.015024309046566486, R2 0.3905641436576843\n",
      "Eval loss 0.01976967230439186, R2 0.3633723258972168\n",
      "epoch 1180, loss 0.015020102262496948, R2 0.39073407649993896\n",
      "Eval loss 0.01976277306675911, R2 0.3635948896408081\n",
      "epoch 1181, loss 0.015015900135040283, R2 0.3909047842025757\n",
      "Eval loss 0.019755885004997253, R2 0.36381614208221436\n",
      "epoch 1182, loss 0.015011705458164215, R2 0.3910747170448303\n",
      "Eval loss 0.019749004393815994, R2 0.36403828859329224\n",
      "epoch 1183, loss 0.01500751357525587, R2 0.39124542474746704\n",
      "Eval loss 0.01974213309586048, R2 0.36425894498825073\n",
      "epoch 1184, loss 0.015003331936895847, R2 0.3914145827293396\n",
      "Eval loss 0.019735271111130714, R2 0.3644798994064331\n",
      "epoch 1185, loss 0.014999154023826122, R2 0.3915840983390808\n",
      "Eval loss 0.019728418439626694, R2 0.3647007942199707\n",
      "epoch 1186, loss 0.014994981698691845, R2 0.39175283908843994\n",
      "Eval loss 0.01972157508134842, R2 0.364921510219574\n",
      "epoch 1187, loss 0.01499081589281559, R2 0.3919229507446289\n",
      "Eval loss 0.01971474289894104, R2 0.36514097452163696\n",
      "epoch 1188, loss 0.014986656606197357, R2 0.39209073781967163\n",
      "Eval loss 0.01970791630446911, R2 0.36536139249801636\n",
      "epoch 1189, loss 0.014982503838837147, R2 0.3922596573829651\n",
      "Eval loss 0.019701100885868073, R2 0.36558085680007935\n",
      "epoch 1190, loss 0.014978358522057533, R2 0.39242804050445557\n",
      "Eval loss 0.019694296643137932, R2 0.36580002307891846\n",
      "epoch 1191, loss 0.014974217861890793, R2 0.3925955891609192\n",
      "Eval loss 0.019687501713633537, R2 0.3660188317298889\n",
      "epoch 1192, loss 0.014970080927014351, R2 0.3927633762359619\n",
      "Eval loss 0.01968071423470974, R2 0.3662373423576355\n",
      "epoch 1193, loss 0.014965951442718506, R2 0.39293062686920166\n",
      "Eval loss 0.01967393420636654, R2 0.3664551377296448\n",
      "epoch 1194, loss 0.014961829409003258, R2 0.39309781789779663\n",
      "Eval loss 0.019667167216539383, R2 0.3666735887527466\n",
      "epoch 1195, loss 0.014957712031900883, R2 0.3932650685310364\n",
      "Eval loss 0.019660405814647675, R2 0.3668913245201111\n",
      "epoch 1196, loss 0.014953600242733955, R2 0.3934319019317627\n",
      "Eval loss 0.01965365558862686, R2 0.36710870265960693\n",
      "epoch 1197, loss 0.014949495904147625, R2 0.39359837770462036\n",
      "Eval loss 0.019646914675831795, R2 0.3673257827758789\n",
      "epoch 1198, loss 0.014945397153496742, R2 0.3937646150588989\n",
      "Eval loss 0.019640183076262474, R2 0.367542564868927\n",
      "epoch 1199, loss 0.014941301196813583, R2 0.39393049478530884\n",
      "Eval loss 0.0196334607899189, R2 0.3677590489387512\n",
      "epoch 1200, loss 0.01493721455335617, R2 0.3940962553024292\n",
      "Eval loss 0.019626745954155922, R2 0.367975652217865\n",
      "epoch 1201, loss 0.014933133497834206, R2 0.39426255226135254\n",
      "Eval loss 0.01962003856897354, R2 0.36819082498550415\n",
      "epoch 1202, loss 0.014929057098925114, R2 0.39442741870880127\n",
      "Eval loss 0.019613344222307205, R2 0.36840707063674927\n",
      "epoch 1203, loss 0.014924987219274044, R2 0.3945927023887634\n",
      "Eval loss 0.019606657326221466, R2 0.3686221241950989\n",
      "epoch 1204, loss 0.014920925721526146, R2 0.3947576880455017\n",
      "Eval loss 0.019599979743361473, R2 0.36883723735809326\n",
      "epoch 1205, loss 0.014916867017745972, R2 0.3949223756790161\n",
      "Eval loss 0.019593311473727226, R2 0.3690517544746399\n",
      "epoch 1206, loss 0.014912813901901245, R2 0.39508605003356934\n",
      "Eval loss 0.019586650654673576, R2 0.36926621198654175\n",
      "epoch 1207, loss 0.01490876916795969, R2 0.3952503800392151\n",
      "Eval loss 0.019580001011490822, R2 0.36948031187057495\n",
      "epoch 1208, loss 0.014904728159308434, R2 0.3954147696495056\n",
      "Eval loss 0.019573358818888664, R2 0.36969447135925293\n",
      "epoch 1209, loss 0.014900692738592625, R2 0.39557796716690063\n",
      "Eval loss 0.019566727802157402, R2 0.3699079751968384\n",
      "epoch 1210, loss 0.014896663837134838, R2 0.39574164152145386\n",
      "Eval loss 0.019560102373361588, R2 0.3701215386390686\n",
      "epoch 1211, loss 0.0148926405236125, R2 0.3959046006202698\n",
      "Eval loss 0.01955348812043667, R2 0.3703345060348511\n",
      "epoch 1212, loss 0.014888623729348183, R2 0.3960679769515991\n",
      "Eval loss 0.019546883180737495, R2 0.37054646015167236\n",
      "epoch 1213, loss 0.014884612523019314, R2 0.3962298035621643\n",
      "Eval loss 0.01954028569161892, R2 0.3707594871520996\n",
      "epoch 1214, loss 0.014880606904625893, R2 0.39639270305633545\n",
      "Eval loss 0.01953369937837124, R2 0.3709713816642761\n",
      "epoch 1215, loss 0.01487660687416792, R2 0.39655500650405884\n",
      "Eval loss 0.019527120515704155, R2 0.3711836338043213\n",
      "epoch 1216, loss 0.014872614294290543, R2 0.3967169523239136\n",
      "Eval loss 0.019520549103617668, R2 0.3713949918746948\n",
      "epoch 1217, loss 0.01486862450838089, R2 0.39687949419021606\n",
      "Eval loss 0.019513988867402077, R2 0.3716062903404236\n",
      "epoch 1218, loss 0.014864642173051834, R2 0.39704006910324097\n",
      "Eval loss 0.019507436081767082, R2 0.37181735038757324\n",
      "epoch 1219, loss 0.014860665425658226, R2 0.3972018361091614\n",
      "Eval loss 0.019500892609357834, R2 0.3720279932022095\n",
      "epoch 1220, loss 0.014856696128845215, R2 0.3973628878593445\n",
      "Eval loss 0.01949435845017433, R2 0.3722386360168457\n",
      "epoch 1221, loss 0.014852728694677353, R2 0.39752334356307983\n",
      "Eval loss 0.019487833604216576, R2 0.3724483251571655\n",
      "epoch 1222, loss 0.014848769642412663, R2 0.39768439531326294\n",
      "Eval loss 0.019481316208839417, R2 0.3726584315299988\n",
      "epoch 1223, loss 0.01484481617808342, R2 0.3978445529937744\n",
      "Eval loss 0.019474806264042854, R2 0.37286823987960815\n",
      "epoch 1224, loss 0.014840868301689625, R2 0.3980048894882202\n",
      "Eval loss 0.01946830563247204, R2 0.3730771541595459\n",
      "epoch 1225, loss 0.014836925081908703, R2 0.3981645703315735\n",
      "Eval loss 0.019461816176772118, R2 0.3732861876487732\n",
      "epoch 1226, loss 0.014832987450063229, R2 0.39832431077957153\n",
      "Eval loss 0.019455336034297943, R2 0.37349504232406616\n",
      "epoch 1227, loss 0.014829057268798351, R2 0.3984842300415039\n",
      "Eval loss 0.019448861479759216, R2 0.3737033009529114\n",
      "epoch 1228, loss 0.014825130812823772, R2 0.39864301681518555\n",
      "Eval loss 0.019442398101091385, R2 0.37391090393066406\n",
      "epoch 1229, loss 0.014821212738752365, R2 0.3988022208213806\n",
      "Eval loss 0.019435940310359, R2 0.3741195797920227\n",
      "epoch 1230, loss 0.014817296527326107, R2 0.39896082878112793\n",
      "Eval loss 0.019429493695497513, R2 0.37432706356048584\n",
      "epoch 1231, loss 0.014813387766480446, R2 0.39911913871765137\n",
      "Eval loss 0.01942305453121662, R2 0.37453460693359375\n",
      "epoch 1232, loss 0.014809484593570232, R2 0.3992781639099121\n",
      "Eval loss 0.019416626542806625, R2 0.374741792678833\n",
      "epoch 1233, loss 0.014805588871240616, R2 0.39943569898605347\n",
      "Eval loss 0.019410202279686928, R2 0.3749484419822693\n",
      "epoch 1234, loss 0.014801694080233574, R2 0.399593710899353\n",
      "Eval loss 0.019403789192438126, R2 0.37515515089035034\n",
      "epoch 1235, loss 0.014797807671129704, R2 0.39975106716156006\n",
      "Eval loss 0.01939738728106022, R2 0.3753611445426941\n",
      "epoch 1236, loss 0.01479392871260643, R2 0.3999086618423462\n",
      "Eval loss 0.01939099095761776, R2 0.37556707859039307\n",
      "epoch 1237, loss 0.01479005254805088, R2 0.40006589889526367\n",
      "Eval loss 0.019384602084755898, R2 0.3757728338241577\n",
      "epoch 1238, loss 0.014786182902753353, R2 0.40022289752960205\n",
      "Eval loss 0.01937822625041008, R2 0.37597817182540894\n",
      "epoch 1239, loss 0.014782317914068699, R2 0.40037965774536133\n",
      "Eval loss 0.01937185600399971, R2 0.3761827349662781\n",
      "epoch 1240, loss 0.014778459444642067, R2 0.4005366563796997\n",
      "Eval loss 0.019365495070815086, R2 0.3763881325721741\n",
      "epoch 1241, loss 0.014774607494473457, R2 0.4006921648979187\n",
      "Eval loss 0.01935914158821106, R2 0.37659311294555664\n",
      "epoch 1242, loss 0.014770759269595146, R2 0.40084850788116455\n",
      "Eval loss 0.019352799281477928, R2 0.3767969608306885\n",
      "epoch 1243, loss 0.014766915701329708, R2 0.401004433631897\n",
      "Eval loss 0.019346460700035095, R2 0.37700068950653076\n",
      "epoch 1244, loss 0.014763081446290016, R2 0.40115976333618164\n",
      "Eval loss 0.019340135157108307, R2 0.3772047758102417\n",
      "epoch 1245, loss 0.014759249985218048, R2 0.4013153910636902\n",
      "Eval loss 0.019333815202116966, R2 0.3774082660675049\n",
      "epoch 1246, loss 0.014755424112081528, R2 0.4014705419540405\n",
      "Eval loss 0.01932750642299652, R2 0.3776114583015442\n",
      "epoch 1247, loss 0.014751603826880455, R2 0.40162551403045654\n",
      "Eval loss 0.019321205094456673, R2 0.3778143525123596\n",
      "epoch 1248, loss 0.01474778912961483, R2 0.40178048610687256\n",
      "Eval loss 0.01931491307914257, R2 0.37801700830459595\n",
      "epoch 1249, loss 0.014743980951607227, R2 0.40193426609039307\n",
      "Eval loss 0.019308624789118767, R2 0.3782194256782532\n",
      "epoch 1250, loss 0.014740176498889923, R2 0.40208882093429565\n",
      "Eval loss 0.019302349537611008, R2 0.3784215450286865\n",
      "epoch 1251, loss 0.014736378565430641, R2 0.40224337577819824\n",
      "Eval loss 0.019296081736683846, R2 0.378623366355896\n",
      "epoch 1252, loss 0.014732583425939083, R2 0.4023972749710083\n",
      "Eval loss 0.01928982324898243, R2 0.37882494926452637\n",
      "epoch 1253, loss 0.014728796668350697, R2 0.4025513529777527\n",
      "Eval loss 0.01928357034921646, R2 0.37902623414993286\n",
      "epoch 1254, loss 0.014725015498697758, R2 0.40270406007766724\n",
      "Eval loss 0.019277328625321388, R2 0.3792271018028259\n",
      "epoch 1255, loss 0.014721238985657692, R2 0.40285724401474\n",
      "Eval loss 0.019271094352006912, R2 0.3794282078742981\n",
      "epoch 1256, loss 0.0147174671292305, R2 0.4030100107192993\n",
      "Eval loss 0.019264869391918182, R2 0.3796284794807434\n",
      "epoch 1257, loss 0.014713701792061329, R2 0.40316319465637207\n",
      "Eval loss 0.0192586500197649, R2 0.3798287510871887\n",
      "epoch 1258, loss 0.014709940180182457, R2 0.40331530570983887\n",
      "Eval loss 0.019252441823482513, R2 0.38002872467041016\n",
      "epoch 1259, loss 0.014706185087561607, R2 0.40346765518188477\n",
      "Eval loss 0.019246239215135574, R2 0.3802284002304077\n",
      "epoch 1260, loss 0.01470243651419878, R2 0.4036198854446411\n",
      "Eval loss 0.01924004778265953, R2 0.3804277777671814\n",
      "epoch 1261, loss 0.014698691666126251, R2 0.4037718176841736\n",
      "Eval loss 0.019233861938118935, R2 0.3806271553039551\n",
      "epoch 1262, loss 0.014694953337311745, R2 0.4039234519004822\n",
      "Eval loss 0.019227685406804085, R2 0.38082587718963623\n",
      "epoch 1263, loss 0.014691216871142387, R2 0.4040752649307251\n",
      "Eval loss 0.01922151818871498, R2 0.38102447986602783\n",
      "epoch 1264, loss 0.014687489718198776, R2 0.40422624349594116\n",
      "Eval loss 0.019215356558561325, R2 0.38122230768203735\n",
      "epoch 1265, loss 0.014683766290545464, R2 0.4043769836425781\n",
      "Eval loss 0.019209204241633415, R2 0.3814210295677185\n",
      "epoch 1266, loss 0.014680049382150173, R2 0.40452802181243896\n",
      "Eval loss 0.01920306123793125, R2 0.3816182613372803\n",
      "epoch 1267, loss 0.014676336199045181, R2 0.4046786427497864\n",
      "Eval loss 0.019196925684809685, R2 0.3818166255950928\n",
      "epoch 1268, loss 0.014672627672553062, R2 0.40482908487319946\n",
      "Eval loss 0.019190799444913864, R2 0.3820136785507202\n",
      "epoch 1269, loss 0.014668924733996391, R2 0.40497928857803345\n",
      "Eval loss 0.01918468251824379, R2 0.38221049308776855\n",
      "epoch 1270, loss 0.014665228314697742, R2 0.40512943267822266\n",
      "Eval loss 0.019178571179509163, R2 0.3824074864387512\n",
      "epoch 1271, loss 0.014661536552011967, R2 0.405278742313385\n",
      "Eval loss 0.019172467291355133, R2 0.3826040029525757\n",
      "epoch 1272, loss 0.014657850377261639, R2 0.4054291844367981\n",
      "Eval loss 0.01916637271642685, R2 0.38280028104782104\n",
      "epoch 1273, loss 0.014654167927801609, R2 0.4055778384208679\n",
      "Eval loss 0.019160285592079163, R2 0.38299649953842163\n",
      "epoch 1274, loss 0.014650492928922176, R2 0.40572667121887207\n",
      "Eval loss 0.019154207780957222, R2 0.383192241191864\n",
      "epoch 1275, loss 0.014646823517978191, R2 0.405875563621521\n",
      "Eval loss 0.01914813742041588, R2 0.3833875060081482\n",
      "epoch 1276, loss 0.01464315876364708, R2 0.40602511167526245\n",
      "Eval loss 0.01914207637310028, R2 0.3835821747779846\n",
      "epoch 1277, loss 0.014639496803283691, R2 0.4061729311943054\n",
      "Eval loss 0.01913602091372013, R2 0.3837776780128479\n",
      "epoch 1278, loss 0.014635843224823475, R2 0.4063209295272827\n",
      "Eval loss 0.019129974767565727, R2 0.3839724063873291\n",
      "epoch 1279, loss 0.014632191509008408, R2 0.40646952390670776\n",
      "Eval loss 0.01912393793463707, R2 0.384166955947876\n",
      "epoch 1280, loss 0.014628547243773937, R2 0.40661710500717163\n",
      "Eval loss 0.01911790855228901, R2 0.38436079025268555\n",
      "epoch 1281, loss 0.01462490763515234, R2 0.4067647457122803\n",
      "Eval loss 0.019111886620521545, R2 0.38455528020858765\n",
      "epoch 1282, loss 0.014621274545788765, R2 0.40691280364990234\n",
      "Eval loss 0.01910587213933468, R2 0.3847479820251465\n",
      "epoch 1283, loss 0.014617646113038063, R2 0.40705907344818115\n",
      "Eval loss 0.01909986510872841, R2 0.3849419951438904\n",
      "epoch 1284, loss 0.01461402140557766, R2 0.40720677375793457\n",
      "Eval loss 0.019093869253993034, R2 0.38513511419296265\n",
      "epoch 1285, loss 0.01461040135473013, R2 0.40735363960266113\n",
      "Eval loss 0.019087878987193108, R2 0.3853277564048767\n",
      "epoch 1286, loss 0.014606786891818047, R2 0.4074997901916504\n",
      "Eval loss 0.019081898033618927, R2 0.38552021980285645\n",
      "epoch 1287, loss 0.014603178948163986, R2 0.4076468348503113\n",
      "Eval loss 0.019075922667980194, R2 0.38571304082870483\n",
      "epoch 1288, loss 0.014599575661122799, R2 0.40779298543930054\n",
      "Eval loss 0.019069956615567207, R2 0.3859051465988159\n",
      "epoch 1289, loss 0.014595977030694485, R2 0.40793895721435547\n",
      "Eval loss 0.019063998013734818, R2 0.3860969543457031\n",
      "epoch 1290, loss 0.014592383988201618, R2 0.40808403491973877\n",
      "Eval loss 0.019058046862483025, R2 0.38628828525543213\n",
      "epoch 1291, loss 0.01458879467099905, R2 0.4082300662994385\n",
      "Eval loss 0.01905210316181183, R2 0.38648003339767456\n",
      "epoch 1292, loss 0.01458521094173193, R2 0.40837496519088745\n",
      "Eval loss 0.01904616877436638, R2 0.3866714835166931\n",
      "epoch 1293, loss 0.014581632800400257, R2 0.4085201025009155\n",
      "Eval loss 0.019040243700146675, R2 0.3868612051010132\n",
      "epoch 1294, loss 0.014578061178326607, R2 0.4086650013923645\n",
      "Eval loss 0.019034327939152718, R2 0.38705265522003174\n",
      "epoch 1295, loss 0.014574493281543255, R2 0.40881043672561646\n",
      "Eval loss 0.01902841404080391, R2 0.38724273443222046\n",
      "epoch 1296, loss 0.014570930041372776, R2 0.4089542627334595\n",
      "Eval loss 0.019022509455680847, R2 0.3874329924583435\n",
      "epoch 1297, loss 0.01456737145781517, R2 0.4090988039970398\n",
      "Eval loss 0.01901661604642868, R2 0.387622594833374\n",
      "epoch 1298, loss 0.014563817530870438, R2 0.40924251079559326\n",
      "Eval loss 0.01901072822511196, R2 0.3878122568130493\n",
      "epoch 1299, loss 0.014560271054506302, R2 0.4093865752220154\n",
      "Eval loss 0.01900484785437584, R2 0.3880012035369873\n",
      "epoch 1300, loss 0.01455672737210989, R2 0.4095303416252136\n",
      "Eval loss 0.018998974934220314, R2 0.3881910443305969\n",
      "epoch 1301, loss 0.014553189277648926, R2 0.40967386960983276\n",
      "Eval loss 0.018993113189935684, R2 0.38837945461273193\n",
      "epoch 1302, loss 0.014549655839800835, R2 0.4098178744316101\n",
      "Eval loss 0.018987253308296204, R2 0.3885681629180908\n",
      "epoch 1303, loss 0.014546128921210766, R2 0.4099602699279785\n",
      "Eval loss 0.01898140460252762, R2 0.3887568712234497\n",
      "epoch 1304, loss 0.014542605727910995, R2 0.41010361909866333\n",
      "Eval loss 0.01897556520998478, R2 0.38894474506378174\n",
      "epoch 1305, loss 0.014539087191224098, R2 0.4102458953857422\n",
      "Eval loss 0.018969733268022537, R2 0.389132559299469\n",
      "epoch 1306, loss 0.0145355723798275, R2 0.4103889465332031\n",
      "Eval loss 0.018963906913995743, R2 0.3893199563026428\n",
      "epoch 1307, loss 0.014532065019011497, R2 0.4105305075645447\n",
      "Eval loss 0.018958086147904396, R2 0.38950759172439575\n",
      "epoch 1308, loss 0.014528561383485794, R2 0.4106733202934265\n",
      "Eval loss 0.018952278420329094, R2 0.38969457149505615\n",
      "epoch 1309, loss 0.014525063335895538, R2 0.4108147621154785\n",
      "Eval loss 0.01894647628068924, R2 0.38988161087036133\n",
      "epoch 1310, loss 0.014521571807563305, R2 0.4109562039375305\n",
      "Eval loss 0.018940681591629982, R2 0.39006805419921875\n",
      "epoch 1311, loss 0.01451808214187622, R2 0.4110979437828064\n",
      "Eval loss 0.01893489435315132, R2 0.3902546167373657\n",
      "epoch 1312, loss 0.014514598995447159, R2 0.41123926639556885\n",
      "Eval loss 0.01892911270260811, R2 0.39044058322906494\n",
      "epoch 1313, loss 0.01451112050563097, R2 0.4113803505897522\n",
      "Eval loss 0.01892334409058094, R2 0.39062637090682983\n",
      "epoch 1314, loss 0.014507647603750229, R2 0.41152095794677734\n",
      "Eval loss 0.01891757920384407, R2 0.3908114433288574\n",
      "epoch 1315, loss 0.014504178427159786, R2 0.41166192293167114\n",
      "Eval loss 0.018911823630332947, R2 0.39099735021591187\n",
      "epoch 1316, loss 0.014500714838504791, R2 0.41180241107940674\n",
      "Eval loss 0.01890607364475727, R2 0.39118272066116333\n",
      "epoch 1317, loss 0.01449725590646267, R2 0.411942720413208\n",
      "Eval loss 0.01890033297240734, R2 0.3913673758506775\n",
      "epoch 1318, loss 0.014493802562355995, R2 0.4120830297470093\n",
      "Eval loss 0.018894599750638008, R2 0.3915519714355469\n",
      "epoch 1319, loss 0.014490352012217045, R2 0.41222280263900757\n",
      "Eval loss 0.018888872116804123, R2 0.3917362093925476\n",
      "epoch 1320, loss 0.014486908912658691, R2 0.41236263513565063\n",
      "Eval loss 0.018883153796195984, R2 0.3919200301170349\n",
      "epoch 1321, loss 0.014483468607068062, R2 0.4125019907951355\n",
      "Eval loss 0.01887744478881359, R2 0.39210444688796997\n",
      "epoch 1322, loss 0.01448003575205803, R2 0.412641704082489\n",
      "Eval loss 0.018871741369366646, R2 0.3922879099845886\n",
      "epoch 1323, loss 0.014476604759693146, R2 0.4127799868583679\n",
      "Eval loss 0.01886604353785515, R2 0.3924716114997864\n",
      "epoch 1324, loss 0.01447317749261856, R2 0.41292011737823486\n",
      "Eval loss 0.018860355019569397, R2 0.3926547169685364\n",
      "epoch 1325, loss 0.014469759538769722, R2 0.4130581021308899\n",
      "Eval loss 0.018854675814509392, R2 0.39283764362335205\n",
      "epoch 1326, loss 0.014466342516243458, R2 0.413196861743927\n",
      "Eval loss 0.018848998472094536, R2 0.3930204510688782\n",
      "epoch 1327, loss 0.014462931081652641, R2 0.41333502531051636\n",
      "Eval loss 0.018843334168195724, R2 0.3932032585144043\n",
      "epoch 1328, loss 0.014459525234997272, R2 0.4134731888771057\n",
      "Eval loss 0.01883767545223236, R2 0.39338451623916626\n",
      "epoch 1329, loss 0.014456124044954777, R2 0.41361093521118164\n",
      "Eval loss 0.018832026049494743, R2 0.3935670256614685\n",
      "epoch 1330, loss 0.014452728442847729, R2 0.4137493968009949\n",
      "Eval loss 0.018826380372047424, R2 0.39374881982803345\n",
      "epoch 1331, loss 0.01444933656603098, R2 0.4138869643211365\n",
      "Eval loss 0.01882074400782585, R2 0.39392954111099243\n",
      "epoch 1332, loss 0.014445948414504528, R2 0.4140239357948303\n",
      "Eval loss 0.018815115094184875, R2 0.3941115736961365\n",
      "epoch 1333, loss 0.0144425667822361, R2 0.41416090726852417\n",
      "Eval loss 0.018809495493769646, R2 0.39429253339767456\n",
      "epoch 1334, loss 0.014439189806580544, R2 0.4142981171607971\n",
      "Eval loss 0.018803877755999565, R2 0.39447325468063354\n",
      "epoch 1335, loss 0.014435816556215286, R2 0.41443490982055664\n",
      "Eval loss 0.01879827305674553, R2 0.39465391635894775\n",
      "epoch 1336, loss 0.014432447962462902, R2 0.41457200050354004\n",
      "Eval loss 0.01879267394542694, R2 0.3948342204093933\n",
      "epoch 1337, loss 0.014429084956645966, R2 0.4147084355354309\n",
      "Eval loss 0.0187870804220438, R2 0.39501434564590454\n",
      "epoch 1338, loss 0.014425727538764477, R2 0.4148441553115845\n",
      "Eval loss 0.018781498074531555, R2 0.3951933979988098\n",
      "epoch 1339, loss 0.014422371052205563, R2 0.414980947971344\n",
      "Eval loss 0.01877591945230961, R2 0.3953741192817688\n",
      "epoch 1340, loss 0.014419023878872395, R2 0.4151160717010498\n",
      "Eval loss 0.01877034828066826, R2 0.3955531716346741\n",
      "epoch 1341, loss 0.014415678568184376, R2 0.4152517318725586\n",
      "Eval loss 0.018764786422252655, R2 0.39573168754577637\n",
      "epoch 1342, loss 0.014412338845431805, R2 0.41538745164871216\n",
      "Eval loss 0.01875923201441765, R2 0.39591115713119507\n",
      "epoch 1343, loss 0.014409002847969532, R2 0.41552257537841797\n",
      "Eval loss 0.01875368319451809, R2 0.3960898518562317\n",
      "epoch 1344, loss 0.014405673369765282, R2 0.41565781831741333\n",
      "Eval loss 0.018748141825199127, R2 0.39626824855804443\n",
      "epoch 1345, loss 0.014402348548173904, R2 0.41579246520996094\n",
      "Eval loss 0.01874260976910591, R2 0.3964468240737915\n",
      "epoch 1346, loss 0.014399026520550251, R2 0.4159277081489563\n",
      "Eval loss 0.018737083300948143, R2 0.3966243863105774\n",
      "epoch 1347, loss 0.01439571101218462, R2 0.4160616993904114\n",
      "Eval loss 0.018731562420725822, R2 0.3968021273612976\n",
      "epoch 1348, loss 0.014392397366464138, R2 0.4161961078643799\n",
      "Eval loss 0.0187260489910841, R2 0.3969796895980835\n",
      "epoch 1349, loss 0.014389089308679104, R2 0.41633009910583496\n",
      "Eval loss 0.01872054487466812, R2 0.3971567153930664\n",
      "epoch 1350, loss 0.014385788701474667, R2 0.4164642095565796\n",
      "Eval loss 0.01871504820883274, R2 0.39733409881591797\n",
      "epoch 1351, loss 0.014382490888237953, R2 0.41659754514694214\n",
      "Eval loss 0.018709558993577957, R2 0.39751070737838745\n",
      "epoch 1352, loss 0.014379195868968964, R2 0.416731595993042\n",
      "Eval loss 0.01870407536625862, R2 0.39768749475479126\n",
      "epoch 1353, loss 0.014375907368957996, R2 0.416864812374115\n",
      "Eval loss 0.018698599189519882, R2 0.39786362648010254\n",
      "epoch 1354, loss 0.014372622594237328, R2 0.4169986844062805\n",
      "Eval loss 0.01869313046336174, R2 0.3980397582054138\n",
      "epoch 1355, loss 0.014369343407452106, R2 0.41713130474090576\n",
      "Eval loss 0.018687667325139046, R2 0.39821547269821167\n",
      "epoch 1356, loss 0.014366068877279758, R2 0.41726410388946533\n",
      "Eval loss 0.018682213500142097, R2 0.39839112758636475\n",
      "epoch 1357, loss 0.014362798072397709, R2 0.41739678382873535\n",
      "Eval loss 0.018676765263080597, R2 0.3985663652420044\n",
      "epoch 1358, loss 0.014359530992805958, R2 0.41752976179122925\n",
      "Eval loss 0.018671328201889992, R2 0.39874184131622314\n",
      "epoch 1359, loss 0.014356269501149654, R2 0.4176618456840515\n",
      "Eval loss 0.018665893003344536, R2 0.3989161252975464\n",
      "epoch 1360, loss 0.014353012666106224, R2 0.4177941679954529\n",
      "Eval loss 0.018660467118024826, R2 0.39909154176712036\n",
      "epoch 1361, loss 0.014349761418998241, R2 0.4179255962371826\n",
      "Eval loss 0.018655050545930862, R2 0.3992658257484436\n",
      "epoch 1362, loss 0.014346513897180557, R2 0.41805732250213623\n",
      "Eval loss 0.018649639561772346, R2 0.3994402289390564\n",
      "epoch 1363, loss 0.014343269169330597, R2 0.4181889295578003\n",
      "Eval loss 0.018644234165549278, R2 0.39961451292037964\n",
      "epoch 1364, loss 0.014340030960738659, R2 0.41832029819488525\n",
      "Eval loss 0.018638834357261658, R2 0.39978837966918945\n",
      "epoch 1365, loss 0.014336795546114445, R2 0.4184522032737732\n",
      "Eval loss 0.018633443862199783, R2 0.39996176958084106\n",
      "epoch 1366, loss 0.014333566650748253, R2 0.4185824990272522\n",
      "Eval loss 0.018628060817718506, R2 0.4001351594924927\n",
      "epoch 1367, loss 0.014330343343317509, R2 0.4187132716178894\n",
      "Eval loss 0.018622685223817825, R2 0.4003082513809204\n",
      "epoch 1368, loss 0.01432712096720934, R2 0.41884422302246094\n",
      "Eval loss 0.018617315217852592, R2 0.4004811644554138\n",
      "epoch 1369, loss 0.014323903247714043, R2 0.41897445917129517\n",
      "Eval loss 0.018611952662467957, R2 0.4006538391113281\n",
      "epoch 1370, loss 0.014320692047476768, R2 0.41910499334335327\n",
      "Eval loss 0.018606597557663918, R2 0.400826096534729\n",
      "epoch 1371, loss 0.014317483641207218, R2 0.41923534870147705\n",
      "Eval loss 0.018601248040795326, R2 0.400998592376709\n",
      "epoch 1372, loss 0.01431428175419569, R2 0.4193649888038635\n",
      "Eval loss 0.018595905974507332, R2 0.40117061138153076\n",
      "epoch 1373, loss 0.01431108359247446, R2 0.41949450969696045\n",
      "Eval loss 0.018590571358799934, R2 0.40134239196777344\n",
      "epoch 1374, loss 0.014307890087366104, R2 0.41962379217147827\n",
      "Eval loss 0.018585246056318283, R2 0.40151387453079224\n",
      "epoch 1375, loss 0.014304700307548046, R2 0.4197540879249573\n",
      "Eval loss 0.01857992447912693, R2 0.4016852378845215\n",
      "epoch 1376, loss 0.014301514253020287, R2 0.4198830723762512\n",
      "Eval loss 0.018574610352516174, R2 0.40185636281967163\n",
      "epoch 1377, loss 0.014298333786427975, R2 0.42001163959503174\n",
      "Eval loss 0.018569305539131165, R2 0.4020270109176636\n",
      "epoch 1378, loss 0.014295157976448536, R2 0.42014026641845703\n",
      "Eval loss 0.018564004451036453, R2 0.40219807624816895\n",
      "epoch 1379, loss 0.014291984960436821, R2 0.4202691912651062\n",
      "Eval loss 0.018558712676167488, R2 0.4023679494857788\n",
      "epoch 1380, loss 0.014288818463683128, R2 0.42039716243743896\n",
      "Eval loss 0.01855342835187912, R2 0.4025384783744812\n",
      "epoch 1381, loss 0.01428565476089716, R2 0.420526385307312\n",
      "Eval loss 0.01854814775288105, R2 0.4027085304260254\n",
      "epoch 1382, loss 0.014282495714724064, R2 0.4206541180610657\n",
      "Eval loss 0.018542876467108727, R2 0.40287846326828003\n",
      "epoch 1383, loss 0.014279341325163841, R2 0.42078202962875366\n",
      "Eval loss 0.0185376089066267, R2 0.40304768085479736\n",
      "epoch 1384, loss 0.014276192523539066, R2 0.4209098219871521\n",
      "Eval loss 0.01853235252201557, R2 0.4032173752784729\n",
      "epoch 1385, loss 0.01427304744720459, R2 0.42103737592697144\n",
      "Eval loss 0.01852709986269474, R2 0.40338629484176636\n",
      "epoch 1386, loss 0.014269905164837837, R2 0.4211646318435669\n",
      "Eval loss 0.018521854653954506, R2 0.4035557508468628\n",
      "epoch 1387, loss 0.014266766607761383, R2 0.42129194736480713\n",
      "Eval loss 0.018516618758440018, R2 0.4037240147590637\n",
      "epoch 1388, loss 0.014263635501265526, R2 0.4214191436767578\n",
      "Eval loss 0.018511386588215828, R2 0.40389180183410645\n",
      "epoch 1389, loss 0.014260508120059967, R2 0.4215460419654846\n",
      "Eval loss 0.018506161868572235, R2 0.4040605425834656\n",
      "epoch 1390, loss 0.014257383532822132, R2 0.42167341709136963\n",
      "Eval loss 0.01850094459950924, R2 0.4042285680770874\n",
      "epoch 1391, loss 0.01425426360219717, R2 0.42179930210113525\n",
      "Eval loss 0.01849573291838169, R2 0.4043963551521301\n",
      "epoch 1392, loss 0.014251148328185081, R2 0.4219256639480591\n",
      "Eval loss 0.01849052868783474, R2 0.4045642018318176\n",
      "epoch 1393, loss 0.014248037710785866, R2 0.4220518469810486\n",
      "Eval loss 0.018485331907868385, R2 0.4047313332557678\n",
      "epoch 1394, loss 0.014244930818676949, R2 0.42217832803726196\n",
      "Eval loss 0.01848014071583748, R2 0.404898464679718\n",
      "epoch 1395, loss 0.01424182765185833, R2 0.4223037362098694\n",
      "Eval loss 0.018474958837032318, R2 0.4050654172897339\n",
      "epoch 1396, loss 0.014238729141652584, R2 0.4224294424057007\n",
      "Eval loss 0.018469780683517456, R2 0.40523195266723633\n",
      "epoch 1397, loss 0.014235636219382286, R2 0.4225549101829529\n",
      "Eval loss 0.01846460998058319, R2 0.40539878606796265\n",
      "epoch 1398, loss 0.014232547022402287, R2 0.42268019914627075\n",
      "Eval loss 0.018459446728229523, R2 0.405565083026886\n",
      "epoch 1399, loss 0.01422946061939001, R2 0.4228053689002991\n",
      "Eval loss 0.01845429092645645, R2 0.4057309031486511\n",
      "epoch 1400, loss 0.014226379804313183, R2 0.42293035984039307\n",
      "Eval loss 0.01844913884997368, R2 0.40589696168899536\n",
      "epoch 1401, loss 0.014223303645849228, R2 0.42305493354797363\n",
      "Eval loss 0.018443994224071503, R2 0.4060624837875366\n",
      "epoch 1402, loss 0.014220232143998146, R2 0.42318040132522583\n",
      "Eval loss 0.018438860774040222, R2 0.40622782707214355\n",
      "epoch 1403, loss 0.014217163436114788, R2 0.4233042001724243\n",
      "Eval loss 0.01843373104929924, R2 0.4063931703567505\n",
      "epoch 1404, loss 0.014214098453521729, R2 0.4234285354614258\n",
      "Eval loss 0.018428606912493706, R2 0.4065577983856201\n",
      "epoch 1405, loss 0.014211039990186691, R2 0.4235530495643616\n",
      "Eval loss 0.01842349022626877, R2 0.4067227840423584\n",
      "epoch 1406, loss 0.014207985252141953, R2 0.4236764907836914\n",
      "Eval loss 0.01841837912797928, R2 0.40688735246658325\n",
      "epoch 1407, loss 0.014204933308064938, R2 0.4238002896308899\n",
      "Eval loss 0.018413273617625237, R2 0.4070512056350708\n",
      "epoch 1408, loss 0.01420188695192337, R2 0.42392367124557495\n",
      "Eval loss 0.01840817742049694, R2 0.4072161912918091\n",
      "epoch 1409, loss 0.014198843389749527, R2 0.4240473508834839\n",
      "Eval loss 0.01840308867394924, R2 0.4073799252510071\n",
      "epoch 1410, loss 0.014195803552865982, R2 0.42417067289352417\n",
      "Eval loss 0.01839800365269184, R2 0.4075436592102051\n",
      "epoch 1411, loss 0.014192769303917885, R2 0.4242941737174988\n",
      "Eval loss 0.018392927944660187, R2 0.407707154750824\n",
      "epoch 1412, loss 0.014189740642905235, R2 0.42441660165786743\n",
      "Eval loss 0.01838785596191883, R2 0.40786951780319214\n",
      "epoch 1413, loss 0.014186713844537735, R2 0.42453938722610474\n",
      "Eval loss 0.01838279329240322, R2 0.4080333113670349\n",
      "epoch 1414, loss 0.014183691702783108, R2 0.42466169595718384\n",
      "Eval loss 0.01837773621082306, R2 0.4081961512565613\n",
      "epoch 1415, loss 0.014180673286318779, R2 0.4247843623161316\n",
      "Eval loss 0.018372686579823494, R2 0.40835875272750854\n",
      "epoch 1416, loss 0.014177661389112473, R2 0.42490655183792114\n",
      "Eval loss 0.018367640674114227, R2 0.4085214138031006\n",
      "epoch 1417, loss 0.014174649491906166, R2 0.4250287413597107\n",
      "Eval loss 0.018362602218985558, R2 0.4086834788322449\n",
      "epoch 1418, loss 0.014171644113957882, R2 0.42515063285827637\n",
      "Eval loss 0.018357569351792336, R2 0.40884536504745483\n",
      "epoch 1419, loss 0.014168644323945045, R2 0.4252721071243286\n",
      "Eval loss 0.01835254579782486, R2 0.40900659561157227\n",
      "epoch 1420, loss 0.014165648259222507, R2 0.42539429664611816\n",
      "Eval loss 0.018347525969147682, R2 0.4091687798500061\n",
      "epoch 1421, loss 0.014162654057145119, R2 0.4255152940750122\n",
      "Eval loss 0.01834251545369625, R2 0.4093303680419922\n",
      "epoch 1422, loss 0.014159663580358028, R2 0.4256365895271301\n",
      "Eval loss 0.018337510526180267, R2 0.40949153900146484\n",
      "epoch 1423, loss 0.014156680554151535, R2 0.42575740814208984\n",
      "Eval loss 0.01833251304924488, R2 0.4096524715423584\n",
      "epoch 1424, loss 0.014153700321912766, R2 0.42587918043136597\n",
      "Eval loss 0.018327519297599792, R2 0.4098132252693176\n",
      "epoch 1425, loss 0.014150724746286869, R2 0.4259989857673645\n",
      "Eval loss 0.01832253485918045, R2 0.4099732041358948\n",
      "epoch 1426, loss 0.014147750101983547, R2 0.426119863986969\n",
      "Eval loss 0.018317554146051407, R2 0.41013413667678833\n",
      "epoch 1427, loss 0.014144781976938248, R2 0.4262405037879944\n",
      "Eval loss 0.01831258088350296, R2 0.4102942943572998\n",
      "epoch 1428, loss 0.014141818508505821, R2 0.42636048793792725\n",
      "Eval loss 0.01830761507153511, R2 0.4104542136192322\n",
      "epoch 1429, loss 0.014138859696686268, R2 0.4264807105064392\n",
      "Eval loss 0.01830265484750271, R2 0.4106139540672302\n",
      "epoch 1430, loss 0.014135901816189289, R2 0.42660045623779297\n",
      "Eval loss 0.018297700211405754, R2 0.4107736349105835\n",
      "epoch 1431, loss 0.014132952317595482, R2 0.4267200827598572\n",
      "Eval loss 0.018292751163244247, R2 0.41093283891677856\n",
      "epoch 1432, loss 0.0141300018876791, R2 0.42684000730514526\n",
      "Eval loss 0.018287811428308487, R2 0.4110921025276184\n",
      "epoch 1433, loss 0.014127058908343315, R2 0.42695939540863037\n",
      "Eval loss 0.018282875418663025, R2 0.4112510085105896\n",
      "epoch 1434, loss 0.014124119654297829, R2 0.4270784258842468\n",
      "Eval loss 0.01827794872224331, R2 0.41140949726104736\n",
      "epoch 1435, loss 0.01412118412554264, R2 0.4271974563598633\n",
      "Eval loss 0.01827302761375904, R2 0.4115678668022156\n",
      "epoch 1436, loss 0.014118251390755177, R2 0.4273170828819275\n",
      "Eval loss 0.01826811209321022, R2 0.4117264747619629\n",
      "epoch 1437, loss 0.01411532424390316, R2 0.4274347424507141\n",
      "Eval loss 0.018263202160596848, R2 0.4118838906288147\n",
      "epoch 1438, loss 0.014112400822341442, R2 0.42755353450775146\n",
      "Eval loss 0.01825829967856407, R2 0.4120420813560486\n",
      "epoch 1439, loss 0.014109480194747448, R2 0.4276726245880127\n",
      "Eval loss 0.018253400921821594, R2 0.4122001528739929\n",
      "epoch 1440, loss 0.014106566086411476, R2 0.42779064178466797\n",
      "Eval loss 0.018248513340950012, R2 0.41235727071762085\n",
      "epoch 1441, loss 0.014103651978075504, R2 0.42790883779525757\n",
      "Eval loss 0.01824362948536873, R2 0.41251468658447266\n",
      "epoch 1442, loss 0.014100744388997555, R2 0.42802655696868896\n",
      "Eval loss 0.018238753080368042, R2 0.41267114877700806\n",
      "epoch 1443, loss 0.014097840525209904, R2 0.42814457416534424\n",
      "Eval loss 0.018233880400657654, R2 0.41282862424850464\n",
      "epoch 1444, loss 0.014094941318035126, R2 0.428261935710907\n",
      "Eval loss 0.018229017034173012, R2 0.4129852056503296\n",
      "epoch 1445, loss 0.01409204676747322, R2 0.4283795952796936\n",
      "Eval loss 0.01822415553033352, R2 0.41314178705215454\n",
      "epoch 1446, loss 0.014089154079556465, R2 0.4284973740577698\n",
      "Eval loss 0.018219303339719772, R2 0.41329747438430786\n",
      "epoch 1447, loss 0.014086266979575157, R2 0.4286136031150818\n",
      "Eval loss 0.018214456737041473, R2 0.4134541153907776\n",
      "epoch 1448, loss 0.014083380810916424, R2 0.42873090505599976\n",
      "Eval loss 0.01820961758494377, R2 0.4136095643043518\n",
      "epoch 1449, loss 0.014080503024160862, R2 0.4288482666015625\n",
      "Eval loss 0.018204784020781517, R2 0.413765013217926\n",
      "epoch 1450, loss 0.014077628031373024, R2 0.42896467447280884\n",
      "Eval loss 0.01819995790719986, R2 0.4139209985733032\n",
      "epoch 1451, loss 0.01407475583255291, R2 0.4290807247161865\n",
      "Eval loss 0.01819513365626335, R2 0.4140763282775879\n",
      "epoch 1452, loss 0.01407188642770052, R2 0.4291977882385254\n",
      "Eval loss 0.01819032058119774, R2 0.4142313599586487\n",
      "epoch 1453, loss 0.014069021679461002, R2 0.4293133020401001\n",
      "Eval loss 0.018185511231422424, R2 0.41438621282577515\n",
      "epoch 1454, loss 0.014066162519156933, R2 0.4294293522834778\n",
      "Eval loss 0.018180707469582558, R2 0.4145408868789673\n",
      "epoch 1455, loss 0.014063306152820587, R2 0.42954516410827637\n",
      "Eval loss 0.018175911158323288, R2 0.4146953225135803\n",
      "epoch 1456, loss 0.01406045351177454, R2 0.4296611547470093\n",
      "Eval loss 0.018171120434999466, R2 0.41484981775283813\n",
      "epoch 1457, loss 0.014057605527341366, R2 0.4297764301300049\n",
      "Eval loss 0.01816633716225624, R2 0.4150036573410034\n",
      "epoch 1458, loss 0.014054758474230766, R2 0.4298917055130005\n",
      "Eval loss 0.018161557614803314, R2 0.4151575565338135\n",
      "epoch 1459, loss 0.014051918871700764, R2 0.43000727891921997\n",
      "Eval loss 0.018156785517930984, R2 0.41531139612197876\n",
      "epoch 1460, loss 0.01404908299446106, R2 0.43012189865112305\n",
      "Eval loss 0.01815202087163925, R2 0.41546469926834106\n",
      "epoch 1461, loss 0.014046248979866505, R2 0.43023747205734253\n",
      "Eval loss 0.018147261813282967, R2 0.4156184792518616\n",
      "epoch 1462, loss 0.014043419621884823, R2 0.4303516149520874\n",
      "Eval loss 0.01814251020550728, R2 0.415770947933197\n",
      "epoch 1463, loss 0.014040594920516014, R2 0.4304664134979248\n",
      "Eval loss 0.01813776046037674, R2 0.4159238934516907\n",
      "epoch 1464, loss 0.014037772081792355, R2 0.43058091402053833\n",
      "Eval loss 0.018133018165826797, R2 0.4160769581794739\n",
      "epoch 1465, loss 0.014034955762326717, R2 0.43069517612457275\n",
      "Eval loss 0.018128283321857452, R2 0.4162290692329407\n",
      "epoch 1466, loss 0.014032143168151379, R2 0.4308096766471863\n",
      "Eval loss 0.018123552203178406, R2 0.4163814187049866\n",
      "epoch 1467, loss 0.014029331505298615, R2 0.43092334270477295\n",
      "Eval loss 0.018118834123015404, R2 0.4165332317352295\n",
      "epoch 1468, loss 0.014026526361703873, R2 0.43103688955307007\n",
      "Eval loss 0.018114114180207253, R2 0.41668516397476196\n",
      "epoch 1469, loss 0.01402372494339943, R2 0.43115031719207764\n",
      "Eval loss 0.018109403550624847, R2 0.41683685779571533\n",
      "epoch 1470, loss 0.014020925387740135, R2 0.4312642812728882\n",
      "Eval loss 0.01810469850897789, R2 0.4169880151748657\n",
      "epoch 1471, loss 0.014018132351338863, R2 0.4313777685165405\n",
      "Eval loss 0.01809999905526638, R2 0.4171399474143982\n",
      "epoch 1472, loss 0.01401534117758274, R2 0.43149083852767944\n",
      "Eval loss 0.018095307052135468, R2 0.4172908663749695\n",
      "epoch 1473, loss 0.014012553729116917, R2 0.43160343170166016\n",
      "Eval loss 0.018090618774294853, R2 0.4174419641494751\n",
      "epoch 1474, loss 0.014009770937263966, R2 0.4317167401313782\n",
      "Eval loss 0.018085937947034836, R2 0.41759252548217773\n",
      "epoch 1475, loss 0.014006990939378738, R2 0.43182992935180664\n",
      "Eval loss 0.018081262707710266, R2 0.4177432060241699\n",
      "epoch 1476, loss 0.014004216529428959, R2 0.43194204568862915\n",
      "Eval loss 0.018076594918966293, R2 0.4178928732872009\n",
      "epoch 1477, loss 0.014001444913446903, R2 0.43205469846725464\n",
      "Eval loss 0.01807193085551262, R2 0.41804319620132446\n",
      "epoch 1478, loss 0.013998676091432571, R2 0.4321669936180115\n",
      "Eval loss 0.01806727424263954, R2 0.41819334030151367\n",
      "epoch 1479, loss 0.013995912857353687, R2 0.4322788715362549\n",
      "Eval loss 0.018062623217701912, R2 0.4183434844017029\n",
      "epoch 1480, loss 0.013993152417242527, R2 0.43239063024520874\n",
      "Eval loss 0.01805797964334488, R2 0.41849303245544434\n",
      "epoch 1481, loss 0.01399039477109909, R2 0.4325026869773865\n",
      "Eval loss 0.018053341656923294, R2 0.4186427593231201\n",
      "epoch 1482, loss 0.013987641781568527, R2 0.43261438608169556\n",
      "Eval loss 0.018048707395792007, R2 0.41879159212112427\n",
      "epoch 1483, loss 0.013984891586005688, R2 0.4327262043952942\n",
      "Eval loss 0.018044080585241318, R2 0.4189409613609314\n",
      "epoch 1484, loss 0.013982146047055721, R2 0.4328370690345764\n",
      "Eval loss 0.018039457499980927, R2 0.4190894365310669\n",
      "epoch 1485, loss 0.013979405164718628, R2 0.4329487085342407\n",
      "Eval loss 0.01803484372794628, R2 0.41923803091049194\n",
      "epoch 1486, loss 0.013976667076349258, R2 0.43305957317352295\n",
      "Eval loss 0.018030231818556786, R2 0.4193865656852722\n",
      "epoch 1487, loss 0.013973934575915337, R2 0.4331708550453186\n",
      "Eval loss 0.018025631085038185, R2 0.41953474283218384\n",
      "epoch 1488, loss 0.013971203938126564, R2 0.43328118324279785\n",
      "Eval loss 0.018021032214164734, R2 0.4196828007698059\n",
      "epoch 1489, loss 0.013968476094305515, R2 0.433391809463501\n",
      "Eval loss 0.01801644079387188, R2 0.419830858707428\n",
      "epoch 1490, loss 0.01396575290709734, R2 0.4335026741027832\n",
      "Eval loss 0.018011856824159622, R2 0.4199783205986023\n",
      "epoch 1491, loss 0.013963032513856888, R2 0.4336124062538147\n",
      "Eval loss 0.018007274717092514, R2 0.4201258420944214\n",
      "epoch 1492, loss 0.013960318639874458, R2 0.43372267484664917\n",
      "Eval loss 0.018002700060606003, R2 0.42027246952056885\n",
      "epoch 1493, loss 0.013957605697214603, R2 0.4338330030441284\n",
      "Eval loss 0.01799813099205494, R2 0.4204201102256775\n",
      "epoch 1494, loss 0.01395489927381277, R2 0.4339423179626465\n",
      "Eval loss 0.017993571236729622, R2 0.4205673336982727\n",
      "epoch 1495, loss 0.013952192850410938, R2 0.434052050113678\n",
      "Eval loss 0.017989013344049454, R2 0.42071425914764404\n",
      "epoch 1496, loss 0.013949492014944553, R2 0.4341616630554199\n",
      "Eval loss 0.017984461039304733, R2 0.42086029052734375\n",
      "epoch 1497, loss 0.013946795836091042, R2 0.4342718720436096\n",
      "Eval loss 0.01797991804778576, R2 0.42100679874420166\n",
      "epoch 1498, loss 0.013944102451205254, R2 0.4343804717063904\n",
      "Eval loss 0.017975378781557083, R2 0.4211529493331909\n",
      "epoch 1499, loss 0.01394141186028719, R2 0.4344896078109741\n",
      "Eval loss 0.017970845103263855, R2 0.4212987422943115\n",
      "epoch 1500, loss 0.013938724994659424, R2 0.43459904193878174\n",
      "Eval loss 0.017966317012906075, R2 0.42144423723220825\n",
      "epoch 1501, loss 0.013936042785644531, R2 0.4347071647644043\n",
      "Eval loss 0.017961792647838593, R2 0.4215906262397766\n",
      "epoch 1502, loss 0.013933364301919937, R2 0.43481647968292236\n",
      "Eval loss 0.017957277595996857, R2 0.42173588275909424\n",
      "epoch 1503, loss 0.013930689543485641, R2 0.4349243640899658\n",
      "Eval loss 0.01795276626944542, R2 0.4218813180923462\n",
      "epoch 1504, loss 0.01392801757901907, R2 0.4350331425666809\n",
      "Eval loss 0.01794826239347458, R2 0.4220261573791504\n",
      "epoch 1505, loss 0.013925347477197647, R2 0.43514126539230347\n",
      "Eval loss 0.017943764105439186, R2 0.4221709966659546\n",
      "epoch 1506, loss 0.013922683894634247, R2 0.43524909019470215\n",
      "Eval loss 0.01793927140533924, R2 0.42231565713882446\n",
      "epoch 1507, loss 0.01392002496868372, R2 0.43535691499710083\n",
      "Eval loss 0.017934786155819893, R2 0.4224599599838257\n",
      "epoch 1508, loss 0.013917366042733192, R2 0.4354649782180786\n",
      "Eval loss 0.017930302768945694, R2 0.422604501247406\n",
      "epoch 1509, loss 0.013914711773395538, R2 0.43557310104370117\n",
      "Eval loss 0.017925826832652092, R2 0.4227484464645386\n",
      "epoch 1510, loss 0.013912063091993332, R2 0.4356803297996521\n",
      "Eval loss 0.017921356484293938, R2 0.4228927493095398\n",
      "epoch 1511, loss 0.013909416273236275, R2 0.43578749895095825\n",
      "Eval loss 0.017916889861226082, R2 0.4230365753173828\n",
      "epoch 1512, loss 0.013906772248446941, R2 0.4358949661254883\n",
      "Eval loss 0.017912432551383972, R2 0.4231799244880676\n",
      "epoch 1513, loss 0.013904132880270481, R2 0.4360017776489258\n",
      "Eval loss 0.01790797896683216, R2 0.4233233332633972\n",
      "epoch 1514, loss 0.01390149723738432, R2 0.4361082911491394\n",
      "Eval loss 0.017903530970215797, R2 0.4234669804573059\n",
      "epoch 1515, loss 0.013898865319788456, R2 0.43621546030044556\n",
      "Eval loss 0.01789908856153488, R2 0.4236096739768982\n",
      "epoch 1516, loss 0.013896236196160316, R2 0.43632233142852783\n",
      "Eval loss 0.017894653603434563, R2 0.42375248670578003\n",
      "epoch 1517, loss 0.0138936135917902, R2 0.4364284873008728\n",
      "Eval loss 0.017890222370624542, R2 0.4238951802253723\n",
      "epoch 1518, loss 0.013890990987420082, R2 0.4365346431732178\n",
      "Eval loss 0.01788579672574997, R2 0.4240376949310303\n",
      "epoch 1519, loss 0.013888373039662838, R2 0.43664103746414185\n",
      "Eval loss 0.017881378531455994, R2 0.4241797924041748\n",
      "epoch 1520, loss 0.013885758817195892, R2 0.43674707412719727\n",
      "Eval loss 0.017876964062452316, R2 0.42432212829589844\n",
      "epoch 1521, loss 0.013883148320019245, R2 0.43685299158096313\n",
      "Eval loss 0.017872558906674385, R2 0.42446398735046387\n",
      "epoch 1522, loss 0.013880541548132896, R2 0.4369587302207947\n",
      "Eval loss 0.017868155613541603, R2 0.42460542917251587\n",
      "epoch 1523, loss 0.013877937570214272, R2 0.43706393241882324\n",
      "Eval loss 0.01786375790834427, R2 0.42474669218063354\n",
      "epoch 1524, loss 0.01387533824890852, R2 0.43716979026794434\n",
      "Eval loss 0.017859365791082382, R2 0.42488884925842285\n",
      "epoch 1525, loss 0.013872738927602768, R2 0.4372749924659729\n",
      "Eval loss 0.017854981124401093, R2 0.42503035068511963\n",
      "epoch 1526, loss 0.013870146125555038, R2 0.4373803734779358\n",
      "Eval loss 0.01785060204565525, R2 0.4251710772514343\n",
      "epoch 1527, loss 0.013867556117475033, R2 0.4374856948852539\n",
      "Eval loss 0.017846226692199707, R2 0.42531174421310425\n",
      "epoch 1528, loss 0.013864968903362751, R2 0.43759018182754517\n",
      "Eval loss 0.01784185692667961, R2 0.42545264959335327\n",
      "epoch 1529, loss 0.013862385414540768, R2 0.4376952052116394\n",
      "Eval loss 0.017837494611740112, R2 0.42559313774108887\n",
      "epoch 1530, loss 0.013859807513654232, R2 0.43780022859573364\n",
      "Eval loss 0.017833136022090912, R2 0.4257333278656006\n",
      "epoch 1531, loss 0.01385723240673542, R2 0.4379042387008667\n",
      "Eval loss 0.01782878488302231, R2 0.4258735775947571\n",
      "epoch 1532, loss 0.013854657299816608, R2 0.4380086660385132\n",
      "Eval loss 0.017824435606598854, R2 0.42601364850997925\n",
      "epoch 1533, loss 0.013852090574800968, R2 0.4381127953529358\n",
      "Eval loss 0.017820093780755997, R2 0.42615383863449097\n",
      "epoch 1534, loss 0.013849525712430477, R2 0.43821728229522705\n",
      "Eval loss 0.017815761268138885, R2 0.42629313468933105\n",
      "epoch 1535, loss 0.01384696364402771, R2 0.43832141160964966\n",
      "Eval loss 0.017811428755521774, R2 0.4264324903488159\n",
      "epoch 1536, loss 0.013844405300915241, R2 0.4384251832962036\n",
      "Eval loss 0.01780710369348526, R2 0.4265716075897217\n",
      "epoch 1537, loss 0.013841850683093071, R2 0.43852877616882324\n",
      "Eval loss 0.017802786082029343, R2 0.4267107844352722\n",
      "epoch 1538, loss 0.0138392997905612, R2 0.43863141536712646\n",
      "Eval loss 0.017798474058508873, R2 0.4268496632575989\n",
      "epoch 1539, loss 0.013836750760674477, R2 0.4387350082397461\n",
      "Eval loss 0.017794165760278702, R2 0.426988422870636\n",
      "epoch 1540, loss 0.013834207318723202, R2 0.4388384222984314\n",
      "Eval loss 0.017789864912629128, R2 0.4271268844604492\n",
      "epoch 1541, loss 0.01383166667073965, R2 0.43894124031066895\n",
      "Eval loss 0.017785565927624702, R2 0.4272648096084595\n",
      "epoch 1542, loss 0.013829129748046398, R2 0.43904417753219604\n",
      "Eval loss 0.017781274393200874, R2 0.4274035692214966\n",
      "epoch 1543, loss 0.013826594687998295, R2 0.4391469955444336\n",
      "Eval loss 0.017776988446712494, R2 0.4275416135787964\n",
      "epoch 1544, loss 0.013824064284563065, R2 0.43924981355667114\n",
      "Eval loss 0.017772706225514412, R2 0.42767947912216187\n",
      "epoch 1545, loss 0.013821537606418133, R2 0.4393521547317505\n",
      "Eval loss 0.017768431454896927, R2 0.42781656980514526\n",
      "epoch 1546, loss 0.01381901279091835, R2 0.43945497274398804\n",
      "Eval loss 0.01776416227221489, R2 0.42795461416244507\n",
      "epoch 1547, loss 0.01381649263203144, R2 0.4395567774772644\n",
      "Eval loss 0.017759894952178, R2 0.42809200286865234\n",
      "epoch 1548, loss 0.01381397619843483, R2 0.4396588206291199\n",
      "Eval loss 0.01775563694536686, R2 0.42822861671447754\n",
      "epoch 1549, loss 0.013811459764838219, R2 0.4397609233856201\n",
      "Eval loss 0.017751384526491165, R2 0.42836642265319824\n",
      "epoch 1550, loss 0.01380894985049963, R2 0.4398627281188965\n",
      "Eval loss 0.01774713583290577, R2 0.4285028576850891\n",
      "epoch 1551, loss 0.01380644366145134, R2 0.4399644136428833\n",
      "Eval loss 0.017742890864610672, R2 0.4286397099494934\n",
      "epoch 1552, loss 0.013803940266370773, R2 0.4400659203529358\n",
      "Eval loss 0.01773865520954132, R2 0.4287761449813843\n",
      "epoch 1553, loss 0.013801438733935356, R2 0.4401674270629883\n",
      "Eval loss 0.01773442141711712, R2 0.4289121627807617\n",
      "epoch 1554, loss 0.013798943720757961, R2 0.4402683973312378\n",
      "Eval loss 0.017730198800563812, R2 0.4290482997894287\n",
      "epoch 1555, loss 0.013796448707580566, R2 0.44036978483200073\n",
      "Eval loss 0.017725976184010506, R2 0.42918431758880615\n",
      "epoch 1556, loss 0.01379395928233862, R2 0.4404705762863159\n",
      "Eval loss 0.017721757292747498, R2 0.42931991815567017\n",
      "epoch 1557, loss 0.013791470788419247, R2 0.44057172536849976\n",
      "Eval loss 0.017717545852065086, R2 0.4294557571411133\n",
      "epoch 1558, loss 0.013788988813757896, R2 0.44067221879959106\n",
      "Eval loss 0.01771334372460842, R2 0.42959123849868774\n",
      "epoch 1559, loss 0.01378650777041912, R2 0.440773069858551\n",
      "Eval loss 0.017709141597151756, R2 0.429725706577301\n",
      "epoch 1560, loss 0.013784032315015793, R2 0.44087350368499756\n",
      "Eval loss 0.01770494505763054, R2 0.42986154556274414\n",
      "epoch 1561, loss 0.013781558722257614, R2 0.44097381830215454\n",
      "Eval loss 0.01770075596868992, R2 0.4299963712692261\n",
      "epoch 1562, loss 0.013779086992144585, R2 0.44107407331466675\n",
      "Eval loss 0.017696572467684746, R2 0.43013113737106323\n",
      "epoch 1563, loss 0.013776620849967003, R2 0.4411739110946655\n",
      "Eval loss 0.01769239455461502, R2 0.4302656650543213\n",
      "epoch 1564, loss 0.01377415657043457, R2 0.4412740468978882\n",
      "Eval loss 0.017688220366835594, R2 0.43039989471435547\n",
      "epoch 1565, loss 0.01377169881016016, R2 0.4413737654685974\n",
      "Eval loss 0.017684051766991615, R2 0.430534303188324\n",
      "epoch 1566, loss 0.013769240118563175, R2 0.4414730668067932\n",
      "Eval loss 0.017679888755083084, R2 0.4306684136390686\n",
      "epoch 1567, loss 0.013766787014901638, R2 0.44157344102859497\n",
      "Eval loss 0.01767573133111, R2 0.43080228567123413\n",
      "epoch 1568, loss 0.013764334842562675, R2 0.4416730999946594\n",
      "Eval loss 0.017671579495072365, R2 0.43093544244766235\n",
      "epoch 1569, loss 0.01376189012080431, R2 0.44177162647247314\n",
      "Eval loss 0.017667431384325027, R2 0.431069552898407\n",
      "epoch 1570, loss 0.013759446330368519, R2 0.4418707489967346\n",
      "Eval loss 0.017663290724158287, R2 0.43120235204696655\n",
      "epoch 1571, loss 0.013757004402577877, R2 0.4419698119163513\n",
      "Eval loss 0.017659151926636696, R2 0.43133580684661865\n",
      "epoch 1572, loss 0.013754568062722683, R2 0.442068874835968\n",
      "Eval loss 0.0176550205796957, R2 0.4314687252044678\n",
      "epoch 1573, loss 0.013752135448157787, R2 0.44216710329055786\n",
      "Eval loss 0.017650894820690155, R2 0.431602418422699\n",
      "epoch 1574, loss 0.013749705627560616, R2 0.4422658681869507\n",
      "Eval loss 0.017646774649620056, R2 0.43173474073410034\n",
      "epoch 1575, loss 0.013747275806963444, R2 0.4423643946647644\n",
      "Eval loss 0.017642658203840256, R2 0.4318673014640808\n",
      "epoch 1576, loss 0.013744852505624294, R2 0.4424627423286438\n",
      "Eval loss 0.017638547345995903, R2 0.43199968338012695\n",
      "epoch 1577, loss 0.013742431066930294, R2 0.44256138801574707\n",
      "Eval loss 0.017634442076086998, R2 0.4321320652961731\n",
      "epoch 1578, loss 0.013740012422204018, R2 0.44265884160995483\n",
      "Eval loss 0.01763034053146839, R2 0.43226397037506104\n",
      "epoch 1579, loss 0.01373759750276804, R2 0.4427570104598999\n",
      "Eval loss 0.017626244574785233, R2 0.4323960542678833\n",
      "epoch 1580, loss 0.01373518630862236, R2 0.4428548216819763\n",
      "Eval loss 0.01762215420603752, R2 0.4325276017189026\n",
      "epoch 1581, loss 0.013732777908444405, R2 0.4429525136947632\n",
      "Eval loss 0.017618069425225258, R2 0.4326590895652771\n",
      "epoch 1582, loss 0.013730373233556747, R2 0.4430500268936157\n",
      "Eval loss 0.017613988369703293, R2 0.43279051780700684\n",
      "epoch 1583, loss 0.013727972283959389, R2 0.4431478977203369\n",
      "Eval loss 0.017609912902116776, R2 0.43292176723480225\n",
      "epoch 1584, loss 0.01372557319700718, R2 0.4432447552680969\n",
      "Eval loss 0.017605843022465706, R2 0.433052659034729\n",
      "epoch 1585, loss 0.013723178766667843, R2 0.44334232807159424\n",
      "Eval loss 0.017601778730750084, R2 0.43318355083465576\n",
      "epoch 1586, loss 0.013720786198973656, R2 0.44343894720077515\n",
      "Eval loss 0.01759772002696991, R2 0.4333142638206482\n",
      "epoch 1587, loss 0.013718397356569767, R2 0.44353604316711426\n",
      "Eval loss 0.017593663185834885, R2 0.4334450364112854\n",
      "epoch 1588, loss 0.013716013170778751, R2 0.4436321258544922\n",
      "Eval loss 0.017589615657925606, R2 0.4335753917694092\n",
      "epoch 1589, loss 0.013713630847632885, R2 0.4437292218208313\n",
      "Eval loss 0.017585569992661476, R2 0.4337056279182434\n",
      "epoch 1590, loss 0.013711252249777317, R2 0.4438256621360779\n",
      "Eval loss 0.017581533640623093, R2 0.43383562564849854\n",
      "epoch 1591, loss 0.013708876445889473, R2 0.4439226984977722\n",
      "Eval loss 0.01757749915122986, R2 0.4339655637741089\n",
      "epoch 1592, loss 0.013706504367291927, R2 0.44401872158050537\n",
      "Eval loss 0.017573470249772072, R2 0.43409526348114014\n",
      "epoch 1593, loss 0.013704134151339531, R2 0.4441143870353699\n",
      "Eval loss 0.017569448798894882, R2 0.43422478437423706\n",
      "epoch 1594, loss 0.013701767660677433, R2 0.4442101716995239\n",
      "Eval loss 0.017565429210662842, R2 0.4343542456626892\n",
      "epoch 1595, loss 0.013699403963983059, R2 0.44430673122406006\n",
      "Eval loss 0.0175614133477211, R2 0.43448352813720703\n",
      "epoch 1596, loss 0.013697043992578983, R2 0.4444020390510559\n",
      "Eval loss 0.017557406798005104, R2 0.43461257219314575\n",
      "epoch 1597, loss 0.013694686815142632, R2 0.44449758529663086\n",
      "Eval loss 0.017553402110934258, R2 0.43474137783050537\n",
      "epoch 1598, loss 0.013692333362996578, R2 0.44459307193756104\n",
      "Eval loss 0.017549404874444008, R2 0.43486952781677246\n",
      "epoch 1599, loss 0.013689981773495674, R2 0.44468849897384644\n",
      "Eval loss 0.017545411363244057, R2 0.43499886989593506\n",
      "epoch 1600, loss 0.013687634840607643, R2 0.44478392601013184\n",
      "Eval loss 0.017541421577334404, R2 0.4351271390914917\n",
      "epoch 1601, loss 0.01368529163300991, R2 0.4448786973953247\n",
      "Eval loss 0.017537439242005348, R2 0.43525558710098267\n",
      "epoch 1602, loss 0.013682949356734753, R2 0.4449737071990967\n",
      "Eval loss 0.01753346249461174, R2 0.435383677482605\n",
      "epoch 1603, loss 0.013680611737072468, R2 0.44506895542144775\n",
      "Eval loss 0.017529485747218132, R2 0.43551182746887207\n",
      "epoch 1604, loss 0.013678277842700481, R2 0.4451632499694824\n",
      "Eval loss 0.01752551831305027, R2 0.43563878536224365\n",
      "epoch 1605, loss 0.013675945810973644, R2 0.4452584385871887\n",
      "Eval loss 0.017521554604172707, R2 0.4357670545578003\n",
      "epoch 1606, loss 0.013673616573214531, R2 0.44535231590270996\n",
      "Eval loss 0.01751759648323059, R2 0.43589454889297485\n",
      "epoch 1607, loss 0.013671291060745716, R2 0.44544708728790283\n",
      "Eval loss 0.017513643950223923, R2 0.4360218644142151\n",
      "epoch 1608, loss 0.013668968342244625, R2 0.4455408453941345\n",
      "Eval loss 0.017509693279862404, R2 0.4361485242843628\n",
      "epoch 1609, loss 0.013666647486388683, R2 0.4456353783607483\n",
      "Eval loss 0.017505750060081482, R2 0.436275839805603\n",
      "epoch 1610, loss 0.01366433221846819, R2 0.4457288980484009\n",
      "Eval loss 0.017501812428236008, R2 0.43640249967575073\n",
      "epoch 1611, loss 0.013662018813192844, R2 0.44582271575927734\n",
      "Eval loss 0.017497878521680832, R2 0.4365296959877014\n",
      "epoch 1612, loss 0.013659708201885223, R2 0.44591647386550903\n",
      "Eval loss 0.017493948340415955, R2 0.43665605783462524\n",
      "epoch 1613, loss 0.01365740317851305, R2 0.4460099935531616\n",
      "Eval loss 0.017490025609731674, R2 0.4367824196815491\n",
      "epoch 1614, loss 0.013655097223818302, R2 0.446103036403656\n",
      "Eval loss 0.01748610846698284, R2 0.4369085431098938\n",
      "epoch 1615, loss 0.013652797788381577, R2 0.4461965560913086\n",
      "Eval loss 0.017482193186879158, R2 0.43703413009643555\n",
      "epoch 1616, loss 0.013650499284267426, R2 0.4462900161743164\n",
      "Eval loss 0.01747828535735607, R2 0.4371604919433594\n",
      "epoch 1617, loss 0.013648204505443573, R2 0.446383535861969\n",
      "Eval loss 0.017474381253123283, R2 0.4372861981391907\n",
      "epoch 1618, loss 0.013645913451910019, R2 0.44647663831710815\n",
      "Eval loss 0.017470480874180794, R2 0.4374116063117981\n",
      "epoch 1619, loss 0.013643625192344189, R2 0.4465690851211548\n",
      "Eval loss 0.017466586083173752, R2 0.43753665685653687\n",
      "epoch 1620, loss 0.013641338795423508, R2 0.4466615915298462\n",
      "Eval loss 0.017462696880102158, R2 0.43766260147094727\n",
      "epoch 1621, loss 0.0136390570551157, R2 0.44675391912460327\n",
      "Eval loss 0.017458811402320862, R2 0.4377877712249756\n",
      "epoch 1622, loss 0.013636777177453041, R2 0.446846604347229\n",
      "Eval loss 0.017454933375120163, R2 0.4379124641418457\n",
      "epoch 1623, loss 0.013634500093758106, R2 0.4469391703605652\n",
      "Eval loss 0.017451057210564613, R2 0.4380369186401367\n",
      "epoch 1624, loss 0.013632228597998619, R2 0.4470309019088745\n",
      "Eval loss 0.01744718849658966, R2 0.43816184997558594\n",
      "epoch 1625, loss 0.013629958964884281, R2 0.4471232295036316\n",
      "Eval loss 0.017443323507905006, R2 0.43828630447387695\n",
      "epoch 1626, loss 0.013627689331769943, R2 0.4472154974937439\n",
      "Eval loss 0.0174394603818655, R2 0.4384106993675232\n",
      "epoch 1627, loss 0.013625426217913628, R2 0.4473074674606323\n",
      "Eval loss 0.017435606569051743, R2 0.43853479623794556\n",
      "epoch 1628, loss 0.013623164966702461, R2 0.4473985433578491\n",
      "Eval loss 0.017431754618883133, R2 0.43865883350372314\n",
      "epoch 1629, loss 0.013620907440781593, R2 0.44749099016189575\n",
      "Eval loss 0.01742791198194027, R2 0.4387827515602112\n",
      "epoch 1630, loss 0.0136186508461833, R2 0.4475817084312439\n",
      "Eval loss 0.017424069344997406, R2 0.43890631198883057\n",
      "epoch 1631, loss 0.01361639890819788, R2 0.4476732611656189\n",
      "Eval loss 0.01742023415863514, R2 0.4390294551849365\n",
      "epoch 1632, loss 0.013614150695502758, R2 0.44776445627212524\n",
      "Eval loss 0.01741640269756317, R2 0.43915271759033203\n",
      "epoch 1633, loss 0.013611903414130211, R2 0.4478555917739868\n",
      "Eval loss 0.017412574961781502, R2 0.43927645683288574\n",
      "epoch 1634, loss 0.013609659858047962, R2 0.44794726371765137\n",
      "Eval loss 0.01740875281393528, R2 0.4393995404243469\n",
      "epoch 1635, loss 0.013607420027256012, R2 0.44803768396377563\n",
      "Eval loss 0.017404938116669655, R2 0.4395219087600708\n",
      "epoch 1636, loss 0.013605182059109211, R2 0.44812846183776855\n",
      "Eval loss 0.01740112341940403, R2 0.4396452307701111\n",
      "epoch 1637, loss 0.013602946884930134, R2 0.4482191205024719\n",
      "Eval loss 0.017397316172719002, R2 0.4397680163383484\n",
      "epoch 1638, loss 0.01360071636736393, R2 0.44830936193466187\n",
      "Eval loss 0.01739351451396942, R2 0.43989014625549316\n",
      "epoch 1639, loss 0.01359848864376545, R2 0.4484003782272339\n",
      "Eval loss 0.01738971658051014, R2 0.4400128722190857\n",
      "epoch 1640, loss 0.013596262782812119, R2 0.4484902620315552\n",
      "Eval loss 0.017385924234986305, R2 0.4401348829269409\n",
      "epoch 1641, loss 0.013594039715826511, R2 0.44858062267303467\n",
      "Eval loss 0.01738213561475277, R2 0.44025617837905884\n",
      "epoch 1642, loss 0.013591821305453777, R2 0.4486702084541321\n",
      "Eval loss 0.017378350719809532, R2 0.4403786063194275\n",
      "epoch 1643, loss 0.013589603826403618, R2 0.44876033067703247\n",
      "Eval loss 0.017374573275446892, R2 0.44050025939941406\n",
      "epoch 1644, loss 0.013587390072643757, R2 0.44884973764419556\n",
      "Eval loss 0.0173707976937294, R2 0.4406217932701111\n",
      "epoch 1645, loss 0.013585180044174194, R2 0.44893980026245117\n",
      "Eval loss 0.017367027699947357, R2 0.44074302911758423\n",
      "epoch 1646, loss 0.013582971878349781, R2 0.4490291476249695\n",
      "Eval loss 0.017363261431455612, R2 0.4408644437789917\n",
      "epoch 1647, loss 0.013580766506493092, R2 0.44911861419677734\n",
      "Eval loss 0.017359502613544464, R2 0.4409855008125305\n",
      "epoch 1648, loss 0.0135785648599267, R2 0.4492079019546509\n",
      "Eval loss 0.017355747520923615, R2 0.4411066174507141\n",
      "epoch 1649, loss 0.013576366007328033, R2 0.44929713010787964\n",
      "Eval loss 0.017351998016238213, R2 0.4412271976470947\n",
      "epoch 1650, loss 0.013574170880019665, R2 0.4493861198425293\n",
      "Eval loss 0.01734825223684311, R2 0.44134730100631714\n",
      "epoch 1651, loss 0.01357197668403387, R2 0.44947516918182373\n",
      "Eval loss 0.017344510182738304, R2 0.4414682984352112\n",
      "epoch 1652, loss 0.013569789007306099, R2 0.4495636820793152\n",
      "Eval loss 0.017340773716568947, R2 0.441588819026947\n",
      "epoch 1653, loss 0.013567600399255753, R2 0.44965243339538574\n",
      "Eval loss 0.017337040975689888, R2 0.4417083263397217\n",
      "epoch 1654, loss 0.013565415516495705, R2 0.44974130392074585\n",
      "Eval loss 0.017333313822746277, R2 0.4418289065361023\n",
      "epoch 1655, loss 0.01356323342770338, R2 0.4498298168182373\n",
      "Eval loss 0.017329592257738113, R2 0.44194871187210083\n",
      "epoch 1656, loss 0.01356105599552393, R2 0.44991832971572876\n",
      "Eval loss 0.0173258725553751, R2 0.4420686364173889\n",
      "epoch 1657, loss 0.013558880425989628, R2 0.450006365776062\n",
      "Eval loss 0.017322158440947533, R2 0.4421879053115845\n",
      "epoch 1658, loss 0.013556706719100475, R2 0.4500945210456848\n",
      "Eval loss 0.017318451777100563, R2 0.44230741262435913\n",
      "epoch 1659, loss 0.013554535806179047, R2 0.45018261671066284\n",
      "Eval loss 0.017314746975898743, R2 0.44242674112319946\n",
      "epoch 1660, loss 0.013552370481193066, R2 0.45027023553848267\n",
      "Eval loss 0.01731104776263237, R2 0.44254589080810547\n",
      "epoch 1661, loss 0.013550205156207085, R2 0.450358510017395\n",
      "Eval loss 0.017307350412011147, R2 0.4426649212837219\n",
      "epoch 1662, loss 0.013548043556511402, R2 0.4504459500312805\n",
      "Eval loss 0.01730366423726082, R2 0.4427836537361145\n",
      "epoch 1663, loss 0.013545884750783443, R2 0.450533926486969\n",
      "Eval loss 0.01729997619986534, R2 0.4429023861885071\n",
      "epoch 1664, loss 0.013543728739023209, R2 0.4506216049194336\n",
      "Eval loss 0.01729629561305046, R2 0.4430214762687683\n",
      "epoch 1665, loss 0.013541576452553272, R2 0.450708270072937\n",
      "Eval loss 0.017292620614171028, R2 0.44313931465148926\n",
      "epoch 1666, loss 0.013539427891373634, R2 0.4507954716682434\n",
      "Eval loss 0.017288947477936745, R2 0.44325757026672363\n",
      "epoch 1667, loss 0.013537280261516571, R2 0.45088255405426025\n",
      "Eval loss 0.01728527992963791, R2 0.44337546825408936\n",
      "epoch 1668, loss 0.013535138219594955, R2 0.4509694576263428\n",
      "Eval loss 0.01728161796927452, R2 0.4434936046600342\n",
      "epoch 1669, loss 0.013532995246350765, R2 0.4510563611984253\n",
      "Eval loss 0.01727795973420143, R2 0.4436115622520447\n",
      "epoch 1670, loss 0.013530855998396873, R2 0.45114314556121826\n",
      "Eval loss 0.01727430522441864, R2 0.44372886419296265\n",
      "epoch 1671, loss 0.013528721407055855, R2 0.45123034715652466\n",
      "Eval loss 0.017270658165216446, R2 0.4438467025756836\n",
      "epoch 1672, loss 0.01352658774703741, R2 0.4513167142868042\n",
      "Eval loss 0.0172670129686594, R2 0.443963885307312\n",
      "epoch 1673, loss 0.01352445874363184, R2 0.4514026641845703\n",
      "Eval loss 0.017263371497392654, R2 0.4440809488296509\n",
      "epoch 1674, loss 0.013522329740226269, R2 0.45148903131484985\n",
      "Eval loss 0.017259735614061356, R2 0.44419825077056885\n",
      "epoch 1675, loss 0.013520206324756145, R2 0.4515751600265503\n",
      "Eval loss 0.017256105318665504, R2 0.4443151354789734\n",
      "epoch 1676, loss 0.013518083840608597, R2 0.45166122913360596\n",
      "Eval loss 0.0172524806112051, R2 0.4444319009780884\n",
      "epoch 1677, loss 0.013515965081751347, R2 0.45174741744995117\n",
      "Eval loss 0.017248855903744698, R2 0.4445486068725586\n",
      "epoch 1678, loss 0.013513850048184395, R2 0.45183318853378296\n",
      "Eval loss 0.01724524050951004, R2 0.44466501474380493\n",
      "epoch 1679, loss 0.013511737808585167, R2 0.4519188404083252\n",
      "Eval loss 0.017241625115275383, R2 0.44478142261505127\n",
      "epoch 1680, loss 0.013509627431631088, R2 0.45200467109680176\n",
      "Eval loss 0.017238017171621323, R2 0.4448971152305603\n",
      "epoch 1681, loss 0.013507518917322159, R2 0.452089786529541\n",
      "Eval loss 0.01723441481590271, R2 0.4450136423110962\n",
      "epoch 1682, loss 0.013505415059626102, R2 0.4521751403808594\n",
      "Eval loss 0.017230814322829247, R2 0.445129930973053\n",
      "epoch 1683, loss 0.01350331213325262, R2 0.45226043462753296\n",
      "Eval loss 0.01722721941769123, R2 0.44524532556533813\n",
      "epoch 1684, loss 0.013501211069524288, R2 0.45234543085098267\n",
      "Eval loss 0.017223628237843513, R2 0.4453609585762024\n",
      "epoch 1685, loss 0.013499116525053978, R2 0.4524306058883667\n",
      "Eval loss 0.017220042645931244, R2 0.4454764723777771\n",
      "epoch 1686, loss 0.013497021049261093, R2 0.45251601934432983\n",
      "Eval loss 0.017216460779309273, R2 0.4455918073654175\n",
      "epoch 1687, loss 0.013494933024048805, R2 0.45260030031204224\n",
      "Eval loss 0.0172128826379776, R2 0.4457070231437683\n",
      "epoch 1688, loss 0.013492843136191368, R2 0.45268529653549194\n",
      "Eval loss 0.017209311947226524, R2 0.4458216428756714\n",
      "epoch 1689, loss 0.01349075697362423, R2 0.452769935131073\n",
      "Eval loss 0.017205744981765747, R2 0.4459368586540222\n",
      "epoch 1690, loss 0.013488675467669964, R2 0.4528541564941406\n",
      "Eval loss 0.01720217987895012, R2 0.4460516571998596\n",
      "epoch 1691, loss 0.013486593961715698, R2 0.45293837785720825\n",
      "Eval loss 0.01719861850142479, R2 0.44616633653640747\n",
      "epoch 1692, loss 0.013484518975019455, R2 0.45302295684814453\n",
      "Eval loss 0.017195068299770355, R2 0.44628071784973145\n",
      "epoch 1693, loss 0.013482443988323212, R2 0.4531068801879883\n",
      "Eval loss 0.017191516235470772, R2 0.44639503955841064\n",
      "epoch 1694, loss 0.013480372726917267, R2 0.453191339969635\n",
      "Eval loss 0.017187969759106636, R2 0.44650959968566895\n",
      "epoch 1695, loss 0.013478303328156471, R2 0.45327484607696533\n",
      "Eval loss 0.01718442514538765, R2 0.44662338495254517\n",
      "epoch 1696, loss 0.013476238586008549, R2 0.45335865020751953\n",
      "Eval loss 0.01718088984489441, R2 0.4467371106147766\n",
      "epoch 1697, loss 0.0134741747751832, R2 0.4534423351287842\n",
      "Eval loss 0.01717735454440117, R2 0.44685107469558716\n",
      "epoch 1698, loss 0.013472113758325577, R2 0.45352596044540405\n",
      "Eval loss 0.017173828557133675, R2 0.4469648003578186\n",
      "epoch 1699, loss 0.013470053672790527, R2 0.4536094665527344\n",
      "Eval loss 0.01717030443251133, R2 0.44707757234573364\n",
      "epoch 1700, loss 0.0134680001065135, R2 0.4536932110786438\n",
      "Eval loss 0.017166785895824432, R2 0.4471914768218994\n",
      "epoch 1701, loss 0.013465948402881622, R2 0.4537760019302368\n",
      "Eval loss 0.017163269221782684, R2 0.44730472564697266\n",
      "epoch 1702, loss 0.013463897630572319, R2 0.45385921001434326\n",
      "Eval loss 0.017159756273031235, R2 0.4474177956581116\n",
      "epoch 1703, loss 0.01346184965223074, R2 0.45394229888916016\n",
      "Eval loss 0.017156250774860382, R2 0.44753021001815796\n",
      "epoch 1704, loss 0.013459805399179459, R2 0.4540252089500427\n",
      "Eval loss 0.017152749001979828, R2 0.44764333963394165\n",
      "epoch 1705, loss 0.013457763940095901, R2 0.45410799980163574\n",
      "Eval loss 0.017149250954389572, R2 0.447756290435791\n",
      "epoch 1706, loss 0.013455724343657494, R2 0.4541911482810974\n",
      "Eval loss 0.017145754769444466, R2 0.4478687047958374\n",
      "epoch 1707, loss 0.013453688472509384, R2 0.4542732834815979\n",
      "Eval loss 0.017142266035079956, R2 0.44798052310943604\n",
      "epoch 1708, loss 0.01345165353268385, R2 0.4543558359146118\n",
      "Eval loss 0.017138781026005745, R2 0.448093056678772\n",
      "epoch 1709, loss 0.013449623249471188, R2 0.4544382095336914\n",
      "Eval loss 0.017135299742221832, R2 0.44820481538772583\n",
      "epoch 1710, loss 0.013447594828903675, R2 0.45452070236206055\n",
      "Eval loss 0.017131824046373367, R2 0.448317289352417\n",
      "epoch 1711, loss 0.013445569202303886, R2 0.4546026587486267\n",
      "Eval loss 0.01712835393846035, R2 0.4484292268753052\n",
      "epoch 1712, loss 0.013443546369671822, R2 0.45468491315841675\n",
      "Eval loss 0.017124885693192482, R2 0.44854074716567993\n",
      "epoch 1713, loss 0.013441525399684906, R2 0.4547666907310486\n",
      "Eval loss 0.017121421173214912, R2 0.448652446269989\n",
      "epoch 1714, loss 0.013439509086310863, R2 0.4548484683036804\n",
      "Eval loss 0.01711796224117279, R2 0.44876348972320557\n",
      "epoch 1715, loss 0.013437492772936821, R2 0.4549306631088257\n",
      "Eval loss 0.017114507034420967, R2 0.4488747715950012\n",
      "epoch 1716, loss 0.013435480184853077, R2 0.45501166582107544\n",
      "Eval loss 0.01711105741560459, R2 0.4489860534667969\n",
      "epoch 1717, loss 0.013433469459414482, R2 0.45509302616119385\n",
      "Eval loss 0.017107611522078514, R2 0.4490969777107239\n",
      "epoch 1718, loss 0.01343146339058876, R2 0.45517486333847046\n",
      "Eval loss 0.017104171216487885, R2 0.44920778274536133\n",
      "epoch 1719, loss 0.013429458253085613, R2 0.4552561640739441\n",
      "Eval loss 0.017100730910897255, R2 0.449318528175354\n",
      "epoch 1720, loss 0.01342745777219534, R2 0.4553377628326416\n",
      "Eval loss 0.017097296193242073, R2 0.44942933320999146\n",
      "epoch 1721, loss 0.01342545822262764, R2 0.4554184079170227\n",
      "Eval loss 0.01709386706352234, R2 0.4495396018028259\n",
      "epoch 1722, loss 0.01342346053570509, R2 0.45549947023391724\n",
      "Eval loss 0.017090443521738052, R2 0.4496498107910156\n",
      "epoch 1723, loss 0.013421466574072838, R2 0.45558077096939087\n",
      "Eval loss 0.017087019979953766, R2 0.44976019859313965\n",
      "epoch 1724, loss 0.013419476337730885, R2 0.4556610584259033\n",
      "Eval loss 0.017083603888750076, R2 0.4498700499534607\n",
      "epoch 1725, loss 0.01341748796403408, R2 0.45574212074279785\n",
      "Eval loss 0.017080193385481834, R2 0.44997990131378174\n",
      "epoch 1726, loss 0.013415501452982426, R2 0.4558221101760864\n",
      "Eval loss 0.017076782882213593, R2 0.45008987188339233\n",
      "epoch 1727, loss 0.013413519598543644, R2 0.45590245723724365\n",
      "Eval loss 0.017073379829525948, R2 0.45019930601119995\n",
      "epoch 1728, loss 0.013411538675427437, R2 0.4559834599494934\n",
      "Eval loss 0.0170699805021286, R2 0.4503081440925598\n",
      "epoch 1729, loss 0.013409559614956379, R2 0.45606333017349243\n",
      "Eval loss 0.017066583037376404, R2 0.4504183530807495\n",
      "epoch 1730, loss 0.01340758427977562, R2 0.45614343881607056\n",
      "Eval loss 0.017063194885849953, R2 0.45052677392959595\n",
      "epoch 1731, loss 0.013405611738562584, R2 0.4562234878540039\n",
      "Eval loss 0.017059804871678352, R2 0.450636625289917\n",
      "epoch 1732, loss 0.013403640128672123, R2 0.4563034772872925\n",
      "Eval loss 0.01705642230808735, R2 0.45074570178985596\n",
      "epoch 1733, loss 0.013401673175394535, R2 0.45638322830200195\n",
      "Eval loss 0.017053043469786644, R2 0.45085400342941284\n",
      "epoch 1734, loss 0.013399707153439522, R2 0.45646339654922485\n",
      "Eval loss 0.017049670219421387, R2 0.45096296072006226\n",
      "epoch 1735, loss 0.013397744856774807, R2 0.4565432071685791\n",
      "Eval loss 0.017046300694346428, R2 0.4510711431503296\n",
      "epoch 1736, loss 0.013395783491432667, R2 0.4566221237182617\n",
      "Eval loss 0.01704293303191662, R2 0.45117926597595215\n",
      "epoch 1737, loss 0.0133938267827034, R2 0.4567013382911682\n",
      "Eval loss 0.017039572820067406, R2 0.45128780603408813\n",
      "epoch 1738, loss 0.013391871005296707, R2 0.45678144693374634\n",
      "Eval loss 0.017036214470863342, R2 0.4513961672782898\n",
      "epoch 1739, loss 0.013389919884502888, R2 0.4568595886230469\n",
      "Eval loss 0.017032859846949577, R2 0.4515041708946228\n",
      "epoch 1740, loss 0.013387970626354218, R2 0.45693904161453247\n",
      "Eval loss 0.01702950894832611, R2 0.45161205530166626\n",
      "epoch 1741, loss 0.013386023230850697, R2 0.45701849460601807\n",
      "Eval loss 0.017026163637638092, R2 0.45171940326690674\n",
      "epoch 1742, loss 0.0133840786293149, R2 0.4570971131324768\n",
      "Eval loss 0.01702282391488552, R2 0.45182734727859497\n",
      "epoch 1743, loss 0.013382136821746826, R2 0.4571759104728699\n",
      "Eval loss 0.0170194860547781, R2 0.45193445682525635\n",
      "epoch 1744, loss 0.013380196876823902, R2 0.4572543501853943\n",
      "Eval loss 0.017016151919960976, R2 0.4520421624183655\n",
      "epoch 1745, loss 0.013378258794546127, R2 0.45733338594436646\n",
      "Eval loss 0.0170128233730793, R2 0.45214933156967163\n",
      "epoch 1746, loss 0.013376325368881226, R2 0.45741140842437744\n",
      "Eval loss 0.017009496688842773, R2 0.4522565007209778\n",
      "epoch 1747, loss 0.013374393805861473, R2 0.4574902057647705\n",
      "Eval loss 0.017006177455186844, R2 0.45236337184906006\n",
      "epoch 1748, loss 0.013372463174164295, R2 0.45756810903549194\n",
      "Eval loss 0.017002860084176064, R2 0.45247018337249756\n",
      "epoch 1749, loss 0.013370536267757416, R2 0.4576462507247925\n",
      "Eval loss 0.01699954643845558, R2 0.4525769352912903\n",
      "epoch 1750, loss 0.01336861215531826, R2 0.4577246904373169\n",
      "Eval loss 0.016996238380670547, R2 0.4526834487915039\n",
      "epoch 1751, loss 0.013366690836846828, R2 0.4578022360801697\n",
      "Eval loss 0.016992934048175812, R2 0.45278966426849365\n",
      "epoch 1752, loss 0.013364770449697971, R2 0.45788055658340454\n",
      "Eval loss 0.016989635303616524, R2 0.4528955817222595\n",
      "epoch 1753, loss 0.013362853787839413, R2 0.4579578638076782\n",
      "Eval loss 0.016986338421702385, R2 0.45300227403640747\n",
      "epoch 1754, loss 0.013360938988626003, R2 0.45803552865982056\n",
      "Eval loss 0.016983045265078545, R2 0.45310813188552856\n",
      "epoch 1755, loss 0.013359026983380318, R2 0.45811372995376587\n",
      "Eval loss 0.016979757696390152, R2 0.453214168548584\n",
      "epoch 1756, loss 0.01335711870342493, R2 0.4581905007362366\n",
      "Eval loss 0.016976473852992058, R2 0.4533199071884155\n",
      "epoch 1757, loss 0.013355213217437267, R2 0.4582682251930237\n",
      "Eval loss 0.01697319559752941, R2 0.45342546701431274\n",
      "epoch 1758, loss 0.01335330680012703, R2 0.45834487676620483\n",
      "Eval loss 0.016969917342066765, R2 0.4535312056541443\n",
      "epoch 1759, loss 0.01335140597075224, R2 0.4584226608276367\n",
      "Eval loss 0.016966644674539566, R2 0.45363640785217285\n",
      "epoch 1760, loss 0.013349507004022598, R2 0.4584992527961731\n",
      "Eval loss 0.016963377594947815, R2 0.4537411332130432\n",
      "epoch 1761, loss 0.013347609899938107, R2 0.4585757851600647\n",
      "Eval loss 0.016960112378001213, R2 0.45384645462036133\n",
      "epoch 1762, loss 0.013345717452466488, R2 0.45865297317504883\n",
      "Eval loss 0.016956854611635208, R2 0.45395171642303467\n",
      "epoch 1763, loss 0.013343824073672295, R2 0.4587295651435852\n",
      "Eval loss 0.016953594982624054, R2 0.45405662059783936\n",
      "epoch 1764, loss 0.013341936282813549, R2 0.4588063955307007\n",
      "Eval loss 0.016950344666838646, R2 0.45416080951690674\n",
      "epoch 1765, loss 0.013340048491954803, R2 0.4588831067085266\n",
      "Eval loss 0.016947098076343536, R2 0.4542657136917114\n",
      "epoch 1766, loss 0.013338162563741207, R2 0.4589594006538391\n",
      "Eval loss 0.016943853348493576, R2 0.454370379447937\n",
      "epoch 1767, loss 0.013336282223463058, R2 0.4590361714363098\n",
      "Eval loss 0.016940612345933914, R2 0.45447468757629395\n",
      "epoch 1768, loss 0.013334402814507484, R2 0.45911192893981934\n",
      "Eval loss 0.01693737879395485, R2 0.4545789957046509\n",
      "epoch 1769, loss 0.01333252526819706, R2 0.4591878652572632\n",
      "Eval loss 0.016934145241975784, R2 0.45468276739120483\n",
      "epoch 1770, loss 0.013330650515854359, R2 0.4592641592025757\n",
      "Eval loss 0.016930919140577316, R2 0.45478689670562744\n",
      "epoch 1771, loss 0.013328778557479382, R2 0.45934009552001953\n",
      "Eval loss 0.01692769303917885, R2 0.45489078760147095\n",
      "epoch 1772, loss 0.013326909393072128, R2 0.45941591262817383\n",
      "Eval loss 0.016924472525715828, R2 0.45499444007873535\n",
      "epoch 1773, loss 0.013325040228664875, R2 0.459492564201355\n",
      "Eval loss 0.016921255737543106, R2 0.4550977349281311\n",
      "epoch 1774, loss 0.013323175720870495, R2 0.4595673680305481\n",
      "Eval loss 0.01691804639995098, R2 0.45520108938217163\n",
      "epoch 1775, loss 0.013321314007043839, R2 0.4596428871154785\n",
      "Eval loss 0.016914837062358856, R2 0.4553046226501465\n",
      "epoch 1776, loss 0.013319454155862331, R2 0.4597189426422119\n",
      "Eval loss 0.01691163145005703, R2 0.455407977104187\n",
      "epoch 1777, loss 0.013317597098648548, R2 0.4597936272621155\n",
      "Eval loss 0.01690843142569065, R2 0.45551103353500366\n",
      "epoch 1778, loss 0.013315741904079914, R2 0.4598690867424011\n",
      "Eval loss 0.01690523698925972, R2 0.4556140899658203\n",
      "epoch 1779, loss 0.01331388857215643, R2 0.4599447250366211\n",
      "Eval loss 0.01690204255282879, R2 0.45571690797805786\n",
      "epoch 1780, loss 0.013312040828168392, R2 0.460019052028656\n",
      "Eval loss 0.016898855566978455, R2 0.4558192491531372\n",
      "epoch 1781, loss 0.013310189358890057, R2 0.4600939154624939\n",
      "Eval loss 0.01689566858112812, R2 0.45592135190963745\n",
      "epoch 1782, loss 0.013308346271514893, R2 0.46016931533813477\n",
      "Eval loss 0.016892487183213234, R2 0.4560239315032959\n",
      "epoch 1783, loss 0.013306502252817154, R2 0.4602441191673279\n",
      "Eval loss 0.016889311373233795, R2 0.45612668991088867\n",
      "epoch 1784, loss 0.013304662890732288, R2 0.46031832695007324\n",
      "Eval loss 0.016886139288544655, R2 0.456229031085968\n",
      "epoch 1785, loss 0.013302824459969997, R2 0.46039289236068726\n",
      "Eval loss 0.016882969066500664, R2 0.4563307762145996\n",
      "epoch 1786, loss 0.01330098882317543, R2 0.4604673385620117\n",
      "Eval loss 0.01687980443239212, R2 0.45643240213394165\n",
      "epoch 1787, loss 0.013299155049026012, R2 0.46054232120513916\n",
      "Eval loss 0.016876643523573875, R2 0.4565345048904419\n",
      "epoch 1788, loss 0.013297324068844318, R2 0.46061599254608154\n",
      "Eval loss 0.01687348634004593, R2 0.4566364884376526\n",
      "epoch 1789, loss 0.013295494019985199, R2 0.4606902003288269\n",
      "Eval loss 0.01687033101916313, R2 0.456737756729126\n",
      "epoch 1790, loss 0.013293669559061527, R2 0.46076422929763794\n",
      "Eval loss 0.01686718314886093, R2 0.4568391442298889\n",
      "epoch 1791, loss 0.013291845098137856, R2 0.4608386158943176\n",
      "Eval loss 0.01686403714120388, R2 0.45694059133529663\n",
      "epoch 1792, loss 0.013290023431181908, R2 0.4609121084213257\n",
      "Eval loss 0.016860896721482277, R2 0.45704126358032227\n",
      "epoch 1793, loss 0.013288204558193684, R2 0.4609858989715576\n",
      "Eval loss 0.016857754439115524, R2 0.4571424126625061\n",
      "epoch 1794, loss 0.013286387547850609, R2 0.4610595703125\n",
      "Eval loss 0.016854621469974518, R2 0.45724380016326904\n",
      "epoch 1795, loss 0.013284573331475258, R2 0.4611331820487976\n",
      "Eval loss 0.01685149222612381, R2 0.45734453201293945\n",
      "epoch 1796, loss 0.013282762840390205, R2 0.4612066149711609\n",
      "Eval loss 0.0168483667075634, R2 0.45744526386260986\n",
      "epoch 1797, loss 0.013280950486660004, R2 0.4612799286842346\n",
      "Eval loss 0.01684524118900299, R2 0.45754575729370117\n",
      "epoch 1798, loss 0.013279144652187824, R2 0.4613534212112427\n",
      "Eval loss 0.016842123121023178, R2 0.45764631032943726\n",
      "epoch 1799, loss 0.013277340680360794, R2 0.4614269733428955\n",
      "Eval loss 0.016839008778333664, R2 0.45774608850479126\n",
      "epoch 1800, loss 0.013275537639856339, R2 0.4615001082420349\n",
      "Eval loss 0.0168358962982893, R2 0.457846462726593\n",
      "epoch 1801, loss 0.013273737393319607, R2 0.46157270669937134\n",
      "Eval loss 0.01683279126882553, R2 0.4579463601112366\n",
      "epoch 1802, loss 0.013271939940750599, R2 0.4616456627845764\n",
      "Eval loss 0.016829686239361763, R2 0.45804697275161743\n",
      "epoch 1803, loss 0.01327014435082674, R2 0.46171867847442627\n",
      "Eval loss 0.016826586797833443, R2 0.4581465721130371\n",
      "epoch 1804, loss 0.013268351554870605, R2 0.4617909789085388\n",
      "Eval loss 0.01682348921895027, R2 0.4582463502883911\n",
      "epoch 1805, loss 0.013266559690237045, R2 0.4618638753890991\n",
      "Eval loss 0.016820397228002548, R2 0.458345890045166\n",
      "epoch 1806, loss 0.013264771550893784, R2 0.4619368314743042\n",
      "Eval loss 0.016817308962345123, R2 0.45844531059265137\n",
      "epoch 1807, loss 0.013262986205518246, R2 0.46200883388519287\n",
      "Eval loss 0.016814224421977997, R2 0.4585440754890442\n",
      "epoch 1808, loss 0.013261201791465282, R2 0.4620818495750427\n",
      "Eval loss 0.01681114360690117, R2 0.45864337682724\n",
      "epoch 1809, loss 0.013259420171380043, R2 0.46215349435806274\n",
      "Eval loss 0.01680806837975979, R2 0.458742618560791\n",
      "epoch 1810, loss 0.013257641345262527, R2 0.46222561597824097\n",
      "Eval loss 0.016804993152618408, R2 0.4588419795036316\n",
      "epoch 1811, loss 0.013255865313112736, R2 0.4622976779937744\n",
      "Eval loss 0.016801921650767326, R2 0.458940327167511\n",
      "epoch 1812, loss 0.013254090212285519, R2 0.4623696804046631\n",
      "Eval loss 0.01679885946214199, R2 0.45903950929641724\n",
      "epoch 1813, loss 0.0132523188367486, R2 0.4624413251876831\n",
      "Eval loss 0.016795795410871506, R2 0.4591381549835205\n",
      "epoch 1814, loss 0.013250548392534256, R2 0.4625133275985718\n",
      "Eval loss 0.01679273694753647, R2 0.45923662185668945\n",
      "epoch 1815, loss 0.013248780742287636, R2 0.4625852704048157\n",
      "Eval loss 0.01678968220949173, R2 0.45933496952056885\n",
      "epoch 1816, loss 0.01324701588600874, R2 0.46265727281570435\n",
      "Eval loss 0.01678663119673729, R2 0.45943278074264526\n",
      "epoch 1817, loss 0.013245251961052418, R2 0.46272796392440796\n",
      "Eval loss 0.016783585771918297, R2 0.45953118801116943\n",
      "epoch 1818, loss 0.013243491761386395, R2 0.462799608707428\n",
      "Eval loss 0.016780542209744453, R2 0.4596293568611145\n",
      "epoch 1819, loss 0.01324173342436552, R2 0.46287113428115845\n",
      "Eval loss 0.01677750051021576, R2 0.45972728729248047\n",
      "epoch 1820, loss 0.013239976949989796, R2 0.4629421830177307\n",
      "Eval loss 0.016774464398622513, R2 0.4598250389099121\n",
      "epoch 1821, loss 0.013238225132226944, R2 0.463013231754303\n",
      "Eval loss 0.016771435737609863, R2 0.4599226117134094\n",
      "epoch 1822, loss 0.013236472383141518, R2 0.46308434009552\n",
      "Eval loss 0.016768407076597214, R2 0.4600203037261963\n",
      "epoch 1823, loss 0.01323472335934639, R2 0.46315503120422363\n",
      "Eval loss 0.016765378415584564, R2 0.4601176977157593\n",
      "epoch 1824, loss 0.013232977129518986, R2 0.4632260799407959\n",
      "Eval loss 0.01676235720515251, R2 0.46021443605422974\n",
      "epoch 1825, loss 0.013231230899691582, R2 0.46329671144485474\n",
      "Eval loss 0.016759341582655907, R2 0.4603118896484375\n",
      "epoch 1826, loss 0.013229488395154476, R2 0.46336740255355835\n",
      "Eval loss 0.01675632782280445, R2 0.46040910482406616\n",
      "epoch 1827, loss 0.013227748684585094, R2 0.4634382128715515\n",
      "Eval loss 0.016753315925598145, R2 0.46050626039505005\n",
      "epoch 1828, loss 0.013226010836660862, R2 0.463508665561676\n",
      "Eval loss 0.016750311478972435, R2 0.46060270071029663\n",
      "epoch 1829, loss 0.013224274851381779, R2 0.46357911825180054\n",
      "Eval loss 0.016747307032346725, R2 0.4606999158859253\n",
      "epoch 1830, loss 0.01322254166007042, R2 0.4636493921279907\n",
      "Eval loss 0.016744308173656464, R2 0.4607961177825928\n",
      "epoch 1831, loss 0.013220811262726784, R2 0.46371984481811523\n",
      "Eval loss 0.0167413130402565, R2 0.46089237928390503\n",
      "epoch 1832, loss 0.013219080865383148, R2 0.46378999948501587\n",
      "Eval loss 0.016738319769501686, R2 0.4609886407852173\n",
      "epoch 1833, loss 0.013217354193329811, R2 0.4638598561286926\n",
      "Eval loss 0.01673533394932747, R2 0.46108514070510864\n",
      "epoch 1834, loss 0.013215630315244198, R2 0.46392977237701416\n",
      "Eval loss 0.01673234812915325, R2 0.46118128299713135\n",
      "epoch 1835, loss 0.01321390736848116, R2 0.4639996290206909\n",
      "Eval loss 0.01672936975955963, R2 0.4612768888473511\n",
      "epoch 1836, loss 0.013212188147008419, R2 0.46406978368759155\n",
      "Eval loss 0.01672639138996601, R2 0.46137309074401855\n",
      "epoch 1837, loss 0.013210470788180828, R2 0.46413904428482056\n",
      "Eval loss 0.01672341860830784, R2 0.4614688754081726\n",
      "epoch 1838, loss 0.013208755291998386, R2 0.46420860290527344\n",
      "Eval loss 0.016720447689294815, R2 0.46156466007232666\n",
      "epoch 1839, loss 0.013207042589783669, R2 0.46427851915359497\n",
      "Eval loss 0.01671747863292694, R2 0.46166008710861206\n",
      "epoch 1840, loss 0.0132053317502141, R2 0.4643477201461792\n",
      "Eval loss 0.016714518889784813, R2 0.4617554545402527\n",
      "epoch 1841, loss 0.013203621841967106, R2 0.464417040348053\n",
      "Eval loss 0.016711561009287834, R2 0.46185070276260376\n",
      "epoch 1842, loss 0.013201916590332985, R2 0.4644864797592163\n",
      "Eval loss 0.016708604991436005, R2 0.46194589138031006\n",
      "epoch 1843, loss 0.013200211338698864, R2 0.46455520391464233\n",
      "Eval loss 0.016705648973584175, R2 0.46204107999801636\n",
      "epoch 1844, loss 0.013198507949709892, R2 0.46462494134902954\n",
      "Eval loss 0.016702702268958092, R2 0.4621359705924988\n",
      "epoch 1845, loss 0.013196809217333794, R2 0.46469300985336304\n",
      "Eval loss 0.016699759289622307, R2 0.4622305631637573\n",
      "epoch 1846, loss 0.01319511141628027, R2 0.46476268768310547\n",
      "Eval loss 0.016696816310286522, R2 0.4623250365257263\n",
      "epoch 1847, loss 0.013193415477871895, R2 0.4648308753967285\n",
      "Eval loss 0.016693880781531334, R2 0.4624195098876953\n",
      "epoch 1848, loss 0.01319172140210867, R2 0.46489959955215454\n",
      "Eval loss 0.016690945252776146, R2 0.4625145196914673\n",
      "epoch 1849, loss 0.013190031051635742, R2 0.46496814489364624\n",
      "Eval loss 0.016688013449311256, R2 0.4626086354255676\n",
      "epoch 1850, loss 0.013188340701162815, R2 0.46503692865371704\n",
      "Eval loss 0.016685087233781815, R2 0.46270298957824707\n",
      "epoch 1851, loss 0.013186655007302761, R2 0.465104877948761\n",
      "Eval loss 0.01668216660618782, R2 0.4627971053123474\n",
      "epoch 1852, loss 0.013184969313442707, R2 0.4651739001274109\n",
      "Eval loss 0.016679244115948677, R2 0.4628910422325134\n",
      "epoch 1853, loss 0.013183288276195526, R2 0.46524184942245483\n",
      "Eval loss 0.01667632907629013, R2 0.4629852771759033\n",
      "epoch 1854, loss 0.013181609101593494, R2 0.4653097987174988\n",
      "Eval loss 0.016673417761921883, R2 0.4630788564682007\n",
      "epoch 1855, loss 0.013179930858314037, R2 0.46537744998931885\n",
      "Eval loss 0.016670506447553635, R2 0.46317219734191895\n",
      "epoch 1856, loss 0.01317825447767973, R2 0.46544647216796875\n",
      "Eval loss 0.016667602583765984, R2 0.46326643228530884\n",
      "epoch 1857, loss 0.013176580891013145, R2 0.46551334857940674\n",
      "Eval loss 0.016664698719978333, R2 0.4633597135543823\n",
      "epoch 1858, loss 0.01317490916699171, R2 0.4655815362930298\n",
      "Eval loss 0.01666179858148098, R2 0.46345311403274536\n",
      "epoch 1859, loss 0.013173239305615425, R2 0.4656493067741394\n",
      "Eval loss 0.016658905893564224, R2 0.46354615688323975\n",
      "epoch 1860, loss 0.013171571306884289, R2 0.46571671962738037\n",
      "Eval loss 0.01665601320564747, R2 0.46363943815231323\n",
      "epoch 1861, loss 0.013169906102120876, R2 0.4657846689224243\n",
      "Eval loss 0.01665312610566616, R2 0.4637325406074524\n",
      "epoch 1862, loss 0.013168244622647762, R2 0.46585190296173096\n",
      "Eval loss 0.01665024273097515, R2 0.4638252854347229\n",
      "epoch 1863, loss 0.013166582211852074, R2 0.4659193158149719\n",
      "Eval loss 0.01664736121892929, R2 0.4639180302619934\n",
      "epoch 1864, loss 0.013164925388991833, R2 0.4659864902496338\n",
      "Eval loss 0.016644485294818878, R2 0.46401065587997437\n",
      "epoch 1865, loss 0.013163268566131592, R2 0.46605372428894043\n",
      "Eval loss 0.016641611233353615, R2 0.46410322189331055\n",
      "epoch 1866, loss 0.0131616136059165, R2 0.4661206603050232\n",
      "Eval loss 0.01663874089717865, R2 0.4641956686973572\n",
      "epoch 1867, loss 0.013159961439669132, R2 0.4661882519721985\n",
      "Eval loss 0.016635872423648834, R2 0.46428751945495605\n",
      "epoch 1868, loss 0.013158310204744339, R2 0.46625465154647827\n",
      "Eval loss 0.016633009538054466, R2 0.46438026428222656\n",
      "epoch 1869, loss 0.013156664557754993, R2 0.46632158756256104\n",
      "Eval loss 0.016630148515105247, R2 0.464471697807312\n",
      "epoch 1870, loss 0.013155017048120499, R2 0.4663888216018677\n",
      "Eval loss 0.016627294942736626, R2 0.46456408500671387\n",
      "epoch 1871, loss 0.013153375126421452, R2 0.46645480394363403\n",
      "Eval loss 0.016624439507722855, R2 0.46465635299682617\n",
      "epoch 1872, loss 0.013151734136044979, R2 0.466522216796875\n",
      "Eval loss 0.01662159152328968, R2 0.46474790573120117\n",
      "epoch 1873, loss 0.013150093145668507, R2 0.46658796072006226\n",
      "Eval loss 0.016618743538856506, R2 0.4648395776748657\n",
      "epoch 1874, loss 0.013148457743227482, R2 0.46665430068969727\n",
      "Eval loss 0.01661590300500393, R2 0.4649311304092407\n",
      "epoch 1875, loss 0.013146822340786457, R2 0.4667208194732666\n",
      "Eval loss 0.0166130643337965, R2 0.4650225043296814\n",
      "epoch 1876, loss 0.013145189732313156, R2 0.46678686141967773\n",
      "Eval loss 0.016610227525234222, R2 0.46511322259902954\n",
      "epoch 1877, loss 0.013143557123839855, R2 0.4668530821800232\n",
      "Eval loss 0.01660739630460739, R2 0.4652050733566284\n",
      "epoch 1878, loss 0.013141927309334278, R2 0.4669193625450134\n",
      "Eval loss 0.01660456694662571, R2 0.4652961492538452\n",
      "epoch 1879, loss 0.013140303082764149, R2 0.466985285282135\n",
      "Eval loss 0.016601739451289177, R2 0.46538716554641724\n",
      "epoch 1880, loss 0.013138677924871445, R2 0.4670509696006775\n",
      "Eval loss 0.01659891940653324, R2 0.4654773473739624\n",
      "epoch 1881, loss 0.01313705649226904, R2 0.4671167731285095\n",
      "Eval loss 0.016596101224422455, R2 0.4655689001083374\n",
      "epoch 1882, loss 0.013135435990989208, R2 0.4671826958656311\n",
      "Eval loss 0.016593286767601967, R2 0.46565937995910645\n",
      "epoch 1883, loss 0.013133815489709377, R2 0.4672486186027527\n",
      "Eval loss 0.01659047231078148, R2 0.46574997901916504\n",
      "epoch 1884, loss 0.013132200576364994, R2 0.46731436252593994\n",
      "Eval loss 0.016587665304541588, R2 0.4658403992652893\n",
      "epoch 1885, loss 0.013130586594343185, R2 0.467379629611969\n",
      "Eval loss 0.016584860160946846, R2 0.4659307599067688\n",
      "epoch 1886, loss 0.013128974474966526, R2 0.4674450159072876\n",
      "Eval loss 0.016582060605287552, R2 0.466020405292511\n",
      "epoch 1887, loss 0.01312736514955759, R2 0.4675098657608032\n",
      "Eval loss 0.016579261049628258, R2 0.46611106395721436\n",
      "epoch 1888, loss 0.01312575489282608, R2 0.4675758481025696\n",
      "Eval loss 0.016576463356614113, R2 0.466200590133667\n",
      "epoch 1889, loss 0.013124150224030018, R2 0.4676406979560852\n",
      "Eval loss 0.016573673114180565, R2 0.46629053354263306\n",
      "epoch 1890, loss 0.01312254648655653, R2 0.4677051305770874\n",
      "Eval loss 0.016570884734392166, R2 0.46638089418411255\n",
      "epoch 1891, loss 0.013120945543050766, R2 0.4677702784538269\n",
      "Eval loss 0.016568100079894066, R2 0.46647047996520996\n",
      "epoch 1892, loss 0.013119345530867577, R2 0.46783536672592163\n",
      "Eval loss 0.016565321013331413, R2 0.4665599465370178\n",
      "epoch 1893, loss 0.013117747381329536, R2 0.46790021657943726\n",
      "Eval loss 0.01656254194676876, R2 0.4666494131088257\n",
      "epoch 1894, loss 0.01311615202575922, R2 0.4679655432701111\n",
      "Eval loss 0.016559768468141556, R2 0.466738760471344\n",
      "epoch 1895, loss 0.013114558532834053, R2 0.46802955865859985\n",
      "Eval loss 0.0165569968521595, R2 0.46682798862457275\n",
      "epoch 1896, loss 0.013112966902554035, R2 0.4680943489074707\n",
      "Eval loss 0.016554227098822594, R2 0.46691685914993286\n",
      "epoch 1897, loss 0.013111378997564316, R2 0.4681587219238281\n",
      "Eval loss 0.016551462933421135, R2 0.46700620651245117\n",
      "epoch 1898, loss 0.013109790161252022, R2 0.468222975730896\n",
      "Eval loss 0.016548702493309975, R2 0.46709489822387695\n",
      "epoch 1899, loss 0.013108205050230026, R2 0.46828770637512207\n",
      "Eval loss 0.016545943915843964, R2 0.4671838879585266\n",
      "epoch 1900, loss 0.01310661993920803, R2 0.4683515429496765\n",
      "Eval loss 0.0165431909263134, R2 0.4672725796699524\n",
      "epoch 1901, loss 0.013105042278766632, R2 0.4684157967567444\n",
      "Eval loss 0.016540437936782837, R2 0.4673612117767334\n",
      "epoch 1902, loss 0.013103462755680084, R2 0.4684794545173645\n",
      "Eval loss 0.01653769053518772, R2 0.46744924783706665\n",
      "epoch 1903, loss 0.013101884163916111, R2 0.468544065952301\n",
      "Eval loss 0.016534946858882904, R2 0.467538058757782\n",
      "epoch 1904, loss 0.013100309297442436, R2 0.4686077833175659\n",
      "Eval loss 0.016532205045223236, R2 0.4676257371902466\n",
      "epoch 1905, loss 0.01309873815625906, R2 0.4686712622642517\n",
      "Eval loss 0.016529466956853867, R2 0.4677143692970276\n",
      "epoch 1906, loss 0.013097166083753109, R2 0.4687346816062927\n",
      "Eval loss 0.016526732593774796, R2 0.4678027629852295\n",
      "epoch 1907, loss 0.013095598667860031, R2 0.46879929304122925\n",
      "Eval loss 0.016524000093340874, R2 0.46789056062698364\n",
      "epoch 1908, loss 0.013094030320644379, R2 0.46886205673217773\n",
      "Eval loss 0.0165212731808424, R2 0.46797841787338257\n",
      "epoch 1909, loss 0.013092463836073875, R2 0.46892601251602173\n",
      "Eval loss 0.016518551856279373, R2 0.4680660367012024\n",
      "epoch 1910, loss 0.01309090293943882, R2 0.46898889541625977\n",
      "Eval loss 0.01651582680642605, R2 0.46815359592437744\n",
      "epoch 1911, loss 0.013089342974126339, R2 0.46905237436294556\n",
      "Eval loss 0.01651311106979847, R2 0.46824121475219727\n",
      "epoch 1912, loss 0.013087783008813858, R2 0.46911609172821045\n",
      "Eval loss 0.01651039347052574, R2 0.4683285355567932\n",
      "epoch 1913, loss 0.013086226768791676, R2 0.46917879581451416\n",
      "Eval loss 0.01650768518447876, R2 0.4684157967567444\n",
      "epoch 1914, loss 0.013084672391414642, R2 0.4692416191101074\n",
      "Eval loss 0.016504976898431778, R2 0.4685029983520508\n",
      "epoch 1915, loss 0.013083118014037609, R2 0.46930527687072754\n",
      "Eval loss 0.016502272337675095, R2 0.4685904383659363\n",
      "epoch 1916, loss 0.013081567361950874, R2 0.4693678021430969\n",
      "Eval loss 0.01649956963956356, R2 0.46867746114730835\n",
      "epoch 1917, loss 0.013080018572509289, R2 0.4694306254386902\n",
      "Eval loss 0.016496870666742325, R2 0.46876436471939087\n",
      "epoch 1918, loss 0.013078472577035427, R2 0.4694933295249939\n",
      "Eval loss 0.016494177281856537, R2 0.46885108947753906\n",
      "epoch 1919, loss 0.01307692751288414, R2 0.4695557951927185\n",
      "Eval loss 0.01649148389697075, R2 0.46893763542175293\n",
      "epoch 1920, loss 0.013075384311378002, R2 0.4696187973022461\n",
      "Eval loss 0.01648879610002041, R2 0.4690242409706116\n",
      "epoch 1921, loss 0.013073843903839588, R2 0.4696815013885498\n",
      "Eval loss 0.016486110165715218, R2 0.46911072731018066\n",
      "epoch 1922, loss 0.013072307221591473, R2 0.46974343061447144\n",
      "Eval loss 0.016483427956700325, R2 0.46919703483581543\n",
      "epoch 1923, loss 0.013070767745375633, R2 0.46980583667755127\n",
      "Eval loss 0.01648074947297573, R2 0.46928316354751587\n",
      "epoch 1924, loss 0.013069234788417816, R2 0.4698678255081177\n",
      "Eval loss 0.016478072851896286, R2 0.4693695306777954\n",
      "epoch 1925, loss 0.013067700900137424, R2 0.4699300527572632\n",
      "Eval loss 0.01647540181875229, R2 0.46945589780807495\n",
      "epoch 1926, loss 0.013066168874502182, R2 0.46999263763427734\n",
      "Eval loss 0.01647273078560829, R2 0.4695415496826172\n",
      "epoch 1927, loss 0.013064640574157238, R2 0.47005438804626465\n",
      "Eval loss 0.016470065340399742, R2 0.4696275591850281\n",
      "epoch 1928, loss 0.013063114136457443, R2 0.4701163172721863\n",
      "Eval loss 0.01646740362048149, R2 0.46971309185028076\n",
      "epoch 1929, loss 0.013061588630080223, R2 0.47017818689346313\n",
      "Eval loss 0.01646474376320839, R2 0.46979856491088867\n",
      "epoch 1930, loss 0.013060065917670727, R2 0.4702397584915161\n",
      "Eval loss 0.016462085768580437, R2 0.46988385915756226\n",
      "epoch 1931, loss 0.01305854320526123, R2 0.4703017473220825\n",
      "Eval loss 0.01645943522453308, R2 0.4699695110321045\n",
      "epoch 1932, loss 0.013057024218142033, R2 0.47036314010620117\n",
      "Eval loss 0.016456784680485725, R2 0.4700550436973572\n",
      "epoch 1933, loss 0.013055507093667984, R2 0.4704253077507019\n",
      "Eval loss 0.016454137861728668, R2 0.4701398015022278\n",
      "epoch 1934, loss 0.013053991831839085, R2 0.4704861640930176\n",
      "Eval loss 0.01645149290561676, R2 0.47022545337677\n",
      "epoch 1935, loss 0.013052479363977909, R2 0.47054773569107056\n",
      "Eval loss 0.0164488535374403, R2 0.4703104496002197\n",
      "epoch 1936, loss 0.013050967827439308, R2 0.47060900926589966\n",
      "Eval loss 0.01644621603190899, R2 0.47039538621902466\n",
      "epoch 1937, loss 0.013049458153545856, R2 0.470670223236084\n",
      "Eval loss 0.016443582251667976, R2 0.47048020362854004\n",
      "epoch 1938, loss 0.013047950342297554, R2 0.4707314372062683\n",
      "Eval loss 0.016440952196717262, R2 0.47056472301483154\n",
      "epoch 1939, loss 0.013046443462371826, R2 0.4707925319671631\n",
      "Eval loss 0.016438324004411697, R2 0.4706496596336365\n",
      "epoch 1940, loss 0.013044941239058971, R2 0.47085368633270264\n",
      "Eval loss 0.01643570140004158, R2 0.47073400020599365\n",
      "epoch 1941, loss 0.013043439015746117, R2 0.47091442346572876\n",
      "Eval loss 0.016433078795671463, R2 0.47081810235977173\n",
      "epoch 1942, loss 0.01304194051772356, R2 0.4709751605987549\n",
      "Eval loss 0.016430459916591644, R2 0.47090256214141846\n",
      "epoch 1943, loss 0.013040442019701004, R2 0.4710361361503601\n",
      "Eval loss 0.016427846625447273, R2 0.4709869623184204\n",
      "epoch 1944, loss 0.013038945384323597, R2 0.4710966944694519\n",
      "Eval loss 0.016425231471657753, R2 0.47107112407684326\n",
      "epoch 1945, loss 0.013037452474236488, R2 0.47115784883499146\n",
      "Eval loss 0.01642262376844883, R2 0.4711551070213318\n",
      "epoch 1946, loss 0.013035960495471954, R2 0.4712177515029907\n",
      "Eval loss 0.016420017927885056, R2 0.47123903036117554\n",
      "epoch 1947, loss 0.01303447037935257, R2 0.47127801179885864\n",
      "Eval loss 0.01641741581261158, R2 0.4713226556777954\n",
      "epoch 1948, loss 0.01303298119455576, R2 0.47133904695510864\n",
      "Eval loss 0.016414815559983253, R2 0.4714065194129944\n",
      "epoch 1949, loss 0.013031494803726673, R2 0.47139912843704224\n",
      "Eval loss 0.016412220895290375, R2 0.47149014472961426\n",
      "epoch 1950, loss 0.013030009344220161, R2 0.4714595675468445\n",
      "Eval loss 0.016409628093242645, R2 0.47157377004623413\n",
      "epoch 1951, loss 0.013028528541326523, R2 0.4715196490287781\n",
      "Eval loss 0.016407035291194916, R2 0.4716570973396301\n",
      "epoch 1952, loss 0.01302704680711031, R2 0.47157931327819824\n",
      "Eval loss 0.016404449939727783, R2 0.471740186214447\n",
      "epoch 1953, loss 0.013025568798184395, R2 0.4716396927833557\n",
      "Eval loss 0.01640186458826065, R2 0.4718235731124878\n",
      "epoch 1954, loss 0.013024091720581055, R2 0.47169917821884155\n",
      "Eval loss 0.016399286687374115, R2 0.47190678119659424\n",
      "epoch 1955, loss 0.013022617436945438, R2 0.47175920009613037\n",
      "Eval loss 0.01639670878648758, R2 0.47198963165283203\n",
      "epoch 1956, loss 0.013021144084632397, R2 0.47181880474090576\n",
      "Eval loss 0.016394132748246193, R2 0.4720722436904907\n",
      "epoch 1957, loss 0.01301966980099678, R2 0.4718787670135498\n",
      "Eval loss 0.016391564160585403, R2 0.4721553325653076\n",
      "epoch 1958, loss 0.013018202967941761, R2 0.4719380736351013\n",
      "Eval loss 0.016388993710279465, R2 0.47223806381225586\n",
      "epoch 1959, loss 0.013016736134886742, R2 0.47199755907058716\n",
      "Eval loss 0.016386428847908974, R2 0.47232067584991455\n",
      "epoch 1960, loss 0.013015270233154297, R2 0.4720568060874939\n",
      "Eval loss 0.01638386771082878, R2 0.4724031686782837\n",
      "epoch 1961, loss 0.013013805262744427, R2 0.47211647033691406\n",
      "Eval loss 0.016381308436393738, R2 0.4724852442741394\n",
      "epoch 1962, loss 0.013012344017624855, R2 0.4721755385398865\n",
      "Eval loss 0.016378751024603844, R2 0.47256791591644287\n",
      "epoch 1963, loss 0.013010883703827858, R2 0.47223520278930664\n",
      "Eval loss 0.016376199200749397, R2 0.47265005111694336\n",
      "epoch 1964, loss 0.013009426183998585, R2 0.4722944498062134\n",
      "Eval loss 0.0163736492395401, R2 0.47273218631744385\n",
      "epoch 1965, loss 0.013007971458137035, R2 0.47235292196273804\n",
      "Eval loss 0.0163711030036211, R2 0.4728142023086548\n",
      "epoch 1966, loss 0.013006515800952911, R2 0.4724125862121582\n",
      "Eval loss 0.016368558630347252, R2 0.47289609909057617\n",
      "epoch 1967, loss 0.013005063869059086, R2 0.47247081995010376\n",
      "Eval loss 0.01636601611971855, R2 0.47297799587249756\n",
      "epoch 1968, loss 0.013003612868487835, R2 0.4725301265716553\n",
      "Eval loss 0.01636348105967045, R2 0.47305959463119507\n",
      "epoch 1969, loss 0.013002166524529457, R2 0.47258853912353516\n",
      "Eval loss 0.016360945999622345, R2 0.47314125299453735\n",
      "epoch 1970, loss 0.013000717386603355, R2 0.4726475477218628\n",
      "Eval loss 0.01635841466486454, R2 0.47322291135787964\n",
      "epoch 1971, loss 0.012999274767935276, R2 0.4727056622505188\n",
      "Eval loss 0.016355887055397034, R2 0.47330421209335327\n",
      "epoch 1972, loss 0.012997831217944622, R2 0.47276443243026733\n",
      "Eval loss 0.016353361308574677, R2 0.4733858108520508\n",
      "epoch 1973, loss 0.012996389530599117, R2 0.47282248735427856\n",
      "Eval loss 0.016350839287042618, R2 0.4734663963317871\n",
      "epoch 1974, loss 0.012994949705898762, R2 0.4728813171386719\n",
      "Eval loss 0.01634831912815571, R2 0.4735480546951294\n",
      "epoch 1975, loss 0.012993511743843555, R2 0.47294002771377563\n",
      "Eval loss 0.016345804557204247, R2 0.4736288785934448\n",
      "epoch 1976, loss 0.012992074713110924, R2 0.47299814224243164\n",
      "Eval loss 0.016343289986252785, R2 0.4737098217010498\n",
      "epoch 1977, loss 0.01299064140766859, R2 0.4730560779571533\n",
      "Eval loss 0.01634078100323677, R2 0.47379016876220703\n",
      "epoch 1978, loss 0.012989209964871407, R2 0.473114550113678\n",
      "Eval loss 0.016338272020220757, R2 0.47387075424194336\n",
      "epoch 1979, loss 0.012987779453396797, R2 0.473172128200531\n",
      "Eval loss 0.01633576676249504, R2 0.47395193576812744\n",
      "epoch 1980, loss 0.012986352667212486, R2 0.4732300043106079\n",
      "Eval loss 0.016333265230059624, R2 0.474032461643219\n",
      "epoch 1981, loss 0.01298492681235075, R2 0.4732876420021057\n",
      "Eval loss 0.016330769285559654, R2 0.4741125702857971\n",
      "epoch 1982, loss 0.012983500957489014, R2 0.47334569692611694\n",
      "Eval loss 0.016328273341059685, R2 0.47419291734695435\n",
      "epoch 1983, loss 0.012982076033949852, R2 0.4734034538269043\n",
      "Eval loss 0.016325779259204865, R2 0.4742738604545593\n",
      "epoch 1984, loss 0.012980653904378414, R2 0.4734611511230469\n",
      "Eval loss 0.016323290765285492, R2 0.4743538498878479\n",
      "epoch 1985, loss 0.01297923643141985, R2 0.4735186696052551\n",
      "Eval loss 0.016320805996656418, R2 0.47443389892578125\n",
      "epoch 1986, loss 0.012977820821106434, R2 0.4735761284828186\n",
      "Eval loss 0.016318323090672493, R2 0.4745136499404907\n",
      "epoch 1987, loss 0.012976404279470444, R2 0.4736335277557373\n",
      "Eval loss 0.016315842047333717, R2 0.4745936989784241\n",
      "epoch 1988, loss 0.012974990531802177, R2 0.473690927028656\n",
      "Eval loss 0.01631336286664009, R2 0.4746735095977783\n",
      "epoch 1989, loss 0.012973577715456486, R2 0.4737486243247986\n",
      "Eval loss 0.016310889273881912, R2 0.474753201007843\n",
      "epoch 1990, loss 0.012972166761755943, R2 0.47380584478378296\n",
      "Eval loss 0.016308417543768883, R2 0.4748329520225525\n",
      "epoch 1991, loss 0.01297075767070055, R2 0.47386282682418823\n",
      "Eval loss 0.01630594953894615, R2 0.47491180896759033\n",
      "epoch 1992, loss 0.012969352304935455, R2 0.47392046451568604\n",
      "Eval loss 0.01630348525941372, R2 0.47499150037765503\n",
      "epoch 1993, loss 0.012967947870492935, R2 0.47397661209106445\n",
      "Eval loss 0.016301020979881287, R2 0.4750710129737854\n",
      "epoch 1994, loss 0.01296654436737299, R2 0.47403329610824585\n",
      "Eval loss 0.01629856415092945, R2 0.47515010833740234\n",
      "epoch 1995, loss 0.012965142726898193, R2 0.474090576171875\n",
      "Eval loss 0.016296103596687317, R2 0.47522950172424316\n",
      "epoch 1996, loss 0.012963742949068546, R2 0.47414714097976685\n",
      "Eval loss 0.01629365235567093, R2 0.47530829906463623\n",
      "epoch 1997, loss 0.012962345965206623, R2 0.47420424222946167\n",
      "Eval loss 0.016291199252009392, R2 0.47538721561431885\n",
      "epoch 1998, loss 0.012960949912667274, R2 0.4742608666419983\n",
      "Eval loss 0.016288751736283302, R2 0.47546595335006714\n",
      "epoch 1999, loss 0.01295955665409565, R2 0.47431719303131104\n",
      "Eval loss 0.01628630794584751, R2 0.475544810295105\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    torch.manual_seed(epoch)\n",
    "    idx = torch.randperm(reward_train_x.shape[0])\n",
    "    reward_train_x = reward_train_x[idx, :]\n",
    "    reward_train_y = reward_train_y[idx, :]\n",
    "    idx = torch.randperm(test_x.shape[0])\n",
    "    reward_test_x = reward_test_x[idx, :]\n",
    "    reward_test_y = reward_test_y[idx, :]\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = reward_lr_model(reward_train_x)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs.squeeze(), reward_train_y.squeeze())\n",
    "    train_losses.append(loss.item())\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute metric\n",
    "    train_metric = metric(outputs.squeeze(), reward_train_y.squeeze())\n",
    "    train_metrics.append(train_metric)\n",
    "\n",
    "    print(\"epoch {}, loss {}, R2 {}\".format(epoch, loss.item(), train_metric))\n",
    "\n",
    "    if epoch % eval_epoch_freq == 0:\n",
    "        with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "            preds = reward_lr_model(reward_test_x)\n",
    "            test_loss = criterion(preds.squeeze(), reward_test_y.squeeze())\n",
    "            test_losses.append(test_loss.item())\n",
    "            # Compute metric\n",
    "            test_metric = metric(preds.squeeze(), reward_test_y.squeeze())\n",
    "            test_metrics.append(test_metric)\n",
    "            print(\"Eval loss {}, R2 {}\".format(test_loss.item(), test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch duration: 0.02s\n",
      "Epoch: 0 Train loss 0.215, Test loss 0.177 Train R2 -7.152, Test R2 -6.301 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 1 Train loss 0.176, Test loss 0.143 Train R2 -5.663, Test R2 -4.883 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 2 Train loss 0.142, Test loss 0.114 Train R2 -4.395, Test R2 -3.688 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 3 Train loss 0.114, Test loss 0.089 Train R2 -3.324, Test R2 -2.691 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 4 Train loss 0.091, Test loss 0.069 Train R2 -2.427, Test R2 -1.862 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 5 Train loss 0.071, Test loss 0.053 Train R2 -1.685, Test R2 -1.190 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 6 Train loss 0.055, Test loss 0.040 Train R2 -1.078, Test R2 -0.657 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 7 Train loss 0.042, Test loss 0.031 Train R2 -0.592, Test R2 -0.259 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 8 Train loss 0.032, Test loss 0.024 Train R2 -0.221, Test R2 0.009 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 9 Train loss 0.025, Test loss 0.020 Train R2 0.037, Test R2 0.156 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 10 Train loss 0.021, Test loss 0.020 Train R2 0.186, Test R2 0.188 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 11 Train loss 0.020, Test loss 0.021 Train R2 0.231, Test R2 0.125 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 12 Train loss 0.021, Test loss 0.024 Train R2 0.190, Test R2 0.004 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 13 Train loss 0.024, Test loss 0.027 Train R2 0.096, Test R2 -0.125 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 14 Train loss 0.027, Test loss 0.030 Train R2 -0.008, Test R2 -0.221 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 15 Train loss 0.029, Test loss 0.031 Train R2 -0.086, Test R2 -0.259 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 16 Train loss 0.029, Test loss 0.030 Train R2 -0.116, Test R2 -0.239 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 17 Train loss 0.029, Test loss 0.028 Train R2 -0.098, Test R2 -0.173 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 18 Train loss 0.027, Test loss 0.026 Train R2 -0.041, Test R2 -0.081 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 19 Train loss 0.025, Test loss 0.024 Train R2 0.037, Test R2 0.019 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 20 Train loss 0.023, Test loss 0.022 Train R2 0.121, Test R2 0.113 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 21 Train loss 0.021, Test loss 0.020 Train R2 0.199, Test R2 0.190 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 22 Train loss 0.019, Test loss 0.018 Train R2 0.263, Test R2 0.247 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 23 Train loss 0.018, Test loss 0.017 Train R2 0.310, Test R2 0.283 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 24 Train loss 0.017, Test loss 0.017 Train R2 0.340, Test R2 0.303 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 25 Train loss 0.017, Test loss 0.017 Train R2 0.355, Test R2 0.309 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 26 Train loss 0.017, Test loss 0.017 Train R2 0.359, Test R2 0.307 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 27 Train loss 0.017, Test loss 0.017 Train R2 0.356, Test R2 0.302 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 28 Train loss 0.017, Test loss 0.017 Train R2 0.349, Test R2 0.295 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 29 Train loss 0.017, Test loss 0.017 Train R2 0.343, Test R2 0.289 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 30 Train loss 0.017, Test loss 0.017 Train R2 0.339, Test R2 0.287 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 31 Train loss 0.017, Test loss 0.017 Train R2 0.339, Test R2 0.290 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 32 Train loss 0.017, Test loss 0.017 Train R2 0.343, Test R2 0.296 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 33 Train loss 0.017, Test loss 0.017 Train R2 0.352, Test R2 0.307 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 34 Train loss 0.017, Test loss 0.016 Train R2 0.364, Test R2 0.320 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 35 Train loss 0.016, Test loss 0.016 Train R2 0.379, Test R2 0.335 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 36 Train loss 0.016, Test loss 0.016 Train R2 0.395, Test R2 0.349 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 37 Train loss 0.016, Test loss 0.015 Train R2 0.411, Test R2 0.363 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 38 Train loss 0.015, Test loss 0.015 Train R2 0.427, Test R2 0.374 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 39 Train loss 0.015, Test loss 0.015 Train R2 0.440, Test R2 0.382 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 40 Train loss 0.015, Test loss 0.015 Train R2 0.450, Test R2 0.387 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 41 Train loss 0.014, Test loss 0.015 Train R2 0.457, Test R2 0.389 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 42 Train loss 0.014, Test loss 0.015 Train R2 0.462, Test R2 0.389 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 43 Train loss 0.014, Test loss 0.015 Train R2 0.464, Test R2 0.388 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 44 Train loss 0.014, Test loss 0.015 Train R2 0.465, Test R2 0.386 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 45 Train loss 0.014, Test loss 0.015 Train R2 0.466, Test R2 0.385 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 46 Train loss 0.014, Test loss 0.015 Train R2 0.468, Test R2 0.386 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 47 Train loss 0.014, Test loss 0.015 Train R2 0.471, Test R2 0.388 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 48 Train loss 0.014, Test loss 0.015 Train R2 0.476, Test R2 0.392 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 49 Train loss 0.014, Test loss 0.015 Train R2 0.482, Test R2 0.397 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 50 Train loss 0.013, Test loss 0.014 Train R2 0.489, Test R2 0.402 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 51 Train loss 0.013, Test loss 0.014 Train R2 0.497, Test R2 0.407 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 52 Train loss 0.013, Test loss 0.014 Train R2 0.505, Test R2 0.411 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 53 Train loss 0.013, Test loss 0.014 Train R2 0.511, Test R2 0.414 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 54 Train loss 0.013, Test loss 0.014 Train R2 0.517, Test R2 0.416 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 55 Train loss 0.013, Test loss 0.014 Train R2 0.522, Test R2 0.418 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 56 Train loss 0.013, Test loss 0.014 Train R2 0.525, Test R2 0.418 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 57 Train loss 0.012, Test loss 0.014 Train R2 0.529, Test R2 0.419 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 58 Train loss 0.012, Test loss 0.014 Train R2 0.532, Test R2 0.419 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 59 Train loss 0.012, Test loss 0.014 Train R2 0.535, Test R2 0.420 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 60 Train loss 0.012, Test loss 0.014 Train R2 0.538, Test R2 0.422 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 61 Train loss 0.012, Test loss 0.014 Train R2 0.543, Test R2 0.424 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 62 Train loss 0.012, Test loss 0.014 Train R2 0.547, Test R2 0.427 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 63 Train loss 0.012, Test loss 0.014 Train R2 0.552, Test R2 0.430 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 64 Train loss 0.012, Test loss 0.014 Train R2 0.557, Test R2 0.433 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 65 Train loss 0.012, Test loss 0.014 Train R2 0.562, Test R2 0.435 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 66 Train loss 0.011, Test loss 0.014 Train R2 0.566, Test R2 0.437 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 67 Train loss 0.011, Test loss 0.014 Train R2 0.570, Test R2 0.439 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 68 Train loss 0.011, Test loss 0.014 Train R2 0.574, Test R2 0.440 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 69 Train loss 0.011, Test loss 0.014 Train R2 0.577, Test R2 0.441 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 70 Train loss 0.011, Test loss 0.014 Train R2 0.581, Test R2 0.442 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 71 Train loss 0.011, Test loss 0.013 Train R2 0.584, Test R2 0.444 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 72 Train loss 0.011, Test loss 0.013 Train R2 0.588, Test R2 0.446 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 73 Train loss 0.011, Test loss 0.013 Train R2 0.591, Test R2 0.448 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 74 Train loss 0.011, Test loss 0.013 Train R2 0.595, Test R2 0.450 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 75 Train loss 0.011, Test loss 0.013 Train R2 0.599, Test R2 0.452 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 76 Train loss 0.010, Test loss 0.013 Train R2 0.603, Test R2 0.453 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 77 Train loss 0.010, Test loss 0.013 Train R2 0.607, Test R2 0.455 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 78 Train loss 0.010, Test loss 0.013 Train R2 0.610, Test R2 0.456 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 79 Train loss 0.010, Test loss 0.013 Train R2 0.613, Test R2 0.457 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 80 Train loss 0.010, Test loss 0.013 Train R2 0.617, Test R2 0.459 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 81 Train loss 0.010, Test loss 0.013 Train R2 0.620, Test R2 0.460 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 82 Train loss 0.010, Test loss 0.013 Train R2 0.623, Test R2 0.462 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 83 Train loss 0.010, Test loss 0.013 Train R2 0.627, Test R2 0.463 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 84 Train loss 0.010, Test loss 0.013 Train R2 0.630, Test R2 0.465 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 85 Train loss 0.010, Test loss 0.013 Train R2 0.634, Test R2 0.466 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 86 Train loss 0.010, Test loss 0.013 Train R2 0.637, Test R2 0.468 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 87 Train loss 0.009, Test loss 0.013 Train R2 0.640, Test R2 0.469 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 88 Train loss 0.009, Test loss 0.013 Train R2 0.644, Test R2 0.471 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 89 Train loss 0.009, Test loss 0.013 Train R2 0.647, Test R2 0.473 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 90 Train loss 0.009, Test loss 0.013 Train R2 0.650, Test R2 0.474 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 91 Train loss 0.009, Test loss 0.013 Train R2 0.653, Test R2 0.476 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 92 Train loss 0.009, Test loss 0.013 Train R2 0.656, Test R2 0.478 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 93 Train loss 0.009, Test loss 0.013 Train R2 0.660, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 94 Train loss 0.009, Test loss 0.013 Train R2 0.663, Test R2 0.482 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 95 Train loss 0.009, Test loss 0.013 Train R2 0.666, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 96 Train loss 0.009, Test loss 0.012 Train R2 0.669, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 97 Train loss 0.009, Test loss 0.012 Train R2 0.672, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 98 Train loss 0.009, Test loss 0.012 Train R2 0.675, Test R2 0.489 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 99 Train loss 0.008, Test loss 0.012 Train R2 0.678, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 100 Train loss 0.008, Test loss 0.012 Train R2 0.682, Test R2 0.492 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 101 Train loss 0.008, Test loss 0.012 Train R2 0.685, Test R2 0.494 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 102 Train loss 0.008, Test loss 0.012 Train R2 0.688, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 103 Train loss 0.008, Test loss 0.012 Train R2 0.691, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 104 Train loss 0.008, Test loss 0.012 Train R2 0.694, Test R2 0.499 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 105 Train loss 0.008, Test loss 0.012 Train R2 0.697, Test R2 0.501 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 106 Train loss 0.008, Test loss 0.012 Train R2 0.700, Test R2 0.503 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 107 Train loss 0.008, Test loss 0.012 Train R2 0.703, Test R2 0.505 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 108 Train loss 0.008, Test loss 0.012 Train R2 0.706, Test R2 0.506 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 109 Train loss 0.008, Test loss 0.012 Train R2 0.709, Test R2 0.508 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 110 Train loss 0.008, Test loss 0.012 Train R2 0.712, Test R2 0.509 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 111 Train loss 0.008, Test loss 0.012 Train R2 0.715, Test R2 0.511 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 112 Train loss 0.007, Test loss 0.012 Train R2 0.718, Test R2 0.512 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 113 Train loss 0.007, Test loss 0.012 Train R2 0.721, Test R2 0.513 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 114 Train loss 0.007, Test loss 0.012 Train R2 0.724, Test R2 0.514 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 115 Train loss 0.007, Test loss 0.012 Train R2 0.727, Test R2 0.515 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 116 Train loss 0.007, Test loss 0.012 Train R2 0.730, Test R2 0.516 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 117 Train loss 0.007, Test loss 0.012 Train R2 0.733, Test R2 0.518 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 118 Train loss 0.007, Test loss 0.012 Train R2 0.736, Test R2 0.518 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 119 Train loss 0.007, Test loss 0.012 Train R2 0.739, Test R2 0.519 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 120 Train loss 0.007, Test loss 0.012 Train R2 0.742, Test R2 0.520 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 121 Train loss 0.007, Test loss 0.012 Train R2 0.745, Test R2 0.521 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 122 Train loss 0.007, Test loss 0.012 Train R2 0.748, Test R2 0.521 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 123 Train loss 0.007, Test loss 0.012 Train R2 0.750, Test R2 0.522 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 124 Train loss 0.007, Test loss 0.012 Train R2 0.753, Test R2 0.522 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 125 Train loss 0.006, Test loss 0.012 Train R2 0.756, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 126 Train loss 0.006, Test loss 0.012 Train R2 0.759, Test R2 0.523 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 127 Train loss 0.006, Test loss 0.012 Train R2 0.762, Test R2 0.523 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 128 Train loss 0.006, Test loss 0.012 Train R2 0.765, Test R2 0.523 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 129 Train loss 0.006, Test loss 0.012 Train R2 0.767, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 130 Train loss 0.006, Test loss 0.012 Train R2 0.770, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 131 Train loss 0.006, Test loss 0.012 Train R2 0.773, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 132 Train loss 0.006, Test loss 0.012 Train R2 0.776, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 133 Train loss 0.006, Test loss 0.012 Train R2 0.778, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 134 Train loss 0.006, Test loss 0.012 Train R2 0.781, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 135 Train loss 0.006, Test loss 0.012 Train R2 0.784, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 136 Train loss 0.006, Test loss 0.012 Train R2 0.786, Test R2 0.524 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 137 Train loss 0.006, Test loss 0.012 Train R2 0.789, Test R2 0.524 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 138 Train loss 0.006, Test loss 0.012 Train R2 0.792, Test R2 0.524 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 139 Train loss 0.005, Test loss 0.012 Train R2 0.794, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 140 Train loss 0.005, Test loss 0.012 Train R2 0.797, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 141 Train loss 0.005, Test loss 0.012 Train R2 0.800, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 142 Train loss 0.005, Test loss 0.012 Train R2 0.802, Test R2 0.523 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 143 Train loss 0.005, Test loss 0.012 Train R2 0.805, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 144 Train loss 0.005, Test loss 0.012 Train R2 0.807, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 145 Train loss 0.005, Test loss 0.012 Train R2 0.810, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 146 Train loss 0.005, Test loss 0.012 Train R2 0.812, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 147 Train loss 0.005, Test loss 0.012 Train R2 0.815, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 148 Train loss 0.005, Test loss 0.012 Train R2 0.817, Test R2 0.522 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 149 Train loss 0.005, Test loss 0.012 Train R2 0.819, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 150 Train loss 0.005, Test loss 0.012 Train R2 0.822, Test R2 0.522 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 151 Train loss 0.005, Test loss 0.012 Train R2 0.824, Test R2 0.521 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 152 Train loss 0.005, Test loss 0.012 Train R2 0.827, Test R2 0.521 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 153 Train loss 0.005, Test loss 0.012 Train R2 0.829, Test R2 0.521 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 154 Train loss 0.004, Test loss 0.012 Train R2 0.831, Test R2 0.521 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 155 Train loss 0.004, Test loss 0.012 Train R2 0.833, Test R2 0.520 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 156 Train loss 0.004, Test loss 0.012 Train R2 0.836, Test R2 0.520 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 157 Train loss 0.004, Test loss 0.012 Train R2 0.838, Test R2 0.520 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 158 Train loss 0.004, Test loss 0.012 Train R2 0.840, Test R2 0.519 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 159 Train loss 0.004, Test loss 0.012 Train R2 0.842, Test R2 0.519 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 160 Train loss 0.004, Test loss 0.012 Train R2 0.845, Test R2 0.518 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 161 Train loss 0.004, Test loss 0.012 Train R2 0.847, Test R2 0.518 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 162 Train loss 0.004, Test loss 0.012 Train R2 0.849, Test R2 0.517 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 163 Train loss 0.004, Test loss 0.012 Train R2 0.851, Test R2 0.517 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 164 Train loss 0.004, Test loss 0.012 Train R2 0.853, Test R2 0.516 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 165 Train loss 0.004, Test loss 0.012 Train R2 0.855, Test R2 0.516 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 166 Train loss 0.004, Test loss 0.012 Train R2 0.857, Test R2 0.515 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 167 Train loss 0.004, Test loss 0.012 Train R2 0.859, Test R2 0.515 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 168 Train loss 0.004, Test loss 0.012 Train R2 0.861, Test R2 0.514 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 169 Train loss 0.004, Test loss 0.012 Train R2 0.863, Test R2 0.514 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 170 Train loss 0.004, Test loss 0.012 Train R2 0.865, Test R2 0.513 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 171 Train loss 0.004, Test loss 0.012 Train R2 0.867, Test R2 0.513 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 172 Train loss 0.003, Test loss 0.012 Train R2 0.869, Test R2 0.513 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 173 Train loss 0.003, Test loss 0.012 Train R2 0.871, Test R2 0.512 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 174 Train loss 0.003, Test loss 0.012 Train R2 0.873, Test R2 0.512 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 175 Train loss 0.003, Test loss 0.012 Train R2 0.875, Test R2 0.511 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 176 Train loss 0.003, Test loss 0.012 Train R2 0.876, Test R2 0.511 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 177 Train loss 0.003, Test loss 0.012 Train R2 0.878, Test R2 0.510 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 178 Train loss 0.003, Test loss 0.012 Train R2 0.880, Test R2 0.510 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 179 Train loss 0.003, Test loss 0.012 Train R2 0.882, Test R2 0.509 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 180 Train loss 0.003, Test loss 0.012 Train R2 0.884, Test R2 0.509 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 181 Train loss 0.003, Test loss 0.012 Train R2 0.885, Test R2 0.508 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 182 Train loss 0.003, Test loss 0.012 Train R2 0.887, Test R2 0.508 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 183 Train loss 0.003, Test loss 0.012 Train R2 0.889, Test R2 0.507 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 184 Train loss 0.003, Test loss 0.012 Train R2 0.891, Test R2 0.507 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 185 Train loss 0.003, Test loss 0.012 Train R2 0.892, Test R2 0.506 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 186 Train loss 0.003, Test loss 0.012 Train R2 0.894, Test R2 0.506 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 187 Train loss 0.003, Test loss 0.012 Train R2 0.896, Test R2 0.505 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 188 Train loss 0.003, Test loss 0.012 Train R2 0.897, Test R2 0.505 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 189 Train loss 0.003, Test loss 0.012 Train R2 0.899, Test R2 0.504 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 190 Train loss 0.003, Test loss 0.012 Train R2 0.900, Test R2 0.504 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 191 Train loss 0.003, Test loss 0.012 Train R2 0.902, Test R2 0.503 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 192 Train loss 0.003, Test loss 0.012 Train R2 0.903, Test R2 0.503 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 193 Train loss 0.003, Test loss 0.012 Train R2 0.905, Test R2 0.503 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 194 Train loss 0.002, Test loss 0.012 Train R2 0.906, Test R2 0.502 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 195 Train loss 0.002, Test loss 0.012 Train R2 0.908, Test R2 0.502 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 196 Train loss 0.002, Test loss 0.012 Train R2 0.909, Test R2 0.502 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 197 Train loss 0.002, Test loss 0.012 Train R2 0.911, Test R2 0.501 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 198 Train loss 0.002, Test loss 0.012 Train R2 0.912, Test R2 0.501 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 199 Train loss 0.002, Test loss 0.012 Train R2 0.914, Test R2 0.501 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 200 Train loss 0.002, Test loss 0.012 Train R2 0.915, Test R2 0.501 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 201 Train loss 0.002, Test loss 0.012 Train R2 0.917, Test R2 0.500 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 202 Train loss 0.002, Test loss 0.012 Train R2 0.918, Test R2 0.500 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 203 Train loss 0.002, Test loss 0.012 Train R2 0.919, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 204 Train loss 0.002, Test loss 0.012 Train R2 0.921, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 205 Train loss 0.002, Test loss 0.012 Train R2 0.922, Test R2 0.500 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 206 Train loss 0.002, Test loss 0.012 Train R2 0.923, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 207 Train loss 0.002, Test loss 0.012 Train R2 0.924, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 208 Train loss 0.002, Test loss 0.012 Train R2 0.926, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 209 Train loss 0.002, Test loss 0.012 Train R2 0.927, Test R2 0.500 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 210 Train loss 0.002, Test loss 0.012 Train R2 0.928, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 211 Train loss 0.002, Test loss 0.012 Train R2 0.929, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 212 Train loss 0.002, Test loss 0.012 Train R2 0.930, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 213 Train loss 0.002, Test loss 0.012 Train R2 0.932, Test R2 0.500 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 214 Train loss 0.002, Test loss 0.012 Train R2 0.933, Test R2 0.499 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 215 Train loss 0.002, Test loss 0.012 Train R2 0.934, Test R2 0.499 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 216 Train loss 0.002, Test loss 0.012 Train R2 0.935, Test R2 0.499 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 217 Train loss 0.002, Test loss 0.012 Train R2 0.936, Test R2 0.499 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 218 Train loss 0.002, Test loss 0.012 Train R2 0.937, Test R2 0.499 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 219 Train loss 0.002, Test loss 0.012 Train R2 0.938, Test R2 0.499 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 220 Train loss 0.002, Test loss 0.012 Train R2 0.939, Test R2 0.499 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 221 Train loss 0.002, Test loss 0.012 Train R2 0.940, Test R2 0.498 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 222 Train loss 0.002, Test loss 0.012 Train R2 0.941, Test R2 0.498 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 223 Train loss 0.002, Test loss 0.012 Train R2 0.942, Test R2 0.498 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 224 Train loss 0.001, Test loss 0.012 Train R2 0.943, Test R2 0.498 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 225 Train loss 0.001, Test loss 0.012 Train R2 0.944, Test R2 0.498 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 226 Train loss 0.001, Test loss 0.012 Train R2 0.945, Test R2 0.497 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 227 Train loss 0.001, Test loss 0.012 Train R2 0.946, Test R2 0.497 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 228 Train loss 0.001, Test loss 0.012 Train R2 0.947, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 229 Train loss 0.001, Test loss 0.012 Train R2 0.948, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 230 Train loss 0.001, Test loss 0.012 Train R2 0.949, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 231 Train loss 0.001, Test loss 0.012 Train R2 0.950, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 232 Train loss 0.001, Test loss 0.012 Train R2 0.951, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 233 Train loss 0.001, Test loss 0.012 Train R2 0.952, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 234 Train loss 0.001, Test loss 0.012 Train R2 0.953, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 235 Train loss 0.001, Test loss 0.012 Train R2 0.953, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 236 Train loss 0.001, Test loss 0.012 Train R2 0.954, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 237 Train loss 0.001, Test loss 0.012 Train R2 0.955, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 238 Train loss 0.001, Test loss 0.012 Train R2 0.956, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 239 Train loss 0.001, Test loss 0.012 Train R2 0.957, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 240 Train loss 0.001, Test loss 0.012 Train R2 0.957, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 241 Train loss 0.001, Test loss 0.012 Train R2 0.958, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 242 Train loss 0.001, Test loss 0.012 Train R2 0.959, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 243 Train loss 0.001, Test loss 0.012 Train R2 0.960, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 244 Train loss 0.001, Test loss 0.012 Train R2 0.960, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 245 Train loss 0.001, Test loss 0.012 Train R2 0.961, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 246 Train loss 0.001, Test loss 0.012 Train R2 0.962, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 247 Train loss 0.001, Test loss 0.012 Train R2 0.963, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 248 Train loss 0.001, Test loss 0.012 Train R2 0.963, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 249 Train loss 0.001, Test loss 0.012 Train R2 0.964, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 250 Train loss 0.001, Test loss 0.012 Train R2 0.965, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 251 Train loss 0.001, Test loss 0.012 Train R2 0.965, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 252 Train loss 0.001, Test loss 0.012 Train R2 0.966, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 253 Train loss 0.001, Test loss 0.012 Train R2 0.967, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 254 Train loss 0.001, Test loss 0.012 Train R2 0.967, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 255 Train loss 0.001, Test loss 0.012 Train R2 0.968, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 256 Train loss 0.001, Test loss 0.012 Train R2 0.968, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 257 Train loss 0.001, Test loss 0.012 Train R2 0.969, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 258 Train loss 0.001, Test loss 0.012 Train R2 0.970, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 259 Train loss 0.001, Test loss 0.012 Train R2 0.970, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 260 Train loss 0.001, Test loss 0.012 Train R2 0.971, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 261 Train loss 0.001, Test loss 0.012 Train R2 0.971, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 262 Train loss 0.001, Test loss 0.012 Train R2 0.972, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 263 Train loss 0.001, Test loss 0.012 Train R2 0.972, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 264 Train loss 0.001, Test loss 0.012 Train R2 0.973, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 265 Train loss 0.001, Test loss 0.012 Train R2 0.973, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 266 Train loss 0.001, Test loss 0.012 Train R2 0.974, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 267 Train loss 0.001, Test loss 0.012 Train R2 0.974, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 268 Train loss 0.001, Test loss 0.012 Train R2 0.975, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 269 Train loss 0.001, Test loss 0.012 Train R2 0.975, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 270 Train loss 0.001, Test loss 0.012 Train R2 0.976, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 271 Train loss 0.001, Test loss 0.012 Train R2 0.976, Test R2 0.497 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 272 Train loss 0.001, Test loss 0.012 Train R2 0.977, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 273 Train loss 0.001, Test loss 0.012 Train R2 0.977, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 274 Train loss 0.001, Test loss 0.012 Train R2 0.978, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 275 Train loss 0.001, Test loss 0.012 Train R2 0.978, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 276 Train loss 0.001, Test loss 0.012 Train R2 0.978, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 277 Train loss 0.001, Test loss 0.012 Train R2 0.979, Test R2 0.497 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 278 Train loss 0.001, Test loss 0.012 Train R2 0.979, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 279 Train loss 0.001, Test loss 0.012 Train R2 0.980, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 280 Train loss 0.001, Test loss 0.012 Train R2 0.980, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 281 Train loss 0.001, Test loss 0.012 Train R2 0.980, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 282 Train loss 0.001, Test loss 0.012 Train R2 0.981, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 283 Train loss 0.000, Test loss 0.012 Train R2 0.981, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 284 Train loss 0.000, Test loss 0.012 Train R2 0.982, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 285 Train loss 0.000, Test loss 0.012 Train R2 0.982, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 286 Train loss 0.000, Test loss 0.012 Train R2 0.982, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 287 Train loss 0.000, Test loss 0.012 Train R2 0.983, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 288 Train loss 0.000, Test loss 0.012 Train R2 0.983, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 289 Train loss 0.000, Test loss 0.012 Train R2 0.983, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 290 Train loss 0.000, Test loss 0.012 Train R2 0.984, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 291 Train loss 0.000, Test loss 0.012 Train R2 0.984, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 292 Train loss 0.000, Test loss 0.012 Train R2 0.984, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 293 Train loss 0.000, Test loss 0.012 Train R2 0.985, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 294 Train loss 0.000, Test loss 0.012 Train R2 0.985, Test R2 0.495 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 295 Train loss 0.000, Test loss 0.012 Train R2 0.985, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 296 Train loss 0.000, Test loss 0.012 Train R2 0.985, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 297 Train loss 0.000, Test loss 0.012 Train R2 0.986, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 298 Train loss 0.000, Test loss 0.012 Train R2 0.986, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 299 Train loss 0.000, Test loss 0.012 Train R2 0.986, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 300 Train loss 0.000, Test loss 0.012 Train R2 0.987, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 301 Train loss 0.000, Test loss 0.012 Train R2 0.987, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 302 Train loss 0.000, Test loss 0.012 Train R2 0.987, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 303 Train loss 0.000, Test loss 0.012 Train R2 0.987, Test R2 0.496 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 304 Train loss 0.000, Test loss 0.012 Train R2 0.988, Test R2 0.496 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 305 Train loss 0.000, Test loss 0.012 Train R2 0.988, Test R2 0.496 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 306 Train loss 0.000, Test loss 0.012 Train R2 0.988, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 307 Train loss 0.000, Test loss 0.012 Train R2 0.988, Test R2 0.496 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 308 Train loss 0.000, Test loss 0.012 Train R2 0.989, Test R2 0.496 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 309 Train loss 0.000, Test loss 0.012 Train R2 0.989, Test R2 0.496 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 310 Train loss 0.000, Test loss 0.012 Train R2 0.989, Test R2 0.496 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 311 Train loss 0.000, Test loss 0.012 Train R2 0.989, Test R2 0.495 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 312 Train loss 0.000, Test loss 0.012 Train R2 0.989, Test R2 0.495 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 313 Train loss 0.000, Test loss 0.012 Train R2 0.990, Test R2 0.496 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 314 Train loss 0.000, Test loss 0.012 Train R2 0.990, Test R2 0.496 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 315 Train loss 0.000, Test loss 0.012 Train R2 0.990, Test R2 0.496 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 316 Train loss 0.000, Test loss 0.012 Train R2 0.990, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 317 Train loss 0.000, Test loss 0.012 Train R2 0.990, Test R2 0.495 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 318 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 319 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 320 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 321 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 322 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 323 Train loss 0.000, Test loss 0.012 Train R2 0.991, Test R2 0.495 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 324 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 325 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 326 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 327 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 328 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 329 Train loss 0.000, Test loss 0.012 Train R2 0.992, Test R2 0.495 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 330 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 331 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 332 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 333 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 334 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 335 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 336 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 337 Train loss 0.000, Test loss 0.012 Train R2 0.993, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 338 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.495 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 339 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 340 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 341 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 342 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 343 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.07s\n",
      "Epoch: 344 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 345 Train loss 0.000, Test loss 0.012 Train R2 0.994, Test R2 0.494 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 346 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.494 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 347 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.494 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 348 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.494 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 349 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.494 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 350 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 351 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 352 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 353 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 354 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 355 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 356 Train loss 0.000, Test loss 0.012 Train R2 0.995, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 357 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.493 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 358 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 359 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 360 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 361 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 362 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 363 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 364 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 365 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 366 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.492 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 367 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 368 Train loss 0.000, Test loss 0.012 Train R2 0.996, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 369 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.01s\n",
      "Epoch: 370 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 371 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 372 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 373 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 374 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 375 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 376 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.491 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 377 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 378 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 379 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 380 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 381 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 382 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 383 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.490 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 384 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.489 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 385 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.489 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 386 Train loss 0.000, Test loss 0.012 Train R2 0.997, Test R2 0.489 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 387 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.489 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 388 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.489 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 389 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.489 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 390 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.489 \n",
      "Training epoch duration: 0.18s\n",
      "Epoch: 391 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.489 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 392 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 393 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 394 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 395 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 396 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 397 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.488 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 398 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 399 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 400 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 401 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 402 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 403 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 404 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 405 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 406 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.487 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 407 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 408 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 409 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 410 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 411 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.08s\n",
      "Epoch: 412 Train loss 0.000, Test loss 0.012 Train R2 0.998, Test R2 0.486 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 413 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.486 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 414 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.486 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 415 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.486 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 416 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.486 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 417 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 418 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 419 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.12s\n",
      "Epoch: 420 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 421 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 422 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 423 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 424 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 425 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.485 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 426 Train loss 0.000, Test loss 0.012 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 427 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 428 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 429 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 430 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 431 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 432 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 433 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.484 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 434 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 435 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 436 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 437 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 438 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 439 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 440 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 441 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 442 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 443 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 444 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.483 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 445 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 446 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 447 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 448 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 449 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 450 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 451 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 452 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 453 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 454 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 455 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.482 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 456 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 457 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 458 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 459 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 460 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.09s\n",
      "Epoch: 461 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.06s\n",
      "Epoch: 462 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.05s\n",
      "Epoch: 463 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 464 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 465 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.481 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 466 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 467 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 468 Train loss 0.000, Test loss 0.013 Train R2 0.999, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 469 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 470 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 471 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 472 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 473 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 474 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 475 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 476 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 477 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 478 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 479 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.480 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 480 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 481 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 482 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 483 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.04s\n",
      "Epoch: 484 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 485 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 486 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 487 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 488 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 489 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 490 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 491 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 492 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 493 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 494 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.03s\n",
      "Epoch: 495 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.479 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 496 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.478 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 497 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.478 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 498 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.478 \n",
      "Training epoch duration: 0.02s\n",
      "Epoch: 499 Train loss 0.000, Test loss 0.013 Train R2 1.000, Test R2 0.478 \n"
     ]
    }
   ],
   "source": [
    "# TEST with model trainer\n",
    "import omegaconf\n",
    "\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.util.model_trainer import ModelTrainerOverriden, MultiModelsTrainer\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "import mbrl.util.common\n",
    "\n",
    "# WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "# Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5,  # 10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\",  # sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.0,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    # \"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None,  # src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "# Params\n",
    "seed = 1\n",
    "device = \"cpu\"\n",
    "target_is_delta = False #Falses\n",
    "normalize = False\n",
    "use_double_dtype = False  # True\n",
    "optim_lr = 0.0001 #learningRate\n",
    "model_wd = 0.000001\n",
    "model_batch_size = dataset_size\n",
    "validation_ratio = test_split_ratio\n",
    "num_epochs = 500 #epochs\n",
    "\n",
    "# Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(env_config, render_mode=None)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# Seed\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# torch_generator = torch.Generator(device=device)\n",
    "# if seed is not None:\n",
    "#     torch_generator.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dynamics model\n",
    "from src.model.simple import Simple, FactoredSimple\n",
    "from src.model.gaussian_process import MultiOutputGP\n",
    "from src.util.util import get_base_dir_path\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    Simple(44, 1, device), #factors, num_layers=2, hid_size=100),\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=[],\n",
    "    model_output_key=[],\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=True,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "# Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "# Load replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    1000,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    # max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=1000)\n",
    "\n",
    "dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    model_batch_size,\n",
    "    validation_ratio,\n",
    "    ensemble_size=len(dynamics_model),\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,\n",
    ")\n",
    "\n",
    "if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "    dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "train_losses, test_losses, train_metrics, test_metrics = model_trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=num_epochs,\n",
    "    evaluate=True,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(train_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(test_losses, \"g\", label=\"Test loss\")\n",
    "ax[1].plot(train_metrics, \"b\", label=\"Train R2\")\n",
    "ax[1].plot(test_metrics, \"g\", label=\"Test R2\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].set_xlabel(\"train epoch\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():  # we don't need gradients in the testing phase\n",
    "    train_preds = dynamics_model.model(reward_train_x)\n",
    "    test_preds = dynamics_model.model(reward_test_x)\n",
    "\n",
    "relative_train_preds = train_preds - reward_train_y\n",
    "relative_test_preds = test_preds - reward_test_y\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Train bar plots\n",
    "n_bins = 100\n",
    "counts, bins = np.histogram(relative_train_preds, bins=n_bins)\n",
    "ax[0].hist(bins[:-1], bins, weights=counts)\n",
    "ax[0].title.set_text(\"Training set\")\n",
    "\n",
    "# Test bar plots\n",
    "n_bins = 100\n",
    "counts, bins = np.histogram(relative_test_preds, bins=n_bins)\n",
    "ax[1].hist(bins[:-1], bins, weights=counts)\n",
    "ax[1].title.set_text(\"Test set\")\n",
    "\n",
    "fig.suptitle(\"preds-true value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MBRL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "\n",
    "# WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "# Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5,  # 10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\",  # sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.0,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    # \"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None,  # src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "# Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 10\n",
    "num_steps = num_episodes * env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False  # True\n",
    "num_particles = 20\n",
    "\n",
    "# Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(env_config, render_mode=None)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "# Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "# Dynamics model\n",
    "model = lr_model\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "# Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "# Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "\n",
    "\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "\n",
    "\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL loop (with pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1, reward: 2.573672088377971.\n",
      "Trial: 2, reward: 2.705402098585997.\n",
      "Trial: 3, reward: 2.322563968184047.\n",
      "Trial: 4, reward: 2.677491420881591.\n",
      "Trial: 5, reward: 2.8242376956986925.\n",
      "Trial: 6, reward: 3.101734248162826.\n",
      "Trial: 7, reward: 1.843696885892893.\n",
      "Trial: 8, reward: 1.903616935018447.\n",
      "Trial: 9, reward: 2.8298462815498127.\n",
      "Trial: 10, reward: 2.7504850048228.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "\n",
    "while current_trial < num_episodes:\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "        # --- Doing env step using the agent ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "\n",
    "        # print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative Reward')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sum(rewards, axis=-1))\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Cumulative Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training MBRL agent with a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import omegaconf\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models\n",
    "import mbrl.planning\n",
    "from mbrl.models.model import Model\n",
    "from mbrl.util.logger import Logger\n",
    "import mbrl.util.common\n",
    "import mbrl.models\n",
    "\n",
    "from src.util.replay_buffer import ReplayBufferOverriden\n",
    "from src.util.model_trainer import ModelTrainer, ModelTrainerOverriden\n",
    "from src.env.bikes import Bikes\n",
    "from src.model.dict_model_wrapper import OneDTransitionRewardModelDictSpace\n",
    "from src.model.simple import Simple\n",
    "\n",
    "# WARNING: Make sure that it makes sense with trained model above\n",
    "\n",
    "# Env config\n",
    "env_config = {\n",
    "    \"num_trucks\": 5,  # 10\n",
    "    \"action_per_day\": 8,\n",
    "    \"next_day_method\": \"random\",  # sequential\n",
    "    \"initial_distribution\": \"zeros\",\n",
    "    \"bikes_per_truck\": 5,\n",
    "    \"start_walk_dist_max\": 0.2,\n",
    "    \"end_walk_dist_max\": 1000.0,\n",
    "    \"trip_duration\": 0.5,\n",
    "    \"past_trip_data\": \"src/env/bikes_data/all_trips_LouVelo_merged.csv\",\n",
    "    \"weather_data\": \"src/env/bikes_data/weather_data.csv\",\n",
    "    \"centroids_coord\": \"src/env/bikes_data/LouVelo_centroids_coords.npy\",\n",
    "    # \"centroids_idx\": #[93,83,80,98,6,84,87,99,56,97], #Number of centroids or list of indices\n",
    "    \"station_dependencies\": None,  # src/env/bikes_data/factors_radius_3.npy\n",
    "}\n",
    "\n",
    "# Params\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "num_episodes = 20\n",
    "num_steps = num_episodes * env_config[\"action_per_day\"]\n",
    "initial_exploration_steps = 500\n",
    "target_is_delta = False\n",
    "normalize = False\n",
    "use_double_dtype = False  # True\n",
    "num_particles = 20\n",
    "optim_lr = 0.1\n",
    "model_wd = 0\n",
    "freq_train_model = 10\n",
    "model_batch_size = num_steps + initial_exploration_steps  # Make sense for LR or GP\n",
    "validation_ratio = 0\n",
    "num_epochs = 25\n",
    "\n",
    "# Agent config\n",
    "optimizer_cfg = {\n",
    "    \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "    \"num_iterations\": 5,\n",
    "    \"elite_ratio\": 0.1,\n",
    "    \"population_size\": 350,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lower_bound\": None,\n",
    "    \"upper_bound\": None,\n",
    "    \"return_mean_elites\": True,\n",
    "    \"device\": device,\n",
    "    \"clipped_normal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Env\n",
    "env_config = omegaconf.DictConfig(env_config)\n",
    "env = Bikes(env_config, render_mode=None)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "print(obs_shape, act_shape)\n",
    "\n",
    "# Seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "torch_generator = torch.Generator(device=device)\n",
    "if seed is not None:\n",
    "    torch_generator.manual_seed(seed)\n",
    "\n",
    "# Dynamics model\n",
    "model = Simple(in_size, out_size, device)\n",
    "\n",
    "dynamics_model = OneDTransitionRewardModelDictSpace(\n",
    "    model,\n",
    "    map_obs=env.map_obs,\n",
    "    map_act=env.map_act,\n",
    "    rescale_obs=env.rescale_obs,\n",
    "    rescale_act=env.rescale_act,\n",
    "    model_input_obs_key=input_obs_keys,\n",
    "    model_input_act_key=input_act_keys,\n",
    "    model_output_key=output_keys,\n",
    "    obs_preprocess_fn=env.obs_preprocess_fn,\n",
    "    obs_postprocess_fn=env.obs_postprocess_fn,\n",
    "    target_is_delta=target_is_delta,\n",
    "    normalize=normalize,\n",
    "    normalize_double_precision=use_double_dtype,\n",
    "    learned_rewards=learned_rewards,\n",
    "    no_delta_list=None,\n",
    "    num_elites=None,\n",
    ")\n",
    "\n",
    "# Model Env\n",
    "model_env = mbrl.models.ModelEnv(\n",
    "    env, dynamics_model, env.termination_fn, None, generator=torch_generator\n",
    ")\n",
    "\n",
    "# Agent\n",
    "optimizer_cfg = omegaconf.DictConfig(optimizer_cfg)\n",
    "agent = mbrl.planning.TrajectoryOptimizerAgent(\n",
    "    optimizer_cfg=optimizer_cfg,\n",
    "    action_lb=env.action_space.low,\n",
    "    action_ub=env.action_space.high,\n",
    "    planning_horizon=env.action_per_day,\n",
    "    replan_freq=1,\n",
    ")\n",
    "\n",
    "\n",
    "def trajectory_eval_fn(initial_state, action_sequences):\n",
    "    return model_env.evaluate_action_sequences(\n",
    "        action_sequences, initial_state=initial_state, num_particles=num_particles\n",
    "    )\n",
    "\n",
    "\n",
    "agent.set_trajectory_eval_fn(trajectory_eval_fn)\n",
    "\n",
    "# Model trainer\n",
    "model_trainer = ModelTrainerOverriden(\n",
    "    model=dynamics_model,\n",
    "    optim_lr=optim_lr,\n",
    "    weight_decay=model_wd,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "\n",
    "# Replay Buffer\n",
    "dtype = np.double if use_double_dtype else np.float32\n",
    "replay_buffer = ReplayBufferOverriden(\n",
    "    num_steps + initial_exploration_steps,\n",
    "    obs_shape,\n",
    "    act_shape,\n",
    "    obs_type=dtype,\n",
    "    action_type=dtype,\n",
    "    reward_type=dtype,\n",
    "    rng=rng,\n",
    "    # max_trajectory_length=None,\n",
    ")\n",
    "\n",
    "load_dir = pathlib.Path(load_dir)\n",
    "replay_buffer.load(str(load_dir), num_to_store=initial_exploration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training MBRL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500\n",
      "Epoch: 0 Train loss 5.370, Test loss 864915072.000\n",
      "1 500\n",
      "Epoch: 1 Train loss 864915200.000, Test loss 281.795\n",
      "1 500\n",
      "Epoch: 2 Train loss 281.795, Test loss 5.353\n",
      "1 500\n",
      "Epoch: 3 Train loss 5.353, Test loss 5.373\n",
      "1 500\n",
      "Epoch: 4 Train loss 5.373, Test loss 5.311\n",
      "1 500\n",
      "Epoch: 5 Train loss 5.311, Test loss 5.156\n",
      "1 500\n",
      "Epoch: 6 Train loss 5.156, Test loss 4.905\n",
      "1 500\n",
      "Epoch: 7 Train loss 4.905, Test loss 4.582\n",
      "1 500\n",
      "Epoch: 8 Train loss 4.582, Test loss 4.256\n",
      "1 500\n",
      "Epoch: 9 Train loss 4.256, Test loss 4.038\n",
      "1 500\n",
      "Epoch: 10 Train loss 4.038, Test loss 4.029\n",
      "1 500\n",
      "Epoch: 11 Train loss 4.029, Test loss 4.146\n",
      "1 500\n",
      "Epoch: 12 Train loss 4.146, Test loss 4.144\n",
      "1 500\n",
      "Epoch: 13 Train loss 4.144, Test loss 4.023\n",
      "1 500\n",
      "Epoch: 14 Train loss 4.023, Test loss 3.897\n",
      "1 500\n",
      "Epoch: 15 Train loss 3.897, Test loss 3.841\n",
      "1 500\n",
      "Epoch: 16 Train loss 3.841, Test loss 3.855\n",
      "1 500\n",
      "Epoch: 17 Train loss 3.855, Test loss 3.906\n",
      "1 500\n",
      "Epoch: 18 Train loss 3.906, Test loss 3.956\n",
      "1 500\n",
      "Epoch: 19 Train loss 3.956, Test loss 3.985\n",
      "1 500\n",
      "Epoch: 20 Train loss 3.985, Test loss 3.986\n",
      "1 500\n",
      "Epoch: 21 Train loss 3.986, Test loss 3.962\n",
      "1 500\n",
      "Epoch: 22 Train loss 3.962, Test loss 3.923\n",
      "1 500\n",
      "Epoch: 23 Train loss 3.923, Test loss 3.881\n",
      "1 500\n",
      "Epoch: 24 Train loss 3.881, Test loss 3.849\n",
      "Mean train loss: 34596623.267 Mean val score: 34596618.086\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  0.\n",
      "  -2. -2.  1.  2.  0. -1. -1.  1. -2.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.01303253]] False\n",
      "Step 1: Reward 0.000.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  1.\n",
      "  -2.  4.  0.  2. -1. -1. -1.  9. -2.  0. -2. -1. -1. -2. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.34636587]] False\n",
      "Step 2: Reward 0.333.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   1.  4. -1.  2. -2. -1.  2. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.23525475]] False\n",
      "Step 3: Reward 0.222.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  1.  0. -1. -1. -2.  0.  0. -1. -2. -2.  2.\n",
      "   6.  4.  2.  5. -2.  1. -1. 12. -2.  3. -2. -1.  2. -2. -2. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.16520643]] False\n",
      "Step 4: Reward 0.152.\n",
      "[[ 0.  0.  0. -1.  0.  0. -1.  0.  0. -1.  1. -2.  0.  1. -1.  1. -2.  1.\n",
      "   6.  4.  5.  5. -2.  1. -1. 17. -2.  3. -2. -1.  2. -2. -2. -1.  0.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3071502]] False\n",
      "Step 5: Reward 0.294.\n",
      "[[ 0.  0.  0. -1.  0.  0.  0.  0.  0. -1.  3. -1.  1.  1. -1.  1. -2.  0.\n",
      "   5.  3.  5.  6. -2. -1. -1. 22. -2.  3. -2. -1.  2. -2. -2. -1.  1. -1.\n",
      "  -1. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.42212343]] False\n",
      "Step 6: Reward 0.409.\n",
      "[[ 1.  0.  0. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  1. -2.  0.\n",
      "   5.  3.  4.  4.  0.  0. -1. 24. -2.  1. -2.  2.  1. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.42969918]] False\n",
      "Step 7: Reward 0.417.\n",
      "[[ 1.  0.  2. -1.  1.  0.  0.  1.  1. -1.  4. -1.  3.  2. -1.  4. -2.  3.\n",
      "   5.  3.  4.  8.  2. -1. -1. 24. -2.  1. -2.  2.  4. -2. -2. -1.  1. -1.\n",
      "   0. -1. -1. -1.  1. -2. -2.  0.  0.  0.]] [[0.26303253]] True\n",
      "Step 8: Reward 0.250.\n",
      "Trial: 1, reward: 2.0776046914154334.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0.  0. -1. -2. -2.  1.\n",
      "  -2. -2.  1. -2.  7. -1. -1. -2. -2.  0. -2.  0. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.20350872]] False\n",
      "Step 9: Reward 0.190.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1.  0.  0. -1. -1. -1.  1.  0. -1. -2. -2.  0.\n",
      "  -2.  0.  4.  3.  7. -1. -1. -2. -1.  0. -2. -1. -1. -2. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.07824992]] False\n",
      "Step 10: Reward 0.065.\n",
      "1 510\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 1 Train loss 3.943, Test loss 3.948\n",
      "1 510\n",
      "Epoch: 2 Train loss 3.948, Test loss 3.943\n",
      "1 510\n",
      "Epoch: 3 Train loss 3.943, Test loss 3.930\n",
      "1 510\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.918\n",
      "1 510\n",
      "Epoch: 5 Train loss 3.918, Test loss 3.914\n",
      "1 510\n",
      "Epoch: 6 Train loss 3.914, Test loss 3.919\n",
      "1 510\n",
      "Epoch: 7 Train loss 3.919, Test loss 3.928\n",
      "1 510\n",
      "Epoch: 8 Train loss 3.928, Test loss 3.935\n",
      "1 510\n",
      "Epoch: 9 Train loss 3.935, Test loss 3.936\n",
      "1 510\n",
      "Epoch: 10 Train loss 3.936, Test loss 3.931\n",
      "1 510\n",
      "Epoch: 11 Train loss 3.931, Test loss 3.921\n",
      "1 510\n",
      "Epoch: 12 Train loss 3.921, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 13 Train loss 3.911, Test loss 3.903\n",
      "1 510\n",
      "Epoch: 14 Train loss 3.903, Test loss 3.902\n",
      "1 510\n",
      "Epoch: 15 Train loss 3.902, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 16 Train loss 3.905, Test loss 3.909\n",
      "1 510\n",
      "Epoch: 17 Train loss 3.909, Test loss 3.912\n",
      "1 510\n",
      "Epoch: 18 Train loss 3.912, Test loss 3.911\n",
      "1 510\n",
      "Epoch: 19 Train loss 3.911, Test loss 3.908\n",
      "1 510\n",
      "Epoch: 20 Train loss 3.908, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 21 Train loss 3.906, Test loss 3.904\n",
      "1 510\n",
      "Epoch: 22 Train loss 3.904, Test loss 3.905\n",
      "1 510\n",
      "Epoch: 23 Train loss 3.905, Test loss 3.906\n",
      "1 510\n",
      "Epoch: 24 Train loss 3.906, Test loss 3.906\n",
      "Mean train loss: 3.919 Mean val score: 3.918\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1.  0.  0. -1.  0. -1. -1.  0.\n",
      "  -1. -1.  7.  6. 10. -1. -1.  0. -1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20161985]] False\n",
      "Step 11: Reward 0.053.\n",
      "[[ 0.  0.  0.  1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -1. -1.  8.  8. 12. -1. -1.  0. -1.  0. -2. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.1531674]] False\n",
      "Step 12: Reward 0.102.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  0.\n",
      "  -1. -1.  8.  8. 11.  1. -1.  3.  1.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12490711]] False\n",
      "Step 13: Reward 0.130.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  0.\n",
      "   3.  0.  8.  8.  9.  1. -1.  3.  2.  0. -2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -1. -1.  0.  0. -2. -1. -1.  0.  0.  0.]] [[-0.04456946]] False\n",
      "Step 14: Reward 0.211.\n",
      "[[ 0.  0.  2. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0.  2. -1. -1.  0.  0.\n",
      "   4.  0.  9.  9. 10. -1. -1.  3.  3.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -1.  1.  6. -1. -2.  2. -1.  0.  0.  0.]] [[0.02421457]] False\n",
      "Step 15: Reward 0.279.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0. -1.  0. -1.  2.  0.  0. -1.  1. -1.  0.  0.\n",
      "   4. -1.  9.  8. 11.  0. -1.  3.  6.  0. -2. -1. -1. -1. -2.  0. -1.  0.\n",
      "  -2. -1.  6. -1. -1.  3. -1.  0.  0.  0.]] [[-0.01402435]] True\n",
      "Step 16: Reward 0.241.\n",
      "Trial: 2, reward: 1.2721946606988341.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   1.  1. -2. -1. -2.  1.  0. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13744873]] False\n",
      "Step 17: Reward 0.118.\n",
      "[[ 1.  0.  1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  1. -1. -1. -1.  4.\n",
      "   1.  1.  2.  1. -2.  1. -1. -2. -2.  0. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.10204709]] False\n",
      "Step 18: Reward 0.357.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  3.\n",
      "   5.  1.  2. -1. -2.  5.  0.  0. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -2.  0.  0.  0.]] [[0.06490421]] False\n",
      "Step 19: Reward 0.320.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0.\n",
      "   7.  1.  0.  2. -2.  6.  0.  1. -2.  0.  0.  2. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[0.00653213]] False\n",
      "Step 20: Reward 0.262.\n",
      "1 520\n",
      "Epoch: 0 Train loss 3.936, Test loss 3.935\n",
      "1 520\n",
      "Epoch: 1 Train loss 3.935, Test loss 3.933\n",
      "1 520\n",
      "Epoch: 2 Train loss 3.933, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 3 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 4 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 5 Train loss 3.930, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 6 Train loss 3.931, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 7 Train loss 3.932, Test loss 3.932\n",
      "1 520\n",
      "Epoch: 8 Train loss 3.932, Test loss 3.931\n",
      "1 520\n",
      "Epoch: 9 Train loss 3.931, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 10 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 11 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 12 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 13 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 14 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 15 Train loss 3.930, Test loss 3.930\n",
      "1 520\n",
      "Epoch: 16 Train loss 3.930, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 17 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 18 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 19 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 20 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 21 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 22 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 23 Train loss 3.929, Test loss 3.929\n",
      "1 520\n",
      "Epoch: 24 Train loss 3.929, Test loss 3.929\n",
      "Mean train loss: 3.931 Mean val score: 3.930\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1. -1. -1.  3. -1.\n",
      "   7.  4.  0.  5. -2.  4. -1.  1.  0.  1.  2.  0. -1. -1. -2.  1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.0207608]] False\n",
      "Step 21: Reward 0.322.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  1.  3. -1.\n",
      "   7.  4.  0.  5.  0.  4. -1.  1.  6. -1.  6. -1. -1. -1. -2. -1. -1.  0.\n",
      "  -2.  2. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.04603767]] False\n",
      "Step 22: Reward 0.296.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1.  3. -1.  1. -1. -1.  0. -1.  2.  1.  3.  1.\n",
      "   7.  4. -2.  5.  2.  4. -1.  1.  9. -1.  4. -1. -1. -1. -2. -1. -1.  0.\n",
      "   0.  4.  0. -1. -1. -2. -1.  0.  0.  0.]] [[-0.00511685]] False\n",
      "Step 23: Reward 0.337.\n",
      "[[ 0.  1. -1. -1.  0. -1.  2.  3.  0. -1. -1. -1.  0.  2.  1.  0.  3.  0.\n",
      "   7.  5.  3.  3.  3.  4. -1.  1. 16.  0.  3. -1.  1.  0. -2. -1. -1. -1.\n",
      "   0.  4.  2. -1.  1. -1. -1.  0.  0.  0.]] [[-0.02454716]] True\n",
      "Step 24: Reward 0.318.\n",
      "Trial: 3, reward: 2.3298202934631616.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   5. -2.  3.  0. -2. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.34246624]] False\n",
      "Step 25: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  2.\n",
      "   4. -2.  3.  6. -2. -1. -1. -2.  0. -1.  1.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3154392]] False\n",
      "Step 26: Reward 0.027.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "   3. -2.  3.  8.  4. -1. -1.  0.  0. -1.  1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.20785084]] False\n",
      "Step 27: Reward 0.135.\n",
      "[[ 0.  0.  0. -1.  1. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.\n",
      "   1.  1.  1. 11.  2. -1.  0.  0.  0.  0. -1. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.03691068]] False\n",
      "Step 28: Reward 0.306.\n",
      "[[-1.  0. -1.  0.  0. -1. -1.  2.  0. -1.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   1.  1.  0. 12.  2. -1.  0.  0. -1.  3. -1. -1. -1. -1. -2.  0. -1.  1.\n",
      "  -2. -1.  2. -1. -2. -1.  1.  0.  0.  0.]] [[-0.00592777]] False\n",
      "Step 29: Reward 0.337.\n",
      "[[-1.  1. -1. -1.  1. -1. -1.  1. -1.  0.  1. -1.  0. -1. -1.  1.  1. -1.\n",
      "   0.  0.  0. 12. -1. -1. -1.  3.  3.  2.  2. -1. -1. -1. -2. -1. -1.  3.\n",
      "   1.  0.  3. -1. -2. -1.  1.  0.  0.  0.]] [[0.0934312]] False\n",
      "Step 30: Reward 0.436.\n",
      "1 530\n",
      "Epoch: 0 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 1 Train loss 3.945, Test loss 3.945\n",
      "1 530\n",
      "Epoch: 2 Train loss 3.945, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 3 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 4 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 5 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 6 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 7 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 8 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 9 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 10 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 11 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 12 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 13 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 14 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 15 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 16 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 17 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 18 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 19 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 20 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 21 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 22 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 23 Train loss 3.944, Test loss 3.944\n",
      "1 530\n",
      "Epoch: 24 Train loss 3.944, Test loss 3.944\n",
      "Mean train loss: 3.944 Mean val score: 3.944\n",
      "[[-1.  0. -1.  0.  2. -1. -1.  0. -1. -1.  1. -1.  1.  1. -1.  1.  3. -1.\n",
      "  -1.  0.  1. 13. -2. -1.  0.  3.  5.  3.  2. -1. -1. -1. -2.  2. -1.  0.\n",
      "   0.  3.  4. -1. -2. -1.  1.  0.  0.  0.]] [[0.12568054]] False\n",
      "Step 31: Reward 0.442.\n",
      "[[-1.  0.  2.  0.  0. -1.  0.  0. -1. -1.  2. -1.  1.  2. -1.  3.  5.  0.\n",
      "  -1.  3.  1. 15. -2.  0.  0.  3.  6.  3.  4. -1. -1. -1. -2.  2.  1.  1.\n",
      "   1.  3.  4. -1. -2.  1.  1.  0.  0.  0.]] [[0.12454933]] True\n",
      "Step 32: Reward 0.441.\n",
      "Trial: 4, reward: 2.1231180275297925.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  1.\n",
      "  -2.  3. -2. -2.  3. -1. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31662714]] False\n",
      "Step 33: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1.  0.\n",
      "  -2.  5. -2. -2.  7. -1.  2. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.2990833]] False\n",
      "Step 34: Reward 0.018.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  1. -1.\n",
      "   1.  5. -2. -1.  7. -1. -1.  0.  1. -1. -2.  0.  2. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.12615095]] False\n",
      "Step 35: Reward 0.190.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2.  1. -1. -1.\n",
      "   4.  5. -2. -1.  5. -1. -1. -1.  3. -1.  0. -1.  0. -1. -2. -1. -1. -1.\n",
      "  -2.  0. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.11095339]] False\n",
      "Step 36: Reward 0.206.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0.  3. -1.  4.\n",
      "   8.  5. -2. -2.  8. -1. -1.  0. -1. -1.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -1.  0.  0.  0.  0.  0.]] [[-0.05114043]] False\n",
      "Step 37: Reward 0.265.\n",
      "[[-1.  0.  3. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -1.  1.  3.  0.  3.\n",
      "   8.  5. -2. -2. 10.  0. -1.  1. -1. -1. -2.  1. -1. -1. -2. -1.  1. -1.\n",
      "   0.  1. -1. -1. -1. -1.  0.  0.  0.  0.]] [[-0.04143333]] False\n",
      "Step 38: Reward 0.275.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  0.  3.  3.  6.\n",
      "   8.  5. -2. -1. 14. -1. -1.  1. -1. -1.  4.  5. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[0.11563092]] False\n",
      "Step 39: Reward 0.432.\n",
      "[[ 2.  1. -1. -1.  0. -1. -1. -1.  1.  2.  0.  0.  0.  0.  1.  3.  6.  6.\n",
      "   8.  7.  4. -1. 14. -1. -1.  1. -1. -1.  4.  4. -1. -1. -2.  1. -1.  2.\n",
      "  -2.  1. -1. -1.  0. -1.  0.  0.  0.  0.]] [[-0.06662714]] True\n",
      "Step 40: Reward 0.250.\n",
      "Trial: 5, reward: 1.6366323976200197.\n",
      "1 540\n",
      "Epoch: 0 Train loss 3.981, Test loss 3.981\n",
      "1 540\n",
      "Epoch: 1 Train loss 3.981, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 2 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 3 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 4 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 5 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 6 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 7 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 8 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 9 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 10 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 11 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 12 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 13 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 14 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 15 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 16 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 17 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 18 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 19 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 20 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 21 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 22 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 23 Train loss 3.980, Test loss 3.980\n",
      "1 540\n",
      "Epoch: 24 Train loss 3.980, Test loss 3.980\n",
      "Mean train loss: 3.980 Mean val score: 3.980\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  2. -1. -1.  1.\n",
      "  -2. -2.  0. -2.  0.  2. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3091395]] False\n",
      "Step 41: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -1.  3.\n",
      "   0.  0.  0.  0. -1.  2. -1.  1. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19086051]] False\n",
      "Step 42: Reward 0.500.\n",
      "[[-1.  0.  1. -1.  2. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0. -1. -1. -1.\n",
      "   0.  3.  1.  6. -2.  3. -1.  1. -2. -1. -2.  0. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.10858202]] False\n",
      "Step 43: Reward 0.418.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1.  0.  0.  2.  0.\n",
      "   1.  2.  3.  5. -2.  2. -1.  3. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.00556806]] False\n",
      "Step 44: Reward 0.304.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1.  0. -1.  2.  3.\n",
      "   4.  4.  5.  7. -1. -1. -1.  3. -2.  0. -1. -1. -1.  0. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.05224666]] False\n",
      "Step 45: Reward 0.361.\n",
      "[[-1.  0.  0.  1.  1.  0.  0. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1.  4.\n",
      "   4.  5.  5.  9.  2. -1. -1.  3. -2. -1.  3.  0. -1.  0. -2.  2. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.09974939]] False\n",
      "Step 46: Reward 0.409.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  0.  0.  1. -1.  0.  1.\n",
      "   7. 10.  7.  9.  1.  0. -1.  3. -1. -1.  3. -1. -1.  0. -1. -1. -1. -1.\n",
      "  -2.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[0.03917512]] False\n",
      "Step 47: Reward 0.348.\n",
      "[[ 0.  0. -1.  1.  0.  0.  4.  0.  0.  1.  0.  5.  3.  0.  1. -1.  3.  1.\n",
      "   7. 11.  7.  9.  2.  1. -1.  3. -1. -1.  3. -1. -1.  0. -1.  1. -1. -1.\n",
      "   0.  3. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10913949]] True\n",
      "Step 48: Reward 0.200.\n",
      "Trial: 6, reward: 2.539882581803094.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0.  0. -1.  1. -1. -1.\n",
      "  -2.  1. -2. -1.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.04247281]] False\n",
      "Step 49: Reward 0.267.\n",
      "[[-1.  1. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "  -2.  3.  0.  2. -2. -1.  1. -2. -2.  1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.0438017]] False\n",
      "Step 50: Reward 0.353.\n",
      "1 550\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 550\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1.  6. -1. -1.\n",
      "  -2.  8.  1.  2. -2. -1.  3. -2. -2.  1. -1. -1. -1. -1. -2.  1. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03686938]] False\n",
      "Step 51: Reward 0.277.\n",
      "[[-1.  0. -1. -1.  0. -1.  1. -1. -1. -1. -1. -1.  0.  0. -1.  7. -1. -1.\n",
      "  -2.  8.  1.  2.  0.  2.  0. -2.  1.  0. -2.  0. -1. -1. -2.  0.  0. -1.\n",
      "  -2.  0. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02903968]] False\n",
      "Step 52: Reward 0.285.\n",
      "[[ 0.  0.  0. -1.  0. -1.  2. -1. -1. -1. -1.  0.  0. -1.  1.  7. -1. -1.\n",
      "   2. 10.  0.  1. -1. -1. -1.  2.  0. -1. -2. -1.  0. -1. -2.  0. -1.  3.\n",
      "  -1. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[0.01174176]] False\n",
      "Step 53: Reward 0.325.\n",
      "[[-1.  1.  0. -1.  0. -1.  1.  0. -1.  2. -1.  0.  0. -1.  6.  7.  1. -1.\n",
      "   4.  7.  0.  0.  2. -1. -1.  2.  2. -1. -1.  1.  0.  0. -2. -1. -1.  3.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.17787033]] False\n",
      "Step 54: Reward 0.492.\n",
      "[[ 1.  1. -1. -1.  0. -1. -1. -1. -1.  4.  0.  2.  0.  0.  5.  7.  1.  1.\n",
      "   4.  8.  0.  0.  2. -1. -1.  3.  4. -1.  1.  1.  1. -1. -2. -1. -1.  2.\n",
      "   0.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.10942185]] False\n",
      "Step 55: Reward 0.423.\n",
      "[[ 0.  1. -1. -1.  1.  0. -1. -1. -1.  4.  2.  2.  0. -1.  7. 10.  1. -1.\n",
      "   4.  8.  0.  3.  2. -1. -1.  6.  4. -1.  1.  1.  2. -1. -2. -1. -1.  1.\n",
      "   2.  2. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.18634492]] True\n",
      "Step 56: Reward 0.500.\n",
      "Trial: 7, reward: 2.921008114240916.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  4.\n",
      "   0. -2. -2. -2. -1. -1.  1. -2. -2. -1. -2. -1. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.16365507]] False\n",
      "Step 57: Reward 0.150.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      "   3. -2. -2. -2. -1.  1.  0. -2. -1. -1. -1.  0. -1. -1. -2.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.01540947]] False\n",
      "Step 58: Reward 0.298.\n",
      "[[-1.  0.  0. -1.  0. -1. -1.  0.  1.  0. -1. -1.  2. -1. -1. -1.  4. -1.\n",
      "   8. -1. -2. -2. -1.  1. -1.  1. -2.  0.  0. -1. -1. -1. -2. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.00258425]] False\n",
      "Step 59: Reward 0.316.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0.  0.  0. -1. -1. -1.  7. -1.\n",
      "   7. -1. -1. -2. -2.  1. -1.  0. -2. -1. -2. -1.  1. -1. -2. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -1. -1.  0.  0.  0.]] [[0.04913563]] False\n",
      "Step 60: Reward 0.363.\n",
      "1 560\n",
      "Epoch: 0 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 1 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 2 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 3 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 4 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 5 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 6 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 7 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 8 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 9 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 10 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 11 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 12 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 13 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 14 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 15 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 16 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 17 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 18 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 19 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 20 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 21 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 22 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 23 Train loss 3.979, Test loss 3.979\n",
      "1 560\n",
      "Epoch: 24 Train loss 3.979, Test loss 3.979\n",
      "Mean train loss: 3.979 Mean val score: 3.979\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0.  1. -1.  1.  1. -1. -1. -1.  9.  0.\n",
      "   8. -2. -1. -1. -2.  2. -1.  0.  2. -1.  1.  1.  0. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2. -1. -1.  1. -1.  0.  0.  0.]] [[-0.05258617]] False\n",
      "Step 61: Reward 0.261.\n",
      "[[-1.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1. -1. -1. 14. -1.\n",
      "   9. -1.  1. -1. -2.  2. -1.  0.  6. -1.  0. -1. -1.  0. -1. -1. -1.  1.\n",
      "  -1.  0.  4. -1.  0.  1. -1.  0.  0.  0.]] [[-0.0411129]] False\n",
      "Step 62: Reward 0.272.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1.  1.  0.  1.  0.  1.  1.  0. 18. -1.\n",
      "   7.  0.  1.  1.  0.  0. -1.  0.  8. -1.  0. -1. -1.  0. -1. -1. -1.  0.\n",
      "  -1. -1.  6. -1.  0.  2.  0.  0.  0.  0.]] [[-0.03090695]] False\n",
      "Step 63: Reward 0.283.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1.  0.  0.  0.  1.  0. 18. -1.\n",
      "   7.  1.  1.  2.  0.  0.  1.  3.  9. -1.  4. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1.  6. -1.  0.  4.  0.  0.  0.  0.]] [[-0.07549722]] True\n",
      "Step 64: Reward 0.238.\n",
      "Trial: 8, reward: 2.1815422317524256.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  -2.  4.  1. -2. -2. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.31359246]] False\n",
      "Step 65: Reward 0.000.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   0.  4.  4.  1. -1. -1.  2.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.18640754]] False\n",
      "Step 66: Reward 0.500.\n",
      "[[-1.  0.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -1. -1.\n",
      "   4.  4.  6.  5. -2. -1. -1.  1. -2.  1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.13637727]] False\n",
      "Step 67: Reward 0.177.\n",
      "[[-1.  0.  1. -1.  0. -1.  1. -1.  1. -1.  1. -1.  0. -1. -1.  2.  0.  0.\n",
      "   2.  5.  5.  4. -2.  0.  0.  1. -2. -1. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.07549722]] False\n",
      "Step 68: Reward 0.238.\n",
      "[[-1.  0. -1.  0.  0. -1.  3.  0. -1. -1.  0. -1.  0. -1. -1.  4.  4.  0.\n",
      "   2.  4.  5.  9. -2. -1. -1.  1. -2.  0. -1. -1.  1.  0. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.00323921]] False\n",
      "Step 69: Reward 0.317.\n",
      "[[ 0.  0. -1.  1.  1.  0.  1. -1. -1. -1. -1. -1.  0. -1.  0.  1.  3. -1.\n",
      "   3.  5.  5. 17. -1. -1. -1.  1. -1. -1.  1.  0.  1.  0. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.02862975]] False\n",
      "Step 70: Reward 0.342.\n",
      "1 570\n",
      "Epoch: 0 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 1 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 2 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 3 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 4 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 5 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 6 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 7 Train loss 4.014, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 8 Train loss 4.015, Test loss 4.015\n",
      "1 570\n",
      "Epoch: 9 Train loss 4.015, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 10 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 11 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 12 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 13 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 14 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 15 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 16 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 17 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 18 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 19 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 20 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 21 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 22 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 23 Train loss 4.014, Test loss 4.014\n",
      "1 570\n",
      "Epoch: 24 Train loss 4.014, Test loss 4.014\n",
      "Mean train loss: 4.014 Mean val score: 4.014\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  0.  0.  0.  0. -1.  5.  1.  0.\n",
      "   3.  4.  6. 17.  3.  1. -1.  3.  2. -1.  1. -1.  1.  0. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.03747821]] False\n",
      "Step 71: Reward 0.275.\n",
      "[[ 1.  0. -1.  1.  0.  1.  2.  0.  0.  1.  2.  0.  0.  3. -1.  5.  6.  0.\n",
      "   3.  4.  6. 17.  1.  2. -1.  3.  2. -1.  1. -1.  2.  0. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.01275909]] True\n",
      "Step 72: Reward 0.300.\n",
      "Trial: 9, reward: 2.1496452322355992.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2. -2. -2. -2.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09053688]] False\n",
      "Step 73: Reward 0.222.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "  -2.  1. -2. -2.  4.  2. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19609243]] False\n",
      "Step 74: Reward 0.117.\n",
      "[[-1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "  -2.  1. -2.  1.  3.  1. -1.  1.  1. -1.  0. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -2.  0.  0.  0.]] [[-0.0705716]] False\n",
      "Step 75: Reward 0.242.\n",
      "[[-1.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.  5.  0.\n",
      "  -2.  1.  3.  3.  4.  1. -1.  1.  3. -1. -1. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.13094091]] False\n",
      "Step 76: Reward 0.182.\n",
      "[[-1.  0. -1. -1.  1. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0.  1.  5. -1.\n",
      "   0.  1.  3.  4.  5. -1. -1.  1.  8. -1. -2. -1.  0. -1. -1. -1. -1.  2.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.15935001]] False\n",
      "Step 77: Reward 0.153.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.  5.  5.  0.\n",
      "   0.  0.  2.  5.  1.  0. -1.  1. 13. -1. -1. -1.  0.  2.  4. -1. -1.  2.\n",
      "  -2. -1. -2. -1. -2. -1.  0.  0.  0.  0.]] [[-0.12814371]] False\n",
      "Step 78: Reward 0.185.\n",
      "[[-1.  0. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1.  2.  6.  5. -1.\n",
      "   0.  2.  0.  8.  3.  2. -1.  1. 13. -1. -2. -1.  0.  0.  4. -1. -1.  2.\n",
      "  -1.  0. -2. -1. -2. -2.  1.  0.  0.  0.]] [[-0.04515347]] False\n",
      "Step 79: Reward 0.268.\n",
      "[[-1.  0. -1. -1.  0.  0. -1.  0. -1.  1.  3.  0.  0. -1.  2.  6.  5.  2.\n",
      "  -1.  2.  0.  9.  5.  1. -1.  3. 14. -1. -2.  3.  0. -1.  4. -1. -1.  2.\n",
      "   0.  0. -1. -1. -2. -2.  2.  0.  0.  0.]] [[-0.07201836]] True\n",
      "Step 80: Reward 0.241.\n",
      "Trial: 10, reward: 1.609265420775104.\n",
      "1 580\n",
      "Epoch: 0 Train loss 4.045, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 1 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 2 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 3 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 4 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 5 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 6 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 7 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 8 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 9 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 10 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 11 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 12 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 13 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 14 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 15 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 16 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 17 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 18 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 19 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 20 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 21 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 22 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 23 Train loss 4.044, Test loss 4.044\n",
      "1 580\n",
      "Epoch: 24 Train loss 4.044, Test loss 4.044\n",
      "Mean train loss: 4.044 Mean val score: 4.044\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0. -1. -2. -1.  5. -2. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.23142757]] False\n",
      "Step 81: Reward 0.080.\n",
      "[[-1.  0. -1. -1.  0. -1. -1.  0.  0. -1. -1. -1.  0. -1. -1. -1. -2.  4.\n",
      "   0. -2.  0. -1. -2. -1.  7. -2. -2.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.07458545]] False\n",
      "Step 82: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1. -1. -1.  0. -1. -1.  0. -1.  1. -1. -1. -1.  1.  3.\n",
      "   9. -2.  0. -2. -2. -1.  5.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[0.01615864]] False\n",
      "Step 83: Reward 0.328.\n",
      "[[-1.  0. -1.  0.  0. -1. -1. -1. -1. -1.  2. -1.  0. -1. -1. -1.  1.  3.\n",
      "  11.  4.  0. -2. -2. -1. -1.  8. -2. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  1. -1. -1. -2. -1. -2.  0.  0.  0.]] [[0.05220881]] False\n",
      "Step 84: Reward 0.364.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  1. -1.  0. -1.  0. -1.  1.  1.\n",
      "   9.  4.  2. -2. -2.  0. -1. 13. -2. -1. -2.  4. -1.  1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -1.  0. -2.  0.  0.  0.]] [[-0.08013505]] False\n",
      "Step 85: Reward 0.231.\n",
      "[[-1.  0.  0.  0.  0. -1.  1. -1. -1.  0. -1. -1.  2.  1.  3. -1.  2. -1.\n",
      "   8.  5.  2.  1. -1. -1.  0. 13. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2.  0. -2. -1. -2.  0. -2.  0.  0.  0.]] [[0.09568706]] False\n",
      "Step 86: Reward 0.407.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  6.  0.  0. -1.  2.  1.  4.  0.\n",
      "  10.  5.  1.  3.  0. -1. -1. 12. -2. -1.  4. -1.  0.  1. -1. -1. -1. -1.\n",
      "  -1. -1.  2. -1.  0.  0. -1.  0.  0.  0.]] [[0.07197165]] False\n",
      "Step 87: Reward 0.383.\n",
      "[[-1.  0.  0. -1.  0.  1. -1. -1.  0.  1.  3. -1.  0.  0.  3.  1.  4.  1.\n",
      "  10.  5.  1.  4. -2. -1.  0. 12. -2. -1.  7. -1. -1. -1.  0. -1. -1. -1.\n",
      "   3. -1.  2. -1.  4.  0. -1.  0.  0.  0.]] [[-0.0816139]] True\n",
      "Step 88: Reward 0.230.\n",
      "Trial: 11, reward: 2.2596846913912443.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  2. -1. -2.  1.  0. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19604295]] False\n",
      "Step 89: Reward 0.115.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  3.\n",
      "  -1.  2.  0. -2.  0.  6. -1. -2. -2.  1. -2. -1. -1. -1. -1.  0. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19142756]] False\n",
      "Step 90: Reward 0.120.\n",
      "1 590\n",
      "Epoch: 0 Train loss 4.066, Test loss 4.066\n",
      "1 590\n",
      "Epoch: 1 Train loss 4.066, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 2 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 3 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 4 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 5 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 6 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 7 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 8 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 9 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 10 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 11 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 12 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 13 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 14 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 15 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 16 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 17 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 18 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 19 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 20 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 21 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 22 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 23 Train loss 4.065, Test loss 4.065\n",
      "1 590\n",
      "Epoch: 24 Train loss 4.065, Test loss 4.065\n",
      "Mean train loss: 4.065 Mean val score: 4.065\n",
      "[[-1.  2. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  3.  3.  0.  0.  5. -1.  1.  1. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -1. -1. -2. -2. -2.  0.  0.  0.]] [[-0.22695008]] False\n",
      "Step 91: Reward 0.083.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1.  1. -1.\n",
      "   0.  3.  3.  3. -1.  9. -1.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2.  0. -2. -1. -1. -2. -2.  0.  0.  0.]] [[-0.13638127]] False\n",
      "Step 92: Reward 0.173.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1.  1. -1.\n",
      "  -1.  4.  7. -1.  0.  7. -1.  5. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.\n",
      "   2.  0. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.12800482]] False\n",
      "Step 93: Reward 0.182.\n",
      "[[-1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0.  0. -1. -1.  4. -1.\n",
      "  -1.  5.  9.  0. -2.  7. -1.  5. -2. -1.  6. -1. -1. -1. -1. -1. -1.  1.\n",
      "   5. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.07412507]] False\n",
      "Step 94: Reward 0.236.\n",
      "[[-1.  0. -1.  0.  0.  1.  0. -1. -1. -1.  0.  0.  1. -1.  4. -1.  5. -1.\n",
      "  -1.  2.  9. -2.  0.  8.  0.  7. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1.\n",
      "   9. -1. -1. -1. -1. -2.  2.  0.  0.  0.]] [[-0.06379126]] False\n",
      "Step 95: Reward 0.246.\n",
      "[[-1.  0.  1.  1.  0. -1. -1.  0. -1. -1.  1. -1.  1. -1.  4.  1.  5. -1.\n",
      "   0.  2. 11. -2. -1.  8. -1.  8. -1. -1.  1. -1. -1. -1.  0. -1. -1.  1.\n",
      "  11. -1. -1. -1. -1. -2.  4.  0.  0.  0.]] [[0.02351034]] True\n",
      "Step 96: Reward 0.333.\n",
      "Trial: 12, reward: 1.4885804796654487.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  0.  3. -1.  0. -1. -1. -2. -2. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.19217595]] False\n",
      "Step 97: Reward 0.118.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2.  0.\n",
      "  -2.  0.  3. -1. -1.  2. -1. -2.  1. -1.  3. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.09243171]] False\n",
      "Step 98: Reward 0.217.\n",
      "[[-1.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  1. -2. -1.\n",
      "  -2.  0.  3.  4. -2.  3.  0. -2.  1. -1.  3.  5. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.159823]] False\n",
      "Step 99: Reward 0.150.\n",
      "[[-1.  2.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  3. -2. -1.\n",
      "   1. -2.  4. 11. -1.  6. -1. -2. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1.\n",
      "  -2. -1. -2. -1. -2. -1. -2.  0.  0.  0.]] [[-0.15090828]] False\n",
      "Step 100: Reward 0.159.\n",
      "1 600\n",
      "Epoch: 0 Train loss 4.076, Test loss 4.076\n",
      "1 600\n",
      "Epoch: 1 Train loss 4.076, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 2 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 3 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 4 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 5 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 6 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 7 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 8 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 9 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 10 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 11 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 12 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 13 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 14 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 15 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 16 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 17 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 18 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 19 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 20 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 21 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 22 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 23 Train loss 4.075, Test loss 4.075\n",
      "1 600\n",
      "Epoch: 24 Train loss 4.075, Test loss 4.075\n",
      "Mean train loss: 4.075 Mean val score: 4.075\n",
      "[[ 0.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1. -1.  4. -2. -1.\n",
      "   2. -2.  5. 15. -1.  3. -1.  1. -1. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2.  0. -2. -1. -2.  0.  0.  0.]] [[-0.07353824]] False\n",
      "Step 101: Reward 0.235.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.  6. -2. -1.\n",
      "   3. -2.  5. 16. -2.  1.  0.  4.  2.  0. -2. -1. -1.  0. -1.  1.  1.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.05061775]] False\n",
      "Step 102: Reward 0.258.\n",
      "[[ 0.  0.  0. -1.  1. -1.  0. -1.  0.  2.  2. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  5. 18.  0.  5.  0.  5.  2. -1. -2.  1. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.03281066]] False\n",
      "Step 103: Reward 0.276.\n",
      "[[ 0.  0. -1. -1.  1. -1.  0. -1.  0.  2.  4. -1.  0. -1.  1.  6. -2. -1.\n",
      "   6. -2.  7. 18.  0.  5.  0.  5.  2.  4. -2.  5. -1.  2. -1. -1.  2.  0.\n",
      "   2. -1. -2.  1. -2. -1. -2.  0.  0.  0.]] [[-0.01420319]] True\n",
      "Step 104: Reward 0.294.\n",
      "Trial: 13, reward: 1.7060666329221275.\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 4. 0. 0.\n",
      "  0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0.6412164]] False\n",
      "Step 105: Reward 0.250.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   0.  0.  0.  2.  1. -1.  0.  3. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19167915]] False\n",
      "Step 106: Reward 0.500.\n",
      "[[ 0.  0.  0. -1.  0. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1. -1.  1.  1.\n",
      "   0.  3. -2.  3.  0.  0. -1.  2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03309149]] False\n",
      "Step 107: Reward 0.275.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  1. -1. -1.  1.  2.\n",
      "   0.  5.  1.  5.  2. -1.  0.  2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -1. -1. -2. -2. -1.  0.  0.  0.]] [[-0.02783304]] False\n",
      "Step 108: Reward 0.280.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0.  0. -1. -1.  7.  1.\n",
      "   2.  4.  0.  6.  0. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1.  0. -2.  0.  0.  0.  0.]] [[-0.03641751]] False\n",
      "Step 109: Reward 0.272.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  0.  3.  5. -1.\n",
      "   1.  4.  1.  8. -1. -1. -1.  0. -2. -1. -2. -1. -1. -1.  0. -1. -1.  0.\n",
      "  -2.  1.  1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.04998752]] False\n",
      "Step 110: Reward 0.258.\n",
      "1 610\n",
      "Epoch: 0 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 1 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 2 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 3 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 4 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 5 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 6 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 7 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 8 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 9 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 10 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 11 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 12 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 13 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 14 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 15 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 16 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 17 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 18 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 19 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 20 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 21 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 22 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 23 Train loss 4.097, Test loss 4.097\n",
      "1 610\n",
      "Epoch: 24 Train loss 4.097, Test loss 4.097\n",
      "Mean train loss: 4.097 Mean val score: 4.097\n",
      "[[ 0.  0.  0. -1.  0.  1.  0. -1.  2.  1. -1.  2.  0.  3. -1.  3.  7.  3.\n",
      "   1.  6.  1.  9. -2. -1. -1.  1. -1.  0. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[-0.07657747]] False\n",
      "Step 111: Reward 0.232.\n",
      "[[ 0.  0. -1. -1.  0.  1. -1. -1.  4.  1. -1.  2.  0.  3. -1.  6.  9.  6.\n",
      "   1.  6.  1.  9.  0. -1. -1.  1.  2.  0.  1. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  2.  1. -1.  1. -1.  0.  0.  0.  0.]] [[0.01433161]] True\n",
      "Step 112: Reward 0.323.\n",
      "Trial: 14, reward: 2.390206018685877.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1.  0. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2. -2.  0.  1.  1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.20824903]] False\n",
      "Step 113: Reward 0.100.\n",
      "[[ 0.  1. -1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  0. -1. -1.  0. -2. -1.\n",
      "   1.  0.  1.  0.  3.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1.  0. -1.\n",
      "  -2.  0. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.03165329]] False\n",
      "Step 114: Reward 0.277.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0.  1. -1.  0. -2. -1.\n",
      "   2.  1.  1. -2.  4.  1.  1. -2. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "   0.  0. -2.  0. -2. -1. -2.  0.  0.  0.]] [[0.0080775]] False\n",
      "Step 115: Reward 0.316.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1.  1.  0. -1.  0.  0. -1. -1. -2.  0.\n",
      "   8.  2.  2. -1.  5. -1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1.  1.\n",
      "  -2. -1. -2.  0.  0. -1. -2.  0.  0.  0.]] [[0.07865575]] False\n",
      "Step 116: Reward 0.387.\n",
      "[[ 0.  0. -1. -1.  0. -1.  0. -1. -1.  0.  2.  1.  0.  0. -1. -1.  1.  1.\n",
      "   9.  1.  2. -1.  6.  0. -1. -2. -2. -1. -2. -1.  0. -1. -1.  2.  0. -1.\n",
      "  -2. -1. -2.  0.  0.  0. -2.  0.  0.  0.]] [[0.05909792]] False\n",
      "Step 117: Reward 0.367.\n",
      "[[ 0.  2. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0.  2.  1. -1.\n",
      "   9.  2.  5.  6.  5.  0.  0. -2. -1. -1. -2.  0.  0. -1. -1. -1. -1. -1.\n",
      "   1.  1. -2.  0.  2.  1. -2.  0.  0.  0.]] [[0.04360282]] False\n",
      "Step 118: Reward 0.352.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  1.  1.  6. -1.\n",
      "   9.  3.  5.  6.  4.  5. -1. -2.  1. -1. -2.  0. -1. -1. -1. -1. -1. -1.\n",
      "   1.  1.  2.  1.  2.  1. -1.  0.  0.  0.]] [[-0.09203281]] False\n",
      "Step 119: Reward 0.216.\n",
      "[[ 0.  4. -1.  0.  0. -1. -1. -1. -1. -1. -1.  2.  0. -1.  4.  0.  5.  0.\n",
      "   9.  2.  8.  7.  2.  9. -1. -2.  1. -1. -2. -1. -1. -1. -1. -1.  2. -1.\n",
      "   1.  2.  1.  1.  2.  1. -1.  0.  0.  0.]] [[-0.14450634]] True\n",
      "Step 120: Reward 0.164.\n",
      "Trial: 15, reward: 2.1789847340999158.\n",
      "1 620\n",
      "Epoch: 0 Train loss 4.102, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 1 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 2 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 3 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 4 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 5 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 6 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 7 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 8 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 9 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 10 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 11 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 12 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 13 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 14 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 15 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 16 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 17 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 18 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 19 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 20 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 21 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 22 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 23 Train loss 4.101, Test loss 4.101\n",
      "1 620\n",
      "Epoch: 24 Train loss 4.101, Test loss 4.101\n",
      "Mean train loss: 4.101 Mean val score: 4.101\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   1. -2.  2. -2.  0.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 121: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  2. -1.\n",
      "   0.  0.  5. -2.  2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.19229937]] False\n",
      "Step 122: Reward 0.500.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0. -1.  4.  1.\n",
      "  -1. -1.  7. -2.  2.  1. -1. -2.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -1. -1. -1. -2.  0.  0.  0.  0.]] [[-0.13378759]] False\n",
      "Step 123: Reward 0.174.\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1.  4. -1.\n",
      "   1.  1.  7. -2. -2.  1. -1. -2.  4. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2.  0.  0.  0.  0.]] [[-0.10890545]] False\n",
      "Step 124: Reward 0.199.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0.  1. -1. -1.  0.  0.  0. -1.  2.  7. -1.\n",
      "   0.  2.  6.  0. -2.  0.  0. -2.  3. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2.  0. -1.  0. -1. -2.  0.  0.  0.  0.]] [[-0.07061857]] False\n",
      "Step 125: Reward 0.237.\n",
      "[[ 0.  0. -1. -1.  1.  3. -1.  0. -1. -1. -1.  1.  0.  3. -1.  2.  7. -1.\n",
      "   0.  1.  6.  0. -1. -1. -1. -2.  7.  2.  0. -1. -1. -1. -1. -1.  0. -1.\n",
      "   1. -1. -2. -1.  0. -2.  0.  0.  0.  0.]] [[-0.01636204]] False\n",
      "Step 126: Reward 0.291.\n",
      "[[ 1.  1. -1. -1.  1.  0.  0. -1. -1.  3. -1.  2.  0.  3. -1.  4.  7.  0.\n",
      "   2.  3.  6.  2. -1.  0.  2.  0.  8.  2. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.01676744]] False\n",
      "Step 127: Reward 0.324.\n",
      "[[ 1.  2. -1. -1.  2.  0. -1. -1. -1.  3. -1.  2.  1.  3. -1.  4. 10.  0.\n",
      "   2.  3.  6.  5. -1.  0.  2.  3. 12.  1. -2.  0. -1. -1. -1. -1.  2. -1.\n",
      "  -2. -1. -2. -1.  1. -2.  0.  0.  0.  0.]] [[0.02563271]] True\n",
      "Step 128: Reward 0.333.\n",
      "Trial: 16, reward: 2.058930292187335.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   2.  3. -2. -2. -2.  1. -1. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.30770063]] False\n",
      "Step 129: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "   3.  2.  3. -2. -1.  2. -1. -2. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.11722444]] False\n",
      "Step 130: Reward 0.190.\n",
      "1 630\n",
      "Epoch: 0 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 1 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 2 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 3 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 4 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 5 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 6 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 7 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 8 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 9 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 10 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 11 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 12 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 13 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 14 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 15 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 16 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 17 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 18 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 19 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 20 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 21 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 22 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 23 Train loss 4.092, Test loss 4.092\n",
      "1 630\n",
      "Epoch: 24 Train loss 4.092, Test loss 4.092\n",
      "Mean train loss: 4.092 Mean val score: 4.092\n",
      "[[ 1.  0.  0. -1.  0. -1. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0. -1.\n",
      "   7.  1.  5. -2. -1.  1. -1. -2. -1. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "   0. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.05655059]] False\n",
      "Step 131: Reward 0.250.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1.  2. -1.\n",
      "   6.  3.  6. -1. -1.  0.  0. -2.  2. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -1. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.06126757]] False\n",
      "Step 132: Reward 0.245.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   5.  5.  6.  4.  0. -1.  0. -2.  4. -1. -2.  0. -1. -1. -1.  1. -1.  0.\n",
      "   0. -1. -2. -1. -1. -1. -2.  0.  0.  0.]] [[-0.0413332]] False\n",
      "Step 133: Reward 0.265.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  5. -1.\n",
      "   7.  6.  5. 10. -1. -1. -1. -2.  4. -1. -2. -1. -1. -1.  0.  0. -1.  0.\n",
      "   0.  1. -2. -1.  1.  0. -1.  0.  0.  0.]] [[-0.11754715]] False\n",
      "Step 134: Reward 0.189.\n",
      "[[ 0.  1. -1. -1.  0. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1.  0.  5. -1.\n",
      "   7.  9.  3. 10. -2. -1. -1.  2.  6. -1. -2. -1. -1.  0. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.18106039]] False\n",
      "Step 135: Reward 0.125.\n",
      "[[ 1.  0. -1. -1.  0.  0. -1. -1. -1. -1.  0. -1.  0. -1. -1.  4.  5. -1.\n",
      "   7. 10.  6.  7. -1. -1. -1.  2.  7.  0. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "   1.  5. -2. -1.  2.  0. -1.  0.  0.  0.]] [[-0.08027323]] True\n",
      "Step 136: Reward 0.226.\n",
      "Trial: 17, reward: 1.4917476054157848.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  1.  1.  1.  1. -1.  2. -2. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[-0.3065506]] False\n",
      "Step 137: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  1. -1. -2. -1.\n",
      "  -2.  4.  1.  1.  3. -1.  3. -2. -2. -1. -2.  2. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -2.  0.  0.  0.]] [[0.3601161]] False\n",
      "Step 138: Reward 0.667.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  2. -1. -1. -1. -1.  0. -1.  0. -1. -2. -1.\n",
      "  -2.  6.  3.  5. -2.  0.  0.  3. -2. -1. -2.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[0.04639059]] False\n",
      "Step 139: Reward 0.353.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1.  0. -1. -1. -1. -1.  1. -1.  3. -1.  1. -1.\n",
      "  -2.  9.  2.  7. -2. -1. -1.  3. -2. -1. -1. -1.  2. -1. -1. -1.  0.  0.\n",
      "  -2. -1. -2. -1.  0. -2. -2.  0.  0.  0.]] [[-0.00655058]] False\n",
      "Step 140: Reward 0.300.\n",
      "1 640\n",
      "Epoch: 0 Train loss 4.094, Test loss 4.094\n",
      "1 640\n",
      "Epoch: 1 Train loss 4.094, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 2 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 3 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 4 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 5 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 6 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 7 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 8 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 9 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 10 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 11 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 12 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 13 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 14 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 15 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 16 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 17 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 18 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 19 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 20 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 21 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 22 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 23 Train loss 4.093, Test loss 4.093\n",
      "1 640\n",
      "Epoch: 24 Train loss 4.093, Test loss 4.093\n",
      "Mean train loss: 4.093 Mean val score: 4.093\n",
      "[[ 0.  0.  0. -1.  0. -1. -1. -1. -1. -1.  0.  0.  0. -1.  4.  0.  2. -1.\n",
      "  -1. 11.  2.  7. -2. -1. -1.  2.  0.  0.  0. -1.  5. -1. -1. -1. -1.  0.\n",
      "  -2. -1. -2. -1.  1. -1.  0.  0.  0.  0.]] [[-0.04637557]] False\n",
      "Step 141: Reward 0.259.\n",
      "[[ 0.  1. -1.  0.  0. -1. -1. -1. -1. -1. -1.  1.  0. -1.  5.  0.  3.  0.\n",
      "  -1.  9.  4.  7.  3. -1. -1.  2.  1.  0.  0. -1.  3. -1. -1.  1. -1. -1.\n",
      "  -2.  0. -2. -1.  1.  0.  0.  0.  0.  0.]] [[-0.06239158]] False\n",
      "Step 142: Reward 0.243.\n",
      "[[ 1.  0. -1.  1.  0. -1. -1. -1. -1.  0. -1.  1.  0.  0.  4.  0.  4.  0.\n",
      "   1. 14.  4.  7.  8.  1. -1.  3.  3. -1. -1. -1.  3. -1. -1. -1.  0.  0.\n",
      "  -1.  0. -2. -1.  1.  0.  2.  0.  0.  0.]] [[0.00300714]] False\n",
      "Step 143: Reward 0.309.\n",
      "[[ 1.  0. -1.  2.  0. -1. -1. -1. -1.  0. -1.  1.  0.  3.  4.  0.  4.  0.\n",
      "   1. 14.  5.  7. 11.  1. -1.  8.  3. -1. -2. -1.  3.  1. -1. -1. -1. -1.\n",
      "  -1.  0. -2. -1.  1.  0.  3.  0.  0.  0.]] [[0.08325407]] True\n",
      "Step 144: Reward 0.389.\n",
      "Trial: 18, reward: 2.519641209837288.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2. -1.\n",
      "  -2.  4. -2. -1.  1. -1. -1.  1. -2. -1. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.2671733]] False\n",
      "Step 145: Reward 0.038.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "   2.  6. -1. -1.  0.  0. -1.  1. -2. -1. -2. -1. -1. -1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.18563482]] False\n",
      "Step 146: Reward 0.120.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -2. -1.\n",
      "   2.  7. -1.  2.  2. -1.  0.  4.  1. -1. -2. -1. -1. -1. -1. -1. -1.  0.\n",
      "  -2. -1.  0. -1. -2. -2. -1.  0.  0.  0.]] [[-0.21723703]] False\n",
      "Step 147: Reward 0.088.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1. -1. -1. -1.\n",
      "   4.  8. -1.  2.  4.  0. -1.  6. -2. -1. -2. -1. -1.  1. -1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.14845325]] False\n",
      "Step 148: Reward 0.157.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1.  3. -1.  0. -1. -1. -1. -1. -1.\n",
      "   6. 11.  4. -3.  3. -1. -1.  6. -1. -1.  1. -1. -1.  0. -1. -1. -1. -1.\n",
      "   2. -1. -2. -1. -2.  0.  0.  0.  0.  0.]] [[-0.11958832]] False\n",
      "Step 149: Reward 0.186.\n",
      "[[ 0.  1. -1. -1.  1. -1. -1. -1. -1. -1.  0. -1.  0. -1. -1. -1.  2. -1.\n",
      "   6. 11.  5. -1.  1.  0. -1.  7. -2. -1.  3. -1. -1. -1. -1. -1. -1. -1.\n",
      "   4. -1.  1. -1. -2.  0.  0.  0.  0.  0.]] [[-0.13629845]] False\n",
      "Step 150: Reward 0.169.\n",
      "1 650\n",
      "Epoch: 0 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 1 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 2 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 3 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 4 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 5 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 6 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 7 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 8 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 9 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 10 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 11 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 12 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 13 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 14 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 15 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 16 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 17 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 18 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 19 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 20 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 21 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 22 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 23 Train loss 4.115, Test loss 4.115\n",
      "1 650\n",
      "Epoch: 24 Train loss 4.115, Test loss 4.115\n",
      "Mean train loss: 4.115 Mean val score: 4.115\n",
      "[[ 0.  0. -1.  0.  0.  1.  0. -1. -1. -1. -1.  0.  0. -1.  1. -1.  0. -1.\n",
      "   6.  8.  6. -3.  0. -1. -1. 10. -1.  0.  0. -1. -1.  0.  0. -1. -1. -1.\n",
      "   8. -1.  1. -1. -1. -1.  3.  0.  0.  0.]] [[-0.00797501]] False\n",
      "Step 151: Reward 0.296.\n",
      "[[ 0.  0.  1.  1.  1. -1. -1.  0.  0. -1.  1. -1.  0. -1.  1. -1.  3. -1.\n",
      "   8.  8.  6. -3. -2.  3. -1. 14. -1.  2.  0. -1. -1. -1.  0. -1. -1. -1.\n",
      "  10. -1.  0. -1. -1. -2.  5.  0.  0.  0.]] [[0.00049061]] True\n",
      "Step 152: Reward 0.305.\n",
      "Trial: 19, reward: 1.3604819974579725.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  2.\n",
      "  -2.  1. -2.  4. -2. -1. -1. -2. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[-0.3042713]] False\n",
      "Step 153: Reward 0.000.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -2.  1.\n",
      "  -2.  3. -2.  4.  0.  2.  2.  1. -2.  0. -2. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 154: Reward 0.500.\n",
      "[[ 0.  1.  2. -1.  0. -1. -1. -1. -1. -1. -1. -1.  1. -1.  2. -1. -2. -1.\n",
      "   1.  3. -2.  4.  2.  2.  0.  1. -2.  2. -2. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.19572869]] False\n",
      "Step 155: Reward 0.500.\n",
      "[[ 0.  0.  1. -1.  1. -1. -1.  0. -1.  1.  1. -1.  0. -1.  2. -1. -2. -1.\n",
      "   1.  3.  3.  6.  2.  2.  0.  1. -2.  0. -2.  0.  1. -1.  1. -1. -1. -1.\n",
      "  -2. -1. -2. -1. -2. -2. -1.  0.  0.  0.]] [[0.09046555]] False\n",
      "Step 156: Reward 0.395.\n",
      "[[ 0.  0. -1. -1.  0. -1. -1. -1.  1. -1. -1. -1.  2. -1.  2. -1. -2. -1.\n",
      "   0.  3.  3.  7.  2.  6.  1.  1. -2.  0.  1. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -2. -1.  0.  0.  0.]] [[-0.03641418]] False\n",
      "Step 157: Reward 0.268.\n",
      "[[ 0.  0. -1.  2.  0. -1.  1. -1. -1.  0. -1.  0.  2.  0.  2. -1. -2. -1.\n",
      "   0.  3.  3.  6.  2. 11.  0.  1.  0.  0.  2. -1.  3. -1.  1.  0. -1. -1.\n",
      "  -2. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.02685452]] False\n",
      "Step 158: Reward 0.331.\n",
      "[[ 0.  0. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  0.  2.  0.  2. -1.\n",
      "   0.  3.  4.  6.  3. 15. -1.  1.  2.  2.  1.  0.  3. -1.  4. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[-0.12780072]] False\n",
      "Step 159: Reward 0.176.\n",
      "[[ 0.  1. -1.  2.  0. -1. -1. -1.  0.  2. -1.  0.  2.  2.  5.  2.  2. -1.\n",
      "   3.  3.  4.  6.  3. 16. -1.  1.  3.  1.  1.  0. -1. -1.  2. -1. -1. -1.\n",
      "  -1. -1. -2. -1. -1. -1. -1.  0.  0.  0.]] [[0.2671573]] True\n",
      "Step 160: Reward 0.571.\n",
      "Trial: 20, reward: 2.7416189724408406.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env_steps = 0\n",
    "current_trial = 0\n",
    "rewards = np.empty((num_episodes, env_config[\"action_per_day\"]))\n",
    "all_training_losses = []\n",
    "all_val_scores = []\n",
    "\n",
    "while current_trial < num_episodes:\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Make 1 episode\n",
    "    while not terminated and not truncated:\n",
    "        # --------------- Model Training -----------------\n",
    "        if env_steps % freq_train_model == 0:\n",
    "            dataset_train, dataset_val = mbrl.util.common.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                model_batch_size,\n",
    "                validation_ratio,\n",
    "                ensemble_size=len(dynamics_model),\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,\n",
    "            )\n",
    "            if hasattr(dynamics_model, \"update_normalizer\"):\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "            training_losses, val_scores = model_trainer.train(\n",
    "                dataset_train,\n",
    "                dataset_val=dataset_val,\n",
    "                num_epochs=num_epochs,\n",
    "                patience=num_epochs,\n",
    "                improvement_threshold=0.01,\n",
    "            )\n",
    "            all_training_losses += training_losses\n",
    "            all_val_scores += val_scores\n",
    "            print(\n",
    "                f\"Mean train loss: {np.mean(training_losses):.3f} Mean val score: {np.mean(val_scores):.3f}\"\n",
    "            )\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        replay_buffer.add(obs, action, next_obs, reward, terminated, truncated)\n",
    "\n",
    "        model_action = action[None, ...]\n",
    "        model_observation = {\n",
    "            \"obs\": obs[None, ...],\n",
    "            \"propagation_indices\": None,\n",
    "        }\n",
    "        model_next_obs, model_reward, model_dones, next_model_state = model_env.step(\n",
    "            model_action, model_observation\n",
    "        )\n",
    "        # TODO: compare actual next_obs and rewards with model env\n",
    "        print(\n",
    "            next_obs - torch.round(model_next_obs).numpy(),\n",
    "            reward - model_reward.numpy(),\n",
    "            terminated,\n",
    "        )\n",
    "\n",
    "        rewards[current_trial, steps_trial] = reward\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        env_steps += 1\n",
    "\n",
    "        print(f\"Step {env_steps}: Reward {reward:.3f}.\")\n",
    "\n",
    "    current_trial += 1\n",
    "    print(f\"Trial: {current_trial }, reward: {total_reward}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(all_training_losses, \"b\", label=\"Training loss\")\n",
    "ax[0].plot(all_val_scores, \"g\", label=\"Eval score\")\n",
    "ax[0].set_xlabel(\"train epoch\")\n",
    "ax[1].plot(np.mean(rewards, axis=-1), \"b\", label=\"Cumulative reward\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
